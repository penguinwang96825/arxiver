{"title": "Can Linguistic Distance help Language Classification? Assessing\n  Hawrami-Zaza and Kurmanji-Sorani", "abstract": "To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.", "published": "2021-10-27 12:52:19", "link": "http://arxiv.org/abs/2110.14398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FacTeR-Check: Semi-automated fact-checking through Semantic Similarity\n  and Natural Language Inference", "abstract": "Our society produces and shares overwhelming amounts of information through\nOnline Social Networks (OSNs). Within this environment, misinformation and\ndisinformation have proliferated, becoming a public safety concern in most\ncountries. Allowing the public and professionals to efficiently find reliable\nevidences about the factual veracity of a claim is a crucial step to mitigate\nthis harmful spread. To this end, we propose FacTeR-Check, a multilingual\narchitecture for semi-automated fact-checking that can be used for either\napplications designed for the general public and by fact-checking\norganisations. FacTeR-Check enables retrieving fact-checked information,\nunchecked claims verification and tracking dangerous information over social\nmedia. This architectures involves several modules developed to evaluate\nsemantic similarity, to calculate natural language inference and to retrieve\ninformation from Online Social Networks. The union of all these components\nbuilds a semi-automated fact-checking tool able of verifying new claims, to\nextract related evidence, and to track the evolution of a hoax on a OSN. While\nindividual modules are validated on related benchmarks (mainly MSTS and SICK),\nthe complete architecture is validated using a new dataset called NLI19-SP that\nis publicly released with COVID-19 related hoaxes and tweets from Spanish\nsocial media. Our results show state-of-the-art performance on the individual\nbenchmarks, as well as producing a useful analysis of the evolution over time\nof 61 different hoaxes.", "published": "2021-10-27 15:44:54", "link": "http://arxiv.org/abs/2110.14532v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoNLI: A Natural Language Inference Dataset for Indonesian", "abstract": "We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We\nadapt the data collection protocol for MNLI and collect nearly 18K sentence\npairs annotated by crowd workers and experts. The expert-annotated data is used\nexclusively as a test set. It is designed to provide a challenging test-bed for\nIndonesian NLI by explicitly incorporating various linguistic phenomena such as\nnumerical reasoning, structural changes, idioms, or temporal and spatial\nreasoning. Experiment results show that XLM-R outperforms other pre-trained\nmodels in our data. The best performance on the expert-annotated data is still\nfar below human performance (13.4% accuracy gap), suggesting that this test set\nis especially challenging. Furthermore, our analysis shows that our\nexpert-annotated data is more diverse and contains fewer annotation artifacts\nthan the crowd-annotated data. We hope this dataset can help accelerate\nprogress in Indonesian NLP research.", "published": "2021-10-27 16:37:13", "link": "http://arxiv.org/abs/2110.14566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pay attention to emoji: Feature Fusion Network with EmoGraph2vec Model\n  for Sentiment Analysis", "abstract": "With the explosive growth of social media, opinionated postings with emojis\nhave increased explosively. Many emojis are used to express emotions,\nattitudes, and opinions. Emoji representation learning can be helpful to\nimprove the performance of emoji-related natural language processing tasks,\nespecially in text sentiment analysis. However, most studies have only utilized\nthe fixed descriptions provided by the Unicode Consortium without consideration\nof actual usage scenarios. As for the sentiment analysis task, many researchers\nignore the emotional impact of the interaction between text and emojis. It\nresults that the emotional semantics of emojis cannot be fully explored. In\nthis work, we propose a method called EmoGraph2vec to learn emoji\nrepresentations by constructing a co-occurrence graph network from social data\nand enriching the semantic information based on an external knowledge base\nEmojiNet to embed emoji nodes. Based on EmoGraph2vec model, we design a novel\nneural network to incorporate text and emoji information into sentiment\nanalysis, which uses a hybrid-attention module combined with TextCNN-based\nclassifier to improve performance. Experimental results show that the proposed\nmodel can outperform several baselines for sentiment analysis on benchmark\ndatasets. Additionally, we conduct a series of ablation and comparison\nexperiments to investigate the effectiveness and interpretability of our model.", "published": "2021-10-27 08:01:10", "link": "http://arxiv.org/abs/2110.14636v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connect-the-Dots: Bridging Semantics between Words and Definitions via\n  Aligning Word Sense Inventories", "abstract": "Word Sense Disambiguation (WSD) aims to automatically identify the exact\nmeaning of one word according to its context. Existing supervised models\nstruggle to make correct predictions on rare word senses due to limited\ntraining data and can only select the best definition sentence from one\npredefined word sense inventory (e.g., WordNet). To address the data sparsity\nproblem and generalize the model to be independent of one predefined inventory,\nwe propose a gloss alignment algorithm that can align definition sentences\n(glosses) with the same meaning from different sense inventories to collect\nrich lexical knowledge. We then train a model to identify semantic equivalence\nbetween a target word in context and one of its glosses using these aligned\ninventories, which exhibits strong transfer capability to many WSD tasks.\nExperiments on benchmark datasets show that the proposed method improves\npredictions on both frequent and rare word senses, outperforming prior work by\n1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on\nWiC Task also indicates that our method can better capture word meanings in\ncontext.", "published": "2021-10-27 00:04:33", "link": "http://arxiv.org/abs/2110.14091v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Verifiers to Solve Math Word Problems", "abstract": "State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.", "published": "2021-10-27 04:49:45", "link": "http://arxiv.org/abs/2110.14168v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Syllabic Quantity Patterns as Rhythmic Features for Latin Authorship\n  Attribution", "abstract": "It is well known that, within the Latin production of written text, peculiar\nmetric schemes were followed not only in poetic compositions, but also in many\nprose works. Such metric patterns were based on so-called syllabic quantity,\ni.e., on the length of the involved syllables, and there is substantial\nevidence suggesting that certain authors had a preference for certain metric\npatterns over others. In this research we investigate the possibility to employ\nsyllabic quantity as a base for deriving rhythmic features for the task of\ncomputational authorship attribution of Latin prose texts. We test the impact\nof these features on the authorship attribution task when combined with other\ntopic-agnostic features. Our experiments, carried out on three different\ndatasets, using two different machine learning methods, show that rhythmic\nfeatures based on syllabic quantity are beneficial in discriminating among\nLatin prose authors.", "published": "2021-10-27 06:25:31", "link": "http://arxiv.org/abs/2110.14203v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New\n  Reasoning Challenge for AI", "abstract": "Many real-world problems require the combined application of multiple\nreasoning abilities employing suitable abstractions, commonsense knowledge, and\ncreative synthesis of problem-solving strategies. To help advance AI systems\ntowards such capabilities, we propose a new reasoning challenge, namely Fermi\nProblems (FPs), which are questions whose answers can only be approximately\nestimated because their precise computation is either impractical or\nimpossible. For example, \"How much would the sea level rise if all ice in the\nworld melted?\" FPs are commonly used in quizzes and interviews to bring out and\nevaluate the creative reasoning abilities of humans. To do the same for AI\nsystems, we present two datasets: 1) A collection of 1k real-world FPs sourced\nfrom quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate\ncomplexity to serve as a sandbox for the harder real-world challenge. In\naddition to question answer pairs, the datasets contain detailed solutions in\nthe form of an executable program and supporting facts, helping in supervision\nand evaluation of intermediate steps. We demonstrate that even extensively\nfine-tuned large scale language models perform poorly on these datasets, on\naverage making estimates that are off by two orders of magnitude. Our\ncontribution is thus the crystallization of several unsolved AI problems into a\nsingle, new challenge that we hope will spur further advances in building\nsystems that can reason.", "published": "2021-10-27 06:39:33", "link": "http://arxiv.org/abs/2110.14207v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emoji-based Co-attention Network for Microblog Sentiment Analysis", "abstract": "Emojis are widely used in online social networks to express emotions,\nattitudes, and opinions. As emotional-oriented characters, emojis can be\nmodeled as important features of emotions towards the recipient or subject for\nsentiment analysis. However, existing methods mainly take emojis as heuristic\ninformation that fails to resolve the problem of ambiguity noise. Recent\nresearches have utilized emojis as an independent input to classify text\nsentiment but they ignore the emotional impact of the interaction between text\nand emojis. It results that the emotional semantics of emojis cannot be fully\nexplored. In this paper, we propose an emoji-based co-attention network that\nlearns the mutual emotional semantics between text and emojis on microblogs.\nOur model adopts the co-attention mechanism based on bidirectional long\nshort-term memory incorporating the text and emojis, and integrates a\nsqueeze-and-excitation block in a convolutional neural network classifier to\nincrease its sensitivity to emotional semantic features. Experimental results\nshow that the proposed method can significantly outperform several baselines\nfor sentiment analysis on short texts of social media.", "published": "2021-10-27 07:23:18", "link": "http://arxiv.org/abs/2110.14227v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anomaly-Injected Deep Support Vector Data Description for Text Outlier\n  Detection", "abstract": "Anomaly detection or outlier detection is a common task in various domains,\nwhich has attracted significant research efforts in recent years. Existing\nworks mainly focus on structured data such as numerical or categorical data;\nhowever, anomaly detection on unstructured textual data is less attended. In\nthis work, we target the textual anomaly detection problem and propose a deep\nanomaly-injected support vector data description (AI-SVDD) framework. AI-SVDD\nnot only learns a more compact representation of the data hypersphere but also\nadopts a small number of known anomalies to increase the discriminative power.\nTo tackle text input, we employ a multilayer perceptron (MLP) network in\nconjunction with BERT to obtain enriched text representations. We conduct\nexperiments on three text anomaly detection applications with multiple\ndatasets. Experimental results show that the proposed AI-SVDD is promising and\noutperforms existing works.", "published": "2021-10-27 19:29:19", "link": "http://arxiv.org/abs/2110.14729v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Dementia from Speech and Transcripts using Transformers", "abstract": "Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Alzheimer's is the most common cause of dementia,\nwhich constitutes a general term for loss of memory. Due to the fact that\ndementia affects speech, existing research initiatives focus on detecting\ndementia from spontaneous speech. However, little work has been done regarding\nthe conversion of speech data to Log-Mel spectrograms and Mel-frequency\ncepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,\nlittle work has been done in terms of both the usage of transformer networks\nand the way the two modalities, i.e., speech and transcripts, are combined in a\nsingle neural network. To address these limitations, first we represent speech\nsignal as an image and employ several pretrained models, with Vision\nTransformer (ViT) achieving the highest evaluation results. Secondly, we\npropose multimodal models. More specifically, our introduced models include\nGated Multimodal Unit in order to control the influence of each modality\ntowards the final classification and crossmodal attention so as to capture in\nan effective way the relationships between the two modalities. Extensive\nexperiments conducted on the ADReSS Challenge dataset demonstrate the\neffectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.", "published": "2021-10-27 21:00:01", "link": "http://arxiv.org/abs/2110.14769v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Combining Vagueness Detection with Deep Learning to Identify Fake News", "abstract": "In this paper, we combine two independent detection methods for identifying\nfake news: the algorithm VAGO uses semantic rules combined with NLP techniques\nto measure vagueness and subjectivity in texts, while the classifier FAKE-CLF\nrelies on Convolutional Neural Network classification and supervised deep\nlearning to classify texts as biased or legitimate. We compare the results of\nthe two methods on four corpora. We find a positive correlation between the\nvagueness and subjectivity measures obtained by VAGO, and the classification of\ntext as biased by FAKE-CLF. The comparison yields mutual benefits: VAGO helps\nexplain the results of FAKE-CLF. Conversely FAKE-CLF helps us corroborate and\nexpand VAGO's database. The use of two complementary techniques (rule-based vs\ndata-driven) proves a fruitful approach for the challenging problem of\nidentifying fake news.", "published": "2021-10-27 21:25:10", "link": "http://arxiv.org/abs/2110.14780v2", "categories": ["cs.CL", "cs.LG", "68T07, 68T50"], "primary_category": "cs.CL"}
{"title": "When is BERT Multilingual? Isolating Crucial Ingredients for\n  Cross-lingual Transfer", "abstract": "While recent work on multilingual language models has demonstrated their\ncapacity for cross-lingual zero-shot transfer on downstream tasks, there is a\nlack of consensus in the community as to what shared properties between\nlanguages enable such transfer. Analyses involving pairs of natural languages\nare often inconclusive and contradictory since languages simultaneously differ\nin many linguistic aspects. In this paper, we perform a large-scale empirical\nstudy to isolate the effects of various linguistic properties by measuring\nzero-shot transfer between four diverse natural languages and their\ncounterparts constructed by modifying aspects such as the script, word order,\nand syntax. Among other things, our experiments show that the absence of\nsub-word overlap significantly affects zero-shot transfer when languages differ\nin their word order, and there is a strong correlation between transfer\nperformance and word embedding alignment between languages (e.g., R=0.94 on the\ntask of NLI). Our results call for focus in multilingual models on explicitly\nimproving word embedding alignment between languages rather than relying on its\nimplicit emergence.", "published": "2021-10-27 21:25:39", "link": "http://arxiv.org/abs/2110.14782v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding", "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.", "published": "2021-10-27 04:57:16", "link": "http://arxiv.org/abs/2110.14170v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Diversity Enhanced Active Learning with Strictly Proper Scoring Rules", "abstract": "We study acquisition functions for active learning (AL) for text\nclassification. The Expected Loss Reduction (ELR) method focuses on a Bayesian\nestimate of the reduction in classification error, recently updated with Mean\nObjective Cost of Uncertainty (MOCU). We convert the ELR framework to estimate\nthe increase in (strictly proper) scores like log probability or negative mean\nsquare error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS). We\nalso prove convergence results borrowing techniques used with MOCU. In order to\nallow better experimentation with the new acquisition functions, we develop a\ncomplementary batch AL algorithm, which encourages diversity in the vector of\nexpected changes in scores for unlabelled data. To allow high performance text\nclassifiers, we combine ensembling and dynamic validation set construction on\npretrained language models. Extensive experimental evaluation then explores how\nthese different acquisition functions perform. The results show that the use of\nmean square error and log probability with BEMPS yields robust acquisition\nfunctions, which consistently outperform the others tested.", "published": "2021-10-27 05:02:11", "link": "http://arxiv.org/abs/2110.14171v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical\n  Reasoning", "abstract": "State-of-the-art approaches to reasoning and question answering over\nknowledge graphs (KGs) usually scale with the number of edges and can only be\napplied effectively on small instance-dependent subgraphs. In this paper, we\naddress this issue by showing that multi-hop and more complex logical reasoning\ncan be accomplished separately without losing expressive power. Motivated by\nthis insight, we propose an approach to multi-hop reasoning that scales\nlinearly with the number of relation types in the graph, which is usually\nsignificantly smaller than the number of edges or nodes. This produces a set of\ncandidate solutions that can be provably refined to recover the solution to the\noriginal problem. Our experiments on knowledge-based question answering show\nthat our approach solves the multi-hop MetaQA dataset, achieves a new\nstate-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude\nmore scalable than competitive approaches, and can achieve compositional\ngeneralization out of the training distribution.", "published": "2021-10-27 08:40:16", "link": "http://arxiv.org/abs/2110.14266v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Deep Learning For Prominence Detection In Children's Read Speech", "abstract": "The detection of perceived prominence in speech has attracted approaches\nranging from the design of linguistic knowledge-based acoustic features to the\nautomatic feature learning from suprasegmental attributes such as pitch and\nintensity contours. We present here, in contrast, a system that operates\ndirectly on segmented speech waveforms to learn features relevant to prominent\nword detection for children's oral fluency assessment. The chosen CRNN\n(convolutional recurrent neural network) framework, incorporating both\nword-level features and sequence information, is found to benefit from the\nperceptually motivated SincNet filters as the first convolutional layer. We\nfurther explore the benefits of the linguistic association between the prosodic\nevents of phrase boundary and prominence with different multi-task\narchitectures. Matching the previously reported performance on the same dataset\nof a random forest ensemble predictor trained on carefully chosen hand-crafted\nacoustic features, we evaluate further the possibly complementary information\nfrom hand-crafted acoustic and pre-trained lexical features.", "published": "2021-10-27 08:51:42", "link": "http://arxiv.org/abs/2110.14273v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Realistic Single-Task Continuous Learning Research for NER", "abstract": "There is an increasing interest in continuous learning (CL), as data privacy\nis becoming a priority for real-world machine learning applications. Meanwhile,\nthere is still a lack of academic NLP benchmarks that are applicable for\nrealistic CL settings, which is a major challenge for the advancement of the\nfield. In this paper we discuss some of the unrealistic data characteristics of\npublic datasets, study the challenges of realistic single-task continuous\nlearning as well as the effectiveness of data rehearsal as a way to mitigate\naccuracy loss. We construct a CL NER dataset from an existing publicly\navailable dataset and release it along with the code to the research community.", "published": "2021-10-27 18:23:31", "link": "http://arxiv.org/abs/2110.14694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Review-based Recommenders", "abstract": "Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].", "published": "2021-10-27 20:17:47", "link": "http://arxiv.org/abs/2110.14747v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Discovering Non-monotonic Autoregressive Orderings with Variational\n  Inference", "abstract": "The predominant approach for language modeling is to process sequences from\nleft to right, but this eliminates a source of information: the order by which\nthe sequence was generated. One strategy to recover this information is to\ndecode both the content and ordering of tokens. Existing approaches supervise\ncontent and ordering by designing problem-specific loss functions and\npre-training with an ordering pre-selected. Other recent works use iterative\nsearch to discover problem-specific orderings for training, but suffer from\nhigh time complexity and cannot be efficiently parallelized. We address these\nlimitations with an unsupervised parallelizable learner that discovers\nhigh-quality generation orders purely from training data -- no domain knowledge\nrequired. The learner contains an encoder network and decoder language model\nthat perform variational inference with autoregressive orders (represented as\npermutation matrices) as latent variables. The corresponding ELBO is not\ndifferentiable, so we develop a practical algorithm for end-to-end optimization\nusing policy gradients. We implement the encoder as a Transformer with\nnon-causal attention that outputs permutations in one forward pass.\nPermutations then serve as target generation orders for training an\ninsertion-based Transformer language model. Empirical results in language\nmodeling tasks demonstrate that our method is context-aware and discovers\norderings that are competitive with or even better than fixed orders.", "published": "2021-10-27 16:08:09", "link": "http://arxiv.org/abs/2110.15797v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evidential Softmax for Sparse Multimodal Distributions in Deep\n  Generative Models", "abstract": "Many applications of generative models rely on the marginalization of their\nhigh-dimensional output probability distributions. Normalization functions that\nyield sparse probability distributions can make exact marginalization more\ncomputationally tractable. However, sparse normalization functions usually\nrequire alternative loss functions for training since the log-likelihood is\nundefined for sparse probability distributions. Furthermore, many sparse\nnormalization functions often collapse the multimodality of distributions. In\nthis work, we present $\\textit{ev-softmax}$, a sparse normalization function\nthat preserves the multimodality of probability distributions. We derive its\nproperties, including its gradient in closed-form, and introduce a continuous\nfamily of approximations to $\\textit{ev-softmax}$ that have full support and\ncan be trained with probabilistic loss functions such as negative\nlog-likelihood and Kullback-Leibler divergence. We evaluate our method on a\nvariety of generative models, including variational autoencoders and\nauto-regressive architectures. Our method outperforms existing dense and sparse\nnormalization techniques in distributional accuracy. We demonstrate that\n$\\textit{ev-softmax}$ successfully reduces the dimensionality of probability\ndistributions while maintaining multimodality.", "published": "2021-10-27 05:32:25", "link": "http://arxiv.org/abs/2110.14182v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dynamic population-based meta-learning for multi-agent communication\n  with natural language", "abstract": "In this work, our goal is to train agents that can coordinate with seen,\nunseen as well as human partners in a multi-agent communication environment\ninvolving natural language. Previous work using a single set of agents has\nshown great progress in generalizing to known partners, however it struggles\nwhen coordinating with unfamiliar agents. To mitigate that, recent work\nexplored the use of population-based approaches, where multiple agents interact\nwith each other with the goal of learning more generic protocols. These\nmethods, while able to result in good coordination between unseen partners,\nstill only achieve so in cases of simple languages, thus failing to adapt to\nhuman partners using natural language. We attribute this to the use of static\npopulations and instead propose a dynamic population-based meta-learning\napproach that builds such a population in an iterative manner. We perform a\nholistic evaluation of our method on two different referential games, and show\nthat our agents outperform all prior work when communicating with seen partners\nand humans. Furthermore, we analyze the natural language generation skills of\nour agents, where we find that our agents also outperform strong baselines.\nFinally, we test the robustness of our agents when communicating with\nout-of-population agents and carefully test the importance of each component of\nour method through ablation studies.", "published": "2021-10-27 07:50:02", "link": "http://arxiv.org/abs/2110.14241v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Closing the Gap Between Time-Domain Multi-Channel Speech Enhancement on\n  Real and Simulation Conditions", "abstract": "The deep learning based time-domain models, e.g. Conv-TasNet, have shown\ngreat potential in both single-channel and multi-channel speech enhancement.\nHowever, many experiments on the time-domain speech enhancement model are done\nin simulated conditions, and it is not well studied whether the good\nperformance can generalize to real-world scenarios. In this paper, we aim to\nprovide an insightful investigation of applying multi-channel Conv-TasNet based\nspeech enhancement to both simulation and real data. Our preliminary\nexperiments show a large performance gap between the two conditions in terms of\nthe ASR performance. Several approaches are applied to close this gap,\nincluding the integration of multi-channel Conv-TasNet into the beamforming\nmodel with various strategies, and the joint training of speech enhancement and\nspeech recognition models. Our experiments on the CHiME-4 corpus show that our\nproposed approaches can greatly reduce the speech recognition performance\ndiscrepancy between simulation and real data, while preserving the strong\nspeech enhancement capability in the frontend.", "published": "2021-10-27 03:01:44", "link": "http://arxiv.org/abs/2110.14139v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Separating Long-Form Speech with Group-Wise Permutation Invariant\n  Training", "abstract": "Multi-talker conversational speech processing has drawn many interests for\nvarious applications such as meeting transcription. Speech separation is often\nrequired to handle overlapped speech that is commonly observed in conversation.\nAlthough the original utterancelevel permutation invariant training-based\ncontinuous speech separation approach has proven to be effective in various\nconditions, it lacks the ability to leverage the long-span relationship of\nutterances and is computationally inefficient due to the highly overlapped\nsliding windows. To overcome these drawbacks, we propose a novel training\nscheme named Group-PIT, which allows direct training of the speech separation\nmodels on the long-form speech with a low computational cost for label\nassignment. Two different speech separation approaches with Group-PIT are\nexplored, including direct long-span speech separation and short-span speech\nseparation with long-span tracking. The experiments on the simulated\nmeeting-style data demonstrate the effectiveness of our proposed approaches,\nespecially in dealing with a very long speech input.", "published": "2021-10-27 03:07:11", "link": "http://arxiv.org/abs/2110.14142v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Temporal Knowledge Distillation for On-device Audio Classification", "abstract": "Improving the performance of on-device audio classification models remains a\nchallenge given the computational limits of the mobile environment. Many\nstudies leverage knowledge distillation to boost predictive performance by\ntransferring the knowledge from large models to on-device models. However, most\nlack a mechanism to distill the essence of the temporal information, which is\ncrucial to audio classification tasks, or similar architecture is often\nrequired. In this paper, we propose a new knowledge distillation method\ndesigned to incorporate the temporal knowledge embedded in attention weights of\nlarge transformer-based models into on-device models. Our distillation method\nis applicable to various types of architectures, including the\nnon-attention-based architectures such as CNNs or RNNs, while retaining the\noriginal network architecture during inference. Through extensive experiments\non both an audio event detection dataset and a noisy keyword spotting dataset,\nwe show that our proposed method improves the predictive performance across\ndiverse on-device architectures.", "published": "2021-10-27 02:29:54", "link": "http://arxiv.org/abs/2110.14131v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-shot Voice Conversion via Self-supervised Prosody Representation\n  Learning", "abstract": "Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an\nattractive research topic as it enables a range of applications like voice\ncustomizing, animation production, and others. Recent work in this area made\nprogress with disentanglement methods that separate utterance content and\nspeaker characteristics from speech audio recordings. However, many of these\nmethods are subject to the leakage of prosody (e.g., pitch, volume), causing\nthe speaker voice in the synthesized speech to be different from the desired\ntarget speakers. To prevent this issue, we propose a novel self-supervised\napproach that effectively learns disentangled pitch and volume representations\nthat can represent the prosody styles of different speakers. We then use the\nlearned prosodic representations as conditional information to train and\nenhance our VC model for zero-shot conversion. In our experiments, we show that\nour prosody representations are disentangled and rich in prosody information.\nMoreover, we demonstrate that the addition of our prosody representations\nimproves our VC performance and surpasses state-of-the-art zero-shot VC\nperformances.", "published": "2021-10-27 13:26:52", "link": "http://arxiv.org/abs/2110.14422v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalizing AUC Optimization to Multiclass Classification for Audio\n  Segmentation With Limited Training Data", "abstract": "Area under the ROC curve (AUC) optimisation techniques developed for neural\nnetworks have recently demonstrated their capabilities in different audio and\nspeech related tasks. However, due to its intrinsic nature, AUC optimisation\nhas focused only on binary tasks so far. In this paper, we introduce an\nextension to the AUC optimisation framework so that it can be easily applied to\nan arbitrary number of classes, aiming to overcome the issues derived from\ntraining data limitations in deep learning solutions. Building upon the\nmulticlass definitions of the AUC metric found in the literature, we define two\nnew training objectives using a one-versus-one and a one-versus-rest approach.\nIn order to demonstrate its potential, we apply them in an audio segmentation\ntask with limited training data that aims to differentiate 3 classes:\nforeground music, background music and no music. Experimental results show that\nour proposal can improve the performance of audio segmentation systems\nsignificantly compared to traditional training criteria such as cross entropy.", "published": "2021-10-27 13:36:04", "link": "http://arxiv.org/abs/2110.14425v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring single-song autoencoding schemes for audio-based music\n  structure analysis", "abstract": "The ability of deep neural networks to learn complex data relations and\nrepresentations is established nowadays, but it generally relies on large sets\nof training data. This work explores a \"piece-specific\" autoencoding scheme, in\nwhich a low-dimensional autoencoder is trained to learn a latent/compressed\nrepresentation specific to a given song, which can then be used to infer the\nsong structure. Such a model does not rely on supervision nor annotations,\nwhich are well-known to be tedious to collect and often ambiguous in Music\nStructure Analysis. We report that the proposed unsupervised auto-encoding\nscheme achieves the level of performance of supervised state-of-the-art methods\nwith 3 seconds tolerance when using a Log Mel spectrogram representation on the\nRWC-Pop dataset.", "published": "2021-10-27 13:48:25", "link": "http://arxiv.org/abs/2110.14437v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Neural Analysis and Synthesis: Reconstructing Speech from\n  Self-Supervised Representations", "abstract": "We present a neural analysis and synthesis (NANSY) framework that can\nmanipulate voice, pitch, and speed of an arbitrary speech signal. Most of the\nprevious works have focused on using information bottleneck to disentangle\nanalysis features for controllable synthesis, which usually results in poor\nreconstruction quality. We address this issue by proposing a novel training\nstrategy based on information perturbation. The idea is to perturb information\nin the original input signal (e.g., formant, pitch, and frequency response),\nthereby letting synthesis networks selectively take essential attributes to\nreconstruct the input signal. Because NANSY does not need any bottleneck\nstructures, it enjoys both high reconstruction quality and controllability.\nFurthermore, NANSY does not require any labels associated with speech data such\nas text and speaker information, but rather uses a new set of analysis\nfeatures, i.e., wav2vec feature and newly proposed pitch feature, Yingram,\nwhich allows for fully self-supervised training. Taking advantage of fully\nself-supervised training, NANSY can be easily extended to a multilingual\nsetting by simply training it with a multilingual dataset. The experiments show\nthat NANSY can achieve significant improvement in performance in several\napplications such as zero-shot voice conversion, pitch shift, and time-scale\nmodification.", "published": "2021-10-27 15:25:26", "link": "http://arxiv.org/abs/2110.14513v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonnegative Tucker Decomposition with Beta-divergence for Music\n  Structure Analysis of Audio Signals", "abstract": "Nonnegative Tucker decomposition (NTD), a tensor decomposition model, has\nreceived increased interest in the recent years because of its ability to\nblindly extract meaningful patterns, in particular in Music Information\nRetrieval. Nevertheless, existing algorithms to compute NTD are mostly designed\nfor the Euclidean loss. This work proposes a multiplicative updates algorithm\nto compute NTD with the beta-divergence loss, often considered a better loss\nfor audio processing. We notably show how to implement efficiently the\nmultiplicative rules using tensor algebra. Finally, we show on a music\nstructure analysis task that unsupervised NTD fitted with beta-divergence loss\noutperforms earlier results obtained with the Euclidean loss.", "published": "2021-10-27 13:46:51", "link": "http://arxiv.org/abs/2110.14434v4", "categories": ["cs.SD", "cs.LG", "cs.NA", "eess.AS", "math.NA", "15-04", "G.1.6; H.5.5"], "primary_category": "cs.SD"}
{"title": "LSTM-RPA: A Simple but Effective Long Sequence Prediction Algorithm for\n  Music Popularity Prediction", "abstract": "The big data about music history contains information about time and users'\nbehavior. Researchers could predict the trend of popular songs accurately by\nanalyzing this data. The traditional trend prediction models can better predict\nthe short trend than the long trend. In this paper, we proposed the improved\nLSTM Rolling Prediction Algorithm (LSTM-RPA), which combines LSTM historical\ninput with current prediction results as model input for next time prediction.\nMeanwhile, this algorithm converts the long trend prediction task into multiple\nshort trend prediction tasks. The evaluation results show that the LSTM-RPA\nmodel increased F score by 13.03%, 16.74%, 11.91%, 18.52%, compared with LSTM,\nBiLSTM, GRU and RNN. And our method outperforms tradi-tional sequence models,\nwhich are ARIMA and SMA, by 10.67% and 3.43% improvement in F score.Code:\nhttps://github.com/maliaosaide/lstm-rpa", "published": "2021-10-27 08:59:09", "link": "http://arxiv.org/abs/2110.15790v1", "categories": ["cs.IR", "cs.AI", "cs.MM", "cs.SD", "cs.SI", "eess.AS"], "primary_category": "cs.IR"}
