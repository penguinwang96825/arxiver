{"title": "MADARi: A Web Interface for Joint Arabic Morphological Annotation and\n  Spelling Correction", "abstract": "In this paper, we introduce MADARi, a joint morphological annotation and\nspelling correction system for texts in Standard and Dialectal Arabic. The\nMADARi framework provides intuitive interfaces for annotating text and managing\nthe annotation process of a large number of sizable documents. Morphological\nannotation includes indicating, for a word, in context, its baseword, clitics,\npart-of-speech, lemma, gloss, and dialect identification. MADARi has a suite of\nutilities to help with annotator productivity. For example, annotators are\nprovided with pre-computed analyses to assist them in their task and reduce the\namount of work needed to complete it. MADARi also allows annotators to query a\nmorphological analyzer for a list of possible analyses in multiple dialects or\nlook up previously submitted analyses. The MADARi management interface enables\na lead annotator to easily manage and organize the whole annotation process\nremotely and concurrently. We describe the motivation, design and\nimplementation of this interface; and we present details from a user study\nworking with this system.", "published": "2018-08-25 09:32:30", "link": "http://arxiv.org/abs/1808.08392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the results of string kernels in sentiment analysis and Arabic\n  dialect identification by adapting them to your test set", "abstract": "Recently, string kernels have obtained state-of-the-art results in various\ntext classification tasks such as Arabic dialect identification or native\nlanguage identification. In this paper, we apply two simple yet effective\ntransductive learning approaches to further improve the results of string\nkernels. The first approach is based on interpreting the pairwise string kernel\nsimilarities between samples in the training set and samples in the test set as\nfeatures. Our second approach is a simple self-training method based on two\nlearning iterations. In the first iteration, a classifier is trained on the\ntraining set and tested on the test set, as usual. In the second iteration, a\nnumber of test samples (to which the classifier associated higher confidence\nscores) are added to the training set for another round of training. However,\nthe ground-truth labels of the added test samples are not necessary. Instead,\nwe use the labels predicted by the classifier in the first training iteration.\nBy adapting string kernels to the test set, we report significantly better\naccuracy rates in English polarity classification and Arabic dialect\nidentification.", "published": "2018-08-25 11:08:28", "link": "http://arxiv.org/abs/1808.08409v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Churn Intent Detection in Multilingual Chatbot Conversations and Social\n  Media", "abstract": "We propose a new method to detect when users express the intent to leave a\nservice, also known as churn. While previous work focuses solely on social\nmedia, we show that this intent can be detected in chatbot conversations. As\ncompanies increasingly rely on chatbots they need an overview of potentially\nchurny users. To this end, we crowdsource and publish a dataset of churn intent\nexpressions in chatbot interactions in German and English. We show that\nclassifiers trained on social media data can detect the same intent in the\ncontext of chatbots.\n  We introduce a classification architecture that outperforms existing work on\nchurn intent detection in social media. Moreover, we show that, using bilingual\nword embeddings, a system trained on combined English and German data\noutperforms monolingual approaches. As the only existing dataset is in English,\nwe crowdsource and publish a novel dataset of German tweets. We thus underline\nthe universal aspect of the problem, as examples of churn intent in English\nhelp us identify churn in German tweets and chatbot conversations.", "published": "2018-08-25 14:24:23", "link": "http://arxiv.org/abs/1808.08432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrases as Foreign Languages in Multilingual Neural Machine\n  Translation", "abstract": "Paraphrases, the rewordings of the same semantic meaning, are useful for\nimproving generalization and translation. However, prior works only explore\nparaphrases at the word or phrase level, not at the sentence or corpus level.\nUnlike previous works that only explore paraphrases at the word or phrase\nlevel, we use different translations of the whole training data that are\nconsistent in structure as paraphrases at the corpus level. We train on\nparallel paraphrases in multiple languages from various sources. We treat\nparaphrases as foreign languages, tag source sentences with paraphrase labels,\nand train on parallel paraphrases in the style of multilingual Neural Machine\nTranslation (NMT). Our multi-paraphrase NMT that trains only on two languages\noutperforms the multilingual baselines. Adding paraphrases improves the rare\nword translation and increases entropy and diversity in lexical choice. Adding\nthe source paraphrases boosts performance better than adding the target ones.\nCombining both the source and the target paraphrases lifts performance further;\ncombining paraphrases with multilingual data helps but has mixed performance.\nWe achieve a BLEU score of 57.2 for French-to-English translation using 24\ncorpus-level paraphrases of the Bible, which outperforms the multilingual\nbaselines and is +34.7 above the single-source single-target NMT baseline.", "published": "2018-08-25 15:20:30", "link": "http://arxiv.org/abs/1808.08438v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models\n  for chemical and disease named entity recognition", "abstract": "We compare the use of LSTM-based and CNN-based character-level word\nembeddings in BiLSTM-CRF models to approach chemical and disease named entity\nrecognition (NER) tasks. Empirical results over the BioCreative V CDR corpus\nshow that the use of either type of character-level word embeddings in\nconjunction with the BiLSTM-CRF models leads to comparable state-of-the-art\nperformance. However, the models using CNN-based character-level word\nembeddings have a computational performance advantage, increasing training time\nover word-based models by 25% while the LSTM-based character-level word\nembeddings more than double the required training time.", "published": "2018-08-25 17:02:29", "link": "http://arxiv.org/abs/1808.08450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Recombination for Efficient Decoding of Neural Machine\n  Translation", "abstract": "In Neural Machine Translation (NMT), the decoder can capture the features of\nthe entire prediction history with neural connections and representations. This\nmeans that partial hypotheses with different prefixes will be regarded\ndifferently no matter how similar they are. However, this might be inefficient\nsince some partial hypotheses can contain only local differences that will not\ninfluence future predictions. In this work, we introduce recombination in NMT\ndecoding based on the concept of the \"equivalence\" of partial hypotheses.\nHeuristically, we use a simple $n$-gram suffix based equivalence function and\nadapt it into beam search decoding. Through experiments on large-scale\nChinese-to-English and English-to-Germen translation tasks, we show that the\nproposed method can obtain similar translation quality with a smaller beam\nsize, making NMT decoding more efficient.", "published": "2018-08-25 23:26:10", "link": "http://arxiv.org/abs/1808.08482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dr. Tux: A Question Answering System for Ubuntu users", "abstract": "Various forums and question answering (Q&A) sites are available online that\nallow Ubuntu users to find results similar to their queries. However, searching\nfor a result is often time consuming as it requires the user to find a specific\nproblem instance relevant to his/her query from a large set of questions. In\nthis paper, we present an automated question answering system for Ubuntu users\ncalled Dr. Tux that is designed to answer user's queries by selecting the most\nsimilar question from an online database. The prototype was implemented in\nPython and uses NLTK and CoreNLP tools for Natural Language Processing. The\ndata for the prototype was taken from the AskUbuntu website which contains\nabout 150k questions. The results obtained from the manual evaluation of the\nprototype were promising while also presenting some interesting opportunities\nfor improvement.", "published": "2018-08-25 05:17:43", "link": "http://arxiv.org/abs/1808.08357v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Meta-Learning for Low-Resource Neural Machine Translation", "abstract": "In this paper, we propose to extend the recently introduced model-agnostic\nmeta-learning algorithm (MAML) for low-resource neural machine translation\n(NMT). We frame low-resource translation as a meta-learning problem, and we\nlearn to adapt to low-resource languages based on multilingual high-resource\nlanguage tasks. We use the universal lexical\nrepresentation~\\citep{gu2018universal} to overcome the input-output mismatch\nacross different languages. We evaluate the proposed meta-learning strategy\nusing eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt,\nNl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,\nLv, Fi, Tr and Ko) as target tasks. We show that the proposed approach\nsignificantly outperforms the multilingual, transfer learning based\napproach~\\citep{zoph2016transfer} and enables us to train a competitive NMT\nsystem with only a fraction of training examples. For instance, the proposed\napproach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing\nonly 16,000 translated words (~600 parallel sentences).", "published": "2018-08-25 15:10:59", "link": "http://arxiv.org/abs/1808.08437v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representing Social Media Users for Sarcasm Detection", "abstract": "We explore two methods for representing authors in the context of textual\nsarcasm detection: a Bayesian approach that directly represents authors'\npropensities to be sarcastic, and a dense embedding approach that can learn\ninteractions between the author and the text. Using the SARC dataset of Reddit\ncomments, we show that augmenting a bidirectional RNN with these\nrepresentations improves performance; the Bayesian approach suffices in\nhomogeneous contexts, whereas the added power of the dense embeddings proves\nvaluable in more diverse ones.", "published": "2018-08-25 21:04:53", "link": "http://arxiv.org/abs/1808.08470v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Convolutional Neural Network with Mixup for Environmental Sound\n  Classification", "abstract": "Environmental sound classification (ESC) is an important and challenging\nproblem. In contrast to speech, sound events have noise-like nature and may be\nproduced by a wide variety of sources. In this paper, we propose to use a novel\ndeep convolutional neural network for ESC tasks. Our network architecture uses\nstacked convolutional and pooling layers to extract high-level feature\nrepresentations from spectrogram-like features. Furthermore, we apply mixup to\nESC tasks and explore its impacts on classification performance and feature\ndistribution. Experiments were conducted on UrbanSound8K, ESC-50 and ESC-10\ndatasets. Our experimental results demonstrated that our ESC system has\nachieved the state-of-the-art performance (83.7%) on UrbanSound8K and\ncompetitive performance on ESC-50 and ESC-10.", "published": "2018-08-25 10:55:04", "link": "http://arxiv.org/abs/1808.08405v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multiobjective Optimization Training of PLDA for Speaker Verification", "abstract": "Most current state-of-the-art text-independent speaker verification systems\ntake probabilistic linear discriminant analysis (PLDA) as their backend\nclassifiers. The parameters of PLDA are often estimated by maximizing the\nobjective function, which focuses on increasing the value of log-likelihood\nfunction, but ignoring the distinction between speakers. In order to better\ndistinguish speakers, we propose a multi-objective optimization training for\nPLDA. Experiment results show that the proposed method has more than 10%\nrelative performance improvement in both EER and MinDCF on the NIST SRE14\ni-vector challenge dataset, and about 20% relative performance improvement in\nEER on the MCE18 dataset.", "published": "2018-08-25 01:48:05", "link": "http://arxiv.org/abs/1808.08344v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient improvement of frequency-domain Kalman filter", "abstract": "The frequency-domain Kalman filter (FKF) has been utilized in many audio\nsignal processing applications due to its fast convergence speed and\nrobustness. However, the performance of the FKF in under-modeling situations\nhas not been investigated. This paper presents an analysis of the steady-state\nbehavior of the commonly used diagonalized FKF and reveals that it suffers from\na biased solution in under-modeling scenarios. Two efficient improvements of\nthe FKF are proposed, both having the benefits of the guaranteed optimal\nsteady-state behavior at the cost of a very limited increase of the\ncomputational burden. The convergence behavior of the proposed algorithms is\nalso compared analytically. Computer simulations are conducted to validate the\nimproved performance of the proposed methods.", "published": "2018-08-25 15:46:03", "link": "http://arxiv.org/abs/1808.08442v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
