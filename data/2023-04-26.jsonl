{"title": "Exploring the Curious Case of Code Prompts", "abstract": "Recent work has shown that prompting language models with code-like\nrepresentations of natural language leads to performance improvements on\nstructured reasoning tasks. However, such tasks comprise only a small subset of\nall natural language tasks. In our work, we seek to answer whether or not\ncode-prompting is the preferred way of interacting with language models in\ngeneral. We compare code and text prompts across three popular GPT models\n(davinci, code-davinci-002, and text-davinci-002) on a broader selection of\ntasks (e.g., QA, sentiment, summarization) and find that with few exceptions,\ncode prompts do not consistently outperform text prompts. Furthermore, we show\nthat the style of code prompt has a large effect on performance for some but\nnot all tasks and that fine-tuning on text instructions leads to better\nrelative performance of code prompts.", "published": "2023-04-26 02:37:52", "link": "http://arxiv.org/abs/2304.13250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Slot and Intent Detection in Low-Resource Languages", "abstract": "Intent detection and slot filling are critical tasks in spoken and natural\nlanguage understanding for task-oriented dialog systems. In this work we\ndescribe our participation in the slot and intent detection for low-resource\nlanguage varieties (SID4LR; Aepli et al. (2023)). We investigate the slot and\nintent detection (SID) tasks using a wide range of models and settings. Given\nthe recent success of multitask-prompted finetuning of large language models,\nwe also test the generalization capability of the recent encoder-decoder model\nmT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have\nnever intentionally seen. We show that our best model outperforms the baseline\nby a large margin (up to +30 F1 points) in both SID tasks", "published": "2023-04-26 05:10:12", "link": "http://arxiv.org/abs/2304.13292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nominal Topology for Data Languages", "abstract": "We propose a novel topological perspective on data languages recognizable by\norbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite\nnominal topological spaces. Assuming globally bounded support sizes, they\ncoincide with nominal Stone spaces and are shown to be dually equivalent to a\nsubcategory of nominal boolean algebras. Recognizable data languages are\ncharacterized as topologically clopen sets of pro-orbit-finite words. In\naddition, we explore the expressive power of pro-orbit-finite equations by\nestablishing a nominal version of Reiterman's pseudovariety theorem.", "published": "2023-04-26 07:11:44", "link": "http://arxiv.org/abs/2304.13337v2", "categories": ["cs.CL", "F.4.3"], "primary_category": "cs.CL"}
{"title": "SCM: Enhancing Large Language Model with Self-Controlled Memory\n  Framework", "abstract": "Large Language Models (LLMs) are constrained by their inability to process\nlengthy inputs, resulting in the loss of critical historical information. To\naddress this limitation, in this paper, we propose the Self-Controlled Memory\n(SCM) framework to enhance the ability of LLMs to maintain long-term memory and\nrecall relevant information. Our SCM framework comprises three key components:\nan LLM-based agent serving as the backbone of the framework, a memory stream\nstoring agent memories, and a memory controller updating memories and\ndetermining when and how to utilize memories from memory stream. Additionally,\nthe proposed SCM is able to process ultra-long texts without any modification\nor fine-tuning, which can integrate with any instruction following LLMs in a\nplug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the\neffectiveness of SCM for handling lengthy inputs. The annotated dataset covers\nthree tasks: long-term dialogues, book summarization, and meeting\nsummarization. Experimental results demonstrate that our method achieves better\nretrieval recall and generates more informative responses compared to\ncompetitive baselines in long-term dialogues.\n(https://github.com/wbbeyourself/SCM4LLMs)", "published": "2023-04-26 07:25:31", "link": "http://arxiv.org/abs/2304.13343v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multidimensional Evaluation for Text Style Transfer Using ChatGPT", "abstract": "We investigate the potential of ChatGPT as a multidimensional evaluator for\nthe task of \\emph{Text Style Transfer}, alongside, and in comparison to,\nexisting automatic metrics as well as human judgements. We focus on a zero-shot\nsetting, i.e. prompting ChatGPT with specific task instructions, and test its\nperformance on three commonly-used dimensions of text style transfer\nevaluation: style strength, content preservation, and fluency. We perform a\ncomprehensive correlation analysis for two transfer directions (and overall) at\ndifferent levels. Compared to existing automatic metrics, ChatGPT achieves\ncompetitive correlations with human judgments. These preliminary results are\nexpected to provide a first glimpse into the role of large language models in\nthe multidimensional evaluation of stylized text generation.", "published": "2023-04-26 11:33:35", "link": "http://arxiv.org/abs/2304.13462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shades of meaning: Uncovering the geometry of ambiguous word\n  representations through contextualised language models", "abstract": "Lexical ambiguity presents a profound and enduring challenge to the language\nsciences. Researchers for decades have grappled with the problem of how\nlanguage users learn, represent and process words with more than one meaning.\nOur work offers new insight into psychological understanding of lexical\nambiguity through a series of simulations that capitalise on recent advances in\ncontextual language models. These models have no grounded understanding of the\nmeanings of words at all; they simply learn to predict words based on the\nsurrounding context provided by other words. Yet, our analyses show that their\nrepresentations capture fine-grained meaningful distinctions between\nunambiguous, homonymous, and polysemous words that align with lexicographic\nclassifications and psychological theorising. These findings provide\nquantitative support for modern psychological conceptualisations of lexical\nambiguity and raise new challenges for understanding of the way that contextual\ninformation shapes the meanings of words across different timescales.", "published": "2023-04-26 14:47:38", "link": "http://arxiv.org/abs/2304.13597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource\n  TweetData for Sentiment Analysis", "abstract": "We present the findings of SemEval-2023 Task 12, a shared task on sentiment\nanalysis for low-resource African languages using Twitter dataset. The task\nfeatured three subtasks; subtask A is monolingual sentiment classification with\n12 tracks which are all monolingual languages, subtask B is multilingual\nsentiment classification using the tracks in subtask A and subtask C is a\nzero-shot sentiment classification. We present the results and findings of\nsubtask A, subtask B and subtask C. We also release the code on github. Our\ngoal is to leverage low-resource tweet data using pre-trained Afro-xlmr-large,\nAfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment (Arabic-camelbert),\nMultilingual-BERT (mBERT) and BERT models for sentiment analysis of 14 African\nlanguages. The datasets for these subtasks consists of a gold standard\nmulti-class labeled Twitter datasets from these languages. Our results\ndemonstrate that Afro-xlmr-large model performed better compared to the other\nmodels in most of the languages datasets. Similarly, Nigerian languages: Hausa,\nIgbo, and Yoruba achieved better performance compared to other languages and\nthis can be attributed to the higher volume of data present in the languages.", "published": "2023-04-26 15:47:50", "link": "http://arxiv.org/abs/2304.13634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine Tuning with Abnormal Examples", "abstract": "Given the prevalence of crowd sourced labor in creating Natural Language\nprocessing datasets, these aforementioned sets have become increasingly large.\nFor instance, the SQUAD dataset currently sits at over 80,000 records. However,\nbecause the English language is rather repetitive in structure, the\ndistribution of word frequencies in the SQUAD dataset's contexts are relatively\nunchanged. By measuring each sentences distance from the co-variate distance of\nfrequencies of all sentences in the dataset, we identify 10,500 examples that\ncreate a more uniform distribution for training. While fine-tuning ELECTRA [4]\non this subset of examples reaches better performance to a model trained on all\n87,000 examples. Herein we introduce a methodology for systematically pruning\ndatasets for fine tuning reaching better out of sample performance.", "published": "2023-04-26 18:59:48", "link": "http://arxiv.org/abs/2304.13783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense\n  Disambiguation with Pretrained Language Models", "abstract": "Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can\nbe finetuned to perform well on diverse tasks such as translation and\nmultilingual word sense disambiguation (WSD). However, they often struggle at\ndisambiguating word sense in a zero-shot setting. To better understand this\ncontrast, we present a new study investigating how well PLMs capture\ncross-lingual word sense with Contextual Word-Level Translation (C-WLT), an\nextension of word-level translation that prompts the model to translate a given\nword in context. We find that as the model size increases, PLMs encode more\ncross-lingual word sense knowledge and better use context to improve WLT\nperformance. Building on C-WLT, we introduce a zero-shot approach for WSD,\ntested on 18 languages from the XL-WSD dataset. Our method outperforms fully\nsupervised baselines on recall for many evaluation languages without additional\ntraining or finetuning. This study presents a first step towards understanding\nhow to best leverage the cross-lingual knowledge inside PLMs for robust\nzero-shot reasoning in any language.", "published": "2023-04-26 19:55:52", "link": "http://arxiv.org/abs/2304.13803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Procedural Knowledge across Commonsense Tasks", "abstract": "Stories about everyday situations are an essential part of human\ncommunication, motivating the need to develop AI agents that can reliably\nunderstand these stories. Despite the long list of supervised methods for story\ncompletion and procedural understanding, current AI has no mechanisms to\nautomatically track and explain procedures in unseen stories. To bridge this\ngap, we study the ability of AI models to transfer procedural knowledge to\nnovel narrative tasks in a transparent manner. We design LEAP: a comprehensive\nframework that integrates state-of-the-art modeling architectures, training\nregimes, and augmentation strategies based on both natural and synthetic\nstories. To address the lack of densely annotated training data, we devise a\nrobust automatic labeler based on few-shot prompting to enhance the augmented\ndata. Our experiments with in- and out-of-domain tasks reveal insights into the\ninterplay of different architectures, training regimes, and augmentation\nstrategies. LEAP's labeler has a clear positive impact on out-of-domain\ndatasets, while the resulting dense annotation provides native explainability.", "published": "2023-04-26 23:24:50", "link": "http://arxiv.org/abs/2304.13867v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Closeness of In-Context Learning and Weight Shifting for Softmax\n  Regression", "abstract": "Large language models (LLMs) are known for their exceptional performance in\nnatural language processing, making them highly effective in many human\nlife-related or even job-related tasks. The attention mechanism in the\nTransformer architecture is a critical component of LLMs, as it allows the\nmodel to selectively focus on specific input parts. The softmax unit, which is\na key part of the attention mechanism, normalizes the attention scores. Hence,\nthe performance of LLMs in various NLP tasks depends significantly on the\ncrucial role played by the attention mechanism with the softmax unit.\n  In-context learning, as one of the celebrated abilities of recent LLMs, is an\nimportant concept in querying LLMs such as ChatGPT. Without further parameter\nupdates, Transformers can learn to predict based on few in-context examples.\nHowever, the reason why Transformers becomes in-context learners is not well\nunderstood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the\nin-context learning from a mathematical perspective based on a linear\nregression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers'\ncapability of learning linear functions in context.\n  In this work, we study the in-context learning based on a softmax regression\nformulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b\n\\|_2$ of Transformer's attention mechanism. We show the upper bounds of the\ndata transformations induced by a single self-attention layer and by\ngradient-descent on a $\\ell_2$ regression loss for softmax prediction function,\nwhich imply that when training self-attention-only Transformers for fundamental\nregression tasks, the models learned by gradient-descent and Transformers show\ngreat similarity.", "published": "2023-04-26 04:33:41", "link": "http://arxiv.org/abs/2304.13276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton\n  Retrieval", "abstract": "Text-to-SQL is a task that converts a natural language question into a\nstructured query language (SQL) to retrieve information from a database. Large\nlanguage models (LLMs) work well in natural language generation tasks, but they\nare not specifically pre-trained to understand the syntax and semantics of SQL\ncommands. In this paper, we propose an LLM-based framework for Text-to-SQL\nwhich retrieves helpful demonstration examples to prompt LLMs. However,\nquestions with different database schemes can vary widely, even if the\nintentions behind them are similar and the corresponding SQL queries exhibit\nsimilarities. Consequently, it becomes crucial to identify the appropriate SQL\ndemonstrations that align with our requirements. We design a de-semanticization\nmechanism that extracts question skeletons, allowing us to retrieve similar\nexamples based on their structural similarity. We also model the relationships\nbetween question tokens and database schema items (i.e., tables and columns) to\nfilter out scheme-related information. Our framework adapts the range of the\ndatabase schema in prompts to balance length and valuable information. A\nfallback mechanism allows for a more detailed schema to be provided if the\ngenerated SQL query fails. Ours outperforms state-of-the-art models and\ndemonstrates strong generalization ability on three cross-domain Text-to-SQL\nbenchmarks.", "published": "2023-04-26 06:02:01", "link": "http://arxiv.org/abs/2304.13301v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables", "abstract": "In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class\nof database systems that can seamlessly query text and tables using SQL. To\nenable seamless querying of textual data using SQL in an MMDB, we propose to\nextend relational databases with so-called multi-modal operators (MMOps) which\nare based on the advances of recent large language models such as GPT-3. The\nmain idea of MMOps is that they allow text collections to be treated as tables\nwithout the need to manually transform the data. As we show in our evaluation,\nour MMDB prototype can not only outperform state-of-the-art approaches such as\ntext-to-table in terms of accuracy and performance but it also requires\nsignificantly less training data to fine-tune the model for an unseen text\ncollection.", "published": "2023-04-26 13:31:04", "link": "http://arxiv.org/abs/2304.13559v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Technical Report: Impact of Position Bias on Language Models in Token\n  Classification", "abstract": "Language Models (LMs) have shown state-of-the-art performance in Natural\nLanguage Processing (NLP) tasks. Downstream tasks such as Named Entity\nRecognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data\nimbalance issues, particularly regarding the ratio of positive to negative\nexamples and class disparities. This paper investigates an often-overlooked\nissue of encoder models, specifically the position bias of positive examples in\ntoken classification tasks. For completeness, we also include decoders in the\nevaluation. We evaluate the impact of position bias using different position\nembedding techniques, focusing on BERT with Absolute Position Embedding (APE),\nRelative Position Embedding (RPE), and Rotary Position Embedding (RoPE).\nTherefore, we conduct an in-depth evaluation of the impact of position bias on\nthe performance of LMs when fine-tuned on token classification benchmarks. Our\nstudy includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD\\_en, and\nTweeBank for POS tagging. We propose an evaluation approach to investigate\nposition bias in transformer models. We show that LMs can suffer from this bias\nwith an average drop ranging from 3\\% to 9\\% in their performance. To mitigate\nthis effect, we propose two methods: Random Position Shifting and Context\nPerturbation, that we apply on batches during the training process. The results\nshow an improvement of $\\approx$ 2\\% in the performance of the model on\nCoNLL03, UD\\_en, and TweeBank.", "published": "2023-04-26 13:57:25", "link": "http://arxiv.org/abs/2304.13567v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toxic comments reduce the activity of volunteer editors on Wikipedia", "abstract": "Wikipedia is one of the most successful collaborative projects in history. It\nis the largest encyclopedia ever created, with millions of users worldwide\nrelying on it as the first source of information as well as for fact-checking\nand in-depth research. As Wikipedia relies solely on the efforts of its\nvolunteer-editors, its success might be particularly affected by toxic speech.\nIn this paper, we analyze all 57 million comments made on user talk pages of\n8.5 million editors across the six most active language editions of Wikipedia\nto study the potential impact of toxicity on editors' behaviour. We find that\ntoxic comments consistently reduce the activity of editors, leading to an\nestimated loss of 0.5-2 active days per user in the short term. This amounts to\nmultiple human-years of lost productivity when considering the number of active\ncontributors to Wikipedia. The effects of toxic comments are even greater in\nthe long term, as they significantly increase the risk of editors leaving the\nproject altogether. Using an agent-based model, we demonstrate that toxicity\nattacks on Wikipedia have the potential to impede the progress of the entire\nproject. Our results underscore the importance of mitigating toxic speech on\ncollaborative platforms such as Wikipedia to ensure their continued success.", "published": "2023-04-26 13:59:25", "link": "http://arxiv.org/abs/2304.13568v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization\n  of Long and Short Summaries", "abstract": "Automatic chart to text summarization is an effective tool for the visually\nimpaired people along with providing precise insights of tabular data in\nnatural language to the user. A large and well-structured dataset is always a\nkey part for data driven models. In this paper, we propose ChartSumm: a\nlarge-scale benchmark dataset consisting of a total of 84,363 charts along with\ntheir metadata and descriptions covering a wide range of topics and chart types\nto generate short and long summaries. Extensive experiments with strong\nbaseline models show that even though these models generate fluent and\ninformative summaries by achieving decent scores in various automatic\nevaluation metrics, they often face issues like suffering from hallucination,\nmissing out important data points, in addition to incorrect explanation of\ncomplex trends in the charts. We also investigated the potential of expanding\nChartSumm to other languages using automated translation tools. These make our\ndataset a challenging benchmark for future research.", "published": "2023-04-26 15:25:24", "link": "http://arxiv.org/abs/2304.13620v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Implicit Feedback to Improve Question Generation", "abstract": "Question Generation (QG) is a task of Natural Language Processing (NLP) that\naims at automatically generating questions from text. Many applications can\nbenefit from automatically generated questions, but often it is necessary to\ncurate those questions, either by selecting or editing them. This task is\ninformative on its own, but it is typically done post-generation, and, thus,\nthe effort is wasted. In addition, most existing systems cannot incorporate\nthis feedback back into them easily. In this work, we present a system, GEN,\nthat learns from such (implicit) feedback. Following a pattern-based approach,\nit takes as input a small set of sentence/question pairs and creates patterns\nwhich are then applied to new unseen sentences. Each generated question, after\nbeing corrected by the user, is used as a new seed in the next iteration, so\nmore patterns are created each time. We also take advantage of the corrections\nmade by the user to score the patterns and therefore rank the generated\nquestions. Results show that GEN is able to improve by learning from both\nlevels of implicit feedback when compared to the version with no learning,\nconsidering the top 5, 10, and 20 questions. Improvements go up from 10%,\ndepending on the metric and strategy used.", "published": "2023-04-26 16:37:47", "link": "http://arxiv.org/abs/2304.13664v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HeySQuAD: A Spoken Question Answering Dataset", "abstract": "Spoken question answering (SQA) systems are critical for digital assistants\nand other real-world use cases, but evaluating their performance is a challenge\ndue to the importance of human-spoken questions. This study presents a new\nlarge-scale community-shared SQA dataset called HeySQuAD, which includes 76k\nhuman-spoken questions, 97k machine-generated questions, and their\ncorresponding textual answers from the SQuAD QA dataset. Our goal is to measure\nthe ability of machines to accurately understand noisy spoken questions and\nprovide reliable answers. Through extensive testing, we demonstrate that\ntraining with transcribed human-spoken and original SQuAD questions leads to a\nsignificant improvement (12.51%) in answering human-spoken questions compared\nto training with only the original SQuAD textual questions. Moreover,\nevaluating with a higher-quality transcription can lead to a further\nimprovement of 2.03%. This research has significant implications for the\ndevelopment of SQA systems and their ability to meet the needs of users in\nreal-world scenarios.", "published": "2023-04-26 17:15:39", "link": "http://arxiv.org/abs/2304.13689v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans\n  and Models", "abstract": "Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.", "published": "2023-04-26 21:41:17", "link": "http://arxiv.org/abs/2304.13835v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions,\n  Experiences and Claims from Social Media using Knowledge-Augmented\n  Pre-trained Language Models", "abstract": "In online forums like Reddit, users share their experiences with medical\nconditions and treatments, including making claims, asking questions, and\ndiscussing the effects of treatments on their health. Building systems to\nunderstand this information can effectively monitor the spread of\nmisinformation and verify user claims. The Task-8 of the 2023 International\nWorkshop on Semantic Evaluation focused on medical applications, specifically\nextracting patient experience- and medical condition-related entities from user\nposts on social media. The Reddit Health Online Talk (RedHot) corpus contains\nposts from medical condition-related subreddits with annotations characterizing\nthe patient experience and medical conditions. In Subtask-1, patient experience\nis characterized by personal experience, questions, and claims. In Subtask-2,\nmedical conditions are characterized by population, intervention, and outcome.\nFor the automatic extraction of patient experiences and medical condition\ninformation, as a part of the challenge, we proposed language-model-based\nextraction systems that ranked $3^{rd}$ on both subtasks' leaderboards. In this\nwork, we describe our approach and, in addition, explore the automatic\nextraction of this information using domain-specific language models and the\ninclusion of external knowledge.", "published": "2023-04-26 23:45:11", "link": "http://arxiv.org/abs/2304.13875v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Association to Generation: Text-only Captioning by Unsupervised\n  Cross-modal Mapping", "abstract": "With the development of Vision-Language Pre-training Models (VLPMs)\nrepresented by CLIP and ALIGN, significant breakthroughs have been achieved for\nassociation-based visual tasks such as image classification and image-text\nretrieval by the zero-shot capability of CLIP without fine-tuning. However,\nCLIP is hard to apply to generation-based tasks. This is due to the lack of\ndecoder architecture and pre-training tasks for generation. Although previous\nworks have created generation capacity for CLIP through additional language\nmodels, a modality gap between the CLIP representations of different modalities\nand the inability of CLIP to model the offset of this gap, which fails the\nconcept to transfer across modalities. To solve the problem, we try to map\nimages/videos to the language modality and generate captions from the language\nmodality. In this paper, we propose the K-nearest-neighbor Cross-modality\nMapping (Knight), a zero-shot method from association to generation. With\ntext-only unsupervised training, Knight achieves State-of-the-Art performance\nin zero-shot methods for image captioning and video captioning. Our code is\navailable at https://github.com/junyangwang0410/Knight.", "published": "2023-04-26 04:06:20", "link": "http://arxiv.org/abs/2304.13273v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Symmetric Dual Encoding Dense Retrieval Framework for\n  Knowledge-Intensive Visual Question Answering", "abstract": "Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a\nquestion about an image whose answer does not lie in the image. This paper\npresents a new pipeline for KI-VQA tasks, consisting of a retriever and a\nreader. First, we introduce DEDR, a symmetric dual encoding dense retrieval\nframework in which documents and queries are encoded into a shared embedding\nspace using uni-modal (textual) and multi-modal encoders. We introduce an\niterative knowledge distillation approach that bridges the gap between the\nrepresentation spaces in these two encoders. Extensive evaluation on two\nwell-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR\noutperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,\nrespectively. Utilizing the passages retrieved by DEDR, we further introduce\nMM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating\na textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and\neach retrieved passage separately and uses all passages jointly in its decoder.\nCompared to competitive baselines in the literature, this approach leads to\n5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA\nand FVQA, respectively.", "published": "2023-04-26 16:14:39", "link": "http://arxiv.org/abs/2304.13649v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "abstract": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.", "published": "2023-04-26 17:52:30", "link": "http://arxiv.org/abs/2304.13712v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of GPT-3.5 and GPT-4 for supporting real-world information\n  needs in healthcare delivery", "abstract": "Despite growing interest in using large language models (LLMs) in healthcare,\ncurrent explorations do not assess the real-world utility and safety of LLMs in\nclinical settings. Our objective was to determine whether two LLMs can serve\ninformation needs submitted by physicians as questions to an informatics\nconsultation service in a safe and concordant manner. Sixty six questions from\nan informatics consult service were submitted to GPT-3.5 and GPT-4 via simple\nprompts. 12 physicians assessed the LLM responses' possibility of patient harm\nand concordance with existing reports from an informatics consultation service.\nPhysician assessments were summarized based on majority vote. For no questions\ndid a majority of physicians deem either LLM response as harmful. For GPT-3.5,\nresponses to 8 questions were concordant with the informatics consult report,\n20 discordant, and 9 were unable to be assessed. There were 29 responses with\nno majority on \"Agree\", \"Disagree\", and \"Unable to assess\". For GPT-4,\nresponses to 13 questions were concordant, 15 discordant, and 3 were unable to\nbe assessed. There were 35 responses with no majority. Responses from both LLMs\nwere largely devoid of overt harm, but less than 20% of the responses agreed\nwith an answer from an informatics consultation service, responses contained\nhallucinated references, and physicians were divided on what constitutes harm.\nThese results suggest that while general purpose LLMs are able to provide safe\nand credible responses, they often do not meet the specific information need of\na given question. A definitive evaluation of the usefulness of LLMs in\nhealthcare settings will likely require additional research on prompt\nengineering, calibration, and custom-tailoring of general purpose models.", "published": "2023-04-26 17:54:28", "link": "http://arxiv.org/abs/2304.13714v3", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "The Internal State of an LLM Knows When It's Lying", "abstract": "While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, one of their most prominent drawbacks is generating inaccurate\nor false information with a confident tone. In this paper, we provide evidence\nthat the LLM's internal state can be used to reveal the truthfulness of\nstatements. This includes both statements provided to the LLM, and statements\nthat the LLM itself generates. Our approach is to train a classifier that\noutputs the probability that a statement is truthful, based on the hidden layer\nactivations of the LLM as it reads or generates the statement. Experiments\ndemonstrate that given a set of test sentences, of which half are true and half\nfalse, our trained classifier achieves an average of 71\\% to 83\\% accuracy\nlabeling which sentences are true versus false, depending on the LLM base\nmodel. Furthermore, we explore the relationship between our classifier's\nperformance and approaches based on the probability assigned to the sentence by\nthe LLM. We show that while LLM-assigned sentence probability is related to\nsentence truthfulness, this probability is also dependent on sentence length\nand the frequencies of words in the sentence, resulting in our trained\nclassifier providing a more reliable approach to detecting truthfulness,\nhighlighting its potential to enhance the reliability of LLM-generated content\nand its practical applicability in real-world scenarios.", "published": "2023-04-26 02:49:38", "link": "http://arxiv.org/abs/2304.13734v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from\n  Literature with GPT-3", "abstract": "Although gold nanorods have been the subject of much research, the pathways\nfor controlling their shape and thereby their optical properties remain largely\nheuristically understood. Although it is apparent that the simultaneous\npresence of and interaction between various reagents during synthesis control\nthese properties, computational and experimental approaches for exploring the\nsynthesis space can be either intractable or too time-consuming in practice.\nThis motivates an alternative approach leveraging the wealth of synthesis\ninformation already embedded in the body of scientific literature by developing\ntools to extract relevant structured data in an automated, high-throughput\nmanner. To that end, we present an approach using the powerful GPT-3 language\nmodel to extract structured multi-step seed-mediated growth procedures and\noutcomes for gold nanorods from unstructured scientific text. GPT-3 prompt\ncompletions are fine-tuned to predict synthesis templates in the form of JSON\ndocuments from unstructured text input with an overall accuracy of $86\\%$. The\nperformance is notable, considering the model is performing simultaneous entity\nrecognition and relation extraction. We present a dataset of 11,644 entities\nextracted from 1,137 papers, resulting in 268 papers with at least one complete\nseed-mediated gold nanorod growth procedure and outcome for a total of 332\ncomplete procedures.", "published": "2023-04-26 22:21:33", "link": "http://arxiv.org/abs/2304.13846v1", "categories": ["physics.app-ph", "cs.CL", "cs.IR"], "primary_category": "physics.app-ph"}
{"title": "The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in\n  Classification Tasks", "abstract": "In the realm of Computational Social Science (CSS), practitioners often\nnavigate complex, low-resource domains and face the costly and time-intensive\nchallenges of acquiring and annotating data. We aim to establish a set of\nguidelines to address such challenges, comparing the use of human-labeled data\nwith synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS\nclassification tasks of varying complexity. Additionally, we examine the impact\nof training data sizes on performance. Our findings reveal that models trained\non human-labeled data consistently exhibit superior or comparable performance\ncompared to their synthetically augmented counterparts. Nevertheless, synthetic\naugmentation proves beneficial, particularly in improving performance on rare\nclasses within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2\nfor zero-shot classification and find that, while they generally display strong\nperformance, they often fall short when compared to specialized classifiers\ntrained on moderately sized training sets.", "published": "2023-04-26 23:09:02", "link": "http://arxiv.org/abs/2304.13861v2", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "All Information is Necessary: Integrating Speech Positive and Negative\n  Information by Contrastive Learning for Speech Enhancement", "abstract": "Monaural speech enhancement (SE) is an ill-posed problem due to the\nirreversible degradation process. Recent methods to achieve SE tasks rely\nsolely on positive information, e.g., ground-truth speech and speech-relevant\nfeatures. Different from the above, we observe that the negative information,\nsuch as original speech mixture and speech-irrelevant features, are valuable to\nguide the SE model training procedure. In this study, we propose a SE model\nthat integrates both speech positive and negative information for improving SE\nperformance by adopting contrastive learning, in which two innovations have\nconsisted. (1) We design a collaboration module (CM), which contains two parts,\ncontrastive attention for separating relevant and irrelevant features via\ncontrastive learning and interactive attention for establishing the correlation\nbetween both speech features in a learnable and self-adaptive manner. (2) We\npropose a contrastive regularization (CR) built upon contrastive learning to\nensure that the estimated speech is pulled closer to the clean speech and\npushed far away from the noisy speech in the representation space by\nintegrating self-supervised models. We term the proposed SE network with CM and\nCR as CMCR-Net. Experimental results demonstrate that our CMCR-Net achieves\ncomparable and superior performance to recent approaches.", "published": "2023-04-26 10:50:05", "link": "http://arxiv.org/abs/2304.13439v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Source-Filter-Based Generative Adversarial Neural Vocoder for High\n  Fidelity Speech Synthesis", "abstract": "This paper proposes a source-filter-based generative adversarial neural\nvocoder named SF-GAN, which achieves high-fidelity waveform generation from\ninput acoustic features by introducing F0-based source excitation signals to a\nneural filter framework. The SF-GAN vocoder is composed of a source module and\na resolution-wise conditional filter module and is trained based on generative\nadversarial strategies. The source module produces an excitation signal from\nthe F0 information, then the resolution-wise convolutional filter module\ncombines the excitation signal with processed acoustic features at various\ntemporal resolutions and finally reconstructs the raw waveform. The\nexperimental results show that our proposed SF-GAN vocoder outperforms the\nstate-of-the-art HiFi-GAN and Fre-GAN in both analysis-synthesis (AS) and\ntext-to-speech (TTS) tasks, and the synthesized speech quality of SF-GAN is\ncomparable to the ground-truth audio.", "published": "2023-04-26 03:43:37", "link": "http://arxiv.org/abs/2304.13270v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
