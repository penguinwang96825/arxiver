{"title": "Character-based Neural Machine Translation", "abstract": "We introduce a neural machine translation model that views the input and\noutput sentences as sequences of characters rather than words. Since word-level\ninformation provides a crucial source of bias, our input model composes\nrepresentations of character sequences into representations of words (as\ndetermined by whitespace boundaries), and then these are translated using a\njoint attention/translation model. In the target language, the translation is\nmodeled as a sequence of word vectors, but each word is generated one character\nat a time, conditional on the previous character generations in each word. As\nthe representation and generation of words is performed at the character level,\nour model is capable of interpreting and generating unseen word forms. A\nsecondary benefit of this approach is that it alleviates much of the challenges\nassociated with preprocessing/tokenization of the source and target languages.\nWe show that our model can achieve translation results that are on par with\nconventional word-based models.", "published": "2015-11-14 17:36:43", "link": "http://arxiv.org/abs/1511.04586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Represent Words in Context with Multilingual Supervision", "abstract": "We present a neural network architecture based on bidirectional LSTMs to\ncompute representations of words in the sentential contexts. These\ncontext-sensitive word representations are suitable for, e.g., distinguishing\ndifferent word senses and other context-modulated variations in meaning. To\nlearn the parameters of our model, we use cross-lingual supervision,\nhypothesizing that a good representation of a word in context will be one that\nis sufficient for selecting the correct translation into a second language. We\nevaluate the quality of our representations as features in three downstream\ntasks: prediction of semantic supersenses (which assign nouns and verbs into a\nfew dozen semantic classes), low resource machine translation, and a lexical\nsubstitution task, and obtain state-of-the-art results on all of these.", "published": "2015-11-14 21:36:38", "link": "http://arxiv.org/abs/1511.04623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Oracle performance for visual captioning", "abstract": "The task of associating images and videos with a natural language description\nhas attracted a great amount of attention recently. Rapid progress has been\nmade in terms of both developing novel algorithms and releasing new datasets.\nIndeed, the state-of-the-art results on some of the standard datasets have been\npushed into the regime where it has become more and more difficult to make\nsignificant improvements. Instead of proposing new models, this work\ninvestigates the possibility of empirically establishing performance upper\nbounds on various visual captioning datasets without extra data labelling\neffort or human evaluation. In particular, it is assumed that visual captioning\nis decomposed into two steps: from visual inputs to visual concepts, and from\nvisual concepts to natural language descriptions. One would be able to obtain\nan upper bound when assuming the first step is perfect and only requiring\ntraining a conditional language model for the second step. We demonstrate the\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\nfor visual concept extraction in the first step and the simplicity of the\nlanguage model for the second step, we show that current state-of-the-art\nmodels fall short when being compared with the learned upper bounds.\nFurthermore, with such a bound, we quantify several important factors\nconcerning image and video captioning: the number of visual concepts captured\nby different models, the trade-off between the amount of visual elements\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\ndifferent datasets.", "published": "2015-11-14 18:02:39", "link": "http://arxiv.org/abs/1511.04590v5", "categories": ["cs.CV", "cs.CL", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Deep Reinforcement Learning with a Natural Language Action Space", "abstract": "This paper introduces a novel architecture for reinforcement learning with\ndeep neural networks designed to handle state and action spaces characterized\nby natural language, as found in text-based games. Termed a deep reinforcement\nrelevance network (DRRN), the architecture represents action and state spaces\nwith separate embedding vectors, which are combined with an interaction\nfunction to approximate the Q-function in reinforcement learning. We evaluate\nthe DRRN on two popular text games, showing superior performance over other\ndeep Q-learning architectures. Experiments with paraphrased action descriptions\nshow that the model is extracting meaning rather than simply memorizing strings\nof text.", "published": "2015-11-14 23:30:39", "link": "http://arxiv.org/abs/1511.04636v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
