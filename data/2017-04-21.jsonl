{"title": "Improving Context Aware Language Models", "abstract": "Increased adaptability of RNN language models leads to improved predictions\nthat benefit many applications. However, current methods do not take full\nadvantage of the RNN structure. We show that the most widely-used approach to\nadaptation (concatenating the context with the word embedding at the input to\nthe recurrent layer) is outperformed by a model that has some low-cost\nimprovements: adaptation of both the hidden and output layers. and a feature\nhashing bias term to capture context idiosyncrasies. Experiments on language\nmodeling and classification tasks using three different corpora demonstrate the\nadvantages of the proposed techniques.", "published": "2017-04-21 02:27:26", "link": "http://arxiv.org/abs/1704.06380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural System Combination for Machine Translation", "abstract": "Neural machine translation (NMT) becomes a new approach to machine\ntranslation and generates much more fluent results compared to statistical\nmachine translation (SMT).\n  However, SMT is usually better than NMT in translation adequacy. It is\ntherefore a promising direction to combine the advantages of both NMT and SMT.\n  In this paper, we propose a neural system combination framework leveraging\nmulti-source NMT, which takes as input the outputs of NMT and SMT systems and\nproduces the final translation.\n  Extensive experiments on the Chinese-to-English translation task show that\nour model archives significant improvement by 5.3 BLEU points over the best\nsingle system output and 3.4 BLEU points over the state-of-the-art traditional\nsystem combination methods.", "published": "2017-04-21 04:36:55", "link": "http://arxiv.org/abs/1704.06393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Semantic Composition with Offset Inference", "abstract": "Count-based distributional semantic models suffer from sparsity due to\nunobserved but plausible co-occurrences in any text collection. This problem is\namplified for models like Anchored Packed Trees (APTs), that take the\ngrammatical type of a co-occurrence into account. We therefore introduce a\nnovel form of distributional inference that exploits the rich type structure in\nAPTs and infers missing data by the same mechanism that is used for semantic\ncomposition.", "published": "2017-04-21 19:47:30", "link": "http://arxiv.org/abs/1704.06692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semantic QA-Based Approach for Text Summarization Evaluation", "abstract": "Many Natural Language Processing and Computational Linguistics applications\ninvolves the generation of new texts based on some existing texts, such as\nsummarization, text simplification and machine translation. However, there has\nbeen a serious problem haunting these applications for decades, that is, how to\nautomatically and accurately assess quality of these applications. In this\npaper, we will present some preliminary results on one especially useful and\nchallenging problem in NLP system evaluation: how to pinpoint content\ndifferences of two text passages (especially for large pas-sages such as\narticles and books). Our idea is intuitive and very different from existing\napproaches. We treat one text passage as a small knowledge base, and ask it a\nlarge number of questions to exhaustively identify all content points in it. By\ncomparing the correctly answered questions from two text passages, we will be\nable to compare their content precisely. The experiment using 2007 DUC\nsummarization corpus clearly shows promising results.", "published": "2017-04-21 15:32:01", "link": "http://arxiv.org/abs/1704.06259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attend to You: Personalized Image Captioning with Context Sequence\n  Memory Networks", "abstract": "We address personalization issues of image captioning, which have not been\ndiscussed yet in previous research. For a query image, we aim to generate a\ndescriptive sentence, accounting for prior knowledge such as the user's active\nvocabularies in previous documents. As applications of personalized image\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\nMemory Network (CSMN). Its unique updates over previous memory network models\ninclude (i) exploiting memory as a repository for multiple types of context\ninformation, (ii) appending previously generated words into memory to capture\nlong-term information without suffering from the vanishing gradient problem,\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\nmemory slots for better context understanding. With quantitative evaluation and\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\nnovel features of CSMN and its performance enhancement for personalized image\ncaptioning over state-of-the-art captioning models.", "published": "2017-04-21 11:29:07", "link": "http://arxiv.org/abs/1704.06485v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "abstract": "Modeling attention in neural multi-source sequence-to-sequence learning\nremains a relatively unexplored area, despite its usefulness in tasks that\nincorporate multiple source languages or modalities. We propose two novel\napproaches to combine the outputs of attention mechanisms over each source\nsequence, flat and hierarchical. We compare the proposed methods with existing\ntechniques and present results of systematic evaluation of those methods on the\nWMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the\nproposed methods achieve competitive results on both tasks.", "published": "2017-04-21 14:39:27", "link": "http://arxiv.org/abs/1704.06567v1", "categories": ["cs.CL", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Scientific Article Summarization Using Citation-Context and Article's\n  Discourse Structure", "abstract": "We propose a summarization approach for scientific articles which takes\nadvantage of citation-context and the document discourse model. While citations\nhave been previously used in generating scientific summaries, they lack the\nrelated context from the referenced article and therefore do not accurately\nreflect the article's content. Our method overcomes the problem of\ninconsistency between the citation summary and the article's content by\nproviding context for each citation. We also leverage the inherent scientific\narticle's discourse for producing better summaries. We show that our proposed\nmethod effectively improves over existing summarization approaches (greater\nthan 30% improvement over the best performing baseline) in terms of\n\\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While the\ndataset we use for evaluation is in the biomedical domain, most of our\napproaches are general and therefore adaptable to other domains.", "published": "2017-04-21 16:17:58", "link": "http://arxiv.org/abs/1704.06619v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "abstract": "Bandit structured prediction describes a stochastic optimization framework\nwhere learning is performed from partial feedback. This feedback is received in\nthe form of a task loss evaluation to a predicted output structure, without\nhaving access to gold standard structures. We advance this framework by lifting\nlinear bandit learning to neural sequence-to-sequence learning problems using\nattention-based recurrent neural networks. Furthermore, we show how to\nincorporate control variates into our learning algorithms for variance\nreduction and improved generalization. We present an evaluation on a neural\nmachine translation task that shows improvements of up to 5.89 BLEU points for\ndomain adaptation from simulated bandit feedback.", "published": "2017-04-21 11:56:00", "link": "http://arxiv.org/abs/1704.06497v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
