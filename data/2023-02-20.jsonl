{"title": "What happens before and after: Multi-Event Commonsense in Event\n  Coreference Resolution", "abstract": "Event coreference models cluster event mentions pertaining to the same\nreal-world event. Recent models rely on contextualized representations to\nrecognize coreference among lexically or contextually similar mentions.\nHowever, models typically fail to leverage commonsense inferences, which is\nparticularly limiting for resolving lexically-divergent mentions. We propose a\nmodel that extends event mentions with temporal commonsense inferences. Given a\ncomplex sentence with multiple events, e.g., \"The man killed his wife and got\narrested\", with the target event \"arrested\", our model generates plausible\nevents that happen before the target event - such as \"the police arrived\", and\nafter it, such as \"he was sentenced\". We show that incorporating such\ninferences into an existing event coreference model improves its performance,\nand we analyze the coreferences in which such temporal knowledge is required.", "published": "2023-02-20 01:51:01", "link": "http://arxiv.org/abs/2302.09715v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving User Controlled Table-To-Text Generation Robustness", "abstract": "In this work we study user controlled table-to-text generation where users\nexplore the content in a table by selecting cells and reading a natural\nlanguage description thereof automatically produce by a natural language\ngenerator. Such generation models usually learn from carefully selected cell\ncombinations (clean cell selections); however, in practice users may select\nunexpected, redundant, or incoherent cell combinations (noisy cell selections).\nIn experiments, we find that models perform well on test sets coming from the\nsame distribution as the train data but their performance drops when evaluated\non realistic noisy user inputs. We propose a fine-tuning regime with additional\nuser-simulated noisy cell selections. Models fine-tuned with the proposed\nregime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test\ncases; and achieve comparable state-of-the-art performance on the ToTTo\ndataset.", "published": "2023-02-20 07:51:15", "link": "http://arxiv.org/abs/2302.09820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "90% F1 Score in Relational Triple Extraction: Is it Real ?", "abstract": "Extracting relational triples from text is a crucial task for constructing\nknowledge bases. Recent advancements in joint entity and relation extraction\nmodels have demonstrated remarkable F1 scores ($\\ge 90\\%$) in accurately\nextracting relational triples from free text. However, these models have been\nevaluated under restrictive experimental settings and unrealistic datasets.\nThey overlook sentences with zero triples (zero-cardinality), thereby\nsimplifying the task. In this paper, we present a benchmark study of\nstate-of-the-art joint entity and relation extraction models under a more\nrealistic setting. We include sentences that lack any triples in our\nexperiments, providing a comprehensive evaluation. Our findings reveal a\nsignificant decline (approximately 10-15\\% in one dataset and 6-14\\% in another\ndataset) in the models' F1 scores within this realistic experimental setup.\nFurthermore, we propose a two-step modeling approach that utilizes a simple\nBERT-based classifier. This approach leads to overall performance improvement\nin these models within the realistic experimental setting.", "published": "2023-02-20 10:30:16", "link": "http://arxiv.org/abs/2302.09887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Two-Sided Discussion of Preregistration of NLP Research", "abstract": "Van Miltenburg et al. (2021) suggest NLP research should adopt\npreregistration to prevent fishing expeditions and to promote publication of\nnegative results. At face value, this is a very reasonable suggestion,\nseemingly solving many methodological problems with NLP research. We discuss\npros and cons -- some old, some new: a) Preregistration is challenged by the\npractice of retrieving hypotheses after the results are known; b)\npreregistration may bias NLP toward confirmatory research; c) preregistration\nmust allow for reclassification of research as exploratory; d) preregistration\nmay increase publication bias; e) preregistration may increase flag-planting;\nf) preregistration may increase p-hacking; and finally, g) preregistration may\nmake us less risk tolerant. We cast our discussion as a dialogue, presenting\nboth sides of the debate.", "published": "2023-02-20 16:48:34", "link": "http://arxiv.org/abs/2302.10086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatIE: Zero-Shot Information Extraction via Chatting with ChatGPT", "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the\nunannotated text. It is challenging due to involving little human intervention.\nChallenging but worthwhile, zero-shot IE reduces the time and effort that data\nlabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,\nChatGPT) show promising performance on zero-shot settings, thus inspiring us to\nexplore prompt-based methods. In this work, we ask whether strong IE models can\nbe constructed by directly prompting LLMs. Specifically, we transform the\nzero-shot IE task into a multi-turn question-answering problem with a two-stage\nframework (ChatIE). With the power of ChatGPT, we extensively evaluate our\nframework on three IE tasks: entity-relation triple extract, named entity\nrecognition, and event extraction. Empirical results on six datasets across two\nlanguages show that ChatIE achieves impressive performance and even surpasses\nsome full-shot models on several datasets (e.g., NYT11-HRL). We believe that\nour work could shed light on building IE models with limited resources.", "published": "2023-02-20 12:57:12", "link": "http://arxiv.org/abs/2302.10205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning\n  for Task-oriented Dialogue Systems", "abstract": "When learning task-oriented dialogue (ToD) agents, reinforcement learning\n(RL) techniques can naturally be utilized to train dialogue strategies to\nachieve user-specific goals. Prior works mainly focus on adopting advanced RL\ntechniques to train the ToD agents, while the design of the reward function is\nnot well studied. This paper aims at answering the question of how to\nefficiently learn and leverage a reward function for training end-to-end (E2E)\nToD agents. Specifically, we introduce two generalized objectives for\nreward-function learning, inspired by the classical learning-to-rank\nliterature. Further, we utilize the learned reward function to guide the\ntraining of the E2E ToD agent. With the proposed techniques, we achieve\ncompetitive results on the E2E response-generation task on the Multiwoz 2.0\ndataset. Source code and checkpoints are publicly released at\nhttps://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.", "published": "2023-02-20 22:10:04", "link": "http://arxiv.org/abs/2302.10342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT: A Meta-Analysis after 2.5 Months", "abstract": "ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and\nmedia attention since its release in November 2022. However, little hard\nevidence is available regarding its perception in various sources. In this\npaper, we analyze over 300,000 tweets and more than 150 scientific papers to\ninvestigate how ChatGPT is perceived and discussed. Our findings show that\nChatGPT is generally viewed as of high quality, with positive sentiment and\nemotions of joy dominating in social media. Its perception has slightly\ndecreased since its debut, however, with joy decreasing and (negative) surprise\non the rise, and it is perceived more negatively in languages other than\nEnglish. In recent scientific papers, ChatGPT is characterized as a great\nopportunity across various fields including the medical domain, but also as a\nthreat concerning ethics and receives mixed assessments for education. Our\ncomprehensive meta-analysis of ChatGPT's current perception after 2.5 months\nsince its release can contribute to shaping the public debate and informing its\nfuture development. We make our data available.", "published": "2023-02-20 15:43:22", "link": "http://arxiv.org/abs/2302.13795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STOA-VLP: Spatial-Temporal Modeling of Object and Action for\n  Video-Language Pre-training", "abstract": "Although large-scale video-language pre-training models, which usually build\na global alignment between the video and the text, have achieved remarkable\nprogress on various downstream tasks, the idea of adopting fine-grained\ninformation during the pre-training stage is not well explored. In this work,\nwe propose STOA-VLP, a pre-training framework that jointly models object and\naction information across spatial and temporal dimensions. More specifically,\nthe model regards object trajectories across frames and multiple action\nfeatures from the video as fine-grained features. Besides, We design two\nauxiliary tasks to better incorporate both kinds of information into the\npre-training process of the video-language model. The first is the dynamic\nobject-text alignment task, which builds a better connection between object\ntrajectories and the relevant noun tokens. The second is the spatial-temporal\naction set prediction, which guides the model to generate consistent action\nfeatures by predicting actions found in the text. Extensive experiments on\nthree downstream tasks (video captioning, text-video retrieval, and video\nquestion answering) demonstrate the effectiveness of our proposed STOA-VLP\n(e.g. 3.7 Rouge-L improvements on MSR-VTT video captioning benchmark, 2.9%\naccuracy improvements on MSVD video question answering benchmark, compared to\nprevious approaches).", "published": "2023-02-20 03:13:45", "link": "http://arxiv.org/abs/2302.09736v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection", "abstract": "Out-of-distribution (OOD) detection is a rapidly growing field due to new\nrobustness and security requirements driven by an increased number of AI-based\nsystems. Existing OOD textual detectors often rely on an anomaly score (e.g.,\nMahalanobis distance) computed on the embedding output of the last layer of the\nencoder. In this work, we observe that OOD detection performance varies greatly\ndepending on the task and layer output. More importantly, we show that the\nusual choice (the last layer) is rarely the best one for OOD detection and that\nfar better results could be achieved if the best layer were picked. To leverage\nthis observation, we propose a data-driven, unsupervised method to combine\nlayer-wise anomaly scores. In addition, we extend classical textual OOD\nbenchmarks by including classification tasks with a greater number of classes\n(up to 77), which reflects more realistic settings. On this augmented\nbenchmark, we show that the proposed post-aggregation methods achieve robust\nand consistent results while removing manual feature selection altogether.\nTheir performance achieves near oracle's best layer performance.", "published": "2023-02-20 09:26:11", "link": "http://arxiv.org/abs/2302.09852v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boosting classification reliability of NLP transformer models in the\n  long run", "abstract": "Transformer-based machine learning models have become an essential tool for\nmany natural language processing (NLP) tasks since the introduction of the\nmethod. A common objective of these projects is to classify text data.\nClassification models are often extended to a different topic and/or time\nperiod. In these situations, deciding how long a classification is suitable for\nand when it is worth re-training our model is difficult. This paper compares\ndifferent approaches to fine-tune a BERT model for a long-running\nclassification task. We use data from different periods to fine-tune our\noriginal BERT model, and we also measure how a second round of annotation could\nboost the classification quality. Our corpus contains over 8 million comments\non COVID-19 vaccination in Hungary posted between September 2020 and December\n2021. Our results show that the best solution is using all available unlabeled\ncomments to fine-tune a model. It is not advisable to focus only on comments\ncontaining words that our model has not encountered before; a more efficient\nsolution is randomly sample comments from the new period. Fine-tuning does not\nprevent the model from losing performance but merely slows it down. In a\nrapidly changing linguistic environment, it is not possible to maintain model\nperformance without regularly annotating new text.", "published": "2023-02-20 14:46:54", "link": "http://arxiv.org/abs/2302.10016v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hashtag-Guided Low-Resource Tweet Classification", "abstract": "Social media classification tasks (e.g., tweet sentiment analysis, tweet\nstance detection) are challenging because social media posts are typically\nshort, informal, and ambiguous. Thus, training on tweets is challenging and\ndemands large-scale human-annotated labels, which are time-consuming and costly\nto obtain. In this paper, we find that providing hashtags to social media\ntweets can help alleviate this issue because hashtags can enrich short and\nambiguous tweets in terms of various information, such as topic, sentiment, and\nstance. This motivates us to propose a novel Hashtag-guided Tweet\nClassification model (HashTation), which automatically generates meaningful\nhashtags for the input tweet to provide useful auxiliary signals for tweet\nclassification. To generate high-quality and insightful hashtags, our hashtag\ngeneration model retrieves and encodes the post-level and entity-level\ninformation across the whole corpus. Experiments show that HashTation achieves\nsignificant improvements on seven low-resource tweet classification tasks, in\nwhich only a limited amount of training data is provided, showing that\nautomatically enriching tweets with model-generated hashtags could\nsignificantly reduce the demand for large-scale human-labeled data. Further\nanalysis demonstrates that HashTation is able to generate high-quality hashtags\nthat are consistent with the tweets and their labels. The code is available at\nhttps://github.com/shizhediao/HashTation.", "published": "2023-02-20 18:21:02", "link": "http://arxiv.org/abs/2302.10143v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Emphasizing Unseen Words: New Vocabulary Acquisition for End-to-End\n  Speech Recognition", "abstract": "Due to the dynamic nature of human language, automatic speech recognition\n(ASR) systems need to continuously acquire new vocabulary. Out-Of-Vocabulary\n(OOV) words, such as trending words and new named entities, pose problems to\nmodern ASR systems that require long training times to adapt their large\nnumbers of parameters. Different from most previous research focusing on\nlanguage model post-processing, we tackle this problem on an earlier processing\nlevel and eliminate the bias in acoustic modeling to recognize OOV words\nacoustically. We propose to generate OOV words using text-to-speech systems and\nto rescale losses to encourage neural networks to pay more attention to OOV\nwords. Specifically, we enlarge the classification loss used for training\nneural networks' parameters of utterances containing OOV words\n(sentence-level), or rescale the gradient used for back-propagation for OOV\nwords (word-level), when fine-tuning a previously trained model on synthetic\naudio. To overcome catastrophic forgetting, we also explore the combination of\nloss rescaling and model regularization, i.e. L2 regularization and elastic\nweight consolidation (EWC). Compared with previous methods that just fine-tune\nsynthetic audio with EWC, the experimental results on the LibriSpeech benchmark\nreveal that our proposed loss rescaling approach can achieve significant\nimprovement on the recall rate with only a slight decrease on word error rate.\nMoreover, word-level rescaling is more stable than utterance-level rescaling\nand leads to higher recall rates and precision on OOV word recognition.\nFurthermore, our proposed combined loss rescaling and weight consolidation\nmethods can support continual learning of an ASR system.", "published": "2023-02-20 02:21:30", "link": "http://arxiv.org/abs/2302.09723v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Persian topic detection based on Human Word association and graph\n  embedding", "abstract": "In this paper, we propose a framework to detect topics in social media based\non Human Word Association. Identifying topics discussed in these media has\nbecome a critical and significant challenge. Most of the work done in this area\nis in English, but much has been done in the Persian language, especially\nmicroblogs written in Persian. Also, the existing works focused more on\nexploring frequent patterns or semantic relationships and ignored the\nstructural methods of language. In this paper, a topic detection framework\nusing HWA, a method for Human Word Association, is proposed. This method uses\nthe concept of imitation of mental ability for word association. This method\nalso calculates the Associative Gravity Force that shows how words are related.\nUsing this parameter, a graph can be generated. The topics can be extracted by\nembedding this graph and using clustering methods. This approach has been\napplied to a Persian language dataset collected from Telegram. Several\nexperimental studies have been performed to evaluate the proposed framework's\nperformance. Experimental results show that this approach works better than\nother topic detection methods.", "published": "2023-02-20 05:46:47", "link": "http://arxiv.org/abs/2302.09775v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition is a challenging research area that aims to\nfuse different modalities to predict human emotion. However, most existing\nmodels that are based on attention mechanisms have difficulty in learning\nemotionally relevant parts on their own. To solve this problem, we propose to\nincorporate external emotion-related knowledge in the co-attention based fusion\nof pre-trained models. To effectively incorporate this knowledge, we enhance\nthe co-attention model with a Bayesian attention module (BAM) where a prior\ndistribution is estimated using the emotion-related knowledge. Experimental\nresults on the IEMOCAP dataset show that the proposed approach can outperform\nseveral state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).", "published": "2023-02-20 09:38:11", "link": "http://arxiv.org/abs/2302.09856v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Awkward to RDataFrame and back", "abstract": "Awkward Arrays and RDataFrame provide two very different ways of performing\ncalculations at scale. By adding the ability to zero-copy convert between them,\nusers get the best of both. It gives users a better flexibility in mixing\ndifferent packages and languages in their analysis. In Awkward Array version 2,\nthe ak.to_rdataframe function presents a view of an Awkward Array as an\nRDataFrame source. This view is generated on demand and the data are not\ncopied. The column readers are generated based on the run-time type of the\nviews. The readers are passed to a generated source derived from\nROOT::RDF::RDataSource. The ak.from_rdataframe function converts the selected\ncolumns as native Awkward Arrays. The details of the implementation exploiting\nJIT techniques are discussed. The examples of analysis of data stored in\nAwkward Arrays via a high-level interface of an RDataFrame are presented. A few\nexamples of the column definition, applying user-defined filters written in\nC++, and plotting or extracting the columnar data as Awkward Arrays are shown.\nCurrent limitations and future plans are discussed.", "published": "2023-02-20 09:41:02", "link": "http://arxiv.org/abs/2302.09860v1", "categories": ["hep-ex", "astro-ph.IM", "cs.CL", "physics.data-an"], "primary_category": "hep-ex"}
{"title": "Can discrete information extraction prompts generalize across language\n  models?", "abstract": "We study whether automatically-induced prompts that effectively extract\ninformation from a language model can also be used, out-of-the-box, to probe\nother language models for the same information. After confirming that discrete\nprompts induced with the AutoPrompt algorithm outperform manual and semi-manual\nprompts on the slot-filling task, we demonstrate a drop in performance for\nAutoPrompt prompts learned on a model and tested on another. We introduce a way\nto induce prompts by mixing language models at training time that results in\nprompts that generalize well across models. We conduct an extensive analysis of\nthe induced prompts, finding that the more general prompts include a larger\nproportion of existing English words and have a less order-dependent and more\nuniform distribution of information across their component tokens. Our work\nprovides preliminary evidence that it's possible to generate discrete prompts\nthat can be induced once and used with a number of different models, and gives\ninsights on the properties characterizing such prompts.", "published": "2023-02-20 09:56:51", "link": "http://arxiv.org/abs/2302.09865v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mental Health Coping Stories on Social Media: A Causal-Inference Study\n  of Papageno Effect", "abstract": "The Papageno effect concerns how media can play a positive role in preventing\nand mitigating suicidal ideation and behaviors. With the increasing ubiquity\nand widespread use of social media, individuals often express and share lived\nexperiences and struggles with mental health. However, there is a gap in our\nunderstanding about the existence and effectiveness of the Papageno effect in\nsocial media, which we study in this paper. In particular, we adopt a\ncausal-inference framework to examine the impact of exposure to mental health\ncoping stories on individuals on Twitter. We obtain a Twitter dataset with\n$\\sim$2M posts by $\\sim$10K individuals. We consider engaging with coping\nstories as the Treatment intervention, and adopt a stratified propensity score\napproach to find matched cohorts of Treatment and Control individuals. We\nmeasure the psychosocial shifts in affective, behavioral, and cognitive\noutcomes in longitudinal Twitter data before and after engaging with the coping\nstories. Our findings reveal that, engaging with coping stories leads to\ndecreased stress and depression, and improved expressive writing, diversity,\nand interactivity. Our work discusses the practical and platform design\nimplications in supporting mental wellbeing.", "published": "2023-02-20 10:25:28", "link": "http://arxiv.org/abs/2302.09885v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Towards Measuring and Scoring Speaker Diarization Fairness", "abstract": "Speaker diarization, or the task of finding \"who spoke and when\", is now used\nin almost every speech processing application. Nevertheless, its fairness has\nnot yet been evaluated because there was no protocol to study its biases one by\none. In this paper we propose a protocol and a scoring method designed to\nevaluate speaker diarization fairness. This protocol is applied on a large\ndataset of spoken utterances and report the performances of speaker diarization\ndepending on the gender, the age, the accent of the speaker and the length of\nthe spoken sentence. Some biases induced by the gender, or the accent of the\nspeaker were identified when we applied a state-of-the-art speaker diarization\nmethod.", "published": "2023-02-20 14:07:43", "link": "http://arxiv.org/abs/2302.09991v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises", "abstract": "While diffusion models have achieved great success in generating continuous\nsignals such as images and audio, it remains elusive for diffusion models in\nlearning discrete sequence data like natural languages. Although recent\nadvances circumvent this challenge of discreteness by embedding discrete tokens\nas continuous surrogates, they still fall short of satisfactory generation\nquality. To understand this, we first dive deep into the denoised training\nprotocol of diffusion-based sequence generative models and determine their\nthree severe problems, i.e., 1) failing to learn, 2) lack of scalability, and\n3) neglecting source conditions. We argue that these problems can be boiled\ndown to the pitfall of the not completely eliminated discreteness in the\nembedding space, and the scale of noises is decisive herein. In this paper, we\nintroduce DINOISER to facilitate diffusion models for sequence generation by\nmanipulating noises. We propose to adaptively determine the range of sampled\nnoise scales for counter-discreteness training; and encourage the proposed\ndiffused sequence learner to leverage source conditions with amplified noise\nscales during inference. Experiments show that DINOISER enables consistent\nimprovement over the baselines of previous diffusion-based sequence generative\nmodels on several conditional sequence modeling benchmarks thanks to both\neffective training and inference strategies. Analyses further verify that\nDINOISER can make better use of source conditions to govern its generative\nprocess.", "published": "2023-02-20 15:14:46", "link": "http://arxiv.org/abs/2302.10025v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Deep Semantics for Test Completion", "abstract": "Writing tests is a time-consuming yet essential task during software\ndevelopment. We propose to leverage recent advances in deep learning for text\nand code generation to assist developers in writing tests. We formalize the\nnovel task of test completion to automatically complete the next statement in a\ntest method based on the context of prior statements and the code under test.\nWe develop TeCo -- a deep learning model using code semantics for test\ncompletion. The key insight underlying TeCo is that predicting the next\nstatement in a test method requires reasoning about code execution, which is\nhard to do with only syntax-level data that existing code completion models\nuse. TeCo extracts and uses six kinds of code semantics data, including the\nexecution result of prior statements and the execution context of the test\nmethod. To provide a testbed for this new task, as well as to evaluate TeCo, we\ncollect a corpus of 130,934 test methods from 1,270 open-source Java projects.\nOur results show that TeCo achieves an exact-match accuracy of 18, which is 29%\nhigher than the best baseline using syntax-level data only. When measuring\nfunctional correctness of generated next statement, TeCo can generate runnable\ncode in 29% of the cases compared to 18% obtained by the best baseline.\nMoreover, TeCo is significantly better than prior work on test oracle\ngeneration.", "published": "2023-02-20 18:53:56", "link": "http://arxiv.org/abs/2302.10166v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Deep Transformers without Shortcuts: Modifying Self-attention for\n  Faithful Signal Propagation", "abstract": "Skip connections and normalisation layers form two standard architectural\ncomponents that are ubiquitous for the training of Deep Neural Networks (DNNs),\nbut whose precise roles are poorly understood. Recent approaches such as Deep\nKernel Shaping have made progress towards reducing our reliance on them, using\ninsights from wide NN kernel theory to improve signal propagation in vanilla\nDNNs (which we define as networks without skips or normalisation). However,\nthese approaches are incompatible with the self-attention layers present in\ntransformers, whose kernels are intrinsically more complicated to analyse and\ncontrol. And so the question remains: is it possible to train deep vanilla\ntransformers? We answer this question in the affirmative by designing several\napproaches that use combinations of parameter initialisations, bias matrices\nand location-dependent rescaling to achieve faithful signal propagation in\nvanilla transformers. Our methods address various intricacies specific to\nsignal propagation in transformers, including the interaction with positional\nencoding and causal masking. In experiments on WikiText-103 and C4, our\napproaches enable deep transformers without normalisation to train at speeds\nmatching their standard counterparts, and deep vanilla transformers to reach\nthe same performance as standard ones after about 5 times more iterations.", "published": "2023-02-20 21:26:25", "link": "http://arxiv.org/abs/2302.10322v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring the Limits of Transfer Learning with Unified Model in the\n  Cybersecurity Domain", "abstract": "With the increase in cybersecurity vulnerabilities of software systems, the\nways to exploit them are also increasing. Besides these, malware threats,\nirregular network interactions, and discussions about exploits in public forums\nare also on the rise. To identify these threats faster, to detect potentially\nrelevant entities from any texts, and to be aware of software vulnerabilities,\nautomated approaches are necessary. Application of natural language processing\n(NLP) techniques in the Cybersecurity domain can help in achieving this.\nHowever, there are challenges such as the diverse nature of texts involved in\nthe cybersecurity domain, the unavailability of large-scale publicly available\ndatasets, and the significant cost of hiring subject matter experts for\nannotations. One of the solutions is building multi-task models that can be\ntrained jointly with limited data. In this work, we introduce a generative\nmulti-task model, Unified Text-to-Text Cybersecurity (UTS), trained on malware\nreports, phishing site URLs, programming code constructs, social media data,\nblogs, news articles, and public forum posts. We show UTS improves the\nperformance of some cybersecurity datasets. We also show that with a few\nexamples, UTS can be adapted to novel unseen tasks and the nature of data", "published": "2023-02-20 22:21:26", "link": "http://arxiv.org/abs/2302.10346v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "A Sidecar Separator Can Convert a Single-Talker Speech Recognition\n  System to a Multi-Talker One", "abstract": "Although automatic speech recognition (ASR) can perform well in common\nnon-overlapping environments, sustaining performance in multi-talker\noverlapping speech recognition remains challenging. Recent research revealed\nthat ASR model's encoder captures different levels of information with\ndifferent layers -- the lower layers tend to have more acoustic information,\nand the upper layers more linguistic. This inspires us to develop a Sidecar\nseparator to empower a well-trained ASR model for multi-talker scenarios by\nseparating the mixed speech embedding between two suitable layers. We\nexperimented with a wav2vec 2.0-based ASR model with a Sidecar mounted. By\nfreezing the parameters of the original model and training only the Sidecar\n(8.7 M, 8.4% of all parameters), the proposed approach outperforms the previous\nstate-of-the-art by a large margin for the 2-speaker mixed LibriMix dataset,\nreaching a word error rate (WER) of 10.36%; and obtains comparable results\n(7.56%) for LibriSpeechMix dataset when limited training.", "published": "2023-02-20 11:09:37", "link": "http://arxiv.org/abs/2302.09908v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChatGPT for Robotics: Design Principles and Model Abilities", "abstract": "This paper presents an experimental study regarding the use of OpenAI's\nChatGPT for robotics applications. We outline a strategy that combines design\nprinciples for prompt engineering and the creation of a high-level function\nlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,\nand form factors. We focus our evaluations on the effectiveness of different\nprompt engineering techniques and dialog strategies towards the execution of\nvarious types of robotics tasks. We explore ChatGPT's ability to use free-form\ndialog, parse XML tags, and to synthesize code, in addition to the use of\ntask-specific prompting functions and closed-loop reasoning through dialogues.\nOur study encompasses a range of tasks within the robotics domain, from basic\nlogical, geometrical, and mathematical reasoning all the way to complex domains\nsuch as aerial navigation, manipulation, and embodied agents. We show that\nChatGPT can be effective at solving several of such tasks, while allowing users\nto interact with it primarily via natural language instructions. In addition to\nthese studies, we introduce an open-sourced research tool called PromptCraft,\nwhich contains a platform where researchers can collaboratively upload and vote\non examples of good prompting schemes for robotics applications, as well as a\nsample robotics simulator with ChatGPT integration, making it easier for users\nto get started with using ChatGPT for robotics.", "published": "2023-02-20 06:39:06", "link": "http://arxiv.org/abs/2306.17582v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "An ASR-free Fluency Scoring Approach with Self-Supervised Learning", "abstract": "A typical fluency scoring system generally relies on an automatic speech\nrecognition (ASR) system to obtain time stamps in input speech for either the\nsubsequent calculation of fluency-related features or directly modeling speech\nfluency with an end-to-end approach. This paper describes a novel ASR-free\napproach for automatic fluency assessment using self-supervised learning (SSL).\nSpecifically, wav2vec2.0 is used to extract frame-level speech features,\nfollowed by K-means clustering to assign a pseudo label (cluster index) to each\nframe. A BLSTM-based model is trained to predict an utterance-level fluency\nscore from frame-level SSL features and the corresponding cluster indexes.\nNeither speech transcription nor time stamp information is required in the\nproposed system. It is ASR-free and can potentially avoid the ASR errors effect\nin practice. Experimental results carried out on non-native English databases\nshow that the proposed approach significantly improves the performance in the\n\"open response\" scenario as compared to previous methods and matches the\nrecently reported performance in the \"read aloud\" scenario.", "published": "2023-02-20 11:50:40", "link": "http://arxiv.org/abs/2302.09928v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learnable Frontends that do not Learn: Quantifying Sensitivity to\n  Filterbank Initialisation", "abstract": "While much of modern speech and audio processing relies on deep neural\nnetworks trained using fixed audio representations, recent studies suggest\ngreat potential in acoustic frontends learnt jointly with a backend. In this\nstudy, we focus specifically on learnable filterbanks. Prior studies have\nreported that in frontends using learnable filterbanks initialised to a mel\nscale, the learned filters do not differ substantially from their\ninitialisation. Using a Gabor-based filterbank, we investigate the sensitivity\nof a learnable filterbank to its initialisation using several initialisation\nstrategies on two audio tasks: voice activity detection and bird species\nidentification. We use the Jensen-Shannon Distance and analysis of the learned\nfilters before and after training. We show that although performance is overall\nimproved, the filterbanks exhibit strong sensitivity to their initialisation\nstrategy. The limited movement from initialised values suggests that alternate\noptimisation strategies may allow a learnable frontend to reach better overall\nperformance.", "published": "2023-02-20 14:43:28", "link": "http://arxiv.org/abs/2302.10014v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Synergy between human and machine approaches to sound/scene recognition\n  and processing: An overview of ICASSP special session", "abstract": "Machine Listening, as usually formalized, attempts to perform a task that is,\nfrom our perspective, fundamentally human-performable, and performed by humans.\nCurrent automated models of Machine Listening vary from purely data-driven\napproaches to approaches imitating human systems. In recent years, the most\npromising approaches have been hybrid in that they have used data-driven\napproaches informed by models of the perceptual, cognitive, and semantic\nprocesses of the human system. Not only does the guidance provided by models of\nhuman perception and domain knowledge enable better, and more generalizable\nMachine Listening, in the converse, the lessons learned from these models may\nbe used to verify or improve our models of human perception themselves. This\npaper summarizes advances in the development of such hybrid approaches, ranging\nfrom Machine Listening models that are informed by models of peripheral (human)\nauditory processes, to those that employ or derive semantic information encoded\nin relations between sounds. The research described herein was presented in a\nspecial session on \"Synergy between human and machine approaches to sound/scene\nrecognition and processing\" at the 2023 ICASSP meeting.", "published": "2023-02-20 02:09:11", "link": "http://arxiv.org/abs/2302.09719v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized speech enhancement combining band-split RNN and speaker\n  attentive module", "abstract": "Target speaker information can be utilized in speech enhancement (SE) models\nto more effectively extract the desired speech. Previous works introduce the\nspeaker embedding into speech enhancement models by means of concatenation or\naffine transformation. In this paper, we propose a speaker attentive module to\ncalculate the attention scores between the speaker embedding and the\nintermediate features, which are used to rescale the features. By merging this\nmodule in the state-of-the-art SE model, we construct the personalized SE model\nfor ICASSP Signal Processing Grand Challenge: DNS Challenge 5 (2023). Our\nsystem achieves a final score of 0.529 on the blind test set of track1 and\n0.549 on track2.", "published": "2023-02-20 12:41:00", "link": "http://arxiv.org/abs/2302.09953v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speech Enhancement via Event-based Query", "abstract": "Existing deep learning based speech enhancement (SE) methods either use blind\nend-to-end training or explicitly incorporate speaker embedding or phonetic\ninformation into the SE network to enhance speech quality. In this paper, we\nperceive speech and noises as different types of sound events and propose an\nevent-based query method for SE. Specifically, representative speech embeddings\nthat can discriminate speech with noises are first pre-trained with the sound\nevent detection (SED) task. The embeddings are then clustered into fixed golden\nspeech queries to assist the SE network to enhance the speech from noisy audio.\nThe golden speech queries can be obtained offline and generalizable to\ndifferent SE datasets and networks. Therefore, little extra complexity is\nintroduced and no enrollment is needed for each speaker. Experimental results\nshow that the proposed method yields significant gains compared with baselines\nand the golden queries are well generalized to different datasets.", "published": "2023-02-20 05:23:06", "link": "http://arxiv.org/abs/2302.11558v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A DNN based Normalized Time-frequency Weighted Criterion for Robust\n  Wideband DoA Estimation", "abstract": "Deep neural networks (DNNs) have greatly benefited direction of arrival (DoA)\nestimation methods for speech source localization in noisy environments.\nHowever, their localization accuracy is still far from satisfactory due to the\nvulnerability to nonspeech interference. To improve the robustness against\ninterference, we propose a DNN based normalized time-frequency (T-F) weighted\ncriterion which minimizes the distance between the candidate steering vectors\nand the filtered snapshots in the T-F domain. Our method requires no\neigendecomposition and uses a simple normalization to prevent the optimization\nobjective from being misled by noisy filtered snapshots. We also study\ndifferent designs of T-F weights guided by a DNN. We find that duplicating the\nHadamard product of speech ratio masks is highly effective and better than\nother techniques such as direct masking and taking the mean in the proposed\napproach. However, the best-performing design of T-F weights is\ncriterion-dependent in general. Experiments show that the proposed method\noutperforms popular DNN based DoA estimation methods including widely used\nsubspace methods in noisy and reverberant environments.", "published": "2023-02-20 18:26:52", "link": "http://arxiv.org/abs/2302.10147v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "VoxSRC 2022: The Fourth VoxCeleb Speaker Recognition Challenge", "abstract": "This paper summarises the findings from the VoxCeleb Speaker Recognition\nChallenge 2022 (VoxSRC-22), which was held in conjunction with INTERSPEECH\n2022. The goal of this challenge was to evaluate how well state-of-the-art\nspeaker recognition systems can diarise and recognise speakers from speech\nobtained \"in the wild\". The challenge consisted of: (i) the provision of\npublicly available speaker recognition and diarisation data from YouTube videos\ntogether with ground truth annotation and standardised evaluation software; and\n(ii) a public challenge and hybrid workshop held at INTERSPEECH 2022. We\ndescribe the four tracks of our challenge along with the baselines, methods,\nand results. We conclude with a discussion on the new domain-transfer focus of\nVoxSRC-22, and on the progression of the challenge from the previous three\neditions.", "published": "2023-02-20 19:27:14", "link": "http://arxiv.org/abs/2302.10248v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-Time Speech Enhancement Using Spectral Subtraction with Minimum\n  Statistics and Spectral Floor", "abstract": "An initial real-time speech enhancement method is presented to reduce the\neffects of additive noise. The method operates in the frequency domain and is a\nform of spectral subtraction. Initially, minimum statistics are used to\ngenerate an estimate of the noise signal in the frequency domain. The use of\nminimum statistics avoids the need for a voice activity detector (VAD) which\nhas proven to be challenging to create. As minimum statistics are used, the\nnoise signal estimate must be multiplied by a scaling factor before subtraction\nfrom the noise corrupted speech signal can take place. A spectral floor is\napplied to the difference to suppress the effects of \"musical noise\". Finally,\na series of further enhancements are considered to reduce the effects of\nresidual noise even further. These methods are compared using time-frequency\nplots to create the final speech enhancement design", "published": "2023-02-20 20:55:53", "link": "http://arxiv.org/abs/2302.10313v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "pykanto: a python library to accelerate research on wild bird song", "abstract": "Studying the vocalisations of wild animals can be a challenge due to the\nlimitations of traditional computational methods, which often are\ntime-consuming and lack reproducibility. Here, I present pykanto, a new\nsoftware package that provides a set of tools to build, manage, and explore\nlarge sound databases. It can automatically find discrete units in animal\nvocalisations, perform semi-supervised labelling of individual repertoires with\na new interactive web app, and feed data to deep learning models to study\nthings like individual signatures and acoustic similarity between individuals\nand populations. To demonstrate its capabilities, I put the library to the test\non the vocalisations of male great tits in Wytham Woods, near Oxford, UK. The\nresults show that the identities of individual birds can be accurately\ndetermined from their songs and that the use of pykanto improves the efficiency\nand reproducibility of the process.", "published": "2023-02-20 22:05:47", "link": "http://arxiv.org/abs/2302.10340v1", "categories": ["cs.SD", "eess.AS", "q-bio.PE", "q-bio.QM", "D.2.2; D.2.13; J.3"], "primary_category": "cs.SD"}
{"title": "Federated Learning for ASR based on Wav2vec 2.0", "abstract": "This paper presents a study on the use of federated learning to train an ASR\nmodel based on a wav2vec 2.0 model pre-trained by self supervision. Carried out\non the well-known TED-LIUM 3 dataset, our experiments show that such a model\ncan obtain, with no use of a language model, a word error rate of 10.92% on the\nofficial TED-LIUM 3 test set, without sharing any data from the different\nusers. We also analyse the ASR performance for speakers depending to their\nparticipation to the federated learning. Since federated learning was first\nintroduced for privacy purposes, we also measure its ability to protect speaker\nidentity. To do that, we exploit an approach to analyze information contained\nin exchanged models based on a neural network footprint on an indicator\ndataset. This analysis is made layer-wise and shows which layers in an\nexchanged wav2vec 2.0 based model bring the speaker identity information.", "published": "2023-02-20 18:36:46", "link": "http://arxiv.org/abs/2302.10790v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
