{"title": "Investigating the Reordering Capability in CTC-based Non-Autoregressive\n  End-to-End Speech Translation", "abstract": "We study the possibilities of building a non-autoregressive speech-to-text\ntranslation model using connectionist temporal classification (CTC), and use\nCTC-based automatic speech recognition as an auxiliary task to improve the\nperformance. CTC's success on translation is counter-intuitive due to its\nmonotonicity assumption, so we analyze its reordering capability. Kendall's tau\ndistance is introduced as the quantitative metric, and gradient-based\nvisualization provides an intuitive way to take a closer look into the model.\nOur analysis shows that transformer encoders have the ability to change the\nword order and points out the future research direction that worth being\nexplored more on non-autoregressive speech translation.", "published": "2021-05-11 07:48:45", "link": "http://arxiv.org/abs/2105.04840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You Traducir This? Machine Translation for Code-Switched Input", "abstract": "Code-Switching (CSW) is a common phenomenon that occurs in multilingual\ngeographic or social contexts, which raises challenging problems for natural\nlanguage processing tools. We focus here on Machine Translation (MT) of CSW\ntexts, where we aim to simultaneously disentangle and translate the two mixed\nlanguages. Due to the lack of actual translated CSW data, we generate\nartificial training data from regular parallel texts. Experiments show this\ntraining strategy yields MT systems that surpass multilingual systems for\ncode-switched texts. These results are confirmed in an alternative task aimed\nat providing contextual translations for a L2 writing assistant.", "published": "2021-05-11 08:06:30", "link": "http://arxiv.org/abs/2105.04846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role of Artificial Intelligence in Detection of Hateful Speech for\n  Hinglish Data on Social Media", "abstract": "Social networking platforms provide a conduit to disseminate our ideas, views\nand thoughts and proliferate information. This has led to the amalgamation of\nEnglish with natively spoken languages. Prevalence of Hindi-English code-mixed\ndata (Hinglish) is on the rise with most of the urban population all over the\nworld. Hate speech detection algorithms deployed by most social networking\nplatforms are unable to filter out offensive and abusive content posted in\nthese code-mixed languages. Thus, the worldwide hate speech detection rate of\naround 44% drops even more considering the content in Indian colloquial\nlanguages and slangs. In this paper, we propose a methodology for efficient\ndetection of unstructured code-mix Hinglish language. Fine-tuning based\napproaches for Hindi-English code-mixed language are employed by utilizing\ncontextual based embeddings such as ELMo (Embeddings for Language Models),\nFLAIR, and transformer-based BERT (Bidirectional Encoder Representations from\nTransformers). Our proposed approach is compared against the pre-existing\nmethods and results are compared for various datasets. Our model outperforms\nthe other methods and frameworks.", "published": "2021-05-11 10:02:28", "link": "http://arxiv.org/abs/2105.04913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing an Automatic Agent for Repeated Language based Persuasion\n  Games", "abstract": "Persuasion games are fundamental in economics and AI research and serve as\nthe basis for important applications. However, work on this setup assumes\ncommunication with stylized messages that do not consist of rich human\nlanguage. In this paper we consider a repeated sender (expert) -- receiver\n(decision maker) game, where the sender is fully informed about the state of\nthe world and aims to persuade the receiver to accept a deal by sending one of\nseveral possible natural language reviews. We design an automatic expert that\nplays this repeated game, aiming to achieve the maximal payoff. Our expert is\nimplemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep\nlearning models that exploit behavioral and linguistic signals in order to\npredict the next action of the decision maker, and the future payoff of the\nexpert given the state of the game and a candidate review. We demonstrate the\nsuperiority of our expert over strong baselines, its adaptability to different\ndecision makers, and that its selected reviews are nicely adapted to the\nproposed deal.", "published": "2021-05-11 12:25:57", "link": "http://arxiv.org/abs/2105.04976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards transparency in NLP shared tasks", "abstract": "This article reports on a survey carried out across the Natural Language\nProcessing (NLP) community. The survey aimed to capture the opinions of the\nresearch community on issues surrounding shared tasks, with respect to both\nparticipation and organisation. Amongst the 175 responses received, both\npositive and negative observations were made. We carried out and report on an\nextensive analysis of these responses, which leads us to propose a Shared Task\nOrganisation Checklist that could support future participants and organisers.\nThe proposed Checklist is flexible enough to accommodate the wide diversity of\nshared tasks in our field and its goal is not to be prescriptive, but rather to\nserve as a tool that encourages shared task organisers to foreground ethical\nbehaviour, beginning with the common issues that the 175 respondents deemed\nimportant. Its usage would not only serve as an instrument to reflect on\nimportant aspects of shared tasks, but would also promote increased\ntransparency around them.", "published": "2021-05-11 13:29:35", "link": "http://arxiv.org/abs/2105.05020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Using Diachronic Distributed Word Representations as Models of\n  Lexical Development", "abstract": "Recent work has shown that distributed word representations can encode\nabstract information from child-directed speech. In this paper, we use\ndiachronic distributed word representations to perform temporal modeling and\nanalysis of lexical development in children. Unlike all previous work, we use\ntemporally sliced corpus to learn distributed word representations of\nchild-speech and child-directed speech under a curriculum-learning setting. In\nour experiments, we perform a lexical categorization task to plot the semantic\nand syntactic knowledge acquisition trajectories in children. Next, we perform\nlinear mixed-effects modeling over the diachronic representational changes to\nstudy the role of input word frequencies in the rate of word acquisition in\nchildren. We also perform a fine-grained analysis of lexical knowledge transfer\nfrom adults to children using Representational Similarity Analysis. Finally, we\nperform a qualitative analysis of the diachronic representations from our\nmodel, which reveals the grounding and word associations in the mental lexicon\nof children. Our experiments demonstrate the ease of usage and effectiveness of\ndiachronic distributed word representations in modeling lexical development.", "published": "2021-05-11 14:44:05", "link": "http://arxiv.org/abs/2105.05091v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Restoring Hebrew Diacritics Without a Dictionary", "abstract": "We demonstrate that it is feasible to diacritize Hebrew script without any\nhuman-curated resources other than plain diacritized text. We present NAKDIMON,\na two-layer character level LSTM, that performs on par with much more\ncomplicated curation-dependent systems, across a diverse array of modern Hebrew\nsources.", "published": "2021-05-11 17:23:29", "link": "http://arxiv.org/abs/2105.05209v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EL-Attention: Memory Efficient Lossless Attention for Generation", "abstract": "Transformer model with multi-head attention requires caching intermediate\nresults for efficient inference in generation tasks. However, cache brings new\nmemory-related costs and prevents leveraging larger batch size for faster\nspeed. We propose memory-efficient lossless attention (called EL-attention) to\naddress this issue. It avoids heavy operations for building multi-head keys and\nvalues, cache for them is not needed. EL-attention constructs an ensemble of\nattention results by expanding query while keeping key and value shared. It\nproduces the same result as multi-head attention with less GPU memory and\nfaster inference speed. We conduct extensive experiments on Transformer, BART,\nand GPT-2 for summarization and question generation tasks. The results show\nEL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.", "published": "2021-05-11 04:37:52", "link": "http://arxiv.org/abs/2105.04779v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Generative Augmentation for Visual Question Answering", "abstract": "Data augmentation has been shown to effectively improve the performance of\nmultimodal machine learning models. This paper introduces a generative model\nfor data augmentation by leveraging the correlations among multiple modalities.\nDifferent from conventional data augmentation approaches that apply low-level\noperations with deterministic heuristics, our method learns a generator that\ngenerates samples of the target modality conditioned on observed modalities in\nthe variational auto-encoder framework. Additionally, the proposed model is\nable to quantify the confidence of augmented data by its generative\nprobability, and can be jointly optimised with a downstream task. Experiments\non Visual Question Answering as downstream task demonstrate the effectiveness\nof the proposed generative model, which is able to improve strong UpDn-based\nmodels to achieve state-of-the-art performance.", "published": "2021-05-11 04:51:26", "link": "http://arxiv.org/abs/2105.04780v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial\n  Algebras: Turning Verification Witnesses Upside Down", "abstract": "The coincidence between initial algebras (IAs) and final coalgebras (FCs) is\na phenomenon that underpins various important results in theoretical computer\nscience. In this paper, we identify a general fibrational condition for the\nIA-FC coincidence, namely in the fiber over an initial algebra in the base\ncategory. Identifying (co)algebras in a fiber as (co)inductive predicates, our\nfibrational IA-FC coincidence allows one to use coinductive witnesses (such as\ninvariants) for verifying inductive properties (such as liveness). Our general\nfibrational theory features the technical condition of stability of chain\ncolimits; we extend the framework to the presence of a monadic effect, too,\nrestricting to fibrations of complete lattice-valued predicates. Practical\nbenefits of our categorical theory are exemplified by new \"upside-down\" witness\nnotions for three verification problems: probabilistic liveness, and acceptance\nand model-checking with respect to bottom-up tree automata.", "published": "2021-05-11 07:10:13", "link": "http://arxiv.org/abs/2105.04817v3", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Rationalization through Concepts", "abstract": "Automated predictions require explanations to be interpretable by humans. One\ntype of explanation is a rationale, i.e., a selection of input features such as\nrelevant text snippets from which the model computes the outcome. However, a\nsingle overall selection does not provide a complete explanation, e.g.,\nweighing several aspects for decisions. To this end, we present a novel\nself-interpretable model called ConRAT. Inspired by how human explanations for\nhigh-level decisions are often based on key concepts, ConRAT extracts a set of\ntext snippets as concepts and infers which ones are described in the document.\nThen, it explains the outcome with a linear aggregation of concepts. Two\nregularizers drive ConRAT to build interpretable concepts. In addition, we\npropose two techniques to boost the rationale and predictive performance\nfurther. Experiments on both single- and multi-aspect sentiment classification\ntasks show that ConRAT is the first to generate concepts that align with human\nrationalization while using only the overall label. Further, it outperforms\nstate-of-the-art methods trained on each aspect label independently.", "published": "2021-05-11 07:46:48", "link": "http://arxiv.org/abs/2105.04837v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning from Reformulations in Conversational Question\n  Answering over Knowledge Graphs", "abstract": "The rise of personal assistants has made conversational question answering\n(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art\nmethods for ConvQA over knowledge graphs (KGs) can only learn from crisp\nquestion-answer pairs found in popular benchmarks. In reality, however, such\ntraining data is hard to come by: users would rarely mark answers explicitly as\ncorrect or wrong. In this work, we take a step towards a more natural learning\nparadigm - from noisy and implicit feedback via question reformulations. A\nreformulation is likely to be triggered by an incorrect system response,\nwhereas a new follow-up question could be a positive signal on the previous\nturn's answer. We present a reinforcement learning model, termed CONQUER, that\ncan learn from a conversational stream of questions and reformulations. CONQUER\nmodels the answering process as multiple agents walking in parallel on the KG,\nwhere the walks are determined by actions sampled using a policy network. This\npolicy network takes the question along with the conversational context as\ninputs and is trained via noisy rewards obtained from the reformulation\nlikelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark\nwith about 11k natural conversations containing around 205k reformulations.\nExperiments show that CONQUER successfully learns to answer conversational\nquestions from noisy reward signals, significantly improving over a\nstate-of-the-art baseline.", "published": "2021-05-11 08:08:35", "link": "http://arxiv.org/abs/2105.04850v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Conversational Entity Linking: Problem Definition and Datasets", "abstract": "Machine understanding of user utterances in conversational systems is of\nutmost importance for enabling engaging and meaningful conversations with\nusers. Entity Linking (EL) is one of the means of text understanding, with\nproven efficacy for various downstream tasks in information retrieval. In this\npaper, we study entity linking for conversational systems. To develop a better\nunderstanding of what EL in a conversational setting entails, we analyze a\nlarge number of dialogues from existing conversational datasets and annotate\nreferences to concepts, named entities, and personal entities using\ncrowdsourcing. Based on the annotated dialogues, we identify the main\ncharacteristics of conversational entity linking. Further, we report on the\nperformance of traditional EL systems on our Conversational Entity Linking\ndataset, ConEL, and present an extension to these methods to better fit the\nconversational setting. The resources released with this paper include\nannotated datasets, detailed descriptions of crowdsourcing setups, as well as\nthe annotations produced by various EL systems. These new resources allow for\nan investigation of how the role of entities in conversations is different from\nthat in documents or isolated short text utterances like queries and tweets,\nand complement existing conversational datasets.", "published": "2021-05-11 09:44:27", "link": "http://arxiv.org/abs/2105.04903v1", "categories": ["cs.CL", "cs.IR", "H.3"], "primary_category": "cs.CL"}
{"title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models\n  Identify Analogies?", "abstract": "Analogies play a central role in human commonsense reasoning. The ability to\nrecognize analogies such as \"eye is to seeing what ear is to hearing\",\nsometimes referred to as analogical proportions, shape how we structure\nknowledge and understand language. Surprisingly, however, the task of\nidentifying such analogies has not yet received much attention in the language\nmodel era. In this paper, we analyze the capabilities of transformer-based\nlanguage models on this unsupervised task, using benchmarks obtained from\neducational settings, as well as more commonly used datasets. We find that\noff-the-shelf language models can identify analogies to a certain extent, but\nstruggle with abstract and complex relations, and results are highly sensitive\nto model architecture and hyperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while configurations using BERT were not able\nto outperform word embedding models. Our results raise important questions for\nfuture work about how, and to what extent, pre-trained language models capture\nknowledge about abstract semantic relations.", "published": "2021-05-11 11:38:49", "link": "http://arxiv.org/abs/2105.04949v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backretrieval: An Image-Pivoted Evaluation Metric for Cross-Lingual Text\n  Representations Without Parallel Corpora", "abstract": "Cross-lingual text representations have gained popularity lately and act as\nthe backbone of many tasks such as unsupervised machine translation and\ncross-lingual information retrieval, to name a few. However, evaluation of such\nrepresentations is difficult in the domains beyond standard benchmarks due to\nthe necessity of obtaining domain-specific parallel language data across\ndifferent pairs of languages. In this paper, we propose an automatic metric for\nevaluating the quality of cross-lingual textual representations using images as\na proxy in a paired image-text evaluation dataset. Experimentally,\nBackretrieval is shown to highly correlate with ground truth metrics on\nannotated datasets, and our analysis shows statistically significant\nimprovements over baselines. Our experiments conclude with a case study on a\nrecipe dataset without parallel cross-lingual data. We illustrate how to judge\ncross-lingual embedding quality with Backretrieval, and validate the outcome\nwith a small human study.", "published": "2021-05-11 12:14:24", "link": "http://arxiv.org/abs/2105.04971v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Joint Text and Label Generation for Spoken Language Understanding", "abstract": "Generalization is a central problem in machine learning, especially when data\nis limited. Using prior information to enforce constraints is the principled\nway of encouraging generalization. In this work, we propose to leverage the\nprior information embedded in pretrained language models (LM) to improve\ngeneralization for intent classification and slot labeling tasks with limited\ntraining data. Specifically, we extract prior knowledge from pretrained LM in\nthe form of synthetic data, which encode the prior implicitly. We fine-tune the\nLM to generate an augmented language, which contains not only text but also\nencodes both intent labels and slot labels. The generated synthetic data can be\nused to train a classifier later. Since the generated data may contain noise,\nwe rephrase the learning from generated data as learning with noisy labels. We\nthen utilize the mixout regularization for the classifier and prove its\neffectiveness to resist label noise in generated data. Empirically, our method\ndemonstrates superior performance and outperforms the baseline by a large\nmargin.", "published": "2021-05-11 14:02:06", "link": "http://arxiv.org/abs/2105.05052v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Generalization using Intrinsically Motivated Compositional\n  Emergent Protocols", "abstract": "Human language has been described as a system that makes \\textit{use of\nfinite means to express an unlimited array of thoughts}. Of particular interest\nis the aspect of compositionality, whereby, the meaning of a compound language\nexpression can be deduced from the meaning of its constituent parts. If\nartificial agents can develop compositional communication protocols akin to\nhuman language, they can be made to seamlessly generalize to unseen\ncombinations. Studies have recognized the role of curiosity in enabling\nlinguistic development in children. In this paper, we seek to use this\nintrinsic feedback in inducing a systematic and unambiguous protolanguage. We\ndemonstrate how compositionality can enable agents to not only interact with\nunseen objects but also transfer skills from one task to another in a zero-shot\nsetting: \\textit{Can an agent, trained to `pull' and `push twice', `pull\ntwice'?}.", "published": "2021-05-11 14:20:26", "link": "http://arxiv.org/abs/2105.05069v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Integrating extracted information from bert and multiple embedding\n  methods with the deep neural network for humour detection", "abstract": "Humour detection from sentences has been an interesting and challenging task\nin the last few years. In attempts to highlight humour detection, most research\nwas conducted using traditional approaches of embedding, e.g., Word2Vec or\nGlove. Recently BERT sentence embedding has also been used for this task. In\nthis paper, we propose a framework for humour detection in short texts taken\nfrom news headlines. Our proposed framework (IBEN) attempts to extract\ninformation from written text via the use of different layers of BERT. After\nseveral trials, weights were assigned to different layers of the BERT model.\nThe extracted information was then sent to a Bi-GRU neural network as an\nembedding matrix. We utilized the properties of some external embedding models.\nA multi-kernel convolution in our neural network was also employed to extract\nhigher-level sentence representations. This framework performed very well on\nthe task of humour detection.", "published": "2021-05-11 15:09:19", "link": "http://arxiv.org/abs/2105.05112v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "kdehumor at semeval-2020 task 7: a neural network model for detecting\n  funniness in dataset humicroedit", "abstract": "This paper describes our contribution to SemEval-2020 Task 7: Assessing Humor\nin Edited News Headlines. Here we present a method based on a deep neural\nnetwork. In recent years, quite some attention has been devoted to humor\nproduction and perception. Our team KdeHumor employs recurrent neural network\nmodels including Bi-Directional LSTMs (BiLSTMs). Moreover, we utilize the\nstate-of-the-art pre-trained sentence embedding techniques. We analyze the\nperformance of our method and demonstrate the contribution of each component of\nour architecture.", "published": "2021-05-11 15:44:03", "link": "http://arxiv.org/abs/2105.05135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Summary Loop: Learning to Write Abstractive Summaries Without\n  Examples", "abstract": "This work presents a new approach to unsupervised abstractive summarization\nbased on maximizing a combination of coverage and fluency for a given length\nconstraint. It introduces a novel method that encourages the inclusion of key\nterms from the original document into the summary: key terms are masked out of\nthe original document and must be filled in by a coverage model using the\ncurrent generated summary. A novel unsupervised training procedure leverages\nthis coverage model along with a fluency model to generate and score summaries.\nWhen tested on popular news summarization datasets, the method outperforms\nprevious unsupervised methods by more than 2 R-1 points, and approaches results\nof competitive supervised methods. Our model attains higher levels of\nabstraction with copied passages roughly two times shorter than prior work, and\nlearns to compress and merge sentences without supervision.", "published": "2021-05-11 23:19:46", "link": "http://arxiv.org/abs/2105.05361v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards an Online Empathetic Chatbot with Emotion Causes", "abstract": "Existing emotion-aware conversational models usually focus on controlling the\nresponse contents to align with a specific emotion class, whereas empathy is\nthe ability to understand and concern the feelings and experience of others.\nHence, it is critical to learn the causes that evoke the users' emotion for\nempathetic responding, a.k.a. emotion causes. To gather emotion causes in\nonline environments, we leverage counseling strategies and develop an\nempathetic chatbot to utilize the causal emotion information. On a real-world\nonline dataset, we verify the effectiveness of the proposed approach by\ncomparing our chatbot with several SOTA methods using automatic metrics,\nexpert-based human judgements as well as user-based online evaluation.", "published": "2021-05-11 02:52:46", "link": "http://arxiv.org/abs/2105.11903v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Separate but Together: Unsupervised Federated Learning for Speech\n  Enhancement from Non-IID Data", "abstract": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.", "published": "2021-05-11 00:47:18", "link": "http://arxiv.org/abs/2105.04727v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Ask Appropriate Questions in Conversational Recommendation", "abstract": "Conversational recommender systems (CRSs) have revolutionized the\nconventional recommendation paradigm by embracing dialogue agents to\ndynamically capture the fine-grained user preference. In a typical\nconversational recommendation scenario, a CRS firstly generates questions to\nlet the user clarify her/his demands and then makes suitable recommendations.\nHence, the ability to generate suitable clarifying questions is the key to\ntimely tracing users' dynamic preferences and achieving successful\nrecommendations. However, existing CRSs fall short in asking high-quality\nquestions because: (1) system-generated responses heavily depends on the\nperformance of the dialogue policy agent, which has to be trained with huge\nconversation corpus to cover all circumstances; and (2) current CRSs cannot\nfully utilize the learned latent user profiles for generating appropriate and\npersonalized responses.\n  To mitigate these issues, we propose the Knowledge-Based Question Generation\nSystem (KBQG), a novel framework for conversational recommendation. Distinct\nfrom previous conversational recommender systems, KBQG models a user's\npreference in a finer granularity by identifying the most relevant relations\nfrom a structured knowledge graph (KG). Conditioned on the varied importance of\ndifferent relations, the generated clarifying questions could perform better in\nimpelling users to provide more details on their preferences. Finially,\naccurate recommendations can be generated in fewer conversational turns.\nFurthermore, the proposed KBQG outperforms all baselines in our experiments on\ntwo real-world datasets.", "published": "2021-05-11 03:58:10", "link": "http://arxiv.org/abs/2105.04774v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T07, 68T30, 68T37", "H.4"], "primary_category": "cs.AI"}
{"title": "Benchmarking down-scaled (not so large) pre-trained language models", "abstract": "Large Transformer-based language models are pre-trained on corpora of varying\nsizes, for a different number of steps and with different batch sizes. At the\nsame time, more fundamental components, such as the pre-training objective or\narchitectural hyperparameters, are modified. In total, it is therefore\ndifficult to ascribe changes in performance to specific factors. Since\nsearching the hyperparameter space over the full systems is too costly, we\npre-train down-scaled versions of several popular Transformer-based\narchitectures on a common pre-training corpus and benchmark them on a subset of\nthe GLUE tasks (Wang et al., 2018). Specifically, we systematically compare\nthree pre-training objectives for different shape parameters and model sizes,\nwhile also varying the number of pre-training steps and the batch size. In our\nexperiments MLM + NSP (BERT-style) consistently outperforms MLM (RoBERTa-style)\nas well as the standard LM objective. Furthermore, we find that additional\ncompute should be mainly allocated to an increased model size, while training\nfor more steps is inefficient. Based on these observations, as a final step we\nattempt to scale up several systems using compound scaling (Tan and Le, 2019)\nadapted to Transformer-based language models.", "published": "2021-05-11 09:01:04", "link": "http://arxiv.org/abs/2105.04876v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Forecasting consumer confidence through semantic network analysis of\n  online news", "abstract": "This research studies the impact of online news on social and economic\nconsumer perceptions through semantic network analysis. Using over 1.8 million\nonline articles on Italian media covering four years, we calculate the semantic\nimportance of specific economic-related keywords to see if words appearing in\nthe articles could anticipate consumers' judgments about the economic situation\nand the Consumer Confidence Index. We use an innovative approach to analyze big\ntextual data, combining methods and tools of text mining and social network\nanalysis. Results show a strong predictive power for the judgments about the\ncurrent households and national situation. Our indicator offers a complementary\napproach to estimating consumer confidence, lessening the limitations of\ntraditional survey-based methods.", "published": "2021-05-11 09:41:25", "link": "http://arxiv.org/abs/2105.04900v2", "categories": ["econ.GN", "cs.CL", "cs.SI", "q-fin.EC", "J.4"], "primary_category": "econ.GN"}
{"title": "Including Signed Languages in Natural Language Processing", "abstract": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.", "published": "2021-05-11 17:37:55", "link": "http://arxiv.org/abs/2105.05222v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Doing Natural Language Processing in A Natural Way: An NLP toolkit based\n  on object-oriented knowledge base and multi-level grammar base", "abstract": "We introduce an NLP toolkit based on object-oriented knowledge base and\nmulti-level grammar base. This toolkit focuses on semantic parsing, it also has\nabilities to discover new knowledge and grammar automatically, new discovered\nknowledge and grammar will be identified by human, and will be used to update\nthe knowledge base and grammar base. This process can be iterated many times to\nimprove the toolkit continuously.", "published": "2021-05-11 17:43:06", "link": "http://arxiv.org/abs/2105.05227v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Addressing \"Documentation Debt\" in Machine Learning Research: A\n  Retrospective Datasheet for BookCorpus", "abstract": "Recent literature has underscored the importance of dataset documentation\nwork for machine learning, and part of this work involves addressing\n\"documentation debt\" for datasets that have been used widely but documented\nsparsely. This paper aims to help address documentation debt for BookCorpus, a\npopular text dataset for training large language models. Notably, researchers\nhave used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,\neven though little to no documentation exists about the dataset's motivation,\ncomposition, collection process, etc. We offer a preliminary datasheet that\nprovides key context and information about BookCorpus, highlighting several\nnotable deficiencies. In particular, we find evidence that (1) BookCorpus\nlikely violates copyright restrictions for many books, (2) BookCorpus contains\nthousands of duplicated books, and (3) BookCorpus exhibits significant skews in\ngenre representation. We also find hints of other potential deficiencies that\ncall for future research, including problematic content, potential skews in\nreligious representation, and lopsided author contributions. While more work\nremains, this initial effort to provide a datasheet for BookCorpus adds to\ngrowing literature that urges more careful and systematic documentation for\nmachine learning datasets.", "published": "2021-05-11 17:59:23", "link": "http://arxiv.org/abs/2105.05241v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Differentiable Signal Processing With Black-Box Audio Effects", "abstract": "We present a data-driven approach to automate audio signal processing by\nincorporating stateful third-party, audio effects as layers within a deep\nneural network. We then train a deep encoder to analyze input audio and control\neffect parameters to perform the desired signal manipulation, requiring only\ninput-target paired audio data as supervision. To train our network with\nnon-differentiable black-box effects layers, we use a fast, parallel stochastic\ngradient approximation scheme within a standard auto differentiation graph,\nyielding efficient end-to-end backpropagation. We demonstrate the power of our\napproach with three separate automatic audio production applications: tube\namplifier emulation, automatic removal of breaths and pops from voice\nrecordings, and automatic music mastering. We validate our results with a\nsubjective listening test, showing our approach not only can enable new\nautomatic audio effects tasks, but can yield results comparable to a\nspecialized, state-of-the-art commercial solution for music mastering.", "published": "2021-05-11 02:20:22", "link": "http://arxiv.org/abs/2105.04752v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Deep scattering network for speech emotion recognition", "abstract": "This paper introduces scattering transform for speech emotion recognition\n(SER). Scattering transform generates feature representations which remain\nstable to deformations and shifting in time and frequency without much loss of\ninformation. In speech, the emotion cues are spread across time and localised\nin frequency. The time and frequency invariance characteristic of scattering\ncoefficients provides a representation robust against emotion irrelevant\nvariations e.g., different speakers, language, gender etc. while preserving the\nvariations caused by emotion cues. Hence, such a representation captures the\nemotion information more efficiently from speech. We perform experiments to\ncompare scattering coefficients with standard mel-frequency cepstral\ncoefficients (MFCCs) over different databases. It is observed that frequency\nscattering performs better than time-domain scattering and MFCCs. We also\ninvestigate layer-wise scattering coefficients to analyse the importance of\ntime shift and deformation stable scalogram and modulation spectrum\ncoefficients for SER. We observe that layer-wise coefficients taken\nindependently also perform better than MFCCs.", "published": "2021-05-11 06:37:41", "link": "http://arxiv.org/abs/2105.04806v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The impact of the additional features on the performance of regression\n  analysis: a case study on regression analysis of music signal", "abstract": "Machine learning techniques nowadays play a vital role in many burning issues\nof real-world problems when it involves data. In addition, when the task is\ncomplex, people are in dilemma in choosing deep learning techniques or going\nwithout them. This paper is about whether we should always rely on deep\nlearning techniques or it is really possible to overcome the performance of\ndeep learning algorithms by simple statistical machine learning algorithms by\nunderstanding the application and processing the data so that it can help in\nincreasing the performance of the algorithm by a notable amount. The paper\nmentions the importance of data preprocessing than that of the selection of the\nalgorithm. It discusses the functions involving trigonometric, logarithmic, and\nexponential terms and also talks about functions that are purely trigonometric.\nFinally, we discuss regression analysis on music signals to justify our claim.", "published": "2021-05-11 06:40:15", "link": "http://arxiv.org/abs/2105.05938v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
