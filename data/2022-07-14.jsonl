{"title": "Overview of Abusive and Threatening Language Detection in Urdu at FIRE\n  2021", "abstract": "With the growth of social media platform influence, the effect of their\nmisuse becomes more and more impactful. The importance of automatic detection\nof threatening and abusive language can not be overestimated. However, most of\nthe existing studies and state-of-the-art methods focus on English as the\ntarget language, with limited work on low- and medium-resource languages. In\nthis paper, we present two shared tasks of abusive and threatening language\ndetection for the Urdu language which has more than 170 million speakers\nworldwide. Both are posed as binary classification tasks where participating\nsystems are required to classify tweets in Urdu into two classes, namely: (i)\nAbusive and Non-Abusive for the first task, and (ii) Threatening and\nNon-Threatening for the second. We present two manually annotated datasets\ncontaining tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening\nand Non-Threatening. The abusive dataset contains 2400 annotated tweets in the\ntrain part and 1100 annotated tweets in the test part. The threatening dataset\ncontains 6000 annotated tweets in the train part and 3950 annotated tweets in\nthe test part. We also provide logistic regression and BERT-based baseline\nclassifiers for both tasks. In this shared task, 21 teams from six countries\nregistered for participation (India, Pakistan, China, Malaysia, United Arab\nEmirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is\nAbusive Language Detection and 9 teams submitted their runs for Subtask B,\nwhich is Threatening Language detection, and seven teams submitted their\ntechnical reports. The best performing system achieved an F1-score value of\n0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based\ntransformer model showed the best performance.", "published": "2022-07-14 07:38:13", "link": "http://arxiv.org/abs/2207.06710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Terminology Management and Sharing Toolkit for Federation of\n  Terminology Databases", "abstract": "Consolidated access to current and reliable terms from different subject\nfields and languages is necessary for content creators and translators.\nTerminology is also needed in AI applications such as machine translation,\nspeech recognition, information extraction, and other natural language\nprocessing tools. In this work, we facilitate standards-based sharing and\nmanagement of terminology resources by providing an open terminology management\nsolution - the EuroTermBank Toolkit. It allows organisations to manage and\nsearch their terms, create term collections, and share them within and outside\nthe organisation by participating in the network of federated databases. The\ndata curated in the federated databases are automatically shared with\nEuroTermBank, the largest multilingual terminology resource in Europe, allowing\ntranslators and language service providers as well as researchers and students\nto access terminology resources in their most current version.", "published": "2022-07-14 08:27:17", "link": "http://arxiv.org/abs/2207.06729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Data-to-Text Generation Based on Small Datasets: Comparing the\n  Added Value of Two Semi-Supervised Learning Approaches on Top of a Large\n  Language Model", "abstract": "This study discusses the effect of semi-supervised learning in combination\nwith pretrained language models for data-to-text generation. It is not known\nwhether semi-supervised learning is still helpful when a large-scale language\nmodel is also supplemented. This study aims to answer this question by\ncomparing a data-to-text system only supplemented with a language model, to two\ndata-to-text systems that are additionally enriched by a data augmentation or a\npseudo-labeling semi-supervised learning approach.\n  Results show that semi-supervised learning results in higher scores on\ndiversity metrics. In terms of output quality, extending the training set of a\ndata-to-text system with a language model using the pseudo-labeling approach\ndid increase text quality scores, but the data augmentation approach yielded\nsimilar scores to the system without training set extension. These results\nindicate that semi-supervised learning approaches can bolster output quality\nand diversity, even when a language model is also present.", "published": "2022-07-14 11:53:04", "link": "http://arxiv.org/abs/2207.06839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beware the Rationalization Trap! When Language Model Explainability\n  Diverges from our Mental Models of Language", "abstract": "Language models learn and represent language differently than humans; they\nlearn the form and not the meaning. Thus, to assess the success of language\nmodel explainability, we need to consider the impact of its divergence from a\nuser's mental model of language. In this position paper, we argue that in order\nto avoid harmful rationalization and achieve truthful understanding of language\nmodels, explanation processes must satisfy three main conditions: (1)\nexplanations have to truthfully represent the model behavior, i.e., have a high\nfidelity; (2) explanations must be complete, as missing information distorts\nthe truth; and (3) explanations have to take the user's mental model into\naccount, progressively verifying a person's knowledge and adapting their\nunderstanding. We introduce a decision tree model to showcase potential reasons\nwhy current explanations fail to reach their objectives. We further emphasize\nthe need for human-centered design to explain the model from multiple\nperspectives, progressively adapting explanations to changing user\nexpectations.", "published": "2022-07-14 13:26:03", "link": "http://arxiv.org/abs/2207.06897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Session-based Cyberbullying Detection in Social Media: A Survey", "abstract": "Cyberbullying is a pervasive problem in online social media, where a bully\nabuses a victim through a social media session. By investigating cyberbullying\nperpetrated through social media sessions, recent research has looked into\nmining patterns and features for modeling and understanding the two defining\ncharacteristics of cyberbullying: repetitive behavior and power imbalance. In\nthis survey paper, we define the Session-based Cyberbullying Detection\nframework that encapsulates the different steps and challenges of the problem.\nBased on this framework, we provide a comprehensive overview of session-based\ncyberbullying detection in social media, delving into existing efforts from a\ndata and methodological perspective. Our review leads us to propose\nevidence-based criteria for a set of best practices to create session-based\ncyberbullying datasets. In addition, we perform benchmark experiments comparing\nthe performance of state-of-the-art session-based cyberbullying detection\nmodels as well as large pre-trained language models across two different\ndatasets. Through our review, we also put forth a set of open challenges as\nfuture research directions.", "published": "2022-07-14 18:56:54", "link": "http://arxiv.org/abs/2207.10639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A methodology to characterize bias and harmful stereotypes in natural\n  language processing in Latin America", "abstract": "Automated decision-making systems, especially those based on natural language\nprocessing, are pervasive in our lives. They are not only behind the internet\nsearch engines we use daily, but also take more critical roles: selecting\ncandidates for a job, determining suspects of a crime, diagnosing autism and\nmore. Such automated systems make errors, which may be harmful in many ways, be\nit because of the severity of the consequences (as in health issues) or because\nof the sheer number of people they affect. When errors made by an automated\nsystem affect a population more than others, we call the system\n\\textit{biased}.\n  Most modern natural language technologies are based on artifacts obtained\nfrom enormous volumes of text using machine learning, namely language models\nand word embeddings. Since they are created by applying subsymbolic machine\nlearning, mostly artificial neural networks, they are opaque and practically\nuninterpretable by direct inspection, thus making it very difficult to audit\nthem.\n  In this paper, we present a methodology that spells out how social\nscientists, domain experts, and machine learning experts can collaboratively\nexplore biases and harmful stereotypes in word embeddings and large language\nmodels. Our methodology is based on the following principles:\n  * focus on the linguistic manifestations of discrimination on word embeddings\nand language models, not on the mathematical properties of the models * reduce\nthe technical barrier for discrimination experts%, be it social scientists,\ndomain experts or other * characterize through a qualitative exploratory\nprocess in addition to a metric-based approach * address mitigation as part of\nthe training process, not as an afterthought", "published": "2022-07-14 01:07:55", "link": "http://arxiv.org/abs/2207.06591v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Layout-Aware Information Extraction for Document-Grounded Dialogue:\n  Dataset, Method and Demonstration", "abstract": "Building document-grounded dialogue systems have received growing interest as\ndocuments convey a wealth of human knowledge and commonly exist in enterprises.\nWherein, how to comprehend and retrieve information from documents is a\nchallenging research problem. Previous work ignores the visual property of\ndocuments and treats them as plain text, resulting in incomplete modality. In\nthis paper, we propose a Layout-aware document-level Information Extraction\ndataset, LIE, to facilitate the study of extracting both structural and\nsemantic knowledge from visually rich documents (VRDs), so as to generate\naccurate responses in dialogue systems. LIE contains 62k annotations of three\nextraction tasks from 4,061 pages in product and official documents, becoming\nthe largest VRD-based information extraction dataset to the best of our\nknowledge. We also develop benchmark methods that extend the token-based\nlanguage model to consider layout features like humans. Empirical results show\nthat layout is critical for VRD-based extraction, and system demonstration also\nverifies that the extracted knowledge can help locate the answers that users\ncare about.", "published": "2022-07-14 07:59:45", "link": "http://arxiv.org/abs/2207.06717v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform\n  Libraries", "abstract": "Discrete Fourier Transform (DFT) libraries are one of the most critical\nsoftware components for scientific computing. Inspired by FFTW, a widely used\nlibrary for DFT HPC calculations, we apply compiler technologies for the\ndevelopment of HPC Fourier transform libraries. In this work, we introduce\nFFTc, a domain-specific language, based on Multi-Level Intermediate\nRepresentation (MLIR), for expressing Fourier Transform algorithms. We present\nthe initial design, implementation, and preliminary results of FFTc.", "published": "2022-07-14 10:31:21", "link": "http://arxiv.org/abs/2207.06803v2", "categories": ["cs.MS", "cs.CL"], "primary_category": "cs.MS"}
{"title": "BERTIN: Efficient Pre-Training of a Spanish Language Model using\n  Perplexity Sampling", "abstract": "The pre-training of large language models usually requires massive amounts of\nresources, both in terms of computation and data. Frequently used web sources\nsuch as Common Crawl might contain enough noise to make this pre-training\nsub-optimal. In this work, we experiment with different sampling methods from\nthe Spanish version of mC4, and present a novel data-centric technique which we\nname $\\textit{perplexity sampling}$ that enables the pre-training of language\nmodels in roughly half the amount of steps and using one fifth of the data. The\nresulting models are comparable to the current state-of-the-art, and even\nachieve better results for certain tasks. Our work is proof of the versatility\nof Transformers, and paves the way for small teams to train their models on a\nlimited budget. Our models are available at this\n$\\href{https://huggingface.co/bertin-project}{URL}$.", "published": "2022-07-14 10:48:42", "link": "http://arxiv.org/abs/2207.06814v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recurrent Memory Transformer", "abstract": "Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n  In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (RMT). Memory allows to store and process local and global\ninformation as well as to pass information between segments of the long\nsequence with the help of recurrence.\n  We implement a memory mechanism with no changes to Transformer model by\nadding special memory tokens to the input or output sequence. Then the model is\ntrained to control both memory operations and sequence representations\nprocessing.\n  Results of experiments show that RMT performs on par with the Transformer-XL\non language modeling for smaller memory sizes and outperforms it for tasks that\nrequire longer sequence processing. We show that adding memory tokens to Tr-XL\nis able to improve its performance. This makes Recurrent Memory Transformer a\npromising architecture for applications that require learning of long-term\ndependencies and general purpose in memory processing, such as algorithmic\ntasks and reasoning.", "published": "2022-07-14 13:00:22", "link": "http://arxiv.org/abs/2207.06881v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scene Text Recognition with Permuted Autoregressive Sequence Models", "abstract": "Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.", "published": "2022-07-14 14:51:50", "link": "http://arxiv.org/abs/2207.06966v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to translate by learning to communicate", "abstract": "We formulate and test a technique to use Emergent Communication (EC) with a\npre-trained multilingual model to improve on modern Unsupervised NMT systems,\nespecially for low-resource languages. It has been argued that the current\ndominant paradigm in NLP of pre-training on text-only corpora will not yield\nrobust natural language understanding systems, and the need for grounded,\ngoal-oriented, and interactive language learning has been high lighted. In our\napproach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC\nimage-reference game, in which the model is incentivized to use multilingual\ngenerations to accomplish a vision-grounded task. The hypothesis is that this\nwill align multiple languages to a shared task space. We present two variants\nof EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a\nbacktranslation-only baseline in all four languages investigated, including the\nlow-resource language Nepali.", "published": "2022-07-14 15:58:06", "link": "http://arxiv.org/abs/2207.07025v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confident Adaptive Language Modeling", "abstract": "Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.", "published": "2022-07-14 17:00:19", "link": "http://arxiv.org/abs/2207.07061v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition in Conversation using Probabilistic Soft Logic", "abstract": "Creating agents that can both appropriately respond to conversations and\nunderstand complex human linguistic tendencies and social cues has been a long\nstanding challenge in the NLP community. A recent pillar of research revolves\naround emotion recognition in conversation (ERC); a sub-field of emotion\nrecognition that focuses on conversations or dialogues that contain two or more\nutterances. In this work, we explore an approach to ERC that exploits the use\nof neural embeddings along with complex structures in dialogues. We implement\nour approach in a framework called Probabilistic Soft Logic (PSL), a\ndeclarative templating language that uses first-order like logical rules, that\nwhen combined with data, define a particular class of graphical model.\nAdditionally, PSL provides functionality for the incorporation of results from\nneural models into PSL models. This allows our model to take advantage of\nadvanced neural methods, such as sentence embeddings, and logical reasoning\nover the structure of a dialogue. We compare our method with state-of-the-art\npurely neural ERC systems, and see almost a 20% improvement. With these\nresults, we provide an extensive qualitative and quantitative analysis over the\nDailyDialog conversation dataset.", "published": "2022-07-14 23:59:06", "link": "http://arxiv.org/abs/2207.07238v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Two-Pass Low Latency End-to-End Spoken Language Understanding", "abstract": "End-to-end (E2E) models are becoming increasingly popular for spoken language\nunderstanding (SLU) systems and are beginning to achieve competitive\nperformance to pipeline-based approaches. However, recent work has shown that\nthese models struggle to generalize to new phrasings for the same intent\nindicating that models cannot understand the semantic content of the given\nutterance. In this work, we incorporated language models pre-trained on\nunlabeled text data inside E2E-SLU frameworks to build strong semantic\nrepresentations. Incorporating both semantic and acoustic information can\nincrease the inference time, leading to high latency when deployed for\napplications like voice assistants. We developed a 2-pass SLU system that makes\nlow latency prediction using acoustic information from the few seconds of the\naudio in the first pass and makes higher quality prediction in the second pass\nby combining semantic and acoustic representations. We take inspiration from\nprior work on 2-pass end-to-end speech recognition systems that attends on both\naudio and first-pass hypothesis using a deliberation network. The proposed\n2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech\nCommands Challenge Set and SLURP dataset and reduces latency, thus improving\nuser experience. Our code and models are publicly available as part of the\nESPnet-SLU toolkit.", "published": "2022-07-14 05:50:16", "link": "http://arxiv.org/abs/2207.06670v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic\n  Knowledge Distillation of Self-Supervised Speech Models", "abstract": "Self-supervised learning (SSL) is seen as a very promising approach with high\nperformance for several speech downstream tasks. Since the parameters of SSL\nmodels are generally so large that training and inference require a lot of\nmemory and computational cost, it is desirable to produce compact SSL models\nwithout a significant performance degradation by applying compression methods\nsuch as knowledge distillation (KD). Although the KD approach is able to shrink\nthe depth and/or width of SSL model structures, there has been little research\non how varying the depth and width impacts the internal representation of the\nsmall-footprint model. This paper provides an empirical study that addresses\nthe question. We investigate the performance on SUPERB while varying the\nstructure and KD methods so as to keep the number of parameters constant; this\nallows us to analyze the contribution of the representation introduced by\nvarying the model architecture. Experiments demonstrate that a certain depth is\nessential for solving content-oriented tasks (e.g. automatic speech\nrecognition) accurately, whereas a certain width is necessary for achieving\nhigh performance on several speaker-oriented tasks (e.g. speaker\nidentification). Based on these observations, we identify, for SUPERB, a more\ncompressed model with better performance than previous studies.", "published": "2022-07-14 12:43:36", "link": "http://arxiv.org/abs/2207.06867v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Low-Resource Quechua ASR Improvement", "abstract": "Automatic Speech Recognition (ASR) is a key element in new services that\nhelps users to interact with an automated system. Deep learning methods have\nmade it possible to deploy systems with word error rates below 5% for ASR of\nEnglish. However, the use of these methods is only available for languages with\nhundreds or thousands of hours of audio and their corresponding transcriptions.\nFor the so-called low-resource languages to speed up the availability of\nresources that can improve the performance of their ASR systems, methods of\ncreating new resources on the basis of existing ones are being investigated. In\nthis paper we describe our data augmentation approach to improve the results of\nASR models for low-resource and agglutinative languages. We carry out\nexperiments developing an ASR for Quechua using the wav2letter++ model. We\nreduced WER by 8.73% through our approach to the base model. The resulting ASR\nmodel obtained 22.75% WER and was trained with 99 hours of original resources\nand 99 hours of synthetic data obtained with a combination of text augmentation\nand synthetic speech generati", "published": "2022-07-14 12:49:15", "link": "http://arxiv.org/abs/2207.06872v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically\n  Ambiguous Settings for Low Resource Languages", "abstract": "We leverage pre-trained language models to solve the task of complex NER for\ntwo low-resource languages: Chinese and Spanish. We use the technique of Whole\nWord Masking(WWM) to boost the performance of masked language modeling\nobjective on large and unsupervised corpora. We experiment with multiple neural\nnetwork architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on\ntop of a fine-tuned BERT layer. All our models outperform the baseline by a\nsignificant margin and our best performing model obtains a competitive position\non the evaluation leaderboard for the blind test set.", "published": "2022-07-14 13:00:41", "link": "http://arxiv.org/abs/2207.06882v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forming Trees with Treeformers", "abstract": "Human language is known to exhibit a nested, hierarchical structure, allowing\nus to form complex sentences out of smaller pieces. However, many\nstate-of-the-art neural networks models such as Transformers have no explicit\nhierarchical structure in its architecture -- that is, they don't have an\ninductive bias toward hierarchical structure. Additionally, Transformers are\nknown to perform poorly on compositional generalization tasks which require\nsuch structures. In this paper, we introduce Treeformer, a general-purpose\nencoder module inspired by the CKY algorithm which learns a composition\noperator and pooling function to construct hierarchical encodings for phrases\nand sentences. Our extensive experiments demonstrate the benefits of\nincorporating hierarchical structure into the Transformer and show significant\nimprovements in compositional generalization as well as in downstream tasks\nsuch as machine translation, abstractive summarization, and various natural\nlanguage understanding tasks.", "published": "2022-07-14 14:39:30", "link": "http://arxiv.org/abs/2207.06960v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Modelling with Pixels", "abstract": "Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust than BERT to orthographic\nattacks and linguistic code-switching, further confirming the benefits of\nmodelling language with pixels.", "published": "2022-07-14 15:20:36", "link": "http://arxiv.org/abs/2207.06991v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language models show human-like content effects on reasoning tasks", "abstract": "Reasoning is a key ability for an intelligent system. Large language models\n(LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit\nmany imperfections. However, human abstract reasoning is also imperfect. For\nexample, human reasoning is affected by our real-world knowledge and beliefs,\nand shows notable \"content effects\"; humans reason more reliably when the\nsemantic content of a problem supports the correct logical inferences. These\ncontent-entangled reasoning patterns play a central role in debates about the\nfundamental nature of human intelligence. Here, we investigate whether language\nmodels $\\unicode{x2014}$ whose prior expectations capture some aspects of human\nknowledge $\\unicode{x2014}$ similarly mix content into their answers to logical\nproblems. We explored this question across three logical reasoning tasks:\nnatural language inference, judging the logical validity of syllogisms, and the\nWason selection task. We evaluate state of the art large language models, as\nwell as humans, and find that the language models reflect many of the same\npatterns observed in humans across these tasks $\\unicode{x2014}$ like humans,\nmodels answer more accurately when the semantic content of a task supports the\nlogical inferences. These parallels are reflected both in answer patterns, and\nin lower-level features like the relationship between model answer\ndistributions and human response times. Our findings have implications for\nunderstanding both these cognitive effects in humans, and the factors that\ncontribute to language model performance.", "published": "2022-07-14 16:51:09", "link": "http://arxiv.org/abs/2207.07051v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated\n  Neural Text Retrievers", "abstract": "Prompt tuning attempts to update few task-specific parameters in pre-trained\nmodels. It has achieved comparable performance to fine-tuning of the full\nparameter set on both language understanding and generation tasks. In this\nwork, we study the problem of prompt tuning for neural text retrievers. We\nintroduce parameter-efficient prompt tuning for text retrieval across\nin-domain, cross-domain, and cross-topic settings. Through an extensive\nanalysis, we show that the strategy can mitigate the two issues --\nparameter-inefficiency and weak generalizability -- faced by fine-tuning based\nretrieval methods. Notably, it can significantly improve the out-of-domain\nzero-shot generalization of the retrieval models. By updating only 0.1% of the\nmodel parameters, the prompt tuning strategy can help retrieval models achieve\nbetter generalization performance than traditional methods in which all\nparameters are updated. Finally, to facilitate research on retrievers'\ncross-topic generalizability, we curate and release an academic retrieval\ndataset with 18K query-results pairs in 87 topics, making it the largest\ntopic-specific one to date.", "published": "2022-07-14 17:40:00", "link": "http://arxiv.org/abs/2207.07087v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combing for Credentials: Active Pattern Extraction from Smart Reply", "abstract": "Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, are\noften fine-tuned to achieve state-of-the-art performance on a downstream task.\nOne natural example is the ``Smart Reply'' application where a pre-trained\nmodel is tuned to provide suggested responses for a given query message. Since\nthe tuning data is often sensitive data such as emails or chat transcripts, it\nis important to understand and mitigate the risk that the model leaks its\ntuning data. We investigate potential information leakage vulnerabilities in a\ntypical Smart Reply pipeline. We consider a realistic setting where the\nadversary can only interact with the underlying model through a front-end\ninterface that constrains what types of queries can be sent to the model.\nPrevious attacks do not work in these settings, but require the ability to send\nunconstrained queries directly to the model. Even when there are no constraints\non the queries, previous attacks typically require thousands, or even millions,\nof queries to extract useful information, while our attacks can extract\nsensitive data in just a handful of queries. We introduce a new type of active\nextraction attack that exploits canonical patterns in text containing sensitive\ndata. We show experimentally that it is possible for an adversary to extract\nsensitive user information present in the training data, even in realistic\nsettings where all interactions with the model must go through a front-end that\nlimits the types of queries. We explore potential mitigation strategies and\ndemonstrate empirically how differential privacy appears to be a reasonably\neffective defense mechanism to such pattern extraction attacks.", "published": "2022-07-14 05:03:56", "link": "http://arxiv.org/abs/2207.10802v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer\n  to Unlabeled Modality", "abstract": "While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for multiple speech processing tasks. In\nparticular, our single model yields 1.2%/1.4%/27.2% speech recognition word\nerror rate on LRS3 with audio-visual/audio/visual input. Codes and models are\navailable at https://github.com/facebookresearch/av_hubert", "published": "2022-07-14 16:21:33", "link": "http://arxiv.org/abs/2207.07036v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "A Neural-Network Framework for the Design of Individualised Hearing-Loss\n  Compensation", "abstract": "Sound processing in the human auditory system is complex and highly\nnon-linear, whereas hearing aids (HAs) still rely on simplified descriptions of\nauditory processing or hearing loss to restore hearing. Even though standard HA\namplification strategies succeed in restoring audibility of faint sounds, they\nstill fall short of providing targeted treatments for complex sensorineural\ndeficits and adverse listening conditions. These shortcomings of current HA\ndevices demonstrate the need for advanced hearing-loss compensation strategies\nthat can effectively leverage the non-linear character of the auditory system.\nHere, we propose a differentiable deep-neural-network (DNN) framework that can\nbe used to train DNN-based HA models based on biophysical auditory-processing\ndifferences between normal-hearing and hearing-impaired systems. We investigate\ndifferent loss functions to accurately compensate for impairments that include\nouter-hair-cell (OHC) loss and cochlear synaptopathy (CS), and evaluate the\nbenefits of our trained DNN-based HA models for speech processing in quiet and\nin noise. Our results show that auditory-processing enhancement was possible\nfor all considered hearing-loss cases, with OHC loss proving easier to\ncompensate than CS. Several objective metrics were considered to estimate the\nexpected speech intelligibility after processing, and these simulations hold\npromise in yielding improved understanding of speech-in-noise for\nhearing-impaired listeners who use our DNN-based HA processing. Since our\nframework can be tuned to the hearing-loss profiles of individual listeners, we\nenter an era where truly individualised and DNN-based hearing-restoration\nstrategies can be developed and be tested experimentally.", "published": "2022-07-14 17:41:22", "link": "http://arxiv.org/abs/2207.07091v3", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Semi-supervised cross-lingual speech emotion recognition", "abstract": "Performance in Speech Emotion Recognition (SER) on a single language has\nincreased greatly in the last few years thanks to the use of deep learning\ntechniques. However, cross-lingual SER remains a challenge in real-world\napplications due to two main factors: the first is the big gap among the source\nand the target domain distributions; the second factor is the major\navailability of unlabeled utterances in contrast to the labeled ones for the\nnew language. Taking into account previous aspects, we propose a\nSemi-Supervised Learning (SSL) method for cross-lingual emotion recognition\nwhen only few labeled examples in the target domain (i.e. the new language) are\navailable. Our method is based on a Transformer and it adapts to the new domain\nby exploiting a pseudo-labeling strategy on the unlabeled utterances. In\nparticular, the use of a hard and soft pseudo-labels approach is investigated.\nWe thoroughly evaluate the performance of the proposed method in a\nspeaker-independent setup on both the source and the new language and show its\nrobustness across five languages belonging to different linguistic strains. The\nexperimental findings indicate that the unweighted accuracy is increased by an\naverage of 40% compared to state-of-the-art methods.", "published": "2022-07-14 09:24:55", "link": "http://arxiv.org/abs/2207.06767v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text\n  Adversarial Attacks", "abstract": "This paper introduces a new synthesis-based defense algorithm for\ncounteracting with a varieties of adversarial attacks developed for challenging\nthe performance of the cutting-edge speech-to-text transcription systems. Our\nalgorithm implements a Sobolev-based GAN and proposes a novel regularizer for\neffectively controlling over the functionality of the entire generative model,\nparticularly the discriminator network during training. Our achieved results\nupon carrying out numerous experiments on the victim DeepSpeech, Kaldi, and\nLingvo speech transcription systems corroborate the remarkable performance of\nour defense approach against a comprehensive range of targeted and non-targeted\nadversarial attacks.", "published": "2022-07-14 12:22:19", "link": "http://arxiv.org/abs/2207.06858v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Proceedings of the ICML 2022 Expressive Vocalizations Workshop and\n  Competition: Recognizing, Generating, and Personalizing Vocal Bursts", "abstract": "This is the Proceedings of the ICML Expressive Vocalization (ExVo)\nCompetition. The ExVo competition focuses on understanding and generating vocal\nbursts: laughs, gasps, cries, and other non-verbal vocalizations that are\ncentral to emotional expression and communication. ExVo 2022, included three\ncompetition tracks using a large-scale dataset of 59,201 vocalizations from\n1,702 speakers. The first, ExVo-MultiTask, requires participants to train a\nmulti-task model to recognize expressed emotions and demographic traits from\nvocal bursts. The second, ExVo-Generate, requires participants to train a\ngenerative model that produces vocal bursts conveying ten different emotions.\nThe third, ExVo-FewShot, requires participants to leverage few-shot learning\nincorporating speaker identity to train a model for the recognition of 10\nemotions conveyed by vocal bursts.", "published": "2022-07-14 14:30:34", "link": "http://arxiv.org/abs/2207.06958v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient spike encoding algorithms for neuromorphic speech recognition", "abstract": "Spiking Neural Networks (SNN) are known to be very effective for neuromorphic\nprocessor implementations, achieving orders of magnitude improvements in energy\nefficiency and computational latency over traditional deep learning approaches.\nComparable algorithmic performance was recently made possible as well with the\nadaptation of supervised training algorithms to the context of SNN. However,\ninformation including audio, video, and other sensor-derived data are typically\nencoded as real-valued signals that are not well-suited to SNN, preventing the\nnetwork from leveraging spike timing information. Efficient encoding from\nreal-valued signals to spikes is therefore critical and significantly impacts\nthe performance of the overall system. To efficiently encode signals into\nspikes, both the preservation of information relevant to the task at hand as\nwell as the density of the encoded spikes must be considered. In this paper, we\nstudy four spike encoding methods in the context of a speaker independent digit\nclassification system: Send on Delta, Time to First Spike, Leaky Integrate and\nFire Neuron and Bens Spiker Algorithm. We first show that all encoding methods\nyield higher classification accuracy using significantly fewer spikes when\nencoding a bio-inspired cochleagram as opposed to a traditional short-time\nFourier transform. We then show that two Send On Delta variants result in\nclassification results comparable with a state of the art deep convolutional\nneural network baseline, while simultaneously reducing the encoded bit rate.\nFinally, we show that several encoding methods result in improved performance\nover the conventional deep learning baseline in certain cases, further\ndemonstrating the power of spike encoding algorithms in the encoding of\nreal-valued signals and that neuromorphic implementation has the potential to\noutperform state of the art techniques.", "published": "2022-07-14 17:22:07", "link": "http://arxiv.org/abs/2207.07073v1", "categories": ["cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
{"title": "Few-shot bioacoustic event detection at the DCASE 2022 challenge", "abstract": "Few-shot sound event detection is the task of detecting sound events, despite\nhaving only a few labelled examples of the class of interest. This framework is\nparticularly useful in bioacoustics, where often there is a need to annotate\nvery long recordings but the expert annotator time is limited. This paper\npresents an overview of the second edition of the few-shot bioacoustic sound\nevent detection task included in the DCASE 2022 challenge. A detailed\ndescription of the task objectives, dataset, and baselines is presented,\ntogether with the main results obtained and characteristics of the submitted\nsystems. This task received submissions from 15 different teams from which 13\nscored higher than the baselines. The highest F-score was of 60% on the\nevaluation set, which leads to a huge improvement over last year's edition.\nHighly-performing methods made use of prototypical networks, transductive\nlearning, and addressed the variable length of events from all target classes.\nFurthermore, by analysing results on each of the subsets we can identify the\nmain difficulties that the systems face, and conclude that few-show bioacoustic\nsound event detection remains an open challenge.", "published": "2022-07-14 09:33:47", "link": "http://arxiv.org/abs/2207.07911v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multitrack Music Transformer", "abstract": "Existing approaches for generating multitrack music with transformer models\nhave been limited in terms of the number of instruments, the length of the\nmusic segments and slow inference. This is partly due to the memory\nrequirements of the lengthy input sequences necessitated by existing\nrepresentations. In this work, we propose a new multitrack music representation\nthat allows a diverse set of instruments while keeping a short sequence length.\nOur proposed Multitrack Music Transformer (MMT) achieves comparable performance\nwith state-of-the-art systems, landing in between two recently proposed models\nin a subjective listening test, while achieving substantial speedups and memory\nreductions over both, making the method attractive for real time improvisation\nor near real time creative applications. Further, we propose a new measure for\nanalyzing musical self-attention and show that the trained model attends more\nto notes that form a consonant interval with the current note and to notes that\nare 4N beats away from the current step.", "published": "2022-07-14 15:06:37", "link": "http://arxiv.org/abs/2207.06983v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-guided Album Cover Art Generation with Genetic Algorithms", "abstract": "Over 60,000 songs are released on Spotify every day, and the competition for\nthe listener's attention is immense. In that regard, the importance of\ncaptivating and inviting cover art cannot be underestimated, because it is\ndeeply entangled with a song's character and the artist's identity, and remains\none of the most important gateways to lead people to discover music. However,\ndesigning cover art is a highly creative, lengthy and sometimes expensive\nprocess that can be daunting, especially for non-professional artists. For this\nreason, we propose a novel deep-learning framework to generate cover art guided\nby audio features. Inspired by VQGAN-CLIP, our approach is highly flexible\nbecause individual components can easily be replaced without the need for any\nretraining. This paper outlines the architectural details of our models and\ndiscusses the optimization challenges that emerge from them. More specifically,\nwe will exploit genetic algorithms to overcome bad local minima and adversarial\nexamples. We find that our framework can generate suitable cover art for most\ngenres, and that the visual features adapt themselves to audio feature changes.\nGiven these results, we believe that our framework paves the road for\nextensions and more advanced applications in audio-guided visual generation\ntasks.", "published": "2022-07-14 18:41:00", "link": "http://arxiv.org/abs/2207.07162v1", "categories": ["cs.SD", "cs.GR", "cs.LG", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
