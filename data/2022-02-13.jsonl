{"title": "A Contrastive Framework for Neural Text Generation", "abstract": "Text generation is of great importance to many natural language processing\napplications. However, maximization-based decoding methods (e.g. beam search)\nof neural language models often lead to degenerate solutions -- the generated\ntext is unnatural and contains undesirable repetitions. Existing approaches\nintroduce stochasticity via sampling or modify training objectives to decrease\nprobabilities of certain tokens (e.g., unlikelihood training). However, they\noften lead to solutions that lack coherence. In this work, we show that an\nunderlying reason for model degeneration is the anisotropic distribution of\ntoken representations. We present a contrastive solution: (i) SimCTG, a\ncontrastive training objective to calibrate the model's representation space,\nand (ii) a decoding method -- contrastive search -- to encourage diversity\nwhile maintaining coherence in the generated text. Extensive experiments and\nanalyses on three benchmarks from two languages demonstrate that our proposed\napproach significantly outperforms current state-of-the-art text generation\nmethods as evaluated by both human and automatic metrics.", "published": "2022-02-13 21:46:14", "link": "http://arxiv.org/abs/2202.06417v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMN at SemEval-2022 Task 11: A Transformer-based System for English\n  Named Entity Recognition", "abstract": "Processing complex and ambiguous named entities is a challenging research\nproblem, but it has not received sufficient attention from the natural language\nprocessing community. In this short paper, we present our participation in the\nEnglish track of SemEval-2022 Task 11: Multilingual Complex Named Entity\nRecognition. Inspired by the recent advances in pretrained Transformer language\nmodels, we propose a simple yet effective Transformer-based baseline for the\ntask. Despite its simplicity, our proposed approach shows competitive results\nin the leaderboard as we ranked 12 over 30 teams. Our system achieved a macro\nF1 score of 72.50% on the held-out test set. We have also explored a data\naugmentation approach using entity linking. While the approach does not improve\nthe final performance, we also discuss it in this paper.", "published": "2022-02-13 05:46:14", "link": "http://arxiv.org/abs/2203.03546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uni-Retriever: Towards Learning The Unified Embedding Based Retriever in\n  Bing Sponsored Search", "abstract": "Embedding based retrieval (EBR) is a fundamental building block in many web\napplications. However, EBR in sponsored search is distinguished from other\ngeneric scenarios and technically challenging due to the need of serving\nmultiple retrieval purposes: firstly, it has to retrieve high-relevance ads,\nwhich may exactly serve user's search intent; secondly, it needs to retrieve\nhigh-CTR ads so as to maximize the overall user clicks. In this paper, we\npresent a novel representation learning framework Uni-Retriever developed for\nBing Search, which unifies two different training modes knowledge distillation\nand contrastive learning to realize both required objectives. On one hand, the\ncapability of making high-relevance retrieval is established by distilling\nknowledge from the ``relevance teacher model''. On the other hand, the\ncapability of making high-CTR retrieval is optimized by learning to\ndiscriminate user's clicked ads from the entire corpus. The two training modes\nare jointly performed as a multi-objective learning process, such that the ads\nof high relevance and CTR can be favored by the generated embeddings. Besides\nthe learning strategy, we also elaborate our solution for EBR serving pipeline\nbuilt upon the substantially optimized DiskANN, where massive-scale EBR can be\nperformed with competitive time and memory efficiency, and accomplished in\nhigh-quality. We make comprehensive offline and online experiments to evaluate\nthe proposed techniques, whose findings may provide useful insights for the\nfuture development of EBR systems. Uni-Retriever has been mainstreamed as the\nmajor retrieval path in Bing's production thanks to the notable improvements on\nthe representation and EBR serving quality.", "published": "2022-02-13 05:20:44", "link": "http://arxiv.org/abs/2202.06212v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Emotion Based Hate Speech Detection using Multimodal Learning", "abstract": "In recent years, monitoring hate speech and offensive language on social\nmedia platforms has become paramount due to its widespread usage among all age\ngroups, races, and ethnicities. Consequently, there have been substantial\nresearch efforts towards automated detection of such content using Natural\nLanguage Processing (NLP). While successfully filtering textual data, no\nresearch has focused on detecting hateful content in multimedia data. With\nincreased ease of data storage and the exponential growth of social media\nplatforms, multimedia content proliferates the internet as much as text data.\nNevertheless, it escapes the automatic filtering systems. Hate speech and\noffensiveness can be detected in multimedia primarily via three modalities,\ni.e., visual, acoustic, and verbal. Our preliminary study concluded that the\nmost essential features in classifying hate speech would be the speaker's\nemotional state and its influence on the spoken words, therefore limiting our\ncurrent research to these modalities. This paper proposes the first multimodal\ndeep learning framework to combine the auditory features representing emotion\nand the semantic features to detect hateful content. Our results demonstrate\nthat incorporating emotional attributes leads to significant improvement over\ntext-based models in detecting hateful multimedia content. This paper also\npresents a new Hate Speech Detection Video Dataset (HSDVD) collected for the\npurpose of multimodal learning as no such dataset exists today.", "published": "2022-02-13 05:39:47", "link": "http://arxiv.org/abs/2202.06218v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Depression Classification Using Articulatory Coordination\n  Features And Hierarchical Attention Based Text Embeddings", "abstract": "Multimodal depression classification has gained immense popularity over the\nrecent years. We develop a multimodal depression classification system using\narticulatory coordination features extracted from vocal tract variables and\ntext transcriptions obtained from an automatic speech recognition tool that\nyields improvements of area under the receiver operating characteristics curve\ncompared to uni-modal classifiers (7.5% and 13.7% for audio and text\nrespectively). We show that in the case of limited training data, a\nsegment-level classifier can first be trained to then obtain a session-wise\nprediction without hindering the performance, using a multi-stage convolutional\nrecurrent neural network. A text model is trained using a Hierarchical\nAttention Network (HAN). The multimodal system is developed by combining\nembeddings from the session-level audio model and the HAN text model", "published": "2022-02-13 07:37:09", "link": "http://arxiv.org/abs/2202.06238v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Assessment of contextualised representations in detecting outcome\n  phrases in clinical trials", "abstract": "Automating the recognition of outcomes reported in clinical trials using\nmachine learning has a huge potential of speeding up access to evidence\nnecessary in healthcare decision-making. Prior research has however\nacknowledged inadequate training corpora as a challenge for the Outcome\ndetection (OD) task. Additionally, several contextualized representations like\nBERT and ELMO have achieved unparalleled success in detecting various diseases,\ngenes, proteins, and chemicals, however, the same cannot be emphatically stated\nfor outcomes, because these models have been relatively under-tested and\nstudied for the OD task. We introduce \"EBM-COMET\", a dataset in which 300\nPubMed abstracts are expertly annotated for clinical outcomes. Unlike prior\nrelated datasets that use arbitrary outcome classifications, we use labels from\na taxonomy recently published to standardize outcome classifications. To\nextract outcomes, we fine-tune a variety of pre-trained contextualized\nrepresentations, additionally, we use frozen contextualized and\ncontext-independent representations in our custom neural model augmented with\nclinically informed Part-Of-Speech embeddings and a cost-sensitive loss\nfunction. We adopt strict evaluation for the trained models by rewarding them\nfor correctly identifying full outcome phrases rather than words within the\nentities i.e. given an outcome \"systolic blood pressure\", the models are\nrewarded a classification score only when they predict all 3 words in sequence,\notherwise, they are not rewarded. We observe our best model (BioBERT) achieve\n81.5\\% F1, 81.3\\% sensitivity and 98.0\\% specificity. We reach a consensus on\nwhich contextualized representations are best suited for detecting outcomes\nfrom clinical-trial abstracts. Furthermore, our best model outperforms scores\npublished on the original EBM-NLP dataset leader-board scores.", "published": "2022-02-13 15:08:00", "link": "http://arxiv.org/abs/2203.03547v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child\n  Interactive Storytelling with Flexible Parental Involvement", "abstract": "Despite its benefits for children's skill development and parent-child\nbonding, many parents do not often engage in interactive storytelling by having\nstory-related dialogues with their child due to limited availability or\nchallenges in coming up with appropriate questions. While recent advances made\nAI generation of questions from stories possible, the fully-automated approach\nexcludes parent involvement, disregards educational goals, and underoptimizes\nfor child engagement. Informed by need-finding interviews and participatory\ndesign (PD) results, we developed StoryBuddy, an AI-enabled system for parents\nto create interactive storytelling experiences. StoryBuddy's design highlighted\nthe need for accommodating dynamic user needs between the desire for parent\ninvolvement and parent-child bonding and the goal of minimizing parent\nintervention when busy. The PD revealed varied assessment and educational goals\nof parents, which StoryBuddy addressed by supporting configuring question types\nand tracking child progress. A user study validated StoryBuddy's usability and\nsuggested design insights for future parent-AI collaboration systems.", "published": "2022-02-13 04:53:28", "link": "http://arxiv.org/abs/2202.06205v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "PQuAD: A Persian Question Answering Dataset", "abstract": "We present Persian Question Answering Dataset (PQuAD), a crowdsourced reading\ncomprehension dataset on Persian Wikipedia articles. It includes 80,000\nquestions along with their answers, with 25% of the questions being\nadversarially unanswerable. We examine various properties of the dataset to\nshow the diversity and the level of its difficulty as an MRC benchmark. By\nreleasing this dataset, we aim to ease research on Persian reading\ncomprehension and development of Persian question answering systems. Our\nexperiments on different state-of-the-art pre-trained contextualized language\nmodels show 74.8% Exact Match (EM) and 87.6% F1-score that can be used as the\nbaseline results for further research on Persian QA.", "published": "2022-02-13 05:42:55", "link": "http://arxiv.org/abs/2202.06219v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Simplified Variant of G\u00f6del's Ontological Argument", "abstract": "A simplified variant of G\\\"odel's ontological argument is presented. The\nsimplified argument is valid already in basic modal logics K or KT, it does not\nsuffer from modal collapse, and it avoids the rather complex predicates of\nessence (Ess.) and necessary existence (NE) as used by G\\\"odel. The variant\npresented has been obtained as a side result of a series of theory\nsimplification experiments conducted in interaction with a modern proof\nassistant system. The starting point for these experiments was the computer\nencoding of G\\\"odel's argument, and then automated reasoning techniques were\nsystematically applied to arrive at the simplified variant presented. The\npresented work thus exemplifies a fruitful human-computer interaction in\ncomputational metaphysics. Whether the presented result increases or decreases\nthe attractiveness and persuasiveness of the ontological argument is a question\nI would like to pass on to philosophy and theology.", "published": "2022-02-13 08:58:04", "link": "http://arxiv.org/abs/2202.06264v3", "categories": ["cs.LO", "cs.AI", "cs.CL", "math.LO"], "primary_category": "cs.LO"}
{"title": "Incremental user embedding modeling for personalized text classification", "abstract": "Individual user profiles and interaction histories play a significant role in\nproviding customized experiences in real-world applications such as chatbots,\nsocial media, retail, and education. Adaptive user representation learning by\nutilizing user personalized information has become increasingly challenging due\nto ever-growing history data. In this work, we propose an incremental user\nembedding modeling approach, in which embeddings of user's recent interaction\nhistories are dynamically integrated into the accumulated history vectors via a\ntransformer encoder. This modeling paradigm allows us to create generalized\nuser representations in a consecutive manner and also alleviate the challenges\nof data management. We demonstrate the effectiveness of this approach by\napplying it to a personalized multi-class classification task based on the\nReddit dataset, and achieve 9% and 30% relative improvement on prediction\naccuracy over a baseline system for two experiment settings through appropriate\ncomment history encoding and task modeling.", "published": "2022-02-13 17:33:35", "link": "http://arxiv.org/abs/2202.06369v1", "categories": ["cs.LG", "cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Scaling Laws Under the Microscope: Predicting Transformer Performance\n  from Small Scale Experiments", "abstract": "Neural scaling laws define a predictable relationship between a model's\nparameter count and its performance after training in the form of a power law.\nHowever, most research to date has not explicitly investigated whether scaling\nlaws can be used to accelerate model development. In this work, we perform such\nan empirical investigation across a wide range of language understanding tasks,\nstarting from models with as few as 10K parameters, and evaluate downstream\nperformance across 9 language understanding tasks. We find that scaling laws\nemerge at finetuning time in some NLP tasks, and that they can also be\nexploited for debugging convergence when training large models. Moreover, for\ntasks where scaling laws exist, they can be used to predict the performance of\nlarger models, which enables effective model selection. However, revealing\nscaling laws requires careful hyperparameter tuning and multiple runs for the\npurpose of uncertainty estimation, which incurs additional overhead, partially\noffsetting the computational benefits.", "published": "2022-02-13 19:13:00", "link": "http://arxiv.org/abs/2202.06387v2", "categories": ["cs.CL", "cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.CL"}
{"title": "Transformer-based Approaches for Legal Text Processing", "abstract": "In this paper, we introduce our approaches using Transformer-based models for\ndifferent problems of the COLIEE 2021 automatic legal text processing\ncompetition. Automated processing of legal documents is a challenging task\nbecause of the characteristics of legal documents as well as the limitation of\nthe amount of data. With our detailed experiments, we found that\nTransformer-based pretrained language models can perform well with automated\nlegal text processing problems with appropriate approaches. We describe in\ndetail the processing steps for each task such as problem formulation, data\nprocessing and augmentation, pretraining, finetuning. In addition, we introduce\nto the community two pretrained models that take advantage of parallel\ntranslations in legal domain, NFSP and NMSP. In which, NFSP achieves the\nstate-of-the-art result in Task 5 of the competition. Although the paper\nfocuses on technical reporting, the novelty of its approaches can also be an\nuseful reference in automated legal document processing using Transformer-based\nmodels.", "published": "2022-02-13 19:59:15", "link": "http://arxiv.org/abs/2202.06397v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Distribution augmentation for low-resource expressive text-to-speech", "abstract": "This paper presents a novel data augmentation technique for text-to-speech\n(TTS), that allows to generate new (text, audio) training examples without\nrequiring any additional data. Our goal is to increase diversity of text\nconditionings available during training. This helps to reduce overfitting,\nespecially in low-resource settings. Our method relies on substituting text and\naudio fragments in a way that preserves syntactical correctness. We take\nadditional measures to ensure that synthesized speech does not contain\nartifacts caused by combining inconsistent audio samples. The perceptual\nevaluations show that our method improves speech quality over a number of\ndatasets, speakers, and TTS architectures. We also demonstrate that it greatly\nimproves robustness of attention-based TTS models.", "published": "2022-02-13 21:19:31", "link": "http://arxiv.org/abs/2202.06409v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "DEEPCHORUS: A Hybrid Model of Multi-scale Convolution and Self-attention\n  for Chorus Detection", "abstract": "Chorus detection is a challenging problem in musical signal processing as the\nchorus often repeats more than once in popular songs, usually with rich\ninstruments and complex rhythm forms. Most of the existing works focus on the\nreceptiveness of chorus sections based on some explicit features such as\nloudness and occurrence frequency. These pre-assumptions for chorus limit the\ngeneralization capacity of these methods, causing misdetection on other\nrepeated sections such as verse. To solve the problem, in this paper we propose\nan end-to-end chorus detection model DeepChorus, reducing the engineering\neffort and the need for prior knowledge. The proposed model includes two main\nstructures: i) a Multi-Scale Network to derive preliminary representations of\nchorus segments, and ii) a Self-Attention Convolution Network to further\nprocess the features into probability curves representing chorus presence. To\nobtain the final results, we apply an adaptive threshold to binarize the\noriginal curve. The experimental results show that DeepChorus outperforms\nexisting state-of-the-art methods in most cases.", "published": "2022-02-13 14:58:11", "link": "http://arxiv.org/abs/2202.06338v2", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Learning long-term music representations via hierarchical contextual\n  constraints", "abstract": "Learning symbolic music representations, especially disentangled\nrepresentations with probabilistic interpretations, has been shown to benefit\nboth music understanding and generation. However, most models are only\napplicable to short-term music, while learning long-term music representations\nremains a challenging task. We have seen several studies attempting to learn\nhierarchical representations directly in an end-to-end manner, but these models\nhave not been able to achieve the desired results and the training process is\nnot stable. In this paper, we propose a novel approach to learn long-term\nsymbolic music representations through contextual constraints. First, we use\ncontrastive learning to pre-train a long-term representation by constraining\nits difference from the short-term representation (extracted by an\noff-the-shelf model). Then, we fine-tune the long-term representation by a\nhierarchical prediction model such that a good long-term representation (e.g.,\nan 8-bar representation) can reconstruct the corresponding short-term ones\n(e.g., the 2-bar representations within the 8-bar range). Experiments show that\nour method stabilizes the training and the fine-tuning steps. In addition, the\ndesigned contextual constraints benefit both reconstruction and\ndisentanglement, significantly outperforming the baselines.", "published": "2022-02-13 01:44:39", "link": "http://arxiv.org/abs/2202.06180v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Sound Localization in the Wild by Cross-Modal Interference\n  Erasing", "abstract": "The task of audio-visual sound source localization has been well studied\nunder constrained scenes, where the audio recordings are clean. However, in\nreal-world scenarios, audios are usually contaminated by off-screen sound and\nbackground noise. They will interfere with the procedure of identifying desired\nsources and building visual-sound connections, making previous studies\nnon-applicable. In this work, we propose the Interference Eraser (IEr)\nframework, which tackles the problem of audio-visual sound source localization\nin the wild. The key idea is to eliminate the interference by redefining and\ncarving discriminative audio representations. Specifically, we observe that the\nprevious practice of learning only a single audio representation is\ninsufficient due to the additive nature of audio signals. We thus extend the\naudio representation with our Audio-Instance-Identifier module, which clearly\ndistinguishes sounding instances when audio signals of different volumes are\nunevenly mixed. Then we erase the influence of the audible but off-screen\nsounds and the silent but visible objects by a Cross-modal Referrer module with\ncross-modality distillation. Quantitative and qualitative evaluations\ndemonstrate that our proposed framework achieves superior results on sound\nlocalization tasks, especially under real-world scenarios. Code is available at\nhttps://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild.", "published": "2022-02-13 21:06:19", "link": "http://arxiv.org/abs/2202.06406v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
