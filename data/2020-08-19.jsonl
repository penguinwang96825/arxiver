{"title": "Victim or Perpetrator? Analysis of Violent Characters Portrayals from\n  Movie Scripts", "abstract": "Violent content in the media can influence viewers' perception of the\nsociety. For example, frequent depictions of certain demographics as victims or\nperpetrators of violence can shape stereotyped attitudes. We propose that\ncomputational methods can aid in the large-scale analysis of violence in\nmovies. The method we develop characterizes aspects of violent content solely\nfrom the language used in the scripts. Thus, our method is applicable to a\nmovie in the earlier stages of content creation even before it is produced.\nThis is complementary to previous works which rely on audio or video post\nproduction. In this work, we identify stereotypes in character roles (i.e.,\nvictim, perpetrator and narrator) based on the demographics of the actor casted\nfor that role. Our results highlight two significant differences in the\nfrequency of portrayals as well as the demographics of the interaction between\nvictims and perpetrators : (1) female characters appear more often as victims,\nand (2) perpetrators are more likely to be White if the victim is Black or\nLatino. To date, we are the first to show that language used in movie scripts\nis a strong indicator of violent content, and that there are systematic\nportrayals of certain demographics as victims and perpetrators in a large\ndataset. This offers novel computational tools to assist in creating awareness\nof representations in storytelling", "published": "2020-08-19 02:18:53", "link": "http://arxiv.org/abs/2008.08225v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinChat: Corpus and evaluation setup for Finnish chat conversations on\n  everyday topics", "abstract": "Creating open-domain chatbots requires large amounts of conversational data\nand related benchmark tasks to evaluate them. Standardized evaluation tasks are\ncrucial for creating automatic evaluation metrics for model development;\notherwise, comparing the models would require resource-expensive human\nevaluation. While chatbot challenges have recently managed to provide a\nplethora of such resources for English, resources in other languages are not\nyet available. In this work, we provide a starting point for Finnish\nopen-domain chatbot research. We describe our collection efforts to create the\nFinnish chat conversation corpus FinChat, which is made available publicly.\nFinChat includes unscripted conversations on seven topics from people of\ndifferent ages. Using this corpus, we also construct a retrieval-based\nevaluation task for Finnish chatbot development. We observe that off-the-shelf\nchatbot models trained on conversational corpora do not perform better than\nchance at choosing the right answer based on automatic metrics, while humans\ncan do the same task almost perfectly. Similarly, in a human evaluation,\nresponses to questions from the evaluation set generated by the chatbots are\npredominantly marked as incoherent. Thus, FinChat provides a challenging\nevaluation set, meant to encourage chatbot development in Finnish.", "published": "2020-08-19 07:58:16", "link": "http://arxiv.org/abs/2008.08315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BabelEnconding at SemEval-2020 Task 3: Contextual Similarity as a\n  Combination of Multilingualism and Language Models", "abstract": "This paper describes the system submitted by our team (BabelEnconding) to\nSemEval-2020 Task 3: Predicting the Graded Effect of Context in Word\nSimilarity. We propose an approach that relies on translation and multilingual\nlanguage models in order to compute the contextual similarity between pairs of\nwords. Our hypothesis is that evidence from additional languages can leverage\nthe correlation with the human generated scores. BabelEnconding was applied to\nboth subtasks and ranked among the top-3 in six out of eight task/language\ncombinations and was the highest scoring system three times.", "published": "2020-08-19 13:46:37", "link": "http://arxiv.org/abs/2008.08439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UoB at SemEval-2020 Task 12: Boosting BERT with Corpus Level Information", "abstract": "Pre-trained language model word representation, such as BERT, have been\nextremely successful in several Natural Language Processing tasks significantly\nimproving on the state-of-the-art. This can largely be attributed to their\nability to better capture semantic information contained within a sentence.\nSeveral tasks, however, can benefit from information available at a corpus\nlevel, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work\nwe test the effectiveness of integrating this information with BERT on the task\nof identifying abuse on social media and show that integrating this information\nwith BERT does indeed significantly improve performance. We participate in\nSub-Task A (abuse detection) wherein we achieve a score within two points of\nthe top performing team and in Sub-Task B (target detection) wherein we are\nranked 4 of the 44 participating teams.", "published": "2020-08-19 16:47:15", "link": "http://arxiv.org/abs/2008.08547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Text Simplification", "abstract": "Text Simplification (TS) aims to reduce the linguistic complexity of content\nto make it easier to understand. Research in TS has been of keen interest,\nespecially as approaches to TS have shifted from manual, hand-crafted rules to\nautomated simplification. This survey seeks to provide a comprehensive overview\nof TS, including a brief description of earlier approaches used, discussion of\nvarious aspects of simplification (lexical, semantic and syntactic), and latest\ntechniques being utilized in the field. We note that the research in the field\nhas clearly shifted towards utilizing deep learning techniques to perform TS,\nwith a specific focus on developing solutions to combat the lack of data\navailable for simplification. We also include a discussion of datasets and\nevaluations metrics commonly used, along with discussion of related fields\nwithin Natural Language Processing (NLP), like semantic similarity.", "published": "2020-08-19 18:12:33", "link": "http://arxiv.org/abs/2008.08612v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Stance Data Set on Polarized Conversations on Twitter about the\n  Efficacy of Hydroxychloroquine as a Treatment for COVID-19", "abstract": "At the time of this study, the SARS-CoV-2 virus that caused the COVID-19\npandemic has spread significantly across the world. Considering the uncertainty\nabout policies, health risks, financial difficulties, etc. the online media,\nspecially the Twitter platform, is experiencing a high volume of activity\nrelated to this pandemic. Among the hot topics, the polarized debates about\nunconfirmed medicines for the treatment and prevention of the disease have\nattracted significant attention from online media users. In this work, we\npresent a stance data set, COVID-CQ, of user-generated content on Twitter in\nthe context of COVID-19. We investigated more than 14 thousand tweets and\nmanually annotated the opinions of the tweet initiators regarding the use of\n\"chloroquine\" and \"hydroxychloroquine\" for the treatment or prevention of\nCOVID-19. To the best of our knowledge, COVID-CQ is the first data set of\nTwitter users' stances in the context of the COVID-19 pandemic, and the largest\nTwitter data set on users' stances towards a claim, in any domain. We have made\nthis data set available to the research community via GitHub. We expect this\ndata set to be useful for many research purposes, including stance detection,\nevolution and dynamics of opinions regarding this outbreak, and changes in\nopinions in response to the exogenous shocks such as policy decisions and\nevents.", "published": "2020-08-19 21:58:58", "link": "http://arxiv.org/abs/2009.01188v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Generating Categories for Sets of Entities", "abstract": "Category systems are central components of knowledge bases, as they provide a\nhierarchical grouping of semantically related concepts and entities. They are a\nunique and valuable resource that is utilized in a broad range of information\naccess tasks. To aid knowledge editors in the manual process of expanding a\ncategory system, this paper presents a method of generating categories for sets\nof entities. First, we employ neural abstractive summarization models to\ngenerate candidate categories. Next, the location within the hierarchy is\nidentified for each candidate. Finally, structure-, content-, and\nhierarchy-based features are used to rank candidates to identify by the most\npromising ones (measured in terms of specificity, hierarchy, and importance).\nWe develop a test collection based on Wikipedia categories and demonstrate the\neffectiveness of the proposed approach.", "published": "2020-08-19 13:31:07", "link": "http://arxiv.org/abs/2008.08428v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transformer based Multilingual document Embedding model", "abstract": "One of the current state-of-the-art multilingual document embedding model\nLASER is based on the bidirectional LSTM neural machine translation model. This\npaper presents a transformer-based sentence/document embedding model, T-LASER,\nwhich makes three significant improvements. Firstly, the BiLSTM layers is\nreplaced by the attention-based transformer layers, which is more capable of\nlearning sequential patterns in longer texts. Secondly, due to the absence of\nrecurrence, T-LASER enables faster parallel computations in the encoder to\ngenerate the text embedding. Thirdly, we augment the NMT translation loss\nfunction with an additional novel distance constraint loss. This distance\nconstraint loss would further bring the embeddings of parallel sentences close\ntogether in the vector space; we call the T-LASER model trained with distance\nconstraint, cT-LASER. Our cT-LASER model significantly outperforms both\nBiLSTM-based LASER and the simpler transformer-based T-LASER.", "published": "2020-08-19 17:51:30", "link": "http://arxiv.org/abs/2008.08567v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Utterance Language Models with Acoustic Error Sampling", "abstract": "The effective exploitation of richer contextual information in language\nmodels (LMs) is a long-standing research problem for automatic speech\nrecognition (ASR). A cross-utterance LM (CULM) is proposed in this paper, which\naugments the input to a standard long short-term memory (LSTM) LM with a\ncontext vector derived from past and future utterances using an extraction\nnetwork. The extraction network uses another LSTM to encode surrounding\nutterances into vectors which are integrated into a context vector using either\na projection of LSTM final hidden states, or a multi-head self-attentive layer.\nIn addition, an acoustic error sampling technique is proposed to reduce the\nmismatch between training and test-time. This is achieved by considering\npossible ASR errors into the model training procedure, and can therefore\nimprove the word error rate (WER). Experiments performed on both AMI and\nSwitchboard datasets show that CULMs outperform the LSTM LM baseline WER. In\nparticular, the CULM with a self-attentive layer-based extraction network and\nacoustic error sampling achieves 0.6% absolute WER reduction on AMI, 0.3% WER\nreduction on the Switchboard part and 0.9% WER reduction on the Callhome part\nof Eval2000 test set over the respective baselines.", "published": "2020-08-19 17:40:11", "link": "http://arxiv.org/abs/2009.01008v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Historical Interaction Data for Improving Conversational\n  Recommender System", "abstract": "Recently, conversational recommender system (CRS) has become an emerging and\npractical research topic. Most of the existing CRS methods focus on learning\neffective preference representations for users from conversation data alone.\nWhile, we take a new perspective to leverage historical interaction data for\nimproving CRS. For this purpose, we propose a novel pre-training approach to\nintegrating both item-based preference sequence (from historical interaction\ndata) and attribute-based preference sequence (from conversation data) via\npre-training methods. We carefully design two pre-training tasks to enhance\ninformation fusion between item- and attribute-based preference. To improve the\nlearning performance, we further develop an effective negative sample generator\nwhich can produce high-quality negative samples. Experiment results on two\nreal-world datasets have demonstrated the effectiveness of our approach for\nimproving CRS.", "published": "2020-08-19 03:43:50", "link": "http://arxiv.org/abs/2008.08247v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Top2Vec: Distributed Representations of Topics", "abstract": "Topic modeling is used for discovering latent semantic structure, usually\nreferred to as topics, in a large collection of documents. The most widely used\nmethods are Latent Dirichlet Allocation and Probabilistic Latent Semantic\nAnalysis. Despite their popularity they have several weaknesses. In order to\nachieve optimal results they often require the number of topics to be known,\ncustom stop-word lists, stemming, and lemmatization. Additionally these methods\nrely on bag-of-words representation of documents which ignore the ordering and\nsemantics of words. Distributed representations of documents and words have\ngained popularity due to their ability to capture semantics of words and\ndocuments. We present $\\texttt{top2vec}$, which leverages joint document and\nword semantic embedding to find $\\textit{topic vectors}$. This model does not\nrequire stop-word lists, stemming or lemmatization, and it automatically finds\nthe number of topics. The resulting topic vectors are jointly embedded with the\ndocument and word vectors with distance between them representing semantic\nsimilarity. Our experiments demonstrate that $\\texttt{top2vec}$ finds topics\nwhich are significantly more informative and representative of the corpus\ntrained on than probabilistic generative models.", "published": "2020-08-19 20:58:27", "link": "http://arxiv.org/abs/2008.09470v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "HeteGCN: Heterogeneous Graph Convolutional Networks for Text\n  Classification", "abstract": "We consider the problem of learning efficient and inductive graph\nconvolutional networks for text classification with a large number of examples\nand features. Existing state-of-the-art graph embedding based methods such as\npredictive text embedding (PTE) and TextGCN have shortcomings in terms of\npredictive performance, scalability and inductive capability. To address these\nlimitations, we propose a heterogeneous graph convolutional network (HeteGCN)\nmodeling approach that unites the best aspects of PTE and TextGCN together. The\nmain idea is to learn feature embeddings and derive document embeddings using a\nHeteGCN architecture with different graphs used across layers. We simplify\nTextGCN by dissecting into several HeteGCN models which (a) helps to study the\nusefulness of individual models and (b) offers flexibility in fusing learned\nembeddings from different models. In effect, the number of model parameters is\nreduced significantly, enabling faster training and improving performance in\nsmall labeled training set scenario. Our detailed experimental studies\ndemonstrate the efficacy of the proposed approach.", "published": "2020-08-19 12:24:35", "link": "http://arxiv.org/abs/2008.12842v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training", "abstract": "Mispronunciation detection is an essential component of the Computer-Assisted\nPronunciation Training (CAPT) systems. State-of-the-art mispronunciation\ndetection models use Deep Neural Networks (DNN) for acoustic modeling, and a\nGoodness of Pronunciation (GOP) based algorithm for pronunciation scoring.\n  However, GOP based scoring models have two major limitations: i.e., (i) They\ndepend on forced alignment which splits the speech into phonetic segments and\nindependently use them for scoring, which neglects the transitions between\nphonemes within the segment;\n  (ii) They only focus on phonetic segments, which fails to consider the\ncontext effects across phonemes (such as liaison, omission, incomplete plosive\nsound, etc.).\n  In this work, we propose the Context-aware Goodness of Pronunciation (CaGOP)\nscoring model. Particularly, two factors namely the transition factor and the\nduration factor are injected into CaGOP scoring.\n  The transition factor identifies the transitions between phonemes and applies\nthem to weight the frame-wise GOP. Moreover, a self-attention based phonetic\nduration modeling is proposed to introduce the duration factor into the scoring\nmodel.\n  The proposed scoring model significantly outperforms baselines, achieving 20%\nand 12% relative improvement over the GOP model on the phoneme-level and\nsentence-level mispronunciation detection respectively.", "published": "2020-08-19 19:38:38", "link": "http://arxiv.org/abs/2008.08647v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HpRNet : Incorporating Residual Noise Modeling for Violin in a\n  Variational Parametric Synthesizer", "abstract": "Generative Models for Audio Synthesis have been gaining momentum in the last\nfew years. More recently, parametric representations of the audio signal have\nbeen incorporated to facilitate better musical control of the synthesized\noutput. In this work, we investigate a parametric model for violin tones, in\nparticular the generative modeling of the residual bow noise to make for more\nnatural tone quality. To aid in our analysis, we introduce a dataset of\nCarnatic Violin Recordings where bow noise is an integral part of the playing\nstyle of higher pitched notes in specific gestural contexts. We obtain insights\nabout each of the harmonic and residual components of the signal, as well as\ntheir interdependence, via observations on the latent space derived in the\ncourse of variational encoding of the spectral envelopes of the sustained\nsounds.", "published": "2020-08-19 12:48:32", "link": "http://arxiv.org/abs/2008.08405v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Aedes Aegypti Mosquitoes through Audio Classification with\n  Convolutional Neural Networks", "abstract": "The incidence of mosquito-borne diseases is significant in under-developed\nregions, mostly due to the lack of resources to implement aggressive control\nmeasurements against mosquito proliferation. A potential strategy to raise\ncommunity awareness regarding mosquito proliferation is building a live map of\nmosquito incidences using smartphone apps and crowdsourcing. In this paper, we\nexplore the possibility of identifying Aedes aegypti mosquitoes using machine\nlearning techniques and audio analysis captured from commercially available\nsmartphones. In summary, we downsampled Aedes aegypti wingbeat recordings and\nused them to train a convolutional neural network (CNN) through supervised\nlearning. As a feature, we used the recording spectrogram to represent the\nmosquito wingbeat frequency over time visually. We trained and compared three\nclassifiers: a binary, a multiclass, and an ensemble of binary classifiers. In\nour evaluation, the binary and ensemble models achieved accuracy of 97.65%\n($\\pm$ 0.55) and 94.56% ($\\pm$ 0.77), respectively, whereas the multiclass had\nan accuracy of 78.12% ($\\pm$ 2.09). The best sensitivity was observed in the\nensemble approach (96.82% $\\pm$ 1.62), followed by the multiclass for the\nparticular case of Aedes aegypti (90.23% $\\pm$ 3.83) and the binary (88.49%\n$\\pm$ 6.68). The binary classifier and the multiclass classifier presented the\nbest balance between precision and recall, with F1-measure close to 90%.\nAlthough the ensemble classifier achieved the lowest precision, thus impairing\nits F1-measure (79.95% $\\pm$ 2.13), it was the most powerful classifier to\ndetect Aedes aegypti in our dataset.", "published": "2020-08-19 00:26:21", "link": "http://arxiv.org/abs/2008.09024v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
