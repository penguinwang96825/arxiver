{"title": "A Hierarchical Location Normalization System for Text", "abstract": "It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.", "published": "2020-01-21 03:10:02", "link": "http://arxiv.org/abs/2001.07320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Length-controllable Abstractive Summarization by Guiding with Summary\n  Prototype", "abstract": "We propose a new length-controllable abstractive summarization model. Recent\nstate-of-the-art abstractive summarization models based on encoder-decoder\nmodels generate only one summary per source text. However, controllable\nsummarization, especially of the length, is an important aspect for practical\napplications. Previous studies on length-controllable abstractive summarization\nincorporate length embeddings in the decoder module for controlling the summary\nlength. Although the length embeddings can control where to stop decoding, they\ndo not decide which information should be included in the summary within the\nlength constraint. Unlike the previous models, our length-controllable\nabstractive summarization model incorporates a word-level extractive module in\nthe encoder-decoder model instead of length embeddings. Our model generates a\nsummary in two steps. First, our word-level extractor extracts a sequence of\nimportant words (we call it the \"prototype text\") from the source text\naccording to the word-level importance scores and the length constraint.\nSecond, the prototype text is used as additional input to the encoder-decoder\nmodel, which generates a summary by jointly encoding and copying words from\nboth the prototype text and source text. Since the prototype text is a guide to\nboth the content and length of the summary, our model can generate an\ninformative and length-controlled summary. Experiments with the CNN/Daily Mail\ndataset and the NEWSROOM dataset show that our model outperformed previous\nmodels in length-controlled settings.", "published": "2020-01-21 04:01:58", "link": "http://arxiv.org/abs/2001.07331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Physical Embedding Model for Knowledge Graphs", "abstract": "Knowledge graph embedding methods learn continuous vector representations for\nentities in knowledge graphs and have been used successfully in a large number\nof applications. We present a novel and scalable paradigm for the computation\nof knowledge graph embeddings, which we dub PYKE . Our approach combines a\nphysical model based on Hooke's law and its inverse with ideas from simulated\nannealing to compute embeddings for knowledge graphs efficiently. We prove that\nPYKE achieves a linear space complexity. While the time complexity for the\ninitialization of our approach is quadratic, the time complexity of each of its\niterations is linear in the size of the input knowledge graph. Hence, PYKE's\noverall runtime is close to linear. Consequently, our approach easily scales up\nto knowledge graphs containing millions of triples. We evaluate our approach\nagainst six state-of-the-art embedding approaches on the DrugBank and DBpedia\ndatasets in two series of experiments. The first series shows that the cluster\npurity achieved by PYKE is up to 26% (absolute) better than that of the state\nof art. In addition, PYKE is more than 22 times faster than existing embedding\nsolutions in the best case. The results of our second series of experiments\nshow that PYKE is up to 23% (absolute) better than the state of art on the task\nof type prediction while maintaining its superior scalability. Our\nimplementation and results are open-source and are available at\nhttp://github.com/dice-group/PYKE.", "published": "2020-01-21 10:02:18", "link": "http://arxiv.org/abs/2001.07418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Sense Embeddings for Syntactic and Semantic Analogy for\n  Portuguese", "abstract": "Word embeddings are numerical vectors which can represent words or concepts\nin a low-dimensional continuous space. These vectors are able to capture useful\nsyntactic and semantic information. The traditional approaches like Word2Vec,\nGloVe and FastText have a strict drawback: they produce a single vector\nrepresentation per word ignoring the fact that ambiguous words can assume\ndifferent meanings. In this paper we use techniques to generate sense\nembeddings and present the first experiments carried out for Portuguese. Our\nexperiments show that sense vectors outperform traditional word vectors in\nsyntactic and semantic analogy tasks, proving that the language resource\ngenerated here can improve the performance of NLP tasks in Portuguese.", "published": "2020-01-21 14:39:20", "link": "http://arxiv.org/abs/2001.07574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "abstract": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a\npretrained language model with \"task descriptions\" in natural language (e.g.,\nRadford et al., 2019). While this approach underperforms its supervised\ncounterpart, we show in this work that the two ideas can be combined: We\nintroduce Pattern-Exploiting Training (PET), a semi-supervised training\nprocedure that reformulates input examples as cloze-style phrases to help\nlanguage models understand a given task. These phrases are then used to assign\nsoft labels to a large set of unlabeled examples. Finally, standard supervised\ntraining is performed on the resulting training set. For several tasks and\nlanguages, PET outperforms supervised training and strong semi-supervised\napproaches in low-resource settings by a large margin.", "published": "2020-01-21 17:57:33", "link": "http://arxiv.org/abs/2001.07676v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where New Words Are Born: Distributional Semantic Analysis of Neologisms\n  and Their Semantic Neighborhoods", "abstract": "We perform statistical analysis of the phenomenon of neology, the process by\nwhich new words emerge in a language, using large diachronic corpora of\nEnglish. We investigate the importance of two factors, semantic sparsity and\nfrequency growth rates of semantic neighbors, formalized in the distributional\nsemantics paradigm. We show that both factors are predictive of word emergence\nalthough we find more support for the latter hypothesis. Besides presenting a\nnew linguistic application of distributional semantics, this study tackles the\nlinguistic question of the role of language-internal factors (in our case,\nsparsity) in language change motivated by language-external factors (reflected\nin frequency growth).", "published": "2020-01-21 19:09:49", "link": "http://arxiv.org/abs/2001.07740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shared task: Lexical semantic change detection in German (Student\n  Project Report)", "abstract": "Recent NLP architectures have illustrated in various ways how semantic change\ncan be captured across time and domains. However, in terms of evaluation there\nis a lack of benchmarks to compare the performance of these systems against\neach other. We present the results of the first shared task on unsupervised\nlexical semantic change detection (LSCD) in German based on the evaluation\nframework proposed by Schlechtweg et al. (2019).", "published": "2020-01-21 21:47:27", "link": "http://arxiv.org/abs/2001.07786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems", "abstract": "In task-oriented dialogue systems the dialogue state tracker (DST) component\nis responsible for predicting the state of the dialogue based on the dialogue\nhistory. Current DST approaches rely on a predefined domain ontology, a fact\nthat limits their effective usage for large scale conversational agents, where\nthe DST constantly needs to be interfaced with ever-increasing services and\nAPIs. Focused towards overcoming this drawback, we propose a domain-aware\ndialogue state tracker, that is completely data-driven and it is modeled to\npredict for dynamic service schemas. The proposed model utilizes domain and\nslot information to extract both domain and slot specific representations for a\ngiven dialogue, and then uses such representations to predict the values of the\ncorresponding slot. Integrating this mechanism with a pretrained language model\n(i.e. BERT), our approach can effectively learn semantic relations.", "published": "2020-01-21 13:41:09", "link": "http://arxiv.org/abs/2001.07526v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R2DE: a NLP approach to estimating IRT parameters of newly generated\n  questions", "abstract": "The main objective of exams consists in performing an assessment of students'\nexpertise on a specific subject. Such expertise, also referred to as skill or\nknowledge level, can then be leveraged in different ways (e.g., to assign a\ngrade to the students, to understand whether a student might need some support,\netc.). Similarly, the questions appearing in the exams have to be assessed in\nsome way before being used to evaluate students. Standard approaches to\nquestions' assessment are either subjective (e.g., assessment by human experts)\nor introduce a long delay in the process of question generation (e.g.,\npretesting with real students). In this work we introduce R2DE (which is a\nRegressor for Difficulty and Discrimination Estimation), a model capable of\nassessing newly generated multiple-choice questions by looking at the text of\nthe question and the text of the possible choices. In particular, it can\nestimate the difficulty and the discrimination of each question, as they are\ndefined in Item Response Theory. We also present the results of extensive\nexperiments we carried out on a real world large scale dataset coming from an\ne-learning platform, showing that our model can be used to perform an initial\nassessment of newly created questions and ease some of the problems that arise\nin question generation.", "published": "2020-01-21 14:31:01", "link": "http://arxiv.org/abs/2001.07569v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving Interaction Quality Estimation with BiLSTMs and the Impact on\n  Dialogue Policy Learning", "abstract": "Learning suitable and well-performing dialogue behaviour in statistical\nspoken dialogue systems has been in the focus of research for many years. While\nmost work which is based on reinforcement learning employs an objective measure\nlike task success for modelling the reward signal, we use a reward based on\nuser satisfaction estimation. We propose a novel estimator and show that it\noutperforms all previous estimators while learning temporal dependencies\nimplicitly. Furthermore, we apply this novel user satisfaction estimation model\nlive in simulated experiments where the satisfaction estimation model is\ntrained on one domain and applied in many other domains which cover a similar\ntask. We show that applying this model results in higher estimated\nsatisfaction, similar task success rates and a higher robustness to noise.", "published": "2020-01-21 15:39:12", "link": "http://arxiv.org/abs/2001.07615v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Emergence of Pragmatics from Referential Game between Theory of Mind\n  Agents", "abstract": "Pragmatics studies how context can contribute to language meanings. In human\ncommunication, language is never interpreted out of context, and sentences can\nusually convey more information than their literal meanings. However, this\nmechanism is missing in most multi-agent systems, restricting the communication\nefficiency and the capability of human-agent interaction. In this paper, we\npropose an algorithm, using which agents can spontaneously learn the ability to\n\"read between lines\" without any explicit hand-designed rules. We integrate the\ntheory of mind (ToM) in a cooperative multi-agent pedagogical situation and\npropose an adaptive reinforcement learning (RL) algorithm to develop a\ncommunication protocol. ToM is a profound cognitive science concept, claiming\nthat people regularly reason about other's mental states, including beliefs,\ngoals, and intentions, to obtain performance advantage in competition,\ncooperation or coalition. With this ability, agents consider language as not\nonly messages but also rational acts reflecting others' hidden states. Our\nexperiments demonstrate the advantage of pragmatic protocols over non-pragmatic\nprotocols. We also show the teaching complexity following the pragmatic\nprotocol empirically approximates to recursive teaching dimension (RTD).", "published": "2020-01-21 19:37:33", "link": "http://arxiv.org/abs/2001.07752v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CheckThat! at CLEF 2020: Enabling the Automatic Identification and\n  Verification of Claims in Social Media", "abstract": "We describe the third edition of the CheckThat! Lab, which is part of the\n2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four\ncomplementary tasks and a related task from previous lab editions, offered in\nEnglish, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter\nstream are worth fact-checking. Task 2 asks to determine whether a claim posted\nin a tweet can be verified using a set of previously fact-checked claims. Task\n3 asks to retrieve text snippets from a given set of Web pages that would be\nuseful for verifying a target tweet's claim. Task 4 asks to predict the\nveracity of a target tweet's claim using a set of Web pages and potentially\nuseful snippets in them. Finally, the lab offers a fifth task that asks to\npredict the check-worthiness of the claims made in English political debates\nand speeches. CheckThat! features a full evaluation framework. The evaluation\nis carried out using mean average precision or precision at rank k for ranking\ntasks, and F1 for classification tasks.", "published": "2020-01-21 06:47:11", "link": "http://arxiv.org/abs/2001.08546v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Robust Deep Learning Framework For Predicting Respiratory Anomalies and\n  Diseases", "abstract": "This paper presents a robust deep learning framework developed to detect\nrespiratory diseases from recordings of respiratory sounds. The complete\ndetection process firstly involves front end feature extraction where\nrecordings are transformed into spectrograms that convey both spectral and\ntemporal information. Then a back-end deep learning model classifies the\nfeatures into classes of respiratory disease or anomaly. Experiments, conducted\nover the ICBHI benchmark dataset of respiratory sounds, evaluate the ability of\nthe framework to classify sounds. Two main contributions are made in this\npaper. Firstly, we provide an extensive analysis of how factors such as\nrespiratory cycle length, time resolution, and network architecture, affect\nfinal prediction accuracy. Secondly, a novel deep learning based framework is\nproposed for detection of respiratory diseases and shown to perform extremely\nwell compared to state of the art methods.", "published": "2020-01-21 15:26:52", "link": "http://arxiv.org/abs/2002.03894v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Power-Efficient Audio Acquisition System for Smart City Applications", "abstract": "Acoustic noise has adverse effects on human activities. Aside from hearing\nimpairment and stress-related illnesses, it can also interfere with spoken\ncommunication, reduce human performance and affect the quality of life. As\nurbanization is intensifying, the potential benefits of reducing noise\npollution in smart-city environments are extensive. Noise levels can be\ncollected and analyzed using a wireless sensor network which can monitor the\nnoise level by using microphones. However, every wireless system struggles in\nterms of the battery requirements needed for continuous data collection and\nmonitoring. In this paper, the design of a testbed for a smart microphone\nsystem is presented. To save power, a microcontroller and an Analog-to-Digital\nConverter (ADC) dynamically switch between high and low power modes in response\nto environmental noise. Specifically, the high powered components are triggered\nby a spike in the acoustic noise level. Three wireless technologies, WiFi (2.4\nGHz), Bluetooth Low Energy (BLE) 4.0 and Zigbee were examined. According to the\nresults, the power consumption of a node can be lowered by 97% when idle based\non the testbed.", "published": "2020-01-21 18:35:14", "link": "http://arxiv.org/abs/2001.08163v1", "categories": ["cs.NI", "cs.SD", "cs.SI", "eess.AS", "eess.SP"], "primary_category": "cs.NI"}
