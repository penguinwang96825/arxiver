{"title": "Hybrid Neural Tagging Model for Open Relation Extraction", "abstract": "Open relation extraction (ORE) remains a challenge to obtain a semantic\nrepresentation by discovering arbitrary relation tuples from the unstructured\ntext. Conventional methods heavily depend on feature engineering or syntactic\nparsing, they are inefficient or error-cascading. Recently, leveraging\nsupervised deep learning structures to address the ORE task is an\nextraordinarily promising way. However, there are two main challenges: (1) The\nlack of enough labeled corpus to support supervised training; (2) The\nexploration of specific neural architecture that adapts to the characteristics\nof open relation extracting. In this paper, to overcome these difficulties, we\nbuild a large-scale, high-quality training corpus in a fully automated way, and\ndesign a tagging scheme to assist in transforming the ORE task into a sequence\ntagging processing. Furthermore, we propose a hybrid neural network model\n(HNN4ORT) for open relation tagging. The model employs the Ordered Neurons LSTM\nto encode potential syntactic information for capturing the associations among\nthe arguments and relations. It also emerges a novel Dual Aware Mechanism,\nincluding Local-aware Attention and Global-aware Convolution. The dual aware\nnesses complement each other so that the model can take the sentence-level\nsemantics as a global perspective, and at the same time implement salient local\nfeatures to achieve sparse annotation. Experimental results on various testing\nsets show that our model can achieve state-of-the-art performances compared to\nthe conventional methods or other neural models.", "published": "2019-07-26 11:29:37", "link": "http://arxiv.org/abs/1908.01761v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations", "abstract": "We present LINSPECTOR WEB, an open source multilingual inspector to analyze\nword representations. Our system provides researchers working in low-resource\nsettings with an easily accessible web based probing tool to gain quick\ninsights into their word embeddings especially outside of the English language.\nTo do this we employ 16 simple linguistic probing tasks such as gender, case\nmarking, and tense for a diverse set of 28 languages. We support probing of\nstatic word embeddings along with pretrained AllenNLP models that are commonly\nused for NLP downstream tasks such as named entity recognition, natural\nlanguage inference and dependency parsing. The results are visualized in a\npolar chart and also provided as a table. LINSPECTOR WEB is available as an\noffline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.", "published": "2019-07-26 08:54:36", "link": "http://arxiv.org/abs/1907.11438v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Self-Attention Network for Chinese Word Segmentation", "abstract": "Neural network has become the dominant method for Chinese word segmentation.\nMost existing models cast the task as sequence labeling, using BiLSTM-CRF for\nrepresenting the input and making output predictions. Recently, attention-based\nsequence models have emerged as a highly competitive alternative to LSTMs,\nwhich allow better running speed by parallelization of computation. We\ninvestigate self attention network for Chinese word segmentation, making\ncomparisons between BiLSTM-CRF models. In addition, the influence of\ncontextualized character embeddings is investigated using BERT, and a method is\nproposed for integrating word information into SAN segmentation. Results show\nthat SAN gives highly competitive results compared with BiLSTMs, with BERT and\nword information further improving segmentation for in-domain and cross-domain\nsegmentation. Our final models give the best results for 6 heterogenous domain\nbenchmarks.", "published": "2019-07-26 12:29:37", "link": "http://arxiv.org/abs/1907.11512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.", "published": "2019-07-26 17:48:29", "link": "http://arxiv.org/abs/1907.11692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Learning Construction Injury Precursors from Text", "abstract": "In light of the increasing availability of digitally recorded safety reports\nin the construction industry, it is important to develop methods to exploit\nthese data to improve our understanding of safety incidents and ability to\nlearn from them. In this study, we compare several approaches to automatically\nlearn injury precursors from raw construction accident reports. More precisely,\nwe experiment with two state-of-the-art deep learning architectures for Natural\nLanguage Processing (NLP), Convolutional Neural Networks (CNN) and Hierarchical\nAttention Networks (HAN), and with the established Term Frequency - Inverse\nDocument Frequency representation (TF-IDF) + Support Vector Machine (SVM)\napproach. For each model, we provide a method to identify (after training) the\ntextual patterns that are, on average, the most predictive of each safety\noutcome. We show that among those pieces of text, valid injury precursors can\nbe found. The proposed methods can also be used by the user to visualize and\nunderstand the models' predictions.", "published": "2019-07-26 19:43:07", "link": "http://arxiv.org/abs/1907.11769v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised and Unsupervised Neural Approaches to Text Readability", "abstract": "We present a set of novel neural supervised and unsupervised approaches for\ndetermining the readability of documents. In the unsupervised setting, we\nleverage neural language models, whereas in the supervised setting, three\ndifferent neural classification architectures are tested. We show that the\nproposed neural unsupervised approach is robust, transferable across languages\nand allows adaptation to a specific readability task and data set. By\nsystematic comparison of several neural architectures on a number of benchmark\nand new labelled readability datasets in two languages, this study also offers\na comprehensive analysis of different neural approaches to readability\nclassification. We expose their strengths and weaknesses, compare their\nperformance to current state-of-the-art classification approaches to\nreadability, which in most cases still rely on extensive feature engineering,\nand propose possibilities for improvements.", "published": "2019-07-26 20:04:57", "link": "http://arxiv.org/abs/1907.11779v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocal Interactivity in Crowds, Flocks and Swarms: Implications for Voice\n  User Interfaces", "abstract": "Recent years have seen an explosion in the availability of Voice User\nInterfaces. However, user surveys suggest that there are issues with respect to\nusability, and it has been hypothesised that contemporary voice-enabled systems\nare missing crucial behaviours relating to user engagement and vocal\ninteractivity. However, it is well established that such ostensive behaviours\nare ubiquitous in the animal kingdom, and that vocalisation provides a means\nthrough which interaction may be coordinated and managed between individuals\nand within groups. Hence, this paper reports results from a study aimed at\nidentifying generic mechanisms that might underpin coordinated collective vocal\nbehaviour with a particular focus on closed-loop negative-feedback control as a\npowerful regulatory process. A computer-based real-time simulation of vocal\ninteractivity is described which has provided a number of insights, including\nthe enumeration of a number of key control variables that may be worthy of\nfurther investigation.", "published": "2019-07-26 16:07:20", "link": "http://arxiv.org/abs/1907.11656v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Pars-ABSA: an Aspect-based Sentiment Analysis dataset for Persian", "abstract": "Due to the increased availability of online reviews, sentiment analysis had\nbeen witnessed a booming interest from the researchers. Sentiment analysis is a\ncomputational treatment of sentiment used to extract and understand the\nopinions of authors. While many systems were built to predict the sentiment of\na document or a sentence, many others provide the necessary detail on various\naspects of the entity (i.e. aspect-based sentiment analysis). Most of the\navailable data resources were tailored to English and the other popular\nEuropean languages. Although Persian is a language with more than 110 million\nspeakers, to the best of our knowledge, there is a lack of public dataset on\naspect-based sentiment analysis for Persian. This paper provides a manually\nannotated Persian dataset, Pars-ABSA, which is verified by 3 native Persian\nspeakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827\nneutral data samples from 5,602 unique reviews. Moreover, as a baseline, this\npaper reports the performance of some state-of-the-art aspect-based sentiment\nanalysis methods with a focus on deep learning, on Pars-ABSA. The obtained\nresults are impressive compared to similar English state-of-the-art.", "published": "2019-07-26 16:19:07", "link": "http://arxiv.org/abs/1908.01815v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DLGNet: A Transformer-based Model for Dialogue Response Generation", "abstract": "Neural dialogue models, despite their successes, still suffer from lack of\nrelevance, diversity, and in many cases coherence in their generated responses.\nThese issues can attributed to reasons including (1) short-range model\narchitectures that capture limited temporal dependencies, (2) limitations of\nthe maximum likelihood training objective, (3) the concave entropy profile of\ndialogue datasets resulting in short and generic responses, and (4) the\nout-of-vocabulary problem leading to generation of a large number of <UNK>\ntokens. On the other hand, transformer-based models such as GPT-2 have\ndemonstrated an excellent ability to capture long-range structures in language\nmodeling tasks. In this paper, we present DLGNet, a transformer-based model for\ndialogue modeling. We specifically examine the use of DLGNet for multi-turn\ndialogue response generation. In our experiments, we evaluate DLGNet on the\nopen-domain Movie Triples dataset and the closed-domain Ubuntu Dialogue\ndataset. DLGNet models, although trained with only the maximum likelihood\nobjective, achieve significant improvements over state-of-the-art multi-turn\ndialogue models. They also produce best performance to date on the two datasets\nbased on several metrics, including BLEU, ROUGE, and distinct n-gram. Our\nanalysis shows that the performance improvement is mostly due to the\ncombination of (1) the long-range transformer architecture with (2) the\ninjection of random informative paddings. Other contributing factors include\nthe joint modeling of dialogue context and response, and the 100% tokenization\ncoverage from the byte pair encoding (BPE).", "published": "2019-07-26 21:53:09", "link": "http://arxiv.org/abs/1908.01841v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Domain Detection", "abstract": "In this paper we introduce domain detection as a new natural language\nprocessing task. We argue that the ability to detect textual segments which are\ndomain-heavy, i.e., sentences or phrases which are representative of and\nprovide evidence for a given domain could enhance the robustness and\nportability of various text classification applications. We propose an\nencoder-detector framework for domain detection and bootstrap classifiers with\nmultiple instance learning (MIL). The model is hierarchically organized and\nsuited to multilabel classification. We demonstrate that despite learning with\nminimal supervision, our model can be applied to text spans of different\ngranularities, languages, and genres. We also showcase the potential of domain\ndetection for text summarization.", "published": "2019-07-26 11:53:15", "link": "http://arxiv.org/abs/1907.11499v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Use/Misuse of the Term 'Phoneme'", "abstract": "The term 'phoneme' lies at the heart of speech science and technology, and\nyet it is not clear that the research community fully appreciates its meaning\nand implications. In particular, it is suspected that many researchers use the\nterm in a casual sense to refer to the sounds of speech, rather than as a well\ndefined abstract concept. If true, this means that some sections of the\ncommunity may be missing an opportunity to understand and exploit the\nimplications of this important psychological phenomenon. Here we review the\ncorrect meaning of the term 'phoneme' and report the results of an\ninvestigation into its use/misuse in the accepted papers at INTERSPEECH-2018.\nIt is confirmed that a significant proportion of the community (i) may not be\naware of the critical difference between `phonetic' and 'phonemic' levels of\ndescription, (ii) may not fully understand the significance of 'phonemic\ncontrast', and as a consequence, (iii) consistently misuse the term 'phoneme'.\nThese findings are discussed, and recommendations are made as to how this\nsituation might be mitigated.", "published": "2019-07-26 15:52:04", "link": "http://arxiv.org/abs/1907.11640v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Localization Uncertainty in Time-Amplitude Stereophonic Reproduction", "abstract": "This article studies the effects of inter-channel time and level differences\nin stereophonic reproduction on perceived localization uncertainty, which is\ndefined as how difficult it is for a listener to tell where a sound source is\nlocated. Towards this end, a computational model of localization uncertainty is\nproposed first. The model calculates inter-aural time and level difference\ncues, and compares them to those associated to free-field point-like sources.\nThe comparison is carried out using a particular distance functional that\nreplicates the increased uncertainty observed experimentally with inconsistent\ninter-aural time and level difference cues. The model is validated by formal\nlistening tests, achieving a Pearson correlation of 0.99. The model is then\nused to predict localization uncertainty for stereophonic setups and a listener\nin central and off-central positions. Results show that amplitude methods\nachieve a slightly lower localization uncertainty for a listener positioned\nexactly in the center of the sweet spot. As soon as the listener moves away\nfrom that position, the situation reverses, with time-amplitude methods\nachieving a lower localization uncertainty.", "published": "2019-07-26 08:31:27", "link": "http://arxiv.org/abs/1907.11425v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Correlation Distance Skip Connection Denoising Autoencoder (CDSK-DAE)\n  for Speech Feature Enhancement", "abstract": "Performance of learning based Automatic Speech Recognition (ASR) is\nsusceptible to noise, especially when it is introduced in the testing data\nwhile not presented in the training data. This work focuses on a feature\nenhancement for noise robust end-to-end ASR system by introducing a novel\nvariant of denoising autoencoder (DAE). The proposed method uses skip\nconnections in both encoder and decoder sides by passing speech information of\nthe target frame from input to the model. It also uses a new objective function\nin training model that uses a correlation distance measure in penalty terms by\nmeasuring dependency of the latent target features and the model (latent\nfeatures and enhanced features obtained from the DAE). Performance of the\nproposed method was compared against a conventional model and a state of the\nart model under both seen and unseen noisy environments of 7 different types of\nbackground noise with different SNR levels (0, 5, 10 and 20 dB). The proposed\nmethod also is tested using linear and non-linear penalty terms as well, where,\nthey both show an improvement on the overall average WER under noisy conditions\nboth seen and unseen in comparison to the state-of-the-art model.", "published": "2019-07-26 02:25:44", "link": "http://arxiv.org/abs/1907.11361v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
