{"title": "Terminologies augmented recurrent neural network model for clinical\n  named entity recognition", "abstract": "We aimed to enhance the performance of a supervised model for clinical\nnamed-entity recognition (NER) using medical terminologies. In order to\nevaluate our system in French, we built a corpus for 5 types of clinical\nentities. We used a terminology-based system as baseline, built upon UMLS and\nSNOMED. Then, we evaluated a biGRU-CRF, and an hybrid system using the\nprediction of the terminology-based system as feature for the biGRU-CRF. In\nEnglish, we evaluated the NER systems on the i2b2-2009 Medication Challenge for\nDrug name recognition, which contained 8,573 entities for 268 documents. In\nFrench, we built APcNER, a corpus of 147 documents annotated for 5 entities\n(drug name, sign or symptom, disease or disorder, diagnostic procedure or lab\ntest and therapeutic procedure). We evaluated each NER systems using exact and\npartial match definition of F-measure for NER. The APcNER contains 4,837\nentities which took 28 hours to annotate, the inter-annotator agreement was\nacceptable for Drug name in exact match (85%) and acceptable for other entity\ntypes in non-exact match (>70%). For drug name recognition on both i2b2-2009\nand APcNER, the biGRU-CRF performed better that the terminology-based system,\nwith an exact-match F-measure of 91.1% versus 73% and 81.9% versus 75%\nrespectively. Moreover, the hybrid system outperformed the biGRU-CRF, with an\nexact-match F-measure of 92.2% versus 91.1% (i2b2-2009) and 88.4% versus 81.9%\n(APcNER). On APcNER corpus, the micro-average F-measure of the hybrid system on\nthe 5 entities was 69.5% in exact match, and 84.1% in non-exact match. APcNER\nis a French corpus for clinical-NER of five type of entities which covers a\nlarge variety of document types. Extending supervised model with terminology\nallowed for an easy performance gain, especially in low regimes of entities,\nand established near state of the art results on the i2b2-2009 corpus.", "published": "2019-04-25 17:37:28", "link": "http://arxiv.org/abs/1904.11473v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing What Different NLP Tasks Teach Machines about Function Word\n  Comprehension", "abstract": "We introduce a set of nine challenge tasks that test for the understanding of\nfunction words. These tasks are created by structurally mutating sentences from\nexisting datasets to target the comprehension of specific types of function\nwords (e.g., prepositions, wh-words). Using these probing tasks, we explore the\neffects of various pretraining objectives for sentence encoders (e.g., language\nmodeling, CCG supertagging and natural language inference (NLI)) on the learned\nrepresentations. Our results show that pretraining on language modeling\nperforms the best on average across our probing tasks, supporting its\nwidespread use for pretraining state-of-the-art NLP models, and CCG\nsupertagging and NLI pretraining perform comparably. Overall, no pretraining\nobjective dominates across the board, and our function word probing tasks\nhighlight several intuitive differences between pretraining objectives, e.g.,\nthat NLI helps the comprehension of negation.", "published": "2019-04-25 19:15:16", "link": "http://arxiv.org/abs/1904.11544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Generation from Rich Semantic Representations", "abstract": "We propose neural models to generate high-quality text from structured\nrepresentations based on Minimal Recursion Semantics (MRS). MRS is a rich\nsemantic representation that encodes more precise semantic detail than other\nrepresentations such as Abstract Meaning Representation (AMR). We show that a\nsequence-to-sequence model that maps a linearization of Dependency MRS, a\ngraph-based representation of MRS, to English text can achieve a BLEU score of\n66.11 when trained on gold data. The performance can be improved further using\na high-precision, broad coverage grammar-based parser to generate a large\nsilver training corpus, achieving a final BLEU score of 77.17 on the full test\nset, and 83.37 on the subset of test data most closely matching the silver data\ndomain. Our results suggest that MRS-based representations are a good choice\nfor applications that need both structured semantics and the ability to produce\nnatural language text as output.", "published": "2019-04-25 20:01:08", "link": "http://arxiv.org/abs/1904.11564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Text Diacritization Using Deep Neural Networks", "abstract": "Diacritization of Arabic text is both an interesting and a challenging\nproblem at the same time with various applications ranging from speech\nsynthesis to helping students learning the Arabic language. Like many other\ntasks or problems in Arabic language processing, the weak efforts invested into\nthis problem and the lack of available (open-source) resources hinder the\nprogress towards solving this problem. This work provides a critical review for\nthe currently existing systems, measures and resources for Arabic text\ndiacritization. Moreover, it introduces a much-needed free-for-all cleaned\ndataset that can be easily used to benchmark any work on Arabic diacritization.\nExtracted from the Tashkeela Corpus, the dataset consists of 55K lines\ncontaining about 2.3M words. After constructing the dataset, existing tools and\nsystems are tested on it. The results of the experiments show that the neural\nShakkala system significantly outperforms traditional rule-based approaches and\nother closed-source tools with a Diacritic Error Rate (DER) of 2.88% compared\nwith 13.78%, which the best DER for the non-neural approach (obtained by the\nMishkal tool).", "published": "2019-04-25 12:29:06", "link": "http://arxiv.org/abs/1905.01965v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Look Who's Talking: Inferring Speaker Attributes from Personal\n  Longitudinal Dialog", "abstract": "We examine a large dialog corpus obtained from the conversation history of a\nsingle individual with 104 conversation partners. The corpus consists of half a\nmillion instant messages, across several messaging platforms. We focus our\nanalyses on seven speaker attributes, each of which partitions the set of\nspeakers, namely: gender; relative age; family member; romantic partner;\nclassmate; co-worker; and native to the same country. In addition to the\ncontent of the messages, we examine conversational aspects such as the time\nmessages are sent, messaging frequency, psycholinguistic word categories,\nlinguistic mirroring, and graph-based features reflecting how people in the\ncorpus mention each other. We present two sets of experiments predicting each\nattribute using (1) short context windows; and (2) a larger set of messages. We\nfind that using all features leads to gains of 9-14% over using message text\nonly.", "published": "2019-04-25 22:12:43", "link": "http://arxiv.org/abs/1904.11610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Zero Resource Speech Challenge 2019: TTS without T", "abstract": "We present the Zero Resource Speech Challenge 2019, which proposes to build a\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\n(text-to-speech without text). We provide raw audio for a target voice in an\nunknown language (the Voice dataset), but no alignment, text or labels.\nParticipants must discover subword units in an unsupervised way (using the Unit\nDiscovery dataset) and align them to the voice recordings in a way that works\nbest for the purpose of synthesizing novel utterances from novel speakers,\nsimilar to the target speaker's voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit discovery\nplus a standard TTS system, and a topline TTS using gold phoneme\ntranscriptions. We present an overview of the 19 submitted systems from 10\nteams and discuss the main results.", "published": "2019-04-25 17:29:16", "link": "http://arxiv.org/abs/1904.11469v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Importance of Copying Mechanism for News Headline Generation", "abstract": "News headline generation is an essential problem of text summarization\nbecause it is constrained, well-defined, and is still hard to solve. Models\nwith a limited vocabulary can not solve it well, as new named entities can\nappear regularly in the news and these entities often should be in the\nheadline. News articles in morphologically rich languages such as Russian\nrequire model modifications due to a large number of possible word forms. This\nstudy aims to validate that models with a possibility of copying words from the\noriginal article performs better than models without such an option. The\nproposed model achieves a mean ROUGE score of 23 on the provided test dataset,\nwhich is 8 points greater than the result of a similar model without a copying\nmechanism. Moreover, the resulting model performs better than any known model\non the new dataset of Russian news.", "published": "2019-04-25 17:39:01", "link": "http://arxiv.org/abs/1904.11475v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "abstract": "We present the task of Spatio-Temporal Video Question Answering, which\nrequires intelligent systems to simultaneously retrieve relevant moments and\ndetect referenced visual concepts (people and objects) to answer natural\nlanguage questions about videos. We first augment the TVQA dataset with 310.8K\nbounding boxes, linking depicted objects to visual concepts in questions and\nanswers. We name this augmented version as TVQA+. We then propose\nSpatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework\nthat grounds evidence in both spatial and temporal domains to answer questions\nabout videos. Comprehensive experiments and analyses demonstrate the\neffectiveness of our framework and how the rich annotations in our TVQA+\ndataset can contribute to the question answering task. Moreover, by performing\nthis joint task, our model is able to produce insightful and interpretable\nspatio-temporal attention visualizations. Dataset and code are publicly\navailable at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus", "published": "2019-04-25 20:37:26", "link": "http://arxiv.org/abs/1904.11574v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Latent Class Model with Application to Speaker Diarization", "abstract": "In this paper, we apply a latent class model (LCM) to the task of speaker\ndiarization. LCM is similar to Patrick Kenny's variational Bayes (VB) method in\nthat it uses soft information and avoids premature hard decisions in its\niterations. In contrast to the VB method, which is based on a generative model,\nLCM provides a framework allowing both generative and discriminative models.\nThe discriminative property is realized through the use of i-vector (Ivec),\nprobabilistic linear discriminative analysis (PLDA), and a support vector\nmachine (SVM) in this work. Systems denoted as LCM-Ivec-PLDA, LCM-Ivec-SVM, and\nLCM-Ivec-Hybrid are introduced. In addition, three further improvements are\napplied to enhance its performance. 1) Adding neighbor windows to extract more\nspeaker information for each short segment. 2) Using a hidden Markov model to\navoid frequent speaker change points. 3) Using an agglomerative hierarchical\ncluster to do initialization and present hard and soft priors, in order to\novercome the problem of initial sensitivity. Experiments on the National\nInstitute of Standards and Technology Rich Transcription 2009 speaker\ndiarization database, under the condition of a single distant microphone, show\nthat the diarization error rate (DER) of the proposed methods has substantial\nrelative improvements compared with mainstream systems. Compared to the VB\nmethod, the relative improvements of LCM-Ivec-PLDA, LCM-Ivec-SVM, and\nLCM-Ivec-Hybrid systems are 23.5%, 27.1%, and 43.0%, respectively. Experiments\non our collected database, CALLHOME97, CALLHOME00 and SRE08 short2-summed trial\nconditions also show that the proposed LCM-Ivec-Hybrid system has the best\noverall performance.", "published": "2019-04-25 02:35:16", "link": "http://arxiv.org/abs/1904.11130v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural\n  Speaker Separation", "abstract": "We address talker-independent monaural speaker separation from the\nperspectives of deep learning and computational auditory scene analysis (CASA).\nSpecifically, we decompose the multi-speaker separation task into the stages of\nsimultaneous grouping and sequential grouping. Simultaneous grouping is first\nperformed in each time frame by separating the spectra of different speakers\nwith a permutation-invariantly trained neural network. In the second stage, the\nframe-level separated spectra are sequentially grouped to different speakers by\na clustering network. The proposed deep CASA approach optimizes frame-level\nseparation and speaker tracking in turn, and produces excellent results for\nboth objectives. Experimental results on the benchmark WSJ0-2mix database show\nthat the new approach achieves the state-of-the-art results with a modest model\nsize.", "published": "2019-04-25 03:57:11", "link": "http://arxiv.org/abs/1904.11148v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Machine Learning For Distributed Acoustic Sensors, Classic versus Image\n  and Deep Neural Networks Approach", "abstract": "Distributed Acoustic Sensing (DAS) using fiber optic cables is a promising\nnew technology for pipeline monitoring and protection. In this work, we applied\nand compared two approaches for event detection using DAS: Classic machine\nlearning approach and the approach based on image processing and deep learning.\nAlthough with both approaches acceptable performance can be achieved, the\npreliminary results show that image based deep learning is more promising\napproach, offering six times lower event detection delay and twelve times lower\nexecution time.", "published": "2019-04-25 19:18:31", "link": "http://arxiv.org/abs/1904.11546v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
