{"title": "In-Order Transition-based Constituent Parsing", "abstract": "Both bottom-up and top-down strategies have been used for neural\ntransition-based constituent parsing. The parsing strategies differ in terms of\nthe order in which they recognize productions in the derivation tree, where\nbottom-up strategies and top-down strategies take post-order and pre-order\ntraversal over trees, respectively. Bottom-up parsers benefit from rich\nfeatures from readily built partial parses, but lack lookahead guidance in the\nparsing process; top-down parsers benefit from non-local guidance for local\ndecisions, but rely on a strong encoder over the input to predict a constituent\nhierarchy before its construction.To mitigate both issues, we propose a novel\nparsing system based on in-order traversal over syntactic trees, designing a\nset of transition actions to find a compromise between bottom-up constituent\ninformation and top-down lookahead information. Based on stack-LSTM, our\npsycholinguistically motivated constituent parsing system achieves 91.8 F1 on\nWSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised\nreranking and 94.2 F1 with semi-supervised reranking, which are the best\nresults on the WSJ benchmark.", "published": "2017-07-17 04:27:11", "link": "http://arxiv.org/abs/1707.05000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Bidirectional Hierarchical Representations for Attention-Based\n  Neural Machine Translation", "abstract": "This paper proposes a hierarchical attentional neural translation model which\nfocuses on enhancing source-side hierarchical representations by covering both\nlocal and global semantic information using a bidirectional tree-based encoder.\nTo maximize the predictive likelihood of target words, a weighted variant of an\nattention mechanism is used to balance the attentive information between\nlexical and phrase vectors. Using a tree-based rare word encoding, the proposed\nmodel is extended to sub-word level to alleviate the out-of-vocabulary (OOV)\nproblem. Empirical results reveal that the proposed model significantly\noutperforms sequence-to-sequence attention-based and tree-based neural\ntranslation models in English-Chinese translation tasks.", "published": "2017-07-17 12:09:08", "link": "http://arxiv.org/abs/1707.05114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Normalize, or Not to Normalize: The Impact of Normalization on\n  Part-of-Speech Tagging", "abstract": "Does normalization help Part-of-Speech (POS) tagging accuracy on noisy,\nnon-canonical data? To the best of our knowledge, little is known on the actual\nimpact of normalization in a real-world scenario, where gold error detection is\nnot available. We investigate the effect of automatic normalization on POS\ntagging of tweets. We also compare normalization to strategies that leverage\nlarge amounts of unlabeled data kept in its raw form. Our results show that\nnormalization helps, but does not add consistently beyond just word embedding\nlayer initialization. The latter approach yields a tagging model that is\ncompetitive with a Twitter state-of-the-art tagger.", "published": "2017-07-17 12:20:53", "link": "http://arxiv.org/abs/1707.05116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task", "abstract": "This paper presents the LIG-CRIStAL submission to the shared Automatic Post-\nEditing task of WMT 2017. We propose two neural post-editing models: a\nmonosource model with a task-specific attention mechanism, which performs\nparticularly well in a low-resource scenario; and a chained architecture which\nmakes use of the source sentence to provide extra context. This latter\narchitecture manages to slightly improve our results when more training data is\navailable. We present and discuss our results on two datasets (en-de and de-en)\nthat are made available for the task.", "published": "2017-07-17 12:25:52", "link": "http://arxiv.org/abs/1707.05118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Reranking for Named Entity Recognition", "abstract": "We propose a neural reranking system for named entity recognition (NER). The\nbasic idea is to leverage recurrent neural network models to learn\nsentence-level patterns that involve named entity mentions. In particular,\ngiven an output sentence produced by a baseline NER model, we replace all\nentity mentions, such as \\textit{Barack Obama}, into their entity types, such\nas \\textit{PER}. The resulting sentence patterns contain direct output\ninformation, yet is less sparse without specific named entities. For example,\n\"PER was born in LOC\" can be such a pattern. LSTM and CNN structures are\nutilised for learning deep representations of such sentences for reranking.\nResults show that our system can significantly improve the NER accuracies over\ntwo different baselines, giving the best reported results on a standard\nbenchmark.", "published": "2017-07-17 12:57:53", "link": "http://arxiv.org/abs/1707.05127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring text datasets by visualizing relevant words", "abstract": "When working with a new dataset, it is important to first explore and\nfamiliarize oneself with it, before applying any advanced machine learning\nalgorithms. However, to the best of our knowledge, no tools exist that quickly\nand reliably give insight into the contents of a selection of documents with\nrespect to what distinguishes them from other documents belonging to different\ncategories. In this paper we propose to extract `relevant words' from a\ncollection of texts, which summarize the contents of documents belonging to a\ncertain class (or discovered cluster in the case of unlabeled datasets), and\nvisualize them in word clouds to allow for a survey of salient features at a\nglance. We compare three methods for extracting relevant words and demonstrate\nthe usefulness of the resulting word clouds by providing an overview of the\nclasses contained in a dataset of scientific publications as well as by\ndiscovering trending topics from recent New York Times article snippets.", "published": "2017-07-17 16:12:34", "link": "http://arxiv.org/abs/1707.05261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Language Model based on PMI Matrix Approximations", "abstract": "In this study, we introduce a new approach for learning language models by\ntraining them to estimate word-context pointwise mutual information (PMI), and\nthen deriving the desired conditional probabilities from PMI at test time.\nSpecifically, we show that with minor modifications to word2vec's algorithm, we\nget principled language models that are closely related to the well-established\nNoise Contrastive Estimation (NCE) based language models. A compelling aspect\nof our approach is that our models are trained with the same simple negative\nsampling objective function that is commonly used in word2vec to learn word\nembeddings.", "published": "2017-07-17 16:21:46", "link": "http://arxiv.org/abs/1707.05266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAG: A Multilingual, Knowledge-base Agnostic and Deterministic Entity\n  Linking Approach", "abstract": "Entity linking has recently been the subject of a significant body of\nresearch. Currently, the best performing approaches rely on trained\nmono-lingual models. Porting these approaches to other languages is\nconsequently a difficult endeavor as it requires corresponding training data\nand retraining of the models. We address this drawback by presenting a novel\nmultilingual, knowledge-based agnostic and deterministic approach to entity\nlinking, dubbed MAG. MAG is based on a combination of context-based retrieval\non structured knowledge bases and graph algorithms. We evaluate MAG on 23 data\nsets and in 7 languages. Our results show that the best approach trained on\nEnglish datasets (PBOH) achieves a micro F-measure that is up to 4 times worse\non datasets in other languages. MAG, on the other hand, achieves\nstate-of-the-art performance on English datasets and reaches a micro F-measure\nthat is up to 0.6 higher than that of PBOH on non-English languages.", "published": "2017-07-17 17:14:52", "link": "http://arxiv.org/abs/1707.05288v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Iterative Deep Learning of Speech Features and Acoustic\n  Tokens with Applications to Spoken Term Detection", "abstract": "In this paper we aim to automatically discover high quality frame-level\nspeech features and acoustic tokens directly from unlabeled speech data. A\nMulti-granular Acoustic Tokenizer (MAT) was proposed for automatic discovery of\nmultiple sets of acoustic tokens from the given corpus. Each acoustic token set\nis specified by a set of hyperparameters describing the model configuration.\nThese different sets of acoustic tokens carry different characteristics for the\ngiven corpus and the language behind, thus can be mutually reinforced. The\nmultiple sets of token labels are then used as the targets of a Multi-target\nDeep Neural Network (MDNN) trained on frame-level acoustic features. Bottleneck\nfeatures extracted from the MDNN are then used as the feedback input to the MAT\nand the MDNN itself in the next iteration. The multi-granular acoustic token\nsets and the frame-level speech features can be iteratively optimized in the\niterative deep learning framework. We call this framework the Multi-granular\nAcoustic Tokenizing Deep Neural Network (MATDNN). The results were evaluated\nusing the metrics and corpora defined in the Zero Resource Speech Challenge\norganized at Interspeech 2015, and improved performance was obtained with a set\nof experiments of query-by-example spoken term detection on the same corpora.\nVisualization for the discovered tokens against the English phonemes was also\nshown.", "published": "2017-07-17 10:20:15", "link": "http://arxiv.org/abs/1707.05315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Question Answering with Memory-Augmented Networks", "abstract": "In this paper, we exploit a memory-augmented neural network to predict\naccurate answers to visual questions, even when those answers occur rarely in\nthe training set. The memory network incorporates both internal and external\nmemory blocks and selectively pays attention to each training exemplar. We show\nthat memory-augmented neural networks are able to maintain a relatively\nlong-term memory of scarce training exemplars, which is important for visual\nquestion answering due to the heavy-tailed distribution of answers in a general\nVQA setting. Experimental results on two large-scale benchmark datasets show\nthe favorable performance of the proposed algorithm with a comparison to state\nof the art.", "published": "2017-07-17 00:42:56", "link": "http://arxiv.org/abs/1707.04968v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Iris: A Conversational Agent for Complex Tasks", "abstract": "Today's conversational agents are restricted to simple standalone commands.\nIn this paper, we present Iris, an agent that draws on human conversational\nstrategies to combine commands, allowing it to perform more complex tasks that\nit has not been explicitly designed to support: for example, composing one\ncommand to \"plot a histogram\" with another to first \"log-transform the data\".\nTo enable this complexity, we introduce a domain specific language that\ntransforms commands into automata that Iris can compose, sequence, and execute\ndynamically by interacting with a user through natural language, as well as a\nconversational type system that manages what kinds of commands can be combined.\nWe have designed Iris to help users with data science tasks, a domain that\nrequires support for command combination. In evaluation, we find that data\nscientists complete a predictive modeling task significantly faster (2.6 times\nspeedup) with Iris than a modern non-conversational programming environment.\nIris supports the same kinds of commands as today's agents, but empowers users\nto weave together these commands to accomplish complex goals.", "published": "2017-07-17 06:55:43", "link": "http://arxiv.org/abs/1707.05015v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Artificial Error Generation with Machine Translation and Syntactic\n  Patterns", "abstract": "Shortage of available training data is holding back progress in the area of\nautomated error detection. This paper investigates two alternative methods for\nartificially generating writing errors, in order to create additional\nresources. We propose treating error generation as a machine translation task,\nwhere grammatically correct text is translated to contain errors. In addition,\nwe explore a system for extracting textual patterns from an annotated corpus,\nwhich can then be used to insert errors into grammatically correct sentences.\nOur experiments show that the inclusion of artificially generated errors\nsignificantly improves error detection accuracy on both FCE and CoNLL 2014\ndatasets.", "published": "2017-07-17 15:38:09", "link": "http://arxiv.org/abs/1707.05236v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "Learning to select data for transfer learning with Bayesian Optimization", "abstract": "Domain similarity measures can be used to gauge adaptability and select\nsuitable data for transfer learning, but existing approaches define ad hoc\nmeasures that are deemed suitable for respective tasks. Inspired by work on\ncurriculum learning, we propose to \\emph{learn} data selection measures using\nBayesian Optimization and evaluate them across models, domains and tasks. Our\nlearned measures outperform existing domain similarity measures significantly\non three tasks: sentiment analysis, part-of-speech tagging, and parsing. We\nshow the importance of complementing similarity with diversity, and that\nlearned measures are -- to some degree -- transferable across models, domains,\nand even tasks.", "published": "2017-07-17 15:53:18", "link": "http://arxiv.org/abs/1707.05246v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Power of Constraint Grammars Revisited", "abstract": "Sequential Constraint Grammar (SCG) (Karlsson, 1990) and its extensions have\nlacked clear connections to formal language theory. The purpose of this article\nis to lay a foundation for these connections by simplifying the definition of\nstrings processed by the grammar and by showing that Nonmonotonic SCG is\nundecidable and that derivations similar to the Generative Phonology exist. The\ncurrent investigations propose resource bounds that restrict the generative\npower of SCG to a subset of context sensitive languages and present a strong\nfinite-state condition for grammars as wholes. We show that a grammar is\nequivalent to a finite-state transducer if it is implemented with a Turing\nmachine that runs in o(n log n) time. This condition opens new finite-state\nhypotheses and avenues for deeper analysis of SCG instances in the way inspired\nby Finite-State Phonology.", "published": "2017-07-17 12:20:15", "link": "http://arxiv.org/abs/1707.05115v1", "categories": ["cs.FL", "cs.AI", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Auxiliary Objectives for Neural Error Detection Models", "abstract": "We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance.", "published": "2017-07-17 15:24:09", "link": "http://arxiv.org/abs/1707.05227v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "Detecting Off-topic Responses to Visual Prompts", "abstract": "Automated methods for essay scoring have made great progress in recent years,\nachieving accuracies very close to human annotators. However, a known weakness\nof such automated scorers is not taking into account the semantic relevance of\nthe submitted text. While there is existing work on detecting answer relevance\ngiven a textual prompt, very little previous research has been done to\nincorporate visual writing prompts. We propose a neural architecture and\nseveral extensions for detecting off-topic responses to visual prompts and\nevaluate it on a dataset of texts written by language learners.", "published": "2017-07-17 15:31:20", "link": "http://arxiv.org/abs/1707.05233v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "graph2vec: Learning Distributed Representations of Graphs", "abstract": "Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels.", "published": "2017-07-17 05:09:03", "link": "http://arxiv.org/abs/1707.05005v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.NE", "cs.SE"], "primary_category": "cs.AI"}
