{"title": "Predicting Job Titles from Job Descriptions with Multi-label Text\n  Classification", "abstract": "Finding a suitable job and hunting for eligible candidates are important to\njob seeking and human resource agencies. With the vast information about job\ndescriptions, employees and employers need assistance to automatically detect\njob titles based on job description texts. In this paper, we propose the\nmulti-label classification approach for predicting relevant job titles from job\ndescription texts, and implement the Bi-GRU-LSTM-CNN with different pre-trained\nlanguage models to apply for the job titles prediction problem. The BERT with\nmultilingual pre-trained model obtains the highest result by F1-scores on both\ndevelopment and test sets, which are 62.20% on the development set, and 47.44%\non the test set.", "published": "2021-12-21 09:31:03", "link": "http://arxiv.org/abs/2112.11052v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrast and Generation Make BART a Good Dialogue Emotion Recognizer", "abstract": "In dialogue systems, utterances with similar semantics may have distinctive\nemotions under different contexts. Therefore, modeling long-range contextual\nemotional relationships with speaker dependency plays a crucial part in\ndialogue emotion recognition. Meanwhile, distinguishing the different emotion\ncategories is non-trivial since they usually have semantically similar\nsentiments. To this end, we adopt supervised contrastive learning to make\ndifferent emotions mutually exclusive to identify similar emotions better.\nMeanwhile, we utilize an auxiliary response generation task to enhance the\nmodel's ability of handling context information, thereby forcing the model to\nrecognize emotions with similar semantics in diverse contexts. To achieve these\nobjectives, we use the pre-trained encoder-decoder model BART as our backbone\nmodel since it is very suitable for both understanding and generation tasks.\nThe experiments on four datasets demonstrate that our proposed model obtains\nsignificantly more favorable results than the state-of-the-art model in\ndialogue emotion recognition. The ablation study further demonstrates the\neffectiveness of supervised contrastive loss and generative loss.", "published": "2021-12-21 13:38:00", "link": "http://arxiv.org/abs/2112.11202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Using Gaze Behaviour for Natural Language Processing", "abstract": "Gaze behaviour has been used as a way to gather cognitive information for a\nnumber of years. In this paper, we discuss the use of gaze behaviour in solving\ndifferent tasks in natural language processing (NLP) without having to record\nit at test time. This is because the collection of gaze behaviour is a costly\ntask, both in terms of time and money. Hence, in this paper, we focus on\nresearch done to alleviate the need for recording gaze behaviour at run time.\nWe also mention different eye tracking corpora in multiple languages, which are\ncurrently available and can be used in natural language processing. We conclude\nour paper by discussing applications in a domain - education - and how learning\ngaze behaviour can help in solving the tasks of complex word identification and\nautomatic essay grading.", "published": "2021-12-21 15:52:56", "link": "http://arxiv.org/abs/2112.15471v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Cross-Lingual Retrieval with Multilingual Text Encoders", "abstract": "In this work we present a systematic empirical study focused on the\nsuitability of the state-of-the-art multilingual encoders for cross-lingual\ndocument and sentence retrieval tasks across a number of diverse language\npairs. We first treat these models as multilingual text encoders and benchmark\ntheir performance in unsupervised ad-hoc sentence- and document-level CLIR. In\ncontrast to supervised language understanding, our results indicate that for\nunsupervised document-level CLIR -- a setup with no relevance judgments for\nIR-specific fine-tuning -- pretrained multilingual encoders on average fail to\nsignificantly outperform earlier models based on CLWEs. For sentence-level\nretrieval, we do obtain state-of-the-art performance: the peak scores, however,\nare met by multilingual encoders that have been further specialized, in a\nsupervised fashion, for sentence understanding tasks, rather than using their\nvanilla 'off-the-shelf' variants. Following these results, we introduce\nlocalized relevance matching for document-level CLIR, where we independently\nscore a query against document sections. In the second part, we evaluate\nmultilingual encoders fine-tuned in a supervised fashion (i.e., we learn to\nrank) on English relevance data in a series of zero-shot language and domain\ntransfer CLIR experiments. Our results show that supervised re-ranking rarely\nimproves the performance of multilingual transformers as unsupervised base\nrankers. Finally, only with in-domain contrastive fine-tuning (i.e., same\ndomain, only language transfer), we manage to improve the ranking quality. We\nuncover substantial empirical differences between cross-lingual retrieval\nresults and results of (zero-shot) cross-lingual transfer for monolingual\nretrieval in target languages, which point to \"monolingual overfitting\" of\nretrieval models trained on monolingual data.", "published": "2021-12-21 08:10:27", "link": "http://arxiv.org/abs/2112.11031v1", "categories": ["cs.CL", "cs.IR", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "An Inference Approach To Question Answering Over Knowledge Graphs", "abstract": "Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.", "published": "2021-12-21 10:07:55", "link": "http://arxiv.org/abs/2112.11070v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Task-oriented Dialogue Systems: performance vs. quality-optima, a review", "abstract": "Task-oriented dialogue systems (TODS) are continuing to rise in popularity as\nvarious industries find ways to effectively harness their capabilities, saving\nboth time and money. However, even state-of-the-art TODS are not yet reaching\ntheir full potential. TODS typically have a primary design focus on completing\nthe task at hand, so the metric of task-resolution should take priority. Other\nconversational quality attributes that may point to the success, or otherwise,\nof the dialogue, may be ignored. This can cause interactions between human and\ndialogue system that leave the user dissatisfied or frustrated. This paper\nexplores the literature on evaluative frameworks of dialogue systems and the\nrole of conversational quality attributes in dialogue systems, looking at if,\nhow, and where they are utilised, and examining their correlation with the\nperformance of the dialogue system.", "published": "2021-12-21 13:16:24", "link": "http://arxiv.org/abs/2112.11176v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supervised Graph Contrastive Pretraining for Text Classification", "abstract": "Contrastive pretraining techniques for text classification has been largely\nstudied in an unsupervised setting. However, oftentimes labeled data from\nrelated tasks which share label semantics with current task is available. We\nhypothesize that using this labeled data effectively can lead to better\ngeneralization on current task. In this paper, we propose a novel way to\neffectively utilize labeled data from related tasks with a graph based\nsupervised contrastive learning approach. We formulate a token-graph by\nextrapolating the supervised information from examples to tokens. Our\nformulation results in an embedding space where tokens with high/low\nprobability of belonging to same class are near/further-away from one another.\nWe also develop detailed theoretical insights which serve as a motivation for\nour method. In our experiments with $13$ datasets, we show our method\noutperforms pretraining schemes by $2.5\\%$ and also example-level contrastive\nlearning based formulation by $1.8\\%$ on average. In addition, we show\ncross-domain effectiveness of our method in a zero-shot setting by $3.91\\%$ on\naverage. Lastly, we also demonstrate our method can be used as a noisy teacher\nin a knowledge distillation setting to significantly improve performance of\ntransformer based models in low labeled data regime by $4.57\\%$ on average.", "published": "2021-12-21 17:47:14", "link": "http://arxiv.org/abs/2112.11389v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Morphological classifiers", "abstract": "This work proposes a new type of classifier called Morphological Classifier\n(MC). MCs aggregate concepts from mathematical morphology and supervised\nlearning. The outcomes of this aggregation are classifiers that may preserve\nshape characteristics of classes, subject to the choice of a stopping criterion\nand structuring element. MCs are fundamentally based on set theory, and their\nclassification model can be a mathematical set itself. Two types of\nmorphological classifiers are proposed in the current work, namely,\nMorphological k-NN (MkNN) and Morphological Dilation Classifier (MDC), which\ndemonstrate the feasibility of the approach. This work provides evidence\nregarding the advantages of MCs, e.g., very fast classification times as well\nas competitive accuracy rates. The performance of MkNN and MDC was tested using\np -dimensional datasets. MCs tied or outperformed 14 well established\nclassifiers in 5 out of 8 datasets. In all occasions, the obtained accuracies\nwere higher than the average accuracy obtained with all classifiers. Moreover,\nthe proposed implementations utilize the power of the Graphics Processing Units\n(GPUs) to speed up processing.", "published": "2021-12-21 17:35:06", "link": "http://arxiv.org/abs/2112.12262v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Entity Tagging with Multimodal Knowledge Base", "abstract": "To enhance research on multimodal knowledge base and multimodal information\nprocessing, we propose a new task called multimodal entity tagging (MET) with a\nmultimodal knowledge base (MKB). We also develop a dataset for the problem\nusing an existing MKB. In an MKB, there are entities and their associated texts\nand images. In MET, given a text-image pair, one uses the information in the\nMKB to automatically identify the related entity in the text-image pair. We\nsolve the task by using the information retrieval paradigm and implement\nseveral baselines using state-of-the-art methods in NLP and CV. We conduct\nextensive experiments and make analyses on the experimental results. The\nresults show that the task is challenging, but current technologies can achieve\nrelatively high performance. We will release the dataset, code, and models for\nfuture research.", "published": "2021-12-21 15:04:57", "link": "http://arxiv.org/abs/2201.00693v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "DB-BERT: a Database Tuning Tool that \"Reads the Manual\"", "abstract": "DB-BERT is a database tuning tool that exploits information gained via\nnatural language analysis of manuals and other relevant text documents. It uses\ntext to identify database system parameters to tune as well as recommended\nparameter values. DB-BERT applies large, pre-trained language models\n(specifically, the BERT model) for text analysis. During an initial training\nphase, it fine-tunes model weights in order to translate natural language hints\ninto recommended settings. At run time, DB-BERT learns to aggregate, adapt, and\nprioritize hints to achieve optimal performance for a specific database system\nand benchmark. Both phases are iterative and use reinforcement learning to\nguide the selection of tuning settings to evaluate (penalizing settings that\nthe database system rejects while rewarding settings that improve performance).\nIn our experiments, we leverage hundreds of text documents about database\ntuning as input for DB-BERT. We compare DB-BERT against various baselines,\nconsidering different benchmarks (TPC-C and TPC-H), metrics (throughput and run\ntime), as well as database systems (Postgres and MySQL). In all cases, DB-BERT\nfinds the best parameter settings among all compared methods. The code of\nDB-BERT is available online at https://itrummer.github.io/dbbert/.", "published": "2021-12-21 01:04:08", "link": "http://arxiv.org/abs/2112.10925v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Regularizing End-to-End Speech Translation with Triangular Decomposition\n  Agreement", "abstract": "End-to-end speech-to-text translation (E2E-ST) is becoming increasingly\npopular due to the potential of its less error propagation, lower latency, and\nfewer parameters. Given the triplet training corpus $\\langle speech,\ntranscription, translation\\rangle$, the conventional high-quality E2E-ST system\nleverages the $\\langle speech, transcription\\rangle$ pair to pre-train the\nmodel and then utilizes the $\\langle speech, translation\\rangle$ pair to\noptimize it further. However, this process only involves two-tuple data at each\nstage, and this loose coupling fails to fully exploit the association between\ntriplet data. In this paper, we attempt to model the joint probability of\ntranscription and translation based on the speech input to directly leverage\nsuch triplet data. Based on that, we propose a novel regularization method for\nmodel training to improve the agreement of dual-path decomposition within\ntriplet data, which should be equal in theory. To achieve this goal, we\nintroduce two Kullback-Leibler divergence regularization terms into the model\ntraining objective to reduce the mismatch between output probabilities of\ndual-path. Then the well-trained model can be naturally transformed as the\nE2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C\nbenchmark demonstrate that our proposed approach significantly outperforms\nstate-of-the-art E2E-ST baselines on all 8 language pairs, while achieving\nbetter performance in the automatic speech recognition task. Our code is\nopen-sourced at https://github.com/duyichao/E2E-ST-TDA.", "published": "2021-12-21 05:24:01", "link": "http://arxiv.org/abs/2112.10991v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An ASP-based Approach to Answering Natural Language Questions for Texts", "abstract": "An approach based on answer set programming (ASP) is proposed in this paper\nfor representing knowledge generated from natural language texts. Knowledge in\na text is modeled using a Neo Davidsonian-like formalism, which is then\nrepresented as an answer set program. Relevant commonsense knowledge is\nadditionally imported from resources such as WordNet and represented in ASP.\nThe resulting knowledge-base can then be used to perform reasoning with the\nhelp of an ASP system. This approach can facilitate many natural language tasks\nsuch as automated question answering, text summarization, and automated\nquestion generation. ASP-based representation of techniques such as default\nreasoning, hierarchical knowledge organization, preferences over defaults,\netc., are used to model commonsense reasoning methods required to accomplish\nthese tasks. In this paper, we describe the CASPR system that we have developed\nto automate the task of answering natural language questions given English\ntext. CASPR can be regarded as a system that answers questions by\n\"understanding\" the text and has been tested on the SQuAD data set, with\npromising results.", "published": "2021-12-21 14:13:06", "link": "http://arxiv.org/abs/2112.11241v1", "categories": ["cs.CL", "cs.LO", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Voice Quality and Pitch Features in Transformer-Based Speech Recognition", "abstract": "Jitter and shimmer measurements have shown to be carriers of voice quality\nand prosodic information which enhance the performance of tasks like speaker\nrecognition, diarization or automatic speech recognition (ASR). However, such\nfeatures have been seldom used in the context of neural-based ASR, where\nspectral features often prevail. In this work, we study the effects of\nincorporating voice quality and pitch features altogether and separately to a\nTransformer-based ASR model, with the intuition that the attention mechanisms\nmight exploit latent prosodic traits. For doing so, we propose separated\nconvolutional front-ends for prosodic and spectral features, showing that this\narchitectural choice yields better results than simple concatenation of such\npitch and voice quality features to mel-spectrogram filterbanks. Furthermore,\nwe find mean Word Error Rate relative reductions of up to 5.6% with the\nLibriSpeech benchmark. Such findings motivate further research on the\napplication of prosody knowledge for increasing the robustness of\nTransformer-based ASR.", "published": "2021-12-21 17:49:06", "link": "http://arxiv.org/abs/2112.11391v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sentence Embeddings and High-speed Similarity Search for Fast Computer\n  Assisted Annotation of Legal Documents", "abstract": "Human-performed annotation of sentences in legal documents is an important\nprerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is\noften time consuming and, hence, expensive. In this paper, we introduce a\nproof-of-concept system for annotating sentences \"laterally.\" The approach is\nbased on the observation that sentences that are similar in meaning often have\nthe same label in terms of a particular type system. We use this observation in\nallowing annotators to quickly view and annotate sentences that are\nsemantically similar to a given sentence, across an entire corpus of documents.\nHere, we present the interface of the system and empirically evaluate the\napproach. The experiments show that lateral annotation has the potential to\nmake the annotation process quicker and more consistent.", "published": "2021-12-21 19:27:21", "link": "http://arxiv.org/abs/2112.11494v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Watch Those Words: Video Falsification Detection Using Word-Conditioned\n  Facial Motion", "abstract": "In today's era of digital misinformation, we are increasingly faced with new\nthreats posed by video falsification techniques. Such falsifications range from\ncheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\nsophisticated AI media synthesis methods), which are becoming perceptually\nindistinguishable from real videos. To tackle this challenge, we propose a\nmulti-modal semantic forensic approach to discover clues that go beyond\ndetecting discrepancies in visual quality, thereby handling both simpler\ncheapfakes and visually persuasive deepfakes. In this work, our goal is to\nverify that the purported person seen in the video is indeed themselves by\ndetecting anomalous facial movements corresponding to the spoken words. We\nleverage the idea of attribution to learn person-specific biometric patterns\nthat distinguish a given speaker from others. We use interpretable Action Units\n(AUs) to capture a person's face and head movement as opposed to deep CNN\nfeatures, and we are the first to use word-conditioned facial motion analysis.\nWe further demonstrate our method's effectiveness on a range of fakes not seen\nin training including those without video manipulation, that were not addressed\nin prior work.", "published": "2021-12-21 01:57:04", "link": "http://arxiv.org/abs/2112.10936v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Towards a Science of Human-AI Decision Making: A Survey of Empirical\n  Studies", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their\nadoption has grown in numerous domains. However, in high-stakes domains such as\ncriminal justice and healthcare, full automation is often not desirable due to\nsafety, ethical, and legal concerns, yet fully manual approaches can be\ninaccurate and time consuming. As a result, there is growing interest in the\nresearch community to augment human decision making with AI assistance. Besides\ndeveloping AI technologies for this purpose, the emerging field of human-AI\ndecision making must embrace empirical approaches to form a foundational\nunderstanding of how humans interact and work with AI to make decisions. To\ninvite and help structure research efforts towards a science of understanding\nand improving human-AI decision making, we survey recent literature of\nempirical human-subject studies on this topic. We summarize the study design\nchoices made in over 100 papers in three important aspects: (1) decision tasks,\n(2) AI models and AI assistance elements, and (3) evaluation metrics. For each\naspect, we summarize current trends, discuss gaps in current practices of the\nfield, and make a list of recommendations for future research. Our survey\nhighlights the need to develop common frameworks to account for the design and\nresearch spaces of human-AI decision making, so that researchers can make\nrigorous choices in study design, and the research community can build on each\nother's work and produce generalizable scientific knowledge. We also hope this\nsurvey will serve as a bridge for HCI and AI communities to work together to\nmutually shape the empirical science and computational technologies for\nhuman-AI decision making.", "published": "2021-12-21 19:00:02", "link": "http://arxiv.org/abs/2112.11471v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Generating Chord Progression from Melody with Flexible Harmonic Rhythm\n  and Controllable Harmonic Density", "abstract": "Melody harmonization, which involves generating a chord progression that\ncomplements a user-provided melody, continues to pose a significant challenge.\nA chord progression must not only be in harmony with the melody, but also\ninterdependent on its rhythmic pattern. While previous neural network-based\nsystems have been successful in producing chord progressions for given\nmelodies, they have not adequately addressed controllable melody harmonization,\nnor have they focused on generating harmonic rhythms with flexibility in the\nrates or patterns of chord changes. This paper presents AutoHarmonizer, a novel\nsystem for harmonic density-controllable melody harmonization with such a\nflexible harmonic rhythm. AutoHarmonizer is equipped with an extensive\nvocabulary of 1,462 chord types and can generate chord progressions that vary\nin harmonic density for a given melody. Experimental results indicate that the\nAutoHarmonizer-generated chord progressions exhibit a diverse range of harmonic\nrhythms and that the system's controllable harmonic density is effective.", "published": "2021-12-21 11:51:51", "link": "http://arxiv.org/abs/2112.11122v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Learning based Monaural Speech Enhancement with\n  Complex-Cycle-Consistent", "abstract": "Recently, self-supervised learning (SSL) techniques have been introduced to\nsolve the monaural speech enhancement problem. Due to the lack of using clean\nphase information, the enhancement performance is limited in most SSL methods.\nTherefore, in this paper, we propose a phase-aware self-supervised learning\nbased monaural speech enhancement method. The latent representations of both\namplitude and phase are studied in two decoders of the foundation autoencoder\n(FAE) with only a limited set of clean speech signals independently. Then, the\ndownstream autoencoder (DAE) learns a shared latent space between the clean\nspeech and mixture representations with a large number of unseen mixtures. A\ncomplex-cycle-consistent (CCC) mechanism is proposed to minimize the\nreconstruction loss between the amplitude and phase domains. Besides, it is\nnoticed that if the speech features are extracted as the multi-resolution\nspectra, the desired information distributed in spectra of different scales can\nbe studied to further boost the performance. The NOISEX and DAPS corpora are\nused to generate mixtures with different interferences to evaluate the efficacy\nof the proposed method. It is highlighted that the clean speech and mixtures\nfed in FAE and DAE are not paired. Both ablation and comparison experimental\nresults show that the proposed method clearly outperforms the state-of-the-art\napproaches.", "published": "2021-12-21 12:23:43", "link": "http://arxiv.org/abs/2112.11142v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Safeguarding test signals for acoustic measurement using arbitrary\n  sounds", "abstract": "We propose a simple method to measure acoustic responses using any sounds by\nconverting them suitable for measurement. This method enables us to use music\npieces for measuring acoustic conditions. It is advantageous to measure such\nconditions without annoying test sounds to listeners. In addition, applying the\nunderlying idea of simultaneous measurement of multiple paths provides\npractically valuable features. For example, it is possible to measure\ndeviations (temporally stable, random, and time-varying) and the impulse\nresponse while reproducing slightly modified contents under target conditions.\nThe key idea of the proposed method is to add relatively small deterministic\nsignals that sound like noise to the original sounds. We call the converted\nsounds safeguarded test signals.", "published": "2021-12-21 17:24:13", "link": "http://arxiv.org/abs/2112.11373v1", "categories": ["cs.SD", "eess.AS", "42-04, 42-08, 68-04"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Learning based Monaural Speech Enhancement with\n  Multi-Task Pre-Training", "abstract": "In self-supervised learning, it is challenging to reduce the gap between the\nenhancement performance on the estimated and target speech signals with existed\npre-tasks. In this paper, we propose a multi-task pre-training method to\nimprove the speech enhancement performance with self-supervised learning.\nWithin the pre-training autoencoder (PAE), only a limited set of clean speech\nsignals are required to learn their latent representations. Meanwhile, to solve\nthe limitation of single pre-task, the proposed masking module exploits the\ndereverberated mask and estimated ratio mask to denoise the mixture as the\nsecond pre-task. Different from the PAE, where the target speech signals are\nestimated, the downstream task autoencoder (DAE) utilizes a large number of\nunlabeled and unseen reverberant mixtures to generate the estimated mixtures.\nThe trained DAE is shared by the learned representations and masks.\nExperimental results on a benchmark dataset demonstrate that the proposed\nmethod outperforms the state-of-the-art approaches.", "published": "2021-12-21 13:08:45", "link": "http://arxiv.org/abs/2112.11459v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Augmented Contrastive Self-Supervised Learning for Audio Invariant\n  Representations", "abstract": "Improving generalization is a major challenge in audio classification due to\nlabeled data scarcity. Self-supervised learning (SSL) methods tackle this by\nleveraging unlabeled data to learn useful features for downstream\nclassification tasks. In this work, we propose an augmented contrastive SSL\nframework to learn invariant representations from unlabeled data. Our method\napplies various perturbations to the unlabeled input data and utilizes\ncontrastive learning to learn representations robust to such perturbations.\nExperimental results on the Audioset and DESED datasets show that our framework\nsignificantly outperforms state-of-the-art SSL and supervised learning methods\non sound/event classification tasks.", "published": "2021-12-21 02:50:53", "link": "http://arxiv.org/abs/2112.10950v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Phonetic Footprint of Parkinson's Disease", "abstract": "As one of the most prevalent neurodegenerative disorders, Parkinson's disease\n(PD) has a significant impact on the fine motor skills of patients. The complex\ninterplay of different articulators during speech production and realization of\nrequired muscle tension become increasingly difficult, thus leading to a\ndysarthric speech. Characteristic patterns such as vowel instability, slurred\npronunciation and slow speech can often be observed in the affected individuals\nand were analyzed in previous studies to determine the presence and progression\nof PD. In this work, we used a phonetic recognizer trained exclusively on\nhealthy speech data to investigate how PD affected the phonetic footprint of\npatients. We rediscovered numerous patterns that had been described in previous\ncontributions although our system had never seen any pathological speech\npreviously. Furthermore, we could show that intermediate activations from the\nneural network could serve as feature vectors encoding information related to\nthe disease state of individuals. We were also able to directly correlate the\nexpert-rated intelligibility of a speaker with the mean confidence of phonetic\npredictions. Our results support the assumption that pathological data is not\nnecessarily required to train systems that are capable of analyzing PD speech.", "published": "2021-12-21 20:44:21", "link": "http://arxiv.org/abs/2112.11514v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
