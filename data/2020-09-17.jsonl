{"title": "Towards Fully 8-bit Integer Inference for the Transformer Model", "abstract": "8-bit integer inference, as a promising direction in reducing both the\nlatency and storage of deep neural networks, has made great progress recently.\nOn the other hand, previous systems still rely on 32-bit floating point for\ncertain functions in complex models (e.g., Softmax in Transformer), and make\nheavy use of quantization and de-quantization. In this work, we show that after\na principled modification on the Transformer architecture, dubbed Integer\nTransformer, an (almost) fully 8-bit integer inference algorithm Scale\nPropagation could be derived. De-quantization is adopted when necessary, which\nmakes the network more efficient. Our experiments on WMT16 En<->Ro, WMT14\nEn<->De and En->Fr translation tasks as well as the WikiText-103 language\nmodelling task show that the fully 8-bit Transformer system achieves comparable\nperformance with the floating point baseline but requires nearly 4x less memory\nfootprint.", "published": "2020-09-17 03:09:10", "link": "http://arxiv.org/abs/2009.08034v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised pre-training and contrastive representation learning for\n  multiple-choice video QA", "abstract": "Video Question Answering (Video QA) requires fine-grained understanding of\nboth video and language modalities to answer the given questions. In this\npaper, we propose novel training schemes for multiple-choice video question\nanswering with a self-supervised pre-training stage and a supervised\ncontrastive learning in the main stage as an auxiliary learning. In the\nself-supervised pre-training stage, we transform the original problem format of\npredicting the correct answer into the one that predicts the relevant question\nto provide a model with broader contextual inputs without any further dataset\nor annotation. For contrastive learning in the main stage, we add a masking\nnoise to the input corresponding to the ground-truth answer, and consider the\noriginal input of the ground-truth answer as a positive sample, while treating\nthe rest as negative samples. By mapping the positive sample closer to the\nmasked input, we show that the model performance is improved. We further employ\nlocally aligned attention to focus more effectively on the video frames that\nare particularly relevant to the given corresponding subtitle sentences. We\nevaluate our proposed model on highly competitive benchmark datasets related to\nmultiple-choice video QA: TVQA, TVQA+, and DramaQA. Experimental results show\nthat our model achieves state-of-the-art performance on all datasets. We also\nvalidate our approaches through further analyses.", "published": "2020-09-17 03:37:37", "link": "http://arxiv.org/abs/2009.08043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Transferability of Minimal Prediction Preserving Inputs in\n  Question Answering", "abstract": "Recent work (Feng et al., 2018) establishes the presence of short,\nuninterpretable input fragments that yield high confidence and accuracy in\nneural models. We refer to these as Minimal Prediction Preserving Inputs\n(MPPIs). In the context of question answering, we investigate competing\nhypotheses for the existence of MPPIs, including poor posterior calibration of\nneural models, lack of pretraining, and \"dataset bias\" (where a model learns to\nattend to spurious, non-generalizable cues in the training data). We discover a\nperplexing invariance of MPPIs to random training seed, model architecture,\npretraining, and training domain. MPPIs demonstrate remarkable transferability\nacross domains achieving significantly higher performance than comparably short\nqueries. Additionally, penalizing over-confidence on MPPIs fails to improve\neither generalization or adversarial robustness. These results suggest the\ninterpretability of MPPIs is insufficient to characterize generalization\ncapacity of these models. We hope this focused investigation encourages more\nsystematic analysis of model behavior outside of the human interpretable\ndistribution of examples.", "published": "2020-09-17 04:58:39", "link": "http://arxiv.org/abs/2009.08070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-switching pre-training for neural machine translation", "abstract": "This paper proposes a new pre-training method, called Code-Switching\nPre-training (CSP for short) for Neural Machine Translation (NMT). Unlike\ntraditional pre-training method which randomly masks some fragments of the\ninput sentence, the proposed CSP randomly replaces some words in the source\nsentence with their translation words in the target language. Specifically, we\nfirstly perform lexicon induction with unsupervised word embedding mapping\nbetween the source and target languages, and then randomly replace some words\nin the input sentence with their translation words according to the extracted\ntranslation lexicons. CSP adopts the encoder-decoder framework: its encoder\ntakes the code-mixed sentence as input, and its decoder predicts the replaced\nfragment of the input sentence. In this way, CSP is able to pre-train the NMT\nmodel by explicitly making the most of the cross-lingual alignment information\nextracted from the source and target monolingual corpus. Additionally, we\nrelieve the pretrain-finetune discrepancy caused by the artificial symbols like\n[mask]. To verify the effectiveness of the proposed method, we conduct\nextensive experiments on unsupervised and supervised NMT. Experimental results\nshow that CSP achieves significant improvements over baselines without\npre-training or with other pre-training methods.", "published": "2020-09-17 06:10:07", "link": "http://arxiv.org/abs/2009.08088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Event Coreference Resolution", "abstract": "Traditional event coreference systems usually rely on pipeline framework and\nhand-crafted features, which often face error propagation problem and have poor\ngeneralization ability. In this paper, we propose an End-to-End Event\nCoreference approach -- E3C neural network, which can jointly model event\ndetection and event coreference resolution tasks, and learn to extract features\nfrom raw text automatically. Furthermore, because event mentions are highly\ndiversified and event coreference is intricately governed by long-distance,\nsemantic-dependent decisions, a type-guided event coreference mechanism is\nfurther proposed in our E3C neural network. Experiments show that our method\nachieves new state-of-the-art performance on two standard datasets.", "published": "2020-09-17 09:00:59", "link": "http://arxiv.org/abs/2009.08153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISCAS at SemEval-2020 Task 5: Pre-trained Transformers for\n  Counterfactual Statement Modeling", "abstract": "ISCAS participated in two subtasks of SemEval 2020 Task 5: detecting\ncounterfactual statements and detecting antecedent and consequence. This paper\ndescribes our system which is based on pre-trained transformers. For the first\nsubtask, we train several transformer-based classifiers for detecting\ncounterfactual statements. For the second subtask, we formulate antecedent and\nconsequence extraction as a query-based question answering problem. The two\nsubsystems both achieved third place in the evaluation. Our system is openly\nreleased at https://github.com/casnlu/ISCAS-SemEval2020Task5.", "published": "2020-09-17 09:28:07", "link": "http://arxiv.org/abs/2009.08171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DSC IIT-ISM at SemEval-2020 Task 6: Boosting BERT with Dependencies for\n  Definition Extraction", "abstract": "We explore the performance of Bidirectional Encoder Representations from\nTransformers (BERT) at definition extraction. We further propose a joint model\nof BERT and Text Level Graph Convolutional Network so as to incorporate\ndependencies into the model. Our proposed model produces better results than\nBERT and achieves comparable results to BERT with fine tuned language model in\nDeftEval (Task 6 of SemEval 2020), a shared task of classifying whether a\nsentence contains a definition or not (Subtask 1).", "published": "2020-09-17 09:48:59", "link": "http://arxiv.org/abs/2009.08180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What if we had no Wikipedia? Domain-independent Term Extraction from a\n  Large News Corpus", "abstract": "One of the most impressive human endeavors of the past two decades is the\ncollection and categorization of human knowledge in the free and accessible\nformat that is Wikipedia. In this work we ask what makes a term worthy of\nentering this edifice of knowledge, and having a page of its own in Wikipedia?\nTo what extent is this a natural product of on-going human discourse and\ndiscussion rather than an idiosyncratic choice of Wikipedia editors?\nSpecifically, we aim to identify such \"wiki-worthy\" terms in a massive news\ncorpus, and see if this can be done with no, or minimal, dependency on actual\nWikipedia entries. We suggest a five-step pipeline for doing so, providing\nbaseline results for all five, and the relevant datasets for benchmarking them.\nOur work sheds new light on the domain-specific Automatic Term Extraction\nproblem, with the problem at hand being a domain-independent variant of it.", "published": "2020-09-17 12:45:46", "link": "http://arxiv.org/abs/2009.08240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Interactive Summarization: an Expansion-Based Framework", "abstract": "Allowing users to interact with multi-document summarizers is a promising\ndirection towards improving and customizing summary results. Different ideas\nfor interactive summarization have been proposed in previous work but these\nsolutions are highly divergent and incomparable. In this paper, we develop an\nend-to-end evaluation framework for expansion-based interactive summarization,\nwhich considers the accumulating information along an interactive session. Our\nframework includes a procedure of collecting real user sessions and evaluation\nmeasures relying on standards, but adapted to reflect interaction. All of our\nsolutions are intended to be released publicly as a benchmark, allowing\ncomparison of future developments in interactive summarization. We demonstrate\nthe use of our framework by evaluating and comparing baseline implementations\nthat we developed for this purpose, which will serve as part of our benchmark.\nOur extensive experimentation and analysis of these systems motivate our design\nchoices and support the viability of our framework.", "published": "2020-09-17 15:48:13", "link": "http://arxiv.org/abs/2009.08380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PhenoTagger: A Hybrid Method for Phenotype Concept Recognition using\n  Human Phenotype Ontology", "abstract": "Automatic phenotype concept recognition from unstructured text remains a\nchallenging task in biomedical text mining research. Previous works that\naddress the task typically use dictionary-based matching methods, which can\nachieve high precision but suffer from lower recall. Recently, machine\nlearning-based methods have been proposed to identify biomedical concepts,\nwhich can recognize more unseen concept synonyms by automatic feature learning.\nHowever, most methods require large corpora of manually annotated data for\nmodel training, which is difficult to obtain due to the high cost of human\nannotation. In this paper, we propose PhenoTagger, a hybrid method that\ncombines both dictionary and machine learning-based methods to recognize Human\nPhenotype Ontology (HPO) concepts in unstructured biomedical text. We first use\nall concepts and synonyms in HPO to construct a dictionary, which is then used\nto automatically build a distantly supervised training dataset for machine\nlearning. Next, a cutting-edge deep learning model is trained to classify each\ncandidate phrase (n-gram from input sentence) into a corresponding concept\nlabel. Finally, the dictionary and machine learning-based prediction results\nare combined for improved performance. Our method is validated with two HPO\ncorpora, and the results show that PhenoTagger compares favorably to previous\nmethods. In addition, to demonstrate the generalizability of our method, we\nretrained PhenoTagger using the disease ontology MEDIC for disease concept\nrecognition to investigate the effect of training on different ontologies.\nExperimental results on the NCBI disease corpus show that PhenoTagger without\nrequiring manually annotated training data achieves competitive performance as\ncompared with state-of-the-art supervised methods.", "published": "2020-09-17 18:00:43", "link": "http://arxiv.org/abs/2009.08478v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small but Mighty: New Benchmarks for Split and Rephrase", "abstract": "Split and Rephrase is a text simplification task of rewriting a complex\nsentence into simpler ones. As a relatively new task, it is paramount to ensure\nthe soundness of its evaluation benchmark and metric. We find that the widely\nused benchmark dataset universally contains easily exploitable syntactic cues\ncaused by its automatic generation process. Taking advantage of such cues, we\nshow that even a simple rule-based model can perform on par with the\nstate-of-the-art model. To remedy such limitations, we collect and release two\ncrowdsourced benchmark datasets. We not only make sure that they contain\nsignificantly more diverse syntax, but also carefully control for their quality\naccording to a well-defined set of criteria. While no satisfactory automatic\nmetric exists, we apply fine-grained manual evaluation based on these criteria\nusing crowdsourcing, showing that our datasets better represent the task and\nare significantly more challenging for the models.", "published": "2020-09-17 23:37:33", "link": "http://arxiv.org/abs/2009.08560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Summarization for Video-containing Documents", "abstract": "Summarization of multimedia data becomes increasingly significant as it is\nthe basis for many real-world applications, such as question answering, Web\nsearch, and so forth. Most existing multi-modal summarization works however\nhave used visual complementary features extracted from images rather than\nvideos, thereby losing abundant information. Hence, we propose a novel\nmulti-modal summarization task to summarize from a document and its associated\nvideo. In this work, we also build a baseline general model with effective\nstrategies, i.e., bi-hop attention and improved late fusion mechanisms to\nbridge the gap between different modalities, and a bi-stream summarization\nstrategy to employ text and video summarization simultaneously. Comprehensive\nexperiments show that the proposed model is beneficial for multi-modal\nsummarization and superior to existing methods. Moreover, we collect a novel\ndataset and it provides a new resource for future study that results from\ndocuments and videos.", "published": "2020-09-17 02:13:14", "link": "http://arxiv.org/abs/2009.08018v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How-to Present News on Social Media: A Causal Analysis of Editing News\n  Headlines for Boosting User Engagement", "abstract": "To reach a broader audience and optimize traffic toward news articles, media\noutlets commonly run social media accounts and share their content with a short\ntext summary. Despite its importance of writing a compelling message in sharing\narticles, the research community does not own a sufficient understanding of\nwhat kinds of editing strategies effectively promote audience engagement. In\nthis study, we aim to fill the gap by analyzing media outlets' current\npractices using a data-driven approach. We first build a parallel corpus of\noriginal news articles and their corresponding tweets that eight media outlets\nshared. Then, we explore how those media edited tweets against original\nheadlines and the effects of such changes. To estimate the effects of editing\nnews headlines for social media sharing in audience engagement, we present a\nsystematic analysis that incorporates a causal inference technique with deep\nlearning; using propensity score matching, it allows for estimating potential\n(dis-)advantages of an editing style compared to counterfactual cases where a\nsimilar news article is shared with a different style. According to the\nanalyses of various editing styles, we report common and differing effects of\nthe styles across the outlets. To understand the effects of various editing\nstyles, media outlets could apply our easy-to-use tool by themselves.", "published": "2020-09-17 06:39:49", "link": "http://arxiv.org/abs/2009.08100v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief\n  States towards Semi-Supervised Learning", "abstract": "Structured belief states are crucial for user goal tracking and database\nquery in task-oriented dialog systems. However, training belief trackers often\nrequires expensive turn-level annotations of every user utterance. In this\npaper we aim at alleviating the reliance on belief state labels in building\nend-to-end dialog systems, by leveraging unlabeled dialog data towards\nsemi-supervised learning. We propose a probabilistic dialog model, called the\nLAtent BElief State (LABES) model, where belief states are represented as\ndiscrete latent variables and jointly modeled with system responses given user\ninputs. Such latent variable modeling enables us to develop semi-supervised\nlearning under the principled variational learning framework. Furthermore, we\nintroduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of\nLABES. In supervised experiments, LABES-S2S obtains strong results on three\nbenchmark datasets of different scales. In utilizing unlabeled dialog data,\nsemi-supervised LABES-S2S significantly outperforms both supervised-only and\nsemi-supervised baselines. Remarkably, we can reduce the annotation demands to\n50% without performance loss on MultiWOZ.", "published": "2020-09-17 07:26:37", "link": "http://arxiv.org/abs/2009.08115v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi$^2$OIE: Multilingual Open Information Extraction Based on\n  Multi-Head Attention with BERT", "abstract": "In this paper, we propose Multi$^2$OIE, which performs open information\nextraction (open IE) by combining BERT with multi-head attention. Our model is\na sequence-labeling system with an efficient and effective argument extraction\nmethod. We use a query, key, and value setting inspired by the Multimodal\nTransformer to replace the previously used bidirectional long short-term memory\narchitecture with multi-head attention. Multi$^2$OIE outperforms existing\nsequence-labeling systems with high computational efficiency on two benchmark\nevaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed\nmethod to multilingual open IE using multilingual BERT. Experimental results on\nnew benchmark datasets introduced for two languages (Spanish and Portuguese)\ndemonstrate that our model outperforms other multilingual systems without\ntraining data for the target languages.", "published": "2020-09-17 08:03:42", "link": "http://arxiv.org/abs/2009.08128v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding", "abstract": "Few-shot learning (FSL) is one of the key future steps in machine learning\nand has raised a lot of attention. However, in contrast to the rapid\ndevelopment in other domains, such as Computer Vision, the progress of FSL in\nNature Language Processing (NLP) is much slower. One of the key reasons for\nthis is the lacking of public benchmarks. NLP FSL researches always report new\nresults on their own constructed few-shot datasets, which is pretty inefficient\nin results comparison and thus impedes cumulative progress. In this paper, we\npresent FewJoint, a novel Few-Shot Learning benchmark for NLP. Different from\nmost NLP FSL research that only focus on simple N-classification problems, our\nbenchmark introduces few-shot joint dialogue language understanding, which\nadditionally covers the structure prediction and multi-task reliance problems.\nThis allows our benchmark to reflect the real-word NLP complexity beyond simple\nN-classification. Our benchmark is used in the few-shot learning contest of\nSMP2020-ECDT task-1. We also provide a compatible FSL platform to ease\nexperiment set-up.", "published": "2020-09-17 08:17:12", "link": "http://arxiv.org/abs/2009.08138v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Label Cohesive and Well-Formed Adversarial Claims", "abstract": "Adversarial attacks reveal important vulnerabilities and flaws of trained\nmodels. One potent type of attack are universal adversarial triggers, which are\nindividual n-grams that, when appended to instances of a class under attack,\ncan trick a model into predicting a target class. However, for inference tasks\nsuch as fact checking, these triggers often inadvertently invert the meaning of\ninstances they are inserted in. In addition, such attacks produce semantically\nnonsensical inputs, as they simply concatenate triggers to existing samples.\nHere, we investigate how to generate adversarial attacks against fact checking\nsystems that preserve the ground truth meaning and are semantically valid. We\nextend the HotFlip attack algorithm used for universal trigger generation by\njointly minimising the target class loss of a fact checking model and the\nentailment class loss of an auxiliary natural language inference model. We then\ntrain a conditional language model to generate semantically valid statements,\nwhich include the found universal triggers. We find that the generated attacks\nmaintain the directionality and semantic validity of the claim better than\nprevious work.", "published": "2020-09-17 10:50:42", "link": "http://arxiv.org/abs/2009.08205v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AIN: Fast and Accurate Sequence Labeling with Approximate Inference\n  Network", "abstract": "The linear-chain Conditional Random Field (CRF) model is one of the most\nwidely-used neural sequence labeling approaches. Exact probabilistic inference\nalgorithms such as the forward-backward and Viterbi algorithms are typically\napplied in training and prediction stages of the CRF model. However, these\nalgorithms require sequential computation that makes parallelization\nimpossible. In this paper, we propose to employ a parallelizable approximate\nvariational inference algorithm for the CRF model. Based on this algorithm, we\ndesign an approximate inference network that can be connected with the encoder\nof the neural CRF model to form an end-to-end network, which is amenable to\nparallelization for faster training and prediction. The empirical results show\nthat our proposed approaches achieve a 12.7-fold improvement in decoding speed\nwith long sentences and a competitive accuracy compared with the traditional\nCRF approach.", "published": "2020-09-17 12:18:43", "link": "http://arxiv.org/abs/2009.08229v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A\n  Case Study on CoQA", "abstract": "Many NLP tasks have benefited from transferring knowledge from contextualized\nword embeddings, however the picture of what type of knowledge is transferred\nis incomplete. This paper studies the types of linguistic phenomena accounted\nfor by language models in the context of a Conversational Question Answering\n(CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT\nand DistilBERT models through systematic error analysis - basic arithmetic\n(counting phrases), compositional semantics (negation and Semantic Role\nLabeling), and lexical semantics (surprisal and antonymy). When enhanced with\nthe relevant linguistic knowledge through multitask learning, the models\nimprove in performance. Ensembles of the enhanced models yield a boost between\n2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the\nhardest question classes. The results show differences in ability to represent\ncompositional and lexical information between RoBERTa, BERT and DistilBERT.", "published": "2020-09-17 13:00:13", "link": "http://arxiv.org/abs/2009.08257v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More Embeddings, Better Sequence Labelers?", "abstract": "Recent work proposes a family of contextual embeddings that significantly\nimproves the accuracy of sequence labelers over non-contextual embeddings.\nHowever, there is no definite conclusion on whether we can build better\nsequence labelers by combining different kinds of embeddings in various\nsettings. In this paper, we conduct extensive experiments on 3 tasks over 18\ndatasets and 8 languages to study the accuracy of sequence labeling with\nvarious embedding concatenations and make three observations: (1) concatenating\nmore embedding variants leads to better accuracy in rich-resource and\ncross-domain settings and some conditions of low-resource settings; (2)\nconcatenating additional contextual sub-word embeddings with contextual\ncharacter embeddings hurts the accuracy in extremely low-resource settings; (3)\nbased on the conclusion of (1), concatenating additional similar contextual\nembeddings cannot lead to further improvements. We hope these conclusions can\nhelp people build stronger sequence labelers in various settings.", "published": "2020-09-17 14:28:27", "link": "http://arxiv.org/abs/2009.08330v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "abstract": "Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.", "published": "2020-09-17 15:25:56", "link": "http://arxiv.org/abs/2009.08366v4", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Computational Approach to Understanding Empathy Expressed in\n  Text-Based Mental Health Support", "abstract": "Empathy is critical to successful mental health support. Empathy measurement\nhas predominantly occurred in synchronous, face-to-face settings, and may not\ntranslate to asynchronous, text-based contexts. Because millions of people use\ntext-based platforms for mental health support, understanding empathy in these\ncontexts is crucial. In this work, we present a computational approach to\nunderstanding how empathy is expressed in online mental health platforms. We\ndevelop a novel unifying theoretically-grounded framework for characterizing\nthe communication of empathy in text-based conversations. We collect and share\na corpus of 10k (post, response) pairs annotated using this empathy framework\nwith supporting evidence for annotations (rationales). We develop a multi-task\nRoBERTa-based bi-encoder model for identifying empathy in conversations and\nextracting rationales underlying its predictions. Experiments demonstrate that\nour approach can effectively identify empathic conversations. We further apply\nthis model to analyze 235k mental health interactions and show that users do\nnot self-learn empathy over time, revealing opportunities for empathy training\nand feedback.", "published": "2020-09-17 17:47:00", "link": "http://arxiv.org/abs/2009.08441v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Meta-Learning for Few-Shot Natural Language\n  Classification Tasks", "abstract": "Self-supervised pre-training of transformer models has revolutionized NLP\napplications. Such pre-training with language modeling objectives provides a\nuseful initial point for parameters that generalize well to new tasks with\nfine-tuning. However, fine-tuning is still data inefficient -- when there are\nfew labeled examples, accuracy can be low. Data efficiency can be improved by\noptimizing pre-training directly for future fine-tuning with few examples; this\ncan be treated as a meta-learning problem. However, standard meta-learning\ntechniques require many training tasks in order to generalize; unfortunately,\nfinding a diverse set of such supervised tasks is usually difficult. This paper\nproposes a self-supervised approach to generate a large, rich, meta-learning\ntask distribution from unlabeled text. This is achieved using a cloze-style\nobjective, but creating separate multi-class classification tasks by gathering\ntokens-to-be blanked from among only a handful of vocabulary terms. This yields\nas many unique meta-training tasks as the number of subsets of vocabulary\nterms. We meta-train a transformer model on this distribution of tasks using a\nrecent meta-learning framework. On 17 NLP tasks, we show that this\nmeta-training leads to better few-shot generalization than language-model\npre-training followed by finetuning. Furthermore, we show how the\nself-supervised tasks can be combined with supervised tasks for meta-learning,\nproviding substantial accuracy gains over previous supervised meta-learning.", "published": "2020-09-17 17:53:59", "link": "http://arxiv.org/abs/2009.08445v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Attention for Unsupervised Dialogue Structure Induction", "abstract": "Inducing a meaningful structural representation from one or a set of\ndialogues is a crucial but challenging task in computational linguistics.\nAdvancement made in this area is critical for dialogue system design and\ndiscourse analysis. It can also be extended to solve grammatical inference. In\nthis work, we propose to incorporate structured attention layers into a\nVariational Recurrent Neural Network (VRNN) model with discrete latent states\nto learn dialogue structure in an unsupervised fashion. Compared to a vanilla\nVRNN, structured attention enables a model to focus on different parts of the\nsource sentence embeddings while enforcing a structural inductive bias.\nExperiments show that on two-party dialogue datasets, VRNN with structured\nattention learns semantic structures that are similar to templates used to\ngenerate this dialogue corpus. While on multi-party dialogue datasets, our\nmodel learns an interactive structure demonstrating its capability of\ndistinguishing speakers or addresses, automatically disentangling dialogues\nwithout explicit human annotation.", "published": "2020-09-17 23:07:03", "link": "http://arxiv.org/abs/2009.08552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generation-Augmented Retrieval for Open-domain Question Answering", "abstract": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.", "published": "2020-09-17 23:08:01", "link": "http://arxiv.org/abs/2009.08553v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Zero-shot Multi-Domain Dialog State Tracking Using Descriptive Rules", "abstract": "In this work, we present a framework for incorporating descriptive logical\nrules in state-of-the-art neural networks, enabling them to learn how to handle\nunseen labels without the introduction of any new training data. The rules are\nintegrated into existing networks without modifying their architecture, through\nan additional term in the network's loss function that penalizes states of the\nnetwork that do not obey the designed rules. As a case of study, the framework\nis applied to an existing neural-based Dialog State Tracker. Our experiments\ndemonstrate that the inclusion of logical rules allows the prediction of unseen\nlabels, without deteriorating the predictive capacity of the original system.", "published": "2020-09-17 18:14:25", "link": "http://arxiv.org/abs/2009.13275v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Transformer-based Large Scale Language Representations using\n  Hardware-friendly Block Structured Pruning", "abstract": "Pre-trained large-scale language models have increasingly demonstrated high\naccuracy on many natural language processing (NLP) tasks. However, the limited\nweight storage and computational speed on hardware platforms have impeded the\npopularity of pre-trained models, especially in the era of edge computing. In\nthis work, we propose an efficient transformer-based large-scale language\nrepresentation using hardware-friendly block structure pruning. We incorporate\nthe reweighted group Lasso into block-structured pruning for optimization.\nBesides the significantly reduced weight storage and computation, the proposed\napproach achieves high compression rates. Experimental results on different\nmodels (BERT, RoBERTa, and DistilBERT) on the General Language Understanding\nEvaluation (GLUE) benchmark tasks show that we achieve up to 5.0x with zero or\nminor accuracy degradation on certain task(s). Our proposed method is also\northogonal to existing compact pre-trained language models such as DistilBERT\nusing knowledge distillation, since a further 1.79x average compression rate\ncan be achieved on top of DistilBERT with zero or minor accuracy degradation.\nIt is suitable to deploy the final compressed model on resource-constrained\nedge devices.", "published": "2020-09-17 04:45:47", "link": "http://arxiv.org/abs/2009.08065v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach to Geographical Candidate Selection through\n  Toponym Matching", "abstract": "Recognizing toponyms and resolving them to their real-world referents is\nrequired for providing advanced semantic access to textual data. This process\nis often hindered by the high degree of variation in toponyms. Candidate\nselection is the task of identifying the potential entities that can be\nreferred to by a toponym previously recognized. While it has traditionally\nreceived little attention in the research community, it has been shown that\ncandidate selection has a significant impact on downstream tasks (i.e. entity\nresolution), especially in noisy or non-standard text. In this paper, we\nintroduce a flexible deep learning method for candidate selection through\ntoponym matching, using state-of-the-art neural network architectures. We\nperform an intrinsic toponym matching evaluation based on several new realistic\ndatasets, which cover various challenging scenarios (cross-lingual and regional\nvariations, as well as OCR errors). We report its performance on candidate\nselection in the context of the downstream task of toponym resolution, both on\nexisting datasets and on a new manually-annotated resource of\nnineteenth-century English OCR'd text.", "published": "2020-09-17 07:24:56", "link": "http://arxiv.org/abs/2009.08114v2", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Modeling Task Effects on Meaning Representation in the Brain via\n  Zero-Shot MEG Prediction", "abstract": "How meaning is represented in the brain is still one of the big open\nquestions in neuroscience. Does a word (e.g., bird) always have the same\nrepresentation, or does the task under which the word is processed alter its\nrepresentation (answering \"can you eat it?\" versus \"can it fly?\")? The brain\nactivity of subjects who read the same word while performing different semantic\ntasks has been shown to differ across tasks. However, it is still not\nunderstood how the task itself contributes to this difference. In the current\nwork, we study Magnetoencephalography (MEG) brain recordings of participants\ntasked with answering questions about concrete nouns. We investigate the effect\nof the task (i.e. the question being asked) on the processing of the concrete\nnoun by predicting the millisecond-resolution MEG recordings as a function of\nboth the semantics of the noun and the task. Using this approach, we test\nseveral hypotheses about the task-stimulus interactions by comparing the\nzero-shot predictions made by these hypotheses for novel tasks and nouns not\nseen during training. We find that incorporating the task semantics\nsignificantly improves the prediction of MEG recordings, across participants.\nThe improvement occurs 475-550ms after the participants first see the word,\nwhich corresponds to what is considered to be the ending time of semantic\nprocessing for a word. These results suggest that only the end of semantic\nprocessing of a word is task-dependent, and pose a challenge for future\nresearch to formulate new hypotheses for earlier task effects as a function of\nthe task and stimuli.", "published": "2020-09-17 17:20:18", "link": "http://arxiv.org/abs/2009.08424v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding Influential Instances for Distantly Supervised Relation\n  Extraction", "abstract": "Distant supervision (DS) is a strong way to expand the datasets for enhancing\nrelation extraction (RE) models but often suffers from high label noise.\nCurrent works based on attention, reinforcement learning, or GAN are black-box\nmodels so they neither provide meaningful interpretation of sample selection in\nDS nor stability on different domains. On the contrary, this work proposes a\nnovel model-agnostic instance sampling method for DS by influence function\n(IF), namely REIF. Our method identifies favorable/unfavorable instances in the\nbag based on IF, then does dynamic instance sampling. We design a fast\ninfluence sampling algorithm that reduces the computational complexity from\n$\\mathcal{O}(mn)$ to $\\mathcal{O}(1)$, with analyzing its robustness on the\nselected sampling function. Experiments show that by simply sampling the\nfavorable instances during training, REIF is able to win over a series of\nbaselines that have complicated architectures. We also demonstrate that REIF\ncan support interpretable instance selection.", "published": "2020-09-17 02:02:07", "link": "http://arxiv.org/abs/2009.09841v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dissecting Lottery Ticket Transformers: Structural and Behavioral Study\n  of Sparse Neural Machine Translation", "abstract": "Recent work on the lottery ticket hypothesis has produced highly sparse\nTransformers for NMT while maintaining BLEU. However, it is unclear how such\npruning techniques affect a model's learned representations. By probing\nTransformers with more and more low-magnitude weights pruned away, we find that\ncomplex semantic information is first to be degraded. Analysis of internal\nactivations reveals that higher layers diverge most over the course of pruning,\ngradually becoming less complex than their dense counterparts. Meanwhile, early\nlayers of sparse models begin to perform more encoding. Attention mechanisms\nremain remarkably consistent as sparsity increases.", "published": "2020-09-17 02:08:45", "link": "http://arxiv.org/abs/2009.13270v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Multimodal Memes Classification: A Survey and Open Research Issues", "abstract": "Memes are graphics and text overlapped so that together they present concepts\nthat become dubious if one of them is absent. It is spread mostly on social\nmedia platforms, in the form of jokes, sarcasm, motivating, etc. After the\nsuccess of BERT in Natural Language Processing (NLP), researchers inclined to\nVisual-Linguistic (VL) multimodal problems like memes classification, image\ncaptioning, Visual Question Answering (VQA), and many more. Unfortunately, many\nmemes get uploaded each day on social media platforms that need automatic\ncensoring to curb misinformation and hate. Recently, this issue has attracted\nthe attention of researchers and practitioners. State-of-the-art methods that\nperformed significantly on other VL dataset, tends to fail on memes\nclassification. In this context, this work aims to conduct a comprehensive\nstudy on memes classification, generally on the VL multimodal problems and\ncutting edge solutions. We propose a generalized framework for VL problems. We\ncover the early and next-generation works on VL problems. Finally, we identify\nand articulate several open research issues and challenges. This is the first\nstudy that presents the generalized view of the advanced classification\ntechniques concerning memes classification to the best of our knowledge. We\nbelieve this study presents a clear road-map for the Machine Learning (ML)\nresearch community to implement and enhance memes classification techniques.", "published": "2020-09-17 16:13:21", "link": "http://arxiv.org/abs/2009.08395v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Utterance-level Intent Recognition from Keywords", "abstract": "This paper focuses on wake on intent (WOI) techniques for platforms with\nlimited compute and memory. Our approach of utterance-level intent\nclassification is based on a sequence of keywords in the utterance instead of a\nsingle fixed key phrase. The keyword sequence is transformed into four types of\ninput features, namely acoustics, phones, word2vec and speech2vec for\nindividual intent learning and then fused decision making. If a wake intent is\ndetected, it will trigger the power-costly ASR afterwards. The system is\ntrained and tested on a newly collected internal dataset in Intel called AMIE,\nwhich will be reported in this paper for the first time. It is demonstrated\nthat our novel technique with the representation of the key-phrases\nsuccessfully achieved a noise robust intent classification in different domains\nincluding in-car human-machine communications. The wake on intent system will\nbe low-power and low-complexity, which makes it suitable for always on\noperations in real life hardware-based applications.", "published": "2020-09-17 04:45:39", "link": "http://arxiv.org/abs/2009.08064v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Speaker Diarization with Relation Network", "abstract": "In this paper, we propose an online speaker diarization system based on\nRelation Network, named RenoSD. Unlike conventional diariztion systems which\nconsist of several independently-optimized modules, RenoSD implements\nvoice-activity-detection (VAD), embedding extraction, and speaker identity\nassociation using a single deep neural network. The most striking feature of\nRenoSD is that it adopts a meta-learning strategy for speaker identity\nassociation. In particular, the relation network learns to learn a deep\ndistance metric in a data-driven way and it can determine through a simple\nforward pass whether two given segments belong to the same speaker. As such,\nRenoSD can be performed in an online manner with low latency. Experimental\nresults on AMI and CALLHOME datasets show that the proposed RenoSD system\nachieves consistent improvements over the state-of-the-art x-vector baseline.\nCompared with an existing online diarization system named UIS-RNN, RenoSD\nachieves a better performance using much fewer training data and at a lower\ntime complexity.", "published": "2020-09-17 09:11:49", "link": "http://arxiv.org/abs/2009.08162v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical Multi-Grained Generative Model for Expressive Speech\n  Synthesis", "abstract": "This paper proposes a hierarchical generative model with a multi-grained\nlatent variable to synthesize expressive speech. In recent years, fine-grained\nlatent variables are introduced into the text-to-speech synthesis that enable\nthe fine control of the prosody and speaking styles of synthesized speech.\nHowever, the naturalness of speech degrades when these latent variables are\nobtained by sampling from the standard Gaussian prior. To solve this problem,\nwe propose a novel framework for modeling the fine-grained latent variables,\nconsidering the dependence on an input text, a hierarchical linguistic\nstructure, and a temporal structure of latent variables. This framework\nconsists of a multi-grained variational autoencoder, a conditional prior, and a\nmulti-level auto-regressive latent converter to obtain the different\ntime-resolution latent variables and sample the finer-level latent variables\nfrom the coarser-level ones by taking into account the input text. Experimental\nresults indicate an appropriate method of sampling fine-grained latent\nvariables without the reference signal at the synthesis stage. Our proposed\nframework also provides the controllability of speaking style in an entire\nutterance.", "published": "2020-09-17 18:00:19", "link": "http://arxiv.org/abs/2009.08474v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds", "abstract": "Testing capacity for COVID-19 remains a challenge globally due to the lack of\nadequate supplies, trained personnel, and sample-processing equipment. These\nproblems are even more acute in rural and underdeveloped regions. We\ndemonstrate that solicited-cough sounds collected over a phone, when analysed\nby our AI model, have statistically significant signal indicative of COVID-19\nstatus (AUC 0.72, t-test,p <0.01,95% CI 0.61-0.83). This holds true for\nasymptomatic patients as well. Towards this, we collect the largest known(to\ndate) dataset of microbiologically confirmed COVID-19 cough sounds from 3,621\nindividuals. When used in a triaging step within an overall testing protocol,\nby enabling risk-stratification of individuals before confirmatory tests, our\ntool can increase the testing capacity of a healthcare system by 43% at disease\nprevalence of 5%, without additional supplies, trained personnel, or physical\ninfrastructure", "published": "2020-09-17 14:59:14", "link": "http://arxiv.org/abs/2009.08790v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporally Guided Music-to-Body-Movement Generation", "abstract": "This paper presents a neural network model to generate virtual violinist's\n3-D skeleton movements from music audio. Improved from the conventional\nrecurrent neural network models for generating 2-D skeleton data in previous\nworks, the proposed model incorporates an encoder-decoder architecture, as well\nas the self-attention mechanism to model the complicated dynamics in body\nmovement sequences. To facilitate the optimization of self-attention model,\nbeat tracking is applied to determine effective sizes and boundaries of the\ntraining examples. The decoder is accompanied with a refining network and a\nbowing attack inference mechanism to emphasize the right-hand behavior and\nbowing attack timing. Both objective and subjective evaluations reveal that the\nproposed model outperforms the state-of-the-art methods. To the best of our\nknowledge, this work represents the first attempt to generate 3-D violinists'\nbody movements considering key features in musical body movement.", "published": "2020-09-17 02:10:05", "link": "http://arxiv.org/abs/2009.08015v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
