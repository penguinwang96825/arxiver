{"title": "Models and Datasets for Cross-Lingual Summarisation", "abstract": "We present a cross-lingual summarisation corpus with long documents in a\nsource language associated with multi-sentence summaries in a target language.\nThe corpus covers twelve language pairs and directions for four European\nlanguages, namely Czech, English, French and German, and the methodology for\nits creation can be applied to several other languages. We derive cross-lingual\ndocument-summary instances from Wikipedia by combining lead paragraphs and\narticles' bodies from language aligned Wikipedia titles. We analyse the\nproposed cross-lingual summarisation task with automatic metrics and validate\nit with a human study. To illustrate the utility of our dataset we report\nexperiments with multi-lingual pre-trained models in supervised, zero- and\nfew-shot, and out-of-domain scenarios.", "published": "2022-02-19 11:55:40", "link": "http://arxiv.org/abs/2202.09583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALCS 2021 Shared Task: Machine Translation for Code-Switched Data", "abstract": "To date, efforts in the code-switching literature have focused for the most\npart on language identification, POS, NER, and syntactic parsing. In this\npaper, we address machine translation for code-switched social media data. We\ncreate a community shared task. We provide two modalities for participation:\nsupervised and unsupervised. For the supervised setting, participants are\nchallenged to translate English into Hindi-English (Eng-Hinglish) in a single\ndirection. For the unsupervised setting, we provide the following language\npairs: English and Spanish-English (Eng-Spanglish), and English and Modern\nStandard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share\ninsights and challenges in curating the \"into\" code-switching language\nevaluation data. Further, we provide baselines for all language pairs in the\nshared task. The leaderboard for the shared task comprises 12 individual system\nsubmissions corresponding to 5 different teams. The best performance achieved\nis 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to\nEnglish.", "published": "2022-02-19 15:39:34", "link": "http://arxiv.org/abs/2202.09625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain\n  Acronym Extraction", "abstract": "Acronym extraction is the task of identifying acronyms and their expanded\nforms in texts that is necessary for various NLP applications. Despite major\nprogress for this task in recent years, one limitation of existing AE research\nis that they are limited to the English language and certain domains (i.e.,\nscientific and biomedical). As such, challenges of AE in other languages and\ndomains is mainly unexplored. Lacking annotated datasets in multiple languages\nand domains has been a major issue to hinder research in this area. To address\nthis limitation, we propose a new dataset for multilingual multi-domain AE.\nSpecifically, 27,200 sentences in 6 typologically different languages and 2\ndomains, i.e., Legal and Scientific, is manually annotated for AE. Our\nextensive experiments on the proposed dataset show that AE in different\nlanguages and different learning settings has unique challenges, emphasizing\nthe necessity of further research on multilingual and multi-domain AE.", "published": "2022-02-19 23:08:38", "link": "http://arxiv.org/abs/2202.09694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-Driven Mitigation of Adversarial Text Perturbation", "abstract": "Social networks have become an indispensable part of our lives, with billions\nof people producing ever-increasing amounts of text. At such scales, content\npolicies and their enforcement become paramount. To automate moderation,\nquestionable content is detected by Natural Language Processing (NLP)\nclassifiers. However, high-performance classifiers are hampered by misspellings\nand adversarial text perturbations. In this paper, we classify intentional and\nunintentional adversarial text perturbation into ten types and propose a\ndeobfuscation pipeline to make NLP models robust to such perturbations. We\npropose Continuous Word2Vec (CW2V), our data-driven method to learn word\nembeddings that ensures that perturbations of words have embeddings similar to\nthose of the original words. We show that CW2V embeddings are generally more\nrobust to text perturbations than embeddings based on character ngrams. Our\nrobust classification pipeline combines deobfuscation and classification, using\nproposed defense methods and word embeddings to classify whether Facebook posts\nare requesting engagement such as likes. Our pipeline results in engagement\nbait classification that goes from 0.70 to 0.67 AUC with adversarial text\nperturbation, while character ngram-based word embedding methods result in\ndownstream classification that goes from 0.76 to 0.64.", "published": "2022-02-19 00:49:12", "link": "http://arxiv.org/abs/2202.09483v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "PETCI: A Parallel English Translation Dataset of Chinese Idioms", "abstract": "Idioms are an important language phenomenon in Chinese, but idiom translation\nis notoriously hard. Current machine translation models perform poorly on idiom\ntranslation, while idioms are sparse in many translation datasets. We present\nPETCI, a parallel English translation dataset of Chinese idioms, aiming to\nimprove idiom translation by both human and machine. The dataset is built by\nleveraging human and machine effort. Baseline generation models show\nunsatisfactory abilities to improve translation, but structure-aware\nclassification models show good performance on distinguishing good\ntranslations. Furthermore, the size of PETCI can be easily increased without\nexpertise. Overall, PETCI can be helpful to language learners and machine\ntranslation systems.", "published": "2022-02-19 03:16:20", "link": "http://arxiv.org/abs/2202.09509v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reward Modeling for Mitigating Toxicity in Transformer-based Language\n  Models", "abstract": "Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.", "published": "2022-02-19 19:26:22", "link": "http://arxiv.org/abs/2202.09662v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is there an aesthetic component of language?", "abstract": "Speakers of all human languages make use of grammatical devices to express\nattributional qualities, feelings, and opinions as well as to provide\nmeta-commentary on topics in discourse. In general, linguists refer to this\ncategory as 'expressives'in spite of the fact that defining exactly what\n'expressives' are remains elusive. The elusiveness of expressives has given\nrise to considerable speculation about the nature of expressivity as a\nlinguistic principle. Specifically, several scholars have pointed out the\n'special' or 'unusual' nature of expressives vis-a-vis 'normal' or 'natural'\nmorpho-syntax.", "published": "2022-02-19 22:18:02", "link": "http://arxiv.org/abs/2202.09689v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their\n  Descriptions", "abstract": "Given the increasing number of livestreaming videos, automatic speech\nrecognition and post-processing for livestreaming video transcripts are crucial\nfor efficient data management as well as knowledge mining. A key step in this\nprocess is punctuation restoration which restores fundamental text structures\nsuch as phrase and sentence boundaries from the video transcripts. This work\npresents a new human-annotated corpus, called BehancePR, for punctuation\nrestoration in livestreaming video transcripts. Our experiments on BehancePR\ndemonstrate the challenges of punctuation restoration for this domain.\nFurthermore, we show that popular natural language processing toolkits are\nincapable of detecting sentence boundary on non-punctuated transcripts of\nlivestreaming videos, calling for more research effort to develop robust models\nfor this area.", "published": "2022-02-19 23:12:57", "link": "http://arxiv.org/abs/2202.09695v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Hate Speech Detection: A Comparative Study", "abstract": "Automated hate speech detection is an important tool in combating the spread\nof hate speech, particularly in social media. Numerous methods have been\ndeveloped for the task, including a recent proliferation of deep-learning based\napproaches. A variety of datasets have also been developed, exemplifying\nvarious manifestations of the hate-speech detection problem. We present here a\nlarge-scale empirical comparison of deep and shallow hate-speech detection\nmethods, mediated through the three most commonly used datasets. Our goal is to\nilluminate progress in the area, and identify strengths and weaknesses in the\ncurrent state-of-the-art. We particularly focus our analysis on measures of\npractical performance, including detection accuracy, computational efficiency,\ncapability in using pre-trained models, and domain generalization. In doing so\nwe aim to provide guidance as to the use of hate-speech detection in practice,\nquantify the state-of-the-art, and identify future research directions. Code\nand dataset are available at\nhttps://github.com/jmjmalik22/Hate-Speech-Detection.", "published": "2022-02-19 03:48:20", "link": "http://arxiv.org/abs/2202.09517v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Transformers know symbolic rules, and would we know if they did?", "abstract": "To improve the explainability of leading Transformer networks used in NLP, it\nis important to tease apart genuine symbolic rules from merely associative\ninput-output patterns. However, we identify several inconsistencies in how\n``symbolicity'' has been construed in recent NLP literature. To mitigate this\nproblem, we propose two criteria to be the most relevant, one pertaining to a\nsystem's internal architecture and the other to the dissociation between\nabstract rules and specific input identities. From this perspective, we\ncritically examine prior work on the symbolic capacities of Transformers, and\ndeem the results to be fundamentally inconclusive for reasons inherent in\nexperiment design. We further maintain that there is no simple fix to this\nproblem, since it arises -- to an extent -- in all end-to-end settings.\nNonetheless, we emphasize the need for more robust evaluation of whether\nnon-symbolic explanations exist for success in seemingly symbolic tasks. To\nfacilitate this, we experiment on four sequence modelling tasks on the T5\nTransformer in two experiment settings: zero-shot generalization, and\ngeneralization across class-specific vocabularies flipped between the training\nand test set. We observe that T5's generalization is markedly stronger in\nsequence-to-sequence tasks than in comparable classification tasks. Based on\nthis, we propose a thus far overlooked analysis, where the Transformer itself\ndoes not need to be symbolic to be part of a symbolic architecture as the\nprocessor, operating on the input and output as external memory components.", "published": "2022-02-19 09:56:38", "link": "http://arxiv.org/abs/2203.00162v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LPC Augment: An LPC-Based ASR Data Augmentation Algorithm for Low and\n  Zero-Resource Children's Dialects", "abstract": "This paper proposes a novel linear prediction coding-based data aug-mentation\nmethod for children's low and zero resource dialect ASR. The data augmentation\nprocedure consists of perturbing the formant peaks of the LPC spectrum during\nLPC analysis and reconstruction. The method is evaluated on two novel\nchildren's speech datasets with one containing California English from the\nSouthern CaliforniaArea and the other containing a mix of Southern American\nEnglish and African American English from the Atlanta, Georgia area. We test\nthe proposed method in training both an HMM-DNN system and an end-to-end system\nto show model-robustness and demonstrate that the algorithm improves ASR\nperformance, especially for zero resource dialect children's task, as compared\nto common data augmentation methods such as VTLP, Speed Perturbation, and\nSpecAugment.", "published": "2022-02-19 05:35:45", "link": "http://arxiv.org/abs/2202.09529v2", "categories": ["eess.AS", "cs.SD", "14J60 (Primary) 14F05, 14J26 (Secondary)", "F.2.2; I.2.7"], "primary_category": "eess.AS"}
{"title": "Can Social Robots Effectively Elicit Curiosity in STEM Topics from K-1\n  Students During Oral Assessments?", "abstract": "This paper presents the results of a pilot study that introduces social\nrobots into kindergarten and first-grade classroom tasks. This study aims to\nunderstand 1) how effective social robots are in administering educational\nactivities and assessments, and 2) if these interactions with social robots can\nserve as a gateway into learning about robotics and STEM for young children. We\nadministered a commonly-used assessment (GFTA3) of speech production using a\nsocial robot and compared the quality of recorded responses to those obtained\nwith a human assessor. In a comparison done between 40 children, we found no\nsignificant differences in the student responses between the two conditions\nover the three metrics used: word repetition accuracy, number of times\nadditional help was needed, and similarity of prosody to the assessor. We also\nfound that interactions with the robot were successfully able to stimulate\ncuriosity in robotics, and therefore STEM, from a large number of the 164\nstudent participants.", "published": "2022-02-19 05:45:20", "link": "http://arxiv.org/abs/2202.09531v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluation of Neuromorphic Spike Encoding of Sound Using Information\n  Theory", "abstract": "The problem of spike encoding of sound consists in transforming a sound\nwaveform into spikes. It is of interest in many domains, including the\ndevelopment of audio-based spiking neural networks, where it is the first and\nmost crucial stage of processing. Many algorithms have been proposed to perform\nspike encoding of sound. However, a systematic approach to quantitatively\nevaluate their performance is currently lacking. We propose the use of an\ninformation-theoretic framework to solve this problem. Specifically, we\nevaluate the coding efficiency of four spike encoding algorithms on two coding\ntasks that consist of coding the fundamental characteristics of sound:\nfrequency and amplitude. The algorithms investigated are: Independent Spike\nCoding, Send-on-Delta coding, Ben's Spiker Algorithm, and Leaky\nIntegrate-and-Fire coding. Using the tools of information theory, we estimate\nthe information that the spikes carry on relevant aspects of an input stimulus.\nWe find disparities in the coding efficiencies of the algorithms, where Leaky\nIntegrate-and-Fire coding performs best. The information-theoretic analysis of\ntheir performance on these coding tasks provides insight on the encoding of\nricher and more complex sound stimuli.", "published": "2022-02-19 14:59:36", "link": "http://arxiv.org/abs/2202.09619v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Predicting emotion from music videos: exploring the relative\n  contribution of visual and auditory information to affective responses", "abstract": "Although media content is increasingly produced, distributed, and consumed in\nmultiple combinations of modalities, how individual modalities contribute to\nthe perceived emotion of a media item remains poorly understood. In this paper\nwe present MusicVideos (MuVi), a novel dataset for affective multimedia content\nanalysis to study how the auditory and visual modalities contribute to the\nperceived emotion of media. The data were collected by presenting music videos\nto participants in three conditions: music, visual, and audiovisual.\nParticipants annotated the music videos for valence and arousal over time, as\nwell as the overall emotion conveyed. We present detailed descriptive\nstatistics for key measures in the dataset and the results of feature\nimportance analyses for each condition. Finally, we propose a novel transfer\nlearning architecture to train Predictive models Augmented with Isolated\nmodality Ratings (PAIR) and demonstrate the potential of isolated modality\nratings for enhancing multimodal emotion recognition. Our results suggest that\nperceptions of arousal are influenced primarily by auditory information, while\nperceptions of valence are more subjective and can be influenced by both visual\nand auditory information. The dataset is made publicly available.", "published": "2022-02-19 07:36:43", "link": "http://arxiv.org/abs/2202.10453v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
