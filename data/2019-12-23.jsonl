{"title": "Knowledge-guided Convolutional Networks for Chemical-Disease Relation\n  Extraction", "abstract": "Background: Automatic extraction of chemical-disease relations (CDR) from\nunstructured text is of essential importance for disease treatment and drug\ndevelopment. Meanwhile, biomedical experts have built many highly-structured\nknowledge bases (KBs), which contain prior knowledge about chemicals and\ndiseases. Prior knowledge provides strong support for CDR extraction. How to\nmake full use of it is worth studying. Results: This paper proposes a novel\nmodel called \"Knowledge-guided Convolutional Networks (KCN)\" to leverage prior\nknowledge for CDR extraction. The proposed model first learns knowledge\nrepresentations including entity embeddings and relation embeddings from KBs.\nThen, entity embeddings are used to control the propagation of context features\ntowards a chemical-disease pair with gated convolutions. After that, relation\nembeddings are employed to further capture the weighted context features by a\nshared attention pooling. Finally, the weighted context features containing\nadditional knowledge information are used for CDR extraction. Experiments on\nthe BioCreative V CDR dataset show that the proposed KCN achieves 71.28%\nF1-score, which outperforms most of the state-of-the-art systems. Conclusions:\nThis paper proposes a novel CDR extraction model KCN to make full use of prior\nknowledge. Experimental results demonstrate that KCN could effectively\nintegrate prior knowledge and contexts for the performance improvement.", "published": "2019-12-23 02:28:10", "link": "http://arxiv.org/abs/1912.10590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Context and Knowledge Representations for Chemical-Disease\n  Relation Extraction", "abstract": "Automatically extracting the relationships between chemicals and diseases is\nsignificantly important to various areas of biomedical research and health\ncare. Biomedical experts have built many large-scale knowledge bases (KBs) to\nadvance the development of biomedical research. KBs contain huge amounts of\nstructured information about entities and relationships, therefore plays a\npivotal role in chemical-disease relation (CDR) extraction. However, previous\nresearches pay less attention to the prior knowledge existing in KBs. This\npaper proposes a neural network-based attention model (NAM) for CDR extraction,\nwhich makes full use of context information in documents and prior knowledge in\nKBs. For a pair of entities in a document, an attention mechanism is employed\nto select important context words with respect to the relation representations\nlearned from KBs. Experiments on the BioCreative V CDR dataset show that\ncombining context and knowledge representations through the attention\nmechanism, could significantly improve the CDR extraction performance while\nachieve comparable results with state-of-the-art systems.", "published": "2019-12-23 03:34:13", "link": "http://arxiv.org/abs/1912.10604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Siamese Networks for Large-Scale Author Identification", "abstract": "Authorship attribution is the process of identifying the author of a text.\nApproaches to tackling it have been conventionally divided into\nclassification-based ones, which work well for small numbers of candidate\nauthors, and similarity-based methods, which are applicable for larger numbers\nof authors or for authors beyond the training set; these existing\nsimilarity-based methods have only embodied static notions of similarity. Deep\nlearning methods, which blur the boundaries between classification-based and\nsimilarity-based approaches, are promising in terms of ability to learn a\nnotion of similarity, but have previously only been used in a conventional\nsmall-closed-class classification setup.\n  Siamese networks have been used to develop learned notions of similarity in\none-shot image tasks, and also for tasks of mostly semantic relatedness in NLP.\nWe examine their application to the stylistic task of authorship attribution on\ndatasets with large numbers of authors, looking at multiple energy functions\nand neural network architectures, and show that they can substantially\noutperform previous approaches.", "published": "2019-12-23 04:38:00", "link": "http://arxiv.org/abs/1912.10616v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics- and Syntax-related Subvectors in the Skip-gram Embeddings", "abstract": "We show that the skip-gram embedding of any word can be decomposed into two\nsubvectors which roughly correspond to semantic and syntactic roles of the\nword.", "published": "2019-12-23 18:01:32", "link": "http://arxiv.org/abs/1912.13413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Protagonist of Sentiment with Aspect Reconstructed Capsule\n  Network", "abstract": "Most recent existing aspect-term level sentiment analysis (ATSA) approaches\ncombined various neural network models with delicately carved attention\nmechanisms built upon given aspect and context to generate refined sentence\nrepresentations for better predictions. In these methods, aspect terms are\nalways provided in both training and testing process which may degrade\naspect-level analysis into sentence-level prediction. However, the annotated\naspect term might be unavailable in real-world scenarios which may challenge\nthe applicability of the existing methods. In this paper, we aim to improve\nATSA by discovering the potential aspect terms of the predicted sentiment\npolarity when the aspect terms of a test sentence are unknown. We access this\ngoal by proposing a capsule network based model named CAPSAR. In CAPSAR,\nsentiment categories are denoted by capsules and aspect term information is\ninjected into sentiment capsules through a sentiment-aspect reconstruction\nprocedure during the training. As a result, coherent patterns between aspects\nand sentimental expressions are encapsulated by these sentiment capsules.\nExperiments on three widely used benchmarks demonstrate these patterns have\npotential in exploring aspect terms from test sentence when only feeding the\nsentence to the model. Meanwhile, the proposed CAPSAR can clearly outperform\nSOTA methods in standard ATSA tasks.", "published": "2019-12-23 13:14:40", "link": "http://arxiv.org/abs/1912.10785v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TextNAS: A Neural Architecture Search Space tailored for Text\n  Representation", "abstract": "Learning text representation is crucial for text classification and other\nlanguage related tasks. There are a diverse set of text representation networks\nin the literature, and how to find the optimal one is a non-trivial problem.\nRecently, the emerging Neural Architecture Search (NAS) techniques have\ndemonstrated good potential to solve the problem. Nevertheless, most of the\nexisting works of NAS focus on the search algorithms and pay little attention\nto the search space. In this paper, we argue that the search space is also an\nimportant human prior to the success of NAS in different applications. Thus, we\npropose a novel search space tailored for text representation. Through\nautomatic search, the discovered network architecture outperforms\nstate-of-the-art models on various public datasets on text classification and\nnatural language inference tasks. Furthermore, some of the design principles\nfound in the automatic network agree well with human intuition.", "published": "2019-12-23 10:51:58", "link": "http://arxiv.org/abs/1912.10729v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BioConceptVec: creating and evaluating literature-based biomedical\n  concept embeddings on a large scale", "abstract": "Capturing the semantics of related biological concepts, such as genes and\nmutations, is of significant importance to many research tasks in computational\nbiology such as protein-protein interaction detection, gene-drug association\nprediction, and biomedical literature-based discovery. Here, we propose to\nleverage state-of-the-art text mining tools and machine learning models to\nlearn the semantics via vector representations (aka. embeddings) of over\n400,000 biological concepts mentioned in the entire PubMed abstracts. Our\nlearned embeddings, namely BioConceptVec, can capture related concepts based on\ntheir surrounding contextual information in the literature, which is beyond\nexact term match or co-occurrence-based methods. BioConceptVec has been\nthoroughly evaluated in multiple bioinformatics tasks consisting of over 25\nmillion instances from nine different biological datasets. The evaluation\nresults demonstrate that BioConceptVec has better performance than existing\nmethods in all tasks. Finally, BioConceptVec is made freely available to the\nresearch community and general public via\nhttps://github.com/ncbi-nlp/BioConceptVec.", "published": "2019-12-23 14:46:46", "link": "http://arxiv.org/abs/1912.10846v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing the phonetic and phonological knowledge of tones in Mandarin TTS\n  models", "abstract": "This study probes the phonetic and phonological knowledge of lexical tones in\nTTS models through two experiments. Controlled stimuli for testing tonal\ncoarticulation and tone sandhi in Mandarin were fed into Tacotron 2 and\nWaveGlow to generate speech samples, which were subject to acoustic analysis\nand human evaluation. Results show that both baseline Tacotron 2 and Tacotron 2\nwith BERT embeddings capture the surface tonal coarticulation patterns well but\nfail to consistently apply the Tone-3 sandhi rule to novel sentences.\nIncorporating pre-trained BERT embeddings into Tacotron 2 improves the\nnaturalness and prosody performance, and yields better generalization of Tone-3\nsandhi rules to novel complex sentences, although the overall accuracy for\nTone-3 sandhi was still low. Given that TTS models do capture some linguistic\nphenomena, it is argued that they can be used to generate and validate certain\nlinguistic hypotheses. On the other hand, it is also suggested that\nlinguistically informed stimuli should be included in the training and the\nevaluation of TTS models.", "published": "2019-12-23 15:25:45", "link": "http://arxiv.org/abs/1912.10915v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mixture of Inference Networks for VAE-based Audio-visual Speech\n  Enhancement", "abstract": "In this paper, we are interested in unsupervised (unknown noise) audio-visual\nspeech enhancement based on variational autoencoders (VAEs), where the\nprobability distribution of clean speech spectra is simulated using an\nencoder-decoder architecture. The trained generative model (decoder) is then\ncombined with a noise model at test time to estimate the clean speech. In the\nspeech enhancement phase (test time), the initialization of the latent\nvariables, which describe the generative process of clean speech via decoder,\nis crucial, as the overall inference problem is non-convex. This is usually\ndone by using the output of the trained encoder where the noisy audio and clean\nvisual data are given as input. Current audio-visual VAE models do not provide\nan effective initialization because the two modalities are tightly coupled\n(concatenated) in the associated architectures. To overcome this issue,\ninspired by mixture models, we introduce the mixture of inference networks\nvariational autoencoder (MIN-VAE). Two encoder networks input, respectively,\naudio and visual data, and the posterior of the latent variables is modeled as\na mixture of two Gaussian distributions output from each encoder network. The\nmixture variable is also latent, and therefore the inference of learning the\noptimal balance between the audio and visual inference networks is unsupervised\nas well. By training a shared decoder, the overall network learns to adaptively\nfuse the two modalities. Moreover, at test time, the visual encoder, which\ntakes (clean) visual data, is used for initialization. A variational inference\napproach is derived to train the proposed generative model. Thanks to the novel\ninference procedure and the robust initialization, the proposed MIN-VAE\nexhibits superior performance on speech enhancement than using the standard\naudio-only as well as audio-visual counterparts.", "published": "2019-12-23 06:55:14", "link": "http://arxiv.org/abs/1912.10647v4", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Transferable Features for Speech Emotion Recognition", "abstract": "Emotion recognition from speech is one of the key steps towards emotional\nintelligence in advanced human-machine interaction. Identifying emotions in\nhuman speech requires learning features that are robust and discriminative\nacross diverse domains that differ in terms of language, spontaneity of speech,\nrecording conditions, and types of emotions. This corresponds to a learning\nscenario in which the joint distributions of features and labels may change\nsubstantially across domains. In this paper, we propose a deep architecture\nthat jointly exploits a convolutional network for extracting domain-shared\nfeatures and a long short-term memory network for classifying emotions using\ndomain-specific features. We use transferable features to enable model\nadaptation from multiple source domains, given the sparseness of speech emotion\ndata and the fact that target domains are short of labeled data. A\ncomprehensive cross-corpora experiment with diverse speech emotion domains\nreveals that transferable features provide gains ranging from 4.3% to 18.4% in\nspeech emotion recognition. We evaluate several domain adaptation approaches,\nand we perform an ablation study to understand which source domains add the\nmost to the overall recognition effectiveness for a given target domain.", "published": "2019-12-23 18:06:08", "link": "http://arxiv.org/abs/1912.11547v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "I.2.6", "I.2.6"], "primary_category": "eess.AS"}
