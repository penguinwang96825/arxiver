{"title": "Grammar construction methods for extended deterministic expressions", "abstract": "Extended regular expressions with counting and interleaving are widely used\nin practice. However the related theoretical studies for this kind of\nexpressions currently cannot meet the need of practical work. This paper\ndevelops syntax definitions for extended deterministic expressions and their\nsubclasses, hope to completely solve the long-standing problem that there are\nno syntax definitions for this kind of expressions, which has become an\nimportant reason for restricting the use of extended expressions.", "published": "2023-01-04 13:49:14", "link": "http://arxiv.org/abs/2301.01621v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical\n  Simplification?", "abstract": "Previous state-of-the-art models for lexical simplification consist of\ncomplex pipelines with several components, each of which requires deep\ntechnical knowledge and fine-tuned interaction to achieve its full potential.\nAs an alternative, we describe a frustratingly simple pipeline based on\nprompted GPT-3 responses, beating competing approaches by a wide margin in\nsettings with few training instances. Our best-performing submission to the\nEnglish language track of the TSAR-2022 shared task consists of an ``ensemble''\nof six different prompt templates with varying context levels. As a\nlate-breaking result, we further detail a language transfer technique that\nallows simplification in languages other than English. Applied to the Spanish\nand Portuguese subset, we achieve state-of-the-art results with only minor\nmodification to the original prompts. Aside from detailing the implementation\nand setup, we spend the remainder of this work discussing the particularities\nof prompting and implications for future work. Code for the experiments is\navailable online at https://github.com/dennlinger/TSAR-2022-Shared-Task", "published": "2023-01-04 18:59:20", "link": "http://arxiv.org/abs/2301.01764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Aspect Explainable Inductive Relation Prediction by Sentence\n  Transformer", "abstract": "Recent studies on knowledge graphs (KGs) show that path-based methods\nempowered by pre-trained language models perform well in the provision of\ninductive and explainable relation predictions. In this paper, we introduce the\nconcepts of relation path coverage and relation path confidence to filter out\nunreliable paths prior to model training to elevate the model performance.\nMoreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict\ninductive relations in KGs. KRST is designed to encode the extracted reliable\npaths in KGs, allowing us to properly cluster paths and provide multi-aspect\nexplanations. We conduct extensive experiments on three real-world datasets.\nThe experimental results show that compared to SOTA models, KRST achieves the\nbest performance in most transductive and inductive test cases (4 of 6), and in\n11 of 12 few-shot test cases.", "published": "2023-01-04 15:33:49", "link": "http://arxiv.org/abs/2301.01664v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MessageNet: Message Classification using Natural Language Processing and\n  Meta-data", "abstract": "In this paper we propose a new Deep Learning (DL) approach for message\nclassification. Our method is based on the state-of-the-art Natural Language\nProcessing (NLP) building blocks, combined with a novel technique for infusing\nthe meta-data input that is typically available in messages such as the sender\ninformation, timestamps, attached image, audio, affiliations, and more. As we\ndemonstrate throughout the paper, going beyond the mere text by leveraging all\navailable channels in the message, could yield an improved representation and\nhigher classification accuracy. To achieve message representation, each type of\ninput is processed in a dedicated block in the neural network architecture that\nis suitable for the data type. Such an implementation enables training all\nblocks together simultaneously, and forming cross channels features in the\nnetwork. We show in the Experiments Section that in some cases, message's\nmeta-data holds an additional information that cannot be extracted just from\nthe text, and when using this information we achieve better performance.\nFurthermore, we demonstrate that our multi-modality block approach outperforms\nother approaches for injecting the meta data to the the text classifier.", "published": "2023-01-04 20:11:00", "link": "http://arxiv.org/abs/2301.01808v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Parameter-Efficient Fine-Tuning Design Spaces", "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.", "published": "2023-01-04 21:00:18", "link": "http://arxiv.org/abs/2301.01821v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comprehensive review of automatic text summarization techniques:\n  method, data, evaluation and coding", "abstract": "We provide a literature review about Automatic Text Summarization (ATS)\nsystems. We consider a citation-based approach. We start with some popular and\nwell-known papers that we have in hand about each topic we want to cover and we\nhave tracked the \"backward citations\" (papers that are cited by the set of\npapers we knew beforehand) and the \"forward citations\" (newer papers that cite\nthe set of papers we knew beforehand). In order to organize the different\nmethods, we present the diverse approaches to ATS guided by the mechanisms they\nuse to generate a summary. Besides presenting the methods, we also present an\nextensive review of the datasets available for summarization tasks and the\nmethods used to evaluate the quality of the summaries. Finally, we present an\nempirical exploration of these methods using the CNN Corpus dataset that\nprovides golden summaries for extractive and abstractive methods.", "published": "2023-01-04 19:20:18", "link": "http://arxiv.org/abs/2301.03403v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Efficient Conformer for Robust Speech Recognition", "abstract": "End-to-end Automatic Speech Recognition (ASR) systems based on neural\nnetworks have seen large improvements in recent years. The availability of\nlarge scale hand-labeled datasets and sufficient computing resources made it\npossible to train powerful deep neural networks, reaching very low Word Error\nRate (WER) on academic benchmarks. However, despite impressive performance on\nclean audio samples, a drop of performance is often observed on noisy speech.\nIn this work, we propose to improve the noise robustness of the recently\nproposed Efficient Conformer Connectionist Temporal Classification (CTC)-based\narchitecture by processing both audio and visual modalities. We improve\nprevious lip reading methods using an Efficient Conformer back-end on top of a\nResNet-18 visual front-end and by adding intermediate CTC losses between\nblocks. We condition intermediate block features on early predictions using\nInter CTC residual modules to relax the conditional independence assumption of\nCTC-based models. We also replace the Efficient Conformer grouped attention by\na more efficient and simpler attention mechanism that we call patch attention.\nWe experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3) datasets. Our experiments show that using audio and\nvisual modalities allows to better recognize speech in the presence of\nenvironmental noise and significantly accelerate training, reaching lower WER\nwith 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)\nmodel achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on\nLRS2 and LRS3 test sets. Code and pretrained models are available at\nhttps://github.com/burchim/AVEC.", "published": "2023-01-04 05:36:56", "link": "http://arxiv.org/abs/2301.01456v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Learning Ambiguity from Crowd Sequential Annotations", "abstract": "Most crowdsourcing learning methods treat disagreement between annotators as\nnoisy labelings while inter-disagreement among experts is often a good\nindicator for the ambiguity and uncertainty that is inherent in natural\nlanguage. In this paper, we propose a framework called Learning Ambiguity from\nCrowd Sequential Annotations (LA-SCA) to explore the inter-disagreement between\nreliable annotators and effectively preserve confusing label information.\nFirst, a hierarchical Bayesian model is developed to infer ground-truth from\ncrowds and group the annotators with similar reliability together. By modeling\nthe relationship between the size of group the annotator involved in, the\nannotator's reliability and element's unambiguity in each sequence,\ninter-disagreement between reliable annotators on ambiguous elements is\ncomputed to obtain label confusing information that is incorporated to\ncost-sensitive sequence labeling. Experimental results on POS tagging and NER\ntasks show that our proposed framework achieves competitive performance in\ninferring ground-truth from crowds and predicting unknown sequences, and\ninterpreting hierarchical clustering results helps discover labeling patterns\nof annotators with similar reliability.", "published": "2023-01-04 12:53:56", "link": "http://arxiv.org/abs/2301.01579v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning\n  Processes", "abstract": "Language models (LMs) can perform complex reasoning either end-to-end, with\nhidden latent state, or compositionally, with transparent intermediate state.\nComposition offers benefits for interpretability and safety, but may need\nworkflow support and infrastructure to remain competitive. We describe iterated\ndecomposition, a human-in-the-loop workflow for developing and refining\ncompositional LM programs. We improve the performance of compositions by\nzooming in on failing components and refining them through decomposition,\nadditional context, chain of thought, etc. To support this workflow, we develop\nICE, an open-source tool for visualizing the execution traces of LM programs.\nWe apply iterated decomposition to three real-world tasks and improve the\naccuracy of LM programs over less compositional baselines: describing the\nplacebo used in a randomized controlled trial (25% to 65%), evaluating\nparticipant adherence to a medical intervention (53% to 70%), and answering NLP\nquestions on the Qasper dataset (38% to 69%). These applications serve as case\nstudies for a workflow that, if automated, could keep ML systems interpretable\nand safe even as they scale to increasingly complex tasks.", "published": "2023-01-04 18:34:25", "link": "http://arxiv.org/abs/2301.01751v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Rumor Classification through a Multimodal Fusion Framework and Ensemble\n  Learning", "abstract": "The proliferation of rumors on social media has become a major concern due to\nits ability to create a devastating impact. Manually assessing the veracity of\nsocial media messages is a very time-consuming task that can be much helped by\nmachine learning. Most message veracity verification methods only exploit\ntextual contents and metadata. Very few take both textual and visual contents,\nand more particularly images, into account. Moreover, prior works have used\nmany classical machine learning models to detect rumors. However, although\nrecent studies have proven the effectiveness of ensemble machine learning\napproaches, such models have seldom been applied. Thus, in this paper, we\npropose a set of advanced image features that are inspired from the field of\nimage quality assessment, and introduce the Multimodal fusiON framework to\nassess message veracIty in social neTwORks (MONITOR), which exploits all\nmessage features by exploring various machine learning models. Moreover, we\ndemonstrate the effectiveness of ensemble learning algorithms for rumor\ndetection by using five metalearning models. Eventually, we conduct extensive\nexperiments on two real-world datasets. Results show that MONITOR outperforms\nstate-of-the-art machine learning baselines and that ensemble models\nsignificantly increase MONITOR's performance.", "published": "2023-01-04 13:15:08", "link": "http://arxiv.org/abs/2302.05289v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.CV"}
{"title": "Validity in Music Information Research Experiments", "abstract": "Validity is the truth of an inference made from evidence, such as data\ncollected in an experiment, and is central to working scientifically. Given the\nmaturity of the domain of music information research (MIR), validity in our\nopinion should be discussed and considered much more than it has been so far.\nConsidering validity in one's work can improve its scientific and engineering\nvalue. Puzzling MIR phenomena like adversarial attacks and performance glass\nceilings become less mysterious through the lens of validity. In this article,\nwe review the subject of validity in general, considering the four major types\nof validity from a key reference: Shadish et al. 2002. We ground our discussion\nof these types with a prototypical MIR experiment: music classification using\nmachine learning. Through this MIR experimentalists can be guided to make valid\ninferences from data collected from their experiments.", "published": "2023-01-04 12:52:47", "link": "http://arxiv.org/abs/2301.01578v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Grid-Based Decimation for Wavelet Transforms with Stably Invertible\n  Implementation", "abstract": "The constant center frequency to bandwidth ratio (Q-factor) of wavelet\ntransforms provides a very natural representation for audio data. However,\ninvertible wavelet transforms have either required non-uniform decimation --\nleading to irregular data structures that are cumbersome to work with -- or\nrequire excessively high oversampling with unacceptable computational overhead.\nHere, we present a novel decimation strategy for wavelet transforms that leads\nto stable representations with oversampling rates close to one and uniform\ndecimation. Specifically, we show that finite implementations of the resulting\nrepresentation are energy-preserving in the sense of frame theory. The obtained\nwavelet coefficients can be stored in a timefrequency matrix with a natural\ninterpretation of columns as time frames and rows as frequency channels. This\nmatrix structure immediately grants access to a large number of algorithms that\nare successfully used in time-frequency audio processing, but could not\npreviously be used jointly with wavelet transforms. We demonstrate the\napplication of our method in processing based on nonnegative matrix\nfactorization, in onset detection, and in phaseless reconstruction.", "published": "2023-01-04 14:32:09", "link": "http://arxiv.org/abs/2301.01640v1", "categories": ["eess.AS", "cs.NA", "math.FA", "math.NA", "65T60, 42C40"], "primary_category": "eess.AS"}
{"title": "Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations", "abstract": "Can conversational videos captured from multiple egocentric viewpoints reveal\nthe map of a scene in a cost-efficient way? We seek to answer this question by\nproposing a new problem: efficiently building the map of a previously unseen 3D\nenvironment by exploiting shared information in the egocentric audio-visual\nobservations of participants in a natural conversation. Our hypothesis is that\nas multiple people (\"egos\") move in a scene and talk among themselves, they\nreceive rich audio-visual cues that can help uncover the unseen areas of the\nscene. Given the high cost of continuously processing egocentric visual\nstreams, we further explore how to actively coordinate the sampling of visual\ninformation, so as to minimize redundancy and reduce power use. To that end, we\npresent an audio-visual deep reinforcement learning approach that works with\nour shared scene mapper to selectively turn on the camera to efficiently chart\nout the space. We evaluate the approach using a state-of-the-art audio-visual\nsimulator for 3D scenes as well as real-world video. Our model outperforms\nprevious state-of-the-art mapping methods, and achieves an excellent\ncost-accuracy tradeoff. Project: http://vision.cs.utexas.edu/projects/chat2map.", "published": "2023-01-04 18:47:32", "link": "http://arxiv.org/abs/2301.02184v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Object Segmentation with Audio Context", "abstract": "Visual objects often have acoustic signatures that are naturally synchronized\nwith them in audio-bearing video recordings. For this project, we explore the\nmultimodal feature aggregation for video instance segmentation task, in which\nwe integrate audio features into our video segmentation model to conduct an\naudio-visual learning scheme. Our method is based on existing video instance\nsegmentation method which leverages rich contextual information across video\nframes. Since this is the first attempt to investigate the audio-visual\ninstance segmentation, a novel dataset, including 20 vocal classes with\nsynchronized video and audio recordings, is collected. By utilizing combined\ndecoder to fuse both video and audio features, our model shows a slight\nimprovements compared to the base model. Additionally, we managed to show the\neffectiveness of different modules by conducting extensive ablations.", "published": "2023-01-04 01:33:42", "link": "http://arxiv.org/abs/2301.10295v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
