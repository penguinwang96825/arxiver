{"title": "Exploiting Linguistic Resources for Neural Machine Translation Using\n  Multi-task Learning", "abstract": "Linguistic resources such as part-of-speech (POS) tags have been extensively\nused in statistical machine translation (SMT) frameworks and have yielded\nbetter performances. However, usage of such linguistic annotations in neural\nmachine translation (NMT) systems has been left under-explored.\n  In this work, we show that multi-task learning is a successful and a easy\napproach to introduce an additional knowledge into an end-to-end neural\nattentional model. By jointly training several natural language processing\n(NLP) tasks in one system, we are able to leverage common information and\nimprove the performance of the individual task.\n  We analyze the impact of three design decisions in multi-task learning: the\ntasks used in training, the training schedule, and the degree of parameter\nsharing across the tasks, which is defined by the network architecture. The\nexperiments are conducted for an German to English translation task. As\nadditional linguistic resources, we exploit POS information and named-entities\n(NE). Experiments show that the translation quality can be improved by up to\n1.5 BLEU points under the low-resource condition. The performance of the POS\ntagger is also improved using the multi-task learning scheme.", "published": "2017-08-03 04:30:37", "link": "http://arxiv.org/abs/1708.00993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRF Autoencoder for Unsupervised Dependency Parsing", "abstract": "Unsupervised dependency parsing, which tries to discover linguistic\ndependency structures from unannotated data, is a very challenging task. Almost\nall previous work on this task focuses on learning generative models. In this\npaper, we develop an unsupervised dependency parsing model based on the CRF\nautoencoder. The encoder part of our model is discriminative and globally\nnormalized which allows us to use rich features as well as universal linguistic\npriors. We propose an exact algorithm for parsing as well as a tractable\nlearning algorithm. We evaluated the performance of our model on eight\nmultilingual treebanks and found that our model achieved comparable performance\nwith state-of-the-art approaches.", "published": "2017-08-03 06:45:31", "link": "http://arxiv.org/abs/1708.01018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Activation Regularization for Language RNNs", "abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for\nmany sequence tasks across natural language processing. Recent research has\nfocused on recurrent dropout techniques or custom RNN cells in order to improve\nperformance. Both of these can require substantial modifications to the machine\nlearning model or to the underlying RNN configurations. We revisit traditional\nregularization techniques, specifically L2 regularization on RNN activations\nand slowness regularization over successive hidden states, to improve the\nperformance of RNNs on the task of language modeling. Both of these techniques\nrequire minimal modification to existing RNN architectures and result in\nperformance improvements comparable or superior to more complicated\nregularization techniques or custom cell architectures. These regularization\ntechniques can be used without any modification on optimized LSTM\nimplementations such as the NVIDIA cuDNN LSTM.", "published": "2017-08-03 05:53:53", "link": "http://arxiv.org/abs/1708.01009v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Reader-Aware Multi-Document Summarization: An Enhanced Model and The\n  First Dataset", "abstract": "We investigate the problem of reader-aware multi-document summarization\n(RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we\nextend a variational auto-encodes (VAEs) based MDS framework by jointly\nconsidering news documents and reader comments. To conduct evaluation for\nsummarization performance, we prepare a new dataset. We describe the methods\nfor data collection, aspect annotation, and summary writing as well as\nscrutinizing by experts. Experimental results show that reader comments can\nimprove the summarization performance, which also demonstrates the usefulness\nof the proposed dataset. The annotated dataset for RA-MDS is available online.", "published": "2017-08-03 09:18:16", "link": "http://arxiv.org/abs/1708.01065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task", "abstract": "We describe the University of Maryland machine translation systems submitted\nto the WMT17 German-English Bandit Learning Task. The task is to adapt a\ntranslation system to a new domain, using only bandit feedback: the system\nreceives a German sentence to translate, produces an English sentence, and only\ngets a scalar score as feedback. Targeting these two challenges (adaptation and\nbandit learning), we built a standard neural machine translation system and\nextended it in two ways: (1) robust reinforcement learning techniques to learn\neffectively from the bandit feedback, and (2) domain adaptation using data\nselection from a large corpus of parallel data.", "published": "2017-08-03 21:42:46", "link": "http://arxiv.org/abs/1708.01318v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
