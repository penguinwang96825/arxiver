{"title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating", "abstract": "While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.", "published": "2024-02-14 00:40:51", "link": "http://arxiv.org/abs/2402.08874v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Language Generation Model for Robust Structure Prediction", "abstract": "Previous work in structured prediction (e.g. NER, information extraction)\nusing single model make use of explicit dataset information, which helps boost\nin-distribution performance but is orthogonal to robust generalization in\nreal-world situations. To overcome this limitation, we propose the Structured\nLanguage Generation Model (SLGM), a framework that reduces sequence-to-sequence\nproblems to classification problems via methodologies in loss calibration and\ndecoding method. Our experimental results show that SLGM is able to maintain\nperformance without explicit dataset information, follow and potentially\nreplace dataset-specific fine-tuning.", "published": "2024-02-14 06:33:22", "link": "http://arxiv.org/abs/2402.08971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Query Focused Disaster Summarization via Instruction-Based\n  Prompting", "abstract": "Automatic summarization of mass-emergency events plays a critical role in\ndisaster management. The second edition of CrisisFACTS aims to advance disaster\nsummarization based on multi-stream fact-finding with a focus on web sources\nsuch as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to\ndevelop systems that can extract key facts from several disaster-related\nevents, which ultimately serve as a summary. This paper describes our method to\ntackle this challenging task. We follow previous work and propose to use a\ncombination of retrieval, reranking, and an embarrassingly simple\ninstruction-following summarization. The two-stage retrieval pipeline relies on\nBM25 and MonoT5, while the summarizer module is based on the open-source Large\nLanguage Model (LLM) LLaMA-13b. For summarization, we explore a Question\nAnswering (QA)-motivated prompting approach and find the evidence useful for\nextracting query-relevant facts. The automatic metrics and human evaluation\nshow strong results but also highlight the gap between open-source and\nproprietary systems.", "published": "2024-02-14 08:22:58", "link": "http://arxiv.org/abs/2402.09008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Enhanced NLP Task Performance\n  through Knowledge Distillation and Optimized Training Strategies", "abstract": "Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural\nLanguage Processing (NLP), showing potential in traditional tasks such as Named\nEntity Recognition (NER). Our study explores a three-phase training strategy\nthat harnesses GPT-4's capabilities to enhance the BERT model's performance on\nNER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC\ndataset without fine-tuning. We then train BERT using a mix of original and\nLLM-annotated data, analyzing the efficacy of LLM annotations against\ntraditional methods. The second phase involves comparative experiments with\ndifferent training regimens, assessing the synergy between distilled and\noriginal data. We observe that sequential strategies, particularly a simple mix\nof training first with distilled data followed by original data, significantly\nboost performance. In the third phase, we investigate various data blending\ntechniques, including sigmoid and power decay functions, to optimize the\ntraining process further. Our results indicate that a strategic mix of\ndistilled and original data markedly elevates the NER capabilities of BERT. Our\napproach presents a scalable methodology that reduces manual annotation costs\nand increases efficiency, making it especially pertinent in resource-limited\nand closed-network environments. The study concludes that while the 'Simple\nMix' strategy yields the best results, understanding its underlying mechanisms\nrequires further research. Future work will also focus on refining prompt\ndesigns and enhancing annotation selection processes, aiming to extend our\nmethodology to diverse NLP tasks.", "published": "2024-02-14 16:10:45", "link": "http://arxiv.org/abs/2402.09282v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Diverse Translation with Perturbed kNN-MT", "abstract": "Generating multiple translation candidates would enable users to choose the\none that satisfies their needs. Although there has been work on diversified\ngeneration, there exists room for improving the diversity mainly because the\nprevious methods do not address the overcorrection problem -- the model\nunderestimates a prediction that is largely different from the training data,\neven if that prediction is likely. This paper proposes methods that generate\nmore diverse translations by introducing perturbed k-nearest neighbor machine\ntranslation (kNN-MT). Our methods expand the search space of kNN-MT and help\nincorporate diverse words into candidates by addressing the overcorrection\nproblem. Our experiments show that the proposed methods drastically improve\ncandidate diversity and control the degree of diversity by tuning the\nperturbation's magnitude.", "published": "2024-02-14 17:46:46", "link": "http://arxiv.org/abs/2402.09344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking", "abstract": "Pretrained large language models have revolutionized many applications but\nstill face challenges related to cultural bias and a lack of cultural\ncommonsense knowledge crucial for guiding cross-culture communication and\ninteractions. Recognizing the shortcomings of existing methods in capturing the\ndiverse and rich cultures across the world, this paper introduces a novel\napproach for massively multicultural knowledge acquisition. Specifically, our\nmethod strategically navigates from densely informative Wikipedia documents on\ncultural topics to an extensive network of linked pages. Leveraging this\nvaluable source of data collection, we construct the CultureAtlas dataset,\nwhich covers a wide range of sub-country level geographical regions and\nethnolinguistic groups, with data cleaning and preprocessing to ensure textual\nassertion sentence self-containment, as well as fine-grained cultural profile\ninformation extraction. Our dataset not only facilitates the evaluation of\nlanguage model performance in culturally diverse contexts but also serves as a\nfoundational tool for the development of culturally sensitive and aware\nlanguage models. Our work marks an important step towards deeper understanding\nand bridging the gaps of cultural disparities in AI, to promote a more\ninclusive and balanced representation of global cultures in the digital domain.", "published": "2024-02-14 18:16:54", "link": "http://arxiv.org/abs/2402.09369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-form evaluation of model editing", "abstract": "Evaluations of model editing currently only use the `next few token'\ncompletions after a prompt. As a result, the impact of these methods on longer\nnatural language generation is largely unknown. We introduce long-form\nevaluation of model editing (LEME) a novel evaluation protocol that measures\nthe efficacy and impact of model editing in long-form generative settings. Our\nprotocol consists of a machine-rated survey and a classifier which correlates\nwell with human ratings. Importantly, we find that our protocol has very little\nrelationship with previous short-form metrics (despite being designed to extend\nefficacy, generalization, locality, and portability into a long-form setting),\nindicating that our method introduces a novel set of dimensions for\nunderstanding model editing methods. Using this protocol, we benchmark a number\nof model editing techniques and present several findings including that, while\nsome methods (ROME and MEMIT) perform well in making consistent edits within a\nlimited scope, they suffer much more from factual drift than other methods.\nFinally, we present a qualitative analysis that illustrates common failure\nmodes in long-form generative settings including internal consistency, lexical\ncohesion, and locality issues.", "published": "2024-02-14 18:45:14", "link": "http://arxiv.org/abs/2402.09394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Premise Order Matters in Reasoning with Large Language Models", "abstract": "Large language models (LLMs) have accomplished remarkable reasoning\nperformance in various domains. However, in the domain of reasoning tasks, we\ndiscover a frailty: LLMs are surprisingly brittle to the ordering of the\npremises, despite the fact that such ordering does not alter the underlying\ntask. In particular, we observe that LLMs achieve the best performance when the\npremise order aligns with the context required in intermediate reasoning steps.\nFor example, in deductive reasoning tasks, presenting the premises in the same\norder as the ground truth proof in the prompt (as opposed to random ordering)\ndrastically increases the model's accuracy. We first examine the effect of\npremise ordering on deductive reasoning on a variety of LLMs, and our\nevaluation shows that permuting the premise order can cause a performance drop\nof over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to\nexamine the ordering effect for mathematical problem-solving, and we again\nobserve a significant drop in accuracy, relative to the original GSM8K\nbenchmark.", "published": "2024-02-14 04:50:18", "link": "http://arxiv.org/abs/2402.08939v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical\n  Reasoning in Large Language Models", "abstract": "Large language models (LLMs) have performed well on several reasoning\nbenchmarks, including ones that test analogical reasoning abilities. However,\nit has been debated whether they are actually performing humanlike abstract\nreasoning or instead employing less general processes that rely on similarity\nto what has been seen in their training data. Here we investigate the\ngenerality of analogy-making abilities previously claimed for LLMs (Webb,\nHolyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs\nand create a set of \"counterfactual\" variants-versions that test the same\nabstract reasoning abilities but that are likely dissimilar from any\npre-training data. We test humans and three GPT models on both the original and\ncounterfactual problems, and show that, while the performance of humans remains\nhigh for all the problems, the GPT models' performance declines sharply on the\ncounterfactual set. This work provides evidence that, despite previously\nreported successes of LLMs on analogical reasoning, these models lack the\nrobustness and generality of human analogy-making.", "published": "2024-02-14 05:52:23", "link": "http://arxiv.org/abs/2402.08955v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Pretraining Vision-Language Model for Difference Visual Question\n  Answering in Longitudinal Chest X-rays", "abstract": "Difference visual question answering (diff-VQA) is a challenging task that\nrequires answering complex questions based on differences between a pair of\nimages. This task is particularly important in reading chest X-ray images\nbecause radiologists often compare multiple images of the same patient taken at\ndifferent times to track disease progression and changes in its severity in\ntheir clinical practice. However, previous works focused on designing specific\nnetwork architectures for the diff-VQA task, missing opportunities to enhance\nthe model's performance using a pretrained vision-language model (VLM). Here,\nwe introduce a novel VLM called PLURAL, which is pretrained on natural and\nlongitudinal chest X-ray data for the diff-VQA task. The model is developed\nusing a step-by-step approach, starting with being pretrained on natural images\nand texts, followed by being trained using longitudinal chest X-ray data. The\nlongitudinal data consist of pairs of X-ray images, along with question-answer\nsets and radiologist's reports that describe the changes in lung abnormalities\nand diseases over time. Our experimental results show that the PLURAL model\noutperforms state-of-the-art methods not only in diff-VQA for longitudinal\nX-rays but also in conventional VQA for a single X-ray image. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed VLM architecture\nand pretraining method in improving the model's performance.", "published": "2024-02-14 06:20:48", "link": "http://arxiv.org/abs/2402.08966v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards better Human-Agent Alignment: Assessing Task Utility in\n  LLM-Powered Applications", "abstract": "The rapid development in the field of Large Language Models (LLMs) has led to\na surge in applications that facilitate collaboration among multiple agents to\nassist humans in their daily tasks. However, a significant gap remains in\nassessing whether LLM-powered applications genuinely enhance user experience\nand task execution efficiency. This highlights the pressing need for methods to\nverify utility of LLM-powered applications, particularly by ensuring alignment\nbetween the application's functionality and end-user needs. We introduce\nAgentEval provides an implementation for the math problems, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the robustness of quantifier's work.", "published": "2024-02-14 08:46:15", "link": "http://arxiv.org/abs/2402.09015v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks", "abstract": "Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.", "published": "2024-02-14 09:01:13", "link": "http://arxiv.org/abs/2402.09025v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and\n  Multi-Objective Instruction Tuning", "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.", "published": "2024-02-14 12:34:58", "link": "http://arxiv.org/abs/2402.09136v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing NLP Models with Strategic Text Augmentation: A Comprehensive\n  Study of Augmentation Methods and Curriculum Strategies", "abstract": "This study conducts a thorough evaluation of text augmentation techniques\nacross a variety of datasets and natural language processing (NLP) tasks to\naddress the lack of reliable, generalized evidence for these methods. It\nexamines the effectiveness of these techniques in augmenting training sets to\nimprove performance in tasks such as topic classification, sentiment analysis,\nand offensive language detection. The research emphasizes not only the\naugmentation methods, but also the strategic order in which real and augmented\ninstances are introduced during training. A major contribution is the\ndevelopment and evaluation of Modified Cyclical Curriculum Learning (MCCL) for\naugmented datasets, which represents a novel approach in the field. Results\nshow that specific augmentation methods, especially when integrated with MCCL,\nsignificantly outperform traditional training approaches in NLP model\nperformance. These results underscore the need for careful selection of\naugmentation techniques and sequencing strategies to optimize the balance\nbetween speed and quality improvement in various NLP tasks. The study concludes\nthat the use of augmentation methods, especially in conjunction with MCCL,\nleads to improved results in various classification tasks, providing a\nfoundation for future advances in text augmentation strategies in NLP.", "published": "2024-02-14 12:41:09", "link": "http://arxiv.org/abs/2402.09141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for\n  Chinese Mental Health Text Analysis", "abstract": "In the current environment, psychological issues are prevalent and\nwidespread, with social media serving as a key outlet for individuals to share\ntheir feelings. This results in the generation of vast quantities of data\ndaily, where negative emotions have the potential to precipitate crisis\nsituations. There is a recognized need for models capable of efficient\nanalysis. While pre-trained language models have demonstrated their\neffectiveness broadly, there's a noticeable gap in pre-trained models tailored\nfor specialized domains like psychology. To address this, we have collected a\nhuge dataset from Chinese social media platforms and enriched it with publicly\navailable datasets to create a comprehensive database encompassing 3.36 million\ntext entries. To enhance the model's applicability to psychological text\nanalysis, we integrated psychological lexicons into the pre-training masking\nmechanism. Building on an existing Chinese language model, we performed\nadaptive training to develop a model specialized for the psychological domain.\nWe evaluated our model's performance across six public datasets, where it\ndemonstrated improvements compared to eight other models. Additionally, in the\nqualitative comparison experiment, our model provided psychologically relevant\npredictions given the masked sentences. Due to concerns regarding data privacy,\nthe dataset will not be made publicly available. However, we have made the\npre-trained models and codes publicly accessible to the community via:\nhttps://github.com/zwzzzQAQ/Chinese-MentalBERT.", "published": "2024-02-14 13:08:25", "link": "http://arxiv.org/abs/2402.09151v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoTutor meets Large Language Models: A Language Model Tutor with Rich\n  Pedagogy and Guardrails", "abstract": "Large Language Models (LLMs) have found several use cases in education,\nranging from automatic question generation to essay evaluation. In this paper,\nwe explore the potential of using Large Language Models (LLMs) to author\nIntelligent Tutoring Systems. A common pitfall of LLMs is their straying from\ndesired pedagogical strategies such as leaking the answer to the student, and\nin general, providing no guarantees. We posit that while LLMs with certain\nguardrails can take the place of subject experts, the overall pedagogical\ndesign still needs to be handcrafted for the best learning results. Based on\nthis principle, we create a sample end-to-end tutoring system named MWPTutor,\nwhich uses LLMs to fill in the state space of a pre-defined finite state\ntransducer. This approach retains the structure and the pedagogy of traditional\ntutoring systems that has been developed over the years by learning scientists\nbut brings in additional flexibility of LLM-based approaches. Through a human\nevaluation study on two datasets based on math word problems, we show that our\nhybrid approach achieves a better overall tutoring score than an instructed,\nbut otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the\nscope for the community to improve its performance by improving individual\nmodules or using different teaching strategies that it can follow.", "published": "2024-02-14 14:53:56", "link": "http://arxiv.org/abs/2402.09216v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Spectral Filters, Dark Signals, and Attention Sinks", "abstract": "Projecting intermediate representations onto the vocabulary is an\nincreasingly popular interpretation tool for transformer-based LLMs, also known\nas the logit lens. We propose a quantitative extension to this approach and\ndefine spectral filters on intermediate representations based on partitioning\nthe singular vectors of the vocabulary embedding and unembedding matrices into\nbands. We find that the signals exchanged in the tail end of the spectrum are\nresponsible for attention sinking (Xiao et al. 2023), of which we provide an\nexplanation. We find that the loss of pretrained models can be kept low despite\nsuppressing sizable parts of the embedding spectrum in a layer-dependent way,\nas long as attention sinking is preserved. Finally, we discover that the\nrepresentation of tokens that draw attention from many tokens have large\nprojections on the tail end of the spectrum.", "published": "2024-02-14 15:01:07", "link": "http://arxiv.org/abs/2402.09221v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SyntaxShap: Syntax-aware Explainability Method for Text Generation", "abstract": "To harness the power of large language models in safety-critical domains, we\nneed to ensure the explainability of their predictions. However, despite the\nsignificant attention to model interpretability, there remains an unexplored\ndomain in explaining sequence-to-sequence tasks using methods tailored for\ntextual data. This paper introduces SyntaxShap, a local, model-agnostic\nexplainability method for text generation that takes into consideration the\nsyntax in the text data. The presented work extends Shapley values to account\nfor parsing-based syntactic dependencies. Taking a game theoric approach,\nSyntaxShap only considers coalitions constraint by the dependency tree. We\nadopt a model-based evaluation to compare SyntaxShap and its weighted form to\nstate-of-the-art explainability methods adapted to text generation tasks, using\ndiverse metrics including faithfulness, coherency, and semantic alignment of\nthe explanations to the model. We show that our syntax-aware method produces\nexplanations that help build more faithful and coherent explanations for\npredictions by autoregressive models. Confronted with the misalignment of human\nand AI model reasoning, this paper also highlights the need for cautious\nevaluation strategies in explainable AI.", "published": "2024-02-14 15:45:56", "link": "http://arxiv.org/abs/2402.09259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via\n  Self-Evaluation", "abstract": "Despite showing increasingly human-like abilities, large language models\n(LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even\nwhen they hold relevant knowledge. To address these hallucinations, current\napproaches typically necessitate high-quality human factuality annotations. In\nthis work, we explore Self-Alignment for Factuality, where we leverage the\nself-evaluation capability of an LLM to provide training signals that steer the\nmodel towards factuality. Specifically, we incorporate Self-Eval, a\nself-evaluation component, to prompt an LLM to validate the factuality of its\nown generated responses solely based on its internal knowledge. Additionally,\nwe design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's\nself-evaluation ability by improving the model's confidence estimation and\ncalibration. We then utilize these self-annotated responses to fine-tune the\nmodel via Direct Preference Optimization algorithm. We show that the proposed\nself-alignment approach substantially enhances factual accuracy over Llama\nfamily models across three key knowledge-intensive tasks on TruthfulQA and\nBioGEN.", "published": "2024-02-14 15:52:42", "link": "http://arxiv.org/abs/2402.09267v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalized Large Language Models", "abstract": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.", "published": "2024-02-14 15:55:30", "link": "http://arxiv.org/abs/2402.09269v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ICDPO: Effectively Borrowing Alignment Capability of Others via\n  In-context Direct Preference Optimization", "abstract": "Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to\nensure the generation of safe content. Due to the heavy cost associated with\nfine-tuning, fine-tuning-free methods have emerged, typically modifying LLM\ndecoding with external auxiliary methods. However, these methods do not\nessentially enhance the LLM itself. In this paper, we rethink the derivation\nprocedures of DPO, based on which we conversely build an instant scorer using\nthe states of the LLM before and after In-context Learning (ICL). Accordingly,\nwe propose a novel approach called In-Context Direct Preference Optimization\n(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with\nICL, generating well-aligned responses as estimated by the aforementioned\ninstant scorer, thereby enhancing the final performance. ICDPO can be further\nenhanced with a two-stage retriever and an upgraded scorer, both offering\nbenefits. Extensive experiments show its effectiveness, particularly in\noutperforming two fine-tuning-free baselines, and it exhibits competitiveness\nwith SFT + LoRA. We also conduct detailed analyses to offer comprehensive\ninsights into ICDPO.", "published": "2024-02-14 17:14:34", "link": "http://arxiv.org/abs/2402.09320v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DoRA: Weight-Decomposed Low-Rank Adaptation", "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.", "published": "2024-02-14 17:59:34", "link": "http://arxiv.org/abs/2402.09353v6", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Copyright Traps for Large Language Models", "abstract": "Questions of fair use of copyright-protected content to train Large Language\nModels (LLMs) are being actively debated. Document-level inference has been\nproposed as a new task: inferring from black-box access to the trained model\nwhether a piece of content has been seen during training. SOTA methods however\nrely on naturally occurring memorization of (part of) the content. While very\neffective against models that memorize significantly, we hypothesize--and later\nconfirm--that they will not work against models that do not naturally memorize,\ne.g. medium-size 1B models. We here propose to use copyright traps, the\ninclusion of fictitious entries in original content, to detect the use of\ncopyrighted materials in LLMs with a focus on models where memorization does\nnot naturally occur. We carefully design a randomized controlled experimental\nsetup, inserting traps into original content (books) and train a 1.3B LLM from\nscratch. We first validate that the use of content in our target model would be\nundetectable using existing methods. We then show, contrary to intuition, that\neven medium-length trap sentences repeated a significant number of times (100)\nare not detectable using existing methods. However, we show that longer\nsequences repeated a large number of times can be reliably detected (AUC=0.75)\nand used as copyright traps. Beyond copyright applications, our findings\ncontribute to the study of LLM memorization: the randomized controlled setup\nenables us to draw causal relationships between memorization and certain\nsequence properties such as repetition in model training data and perplexity.", "published": "2024-02-14 18:09:53", "link": "http://arxiv.org/abs/2402.09363v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context\n  Learning in Factuality Evaluation", "abstract": "With the widespread adoption of large language models (LLMs) in numerous\napplications, the challenge of factuality and the propensity for hallucinations\nhas emerged as a significant concern. To address this issue, particularly in\nretrieval-augmented in-context learning, we introduce the hierarchical graph of\nthoughts (HGOT), a structured, multi-layered graph approach designed to enhance\nthe retrieval of pertinent passages during in-context learning. The framework\nutilizes the emergent planning capabilities of LLMs, employing the\ndivide-and-conquer strategy to break down complex queries into manageable\nsub-queries. It refines self-consistency majority voting for answer selection,\nwhich incorporates the recently proposed citation recall and precision metrics\nto assess the quality of thoughts, linking an answer's credibility\nintrinsically to the thought's quality. This methodology introduces a weighted\nsystem in majority voting, prioritizing answers based on the citation quality\nof their thoughts. Additionally, we propose a scoring mechanism for evaluating\nretrieved passages, considering factors such as citation frequency and quality,\nself-consistency confidence, and the retrieval module's ranking. Experiments\nindicate that HGOT excels as a versatile approach, outperforming competing\nmodels in FEVER by up to $7\\%$ and matching leading models such as\nRetrieve-then-Read in Open-SQuAD, and DSP in HotPotQA, demonstrating its\nefficacy in enhancing LLMs' factuality.", "published": "2024-02-14 18:41:19", "link": "http://arxiv.org/abs/2402.09390v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Changes by Butterflies: Farsighted Forecasting with Group Reservoir\n  Transformer", "abstract": "In Chaos, a minor divergence between two initial conditions exhibits\nexponential amplification over time, leading to far-away outcomes, known as the\nbutterfly effect. Thus, the distant future is full of uncertainty and hard to\nforecast. We introduce Group Reservoir Transformer to predict long-term events\nmore accurately and robustly by overcoming two challenges in Chaos: (1) the\nextensive historical sequences and (2) the sensitivity to initial conditions. A\nreservoir is attached to a Transformer to efficiently handle arbitrarily long\nhistorical lengths, with an extension of a group of reservoirs to reduce the\nsensitivity to the initialization variations. Our architecture consistently\noutperforms state-of-the-art models in multivariate time series, including\nTimeLLM, GPT2TS, PatchTST, DLinear, TimeNet, and the baseline Transformer, with\nan error reduction of up to -59\\% in various fields such as ETTh, ETTm, and air\nquality, demonstrating that an ensemble of butterfly learning can improve the\nadequacy and certainty of event prediction, despite of the traveling time to\nthe unknown future.", "published": "2024-02-14 20:48:58", "link": "http://arxiv.org/abs/2402.09573v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emerging Opportunities of Using Large Language Models for Translation\n  Between Drug Molecules and Indications", "abstract": "A drug molecule is a substance that changes the organism's mental or physical\nstate. Every approved drug has an indication, which refers to the therapeutic\nuse of that drug for treating a particular medical condition. While the Large\nLanguage Model (LLM), a generative Artificial Intelligence (AI) technique, has\nrecently demonstrated effectiveness in translating between molecules and their\ntextual descriptions, there remains a gap in research regarding their\napplication in facilitating the translation between drug molecules and\nindications, or vice versa, which could greatly benefit the drug discovery\nprocess. The capability of generating a drug from a given indication would\nallow for the discovery of drugs targeting specific diseases or targets and\nultimately provide patients with better treatments. In this paper, we first\npropose a new task, which is the translation between drug molecules and\ncorresponding indications, and then test existing LLMs on this new task.\nSpecifically, we consider nine variations of the T5 LLM and evaluate them on\ntwo public datasets obtained from ChEMBL and DrugBank. Our experiments show the\nearly results of using LLMs for this task and provide a perspective on the\nstate-of-the-art. We also emphasize the current limitations and discuss future\nwork that has the potential to improve the performance on this task. The\ncreation of molecules from indications, or vice versa, will allow for more\nefficient targeting of diseases and significantly reduce the cost of drug\ndiscovery, with the potential to revolutionize the field of drug discovery in\nthe era of generative AI.", "published": "2024-02-14 21:33:13", "link": "http://arxiv.org/abs/2402.09588v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LogicPrpBank: A Corpus for Logical Implication and Equivalence", "abstract": "Logic reasoning has been critically needed in problem-solving and\ndecision-making. Although Language Models (LMs) have demonstrated capabilities\nof handling multiple reasoning tasks (e.g., commonsense reasoning), their\nability to reason complex mathematical problems, specifically propositional\nlogic, remains largely underexplored. This lack of exploration can be\nattributed to the limited availability of annotated corpora. Here, we present a\nwell-labeled propositional logic corpus, LogicPrpBank, containing 7093\nPropositional Logic Statements (PLSs) across six mathematical subjects, to\nstudy a brand-new task of reasoning logical implication and equivalence. We\nbenchmark LogicPrpBank with widely-used LMs to show that our corpus offers a\nuseful resource for this challenging task and there is ample room for model\nimprovement.", "published": "2024-02-14 22:36:07", "link": "http://arxiv.org/abs/2402.09609v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning over Uncertain Text by Generative Large Language Models", "abstract": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs.", "published": "2024-02-14 23:05:44", "link": "http://arxiv.org/abs/2402.09614v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL\n  Models", "abstract": "Non-autoregressive automatic speech recognition (NASR) models have gained\nattention due to their parallelism and fast inference. The encoder-based NASR,\ne.g. connectionist temporal classification (CTC), can be initialized from the\nspeech foundation models (SFM) but does not account for any dependencies among\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\ndependency problem but is not able to efficiently integrate SFM. Inspired by\nthe success of recent work of speech-text joint pre-training with a shared\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\nencoder as the major module, which can be the SFM. The encoder plays the role\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\nof the encoder accepts the speech signal as input, while the concatenation of\nthe speech signal and the token-level acoustic embedding is used as the input\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\nmodel parameters. Our codes are publicly available.", "published": "2024-02-14 02:11:04", "link": "http://arxiv.org/abs/2402.08898v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MaxMin-RLHF: Alignment with Diverse Human Preferences", "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to\nhuman preferences by employing a singular reward model derived from preference\ndata. However, such an approach overlooks the rich diversity of human\npreferences inherent in data collected from multiple users. In this work, we\nfirst derive an impossibility result of alignment with single reward RLHF,\nthereby highlighting its insufficiency in representing diverse human\npreferences. To provide an equitable solution to the problem, we learn a\nmixture of preference distributions via an expectation-maximization algorithm\nand propose a MaxMin alignment objective for policy learning inspired by the\nEgalitarian principle in social choice theory to better represent diverse human\npreferences. We elucidate the connection of our proposed approach to\ndistributionally robust optimization and general utility RL, thereby\nhighlighting the generality and robustness of our proposed solution. We present\ncomprehensive experimental results on small-scale (GPT-2) and large-scale\nlanguage models (with Tulu2-7B) and show the efficacy of the proposed approach\nin the presence of diversity among human preferences. Our algorithm achieves an\naverage improvement of more than 16% in win-rates over conventional RLHF\nalgorithms and improves the win-rate (accuracy) for minority groups by over 33%\nwithout compromising the performance of majority groups, showcasing the\nrobustness and fairness of our approach. We remark that our findings in this\nwork are not only limited to language models but also extend to reinforcement\nlearning in general.", "published": "2024-02-14 03:56:27", "link": "http://arxiv.org/abs/2402.08925v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware\n  Decoding", "abstract": "As large language models (LLMs) become increasingly integrated into\nreal-world applications such as code generation and chatbot assistance,\nextensive efforts have been made to align LLM behavior with human values,\nincluding safety. Jailbreak attacks, aiming to provoke unintended and unsafe\nbehaviors from LLMs, remain a significant/leading LLM safety threat. In this\npaper, we aim to defend LLMs against jailbreak attacks by introducing\nSafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and\nharmless responses to user queries. Our insight in developing SafeDecoding is\nbased on the observation that, even though probabilities of tokens representing\nharmful contents outweigh those representing harmless responses, safety\ndisclaimers still appear among the top tokens after sorting tokens by\nprobability in descending order. This allows us to mitigate jailbreak attacks\nby identifying safety disclaimers and amplifying their token probabilities,\nwhile simultaneously attenuating the probabilities of token sequences that are\naligned with the objectives of jailbreak attacks. We perform extensive\nexperiments on five LLMs using six state-of-the-art jailbreak attacks and four\nbenchmark datasets. Our results show that SafeDecoding significantly reduces\nthe attack success rate and harmfulness of jailbreak attacks without\ncompromising the helpfulness of responses to benign user queries. SafeDecoding\noutperforms six defense methods.", "published": "2024-02-14 06:54:31", "link": "http://arxiv.org/abs/2402.08983v4", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks", "abstract": "Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.", "published": "2024-02-14 13:45:19", "link": "http://arxiv.org/abs/2402.09177v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "(Ir)rationality and Cognitive Biases in Large Language Models", "abstract": "Do large language models (LLMs) display rational reasoning? LLMs have been\nshown to contain human biases due to the data they have been trained on;\nwhether this is reflected in rational reasoning remains less clear. In this\npaper, we answer this question by evaluating seven language models using tasks\nfrom the cognitive psychology literature. We find that, like humans, LLMs\ndisplay irrationality in these tasks. However, the way this irrationality is\ndisplayed does not reflect that shown by humans. When incorrect answers are\ngiven by LLMs to these tasks, they are often incorrect in ways that differ from\nhuman-like biases. On top of this, the LLMs reveal an additional layer of\nirrationality in the significant inconsistency of the responses. Aside from the\nexperimental results, this paper seeks to make a methodological contribution by\nshowing how we can assess and compare different capabilities of these types of\nmodels, in this case with respect to rational reasoning.", "published": "2024-02-14 14:17:21", "link": "http://arxiv.org/abs/2402.09193v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Ten Words Only Still Help: Improving Black-Box AI-Generated Text\n  Detection via Proxy-Guided Efficient Re-Sampling", "abstract": "With the rapidly increasing application of large language models (LLMs),\ntheir abuse has caused many undesirable societal problems such as fake news,\nacademic dishonesty, and information pollution. This makes AI-generated text\n(AIGT) detection of great importance. Among existing methods, white-box methods\nare generally superior to black-box methods in terms of performance and\ngeneralizability, but they require access to LLMs' internal states and are not\napplicable to black-box settings. In this paper, we propose to estimate word\ngeneration probabilities as pseudo white-box features via multiple re-sampling\nto help improve AIGT detection under the black-box setting. Specifically, we\ndesign POGER, a proxy-guided efficient re-sampling method, which selects a\nsmall subset of representative words (e.g., 10 words) for performing multiple\nre-sampling in black-box AIGT detection. Experiments on datasets containing\ntexts from humans and seven LLMs show that POGER outperforms all baselines in\nmacro F1 under black-box, partial white-box, and out-of-distribution settings\nand maintains lower re-sampling costs than its existing counterparts.", "published": "2024-02-14 14:32:16", "link": "http://arxiv.org/abs/2402.09199v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tell Me More! Towards Implicit User Intention Understanding of Language\n  Model Driven Agents", "abstract": "Current language model-driven agents often lack mechanisms for effective user\nparticipation, which is crucial given the vagueness commonly found in user\ninstructions. Although adept at devising strategies and performing tasks, these\nagents struggle with seeking clarification and grasping precise user\nintentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a\nnovel benchmark designed to inspect users' implicit intentions through explicit\nqueries. Next, we propose the incorporation of model experts as the upstream in\nagent designs to enhance user-agent interaction. Employing IN3, we empirically\ntrain Mistral-Interact, a powerful model that proactively assesses task\nvagueness, inquires user intentions, and refines them into actionable goals\nbefore starting downstream agent task execution. Integrating it into the XAgent\nframework, we comprehensively evaluate the enhanced agent system regarding user\ninstruction understanding and execution, revealing that our approach notably\nexcels at identifying vague user tasks, recovering and summarizing critical\nmissing information, setting precise and necessary agent execution goals, and\nminimizing redundant tool usage, thus boosting overall efficiency. All the data\nand codes are released.", "published": "2024-02-14 14:36:30", "link": "http://arxiv.org/abs/2402.09205v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey", "abstract": "Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.", "published": "2024-02-14 16:14:03", "link": "http://arxiv.org/abs/2402.09283v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers Can Achieve Length Generalization But Not Robustly", "abstract": "Length generalization, defined as the ability to extrapolate from shorter\ntraining sequences to longer test ones, is a significant challenge for language\nmodels. This issue persists even with large-scale Transformers handling\nrelatively straightforward tasks. In this paper, we test the Transformer's\nability of length generalization using the task of addition of two integers. We\nshow that the success of length generalization is intricately linked to the\ndata format and the type of position encoding. Using the right combination of\ndata format and position encodings, we show for the first time that standard\nTransformers can extrapolate to a sequence length that is 2.5x the input\nlength. Nevertheless, unlike in-distribution generalization, length\ngeneralization remains fragile, significantly influenced by factors like random\nweight initialization and training data order, leading to large variances\nacross different random seeds.", "published": "2024-02-14 18:18:29", "link": "http://arxiv.org/abs/2402.09371v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LlaSMol: Advancing Large Language Models for Chemistry with a\n  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset", "abstract": "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing research\nindicates that their performance on chemistry tasks is discouragingly low. In\nthis paper, however, we demonstrate that our developed LLMs can achieve very\nstrong results on a comprehensive set of chemistry tasks, outperforming the\nmost advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish\nthis, we propose SMolInstruct, a large-scale, comprehensive, and high-quality\ndataset for instruction tuning. It contains 14 selected chemistry tasks and\nover three million samples, laying a solid foundation for training and\nevaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of\nopen-source LLMs, among which, we find that Mistral serves as the best base\nmodel for chemistry tasks. Our analysis further demonstrates the critical role\nof the proposed dataset in driving the performance improvements.", "published": "2024-02-14 18:42:25", "link": "http://arxiv.org/abs/2402.09391v4", "categories": ["cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential\n  Reasoning Ability", "abstract": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.", "published": "2024-02-14 18:59:33", "link": "http://arxiv.org/abs/2402.09404v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STEER: Assessing the Economic Rationality of Large Language Models", "abstract": "There is increasing interest in using LLMs as decision-making \"agents.\" Doing\nso includes many degrees of freedom: which model should be used; how should it\nbe prompted; should it be asked to introspect, conduct chain-of-thought\nreasoning, etc? Settling these questions -- and more broadly, determining\nwhether an LLM agent is reliable enough to be trusted -- requires a methodology\nfor assessing such an agent's economic rationality. In this paper, we provide\none. We begin by surveying the economic literature on rational decision making,\ntaxonomizing a large set of fine-grained \"elements\" that an agent should\nexhibit, along with dependencies between them. We then propose a benchmark\ndistribution that quantitatively scores an LLMs performance on these elements\nand, combined with a user-provided rubric, produces a \"STEER report card.\"\nFinally, we describe the results of a large-scale empirical experiment with 14\ndifferent LLMs, characterizing the both current state of the art and the impact\nof different model sizes on models' ability to exhibit rational behavior.", "published": "2024-02-14 20:05:26", "link": "http://arxiv.org/abs/2402.09552v2", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Towards Privacy-Aware Sign Language Translation at Scale", "abstract": "A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.", "published": "2024-02-14 22:57:03", "link": "http://arxiv.org/abs/2402.09611v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "API Pack: A Massive Multi-Programming Language Dataset for API Call\n  Generation", "abstract": "We introduce API Pack, a massive multi-programming language dataset\ncontaining over one million instruction-API calls for improving the API call\ngeneration capabilities of large language models. Our evaluation highlights\nthree key findings: First, fine-tuning on API Pack enables open-source models\nto outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.\nWe show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API\nPack. Second, fine-tuning on a large dataset in one language, combined with\nsmaller datasets from others, improves API generation accuracy across multiple\nlanguages. Third, we confirm the benefits of larger datasets for API\ngeneralization, as increasing fine-tuning data to one million instances\nenhances generalization to new APIs. To support further research, we\nopen-source the API Pack dataset, trained model, and code at\nhttps://github.com/zguo0525/API-Pack.", "published": "2024-02-14 23:09:15", "link": "http://arxiv.org/abs/2402.09615v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalization in Healthcare AI: Evaluation of a Clinical Large Language\n  Model", "abstract": "Advances in large language models (LLMs) provide new opportunities in\nhealthcare for improved patient care, clinical decision-making, and enhancement\nof physician and administrator workflows. However, the potential of these\nmodels importantly depends on their ability to generalize effectively across\nclinical environments and populations, a challenge often underestimated in\nearly development. To better understand reasons for these challenges and inform\nmitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s\nclinical notes, analyzing its performance on 30-day all-cause readmission\nprediction focusing on variability across hospitals and patient\ncharacteristics. We found poorer generalization particularly in hospitals with\nfewer samples, among patients with government and unspecified insurance, the\nelderly, and those with high comorbidities. To understand reasons for lack of\ngeneralization, we investigated sample sizes for fine-tuning, note content\n(number of words per note), patient characteristics (comorbidity level, age,\ninsurance type, borough), and health system aspects (hospital, all-cause 30-day\nreadmission, and mortality rates). We used descriptive statistics and\nsupervised classification to identify features. We found that, along with\nsample size, patient age, number of comorbidities, and the number of words in\nnotes are all important factors related to generalization. Finally, we compared\nlocal fine-tuning (hospital specific), instance-based augmented fine-tuning and\ncluster-based fine-tuning for improving generalization. Among these, local\nfine-tuning proved most effective, increasing AUC by 0.25% to 11.74% (most\nhelpful in settings with limited data). Overall, this study provides new\ninsights for enhancing the deployment of large language models in the\nsocietally important domain of healthcare, and improving their performance for\nbroader populations.", "published": "2024-02-14 06:24:52", "link": "http://arxiv.org/abs/2402.10965v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regional inflation analysis using social network data", "abstract": "Inflation is one of the most important macroeconomic indicators that have a\ngreat impact on the population of any country and region. Inflation is\ninfluenced by range of factors, one of which is inflation expectations. Many\ncentral banks take this factor into consideration while implementing monetary\npolicy within the inflation targeting regime. Nowadays, a lot of people are\nactive users of the Internet, especially social networks. There is a hypothesis\nthat people search, read, and discuss mainly only those issues that are of\nparticular interest to them. It is logical to assume that the dynamics of\nprices may also be in the focus of user discussions. So, such discussions could\nbe regarded as an alternative source of more rapid information about inflation\nexpectations. This study is based on unstructured data from Vkontakte social\nnetwork to analyze upward and downward inflationary trends (on the example of\nthe Omsk region). The sample of more than 8.5 million posts was collected\nbetween January 2010 and May 2022. The authors used BERT neural networks to\nsolve the problem. These models demonstrated better results than the benchmarks\n(e.g., logistic regression, decision tree classifier, etc.). It makes possible\nto define pro-inflationary and disinflationary types of keywords in different\ncontexts and get their visualization with SHAP method. This analysis provides\nadditional operational information about inflationary processes at the regional\nlevel The proposed approach can be scaled for other regions. At the same time\nthe limitation of the work is the time and power costs for the initial training\nof similar models for all regions of Russia.", "published": "2024-02-14 02:33:17", "link": "http://arxiv.org/abs/2403.00774v2", "categories": ["q-fin.ST", "cs.CL", "cs.SI"], "primary_category": "q-fin.ST"}
{"title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "abstract": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.", "published": "2024-02-14 05:57:58", "link": "http://arxiv.org/abs/2402.08957v3", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG", "cs.PL"], "primary_category": "cs.AI"}
{"title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models", "abstract": "The imperative need to scale computation across numerous nodes highlights the\nsignificance of efficient parallel computing, particularly in the realm of\nMessage Passing Interface (MPI) integration. The challenging parallel\nprogramming task of generating MPI-based parallel programs has remained\nunexplored. This study first investigates the performance of state-of-the-art\nlanguage models in generating MPI-based parallel programs. Findings reveal that\nwidely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual\ncode models) exhibit notable performance degradation, when generating MPI-based\nprograms compared to general-purpose programs. In contrast, domain-specific\nmodels such as MonoCoder, which are pretrained on MPI-related programming\nlanguages of C and C++, outperform larger models. Subsequently, we introduce a\ndedicated downstream task of MPI-based program generation by fine-tuning\nMonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose\nan innovative preprocessing for completion only after observing the whole code,\nthus enabling better completion with a wider context. Comparative analysis\nagainst GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation\nmethod, demonstrates that MPIrigen excels in generating accurate MPI functions\nup to 0.8 accuracy in location and function predictions, and with more than 0.9\naccuracy for argument predictions. The success of this tailored solution\nunderscores the importance of domain-specific fine-tuning in optimizing\nlanguage models for parallel computing code generation, paving the way for a\nnew generation of automatic parallelization tools. The sources of this work are\navailable at our GitHub MPIrigen repository:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen", "published": "2024-02-14 12:24:21", "link": "http://arxiv.org/abs/2402.09126v2", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.DC"}
{"title": "Reinforcement Learning from Human Feedback with Active Queries", "abstract": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.", "published": "2024-02-14 18:58:40", "link": "http://arxiv.org/abs/2402.09401v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sound Field Reconstruction Using a Compact Acoustics-informed Neural\n  Network", "abstract": "Sound field reconstruction (SFR) augments the information of a sound field\ncaptured by a microphone array. Conventional SFR methods using basis function\ndecomposition are straightforward and computationally efficient, but may\nrequire more microphones than needed to measure the sound field. Recent studies\nshow that pure data-driven and learning-based methods are promising in some SFR\ntasks, but they are usually computationally heavy and may fail to reconstruct a\nphysically valid sound field. This paper proposes a compact acoustics-informed\nneural network (AINN) method for SFR, whereby the Helmholtz equation is\nexploited to regularize the neural network. As opposed to pure data-driven\napproaches that solely rely on measured sound pressures, the integration of the\nHelmholtz equation improves robustness of the neural network against variations\nduring the measurement processes and prompts the generation of physically valid\nreconstructions. The AINN is designed to be compact, and is able to predict not\nonly the sound pressures but also sound pressure gradients within a spatial\nregion of interest based on measured sound pressures along the boundary.\nNumerical experiments with acoustic transfer functions measured in different\nenvironments demonstrate the superiority of the AINN method over the\ntraditional cylinder harmonic decomposition and the singular value\ndecomposition methods.", "published": "2024-02-14 02:32:18", "link": "http://arxiv.org/abs/2402.08904v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listening to Multi-talker Conversations: Modular and End-to-end\n  Perspectives", "abstract": "Since the first speech recognition systems were built more than 30 years ago,\nimprovement in voice technology has enabled applications such as smart\nassistants and automated customer support. However, conversation intelligence\nof the future requires recognizing free-flowing multi-party conversations,\nwhich is a crucial and challenging component that still remains unsolved. In\nthis dissertation, we focus on this problem of speaker-attributed multi-talker\nspeech recognition, and propose two perspectives which result from its\nprobabilistic formulation.\n  In the modular perspective, we build a pipeline of sub-tasks involving\nspeaker diarization, target speaker extraction, and speech recognition. Our\nfirst contribution is a method to perform overlap-aware diarization by\nreformulating spectral clustering as a constrained optimization problem. We\nalso describe an algorithm to ensemble diarization outputs, either to combine\noverlap-aware systems or to perform multi-channel diarization by late fusion.\nOnce speaker segments are identified, we robustly extract single-speaker\nutterances from the mixture using a GPU-accelerated implementation of guided\nsource separation, which allows us to use an off-the-shelf ASR system to obtain\nspeaker-attributed transcripts.\n  Since the modular approach suffers from error propagation, we propose an\nalternate \"end-to-end\" perspective on the problem. For this, we describe the\nStreaming Unmixing and Recognition Transducer (SURT). We show how to train SURT\nmodels efficiently by carefully designing the network architecture, objective\nfunctions, and mixture simulation techniques. Finally, we add an auxiliary\nspeaker branch to enable joint prediction of speaker labels synchronized with\nthe speech tokens. We demonstrate that training on synthetic mixtures and\nadapting with real data helps these models transfer well for streaming\ntranscription of real meeting sessions.", "published": "2024-02-14 04:21:11", "link": "http://arxiv.org/abs/2402.08932v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mixture to Mixture: Leveraging Close-talk Mixtures as Weak-supervision\n  for Speech Separation", "abstract": "We propose mixture to mixture (M2M) training, a weakly-supervised neural\nspeech separation algorithm that leverages close-talk mixtures as a weak\nsupervision for training discriminative models to separate far-field mixtures.\nOur idea is that, for a target speaker, its close-talk mixture has a much\nhigher signal-to-noise ratio (SNR) of the target speaker than any far-field\nmixtures, and hence could be utilized to design a weak supervision for\nseparation. To realize this, at each training step we feed a far-field mixture\nto a deep neural network (DNN) to produce an intermediate estimate for each\nspeaker, and, for each of considered close-talk and far-field microphones, we\nlinearly filter the DNN estimates and optimize a loss so that the filtered\nestimates of all the speakers can sum up to the mixture captured by each of the\nconsidered microphones. Evaluation results on a 2-speaker separation task in\nsimulated reverberant conditions show that M2M can effectively leverage\nclose-talk mixtures as a weak supervision for separating far-field mixtures.", "published": "2024-02-14 17:03:04", "link": "http://arxiv.org/abs/2402.09313v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot\n  Text-to-Speech", "abstract": "Zero-shot text-to-speech (TTS) has gained significant attention due to its\npowerful voice cloning capabilities, requiring only a few seconds of unseen\nspeaker voice prompts. However, all previous work has been developed for\ncloud-based systems. Taking autoregressive models as an example, although these\napproaches achieve high-fidelity voice cloning, they fall short in terms of\ninference speed, model size, and robustness. Therefore, we propose\nMobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech\nsystem based on mobile devices for the first time. Specifically: 1) leveraging\ndiscrete codec, we design a parallel speech mask decoder module called SMD,\nwhich incorporates hierarchical information from the speech codec and weight\nmechanisms across different codec layers during the generation process.\nMoreover, to bridge the gap between text and speech, we introduce a high-level\nprobabilistic mask that simulates the progression of information flow from less\nto more during speech generation. 2) For speaker prompts, we extract\nfine-grained prompt duration from the prompt speech and incorporate text,\nprompt speech by cross attention in SMD. We demonstrate the effectiveness of\nMobileSpeech on multilingual datasets at different levels, achieving\nstate-of-the-art results in terms of generating speed and speech quality.\nMobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully\ndeployed MobileSpeech on mobile devices. Audio samples are available at\n\\url{https://mobilespeech.github.io/} .", "published": "2024-02-14 18:24:41", "link": "http://arxiv.org/abs/2402.09378v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Adaptation for Contrastive Audio-Language Models", "abstract": "Audio-Language Models (ALM) aim to be general-purpose audio models by\nproviding zero-shot capabilities at test time. The zero-shot performance of ALM\nimproves by using suitable text prompts for each domain. The text prompts are\nusually hand-crafted through an ad-hoc process and lead to a drop in ALM\ngeneralization and out-of-distribution performance. Existing approaches to\nimprove domain performance, like few-shot learning or fine-tuning, require\naccess to annotated data and iterations of training. Therefore, we propose a\ntest-time domain adaptation method for ALMs that does not require access to\nannotations. Our method learns a domain vector by enforcing consistency across\naugmented views of the testing audio. We extensively evaluate our approach on\n12 downstream tasks across domains. With just one example, our domain\nadaptation method leads to 3.2% (max 8.4%) average zero-shot performance\nimprovement. After adaptation, the model still retains the generalization\nproperty of ALMs.", "published": "2024-02-14 21:25:06", "link": "http://arxiv.org/abs/2402.09585v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality", "abstract": "The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP\n2023 is to promote and support collaborative research on machine learning for\n3D audio signal processing, with a specific emphasis on 3D speech enhancement\nand 3D Sound Event Localization and Detection in Extended Reality applications.\nAs part of our latest competition, we provide a brand-new dataset, which\nmaintains the same general characteristics of the L3DAS21 and L3DAS22 datasets,\nbut with first-order Ambisonics recordings from multiple reverberant simulated\nenvironments. Moreover, we start exploring an audio-visual scenario by\nproviding images of these environments, as perceived by the different\nmicrophone positions and orientations. We also propose updated baseline models\nfor both tasks that can now support audio-image couples as input and a\nsupporting API to replicate our results. Finally, we present the results of the\nparticipants. Further details about the challenge are available at\nhttps://www.l3das.com/icassp2023.", "published": "2024-02-14 15:34:28", "link": "http://arxiv.org/abs/2402.09245v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning\n  of Music Audio", "abstract": "We present PECMAE, an interpretable model for music audio classification\nbased on prototype learning. Our model is based on a previous method, APNet,\nwhich jointly learns an autoencoder and a prototypical network. Instead, we\npropose to decouple both training processes. This enables us to leverage\nexisting self-supervised autoencoders pre-trained on much larger data\n(EnCodecMAE), providing representations with better generalization. APNet\nallows prototypes' reconstruction to waveforms for interpretability relying on\nthe nearest training data samples. In contrast, we explore using a diffusion\ndecoder that allows reconstruction without such dependency. We evaluate our\nmethod on datasets for music instrument classification (Medley-Solos-DB) and\ngenre recognition (GTZAN and a larger in-house dataset), the latter being a\nmore challenging task not addressed with prototypical networks before. We find\nthat the prototype-based models preserve most of the performance achieved with\nthe autoencoder embeddings, while the sonification of prototypes benefits\nunderstanding the behavior of the classifier.", "published": "2024-02-14 17:13:36", "link": "http://arxiv.org/abs/2402.09318v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\n  and Editing via Content-based Controls", "abstract": "Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.", "published": "2024-02-14 19:00:01", "link": "http://arxiv.org/abs/2402.09508v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
