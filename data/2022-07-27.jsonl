{"title": "Contextual Information and Commonsense Based Prompt for Emotion\n  Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) aims to detect the emotion for each\nutterance in a given conversation. The newly proposed ERC models have leveraged\npre-trained language models (PLMs) with the paradigm of pre-training and\nfine-tuning to obtain good performance. However, these models seldom exploit\nPLMs' advantages thoroughly, and perform poorly for the conversations lacking\nexplicit emotional expressions. In order to fully leverage the latent knowledge\nrelated to the emotional expressions in utterances, we propose a novel ERC\nmodel CISPER with the new paradigm of prompt and language model (LM) tuning.\nSpecifically, CISPER is equipped with the prompt blending the contextual\ninformation and commonsense related to the interlocutor's utterances, to\nachieve ERC more effectively. Our extensive experiments demonstrate CISPER's\nsuperior performance over the state-of-the-art ERC models, and the\neffectiveness of leveraging these two kinds of significant prompt information\nfor performance gains. To reproduce our experimental results conveniently,\nCISPER's sourcecode and the datasets have been shared at\nhttps://github.com/DeqingYang/CISPER.", "published": "2022-07-27 02:34:05", "link": "http://arxiv.org/abs/2207.13254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RealTime QA: What's the Answer Right Now?", "abstract": "We introduce REALTIME QA, a dynamic question answering (QA) platform that\nannounces questions and evaluates systems on a regular basis (weekly in this\nversion). REALTIME QA inquires about the current world, and QA systems need to\nanswer questions about novel events or information. It therefore challenges\nstatic, conventional assumptions in open-domain QA datasets and pursues\ninstantaneous applications. We build strong baseline models upon large\npretrained language models, including GPT-3 and T5. Our benchmark is an ongoing\neffort, and this paper presents real-time evaluation results over the past\nyear. Our experimental results show that GPT-3 can often properly update its\ngeneration results, based on newly-retrieved documents, highlighting the\nimportance of up-to-date information retrieval. Nonetheless, we find that GPT-3\ntends to return outdated answers when retrieved documents do not provide\nsufficient information to find an answer. This suggests an important avenue for\nfuture research: can an open-domain QA system identify such unanswerable cases\nand communicate with the user or even the retrieval module to modify the\nretrieval results? We hope that REALTIME QA will spur progress in instantaneous\napplications of question answering and beyond.", "published": "2022-07-27 07:26:01", "link": "http://arxiv.org/abs/2207.13332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Neighbors Enough? Multi-Head Neural n-gram can be Alternative to\n  Self-attention", "abstract": "Impressive performance of Transformer has been attributed to self-attention,\nwhere dependencies between entire input in a sequence are considered at every\nposition. In this work, we reform the neural $n$-gram model, which focuses on\nonly several surrounding representations of each position, with the multi-head\nmechanism as in Vaswani et al.(2017). Through experiments on\nsequence-to-sequence tasks, we show that replacing self-attention in\nTransformer with multi-head neural $n$-gram can achieve comparable or better\nperformance than Transformer. From various analyses on our proposed method, we\nfind that multi-head neural $n$-gram is complementary to self-attention, and\ntheir combinations can further improve performance of vanilla Transformer.", "published": "2022-07-27 08:20:00", "link": "http://arxiv.org/abs/2207.13354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Leaf Clinical Trials Corpus: a new resource for query generation\n  from clinical trial eligibility criteria", "abstract": "Identifying cohorts of patients based on eligibility criteria such as medical\nconditions, procedures, and medication use is critical to recruitment for\nclinical trials. Such criteria are often most naturally described in free-text,\nusing language familiar to clinicians and researchers. In order to identify\npotential participants at scale, these criteria must first be translated into\nqueries on clinical databases, which can be labor-intensive and error-prone.\nNatural language processing (NLP) methods offer a potential means of such\nconversion into database queries automatically. However they must first be\ntrained and evaluated using corpora which capture clinical trials criteria in\nsufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT)\ncorpus, a human-annotated corpus of over 1,000 clinical trial eligibility\ncriteria descriptions using highly granular structured labels capturing a range\nof biomedical phenomena. We provide details of our schema, annotation process,\ncorpus quality, and statistics. Additionally, we present baseline information\nextraction results on this corpus as benchmarks for future work.", "published": "2022-07-27 19:22:24", "link": "http://arxiv.org/abs/2207.13757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CompText: Visualizing, Comparing & Understanding Text Corpus", "abstract": "A common practice in Natural Language Processing (NLP) is to visualize the\ntext corpus without reading through the entire literature, still grasping the\ncentral idea and key points described. For a long time, researchers focused on\nextracting topics from the text and visualizing them based on their relative\nsignificance in the corpus. However, recently, researchers started coming up\nwith more complex systems that not only expose the topics of the corpus but\nalso word closely related to the topic to give users a holistic view. These\ndetailed visualizations spawned research on comparing text corpora based on\ntheir visualization. Topics are often compared to idealize the difference\nbetween corpora. However, to capture greater semantics from different corpora,\nresearchers have started to compare texts based on the sentiment of the topics\nrelated to the text. Comparing the words carrying the most weightage, we can\nget an idea about the important topics for corpus. There are multiple existing\ntexts comparing methods present that compare topics rather than sentiments but\nwe feel that focusing on sentiment-carrying words would better compare the two\ncorpora. Since only sentiments can explain the real feeling of the text and not\njust the topic, topics without sentiments are just nouns. We aim to\ndifferentiate the corpus with a focus on sentiment, as opposed to comparing all\nthe words appearing in the two corpora. The rationale behind this is, that the\ntwo corpora do not many have identical words for side-by-side comparison, so\ncomparing the sentiment words gives us an idea of how the corpora are appealing\nto the emotions of the reader. We can argue that the entropy or the\nunexpectedness and divergence of topics should also be of importance and help\nus to identify key pivot points and the importance of certain topics in the\ncorpus alongside relative sentiment.", "published": "2022-07-27 20:04:31", "link": "http://arxiv.org/abs/2207.13771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNIMIB at TREC 2021 Clinical Trials Track", "abstract": "This contribution summarizes the participation of the UNIMIB team to the TREC\n2021 Clinical Trials Track. We have investigated the effect of different query\nrepresentations combined with several retrieval models on the retrieval\nperformance. First, we have implemented a neural re-ranking approach to study\nthe effectiveness of dense text representations. Additionally, we have\ninvestigated the effectiveness of a novel decision-theoretic model for\nrelevance estimation. Finally, both of the above relevance models have been\ncompared with standard retrieval approaches. In particular, we combined a\nkeyword extraction method with a standard retrieval process based on the BM25\nmodel and a decision-theoretic relevance model that exploits the\ncharacteristics of this particular search task. The obtained results show that\nthe proposed keyword extraction method improves 84% of the queries over the\nTREC's median NDCG@10 measure when combined with either traditional or\ndecision-theoretic relevance models. Moreover, regarding RPEC@10, the employed\ndecision-theoretic model improves 85% of the queries over the reported TREC's\nmedian value.", "published": "2022-07-27 13:39:30", "link": "http://arxiv.org/abs/2207.13514v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Variational AutoEncoder for Transformers with Nonparametric\n  Variational Information Bottleneck", "abstract": "We propose a VAE for Transformers by developing a variational information\nbottleneck regulariser for Transformer embeddings. We formalise the embedding\nspace of Transformer encoders as mixture probability distributions, and use\nBayesian nonparametrics to derive a nonparametric variational information\nbottleneck (NVIB) for such attention-based embeddings. The variable number of\nmixture components supported by nonparametric methods captures the variable\nnumber of vectors supported by attention, and the exchangeability of our\nnonparametric distributions captures the permutation invariance of attention.\nThis allows NVIB to regularise the number of vectors accessible with attention,\nas well as the amount of information in individual vectors. By regularising the\ncross-attention of a Transformer encoder-decoder with NVIB, we propose a\nnonparametric variational autoencoder (NVAE). Initial experiments on training a\nNVAE on natural language text show that the induced embedding space has the\ndesired properties of a VAE for Transformers.", "published": "2022-07-27 13:59:23", "link": "http://arxiv.org/abs/2207.13529v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Spam Reviews on Vietnamese E-commerce Websites", "abstract": "The reviews of customers play an essential role in online shopping. People\noften refer to reviews or comments of previous customers to decide whether to\nbuy a new product. Catching up with this behavior, some people create untruths\nand illegitimate reviews to hoax customers about the fake quality of products.\nThese are called spam reviews, confusing consumers on online shopping platforms\nand negatively affecting online shopping behaviors. We propose the dataset\ncalled ViSpamReviews, which has a strict annotation procedure for detecting\nspam reviews on e-commerce platforms. Our dataset consists of two tasks: the\nbinary classification task for detecting whether a review is spam or not and\nthe multi-class classification task for identifying the type of spam. The\nPhoBERT obtained the highest results on both tasks, 86.89% and 72.17%,\nrespectively, by macro average F1 score.", "published": "2022-07-27 10:37:14", "link": "http://arxiv.org/abs/2207.14636v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prediction of User Request and Complaint in Spoken Customer-Agent\n  Conversations", "abstract": "We present the corpus called HealthCall. This was recorded in real-life\nconditions in the call center of Malakoff Humanis. It includes two separate\naudio channels, the first one for the customer and the second one for the\nagent. Each conversation was anonymized respecting the General Data Protection\nRegulation. This corpus includes a transcription of the spoken conversations\nand was divided into two sets: Train and Devel sets. Two important customer\nrelationship management tasks were assessed on the HealthCall corpus: Automatic\nprediction of type of user requests and complaints detection. For this purpose,\nwe have investigated 14 feature sets: 6 linguistic feature sets, 6 audio\nfeature sets and 2 vocal interaction feature sets. We have used Bidirectional\nEncoder Representation from Transformers models for the linguistic features,\nopenSMILE and Wav2Vec 2.0 for the audio features. The vocal interaction feature\nsets were designed and developed from Turn Takings. The results show that the\nlinguistic features always give the best results (91.2% for the Request task\nand 70.3% for the Complaint task). The Wav2Vec 2.0 features seem more suitable\nfor these two tasks than the ComPaRe16 features. Vocal interaction features\noutperformed ComPaRe16 features on Complaint task with a 57% rate achieved with\nonly six features.", "published": "2022-07-27 21:29:38", "link": "http://arxiv.org/abs/2208.10249v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty-based Visual Question Answering: Estimating Semantic\n  Inconsistency between Image and Knowledge Base", "abstract": "Knowledge-based visual question answering (KVQA) task aims to answer\nquestions that require additional external knowledge as well as an\nunderstanding of images and questions. Recent studies on KVQA inject an\nexternal knowledge in a multi-modal form, and as more knowledge is used,\nirrelevant information may be added and can confuse the question answering. In\norder to properly use the knowledge, this study proposes the following: 1) we\nintroduce a novel semantic inconsistency measure computed from caption\nuncertainty and semantic similarity; 2) we suggest a new external knowledge\nassimilation method based on the semantic inconsistency measure and apply it to\nintegrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed\nmethod is evaluated with the OK-VQA dataset and achieves the state-of-the-art\nperformance.", "published": "2022-07-27 01:58:29", "link": "http://arxiv.org/abs/2207.13242v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of\n  Deep Neural Networks", "abstract": "The last decade of machine learning has seen drastic increases in scale and\ncapabilities. Deep neural networks (DNNs) are increasingly being deployed in\nthe real world. However, they are difficult to analyze, raising concerns about\nusing them without a rigorous understanding of how they function. Effective\ntools for interpreting them will be important for building more trustworthy AI\nby helping to identify problems, fix bugs, and improve basic understanding. In\nparticular, \"inner\" interpretability techniques, which focus on explaining the\ninternal components of DNNs, are well-suited for developing a mechanistic\nunderstanding, guiding manual modifications, and reverse engineering solutions.\n  Much recent work has focused on DNN interpretability, and rapid progress has\nthus far made a thorough systematization of methods difficult. In this survey,\nwe review over 300 works with a focus on inner interpretability tools. We\nintroduce a taxonomy that classifies methods by what part of the network they\nhelp to explain (weights, neurons, subnetworks, or latent representations) and\nwhether they are implemented during (intrinsic) or after (post hoc) training.\nTo our knowledge, we are also the first to survey a number of connections\nbetween interpretability research and work in adversarial robustness, continual\nlearning, modularity, network compression, and studying the human visual\nsystem. We discuss key challenges and argue that the status quo in\ninterpretability research is largely unproductive. Finally, we highlight the\nimportance of future work that emphasizes diagnostics, debugging, adversaries,\nand benchmarking in order to make interpretability tools more useful to\nengineers in practical applications.", "published": "2022-07-27 01:59:13", "link": "http://arxiv.org/abs/2207.13243v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Modelling Social Context for Fake News Detection: A Graph Neural Network\n  Based Approach", "abstract": "Detection of fake news is crucial to ensure the authenticity of information\nand maintain the news ecosystems reliability. Recently, there has been an\nincrease in fake news content due to the recent proliferation of social media\nand fake content generation techniques such as Deep Fake. The majority of the\nexisting modalities of fake news detection focus on content based approaches.\nHowever, most of these techniques fail to deal with ultra realistic synthesized\nmedia produced by generative models. Our recent studies find that the\npropagation characteristics of authentic and fake news are distinguishable,\nirrespective of their modalities. In this regard, we have investigated the\nauxiliary information based on social context to detect fake news. This paper\nhas analyzed the social context of fake news detection with a hybrid graph\nneural network based approach. This hybrid model is based on integrating a\ngraph neural network on the propagation of news and bi directional encoder\nrepresentations from the transformers model on news content to learn the text\nfeatures. Thus this proposed approach learns the content as well as the context\nfeatures and hence able to outperform the baseline models with an f1 score of\n0.91 on PolitiFact and 0.93 on the Gossipcop dataset, respectively", "published": "2022-07-27 12:58:33", "link": "http://arxiv.org/abs/2207.13500v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Subword Dictionary Learning and Segmentation Techniques for Automatic\n  Speech Recognition in Tamil and Kannada", "abstract": "We present automatic speech recognition (ASR) systems for Tamil and Kannada\nbased on subword modeling to effectively handle unlimited vocabulary due to the\nhighly agglutinative nature of the languages. We explore byte pair encoding\n(BPE), and proposed a variant of this algorithm named extended-BPE, and\nMorfessor tool to segment each word as subwords. We have effectively\nincorporated maximum likelihood (ML) and Viterbi estimation techniques with\nweighted finite state transducers (WFST) framework in these algorithms to learn\nthe subword dictionary from a large text corpus. Using the learnt subword\ndictionary, the words in training data transcriptions are segmented to subwords\nand we train deep neural network ASR systems which recognize subword sequence\nfor any given test speech utterance. The output subword sequence is then\npost-processed using deterministic rules to get the final word sequence such\nthat the actual number of words that can be recognized is much larger. For\nTamil ASR, We use 152 hours of data for training and 65 hours for testing,\nwhereas for Kannada ASR, we use 275 hours for training and 72 hours for\ntesting. Upon experimenting with different combination of segmentation and\nestimation techniques, we find that the word error rate (WER) reduces\ndrastically when compared to the baseline word-level ASR, achieving a maximum\nabsolute WER reduction of 6.24% and 6.63% for Tamil and Kannada respectively.", "published": "2022-07-27 07:24:34", "link": "http://arxiv.org/abs/2207.13331v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge-driven Subword Grammar Modeling for Automatic Speech\n  Recognition in Tamil and Kannada", "abstract": "In this paper, we present specially designed automatic speech recognition\n(ASR) systems for the highly agglutinative and inflective languages of Tamil\nand Kannada that can recognize unlimited vocabulary of words. We use subwords\nas the basic lexical units for recognition and construct subword grammar\nweighted finite state transducer (SG-WFST) graphs for word segmentation that\ncaptures most of the complex word formation rules of the languages. We have\nidentified the following category of words (i) verbs, (ii) nouns, (ii)\npronouns, and (iv) numbers. The prefix, infix and suffix lists of subwords are\ncreated for each of these categories and are used to design the SG-WFST graphs.\nWe also present a heuristic segmentation algorithm that can even segment\nexceptional words that do not follow the rules encapsulated in the SG-WFST\ngraph. Most of the data-driven subword dictionary creation algorithms are\ncomputation driven, and hence do not guarantee morpheme-like units and so we\nhave used the linguistic knowledge of the languages and manually created the\nsubword dictionaries and the graphs. Finally, we train a deep neural network\nacoustic model and combine it with the pronunciation lexicon of the subword\ndictionary and the SG-WFST graph to build the subword-ASR systems. Since the\nsubword-ASR produces subword sequences as output for a given test speech, we\npost-process its output to get the final word sequence, so that the actual\nnumber of words that can be recognized is much higher. Upon experimenting the\nsubword-ASR system with the IISc-MILE Tamil and Kannada ASR corpora, we observe\nan absolute word error rate reduction of 12.39% and 13.56% over the baseline\nword-based ASR systems for Tamil and Kannada, respectively.", "published": "2022-07-27 07:29:27", "link": "http://arxiv.org/abs/2207.13333v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-To-End Audiovisual Feature Fusion for Active Speaker Detection", "abstract": "Active speaker detection plays a vital role in human-machine interaction.\nRecently, a few end-to-end audiovisual frameworks emerged. However, these\nmodels' inference time was not explored and are not applicable for real-time\napplications due to their complexity and large input size. In addition, they\nexplored a similar feature extraction strategy that employs the ConvNet on\naudio and visual inputs. This work presents a novel two-stream end-to-end\nframework fusing features extracted from images via VGG-M with raw Mel\nFrequency Cepstrum Coefficients features extracted from the audio waveform. The\nnetwork has two BiGRU layers attached to each stream to handle each stream's\ntemporal dynamic before fusion. After fusion, one BiGRU layer is attached to\nmodel the joint temporal dynamics. The experiment result on the\nAVA-ActiveSpeaker dataset indicates that our new feature extraction strategy\nshows more robustness to noisy signals and better inference time than models\nthat employed ConvNet on both modalities. The proposed model predicts within\n44.41 ms, which is fast enough for real-time applications. Our best-performing\nmodel attained 88.929% accuracy, nearly the same detection result as\nstate-of-the-art -work.", "published": "2022-07-27 10:25:59", "link": "http://arxiv.org/abs/2207.13434v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation", "abstract": "End-to-end speech synthesis models directly convert the input characters into\nan audio representation (e.g., spectrograms). Despite their impressive\nperformance, such models have difficulty disambiguating the pronunciations of\nidentically spelled words. To mitigate this issue, a separate\nGrapheme-to-Phoneme (G2P) model can be employed to convert the characters into\nphonemes before synthesizing the audio. This paper proposes SoundChoice, a\nnovel G2P architecture that processes entire sentences rather than operating at\nthe word level. The proposed architecture takes advantage of a weighted\nhomograph loss (that improves disambiguation), exploits curriculum learning\n(that gradually switches from word-level to sentence-level G2P), and integrates\nword embeddings from BERT (for further performance improvement). Moreover, the\nmodel inherits the best practices in speech recognition, including multi-task\nlearning with Connectionist Temporal Classification (CTC) and beam search with\nan embedded language model. As a result, SoundChoice achieves a Phoneme Error\nRate (PER) of 2.65% on whole-sentence transcription using data from LibriSpeech\nand Wikipedia. Index Terms grapheme-to-phoneme, speech synthesis,\ntext-tospeech, phonetics, pronunciation, disambiguation.", "published": "2022-07-27 01:14:59", "link": "http://arxiv.org/abs/2207.13703v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
