{"title": "ALT: um software para an\u00e1lise de legibilidade de textos em L\u00edngua\n  Portuguesa", "abstract": "In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n  --\n  No est\\'agio inicial da vida humana a comunica\\c{c}\\~ao, vista como um\nprocesso de intera\\c{c}\\~ao social, foi sempre o melhor caminho para o consenso\nentre as partes. O entendimento e a credibilidade nesse processo s\\~ao\nfundamentais para que o acordo m\\'utuo seja validado. Mas, como faz\\^e-lo de\nforma que essa comunica\\c{c}\\~ao alcance a grande massa? Esse \\'e o principal\ndesafio quando o que se busca \\'e a difus\\~ao da informa\\c{c}\\~ao e a sua\naprova\\c{c}\\~ao. Nesse contexto, este estudo apresenta o software ALT,\ndesenvolvido a partir de m\\'etricas de legibilidade originais adaptadas para a\nL\\'ingua Portuguesa, dispon\\'ivel na web, para reduzir as dificuldades na\ncomunica\\c{c}\\~ao. O desenvolvimento do software foi motivado pela teoria do\nagir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para\nmedir a credibilidade do discurso nos canais de comunica\\c{c}\\~ao utilizados\npara construir e manter uma rela\\c{c}\\~ao segura e saud\\'avel com o p\\'ublico.", "published": "2022-03-23 02:03:46", "link": "http://arxiv.org/abs/2203.12135v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Memorization in NLP", "abstract": "A recent study by Feldman (2020) proposed a long-tail theory to explain the\nmemorization behavior of deep learning models. However, memorization has not\nbeen empirically verified in the context of NLP, a gap addressed by this work.\nIn this paper, we use three different NLP tasks to check if the long-tail\ntheory holds. Our experiments demonstrate that top-ranked memorized training\ninstances are likely atypical, and removing the top-memorized training\ninstances leads to a more serious drop in test accuracy compared with removing\ntraining instances randomly. Furthermore, we develop an attribution method to\nbetter understand why a training instance is memorized. We empirically show\nthat our memorization attribution method is faithful, and share our interesting\nfinding that the top-memorized parts of a training instance tend to be features\nnegatively correlated with the class label.", "published": "2022-03-23 03:27:56", "link": "http://arxiv.org/abs/2203.12171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Theoretically Grounded Benchmark for Evaluating Machine Commonsense", "abstract": "Programming machines with commonsense reasoning (CSR) abilities is a\nlongstanding challenge in the Artificial Intelligence community. Current CSR\nbenchmarks use multiple-choice (and in relatively fewer cases, generative)\nquestion-answering instances to evaluate machine commonsense. Recent progress\nin transformer-based language representation models suggest that considerable\nprogress has been made on existing benchmarks. However, although tens of CSR\nbenchmarks currently exist, and are growing, it is not evident that the full\nsuite of commonsense capabilities have been systematically evaluated.\nFurthermore, there are doubts about whether language models are 'fitting' to a\nbenchmark dataset's training partition by picking up on subtle, but normatively\nirrelevant (at least for CSR), statistical features to achieve good performance\non the testing partition. To address these challenges, we propose a benchmark\ncalled Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based\non discriminative question answering, but with questions designed to evaluate\ndiverse aspects of commonsense, such as space, time, and world states. TG-CSR\nis based on a subset of commonsense categories first proposed as a viable\ntheory of commonsense by Gordon and Hobbs. The benchmark is also designed to be\nfew-shot (and in the future, zero-shot), with only a few training and\nvalidation examples provided. This report discusses the structure and\nconstruction of the benchmark. Preliminary results suggest that the benchmark\nis challenging even for advanced language representation models designed for\ndiscriminative CSR question answering tasks.\n  Benchmark access and leaderboard:\nhttps://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:\nhttps://usc-isi-i2.github.io/TGCSR/", "published": "2022-03-23 04:06:01", "link": "http://arxiv.org/abs/2203.12184v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AbductionRules: Training Transformers to Explain Unexpected Inputs", "abstract": "Transformers have recently been shown to be capable of reliably performing\nlogical reasoning over facts and rules expressed in natural language, but\nabductive reasoning - inference to the best explanation of an unexpected\nobservation - has been underexplored despite significant applications to\nscientific discovery, common-sense reasoning, and model interpretability.\n  We present AbductionRules, a group of natural language datasets designed to\ntrain and test generalisable abduction over natural-language knowledge bases.\nWe use these datasets to finetune pretrained Transformers and discuss their\nperformance, finding that our models learned generalisable abductive techniques\nbut also learned to exploit the structure of our data. Finally, we discuss the\nviability of this approach to abductive reasoning and ways in which it may be\nimproved in future work.", "published": "2022-03-23 04:18:30", "link": "http://arxiv.org/abs/2203.12186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Vectorized Lexical Constraints for Neural Machine\n  Translation", "abstract": "Lexically constrained neural machine translation (NMT), which controls the\ngeneration of NMT models with pre-specified constraints, is important in many\npractical scenarios. Due to the representation gap between discrete constraints\nand continuous vectors in NMT models, most existing works choose to construct\nsynthetic data or modify the decoding algorithm to impose lexical constraints,\ntreating the NMT model as a black box. In this work, we propose to open this\nblack box by directly integrating the constraints into NMT models.\nSpecifically, we vectorize source and target constraints into continuous keys\nand values, which can be utilized by the attention modules of NMT models. The\nproposed integration method is based on the assumption that the correspondence\nbetween keys and values in attention modules is naturally suitable for modeling\nconstraint pairs. Experimental results show that our method consistently\noutperforms several representative baselines on four language pairs,\ndemonstrating the superiority of integrating vectorized lexical constraints.", "published": "2022-03-23 05:54:37", "link": "http://arxiv.org/abs/2203.12210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Named Entity Recognition with Self-describing Networks", "abstract": "Few-shot NER needs to effectively capture information from limited instances\nand transfer useful knowledge from external resources. In this paper, we\npropose a self-describing mechanism for few-shot NER, which can effectively\nleverage illustrative instances and precisely transfer knowledge from external\nresources by describing both entity types and mentions using a universal\nconcept set. Specifically, we design Self-describing Networks (SDNet), a\nSeq2Seq generation model which can universally describe mentions using\nconcepts, automatically map novel entity types to concepts, and adaptively\nrecognize entities on-demand. We pre-train SDNet with large-scale corpus, and\nconduct experiments on 8 benchmarks from different domains. Experiments show\nthat SDNet achieves competitive performances on all benchmarks and achieves the\nnew state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and\nrobustness.", "published": "2022-03-23 07:56:27", "link": "http://arxiv.org/abs/2203.12252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis", "abstract": "Many studies on dialog emotion analysis focus on utterance-level emotion\nonly. These models hence are not optimized for dialog-level emotion detection,\ni.e. to predict the emotion category of a dialog as a whole. More importantly,\nthese models cannot benefit from the context provided by the whole dialog. In\nreal-world applications, annotations to dialog could fine-grained, including\nboth utterance-level tags (e.g. speaker type, intent category, and emotion\ncategory), and dialog-level tags (e.g. user satisfaction, and emotion curve\ncategory). In this paper, we propose a Context-based Hierarchical Attention\nCapsule~(Chat-Capsule) model, which models both utterance-level and\ndialog-level emotions and their interrelations. On a dialog dataset collected\nfrom customer support of an e-commerce platform, our model is also able to\npredict user satisfaction and emotion curve category. Emotion curve refers to\nthe change of emotions along the development of a conversation. Experiments\nshow that the proposed Chat-Capsule outperform state-of-the-art baselines on\nboth benchmark dataset and proprietary dataset. Source code will be released\nupon acceptance.", "published": "2022-03-23 08:04:30", "link": "http://arxiv.org/abs/2203.12254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument\n  Mining Tasks", "abstract": "Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.", "published": "2022-03-23 08:07:32", "link": "http://arxiv.org/abs/2203.12257v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible\n  Risks from a Causal View", "abstract": "Prompt-based probing has been widely used in evaluating the abilities of\npretrained language models (PLMs). Unfortunately, recent studies have\ndiscovered such an evaluation may be inaccurate, inconsistent and unreliable.\nFurthermore, the lack of understanding its inner workings, combined with its\nwide applicability, has the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world applications. To discover,\nunderstand and quantify the risks, this paper investigates the prompt-based\nprobing from a causal view, highlights three critical biases which could induce\nbiased results and conclusions, and proposes to conduct debiasing via causal\nintervention. This paper provides valuable insights for the design of unbiased\ndatasets, better probing frameworks and more reliable evaluations of pretrained\nlanguage models. Furthermore, our conclusions also echo that we need to rethink\nthe criteria for identifying better pretrained language models. We openly\nreleased the source code and data at https://github.com/c-box/causalEval.", "published": "2022-03-23 08:10:07", "link": "http://arxiv.org/abs/2203.12258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECO v1: Towards Event-Centric Opinion Mining", "abstract": "Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.", "published": "2022-03-23 08:20:45", "link": "http://arxiv.org/abs/2203.12264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training to Match for Unified Low-shot Relation Extraction", "abstract": "Low-shot relation extraction~(RE) aims to recognize novel relations with very\nfew or even no samples, which is critical in real scenario application.\nFew-shot and zero-shot RE are two representative low-shot RE tasks, which seem\nto be with similar target but require totally different underlying abilities.\nIn this paper, we propose Multi-Choice Matching Networks to unify low-shot\nrelation extraction. To fill in the gap between zero-shot and few-shot RE, we\npropose the triplet-paraphrase meta-training, which leverages triplet\nparaphrase to pre-train zero-shot label matching ability and uses meta-learning\nparadigm to learn few-shot instance summarizing ability. Experimental results\non three different low-shot RE tasks show that the proposed method outperforms\nstrong baselines by a large margin, and achieve the best performance on\nfew-shot RE leaderboard.", "published": "2022-03-23 08:43:52", "link": "http://arxiv.org/abs/2203.12274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through\n  Regularized Self-Attention", "abstract": "Sparse Transformer has recently attracted a lot of attention since the\nability for reducing the quadratic dependency on the sequence length. We argue\nthat two factors, information bottleneck sensitivity and inconsistency between\ndifferent attention topologies, could affect the performance of the Sparse\nTransformer. This paper proposes a well-designed model named ERNIE-Sparse. It\nconsists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to\nsequentially unify local and global information. (ii) Self-Attention\nRegularization (SAR) method, a novel regularization designed to minimize the\ndistance for transformers with different attention topologies. To evaluate the\neffectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we\nperform experiments on a multi-modal long sequence modeling task benchmark,\nLong Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse\nsignificantly outperforms a variety of strong baseline methods including the\ndense attention and other efficient sparse attention methods and achieves\nimprovements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the\neffectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text\nclassification and 2 QA downstream tasks, achieve improvements on\nclassification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%\n(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior\nperformance.", "published": "2022-03-23 08:47:01", "link": "http://arxiv.org/abs/2203.12276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Structure Generation for Universal Information Extraction", "abstract": "Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.", "published": "2022-03-23 08:49:29", "link": "http://arxiv.org/abs/2203.12277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based System for Personality and Interpersonal Reactivity\n  Prediction", "abstract": "This paper describes our proposed method for the Workshop on Computational\nApproaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) 2022\nshared task on Personality Prediction (PER) and Reactivity Index Prediction\n(IRI). In this paper, we adopt the prompt-based learning method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide knowledge of the extra personalized information for\nenhancing the pre-trained model. Data augmentation and model ensemble are\nadopted for obtaining better results. Moreover, we also provided the online\nsoftware demonstration and the codes of the software for further research.", "published": "2022-03-23 15:22:34", "link": "http://arxiv.org/abs/2203.12481v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational historical linguistics and language diversity in South\n  Asia", "abstract": "South Asia is home to a plethora of languages, many of which severely lack\naccess to new language technologies. This linguistic diversity also results in\na research environment conducive to the study of comparative, contact, and\nhistorical linguistics -- fields which necessitate the gathering of extensive\ndata from many languages. We claim that data scatteredness (rather than\nscarcity) is the primary obstacle in the development of South Asian language\ntechnology, and suggest that the study of language history is uniquely aligned\nwith surmounting this obstacle. We review recent developments in and at the\nintersection of South Asian NLP and historical-comparative linguistics,\ndescribing our and others' current efforts in this area. We also offer new\nstrategies towards breaking the data barrier.", "published": "2022-03-23 16:36:24", "link": "http://arxiv.org/abs/2203.12524v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Dynamically Refined Regularization for Improving Cross-corpora Hate\n  Speech Detection", "abstract": "Hate speech classifiers exhibit substantial performance degradation when\nevaluated on datasets different from the source. This is due to learning\nspurious correlations between words that are not necessarily relevant to\nhateful language, and hate speech labels from the training corpus. Previous\nwork has attempted to mitigate this problem by regularizing specific terms from\npre-defined static dictionaries. While this has been demonstrated to improve\nthe generalizability of classifiers, the coverage of such methods is limited\nand the dictionaries require regular manual updates from human experts. In this\npaper, we propose to automatically identify and reduce spurious correlations\nusing attribution methods with dynamic refinement of the list of terms that\nneed to be regularized during training. Our approach is flexible and improves\nthe cross-corpora performance over previous work independently and in\ncombination with pre-defined dictionaries.", "published": "2022-03-23 16:58:10", "link": "http://arxiv.org/abs/2203.12536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Improving Model Robustness? Look at Both\n  Prediction and Interpretation", "abstract": "Neural language models show vulnerability to adversarial examples which are\nsemantically similar to their original counterparts with a few words replaced\nby their synonyms. A common way to improve model robustness is adversarial\ntraining which follows two steps-collecting adversarial examples by attacking a\ntarget model, and fine-tuning the model on the augmented dataset with these\nadversarial examples. The objective of traditional adversarial training is to\nmake a model produce the same correct predictions on an original/adversarial\nexample pair. However, the consistency between model decision-makings on two\nsimilar texts is ignored. We argue that a robust model should behave\nconsistently on original/adversarial example pairs, that is making the same\npredictions (what) based on the same reasons (how) which can be reflected by\nconsistent interpretations. In this work, we propose a novel feature-level\nadversarial training method named FLAT. FLAT aims at improving model robustness\nin terms of both predictions and interpretations. FLAT incorporates variational\nword masks in neural networks to learn global word importance and play as a\nbottleneck teaching the model to make predictions based on important words.\nFLAT explicitly shoots at the vulnerability problem caused by the mismatch\nbetween model understandings on the replaced words and their synonyms in\noriginal/adversarial example pairs by regularizing the corresponding global\nword importance scores. Experiments show the effectiveness of FLAT in improving\nthe robustness with respect to both predictions and interpretations of four\nneural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks\non four text classification tasks. The models trained via FLAT also show better\nrobustness than baseline models on unforeseen adversarial examples across\ndifferent attacks.", "published": "2022-03-23 20:04:14", "link": "http://arxiv.org/abs/2203.12709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Converse: A Tree-Based Modular Task-Oriented Dialogue System", "abstract": "Creating a system that can have meaningful conversations with humans to help\naccomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).\nIt has defined the meaning of AI since the beginning. A lot has been\naccomplished in this area recently, with voice assistant products entering our\ndaily lives and chat bot systems becoming commonplace in customer service. At\nfirst glance there seems to be no shortage of options for dialogue systems.\nHowever, the frequently deployed dialogue systems today seem to all struggle\nwith a critical weakness - they are hard to build and harder to maintain. At\nthe core of the struggle is the need to script every single turn of\ninteractions between the bot and the human user. This makes the dialogue\nsystems more difficult to maintain as the tasks become more complex and more\ntasks are added to the system. In this paper, we propose Converse, a flexible\ntree-based modular task-oriented dialogue system. Converse uses an and-or tree\nstructure to represent tasks and offers powerful multi-task dialogue\nmanagement. Converse supports task dependency and task switching, which are\nunique features compared to other open-source dialogue frameworks. At the same\ntime, Converse aims to make the bot building process easy and simple, for both\nprofessional and non-professional software developers. The code is available at\nhttps://github.com/salesforce/Converse.", "published": "2022-03-23 04:19:05", "link": "http://arxiv.org/abs/2203.12187v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions", "abstract": "The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging", "published": "2022-03-23 07:07:11", "link": "http://arxiv.org/abs/2203.12235v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Framework for Fast Polarity Labelling of Massive Data Streams", "abstract": "Many of the existing sentiment analysis techniques are based on supervised\nlearning, and they demand the availability of valuable training datasets to\ntrain their models. When dataset freshness is critical, the annotating of high\nspeed unlabelled data streams becomes critical but remains an open problem. In\nthis paper, we propose PLStream, a novel Apache Flink-based framework for fast\npolarity labelling of massive data streams, like Twitter tweets or online\nproduct reviews. We address the associated implementation challenges and\npropose a list of techniques including both algorithmic improvements and system\noptimizations. A thorough empirical validation with two real-world workloads\ndemonstrates that PLStream is able to generate high quality labels (almost 80%\naccuracy) in the presence of high-speed continuous unlabelled data streams\n(almost 16,000 tuples/sec) without any manual efforts.", "published": "2022-03-23 12:41:38", "link": "http://arxiv.org/abs/2203.12368v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Survey on Cross-Lingual Summarization", "abstract": "Cross-lingual summarization is the task of generating a summary in one\nlanguage (e.g., English) for the given document(s) in a different language\n(e.g., Chinese). Under the globalization background, this task has attracted\nincreasing attention of the computational linguistics community. Nevertheless,\nthere still remains a lack of comprehensive review for this task. Therefore, we\npresent the first systematic critical review on the datasets, approaches, and\nchallenges in this field. Specifically, we carefully organize existing datasets\nand approaches according to different construction methods and solution\nparadigms, respectively. For each type of datasets or approaches, we thoroughly\nintroduce and summarize previous efforts and further compare them with each\nother to provide deeper analyses. In the end, we also discuss promising\ndirections and offer our thoughts to facilitate future research. This survey is\nfor both beginners and experts in cross-lingual summarization, and we hope it\nwill serve as a starting point as well as a source of new ideas for researchers\nand engineers interested in this area.", "published": "2022-03-23 16:24:21", "link": "http://arxiv.org/abs/2203.12515v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual\n  Role Reversal", "abstract": "Language models excel at generating coherent text, and model compression\ntechniques such as knowledge distillation have enabled their use in\nresource-constrained settings. However, these models can be biased in multiple\nways, including the unfounded association of male and female genders with\ngender-neutral professions. Therefore, knowledge distillation without any\nfairness constraints may preserve or exaggerate the teacher model's biases onto\nthe distilled model. To this end, we present a novel approach to mitigate\ngender disparity in text generation by learning a fair model during knowledge\ndistillation. We propose two modifications to the base knowledge distillation\nbased on counterfactual role reversal$\\unicode{x2014}$modifying teacher\nprobabilities and augmenting the training set. We evaluate gender polarity\nacross professions in open-ended text generated from the resulting distilled\nand finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial\nreduction in gender disparity with only a minor compromise in utility. Finally,\nwe observe that language models that reduce gender polarity in language\ngeneration do not improve embedding fairness or downstream classification\nfairness.", "published": "2022-03-23 17:34:35", "link": "http://arxiv.org/abs/2203.12574v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linearizing Transformer with Key-Value Memory", "abstract": "Efficient transformer variants with linear time complexity have been\ndeveloped to mitigate the quadratic computational overhead of the vanilla\ntransformer. Among them are low-rank projection methods such as Linformer and\nkernel-based Transformers. Despite their unique merits, they usually suffer\nfrom a performance drop comparing with the vanilla transformer on many sequence\ngeneration tasks, and often fail to obtain computation gain when the generation\nis short. We propose MemSizer, an approach towards closing the performance gap\nwhile improving the efficiency even with short generation. It projects the\nsource sequences into lower dimension representations like Linformer, while\nenjoying efficient recurrent-style incremental computation similar to\nkernel-based transformers. This yields linear computation time and constant\nmemory complexity at inference time. MemSizer also employs a lightweight\nmulti-head mechanism which renders the computation as light as a single-head\nmodel. We demonstrate that MemSizer provides an improved balance between\nefficiency and accuracy over the vanilla transformer and other efficient\ntransformer variants in three typical sequence generation tasks, including\nmachine translation, abstractive text summarization, and language modeling.", "published": "2022-03-23 18:10:18", "link": "http://arxiv.org/abs/2203.12644v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ThingTalk: An Extensible, Executable Representation Language for\n  Task-Oriented Dialogues", "abstract": "Task-oriented conversational agents rely on semantic parsers to translate\nnatural language to formal representations. In this paper, we propose the\ndesign and rationale of the ThingTalk formal representation, and how the design\nimproves the development of transactional task-oriented agents.\n  ThingTalk is built on four core principles: (1) representing user requests\ndirectly as executable statements, covering all the functionality of the agent,\n(2) representing dialogues formally and succinctly to support accurate\ncontextual semantic parsing, (3) standardizing types and interfaces to maximize\nreuse between agents, and (4) allowing multiple, independently-developed agents\nto be composed in a single virtual assistant. ThingTalk is developed as part of\nthe Genie Framework that allows developers to quickly build transactional\nagents given a database and APIs.\n  We compare ThingTalk to existing representations: SMCalFlow, SGD, TreeDST.\nCompared to the others, the ThingTalk design is both more general and more\ncost-effective. Evaluated on the MultiWOZ benchmark, using ThingTalk and\nassociated tools yields a new state of the art accuracy of 79% turn-by-turn.", "published": "2022-03-23 22:40:50", "link": "http://arxiv.org/abs/2203.12751v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "An Empirical Study on Learning and Improving the Search Objective for\n  Unsupervised Paraphrasing", "abstract": "Research in unsupervised text generation has been gaining attention over the\nyears. One recent approach is local search towards a heuristically defined\nobjective, which specifies language fluency, semantic meanings, and other\ntask-specific attributes. Search in the sentence space is realized by\nword-level edit operations including insertion, replacement, and deletion.\nHowever, such objective function is manually designed with multiple components.\nAlthough previous work has shown maximizing this objective yields good\nperformance in terms of true measure of success (i.e. BLEU and iBLEU), the\nobjective landscape is considered to be non-smooth with significant noises,\nposing challenges for optimization. In this dissertation, we address the\nresearch problem of smoothing the noise in the heuristic search objective by\nlearning to model the search dynamics. Then, the learned model is combined with\nthe original objective function to guide the search in a bootstrapping fashion.\nExperimental results show that the learned models combined with the original\nsearch objective can indeed provide a smoothing effect, improving the search\nperformance by a small margin.", "published": "2022-03-23 00:30:28", "link": "http://arxiv.org/abs/2203.12106v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Expressive Speaking Style Modelling with Hierarchical Context\n  Information for Mandarin Speech Synthesis", "abstract": "Previous works on expressive speech synthesis mainly focus on current\nsentence. The context in adjacent sentences is neglected, resulting in\ninflexible speaking style for the same text, which lacks speech variations. In\nthis paper, we propose a hierarchical framework to model speaking style from\ncontext. A hierarchical context encoder is proposed to explore a wider range of\ncontextual information considering structural relationship in context,\nincluding inter-phrase and inter-sentence relations. Moreover, to encourage\nthis encoder to learn style representation better, we introduce a novel\ntraining strategy with knowledge distillation, which provides the target for\nencoder training. Both objective and subjective evaluations on a Mandarin\nlecture dataset demonstrate that the proposed method can significantly improve\nthe naturalness and expressiveness of the synthesized speech.", "published": "2022-03-23 05:27:57", "link": "http://arxiv.org/abs/2203.12201v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Input-specific Attention Subnetworks for Adversarial Detection", "abstract": "Self-attention heads are characteristic of Transformer models and have been\nwell studied for interpretability and pruning. In this work, we demonstrate an\naltogether different utility of attention heads, namely for adversarial\ndetection. Specifically, we propose a method to construct input-specific\nattention subnetworks (IAS) from which we extract three features to\ndiscriminate between authentic and adversarial inputs. The resultant detector\nsignificantly improves (by over 7.5%) the state-of-the-art adversarial\ndetection accuracy for the BERT encoder on 10 NLU datasets with 11 different\nadversarial attack types. We also demonstrate that our method (a) is more\naccurate for larger models which are likely to have more spurious correlations\nand thus vulnerable to adversarial attack, and (b) performs well even with\nmodest training sets of adversarial examples.", "published": "2022-03-23 09:46:41", "link": "http://arxiv.org/abs/2203.12298v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The VoicePrivacy 2022 Challenge Evaluation Plan", "abstract": "For new participants - Executive summary: (1) The task is to develop a voice\nanonymization system for speech data which conceals the speaker's voice\nidentity while protecting linguistic content, paralinguistic attributes,\nintelligibility and naturalness. (2) Training, development and evaluation\ndatasets are provided in addition to 3 different baseline anonymization\nsystems, evaluation scripts, and metrics. Participants apply their developed\nanonymization systems, run evaluation scripts and submit objective evaluation\nresults and anonymized speech data to the organizers. (3) Results will be\npresented at a workshop held in conjunction with INTERSPEECH 2022 to which all\nparticipants are invited to present their challenge systems and to submit\nadditional workshop papers.\n  For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:\n(1) A stronger, semi-informed attack model in the form of an automatic speaker\nverification (ASV) system trained on anonymized (per-utterance) speech data.\n(2) Complementary metrics comprising the equal error rate (EER) as a privacy\nmetric, the word error rate (WER) as a primary utility metric, and the pitch\ncorrelation and gain of voice distinctiveness as secondary utility metrics. (3)\nA new ranking policy based upon a set of minimum target privacy requirements.", "published": "2022-03-23 15:05:18", "link": "http://arxiv.org/abs/2203.12468v3", "categories": ["eess.AS", "cs.CL", "cs.CR"], "primary_category": "eess.AS"}
{"title": "A Context-Aware Feature Fusion Framework for Punctuation Restoration", "abstract": "To accomplish the punctuation restoration task, most existing approaches\nfocused on leveraging extra information (e.g., part-of-speech tags) or\naddressing the class imbalance problem. Recent works have widely applied the\ntransformer-based language models and significantly improved their\neffectiveness. To the best of our knowledge, an inherent issue has remained\nneglected: the attention of individual heads in the transformer will be diluted\nor powerless while feeding the long non-punctuation utterances. Since those\nprevious contexts, not the followings, are comparatively more valuable to the\ncurrent position, it's hard to achieve a good balance by independent attention.\nIn this paper, we propose a novel Feature Fusion framework based on two-type\nAttentions (FFA) to alleviate the shortage. It introduces a two-stream\narchitecture. One module involves interaction between attention heads to\nencourage the communication, and another masked attention module captures the\ndependent feature representation. Then, it aggregates two feature embeddings to\nfuse information and enhances context-awareness. The experiments on the popular\nbenchmark dataset IWSLT demonstrate that our approach is effective. Without\nadditional data, it obtains comparable performance to the current\nstate-of-the-art models.", "published": "2022-03-23 15:29:28", "link": "http://arxiv.org/abs/2203.12487v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic\n  Information for Language Identification", "abstract": "We propose a novel model to hierarchically incorporate phoneme and\nphonotactic information for language identification (LID) without requiring\nphoneme annotations for training. In this model, named PHO-LID, a\nself-supervised phoneme segmentation task and a LID task share a convolutional\nneural network (CNN) module, which encodes both language identity and\nsequential phonemic information in the input speech to generate an intermediate\nsequence of phonotactic embeddings. These embeddings are then fed into\ntransformer encoder layers for utterance-level LID. We call this architecture\nCNN-Trans. We evaluate it on AP17-OLR data and the MLS14 set of NIST LRE 2017,\nand show that the PHO-LID model with multi-task optimization exhibits the\nhighest LID performance among all models, achieving over 40% relative\nimprovement in terms of average cost on AP17-OLR data compared to a CNN-Trans\nmodel optimized only for LID. The visualized confusion matrices imply that our\nproposed method achieves higher performance on languages of the same cluster in\nNIST LRE 2017 data than the CNN-Trans model. A comparison between predicted\nphoneme boundaries and corresponding audio spectrograms illustrates the\nleveraging of phoneme information for LID.", "published": "2022-03-23 12:38:38", "link": "http://arxiv.org/abs/2203.12366v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Scalable Model Specialization Framework for Training and Inference\n  using Submodels and its Application to Speech Model Personalization", "abstract": "Model fine-tuning and adaptation have become a common approach for model\nspecialization for downstream tasks or domains. Fine-tuning the entire model or\na subset of the parameters using light-weight adaptation has shown considerable\nsuccess across different specialization tasks. Fine-tuning a model for a large\nnumber of domains typically requires starting a new training job for every\ndomain posing scaling limitations. Once these models are trained, deploying\nthem also poses significant scalability challenges for inference for real-time\napplications. In this paper, building upon prior light-weight adaptation\ntechniques, we propose a modular framework that enables us to substantially\nimprove scalability for model training and inference. We introduce Submodels\nthat can be quickly and dynamically loaded for on-the-fly inference. We also\npropose multiple approaches for training those Submodels in parallel using an\nembedding space in the same training job. We test our framework on an extreme\nuse-case which is speech model personalization for atypical speech, requiring a\nSubmodel for each user. We obtain 128x Submodel throughput with a fixed\ncomputation budget without a loss of accuracy. We also show that learning a\nspeaker-embedding space can scale further and reduce the amount of\npersonalization training data required per speaker.", "published": "2022-03-23 17:21:39", "link": "http://arxiv.org/abs/2203.12559v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Generation Using an LSTM", "abstract": "Over the past several years, deep learning for sequence modeling has grown in\npopularity. To achieve this goal, LSTM network structures have proven to be\nvery useful for making predictions for the next output in a series. For\ninstance, a smartphone predicting the next word of a text message could use an\nLSTM. We sought to demonstrate an approach of music generation using Recurrent\nNeural Networks (RNN). More specifically, a Long Short-Term Memory (LSTM)\nneural network. Generating music is a notoriously complicated task, whether\nhandmade or generated, as there are a myriad of components involved. Taking\nthis into account, we provide a brief synopsis of the intuition, theory, and\napplication of LSTMs in music generation, develop and present the network we\nfound to best achieve this goal, identify and address issues and challenges\nfaced, and include potential future improvements for our network.", "published": "2022-03-23 00:13:41", "link": "http://arxiv.org/abs/2203.12105v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Adversarial Robustness of Large-scale Audio Visual Learning", "abstract": "As audio-visual systems are being deployed for safety-critical tasks such as\nsurveillance and malicious content filtering, their robustness remains an\nunder-studied area. Existing published work on robustness either does not scale\nto large-scale dataset, or does not deal with multiple modalities. This work\naims to study several key questions related to multi-modal learning through the\nlens of robustness: 1) Are multi-modal models necessarily more robust than\nuni-modal models? 2) How to efficiently measure the robustness of multi-modal\nlearning? 3) How to fuse different modalities to achieve a more robust\nmulti-modal model? To understand the robustness of the multi-modal model in a\nlarge-scale setting, we propose a density-based metric, and a convexity metric\nto efficiently measure the distribution of each modality in high-dimensional\nlatent space. Our work provides a theoretical intuition together with empirical\nevidence showing how multi-modal fusion affects adversarial robustness through\nthese metrics. We further devise a mix-up strategy based on our metrics to\nimprove the robustness of the trained model. Our experiments on AudioSet and\nKinetics-Sounds verify our hypothesis that multi-modal models are not\nnecessarily more robust than their uni-modal counterparts in the face of\nadversarial examples. We also observe our mix-up trained method could achieve\nas much protection as traditional adversarial training, offering a\ncomputationally cheap alternative. Implementation:\nhttps://github.com/lijuncheng16/AudioSetDoneRight", "published": "2022-03-23 01:31:17", "link": "http://arxiv.org/abs/2203.12122v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for\n  Speech Enhancement", "abstract": "Previously proposed FullSubNet has achieved outstanding performance in Deep\nNoise Suppression (DNS) Challenge and attracted much attention. However, it\nstill encounters issues such as input-output mismatch and coarse processing for\nfrequency bands. In this paper, we propose an extended single-channel real-time\nspeech enhancement framework called FullSubNet+ with following significant\nimprovements. First, we design a lightweight multi-scale time sensitive channel\nattention (MulCA) module which adopts multi-scale convolution and channel\nattention mechanism to help the network focus on more discriminative frequency\nbands for noise reduction. Then, to make full use of the phase information in\nnoisy speech, our model takes all the magnitude, real and imaginary\nspectrograms as inputs. Moreover, by replacing the long short-term memory\n(LSTM) layers in original full-band model with stacked temporal convolutional\nnetwork (TCN) blocks, we design a more efficient full-band module called\nfull-band extractor. The experimental results in DNS Challenge dataset show the\nsuperior performance of our FullSubNet+, which reaches the state-of-the-art\n(SOTA) performance and outperforms other existing speech enhancement\napproaches.", "published": "2022-03-23 04:33:09", "link": "http://arxiv.org/abs/2203.12188v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantitative Evaluation Approach for Translation of Perceptual\n  Soundscape Attributes: Initial Application to the Thai Language", "abstract": "Translation of perceptual soundscape attributes from one language to another\nremains a challenging task that requires a high degree of fidelity in both\npsychoacoustic and psycholinguistic senses across the target population. Due to\nthe inherently subjective nature of human perception, translating soundscape\nattributes using only small focus group discussion or expert panels could lead\nto translations with psycholinguistic meanings that, in a non-expert setting,\ndeviate or distort from that of the source language. In this work, we present a\nquantitative evaluation method based on the circumplex model of soundscape\nperception to assess the overall translation quality across a set of criteria.\nAs an initial application domain, we demonstrated the use of the quantitative\nevaluation framework in the context of an English-to-Thai translation of\nsoundscape attributes.", "published": "2022-03-23 07:37:53", "link": "http://arxiv.org/abs/2203.12245v3", "categories": ["cs.SD", "eess.AS", "stat.AP", "stat.ME"], "primary_category": "cs.SD"}
{"title": "A combination between VQ and covariance matrices for speaker recognition", "abstract": "This paper presents a new algorithm for speaker recognition based on the\ncombination between the classical Vector Quantization (VQ) and Covariance\nMatrix (CM) methods. The combined VQ-CM method improves the identification\nrates of each method alone, with comparable computational burden. It offers a\nstraightforward procedure to obtain a model similar to GMM with full covariance\nmatrices. Experimental results also show that it is more robust against noise\nthan VQ or CM alone.", "published": "2022-03-23 10:06:41", "link": "http://arxiv.org/abs/2203.12306v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wider or Deeper Neural Network Architecture for Acoustic Scene\n  Classification with Mismatched Recording Devices", "abstract": "In this paper, we present a robust and low complexity system for Acoustic\nScene Classification (ASC), the task of identifying the scene of an audio\nrecording. We first construct an ASC baseline system in which a novel\ninception-residual-based network architecture is proposed to deal with the\nmismatched recording device issue. To further improve the performance but still\nsatisfy the low complexity model, we apply two techniques: ensemble of multiple\nspectrograms and channel reduction on the ASC baseline system. By conducting\nextensive experiments on the benchmark DCASE 2020 Task 1A Development dataset,\nwe achieve the best model performing an accuracy of 69.9% and a low complexity\nof 2.4M trainable parameters, which is competitive to the state-of-the-art ASC\nsystems and potential for real-life applications on edge devices.", "published": "2022-03-23 10:27:41", "link": "http://arxiv.org/abs/2203.12314v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data", "abstract": "Training of speech enhancement systems often does not incorporate knowledge\nof human perception and thus can lead to unnatural sounding results.\nIncorporating psychoacoustically motivated speech perception metrics as part of\nmodel training via a predictor network has recently gained interest. However,\nthe performance of such predictors is limited by the distribution of metric\nscores that appear in the training data. In this work, we propose MetricGAN+/-\n(an extension of MetricGAN+, one such metric-motivated system) which introduces\nan additional network - a \"de-generator\" which attempts to improve the\nrobustness of the prediction network (and by extension of the generator) by\nensuring observation of a wider range of metric scores in training.\nExperimental results on the VoiceBank-DEMAND dataset show relative improvement\nin PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better\ngeneralisation to unseen noise and speech.", "published": "2022-03-23 12:42:28", "link": "http://arxiv.org/abs/2203.12369v5", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An interactive music infilling interface for pop music composition", "abstract": "Artificial intelligence (AI) has been widely applied to music generation\ntopics such as continuation, melody/harmony generation, genre transfer and\nmusic infilling application. Although with the burst interest to apply AI to\nmusic, there are still few interfaces for the musicians to take advantage of\nthe latest progress of the AI technology. This makes those tools less valuable\nin practice and harder to find its advantage/drawbacks without utilizing them\nin the real scenario. This work builds a max patch for interactive music\ninfilling application with different levels of control, including track\ndensity/polyphony/occupation rate and bar tonal tension control. The user can\nselect the melody/bass/harmony track as the infilling content up to 16 bars.\nThe infilling algorithm is based on the author's previous work, and the\ninterface sends/receives messages to the AI system hosted in the cloud. This\ninterface lowers the barrier of AI technology and can generate different\nvariations of the selected content. Those results can give several alternatives\nto the musicians' composition, and the interactive process realizes the value\nof the AI infilling system.", "published": "2022-03-23 21:40:01", "link": "http://arxiv.org/abs/2203.12736v1", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
