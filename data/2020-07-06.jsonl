{"title": "Reflection-based Word Attribute Transfer", "abstract": "Word embeddings, which often represent such analogic relations as king - man\n+ woman = queen, can be used to change a word's attribute, including its\ngender. For transferring king into queen in this analogy-based manner, we\nsubtract a difference vector man - woman based on the knowledge that king is\nmale. However, developing such knowledge is very costly for words and\nattributes. In this work, we propose a novel method for word attribute transfer\nbased on reflection mappings without such an analogy operation. Experimental\nresults show that our proposed method can transfer the word attributes of the\ngiven words without changing the words that do not have the target attributes.", "published": "2020-07-06 09:17:14", "link": "http://arxiv.org/abs/2007.02598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Broad-Coverage Deep Semantic Lexicon for Verbs", "abstract": "Progress on deep language understanding is inhibited by the lack of a broad\ncoverage lexicon that connects linguistic behavior to ontological concepts and\naxioms. We have developed COLLIE-V, a deep lexical resource for verbs, with the\ncoverage of WordNet and syntactic and semantic details that meet or exceed\nexisting resources. Bootstrapping from a hand-built lexicon and ontology, new\nontological concepts and lexical entries, together with semantic role\npreferences and entailment axioms, are automatically derived by combining\nmultiple constraints from parsing dictionary definitions and examples. We\nevaluated the accuracy of the technique along a number of different dimensions\nand were able to obtain high accuracy in deriving new concepts and lexical\nentries. COLLIE-V is publicly available.", "published": "2020-07-06 12:03:14", "link": "http://arxiv.org/abs/2007.02670v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Bilingual Dictionary Based Neural Machine Translation without Using\n  Parallel Sentences", "abstract": "In this paper, we propose a new task of machine translation (MT), which is\nbased on no parallel sentences but can refer to a ground-truth bilingual\ndictionary. Motivated by the ability of a monolingual speaker learning to\ntranslate via looking up the bilingual dictionary, we propose the task to see\nhow much potential an MT system can attain using the bilingual dictionary and\nlarge scale monolingual corpora, while is independent on parallel sentences. We\npropose anchored training (AT) to tackle the task. AT uses the bilingual\ndictionary to establish anchoring points for closing the gap between source\nlanguage and target language. Experiments on various language pairs show that\nour approaches are significantly better than various baselines, including\ndictionary-based word-by-word translation, dictionary-supervised cross-lingual\nword embedding transformation, and unsupervised MT. On distant language pairs\nthat are hard for unsupervised MT to perform well, AT performs remarkably\nbetter, achieving performances comparable to supervised SMT trained on more\nthan 4M parallel sentences.", "published": "2020-07-06 12:05:27", "link": "http://arxiv.org/abs/2007.02671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Polarity Detection on Bengali Book Reviews Using Multinomial\n  Naive Bayes", "abstract": "Recently, sentiment polarity detection has increased attention to NLP\nresearchers due to the massive availability of customer's opinions or reviews\nin the online platform. Due to the continued expansion of e-commerce sites, the\nrate of purchase of various products, including books, are growing enormously\namong the people. Reader's opinions/reviews affect the buying decision of a\ncustomer in most cases. This work introduces a machine learning-based technique\nto determine sentiment polarities (either positive or negative category) from\nBengali book reviews. To assess the effectiveness of the proposed technique, a\ncorpus with 2000 reviews on Bengali books is developed. A comparative analysis\nwith various approaches (such as logistic regression, naive Bayes, SVM, and\nSGD) also performed by taking into consideration of the unigram, bigram, and\ntrigram features, respectively. Experimental result reveals that the\nmultinomial Naive Bayes with unigram feature outperforms the other techniques\nwith 84% accuracy on the test set.", "published": "2020-07-06 13:58:51", "link": "http://arxiv.org/abs/2007.02758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DART: Open-Domain Structured Data Record to Text Generation", "abstract": "We present DART, an open domain structured DAta Record to Text generation\ndataset with over 82k instances (DARTs). Data-to-Text annotations can be a\ncostly process, especially when dealing with tables which are the major source\nof structured data and contain nontrivial structures. To this end, we propose a\nprocedure of extracting semantic triples from tables that encodes their\nstructures by exploiting the semantic dependencies among table headers and the\ntable title. Our dataset construction framework effectively merged\nheterogeneous sources from open domain semantic parsing and dialogue-act-based\nmeaning representation tasks by utilizing techniques such as: tree ontology\nannotation, question-answer pair to declarative sentence conversion, and\npredicate unification, all with minimum post-editing. We present systematic\nevaluation on DART as well as new state-of-the-art results on WebNLG 2017 to\nshow that DART (1) poses new challenges to existing data-to-text datasets and\n(2) facilitates out-of-domain generalization. Our data and code can be found at\nhttps://github.com/Yale-LILY/dart.", "published": "2020-07-06 16:35:30", "link": "http://arxiv.org/abs/2007.02871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Spoken Word Representations from Convolutional\n  Autoencoders", "abstract": "A lot of work has been done to build text-based language models for\nperforming different NLP tasks, but not much research has been done in the case\nof audio-based language models. This paper proposes a Convolutional Autoencoder\nbased neural architecture to model syntactically and semantically adequate\ncontextualized representations of varying length spoken words. The use of such\nrepresentations can not only lead to great advances in the audio-based NLP\ntasks but can also curtail the loss of information like tone, expression,\naccent, etc while converting speech to text to perform these tasks. The\nperformance of the proposed model is validated by (1) examining the generated\nvector space, and (2) evaluating its performance on three benchmark datasets\nfor measuring word similarities, against existing widely used text-based\nlanguage models that are trained on the transcriptions. The proposed model was\nable to demonstrate its robustness when compared to the other two\nlanguage-based models.", "published": "2020-07-06 16:48:11", "link": "http://arxiv.org/abs/2007.02880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords", "abstract": "We present a new release of the Czech-English parallel corpus CzEng 2.0\nconsisting of over 2 billion words (2 \"gigawords\") in each language. The corpus\ncontains document-level information and is filtered with several techniques to\nlower the amount of noise. In addition to the data in the previous version of\nCzEng, it contains new authentic and also high-quality synthetic parallel data.\nCzEng is freely available for research and educational purposes.", "published": "2020-07-06 18:48:11", "link": "http://arxiv.org/abs/2007.03006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMVE at SemEval-2020 Task 4: Commonsense Validation and Explanation\n  using Pretraining Language Model", "abstract": "This paper describes our submission to subtask a and b of SemEval-2020 Task\n4. For subtask a, we use a ALBERT based model with improved input form to pick\nout the common sense statement from two statement candidates. For subtask b, we\nuse a multiple choice model enhanced by hint sentence mechanism to select the\nreason from given options about why a statement is against common sense.\nBesides, we propose a novel transfer learning strategy between subtasks which\nhelp improve the performance. The accuracy scores of our system are 95.6 / 94.9\non official test set and rank 7$^{th}$ / 2$^{nd}$ on Post-Evaluation\nleaderboard.", "published": "2020-07-06 05:51:10", "link": "http://arxiv.org/abs/2007.02540v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Relevance Transformer: Generating Concise Code Snippets with Relevance\n  Feedback", "abstract": "Tools capable of automatic code generation have the potential to augment\nprogrammer's capabilities. While straightforward code retrieval is incorporated\ninto many IDEs, an emerging area is explicit code generation. Code generation\nis currently approached as a Machine Translation task, with Recurrent Neural\nNetwork (RNN) based encoder-decoder architectures trained on code-description\npairs. In this work we introduce and study modern Transformer architectures for\nthis task. We further propose a new model called the Relevance Transformer that\nincorporates external knowledge using pseudo-relevance feedback. The Relevance\nTransformer biases the decoding process to be similar to existing retrieved\ncode while enforcing diversity. We perform experiments on multiple standard\nbenchmark datasets for code generation including Django, Hearthstone, and\nCoNaLa. The results show improvements over state-of-the-art methods based on\nBLEU evaluation. The Relevance Transformer model shows the potential of\nTransformer-based architectures for code generation and introduces a method of\nincorporating pseudo-relevance feedback during inference.", "published": "2020-07-06 09:54:17", "link": "http://arxiv.org/abs/2007.02609v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Labeling of Multilingual Breast MRI Reports", "abstract": "Medical reports are an essential medium in recording a patient's condition\nthroughout a clinical trial. They contain valuable information that can be\nextracted to generate a large labeled dataset needed for the development of\nclinical tools. However, the majority of medical reports are stored in an\nunregularized format, and a trained human annotator (typically a doctor) must\nmanually assess and label each case, resulting in an expensive and time\nconsuming procedure. In this work, we present a framework for developing a\nmultilingual breast MRI report classifier using a custom-built language\nrepresentation called LAMBR. Our proposed method overcomes practical challenges\nfaced in clinical settings, and we demonstrate improved performance in\nextracting labels from medical reports when compared with conventional\napproaches.", "published": "2020-07-06 19:22:44", "link": "http://arxiv.org/abs/2007.03028v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Spoken Language Representations with Neural Lattice Language\n  Modeling", "abstract": "Pre-trained language models have achieved huge improvement on many NLP tasks.\nHowever, these methods are usually designed for written text, so they do not\nconsider the properties of spoken language. Therefore, this paper aims at\ngeneralizing the idea of language model pre-training to lattices generated by\nrecognition systems. We propose a framework that trains neural lattice language\nmodels to provide contextualized representations for spoken language\nunderstanding tasks. The proposed two-stage pre-training approach reduces the\ndemands of speech data and has better efficiency. Experiments on intent\ndetection and dialogue act recognition datasets demonstrate that our proposed\nmethod consistently outperforms strong baselines when evaluated on spoken\ninputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.", "published": "2020-07-06 10:38:03", "link": "http://arxiv.org/abs/2007.02629v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters", "abstract": "We study training a single acoustic model for multiple languages with the aim\nof improving automatic speech recognition (ASR) performance on low-resource\nlanguages, and over-all simplifying deployment of ASR systems that support\ndiverse languages. We perform an extensive benchmark on 51 languages, with\nvarying amount of training data by language(from 100 hours to 1100 hours). We\ncompare three variants of multilingual training from a single joint model\nwithout knowing the input language, to using this information, to multiple\nheads (one per language cluster). We show that multilingual training of ASR\nmodels on several languages can improve recognition performance, in particular,\non low resource languages. We see 20.9%, 23% and 28.8% average WER relative\nreduction compared to monolingual baselines on joint model, joint model with\nlanguage input and multi head model respectively. To our knowledge, this is the\nfirst work studying multilingual ASR at massive scale, with more than 50\nlanguages and more than 16,000 hours of audio across them.", "published": "2020-07-06 18:43:38", "link": "http://arxiv.org/abs/2007.03001v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Contextual Embeddings for Address Classification in E-commerce", "abstract": "E-commerce customers in developing nations like India tend to follow no fixed\nformat while entering shipping addresses. Parsing such addresses is challenging\nbecause of a lack of inherent structure or hierarchy. It is imperative to\nunderstand the language of addresses, so that shipments can be routed without\ndelays. In this paper, we propose a novel approach towards understanding\ncustomer addresses by deriving motivation from recent advances in Natural\nLanguage Processing (NLP). We also formulate different pre-processing steps for\naddresses using a combination of edit distance and phonetic algorithms. Then we\napproach the task of creating vector representations for addresses using\nWord2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these\napproaches with respect to sub-region classification task for North and South\nIndian cities. Through experiments, we demonstrate the effectiveness of\ngeneralized RoBERTa model, pre-trained over a large address corpus for language\nmodelling task. Our proposed RoBERTa model achieves a classification accuracy\nof around 90% with minimal text preprocessing for sub-region classification\ntask outperforming all other approaches. Once pre-trained, the RoBERTa model\ncan be fine-tuned for various downstream tasks in supply chain like pincode\nsuggestion and geo-coding. The model generalizes well for such tasks even with\nlimited labelled data. To the best of our knowledge, this is the first of its\nkind research proposing a novel approach of understanding customer addresses in\ne-commerce domain by pre-training language models and fine-tuning them for\ndifferent purposes.", "published": "2020-07-06 19:06:34", "link": "http://arxiv.org/abs/2007.03020v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ResNeXt and Res2Net Structures for Speaker Verification", "abstract": "The ResNet-based architecture has been widely adopted to extract speaker\nembeddings for text-independent speaker verification systems. By introducing\nthe residual connections to the CNN and standardizing the residual blocks, the\nResNet structure is capable of training deep networks to achieve highly\ncompetitive recognition performance. However, when the input feature space\nbecomes more complicated, simply increasing the depth and width of the ResNet\nnetwork may not fully realize its performance potential. In this paper, we\npresent two extensions of the ResNet architecture, ResNeXt and Res2Net, for\nspeaker verification. Originally proposed for image recognition, the ResNeXt\nand Res2Net introduce two more dimensions, cardinality and scale, in addition\nto depth and width, to improve the model's representation capacity. By\nincreasing the scale dimension, the Res2Net model can represent multi-scale\nfeatures with various granularities, which particularly facilitates speaker\nverification for short utterances. We evaluate our proposed systems on three\nspeaker verification tasks. Experiments on the VoxCeleb test set demonstrated\nthat the ResNeXt and Res2Net can significantly outperform the conventional\nResNet model. The Res2Net model achieved superior performance by reducing the\nEER by 18.5% relative. Experiments on the other two internal test sets of\nmismatched conditions further confirmed the generalization of the ResNeXt and\nRes2Net architectures against noisy environment and segment length variations.", "published": "2020-07-06 00:18:41", "link": "http://arxiv.org/abs/2007.02480v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revisiting Representation Learning for Singing Voice Separation with\n  Sinkhorn Distances", "abstract": "In this work we present a method for unsupervised learning of audio\nrepresentations, focused on the task of singing voice separation. We build upon\na previously proposed method for learning representations of time-domain music\nsignals with a re-parameterized denoising autoencoder, extending it by using\nthe family of Sinkhorn distances with entropic regularization. We evaluate our\nmethod on the freely available MUSDB18 dataset of professionally produced music\nrecordings, and our results show that Sinkhorn distances with small strength of\nentropic regularization are marginally improving the performance of informed\nsinging voice separation. By increasing the strength of the entropic\nregularization, the learned representations of the mixture signal consists of\nalmost perfectly additive and distinctly structured sources.", "published": "2020-07-06 14:30:21", "link": "http://arxiv.org/abs/2007.02780v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Scene Classification with Spectrogram Processing Strategies", "abstract": "Recently, convolutional neural networks (CNN) have achieved the\nstate-of-the-art performance in acoustic scene classification (ASC) task. The\naudio data is often transformed into two-dimensional spectrogram\nrepresentations, which are then fed to the neural networks. In this paper, we\nstudy the problem of efficiently taking advantage of different spectrogram\nrepresentations through discriminative processing strategies. There are two\nmain contributions. The first contribution is exploring the impact of the\ncombination of multiple spectrogram representations at different stages, which\nprovides a meaningful reference for the effective spectrogram fusion. The\nsecond contribution is that the processing strategies in multiple frequency\nbands and multiple temporal frames are proposed to make fully use of a single\nspectrogram representation. The proposed spectrogram processing strategies can\nbe easily transferred to any network structures. The experiments are carried\nout on the DCASE 2020 Task1 datasets, and the results show that our method\ncould achieve the accuracy of 81.8% (official baseline: 54.1%) and 92.1%\n(official baseline: 87.3%) on the officially provided fold 1 evaluation dataset\nof Task1A and Task1B, respectively.", "published": "2020-07-06 15:18:47", "link": "http://arxiv.org/abs/2007.03781v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Sub-sampling of Audio Feature Sequences for Automated Audio\n  Captioning", "abstract": "Audio captioning is the task of automatically creating a textual description\nfor the contents of a general audio signal. Typical audio captioning methods\nrely on deep neural networks (DNNs), where the target of the DNN is to map the\ninput audio sequence to an output sequence of words, i.e. the caption. Though,\nthe length of the textual description is considerably less than the length of\nthe audio signal, for example 10 words versus some thousands of audio feature\nvectors. This clearly indicates that an output word corresponds to multiple\ninput feature vectors. In this work we present an approach that focuses on\nexplicitly taking advantage of this difference of lengths between sequences, by\napplying a temporal sub-sampling to the audio input sequence. We employ a\nsequence-to-sequence method, which uses a fixed-length vector as an output from\nthe encoder, and we apply temporal sub-sampling between the RNNs of the\nencoder. We evaluate the benefit of our approach by employing the freely\navailable dataset Clotho and we evaluate the impact of different factors of\ntemporal sub-sampling. Our results show an improvement to all considered\nmetrics.", "published": "2020-07-06 12:19:23", "link": "http://arxiv.org/abs/2007.02676v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Depthwise Separable Convolutions Versus Recurrent Neural Networks for\n  Monaural Singing Voice Separation", "abstract": "Recent approaches for music source separation are almost exclusively based on\ndeep neural networks, mostly employing recurrent neural networks (RNNs).\nAlthough RNNs are in many cases superior than other types of deep neural\nnetworks for sequence processing, they are known to have specific difficulties\nin training and parallelization, especially for the typically long sequences\nencountered in music source separation. In this paper we present a use-case of\nreplacing RNNs with depth-wise separable (DWS) convolutions, which are a\nlightweight and faster variant of the typical convolutions. We focus on singing\nvoice separation, employing an RNN architecture, and we replace the RNNs with\nDWS convolutions (DWS-CNNs). We conduct an ablation study and examine the\neffect of the number of channels and layers of DWS-CNNs on the source\nseparation performance, by utilizing the standard metrics of\nsignal-to-artifacts, signal-to-interference, and signal-to-distortion ratio.\nOur results show that by replacing RNNs with DWS-CNNs yields an improvement of\n1.20, 0.06, 0.37 dB, respectively, while using only 20.57% of the amount of\nparameters of the RNN architecture.", "published": "2020-07-06 12:32:34", "link": "http://arxiv.org/abs/2007.02683v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
