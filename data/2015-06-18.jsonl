{"title": "Comparing the writing style of real and artificial papers", "abstract": "Recent years have witnessed the increase of competition in science. While\npromoting the quality of research in many cases, an intense competition among\nscientists can also trigger unethical scientific behaviors. To increase the\ntotal number of published papers, some authors even resort to software tools\nthat are able to produce grammatical, but meaningless scientific manuscripts.\nBecause automatically generated papers can be misunderstood as real papers, it\nbecomes of paramount importance to develop means to identify these scientific\nfrauds. In this paper, I devise a methodology to distinguish real manuscripts\nfrom those generated with SCIGen, an automatic paper generator. Upon modeling\ntexts as complex networks (CN), it was possible to discriminate real from fake\npapers with at least 89\\% of accuracy. A systematic analysis of features\nrelevance revealed that the accessibility and betweenness were useful in\nparticular cases, even though the relevance depended upon the dataset. The\nsuccessful application of the methods described here show, as a proof of\nprinciple, that network features can be used to identify scientific gibberish\npapers. In addition, the CN-based approach can be combined in a straightforward\nfashion with traditional statistical language processing methods to improve the\nperformance in identifying artificially generated papers.", "published": "2015-06-18 14:46:15", "link": "http://arxiv.org/abs/1506.05702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"The Sum of Its Parts\": Joint Learning of Word and Phrase\n  Representations with Autoencoders", "abstract": "Recently, there has been a lot of effort to represent words in continuous\nvector spaces. Those representations have been shown to capture both semantic\nand syntactic information about words. However, distributed representations of\nphrases remain a challenge. We introduce a novel model that jointly learns word\nvector representations and their summation. Word representations are learnt\nusing the word co-occurrence statistical information. To embed sequences of\nwords (i.e. phrases) with different sizes into a common semantic space, we\npropose to average word vector representations. In contrast with previous\nmethods which reported a posteriori some compositionality aspects by simple\nsummation, we simultaneously train words to sum, while keeping the maximum\ninformation from the original vectors. We evaluate the quality of the word\nrepresentations on several classical word evaluation tasks, and we introduce a\nnovel task to evaluate the quality of the phrase representations. While our\ndistributed representations compete with other methods of learning word\nrepresentations on word evaluations, we show that they give better performance\non the phrase evaluation. Such representations of phrases could be interesting\nfor many tasks in natural language processing.", "published": "2015-06-18 14:46:44", "link": "http://arxiv.org/abs/1506.05703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing and evaluating extended Lambek calculi", "abstract": "Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was\ninnovative in many ways, notably as a precursor of linear logic. But it also\nshowed that we could treat our grammatical framework as a logic (as opposed to\na logical theory). However, though it was successful in giving at least a basic\ntreatment of many linguistic phenomena, it was also clear that a slightly more\nexpressive logical calculus was needed for many other cases. Therefore, many\nextensions and variants of the Lambek calculus have been proposed, since the\neighties and up until the present day. As a result, there is now a large class\nof calculi, each with its own empirical successes and theoretical results, but\nalso each with its own logical primitives. This raises the question: how do we\ncompare and evaluate these different logical formalisms? To answer this\nquestion, I present two unifying frameworks for these extended Lambek calculi.\nBoth are proof net calculi with graph contraction criteria. The first calculus\nis a very general system: you specify the structure of your sequents and it\ngives you the connectives and contractions which correspond to it. The calculus\ncan be extended with structural rules, which translate directly into graph\nrewrite rules. The second calculus is first-order (multiplicative\nintuitionistic) linear logic, which turns out to have several other,\nindependently proposed extensions of the Lambek calculus as fragments. I will\nillustrate the use of each calculus in building bridges between analyses\nproposed in different frameworks, in highlighting differences and in helping to\nidentify problems.", "published": "2015-06-18 07:10:26", "link": "http://arxiv.org/abs/1506.05561v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
