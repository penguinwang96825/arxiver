{"title": "Empirical Evaluation of RNN Architectures on Sentence Classification\n  Task", "abstract": "Recurrent Neural Networks have achieved state-of-the-art results for many\nproblems in NLP and two most popular RNN architectures are Tail Model and\nPooling Model. In this paper, a hybrid architecture is proposed and we present\nthe first empirical study using LSTMs to compare performance of the three RNN\nstructures on sentence classification task. Experimental results show that the\nMax Pooling Model or Hybrid Max Pooling Model achieves the best performance on\nmost datasets, while Tail Model does not outperform other models.", "published": "2016-09-29 01:53:08", "link": "http://arxiv.org/abs/1609.09171v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Sentence Representation with Guidance of Human Attention", "abstract": "Recently, much progress has been made in learning general-purpose sentence\nrepresentations that can be used across domains. However, most of the existing\nmodels typically treat each word in a sentence equally. In contrast, extensive\nstudies have proven that human read sentences efficiently by making a sequence\nof fixation and saccades. This motivates us to improve sentence representations\nby assigning different weights to the vectors of the component words, which can\nbe treated as an attention mechanism on single sentences. To that end, we\npropose two novel attention models, in which the attention weights are derived\nusing significant predictors of human reading time, i.e., Surprisal, POS tags\nand CCG supertags. The extensive experiments demonstrate that the proposed\nmethods significantly improve upon the state-of-the-art sentence representation\nmodels.", "published": "2016-09-29 03:26:53", "link": "http://arxiv.org/abs/1609.09189v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent\n  Neural Networks", "abstract": "This work focuses on the rapid development of linguistic annotation tools for\nresource-poor languages. We experiment several cross-lingual annotation\nprojection methods using Recurrent Neural Networks (RNN) models. The\ndistinctive feature of our approach is that our multilingual word\nrepresentation requires only a parallel corpus between the source and target\nlanguage. More precisely, our method has the following characteristics: (a) it\ndoes not use word alignment information, (b) it does not assume any knowledge\nabout foreign languages, which makes it applicable to a wide range of\nresource-poor languages, (c) it provides truly multilingual taggers. We\ninvestigate both uni- and bi-directional RNN models and propose a method to\ninclude external information (for instance low level information from POS) in\nthe RNN to train higher level taggers (for instance, super sense taggers). We\ndemonstrate the validity and genericity of our model by using parallel corpora\n(obtained by manual or automatic translation). Our experiments are conducted to\ninduce cross-lingual POS and super sense taggers.", "published": "2016-09-29 15:19:13", "link": "http://arxiv.org/abs/1609.09382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Dependency Parsers with Partial Annotation", "abstract": "Recently, these has been a surge on studying how to obtain partially\nannotated data for model supervision. However, there still lacks a systematic\nstudy on how to train statistical models with partial annotation (PA). Taking\ndependency parsing as our case study, this paper describes and compares two\nstraightforward approaches for three mainstream dependency parsers. The first\napproach is previously proposed to directly train a log-linear graph-based\nparser (LLGPar) with PA based on a forest-based objective. This work for the\nfirst time proposes the second approach to directly training a linear\ngraph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA\nbased on the idea of constrained decoding. We conduct extensive experiments on\nPenn Treebank under three different settings for simulating PA, i.e., random\ndependencies, most uncertain dependencies, and dependencies with divergent\noutputs from the three parsers. The results show that LLGPar is most effective\nin learning from PA and LTPar lags behind the graph-based counterparts by large\nmargin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar\nto complete PA into full annotation (FA).", "published": "2016-09-29 08:12:14", "link": "http://arxiv.org/abs/1609.09247v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing", "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a\nsemantic slot-filling task to explore how much syntactic supervision is\nrequired for downstream semantic analysis. This extrinsic, task-based\nevaluation provides a unique window to explore the strengths and weaknesses of\nsemantics captured by unsupervised grammar induction systems. We release a new\nFreebase semantic parsing dataset called SPADES (Semantic PArsing of\nDEclarative Sentences) containing 93K cloze-style questions paired with\nanswers. We evaluate all our models on this dataset. Our code and data are\navailable at https://github.com/sivareddyg/graph-parser.", "published": "2016-09-29 16:09:29", "link": "http://arxiv.org/abs/1609.09405v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic Browsing for Research Papers with Hierarchical Latent Tree\n  Analysis", "abstract": "Academic researchers often need to face with a large collection of research\npapers in the literature. This problem may be even worse for postgraduate\nstudents who are new to a field and may not know where to start. To address\nthis problem, we have developed an online catalog of research papers where the\npapers have been automatically categorized by a topic model. The catalog\ncontains 7719 papers from the proceedings of two artificial intelligence\nconferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet\nAllocation, we use a recently proposed method called hierarchical latent tree\nanalysis for topic modeling. The resulting topic model contains a hierarchy of\ntopics so that users can browse the topics from the top level to the bottom\nlevel. The topic model contains a manageable number of general topics at the\ntop level and allows thousands of fine-grained topics at the bottom level. It\nalso can detect topics that have emerged recently.", "published": "2016-09-29 03:22:01", "link": "http://arxiv.org/abs/1609.09188v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders", "abstract": "We present a novel semi-supervised approach for sequence transduction and\napply it to semantic parsing. The unsupervised component is based on a\ngenerative model in which latent sentences generate the unpaired logical forms.\nWe apply this method to a number of semantic parsing tasks focusing on domains\nwith limited access to labelled training data and extend those datasets with\nsynthetically generated logical forms.", "published": "2016-09-29 12:20:13", "link": "http://arxiv.org/abs/1609.09315v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
