{"title": "Hate speech, Censorship, and Freedom of Speech: The Changing Policies of\n  Reddit", "abstract": "This paper examines the shift in focus on content policies and user attitudes\non the social media platform Reddit. We do this by focusing on comments from\ngeneral Reddit users from five posts made by admins (moderators) on updates to\nReddit Content Policy. All five concern the nature of what kind of content is\nallowed to be posted on Reddit, and which measures will be taken against\ncontent that violates these policies. We use topic modeling to probe how the\ngeneral discourse for Redditors has changed around limitations on content, and\nlater, limitations on hate speech, or speech that incites violence against a\nparticular group. We show that there is a clear shift in both the contents and\nthe user attitudes that can be linked to contemporary societal upheaval as well\nas newly passed laws and regulations, and contribute to the wider discussion on\nhate speech moderation.", "published": "2022-03-18 00:46:58", "link": "http://arxiv.org/abs/2203.09673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEAM: Dialogue Coherence Evaluation using AMR-based Semantic\n  Manipulations", "abstract": "Automatic evaluation metrics are essential for the rapid development of\nopen-domain dialogue systems as they facilitate hyper-parameter tuning and\ncomparison between models. Although recently proposed trainable\nconversation-level metrics have shown encouraging results, the quality of the\nmetrics is strongly dependent on the quality of training data. Prior works\nmainly resort to heuristic text-level manipulations (e.g. utterances shuffling)\nto bootstrap incoherent conversations (negative examples) from coherent\ndialogues (positive examples). Such approaches are insufficient to\nappropriately reflect the incoherence that occurs in interactions between\nadvanced dialogue models and humans. To tackle this problem, we propose DEAM, a\nDialogue coherence Evaluation metric that relies on Abstract Meaning\nRepresentation (AMR) to apply semantic-level Manipulations for incoherent\n(negative) data generation. AMRs naturally facilitate the injection of various\ntypes of incoherence sources, such as coreference inconsistency, irrelevancy,\ncontradictions, and decrease engagement, at the semantic level, thus resulting\nin more natural incoherent samples. Our experiments show that DEAM achieves\nhigher correlations with human judgments compared to baseline methods on\nseveral dialog datasets by significant margins. We also show that DEAM can\ndistinguish between coherent and incoherent dialogues generated by baseline\nmanipulations, whereas those baseline models cannot detect incoherent examples\ngenerated by DEAM. Our results demonstrate the potential of AMR-based semantic\nmanipulations for natural negative example generation.", "published": "2022-03-18 03:11:35", "link": "http://arxiv.org/abs/2203.09711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRS: Combining Generation and Revision in Unsupervised Sentence\n  Simplification", "abstract": "We propose GRS: an unsupervised approach to sentence simplification that\ncombines text generation and text revision. We start with an iterative\nframework in which an input sentence is revised using explicit edit operations,\nand add paraphrasing as a new edit operation. This allows us to combine the\nadvantages of generative and revision-based approaches: paraphrasing captures\ncomplex edit operations, and the use of explicit edit operations in an\niterative manner provides controllability and interpretability. We demonstrate\nthese advantages of GRS compared to existing methods on the Newsela and ASSET\ndatasets.", "published": "2022-03-18 04:52:54", "link": "http://arxiv.org/abs/2203.09742v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias\n  in Speech Translation", "abstract": "Gender bias is largely recognized as a problematic phenomenon affecting\nlanguage technologies, with recent studies underscoring that it might surface\ndifferently across languages. However, most of current evaluation practices\nadopt a word-level focus on a narrow set of occupational nouns under synthetic\nconditions. Such protocols overlook key features of grammatical gender\nlanguages, which are characterized by morphosyntactic chains of gender\nagreement, marked on a variety of lexical items and parts-of-speech (POS). To\novercome this limitation, we enrich the natural, gender-sensitive MuST-SHE\ncorpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS\nand agreement chains), and explore to what extent different lexical categories\nand agreement phenomena are impacted by gender skews. Focusing on speech\ntranslation, we conduct a multifaceted evaluation on three language directions\n(English-French/Italian/Spanish), with models trained on varying amounts of\ndata and different word segmentation techniques. By shedding light on model\nbehaviours, gender bias, and its detection at several levels of granularity,\nour findings emphasize the value of dedicated analyses beyond aggregated\noverall results.", "published": "2022-03-18 11:14:16", "link": "http://arxiv.org/abs/2203.09866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCoT: Sense Clustering over Time: a tool for the analysis of lexical\n  change", "abstract": "We present Sense Clustering over Time (SCoT), a novel network-based tool for\nanalysing lexical change. SCoT represents the meanings of a word as clusters of\nsimilar words. It visualises their formation, change, and demise. There are two\nmain approaches to the exploration of dynamic networks: the discrete one\ncompares a series of clustered graphs from separate points in time. The\ncontinuous one analyses the changes of one dynamic network over a time-span.\nSCoT offers a new hybrid solution. First, it aggregates time-stamped documents\ninto intervals and calculates one sense graph per discrete interval. Then, it\nmerges the static graphs to a new type of dynamic semantic neighbourhood graph\nover time. The resulting sense clusters offer uniquely detailed insights into\nlexical change over continuous intervals with model transparency and\nprovenance. SCoT has been successfully used in a European study on the changing\nmeaning of `crisis'.", "published": "2022-03-18 12:04:09", "link": "http://arxiv.org/abs/2203.09892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Multilingual Language Models Capture Differing Moral Norms?", "abstract": "Massively multilingual sentence representations are trained on large corpora\nof uncurated data, with a very imbalanced proportion of languages included in\nthe training. This may cause the models to grasp cultural values including\nmoral judgments from the high-resource languages and impose them on the\nlow-resource languages. The lack of data in certain languages can also lead to\ndeveloping random and thus potentially harmful beliefs. Both these issues can\nnegatively influence zero-shot cross-lingual model transfer and potentially\nlead to harmful outcomes. Therefore, we aim to (1) detect and quantify these\nissues by comparing different models in different languages, (2) develop\nmethods for improving undesirable properties of the models. Our initial\nexperiments using the multilingual model XLM-R show that indeed multilingual\nLMs capture moral norms, even with potentially higher human-agreement than\nmonolingual ones. However, it is not yet clear to what extent these moral norms\ndiffer between languages.", "published": "2022-03-18 12:26:37", "link": "http://arxiv.org/abs/2203.09904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News Detection Using Majority Voting Technique", "abstract": "Due to the evolution of the Web and social network platforms it becomes very\neasy to disseminate the information. Peoples are creating and sharing more\ninformation than ever before, which may be misleading, misinformation or fake\ninformation. Fake news detection is a crucial and challenging task due to the\nunstructured nature of the available information. In the recent years,\nresearchers have provided significant solutions to tackle with the problem of\nfake news detection, but due to its nature there are still many open issues. In\nthis paper, we have proposed majority voting approach to detect fake news\narticles. We have used different textual properties of fake and real news. We\nhave used publicly available fake news dataset, comprising of 20,800 news\narticles among which 10,387 are real and 10,413 are fake news labeled as binary\n0 and 1. For the evaluation of our approach, we have used commonly used machine\nlearning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random\nForest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the\naforementioned classifiers, we built a multi-model fake news detection system\nusing Majority Voting technique to achieve the more accurate results. The\nexperimental results show that, our proposed approach achieved accuracy of\n96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation\nconfirms that, Majority Voting technique achieved more acceptable results as\ncompare to individual learning technique.", "published": "2022-03-18 13:24:03", "link": "http://arxiv.org/abs/2203.09936v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based Generative Approach towards Multi-Hierarchical Medical\n  Dialogue State Tracking", "abstract": "The medical dialogue system is a promising application that can provide great\nconvenience for patients. The dialogue state tracking (DST) module in the\nmedical dialogue system which interprets utterances into the machine-readable\nstructure for downstream tasks is particularly challenging. Firstly, the states\nneed to be able to represent compound entities such as symptoms with their body\npart or diseases with degrees of severity to provide enough information for\ndecision support. Secondly, these named entities in the utterance might be\ndiscontinuous and scattered across sentences and speakers. These also make it\ndifficult to annotate a large corpus which is essential for most methods.\nTherefore, we first define a multi-hierarchical state structure. We annotate\nand publish a medical dialogue dataset in Chinese. To the best of our\nknowledge, there are no publicly available ones before. Then we propose a\nPrompt-based Generative Approach which can generate slot values with\nmulti-hierarchies incrementally using a top-down approach. A dialogue style\nprompt is also supplemented to utilize the large unlabeled dialogue corpus to\nalleviate the data scarcity problem. The experiments show that our approach\noutperforms other DST methods and is rather effective in the scenario with\nlittle data.", "published": "2022-03-18 13:28:27", "link": "http://arxiv.org/abs/2203.09946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented\n  Cross-lingual Natural Language Understanding", "abstract": "Task-oriented personal assistants enable people to interact with a host of\ndevices and services using natural language. One of the challenges of making\nneural dialogue systems available to more users is the lack of training data\nfor all but a few languages. Zero-shot methods try to solve this issue by\nacquiring task knowledge in a high-resource language such as English with the\naim of transferring it to the low-resource language(s). To this end, we\nintroduce CrossAligner, the principal method of a variety of effective\napproaches for zero-shot cross-lingual transfer based on learning alignment\nfrom unlabelled parallel data. We present a quantitative analysis of individual\nmethods as well as their weighted combinations, several of which exceed\nstate-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test\nsets and three benchmark multilingual datasets. A detailed qualitative error\nanalysis of the best methods shows that our fine-tuned language models can\nzero-shot transfer the task knowledge better than anticipated.", "published": "2022-03-18 14:18:12", "link": "http://arxiv.org/abs/2203.09982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CaMEL: Case Marker Extraction without Labels", "abstract": "We introduce CaMEL (Case Marker Extraction without Labels), a novel and\nchallenging task in computational morphology that is especially relevant for\nlow-resource languages. We propose a first model for CaMEL that uses a\nmassively multilingual corpus to extract case markers in 83 languages based\nonly on a noun phrase chunker and an alignment system. To evaluate CaMEL, we\nautomatically construct a silver standard from UniMorph. The case markers\nextracted by our model can be used to detect and visualise similarities and\ndifferences between the case systems of different languages as well as to\nannotate fine-grained deep cases in languages in which they are not overtly\nmarked.", "published": "2022-03-18 15:20:14", "link": "http://arxiv.org/abs/2203.10010v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges and Strategies in Cross-Cultural NLP", "abstract": "Various efforts in the Natural Language Processing (NLP) community have been\nmade to accommodate linguistic diversity and serve speakers of many different\nlanguages. However, it is important to acknowledge that speakers and the\ncontent they produce and require, vary not just by language, but also by\nculture. Although language and culture are tightly linked, there are important\ndifferences. Analogous to cross-lingual and multilingual NLP, cross-cultural\nand multicultural NLP considers these differences in order to better serve\nusers of NLP systems. We propose a principled framework to frame these efforts,\nand survey existing and potential strategies.", "published": "2022-03-18 15:36:57", "link": "http://arxiv.org/abs/2203.10020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RELIC: Retrieving Evidence for Literary Claims", "abstract": "Humanities scholars commonly provide evidence for claims that they make about\na work of literature (e.g., a novel) in the form of quotations from the work.\nWe collect a large-scale dataset (RELiC) of 78K literary quotations and\nsurrounding critical analysis and use it to formulate the novel task of\nliterary evidence retrieval, in which models are given an excerpt of literary\nanalysis surrounding a masked quotation and asked to retrieve the quoted\npassage from the set of all passages in the work. Solving this retrieval task\nrequires a deep understanding of complex literary and linguistic phenomena,\nwhich proves challenging to methods that overwhelmingly rely on lexical and\nsemantic similarity matching. We implement a RoBERTa-based dense passage\nretriever for this task that outperforms existing pretrained information\nretrieval baselines; however, experiments and analysis by human domain experts\nindicate that there is substantial room for improvement over our dense\nretriever.", "published": "2022-03-18 16:56:08", "link": "http://arxiv.org/abs/2203.10053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating Bandit Learning from User Feedback for Extractive Question\n  Answering", "abstract": "We study learning from user feedback for extractive question answering by\nsimulating feedback using supervised data. We cast the problem as contextual\nbandit learning, and analyze the characteristics of several learning scenarios\nwith focus on reducing data annotation. We show that systems initially trained\non a small number of examples can dramatically improve given feedback from\nusers on model-predicted answers, and that one can use existing datasets to\ndeploy systems in new domains without any annotation, but instead improving the\nsystem on-the-fly via user feedback.", "published": "2022-03-18 17:47:58", "link": "http://arxiv.org/abs/2203.10079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Coreference Resolution for Contentious Politics Events", "abstract": "We propose a dataset for event coreference resolution, which is based on\nrandom samples drawn from multiple sources, languages, and countries. Early\nscholarship on event information collection has not quantified the contribution\nof event coreference resolution. We prepared and analyzed a representative\nmultilingual corpus and measured the performance and contribution of the\nstate-of-the-art event coreference resolution approaches. We found that almost\nhalf of the event mentions in documents co-occur with other event mentions and\nthis makes it inevitable to obtain erroneous or partial event information. We\nshowed that event coreference resolution could help improving this situation.\nOur contribution sheds light on a challenge that has been overlooked or hard to\nstudy to date. Future event information collection studies can be designed\nbased on the results we present in this report. The repository for this study\nis on https://github.com/emerging-welfare/ECR4-Contentious-Politics.", "published": "2022-03-18 18:50:45", "link": "http://arxiv.org/abs/2203.10123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive\n  Weakly-Supervised Learning", "abstract": "Weakly-supervised learning (WSL) has shown promising results in addressing\nlabel scarcity on many NLP tasks, but manually designing a comprehensive,\nhigh-quality labeling rule set is tedious and difficult. We study interactive\nweakly-supervised learning -- the problem of iteratively and automatically\ndiscovering novel labeling rules from data to improve the WSL model. Our\nproposed model, named PRBoost, achieves this goal via iterative prompt-based\nrule discovery and model boosting. It uses boosting to identify large-error\ninstances and then discovers candidate rules from them by prompting pre-trained\nLMs with rule templates. The candidate rules are judged by human experts, and\nthe accepted rules are used to generate complementary weak labels and\nstrengthen the current model. Experiments on four tasks show PRBoost\noutperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with\nfully supervised models. Our Implementation is available at\n\\url{https://github.com/rz-zhang/PRBoost}.", "published": "2022-03-18 04:23:20", "link": "http://arxiv.org/abs/2203.09735v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning", "abstract": "Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.", "published": "2022-03-18 07:07:56", "link": "http://arxiv.org/abs/2203.09770v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BIOS: An Algorithmically Generated Biomedical Knowledge Graph", "abstract": "Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for\nbiomedical and healthcare big data and artificial intelligence (AI),\nfacilitating natural language processing, model development, and data exchange.\nFor decades, these knowledge graphs have been developed via expert curation;\nhowever, this method can no longer keep up with today's AI development, and a\ntransition to algorithmically generated BioMedKGs is necessary. In this work,\nwe introduce the Biomedical Informatics Ontology System (BIOS), the first\nlarge-scale publicly available BioMedKG generated completely by machine\nlearning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million\nterms in two languages, and 7.3 million relation triplets. We present the\nmethodology for developing BIOS, including the curation of raw biomedical\nterms, computational identification of synonymous terms and aggregation of\nthese terms to create concept nodes, semantic type classification of the\nconcepts, relation identification, and biomedical machine translation. We\nprovide statistics on the current BIOS content and perform preliminary\nassessments of term quality, synonym grouping, and relation extraction. The\nresults suggest that machine learning-based BioMedKG development is a viable\nalternative to traditional expert curation.", "published": "2022-03-18 14:09:22", "link": "http://arxiv.org/abs/2203.09975v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-Text Multi-Modal Pre-training for Medical Representation Learning", "abstract": "As the volume of Electronic Health Records (EHR) sharply grows, there has\nbeen emerging interest in learning the representation of EHR for healthcare\napplications. Representation learning of EHR requires appropriate modeling of\nthe two dominant modalities in EHR: structured data and unstructured text. In\nthis paper, we present MedGTX, a pre-trained model for multi-modal\nrepresentation learning of the structured and textual EHR data. MedGTX uses a\nnovel graph encoder to exploit the graphical nature of structured EHR data, and\na text encoder to handle unstructured text, and a cross-modal encoder to learn\na joint representation space. We pre-train our model through four proxy tasks\non MIMIC-III, an open-source EHR data, and evaluate our model on two clinical\nbenchmarks and three novel downstream tasks which tackle real-world problems in\nEHR data. The results consistently show the effectiveness of pre-training the\nmodel for joint representation of both structured and unstructured information\nfrom EHR. Given the promising performance of MedGTX, we believe this work opens\na new door to jointly understanding the two fundamental modalities of EHR data.", "published": "2022-03-18 14:45:42", "link": "http://arxiv.org/abs/2203.09994v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offensive Language Detection in Under-resourced Algerian Dialectal\n  Arabic Language", "abstract": "This paper addresses the problem of detecting the offensive and abusive\ncontent in Facebook comments, where we focus on the Algerian dialectal Arabic\nwhich is one of under-resourced languages. The latter has a variety of dialects\nmixed with different languages (i.e. Berber, French and English). In addition,\nwe deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due\nto the scarcity of works on the same language, we have built a new corpus\nregrouping more than 8.7k texts manually annotated as normal, abusive and\noffensive. We have conducted a series of experiments using the state-of-the-art\nclassifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.\nThe results showed acceptable performances, but the problem requires further\ninvestigation on linguistic features to increase the identification accuracy.", "published": "2022-03-18 15:42:21", "link": "http://arxiv.org/abs/2203.10024v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Probing Factually Grounded Content Transfer with Factual Ablation", "abstract": "Despite recent success, large neural models often generate factually\nincorrect text. Compounding this is the lack of a standard automatic evaluation\nfor factuality--it cannot be meaningfully improved if it cannot be measured.\nGrounded generation promises a path to solving both of these problems: models\ndraw on a reliable external document (grounding) for factual information,\nsimplifying the challenge of factuality. Measuring factuality is also\nsimplified--to factual consistency, testing whether the generation agrees with\nthe grounding, rather than all facts. Yet, without a standard automatic metric\nfor factual consistency, factually grounded generation remains an open problem.\n  We study this problem for content transfer, in which generations extend a\nprompt, using information from factual grounding. Particularly, this domain\nallows us to introduce the notion of factual ablation for automatically\nmeasuring factual consistency: this captures the intuition that the model\nshould be less likely to produce an output given a less relevant grounding\ndocument. In practice, we measure this by presenting a model with two grounding\ndocuments, and the model should prefer to use the more factually relevant one.\nWe contribute two evaluation sets to measure this. Applying our new evaluation,\nwe propose multiple novel methods improving over strong baselines.", "published": "2022-03-18 19:18:54", "link": "http://arxiv.org/abs/2203.10133v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Intensification for Sign Language Generation: A Computational\n  Approach", "abstract": "End-to-end sign language generation models do not accurately represent the\nprosody in sign language. A lack of temporal and spatial variations leads to\npoor-quality generated presentations that confuse human interpreters. In this\npaper, we aim to improve the prosody in generated sign languages by modeling\nintensification in a data-driven manner. We present different strategies\ngrounded in linguistics of sign language that inform how intensity modifiers\ncan be represented in gloss annotations. To employ our strategies, we first\nannotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,\nwith different levels of intensification. We then use a supervised intensity\ntagger to extend the annotated dataset and obtain labels for the remaining\nportion of it. This enhanced dataset is then used to train state-of-the-art\ntransformer models for sign language generation. We find that our efforts in\nintensification modeling yield better results when evaluated with automatic\nmetrics. Human evaluation also indicates a higher preference of the videos\ngenerated using our model.", "published": "2022-03-18 01:13:21", "link": "http://arxiv.org/abs/2203.09679v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech\n  Synthesis and Editing", "abstract": "Recently, speech representation learning has improved many speech-related\ntasks such as speech recognition, speech classification, and speech-to-text\ntranslation. However, all the above tasks are in the direction of speech\nunderstanding, but for the inverse direction, speech synthesis, the potential\nof representation learning is yet to be realized, due to the challenging nature\nof generating high-quality speech. To address this problem, we propose our\nframework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which\nreconstructs masked acoustic signals with text input and acoustic-text\nalignment during training. In this way, the pretrained model can generate high\nquality reconstructed spectrogram, which can be applied to the speech editing\nand unseen speaker TTS directly. Experiments show A$^3$T outperforms SOTA\nmodels on speech editing, and improves multi-speaker speech synthesis without\nthe external speaker verification model.", "published": "2022-03-18 01:36:25", "link": "http://arxiv.org/abs/2203.09690v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improve few-shot voice cloning using multi-modal learning", "abstract": "Recently, few-shot voice cloning has achieved a significant improvement.\nHowever, most models for few-shot voice cloning are single-modal, and\nmulti-modal few-shot voice cloning has been understudied. In this paper, we\npropose to use multi-modal learning to improve the few-shot voice cloning\nperformance. Inspired by the recent works on unsupervised speech\nrepresentation, the proposed multi-modal system is built by extending Tacotron2\nwith an unsupervised speech representation module. We evaluate our proposed\nsystem in two few-shot voice cloning scenarios, namely few-shot\ntext-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate\nthat the proposed multi-modal learning can significantly improve the few-shot\nvoice cloning performance over their counterpart single-modal systems.", "published": "2022-03-18 02:57:32", "link": "http://arxiv.org/abs/2203.09708v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Are You Robert or RoBERTa? Deceiving Online Authorship Attribution\n  Models Using Neural Text Generators", "abstract": "Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.", "published": "2022-03-18 09:19:14", "link": "http://arxiv.org/abs/2203.09813v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaVocoder: Adaptive Vocoder for Custom Voice", "abstract": "Custom voice is to construct a personal speech synthesis system by adapting\nthe source speech synthesis model to the target model through the target few\nrecordings. The solution to constructing a custom voice is to combine an\nadaptive acoustic model with a robust vocoder. However, training a robust\nvocoder usually requires a multi-speaker dataset, which should include various\nage groups and various timbres, so that the trained vocoder can be used for\nunseen speakers. Collecting such a multi-speaker dataset is difficult, and the\ndataset distribution always has a mismatch with the distribution of the target\nspeaker dataset. This paper proposes an adaptive vocoder for custom voice from\nanother novel perspective to solve the above problems. The adaptive vocoder\nmainly uses a cross-domain consistency loss to solve the overfitting problem\nencountered by the GAN-based neural vocoder in the transfer learning of\nfew-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.\nFirst, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,\nrespectively. Then, fine-tune it on the internal dataset VXI-children with few\nadaptation data. The empirical results show that a high-quality custom voice\nsystem can be built by combining a adaptive acoustic model with a adaptive\nvocoder.", "published": "2022-03-18 10:03:37", "link": "http://arxiv.org/abs/2203.09825v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Lithuanian grammatical error correction", "abstract": "Everyone wants to write beautiful and correct text, yet the lack of language\nskills, experience, or hasty typing can result in errors. By employing the\nrecent advances in transformer architectures, we construct a grammatical error\ncorrection model for Lithuanian, the language rich in archaic features. We\ncompare subword and byte-level approaches and share our best trained model,\nachieving F$_{0.5}$=0.92, and accompanying code, in an online open-source\nrepository.", "published": "2022-03-18 13:59:02", "link": "http://arxiv.org/abs/2203.09963v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "68T07, 68T50, 68T05", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "FORCE: A Framework of Rule-Based Conversational Recommender System", "abstract": "The conversational recommender systems (CRSs) have received extensive\nattention in recent years. However, most of the existing works focus on various\ndeep learning models, which are largely limited by the requirement of\nlarge-scale human-annotated datasets. Such methods are not able to deal with\nthe cold-start scenarios in industrial products. To alleviate the problem, we\npropose FORCE, a Framework Of Rule-based Conversational Recommender system that\nhelps developers to quickly build CRS bots by simple configuration. We conduct\nexperiments on two datasets in different languages and domains to verify its\neffectiveness and usability.", "published": "2022-03-18 15:01:32", "link": "http://arxiv.org/abs/2203.10001v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Report from the NSF Future Directions Workshop on Automatic Evaluation\n  of Dialog: Research Directions and Challenges", "abstract": "This is a report on the NSF Future Directions Workshop on Automatic\nEvaluation of Dialog. The workshop explored the current state of the art along\nwith its limitations and suggested promising directions for future work in this\nimportant and very rapidly changing area of research.", "published": "2022-03-18 15:21:11", "link": "http://arxiv.org/abs/2203.10012v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Consonant-Vowel Transition Models Based on Deep Learning for Objective\n  Evaluation of Articulation", "abstract": "Spectro-temporal dynamics of consonant-vowel (CV) transition regions are\nconsidered to provide robust cues related to articulation. In this work, we\npropose an objective measure of precise articulation, dubbed the objective\narticulation measure (OAM), by analyzing the CV transitions segmented around\nvowel onsets. The OAM is derived based on the posteriors of a convolutional\nneural network pre-trained to classify between different consonants using CV\nregions as input. We demonstrate the OAM is correlated with perceptual measures\nin a variety of contexts including (a) adult dysarthric speech, (b) the speech\nof children with cleft lip/palate, and (c) a database of accented English\nspeech from native Mandarin and Spanish speakers.", "published": "2022-03-18 16:58:26", "link": "http://arxiv.org/abs/2203.10054v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DGC-vector: A new speaker embedding for zero-shot voice conversion", "abstract": "Recently, more and more zero-shot voice conversion algorithms have been\nproposed. As a fundamental part of zero-shot voice conversion, speaker\nembeddings are the key to improving the converted speech's speaker similarity.\nIn this paper, we study the impact of speaker embeddings on zero-shot voice\nconversion performance. To better represent the characteristics of the target\nspeaker and improve the speaker similarity in zero-shot voice conversion, we\npropose a novel speaker representation method in this paper. Our method\ncombines the advantages of D-vector, global style token (GST) based speaker\nrepresentation and auxiliary supervision. Objective and subjective evaluations\nshow that the proposed method achieves a decent performance on zero-shot voice\nconversion and significantly improves speaker similarity over D-vector and\nGST-based speaker embedding.", "published": "2022-03-18 03:38:34", "link": "http://arxiv.org/abs/2203.09722v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Estimation of Consistent Time Delays in Subsample via\n  Auxiliary-Function-Based Iterative Updates", "abstract": "In this paper, we propose a new algorithm for the estimation of multiple time\ndelays (TDs). Since a TD is a fundamental spatial cue for sensor array signal\nprocessing techniques, many methods for estimating it have been studied. Most\nof them, including generalized cross correlation (CC)-based methods, focus on\nhow to estimate a TD between two sensors. These methods can then be easily\nadapted for multiple TDs by applying them to every pair of a reference sensor\nand another one. However, these pairwise methods can use only the partial\ninformation obtained by the selected sensors, resulting in inconsistent TD\nestimates and limited estimation accuracy. In contrast, we propose joint\noptimization of entire TD parameters, where spatial information obtained from\nall sensors is taken into account. We also introduce a consistent constraint\nregarding TD parameters to the observation model. We then consider a\nmultidimensional CC (MCC) as the objective function, which is derived on the\nbasis of maximum likelihood estimation. To maximize the MCC, which is a\nnonconvex function, we derive the auxiliary function for the MCC and design\nefficient update rules. We additionally estimate the amplitudes of the transfer\nfunctions for supporting the TD estimation, where we maximize the Rayleigh\nquotient under the non-negative constraint. We experimentally analyze essential\nfeatures of the proposed method and evaluate its effectiveness in TD\nestimation. Code will be available at https://github.com/onolab-tmu/AuxTDE.", "published": "2022-03-18 03:38:35", "link": "http://arxiv.org/abs/2203.09723v2", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Personalized Filled-pause Generation with Group-wise Prediction Models", "abstract": "In this paper, we propose a method to generate personalized filled pauses\n(FPs) with group-wise prediction models. Compared with fluent text generation,\ndisfluent text generation has not been widely explored. To generate more\nhuman-like texts, we addressed disfluent text generation. The usage of\ndisfluency, such as FPs, rephrases, and word fragments, differs from speaker to\nspeaker, and thus, the generation of personalized FPs is required. However, it\nis difficult to predict them because of the sparsity of position and the\nfrequency difference between more and less frequently used FPs. Moreover, it is\nsometimes difficult to adapt FP prediction models to each speaker because of\nthe large variation of the tendency within each speaker. To address these\nissues, we propose a method to build group-dependent prediction models by\ngrouping speakers on the basis of their tendency to use FPs. This method does\nnot require a large amount of data and time to train each speaker model. We\nfurther introduce a loss function and a word embedding model suitable for FP\nprediction. Our experimental results demonstrate that group-dependent models\ncan predict FPs with higher scores than a non-personalized one and the\nintroduced loss function and word embedding model improve the prediction\nperformance.", "published": "2022-03-18 13:55:32", "link": "http://arxiv.org/abs/2203.09961v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soft Smoothness for Audio Inpainting Using a Latent Matrix Model in\n  Delay-embedded Space", "abstract": "Here, we propose a new reconstruction method of smooth time-series signals. A\nkey concept of this study is not considering the model in signal space, but in\ndelay-embedded space. In other words, we indirectly represent a time-series\nsignal as an output of inverse delay-embedding of a matrix, and the matrix is\nconstrained. Based on the model under inverse delay-embedding, we propose to\nconstrain the matrix to be rank-1 with smooth factor vectors. The proposed\nmodel is closely related to the convolutional model, and quadratic variation\n(QV) regularization. Especially, the proposed method can be characterized as a\ngeneralization of QV regularization. In addition, we show that the proposed\nmethod provides the softer smoothness than QV regularization. Experiments of\naudio inpainting and declipping are conducted to show its advantages in\ncomparison with several existing interpolation methods and sparse modeling.", "published": "2022-03-18 04:58:18", "link": "http://arxiv.org/abs/2203.09746v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speaker Embedding-aware Neural Diarization: an Efficient Framework for\n  Overlapping Speech Diarization in Meeting Scenarios", "abstract": "Overlapping speech diarization has been traditionally treated as a\nmulti-label classification problem. In this paper, we reformulate this task as\na single-label prediction problem by encoding multiple binary labels into a\nsingle label with the power set, which represents the possible combinations of\ntarget speakers. This formulation has two benefits. First, the overlaps of\ntarget speakers are explicitly modeled. Second, threshold selection is no\nlonger needed. Through this formulation, we propose the speaker embedding-aware\nneural diarization (SEND) framework, where a speech encoder, a speaker encoder,\ntwo similarity scorers, and a post-processing network are jointly optimized to\npredict the encoded labels according to the similarities between speech\nfeatures and speaker embeddings. Experimental results show that SEND has a\nstable learning process and can be trained on highly overlapped data without\nextra initialization. More importantly, our method achieves the\nstate-of-the-art performance in real meeting scenarios with fewer model\nparameters and lower computational complexity.", "published": "2022-03-18 06:40:39", "link": "http://arxiv.org/abs/2203.09767v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?", "abstract": "This work digs into a root question in human perception: can face geometry be\ngleaned from one's voices? Previous works that study this question only adopt\ndevelopments in image synthesis and convert voices into face images to show\ncorrelations, but working on the image domain unavoidably involves predicting\nattributes that voices cannot hint, including facial textures, hairstyles, and\nbackgrounds. We instead investigate the ability to reconstruct 3D faces to\nconcentrate on only geometry, which is much more physiologically grounded. We\npropose our analysis framework, Cross-Modal Perceptionist, under both\nsupervised and unsupervised learning. First, we construct a dataset,\nVoxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,\nmaking supervised learning possible. Second, we use a knowledge distillation\nmechanism to study whether face geometry can still be gleaned from voices\nwithout paired voices and 3D face data under limited availability of 3D face\nscans. We break down the core question into four parts and perform visual and\nnumerical analyses as responses to the core question. Our findings echo those\nin physiology and neuroscience about the correlation between voices and facial\nstructures. The work provides future human-centric cross-modal learning with\nexplainable foundations. See our project page:\nhttps://choyingw.github.io/works/Voice2Mesh/index.html", "published": "2022-03-18 10:03:07", "link": "http://arxiv.org/abs/2203.09824v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Representative Subset Selection for Efficient Fine-Tuning in\n  Self-Supervised Speech Recognition", "abstract": "Self-supervised speech recognition models require considerable labeled\ntraining data for learning high-fidelity representations for Automatic Speech\nRecognition (ASR) which is computationally demanding and time-consuming. We\nconsider the task of identifying an optimal subset of data for efficient\nfine-tuning in self-supervised speech models for ASR. We discover that the\ndataset pruning strategies used in vision tasks for sampling the most\ninformative examples do not perform better than random subset selection on\nfine-tuning self-supervised ASR. We then present the COWERAGE algorithm for\nrepresentative subset selection in self-supervised ASR. COWERAGE is based on\nour finding that ensuring the coverage of examples based on training Word Error\nRate (WER) in the early training epochs leads to better generalization\nperformance. Extensive experiments with the wav2vec 2.0 and HuBERT model on\nTIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGE\nand its transferability across models, with up to 17% relative WER improvement\nover existing dataset pruning methods and random sampling. We also demonstrate\nthat the coverage of training instances in terms of WER values ensures the\ninclusion of phonemically diverse examples, leading to better test accuracy in\nself-supervised speech recognition models.", "published": "2022-03-18 10:12:24", "link": "http://arxiv.org/abs/2203.09829v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition", "abstract": "Recent works have revealed the vulnerability of automatic speech recognition\n(ASR) models to adversarial examples (AEs), i.e., small perturbations that\ncause an error in the transcription of the audio signal. Studying audio\nadversarial attacks is therefore the first step towards robust ASR. Despite the\nsignificant progress made in attacking audio examples, the black-box attack\nremains challenging because only the hard-label information of transcriptions\nis provided. Due to this limited information, existing black-box methods often\nrequire an excessive number of queries to attack a single audio example. In\nthis paper, we introduce NP-Attack, a neural predictor-based method, which\nprogressively evolves the search towards a small adversarial perturbation.\nGiven a perturbation direction, our neural predictor directly estimates the\nsmallest perturbation that causes a mistranscription. In particular, it enables\nNP-Attack to accurately learn promising perturbation directions via\ngradient-based optimization. Experimental results show that NP-Attack achieves\ncompetitive results with other state-of-the-art black-box adversarial attacks\nwhile requiring a significantly smaller number of queries. The code of\nNP-Attack is available online.", "published": "2022-03-18 10:37:20", "link": "http://arxiv.org/abs/2203.09849v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic analysis of Categorical Verbal Fluency for Mild Cognitive\n  Impartment detection: a non-linear language independent approach", "abstract": "Alzheimer's disease (AD) is one the main causes of dementia in the world and\nthe patients develop severe disability and sometime full dependence. In\nprevious stages Mild Cognitive Impairment (MCI) produces cognitive loss but not\nsevere enough to interfere with daily life. This work, on selection of\nbiomarkers from speech for the detection of AD, is part of a wide-ranging cross\nstudy for the diagnosis of Alzheimer. Specifically in this work a task for\ndetection of MCI has been used. The task analyzes Categorical Verbal Fluency.\nThe automatic classification is carried out by SVM over classical linear\nfeatures, Castiglioni fractal dimension and Permutation Entropy. Finally the\nmost relevant features are selected by ANOVA test. The promising results are\nover 50% for MCI", "published": "2022-03-18 11:40:15", "link": "http://arxiv.org/abs/2203.09878v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Identification of Hypokinetic Dysarthria Using Acoustic Analysis of Poem\n  Recitation", "abstract": "Up to 90 % of patients with Parkinson's disease (PD) suffer from hypokinetic\ndysarthria (HD). In this work, we analysed the power of conventional speech\nfeatures quantifying imprecise articulation, dysprosody, speech dysfluency and\nspeech quality deterioration extracted from a specialized poem recitation task\nto discriminate dysarthric and healthy speech. For this purpose, 152 speakers\n(53 healthy speakers, 99 PD patients) were examined. Only mildly strong\ncorrelation between speech features and clinical status of the speakers was\nobserved. In the case of univariate classification analysis, sensitivity of\n62.63% (imprecise articulation), 61.62% (dysprosody), 71.72% (speech\ndysfluency) and 59.60% (speech quality deterioration) was achieved.\nMultivariate classification analysis improved the classification performance.\nSensitivity of 83.42% using only two features describing imprecise articulation\nand speech quality deterioration in HD was achieved. We showed the promising\npotential of the selected speech features and especially the use of poem\nrecitation task to quantify and identify HD in PD.", "published": "2022-03-18 11:45:01", "link": "http://arxiv.org/abs/2203.09880v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "A Lightweight Instrument-Agnostic Model for Polyphonic Note\n  Transcription and Multipitch Estimation", "abstract": "Automatic Music Transcription (AMT) has been recognized as a key enabling\ntechnology with a wide range of applications. Given the task's complexity, best\nresults have typically been reported for systems focusing on specific settings,\ne.g. instrument-specific systems tend to yield improved results over\ninstrument-agnostic methods. Similarly, higher accuracy can be obtained when\nonly estimating frame-wise $f_0$ values and neglecting the harder note event\ndetection. Despite their high accuracy, such specialized systems often cannot\nbe deployed in the real-world. Storage and network constraints prohibit the use\nof multiple specialized models, while memory and run-time constraints limit\ntheir complexity. In this paper, we propose a lightweight neural network for\nmusical instrument transcription, which supports polyphonic outputs and\ngeneralizes to a wide variety of instruments (including vocals). Our model is\ntrained to jointly predict frame-wise onsets, multipitch and note activations,\nand we experimentally show that this multi-output structure improves the\nresulting frame-level note accuracy. Despite its simplicity, benchmark results\nshow our system's note estimation to be substantially better than a comparable\nbaseline, and its frame-level accuracy to be only marginally below those of\nspecialized state-of-the-art AMT systems. With this work we hope to encourage\nthe community to further investigate low-resource, instrument-agnostic AMT\nsystems.", "published": "2022-03-18 12:07:36", "link": "http://arxiv.org/abs/2203.09893v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RoSS: Utilizing Robotic Rotation for Audio Source Separation", "abstract": "This paper considers the problem of audio source separation where the goal is\nto isolate a target audio signal (say Alice's speech) from a mixture of\nmultiple interfering signals (e.g., when many people are talking). This problem\nhas gained renewed interest mainly due to the significant growth in voice\ncontrolled devices, including robots in homes, offices, and other public\nfacilities. Although a rich body of work exists on the core topic of source\nseparation, we find that robotic motion of the microphone -- say the robot's\nhead -- is a complementary opportunity to past approaches. Briefly, we show\nthat rotating the microphone array to the correct orientation can produce\ndesired aliasing between two interferers, causing the two interferers to pose\nas one. In other words, a mixture of K signals becomes a mixture of (K-1), a\nmathematically concrete gain. We show that the gain translates well to practice\nprovided two mobility-related challenges can be mitigated. This paper is\nfocused on mitigating these challenges and demonstrating the end-to-end\nperformance on a fully functional prototype. We believe that our Rotational\nSource Separation module RoSS could be plugged into actual robot heads, or into\nother devices (like Amazon Show) that are also capable of rotation.", "published": "2022-03-18 17:38:21", "link": "http://arxiv.org/abs/2203.10072v1", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the role of Lip Articulation in Visual Speech Perception", "abstract": "Generating realistic lip motion from audio to simulate speech production is\ncritical for driving natural character animation. Previous research has shown\nthat traditional metrics used to optimize and assess models for generating lip\nmotion from speech are not a good indicator of subjective opinion of animation\nquality. Devising metrics that align with subjective opinion first requires\nunderstanding what impacts human perception of quality. In this work, we focus\non the degree of articulation and run a series of experiments to study how\narticulation strength impacts human perception of lip motion accompanying\nspeech. Specifically, we study how increasing under-articulated (dampened) and\nover-articulated (exaggerated) lip motion affects human perception of quality.\nWe examine the impact of articulation strength on human perception when\nconsidering only lip motion, where viewers are presented with talking faces\nrepresented by landmarks, and in the context of embodied characters, where\nviewers are presented with photo-realistic videos. Our results show that\nviewers prefer over-articulated lip motion consistently more than\nunder-articulated lip motion and that this preference generalizes across\ndifferent speakers and embodiments.", "published": "2022-03-18 18:25:08", "link": "http://arxiv.org/abs/2203.10117v4", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
