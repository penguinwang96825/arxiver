{"title": "Improving Information Extraction by Acquiring External Evidence with\n  Reinforcement Learning", "abstract": "Most successful information extraction systems operate with access to a large\ncollection of documents. In this work, we explore the task of acquiring and\nincorporating external evidence to improve extraction accuracy in domains where\nthe amount of training data is scarce. This process entails issuing search\nqueries, extraction from new sources and reconciliation of extracted values,\nwhich are repeated until sufficient evidence is collected. We approach the\nproblem using a reinforcement learning framework where our model learns to\nselect optimal actions based on contextual information. We employ a deep\nQ-network, trained to optimize a reward function that reflects extraction\naccuracy while penalizing extra effort. Our experiments on two databases -- of\nshooting incidents, and food adulteration cases -- demonstrate that our system\nsignificantly outperforms traditional extractors and a competitive\nmeta-classifier baseline.", "published": "2016-03-25 16:38:54", "link": "http://arxiv.org/abs/1603.07954v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Syntactic Regularities for Hundreds of Languages", "abstract": "This paper presents a comparison of classification methods for linguistic\ntypology for the purpose of expanding an extensive, but sparse language\nresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,\n2013). We experimented with a variety of regression and nearest-neighbor\nmethods for use in classification over a set of 325 languages and six syntactic\nrules drawn from WALS. To classify each rule, we consider the typological\nfeatures of the other five rules; linguistic features extracted from a\nword-aligned Bible in each language; and genealogical features (genus and\nfamily) of each language. In general, we find that propagating the majority\nlabel among all languages of the same genus achieves the best accuracy in label\npre- diction. Following this, a logistic regression model that combines\ntypological and linguistic features offers the next best performance.\nInterestingly, this model actually outperforms the majority labels among all\nlanguages of the same family.", "published": "2016-03-25 20:09:29", "link": "http://arxiv.org/abs/1603.08016v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "abstract": "We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.", "published": "2016-03-25 20:32:21", "link": "http://arxiv.org/abs/1603.08023v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "On the Compression of Recurrent Neural Networks with an Application to\n  LVCSR acoustic modeling for Embedded Speech Recognition", "abstract": "We study the problem of compressing recurrent neural networks (RNNs). In\nparticular, we focus on the compression of RNN acoustic models, which are\nmotivated by the goal of building compact and accurate speech recognition\nsystems which can be run efficiently on mobile devices. In this work, we\npresent a technique for general recurrent model compression that jointly\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\nfind that the proposed technique allows us to reduce the size of our Long\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\nnegligible loss in accuracy.", "published": "2016-03-25 21:43:28", "link": "http://arxiv.org/abs/1603.08042v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "\"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles\n  for Deletion Discussions", "abstract": "This thesis focuses on gaining linguistic insights into textual discussions\non a word level. It was of special interest to distinguish messages that\nconstructively contribute to a discussion from those that are detrimental to\nthem. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are\nindicators for either of the two discussion styles. These messages are nowadays\noften used in guidelines for successful communication. Although their effects\nhave been successfully evaluated multiple times, a large-scale analysis has\nnever been conducted.\n  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions\ntogether with the records of blocked users and developed a fully automated\ncreation of an annotated data set. In this data set, messages were labelled\neither constructive or disruptive. We applied binary classifiers to the data to\ndetermine characteristic words for both discussion styles. Thereby, we also\ninvestigated whether function words like pronouns and conjunctions play an\nimportant role in distinguishing the two.\n  We found that \"You\"-messages were a strong indicator for disruptive messages\nwhich matches their attributed effects on communication. However, we found\n\"I\"-messages to be indicative for disruptive messages as well which is contrary\nto their attributed effects. The importance of function words could neither be\nconfirmed nor refuted. Other characteristic words for either communication\nstyle were not found. Yet, the results suggest that a different model might\nrepresent disruptive and constructive messages in textual discussions better.", "published": "2016-03-25 22:36:40", "link": "http://arxiv.org/abs/1603.08048v1", "categories": ["cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
