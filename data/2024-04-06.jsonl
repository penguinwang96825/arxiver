{"title": "KazQAD: Kazakh Open-Domain Question Answering Dataset", "abstract": "We introduce KazQAD -- a Kazakh open-domain question answering (ODQA) dataset\n-- that can be used in both reading comprehension and full ODQA settings, as\nwell as for information retrieval experiments. KazQAD contains just under 6,000\nunique questions with extracted short answers and nearly 12,000 passage-level\nrelevance judgements. We use a combination of machine translation, Wikipedia\nsearch, and in-house manual annotation to ensure annotation efficiency and data\nquality. The questions come from two sources: translated items from the Natural\nQuestions (NQ) dataset (only for training) and the original Kazakh Unified\nNational Testing (UNT) exam (for development and testing). The accompanying\ntext corpus contains more than 800,000 passages from the Kazakh Wikipedia. As a\nsupplementary dataset, we release around 61,000 question-passage-answer triples\nfrom the NQ dataset that have been machine-translated into Kazakh. We develop\nbaseline retrievers and readers that achieve reasonable scores in retrieval\n(NDCG@10 = 0.389 MRR = 0.382), reading comprehension (EM = 38.5 F1 = 54.2), and\nfull ODQA (EM = 17.8 F1 = 28.7) settings. Nevertheless, these results are\nsubstantially lower than state-of-the-art results for English QA collections,\nand we think that there should still be ample room for improvement. We also\nshow that the current OpenAI's ChatGPTv3.5 is not able to answer KazQAD test\nquestions in the closed-book setting with acceptable quality. The dataset is\nfreely available under the Creative Commons licence (CC BY-SA) at\nhttps://github.com/IS2AI/KazQAD.", "published": "2024-04-06 03:40:36", "link": "http://arxiv.org/abs/2404.04487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Visual and Text Prompting for Improved Object-Centric Perception\n  with Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) such as GPT-4V and Gemini Pro face\nchallenges in achieving human-level perception in Visual Question Answering\n(VQA), particularly in object-oriented perception tasks which demand\nfine-grained understanding of object identities, locations or attributes, as\nindicated by empirical findings. This is mainly due to their limited capability\nto effectively integrate complex visual cues with textual information and\npotential object hallucinations. In this paper, we present a novel approach,\nJoint Visual and Text Prompting (VTPrompt), that employs fine-grained visual\ninformation to enhance the capability of MLLMs in VQA, especially for\nobject-oriented perception. VTPrompt merges visual and text prompts to extract\nkey concepts from textual questions and employs a detection model to highlight\nrelevant objects as visual prompts in images. The processed images alongside\ntext prompts are subsequently fed into MLLMs to produce more accurate answers.\nOur experiments with GPT-4V and Gemini Pro, on three benchmarks, i.e., MME ,\nMMB and POPE, demonstrate significant improvements. Particularly, our method\nled to a score improvement of up to 183.5 for GPT-4V on MME and enhanced MMB\nperformance by 8.17\\% for GPT-4V and 15.69\\% for Gemini Pro.", "published": "2024-04-06 05:59:02", "link": "http://arxiv.org/abs/2404.04514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Morphology-Based Investigation of Positional Encodings", "abstract": "Contemporary deep learning models effectively handle languages with diverse\nmorphology despite not being directly integrated into them. Morphology and word\norder are closely linked, with the latter incorporated into transformer-based\nmodels through positional encodings. This prompts a fundamental inquiry: Is\nthere a correlation between the morphological complexity of a language and the\nutilization of positional encoding in pre-trained language models? In pursuit\nof an answer, we present the first study addressing this question, encompassing\n22 languages and 5 downstream tasks. Our findings reveal that the importance of\npositional encoding diminishes with increasing morphological complexity in\nlanguages. Our study motivates the need for a deeper understanding of\npositional encoding, augmenting them to better reflect the different languages\nunder consideration.", "published": "2024-04-06 07:10:47", "link": "http://arxiv.org/abs/2404.04530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Large Language Models (LLMs): False Attribution", "abstract": "In this work, we provide insight into one important limitation of large\nlanguage models (LLMs), i.e. false attribution, and introduce a new\nhallucination metric - Simple Hallucination Index (SHI). The task of automatic\nauthor attribution for relatively small chunks of text is an important NLP task\nbut can be challenging. We empirically evaluate the power of 3 open SotA LLMs\nin zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as\nhuman annotation can be costly. We collected the top 10 most popular books,\naccording to Project Gutenberg, divided each one into equal chunks of 400\nwords, and asked each LLM to predict the author. We then randomly sampled 162\nchunks for human evaluation from each of the annotated books, based on the\nerror margin of 7% and a confidence level of 95% for the book with the most\nchunks (Great Expectations by Charles Dickens, having 922 chunks). The average\nresults show that Mixtral 8x7B has the highest prediction accuracy, the lowest\nSHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996,\nrespectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B\nsuffers from high hallucinations for 3 books, rising as high as an SHI of 0.87\n(in the range 0-1, where 1 is the worst). The strong negative correlation of\naccuracy and SHI, given by r, demonstrates the fidelity of the new\nhallucination metric, which is generalizable to other tasks. We publicly\nrelease the annotated chunks of data and our codes to aid the reproducibility\nand evaluation of other models.", "published": "2024-04-06 13:38:15", "link": "http://arxiv.org/abs/2404.04631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context versus Prior Knowledge in Language Models", "abstract": "To answer a question, language models often need to integrate prior knowledge\nlearned during pretraining and new information presented in context. We\nhypothesize that models perform this integration in a predictable way across\ndifferent questions and contexts: models will rely more on prior knowledge for\nquestions about entities (e.g., persons, places, etc.) that they are more\nfamiliar with due to higher exposure in the training corpus, and be more easily\npersuaded by some contexts than others. To formalize this problem, we propose\ntwo mutual information-based metrics to measure a model's dependency on a\ncontext and on its prior about an entity: first, the persuasion score of a\ngiven context represents how much a model depends on the context in its\ndecision, and second, the susceptibility score of a given entity represents how\nmuch the model can be swayed away from its original answer distribution about\nan entity. We empirically test our metrics for their validity and reliability.\nFinally, we explore and find a relationship between the scores and the model's\nexpected familiarity with an entity, and provide two use cases to illustrate\ntheir benefits.", "published": "2024-04-06 13:46:53", "link": "http://arxiv.org/abs/2404.04633v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual\n  Knowledge Alignment, But Only Shallowly", "abstract": "Despite their strong ability to retrieve knowledge in English, current large\nlanguage models show imbalance abilities in different languages. Two approaches\nare proposed to address this, i.e., multilingual pretraining and multilingual\ninstruction tuning. However, whether and how do such methods contribute to the\ncross-lingual knowledge alignment inside the models is unknown. In this paper,\nwe propose CLiKA, a systematic framework to assess the cross-lingual knowledge\nalignment of LLMs in the Performance, Consistency and Conductivity levels, and\nexplored the effect of multilingual pretraining and instruction tuning on the\ndegree of alignment. Results show that: while both multilingual pretraining and\ninstruction tuning are beneficial for cross-lingual knowledge alignment, the\ntraining strategy needs to be carefully designed. Namely, continued pretraining\nimproves the alignment of the target language at the cost of other languages,\nwhile mixed pretraining affect other languages less. Also, the overall\ncross-lingual knowledge alignment, especially in the conductivity level, is\nunsatisfactory for all tested LLMs, and neither multilingual pretraining nor\ninstruction tuning can substantially improve the cross-lingual knowledge\nconductivity.", "published": "2024-04-06 15:25:06", "link": "http://arxiv.org/abs/2404.04659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Order-Based Pre-training Strategies for Procedural Text Understanding", "abstract": "In this paper, we propose sequence-based pretraining methods to enhance\nprocedural understanding in natural language processing. Procedural text,\ncontaining sequential instructions to accomplish a task, is difficult to\nunderstand due to the changing attributes of entities in the context. We focus\non recipes, which are commonly represented as ordered instructions, and use\nthis order as a supervision signal. Our work is one of the first to compare\nseveral 'order as-supervision' transformer pre-training methods, including\nPermutation Classification, Embedding Regression, and Skip-Clip, and shows that\nthese methods give improved results compared to the baselines and SoTA LLMs on\ntwo downstream Entity-Tracking datasets: NPN-Cooking dataset in recipe domain\nand ProPara dataset in open domain. Our proposed methods address the\nnon-trivial Entity Tracking Task that requires prediction of entity states\nacross procedure steps, which requires understanding the order of steps. These\nmethods show an improvement over the best baseline by 1.6% and 7-9% on\nNPN-Cooking and ProPara Datasets respectively across metrics.", "published": "2024-04-06 16:39:07", "link": "http://arxiv.org/abs/2404.04676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Brain Surgeon: Large Language Models Can be Compressed\n  Leaving No Language Behind", "abstract": "Large Language Models (LLMs) have ushered in a new era in Natural Language\nProcessing, but their massive size demands effective compression techniques for\npracticality. Although numerous model compression techniques have been\ninvestigated, they typically rely on a calibration set that overlooks the\nmultilingual context and results in significant accuracy degradation for\nlow-resource languages. This paper introduces Multilingual Brain Surgeon (MBS),\na novel calibration data sampling method for multilingual LLMs compression. MBS\novercomes the English-centric limitations of existing methods by sampling\ncalibration data from various languages proportionally to the language\ndistribution of the model training datasets. Our experiments, conducted on the\nBLOOM multilingual LLM, demonstrate that MBS improves the performance of\nexisting English-centric compression methods, especially for low-resource\nlanguages. We also uncover the dynamics of language interaction during\ncompression, revealing that the larger the proportion of a language in the\ntraining set and the more similar the language is to the calibration language,\nthe better performance the language retains after compression. In conclusion,\nMBS presents an innovative approach to compressing multilingual LLMs,\naddressing the performance disparities and improving the language inclusivity\nof existing compression techniques.", "published": "2024-04-06 22:16:32", "link": "http://arxiv.org/abs/2404.04748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model (LLM) AI text generation detection based on\n  transformer deep learning algorithm", "abstract": "In this paper, a tool for detecting LLM AI text generation is developed based\non the Transformer model, aiming to improve the accuracy of AI text generation\ndetection and provide reference for subsequent research. Firstly the text is\nUnicode normalised, converted to lowercase form, characters other than\nnon-alphabetic characters and punctuation marks are removed by regular\nexpressions, spaces are added around punctuation marks, first and last spaces\nare removed, consecutive ellipses are replaced with single spaces and the text\nis connected using the specified delimiter. Next remove non-alphabetic\ncharacters and extra whitespace characters, replace multiple consecutive\nwhitespace characters with a single space and again convert to lowercase form.\nThe deep learning model combines layers such as LSTM, Transformer and CNN for\ntext classification or sequence labelling tasks. The training and validation\nsets show that the model loss decreases from 0.127 to 0.005 and accuracy\nincreases from 94.96 to 99.8, indicating that the model has good detection and\nclassification ability for AI generated text. The test set confusion matrix and\naccuracy show that the model has 99% prediction accuracy for AI-generated text,\nwith a precision of 0.99, a recall of 1, and an f1 score of 0.99, achieving a\nvery high classification accuracy. Looking forward, it has the prospect of wide\napplication in the field of AI text detection.", "published": "2024-04-06 06:22:45", "link": "http://arxiv.org/abs/2405.06652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Soft-Prompting with Graph-of-Thought for Multi-modal Representation\n  Learning", "abstract": "The chain-of-thought technique has been received well in multi-modal tasks.\nIt is a step-by-step linear reasoning process that adjusts the length of the\nchain to improve the performance of generated prompts. However, human thought\nprocesses are predominantly non-linear, as they encompass multiple aspects\nsimultaneously and employ dynamic adjustment and updating mechanisms.\nTherefore, we propose a novel Aggregation-Graph-of-Thought (AGoT) mechanism for\nsoft-prompt tuning in multi-modal representation learning. The proposed AGoT\nmodels the human thought process not only as a chain but also models each step\nas a reasoning aggregation graph to cope with the overlooked multiple aspects\nof thinking in single-step reasoning. This turns the entire reasoning process\ninto prompt aggregation and prompt flow operations. Experiments show that our\nmulti-modal model enhanced with AGoT soft-prompting achieves good results in\nseveral tasks such as text-image retrieval, visual question answering, and\nimage recognition. In addition, we demonstrate that it has good domain\ngeneralization performance due to better reasoning.", "published": "2024-04-06 07:39:44", "link": "http://arxiv.org/abs/2404.04538v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment\n  Analysis", "abstract": "Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment\nby leveraging language, visual, and acoustic modalities. Despite the remarkable\nperformance exhibited by previous MSA approaches, the presence of inherent\nmultimodal heterogeneities poses a challenge, with the contribution of\ndifferent modalities varying considerably. Past research predominantly focused\non improving representation learning techniques and feature fusion strategies.\nHowever, many of these efforts overlooked the variation in semantic richness\namong different modalities, treating each modality uniformly. This approach may\nlead to underestimating the significance of strong modalities while\noveremphasizing the importance of weak ones. Motivated by these insights, we\nintroduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the\npredominant role of the text modality in MSA. Specifically, for each multimodal\nsample, by taking unaligned sequences of the three modalities as inputs, we\ninitially allocate the extracted unimodal features into a visual-text and an\nacoustic-text pair. Subsequently, we implement self-attention on the text\nmodality and apply text-queried cross-attention to the visual and acoustic\nmodalities. To mitigate the influence of noise signals and redundant features,\nwe incorporate a gated control mechanism into the framework. Additionally, we\nintroduce unimodal joint learning to gain a deeper understanding of homogeneous\nemotional tendencies across diverse modalities through backpropagation.\nExperimental results demonstrate that TCAN consistently outperforms\nstate-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).", "published": "2024-04-06 07:56:09", "link": "http://arxiv.org/abs/2404.04545v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Towards Analyzing and Understanding the Limitations of DPO: A\n  Theoretical Perspective", "abstract": "Direct Preference Optimization (DPO), which derives reward signals directly\nfrom pairwise preference data, has shown its effectiveness on aligning Large\nLanguage Models (LLMs) with human preferences. Despite its widespread use\nacross various tasks, DPO has been criticized for its sensitivity to the SFT's\neffectiveness and its hindrance to the learning capacity towards\nhuman-preferred responses, leading to less satisfactory performance. To\novercome those limitations, the theoretical understanding of DPO are\nindispensable but still lacking. To this end, we take a step towards\ntheoretically analyzing and understanding the limitations of DPO. Specifically,\nwe provide an analytical framework using the field theory to analyze the\noptimization process of DPO. By analyzing the gradient vector field of the DPO\nloss function, we find that the DPO loss function decreases the probability of\nproducing human dispreferred data at a faster rate than it increases the\nprobability of producing preferred data. This provides theoretical insights for\nunderstanding the limitations of DPO discovered in the related research\nexperiments, thereby setting the foundation for its improvement.", "published": "2024-04-06 13:24:37", "link": "http://arxiv.org/abs/2404.04626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Navigating the Landscape of Hint Generation Research: From the Past to\n  the Future", "abstract": "Digital education has gained popularity in the last decade, especially after\nthe COVID-19 pandemic. With the improving capabilities of large language models\nto reason and communicate with users, envisioning intelligent tutoring systems\n(ITSs) that can facilitate self-learning is not very far-fetched. One integral\ncomponent to fulfill this vision is the ability to give accurate and effective\nfeedback via hints to scaffold the learning process. In this survey article, we\npresent a comprehensive review of prior research on hint generation, aiming to\nbridge the gap between research in education and cognitive science, and\nresearch in AI and Natural Language Processing. Informed by our findings, we\npropose a formal definition of the hint generation task, and discuss the\nroadmap of building an effective hint generation system aligned with the formal\ndefinition, including open challenges, future directions and ethical\nconsiderations.", "published": "2024-04-06 20:42:46", "link": "http://arxiv.org/abs/2404.04728v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "What Happens When Small Is Made Smaller? Exploring the Impact of\n  Compression on Small Data Pretrained Language Models", "abstract": "Compression techniques have been crucial in advancing machine learning by\nenabling efficient training and deployment of large-scale language models.\nHowever, these techniques have received limited attention in the context of\nlow-resource language models, which are trained on even smaller amounts of data\nand under computational constraints, a scenario known as the \"low-resource\ndouble-bind.\" This paper investigates the effectiveness of pruning, knowledge\ndistillation, and quantization on an exclusively low-resourced, small-data\nlanguage model, AfriBERTa. Through a battery of experiments, we assess the\neffects of compression on performance across several metrics beyond accuracy.\nOur study provides evidence that compression techniques significantly improve\nthe efficiency and effectiveness of small-data language models, confirming that\nthe prevailing beliefs regarding the effects of compression on large, heavily\nparameterized models hold true for less-parameterized, small-data models.", "published": "2024-04-06 23:52:53", "link": "http://arxiv.org/abs/2404.04759v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators", "abstract": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98.", "published": "2024-04-06 02:29:02", "link": "http://arxiv.org/abs/2404.04475v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe\n  Biomedical Natural Language Inference for Clinical Trials", "abstract": "Large Language models (LLMs) have demonstrated state-of-the-art performance\nin various natural language processing (NLP) tasks across multiple domains, yet\nthey are prone to shortcut learning and factual inconsistencies. This research\ninvestigates LLMs' robustness, consistency, and faithful reasoning when\nperforming Natural Language Inference (NLI) on breast cancer Clinical Trial\nReports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural\nLanguage Inference for Clinical Trials. We examine the reasoning capabilities\nof LLMs and their adeptness at logical problem-solving. A comparative analysis\nis conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro\nunder zero-shot settings using Retrieval-Augmented Generation (RAG) framework,\nintegrating various reasoning chains. The evaluation yields an F1 score of\n0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test\ndataset.", "published": "2024-04-06 05:44:53", "link": "http://arxiv.org/abs/2404.04510v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITK at SemEval-2024 Task 1: Contrastive Learning and Autoencoders for\n  Semantic Textual Relatedness in Multilingual Texts", "abstract": "This paper describes our system developed for the SemEval-2024 Task 1:\nSemantic Textual Relatedness. The challenge is focused on automatically\ndetecting the degree of relatedness between pairs of sentences for 14 languages\nincluding both high and low-resource Asian and African languages. Our team\nparticipated in two subtasks consisting of Track A: supervised and Track B:\nunsupervised. This paper focuses on a BERT-based contrastive learning and\nsimilarity metric based approach primarily for the supervised track while\nexploring autoencoders for the unsupervised track. It also aims on the creation\nof a bigram relatedness corpus using negative sampling strategy, thereby\nproducing refined word embeddings.", "published": "2024-04-06 05:58:42", "link": "http://arxiv.org/abs/2404.04513v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models as Critical Thinking Tools: A Case Study of Philosophers", "abstract": "Current work in language models (LMs) helps us speed up or even skip thinking\nby accelerating and automating cognitive work. But can LMs help us with\ncritical thinking -- thinking in deeper, more reflective ways which challenge\nassumptions, clarify ideas, and engineer new concepts? We treat philosophy as a\ncase study in critical thinking, and interview 21 professional philosophers\nabout how they engage in critical thinking and on their experiences with LMs.\nWe find that philosophers do not find LMs to be useful because they lack a\nsense of selfhood (memory, beliefs, consistency) and initiative (curiosity,\nproactivity). We propose the selfhood-initiative model for critical thinking\ntools to characterize this gap. Using the model, we formulate three roles LMs\ncould play as critical thinking tools: the Interlocutor, the Monitor, and the\nRespondent. We hope that our work inspires LM researchers to further develop\nLMs as critical thinking tools and philosophers and other 'critical thinkers'\nto imagine intellectually substantive uses of LMs.", "published": "2024-04-06 06:12:13", "link": "http://arxiv.org/abs/2404.04516v2", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "IITK at SemEval-2024 Task 4: Hierarchical Embeddings for Detection of\n  Persuasion Techniques in Memes", "abstract": "Memes are one of the most popular types of content used in an online\ndisinformation campaign. They are primarily effective on social media platforms\nsince they can easily reach many users. Memes in a disinformation campaign\nachieve their goal of influencing the users through several rhetorical and\npsychological techniques, such as causal oversimplification, name-calling, and\nsmear. The SemEval 2024 Task 4 \\textit{Multilingual Detection of Persuasion\nTechnique in Memes} on identifying such techniques in the memes is divided\nacross three sub-tasks: ($\\mathbf{1}$) Hierarchical multi-label classification\nusing only textual content of the meme, ($\\mathbf{2}$) Hierarchical multi-label\nclassification using both, textual and visual content of the meme and\n($\\mathbf{3}$) Binary classification of whether the meme contains a persuasion\ntechnique or not using it's textual and visual content. This paper proposes an\nensemble of Class Definition Prediction (CDP) and hyperbolic embeddings-based\napproaches for this task. We enhance meme classification accuracy and\ncomprehensiveness by integrating HypEmo's hierarchical label embeddings (Chen\net al., 2023) and a multi-task learning framework for emotion prediction. We\nachieve a hierarchical F1-score of 0.60, 0.67, and 0.48 on the respective\nsub-tasks.", "published": "2024-04-06 06:28:02", "link": "http://arxiv.org/abs/2404.04520v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text\n  Reranking with Large Language Models", "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized\nin Large Language Models (LLMs) to improve the down-streaming tasks without the\ncost of fine-tuing the whole LLMs. Recent studies have shown how to effectively\nuse PEFT for fine-tuning LLMs in ranking tasks with convincing performance;\nthere are some limitations, including the learned prompt being fixed for\ndifferent documents, overfitting to specific tasks, and low adaptation ability.\nIn this paper, we introduce a query-dependent parameter efficient fine-tuning\n(Q-PEFT) approach for text reranking to leak the information of the true\nqueries to LLMs and then make the generation of true queries from input\ndocuments much easier. Specifically, we utilize the query to extract the\ntop-$k$ tokens from concatenated documents, serving as contextual clues. We\nfurther augment Q-PEFT by substituting the retrieval mechanism with a\nmulti-head attention layer to achieve end-to-end training and cover all the\ntokens in the documents, guiding the LLMs to generate more document-specific\nsynthetic queries, thereby further improving the reranking performance.\nExtensive experiments are conducted on four public datasets, demonstrating the\neffectiveness of our proposed approach.", "published": "2024-04-06 06:44:41", "link": "http://arxiv.org/abs/2404.04522v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITK at SemEval-2024 Task 10: Who is the speaker? Improving Emotion\n  Recognition and Flip Reasoning in Conversations via Speaker Embeddings", "abstract": "This paper presents our approach for the SemEval-2024 Task 10: Emotion\nDiscovery and Reasoning its Flip in Conversations. For the Emotion Recognition\nin Conversations (ERC) task, we utilize a masked-memory network along with\nspeaker participation. We propose a transformer-based speaker-centric model for\nthe Emotion Flip Reasoning (EFR) task. We also introduce Probable Trigger Zone,\na region of the conversation that is more likely to contain the utterances\ncausing the emotion to flip. For sub-task 3, the proposed approach achieves a\n5.9 (F1 score) improvement over the task baseline. The ablation study results\nhighlight the significance of various design choices in the proposed method.", "published": "2024-04-06 06:47:44", "link": "http://arxiv.org/abs/2404.04525v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HyperTTS: Parameter Efficient Adaptation in Text to Speech using\n  Hypernetworks", "abstract": "Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal\nfrom the text domain to the speech domain. While developing TTS architectures\nthat train and test on the same set of speakers has seen significant\nimprovements, out-of-domain speaker performance still faces enormous\nlimitations. Domain adaptation on a new set of speakers can be achieved by\nfine-tuning the whole model for each new domain, thus making it\nparameter-inefficient. This problem can be solved by Adapters that provide a\nparameter-efficient alternative to domain adaptation. Although famous in NLP,\nspeech synthesis has not seen much improvement from Adapters. In this work, we\npresent HyperTTS, which comprises a small learnable network, \"hypernetwork\",\nthat generates parameters of the Adapter blocks, allowing us to condition\nAdapters on speaker representations and making them dynamic. Extensive\nevaluations of two domain adaptation settings demonstrate its effectiveness in\nachieving state-of-the-art performance in the parameter-efficient regime. We\nalso compare different variants of HyperTTS, comparing them with baselines in\ndifferent studies. Promising results on the dynamic adaptation of adapter\nparameters using hypernetworks open up new avenues for domain-generic\nmulti-speaker TTS systems. The audio samples and code are available at\nhttps://github.com/declare-lab/HyperTTS.", "published": "2024-04-06 14:34:46", "link": "http://arxiv.org/abs/2404.04645v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Binary Classifier Optimization for Large Language Model Alignment", "abstract": "Aligning Large Language Models (LLMs) to human preferences through preference\noptimization has been crucial but labor-intensive, necessitating for each\nprompt a comparison of both a chosen and a rejected text completion by\nevaluators. Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that\nLLMs can be aligned using merely binary \"thumbs-up\" or \"thumbs-down\" signals on\neach prompt-completion pair. In this paper, we present theoretical foundations\nto explain the successful alignment achieved through these binary signals. Our\nanalysis uncovers a new perspective: optimizing a binary classifier, whose\nlogit is a reward, implicitly induces minimizing the Direct Preference\nOptimization (DPO) loss. In the process of this discovery, we identified two\ntechniques for effective alignment: reward shift and underlying distribution\nmatching. Consequently, we propose a new algorithm, \\textit{Binary Classifier\nOptimization}, that integrates the techniques. We validate our methodology in\ntwo settings: first, on a paired preference dataset, where our method performs\non par with DPO and KTO; and second, on binary signal datasets simulating\nreal-world conditions with divergent underlying distributions between thumbs-up\nand thumbs-down data. Our model consistently demonstrates effective and robust\nalignment across two base LLMs and three different binary signal datasets,\nshowcasing the strength of our approach to learning from binary feedback.", "published": "2024-04-06 15:20:59", "link": "http://arxiv.org/abs/2404.04656v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks", "abstract": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metrics based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information.", "published": "2024-04-06 16:16:30", "link": "http://arxiv.org/abs/2404.04671v3", "categories": ["cs.CL", "cs.LG", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Multicalibration for Confidence Scoring in LLMs", "abstract": "This paper proposes the use of \"multicalibration\" to yield interpretable and\nreliable confidence scores for outputs generated by large language models\n(LLMs). Multicalibration asks for calibration not just marginally, but\nsimultaneously across various intersecting groupings of the data. We show how\nto form groupings for prompt/completion pairs that are correlated with the\nprobability of correctness via two techniques: clustering within an embedding\nspace, and \"self-annotation\" - querying the LLM by asking it various yes-or-no\nquestions about the prompt. We also develop novel variants of multicalibration\nalgorithms that offer performance improvements by reducing their tendency to\noverfit. Through systematic benchmarking across various question answering\ndatasets and LLMs, we show how our techniques can yield confidence scores that\nprovide substantial improvements in fine-grained measures of both calibration\nand accuracy compared to existing methods.", "published": "2024-04-06 17:33:37", "link": "http://arxiv.org/abs/2404.04689v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "PoLLMgraph: Unraveling Hallucinations in Large Language Models via State\n  Transition Dynamics", "abstract": "Despite tremendous advancements in large language models (LLMs) over recent\nyears, a notably urgent challenge for their practical deployment is the\nphenomenon of hallucination, where the model fabricates facts and produces\nnon-factual statements. In response, we propose PoLLMgraph, a Polygraph for\nLLMs, as an effective model-based white-box detection and forecasting approach.\nPoLLMgraph distinctly differs from the large body of existing research that\nconcentrates on addressing such challenges through black-box evaluations. In\nparticular, we demonstrate that hallucination can be effectively detected by\nanalyzing the LLM's internal state transition dynamics during generation via\ntractable probabilistic models. Experimental results on various open-source\nLLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods\nby a considerable margin, evidenced by over 20% improvement in AUC-ROC on\ncommon benchmarking datasets like TruthfulQA. Our work paves a new way for\nmodel-based white-box analysis of LLMs, motivating the research community to\nfurther explore, understand, and refine the intricate dynamics of LLM\nbehaviors.", "published": "2024-04-06 20:02:20", "link": "http://arxiv.org/abs/2404.04722v1", "categories": ["cs.CL", "cs.CR", "cs.SE"], "primary_category": "cs.CL"}
{"title": "MACM: Utilizing a Multi-Agent System for Condition Mining in Solving\n  Complex Mathematical Problems", "abstract": "Recent advancements in large language models, such as GPT-4, have\ndemonstrated remarkable capabilities in processing standard queries. Despite\nthese advancements, their performance substantially declines in\n\\textbf{advanced mathematical problems requiring complex, multi-step logical\nreasoning}. To enhance their inferential capabilities, current research has\ndelved into \\textit{prompting engineering}, exemplified by methodologies such\nas the Tree of Thought and Graph of Thought. Nonetheless, these existing\napproaches encounter two significant limitations. Firstly, their effectiveness\nin tackling complex mathematical problems is somewhat constrained. Secondly,\nthe necessity to design distinct prompts for individual problems hampers their\ngeneralizability. In response to these limitations, this paper introduces the\n\\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting\nmethod. It not only resolves intricate mathematical problems but also\ndemonstrates strong generalization capabilities across various mathematical\ncontexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most\nchallenging level five mathematical problems in the MATH dataset increase from\n$\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in\n\\url{https://github.com/bin123apple/MACM}.", "published": "2024-04-06 21:39:01", "link": "http://arxiv.org/abs/2404.04735v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Goal-guided Generative Prompt Injection Attack on Large Language Models", "abstract": "Current large language models (LLMs) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. A large number of users can\neasily inject adversarial text or instructions through the user interface, thus\ncausing LLMs model security challenges. Although there is currently a large\namount of research on prompt injection attacks, most of these black-box attacks\nuse heuristic strategies. It is unclear how these heuristic strategies relate\nto the success rate of attacks and thus effectively improve model robustness.\nTo solve this problem, we redefine the goal of the attack: to maximize the KL\ndivergence between the conditional probabilities of the clean text and the\nadversarial text. Furthermore, we prove that maximizing the KL divergence is\nequivalent to maximizing the Mahalanobis distance between the embedded\nrepresentation $x$ and $x'$ of the clean text and the adversarial text when the\nconditional probability is a Gaussian distribution and gives a quantitative\nrelationship on $x$ and $x'$. Then we designed a simple and effective\ngoal-guided generative prompt injection strategy (G2PIA) to find an injection\ntext that satisfies specific constraints to achieve the optimal attack effect\napproximately. It is particularly noteworthy that our attack method is a\nquery-free black-box attack method with low computational cost. Experimental\nresults on seven LLM models and four datasets show the effectiveness of our\nattack method.", "published": "2024-04-06 06:17:10", "link": "http://arxiv.org/abs/2404.07234v4", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "RecGPT: Generative Personalized Prompts for Sequential Recommendation\n  via ChatGPT Training Paradigm", "abstract": "ChatGPT has achieved remarkable success in natural language understanding.\nConsidering that recommendation is indeed a conversation between users and the\nsystem with items as words, which has similar underlying pattern with ChatGPT,\nwe design a new chat framework in item index level for the recommendation task.\nOur novelty mainly contains three parts: model, training and inference. For the\nmodel part, we adopt Generative Pre-training Transformer (GPT) as the\nsequential recommendation model and design a user modular to capture\npersonalized information. For the training part, we adopt the two-stage\nparadigm of ChatGPT, including pre-training and fine-tuning. In the\npre-training stage, we train GPT model by auto-regression. In the fine-tuning\nstage, we train the model with prompts, which include both the newly-generated\nresults from the model and the user's feedback. For the inference part, we\npredict several user interests as user representations in an autoregressive\nmanner. For each interest vector, we recall several items with the highest\nsimilarity and merge the items recalled by all interest vectors into the final\nresult. We conduct experiments with both offline public datasets and online A/B\ntest to demonstrate the effectiveness of our proposed method.", "published": "2024-04-06 12:38:54", "link": "http://arxiv.org/abs/2404.08675v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models'\n  Safety through Red Teaming", "abstract": "When building Large Language Models (LLMs), it is paramount to bear safety in\nmind and protect them with guardrails. Indeed, LLMs should never generate\ncontent promoting or normalizing harmful, illegal, or unethical behavior that\nmay contribute to harm to individuals or society. This principle applies to\nboth normal and adversarial use. In response, we introduce ALERT, a large-scale\nbenchmark to assess safety based on a novel fine-grained risk taxonomy. It is\ndesigned to evaluate the safety of LLMs through red teaming methodologies and\nconsists of more than 45k instructions categorized using our novel taxonomy. By\nsubjecting LLMs to adversarial testing scenarios, ALERT aims to identify\nvulnerabilities, inform improvements, and enhance the overall safety of the\nlanguage models. Furthermore, the fine-grained taxonomy enables researchers to\nperform an in-depth evaluation that also helps one to assess the alignment with\nvarious policies. In our experiments, we extensively evaluate 10 popular open-\nand closed-source LLMs and demonstrate that many of them still struggle to\nattain reasonable levels of safety.", "published": "2024-04-06 15:01:47", "link": "http://arxiv.org/abs/2404.08676v3", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "Mathematics of the MML functional quantizer modules for VCV Rack\n  software synthesizer", "abstract": "We detail the mathematical formulation of the line of \"functional quantizer\"\nmodules developed by the Mathematics and Music Lab (MML) at Michigan\nTechnological University, for the VCV Rack software modular synthesizer\nplatform, which allow synthesizer players to tune oscillators to new musical\nscales based on mathematical functions. For example, we describe the\nrecently-released MML Logarithmic Quantizer (LOG QNT) module that tunes\nsynthesizer oscillators to the non-Pythagorean musical scale introduced by\nindie band The Apples in Stereo.", "published": "2024-04-06 21:56:16", "link": "http://arxiv.org/abs/2404.04739v3", "categories": ["cs.SD", "eess.AS", "math.HO"], "primary_category": "cs.SD"}
{"title": "A Novel Bi-LSTM And Transformer Architecture For Generating Tabla Music", "abstract": "Introduction: Music generation is a complex task that has received\nsignificant attention in recent years, and deep learning techniques have shown\npromising results in this field. Objectives: While extensive work has been\ncarried out on generating Piano and other Western music, there is limited\nresearch on generating classical Indian music due to the scarcity of Indian\nmusic in machine-encoded formats. In this technical paper, methods for\ngenerating classical Indian music, specifically tabla music, is proposed.\nInitially, this paper explores piano music generation using deep learning\narchitectures. Then the fundamentals are extended to generating tabla music.\nMethods: Tabla music in waveform (.wav) files are pre-processed using the\nlibrosa library in Python. A novel Bi-LSTM with an Attention approach and a\ntransformer model are trained on the extracted features and labels. Results:\nThe models are then used to predict the next sequences of tabla music. A loss\nof 4.042 and MAE of 1.0814 are achieved with the Bi-LSTM model. With the\ntransformer model, a loss of 55.9278 and MAE of 3.5173 are obtained for tabla\nmusic generation. Conclusion: The resulting music embodies a harmonious fusion\nof novelty and familiarity, pushing the limits of music composition to new\nhorizons.", "published": "2024-04-06 16:15:02", "link": "http://arxiv.org/abs/2404.05765v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
