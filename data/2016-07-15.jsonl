{"title": "Attention-over-Attention Neural Networks for Reading Comprehension", "abstract": "Cloze-style queries are representative problems in reading comprehension.\nOver the past few months, we have seen much progress that utilizing neural\nnetwork approach to solve Cloze-style questions. In this paper, we present a\nnovel model called attention-over-attention reader for the Cloze-style reading\ncomprehension task. Our model aims to place another attention mechanism over\nthe document-level attention, and induces \"attended attention\" for final\npredictions. Unlike the previous works, our neural network model requires less\npre-defined hyper-parameters and uses an elegant architecture for modeling.\nExperimental results show that the proposed attention-over-attention model\nsignificantly outperforms various state-of-the-art systems by a large margin in\npublic datasets, such as CNN and Children's Book Test datasets.", "published": "2016-07-15 09:10:11", "link": "http://arxiv.org/abs/1607.04423v4", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Neural Discourse Modeling of Conversations", "abstract": "Deep neural networks have shown recent promise in many language-related tasks\nsuch as the modeling of conversations. We extend RNN-based sequence to sequence\nmodels to capture the long range discourse across many turns of conversation.\nWe perform a sensitivity analysis on how much additional context affects\nperformance, and provide quantitative and qualitative evidence that these\nmodels are able to capture discourse relationships across multiple utterances.\nOur results quantifies how adding an additional RNN layer for modeling\ndiscourse improves the quality of output utterances and providing more of the\nprevious conversation as input also improves performance. By searching the\ngenerated outputs for specific discourse markers we show how neural discourse\nmodels can exhibit increased coherence and cohesion in conversations.", "published": "2016-07-15 16:43:40", "link": "http://arxiv.org/abs/1607.04576v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Enriching Word Vectors with Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks.", "published": "2016-07-15 18:27:55", "link": "http://arxiv.org/abs/1607.04606v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the efficient representation and execution of deep acoustic models", "abstract": "In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of a neural\nnetwork from 32-bit floating point values to 8-bit integer values. The proposed\nquantization scheme leads to significant memory savings and enables the use of\noptimized hardware instructions for integer arithmetic, thus significantly\nreducing the cost of inference. Finally, we propose a \"quantization aware\"\ntraining process that applies the proposed scheme during network training and\nfind that it allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them to a long\nshort-term memory-based acoustic model on an open-ended large vocabulary speech\nrecognition task.", "published": "2016-07-15 23:31:45", "link": "http://arxiv.org/abs/1607.04683v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Neural Tree Indexers for Text Understanding", "abstract": "Recurrent neural networks (RNNs) process input text sequentially and model\nthe conditional transition between word tokens. In contrast, the advantages of\nrecursive networks include that they explicitly model the compositionality and\nthe recursive structure of natural language. However, the current recursive\narchitecture is limited by its dependence on syntactic tree. In this paper, we\nintroduce a robust syntactic parsing-independent tree structured model, Neural\nTree Indexers (NTI) that provides a middle ground between the sequential RNNs\nand the syntactic treebased recursive models. NTI constructs a full n-ary tree\nby processing the input text with its node function in a bottom-up fashion.\nAttention mechanism can then be applied to both structure and node function. We\nimplemented and evaluated a binarytree model of NTI, showing the model achieved\nthe state-of-the-art performance on three different NLP tasks: natural language\ninference, answer sentence selection, and sentence classification,\noutperforming state-of-the-art recurrent and recursive neural networks.", "published": "2016-07-15 12:59:01", "link": "http://arxiv.org/abs/1607.04492v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
