{"title": "Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) generally requires a deep\nunderstanding of the contextual information, including the words associated\nwith the aspect terms and their syntactic dependencies. Most existing studies\nemploy advanced encoders (e.g., pre-trained models) to capture such context,\nespecially large language models (LLMs). However, training these encoders is\nresource-intensive, and in many cases, the available data is insufficient for\nnecessary fine-tuning. Therefore it is challenging for learning LLMs within\nsuch restricted environments and computation efficiency requirement. As a\nresult, it motivates the exploration of plug-and-play methods that adapt LLMs\nto ABSA with minimal effort. In this paper, we propose an approach that\nintegrates extendable components capable of incorporating various types of\nsyntactic knowledge, such as constituent syntax, word dependencies, and\ncombinatory categorial grammar (CCG). Specifically, we propose a memory module\nthat records syntactic information and is incorporated into LLMs to instruct\nthe prediction of sentiment polarities. Importantly, this encoder acts as a\nversatile, detachable plugin that is trained independently of the LLM. We\nconduct experiments on benchmark datasets, which show that our approach\noutperforms strong baselines and previous approaches, thus demonstrates its\neffectiveness.", "published": "2025-06-15 23:16:12", "link": "http://arxiv.org/abs/2506.12991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing", "abstract": "Retrieval-Augmented Generation (RAG) systems address factual inconsistencies\nin Large Language Models by grounding generation in external knowledge, yet\nthey face a fundamental efficiency problem: simple queries consume\ncomputational resources equivalent to complex multi-hop reasoning tasks. We\npresent SymRAG, a neuro-symbolic framework that introduces adaptive query\nrouting based on real-time complexity and system load assessments. SymRAG\ndynamically selects symbolic, neural, or hybrid processing paths to align\nresource use with query demands. Evaluated on 2,000 queries from HotpotQA and\nDROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%\nexact match accuracy with significantly lower CPU utilization (3.6--6.2%) and\nprocessing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%\nincrease in processing time, highlighting the framework's impact. These results\nunderscore the potential of adaptive neuro-symbolic routing for scalable,\nsustainable AI systems.", "published": "2025-06-15 22:35:43", "link": "http://arxiv.org/abs/2506.12981v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation", "abstract": "Media outlets are becoming more partisan and polarized nowadays. Most\nprevious work focused on detecting media bias. In this paper, we aim to\nmitigate media bias by generating a neutralized summary given multiple articles\npresenting different ideological views. Motivated by the critical role of\nevents and event relations in media bias detection, we propose to increase\nawareness of bias in LLMs via multi-document events reasoning and use a\nmulti-document event relation graph to guide the summarization process. This\ngraph contains rich event information useful to reveal bias: four common types\nof in-doc event relations to reflect content framing bias, cross-doc event\ncoreference relation to reveal content selection bias, and event-level moral\nopinions to highlight opinionated framing bias. We further develop two\nstrategies to incorporate the multi-document event relation graph for\nneutralized summarization. Firstly, we convert a graph into natural language\ndescriptions and feed the textualized graph into LLMs as a part of a hard text\nprompt. Secondly, we encode the graph with graph attention network and insert\nthe graph embedding into LLMs as a soft prompt. Both automatic evaluation and\nhuman evaluation confirm that our approach effectively mitigates both lexical\nand informational media bias, and meanwhile improves content preservation.", "published": "2025-06-15 22:14:59", "link": "http://arxiv.org/abs/2506.12978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Role of Data Quality in Training Bilingual Language Models", "abstract": "Bilingual and multilingual language models offer a promising path toward\nscaling NLP systems across diverse languages and users. However, their\nperformance often varies wildly between languages as prior works show that\nadding more languages can degrade performance for some languages (such as\nEnglish), while improving others (typically more data constrained languages).\nIn this work, we investigate causes of these inconsistencies by comparing\nbilingual and monolingual language models. Our analysis reveals that unequal\ndata quality, not just data quantity, is a major driver of performance\ndegradation in bilingual settings. We propose a simple yet effective data\nfiltering strategy to select higher-quality bilingual training data with only\nhigh quality English data. Applied to French, German, and Chinese, our approach\nimproves monolingual performance by 2-4% and reduces bilingual model\nperformance gaps to 1%. These results highlight the overlooked importance of\ndata quality in multilingual pretraining and offer a practical recipe for\nbalancing performance.", "published": "2025-06-15 21:08:51", "link": "http://arxiv.org/abs/2506.12966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.", "published": "2025-06-15 19:42:58", "link": "http://arxiv.org/abs/2506.12953v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance", "abstract": "Large Language models have demonstrated promising performance in research\nideation across scientific domains. Hypothesis development, the process of\ngenerating a highly specific declarative statement connecting a research idea\nwith empirical validation, has received relatively less attention. Existing\napproaches trivially deploy retrieval augmentation and focus only on the\nquality of the final output ignoring the underlying reasoning process behind\nideation. We present $\\texttt{HypER}$ ($\\textbf{Hyp}$othesis Generation with\n$\\textbf{E}$xplanation and $\\textbf{R}$easoning), a small language model (SLM)\ntrained for literature-guided reasoning and evidence-based hypothesis\ngeneration. $\\texttt{HypER}$ is trained in a multi-task setting to discriminate\nbetween valid and invalid scientific reasoning chains in presence of controlled\ndistractions. We find that $\\texttt{HypER}$ outperformes the base model,\ndistinguishing valid from invalid reasoning chains (+22\\% average absolute F1),\ngenerates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with\nhigh feasibility and impact as judged by human experts ($>$3.5 on 5-point\nLikert scale).", "published": "2025-06-15 18:41:23", "link": "http://arxiv.org/abs/2506.12937v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation", "abstract": "In clinical operations, teamwork can be the crucial factor that determines\nthe final outcome. Prior studies have shown that sufficient collaboration is\nthe key factor that determines the outcome of an operation. To understand how\nthe team practices teamwork during the operation, we collected CliniDial from\nsimulations of medical operations. CliniDial includes the audio data and its\ntranscriptions, the simulated physiology signals of the patient manikins, and\nhow the team operates from two camera angles. We annotate behavior codes\nfollowing an existing framework to understand the teamwork process for\nCliniDial. We pinpoint three main characteristics of our dataset, including its\nlabel imbalances, rich and natural interactions, and multiple modalities, and\nconduct experiments to test existing LLMs' capabilities on handling data with\nthese characteristics. Experimental results show that CliniDial poses\nsignificant challenges to the existing models, inviting future effort on\ndeveloping methods that can deal with real-world clinical data. We open-source\nthe codebase at https://github.com/MichiganNLP/CliniDial", "published": "2025-06-15 18:39:24", "link": "http://arxiv.org/abs/2506.12936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models", "abstract": "While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.", "published": "2025-06-15 18:26:08", "link": "http://arxiv.org/abs/2506.12935v1", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sectoral Coupling in Linguistic State Space", "abstract": "This work presents a formal framework for quantifying the internal\ndependencies between functional subsystems within artificial agents whose\nbelief states are composed of structured linguistic fragments. Building on the\nSemantic Manifold framework, which organizes belief content into functional\nsectors and stratifies them across hierarchical levels of abstraction, we\nintroduce a system of sectoral coupling constants that characterize how one\ncognitive sector influences another within a fixed level of abstraction. The\ncomplete set of these constants forms an agent-specific coupling profile that\ngoverns internal information flow, shaping the agent's overall processing\ntendencies and cognitive style. We provide a detailed taxonomy of these\nintra-level coupling roles, covering domains such as perceptual integration,\nmemory access and formation, planning, meta-cognition, execution control, and\naffective modulation. We also explore how these coupling profiles generate\nfeedback loops, systemic dynamics, and emergent signatures of cognitive\nbehavior. Methodologies for inferring these profiles from behavioral or\ninternal agent data are outlined, along with a discussion of how these\ncouplings evolve across abstraction levels. This framework contributes a\nmechanistic and interpretable approach to modeling complex cognition, with\napplications in AI system design, alignment diagnostics, and the analysis of\nemergent agent behavior.", "published": "2025-06-15 17:58:54", "link": "http://arxiv.org/abs/2506.12927v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks", "abstract": "Comparative studies of news coverage are challenging to conduct because\nmethods to identify news articles about the same event in different languages\nrequire expertise that is difficult to scale. We introduce an AI-powered method\nfor identifying news articles based on an event FINGERPRINT, which is a minimal\nset of metadata required to identify critical events. Our event coverage\nidentification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),\nefficiently identifies news articles about critical world events, specifically\nterrorist attacks and several types of natural disasters. FAME does not require\ntraining data and is able to automatically and efficiently identify news\narticles that discuss an event given its fingerprint: time, location, and class\n(such as storm or flood). The method achieves state-of-the-art performance and\nscales to massive databases of tens of millions of news articles and hundreds\nof events happening globally. We use FAME to identify 27,441 articles that\ncover 470 natural disaster and terrorist attack events that happened in 2020.\nTo this end, we use a massive database of news articles in three languages from\nMediaCloud, and three widely used, expert-curated databases of critical events:\nEM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior\nliterature: coverage of disasters and terrorist attacks correlates to death\ncounts, to the GDP of a country where the event occurs, and to trade volume\nbetween the reporting country and the country where the event occurred. We\nshare our NLP annotations and cross-country media attention data to support the\nefforts of researchers and media monitoring organizations.", "published": "2025-06-15 17:50:08", "link": "http://arxiv.org/abs/2506.12925v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "K.4.2"], "primary_category": "cs.IR"}
{"title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization", "abstract": "With the rapid improvement in the general capabilities of LLMs, LLM\npersonalization, i.e., how to build LLM systems that can generate personalized\nresponses or services that are tailored to distinct user personas, has become\nan increasingly important research and engineering problem. However, unlike\nmany new challenging benchmarks being released for evaluating the\ngeneral/reasoning capabilities, the lack of high-quality benchmarks for\nevaluating LLM personalization greatly hinders progress in this field. To\naddress this, we introduce PersonaFeedback, a new benchmark that directly\nevaluates LLMs' ability to provide personalized responses given pre-defined\nuser personas and queries. Unlike existing benchmarks that require models to\ninfer implicit user personas from historical interactions, PersonaFeedback\ndecouples persona inference from personalization, focusing on evaluating the\nmodel's ability to generate responses tailored to explicit personas.\nPersonaFeedback consists of 8298 human-annotated test cases, which are\ncategorized into easy, medium, and hard tiers based on the contextual\ncomplexity of the user personas and the difficulty in distinguishing subtle\ndifferences between two personalized responses. We conduct comprehensive\nevaluations across a wide range of models. The empirical results reveal that\neven state-of-the-art LLMs that can solve complex real-world reasoning tasks\ncould fall short on the hard tier of PersonaFeedback where even human\nevaluators may find the distinctions challenging. Furthermore, we conduct an\nin-depth analysis of failure modes across various types of systems,\ndemonstrating that the current retrieval-augmented framework should not be seen\nas a de facto solution for personalization tasks. All benchmark data,\nannotation protocols, and the evaluation pipeline will be publicly available to\nfacilitate future research on LLM personalization.", "published": "2025-06-15 17:19:19", "link": "http://arxiv.org/abs/2506.12915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciDA: Scientific Dynamic Assessor of LLMs", "abstract": "Advancement in Large Language Models (LLMs) reasoning capabilities enables\nthem to solve scientific problems with enhanced efficacy. Thereby, a\nhigh-quality benchmark for comprehensive and appropriate assessment holds\nsignificance, while existing ones either confront the risk of data\ncontamination or lack involved disciplines. To be specific, due to the data\nsource overlap of LLMs training and static benchmark, the keys or number\npattern of answers inadvertently memorized (i.e. data contamination), leading\nto systematic overestimation of their reasoning capabilities, especially\nnumerical reasoning. We propose SciDA, a multidisciplinary benchmark that\nconsists exclusively of over 1k Olympic-level numerical computation problems,\nallowing randomized numerical initializations for each inference round to avoid\nreliance on fixed numerical patterns. We conduct a series of experiments with\nboth closed-source and open-source top-performing LLMs, and it is observed that\nthe performance of LLMs drop significantly under random numerical\ninitialization. Thus, we provide truthful and unbiased assessments of the\nnumerical reasoning capabilities of LLMs. The data is available at\nhttps://huggingface.co/datasets/m-a-p/SciDA", "published": "2025-06-15 16:57:14", "link": "http://arxiv.org/abs/2506.12909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JEBS: A Fine-grained Biomedical Lexical Simplification Task", "abstract": "Online medical literature has made health information more available than\never, however, the barrier of complex medical jargon prevents the general\npublic from understanding it. Though parallel and comparable corpora for\nBiomedical Text Simplification have been introduced, these conflate the many\nsyntactic and lexical operations involved in simplification. To enable more\ntargeted development and evaluation, we present a fine-grained lexical\nsimplification task and dataset, Jargon Explanations for Biomedical\nSimplification (JEBS, https://github.com/bill-from-ri/JEBS-data ). The JEBS\ntask involves identifying complex terms, classifying how to replace them, and\ngenerating replacement text. The JEBS dataset contains 21,595 replacements for\n10,314 terms across 400 biomedical abstracts and their manually simplified\nversions. Additionally, we provide baseline results for a variety of rule-based\nand transformer-based systems for the three sub-tasks. The JEBS task, data, and\nbaseline results pave the way for development and rigorous evaluation of\nsystems for replacing or explaining complex biomedical terms.", "published": "2025-06-15 16:01:20", "link": "http://arxiv.org/abs/2506.12898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language", "abstract": "Legal passage retrieval is an important task that assists legal practitioners\nin the time-intensive process of finding relevant precedents to support legal\narguments. This study investigates the task of retrieving legal passages or\nparagraphs from decisions of the Court of Justice of the European Union (CJEU),\nwhose language is highly structured and formulaic, leading to repetitive\npatterns. Understanding when lexical or semantic models are more effective at\nhandling the repetitive nature of legal language is key to developing retrieval\nsystems that are more accurate, efficient, and transparent for specific legal\ndomains. To this end, we explore when this routinized legal language is better\nsuited for retrieval using methods that rely on lexical and statistical\nfeatures, such as BM25, or dense retrieval models trained to capture semantic\nand contextual information. A qualitative and quantitative analysis with three\ncomplementary metrics shows that both lexical and dense models perform well in\nscenarios with more repetitive usage of language, whereas BM25 performs better\nthan the dense models in more nuanced scenarios where repetition and\nverbatim~quotes are less prevalent and in longer queries. Our experiments also\nshow that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4\nout of 7 performance metrics. However, fine-tuning a dense model on\ndomain-specific data led to improved performance, surpassing BM25 in most\nmetrics, and we analyze the effect of the amount of data used in fine-tuning on\nthe model's performance and temporal robustness. The code, dataset and appendix\nrelated to this work are available on:\nhttps://github.com/larimo/lexsem-legal-ir.", "published": "2025-06-15 15:53:38", "link": "http://arxiv.org/abs/2506.12895v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality", "abstract": "This work presents three different approaches to address the ArchEHR-QA 2025\nShared Task on automated patient question answering. We introduce an end-to-end\nprompt-based baseline and two two-step methods to divide the task, without\nutilizing any external knowledge. Both two step approaches first extract\nessential sentences from the clinical text, by prompt or similarity ranking,\nand then generate the final answer from these notes. Results indicate that the\nre-ranker based two-step system performs best, highlighting the importance of\nselecting the right approach for each subtask. Our best run achieved an overall\nscore of 0.44, ranking 8th out of 30 on the leaderboard, securing the top\nposition in overall factuality.", "published": "2025-06-15 15:32:49", "link": "http://arxiv.org/abs/2506.12886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning", "abstract": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.", "published": "2025-06-15 14:21:28", "link": "http://arxiv.org/abs/2506.12860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transforming Chatbot Text: A Sequence-to-Sequence Approach", "abstract": "Due to advances in Large Language Models (LLMs) such as ChatGPT, the boundary\nbetween human-written text and AI-generated text has become blurred.\nNevertheless, recent work has demonstrated that it is possible to reliably\ndetect GPT-generated text. In this paper, we adopt a novel strategy to\nadversarially transform GPT-generated text using sequence-to-sequence (Seq2Seq)\nmodels, with the goal of making the text more human-like. We experiment with\nthe Seq2Seq models T5-small and BART which serve to modify GPT-generated\nsentences to include linguistic, structural, and semantic components that may\nbe more typical of human-authored text. Experiments show that classification\nmodels trained to distinguish GPT-generated text are significantly less\naccurate when tested on text that has been modified by these Seq2Seq models.\nHowever, after retraining classification models on data generated by our\nSeq2Seq technique, the models are able to distinguish the transformed\nGPT-generated text from human-generated text with high accuracy. This work adds\nto the accumulating knowledge of text transformation as a tool for both attack\n-- in the sense of defeating classification models -- and defense -- in the\nsense of improved classifiers -- thereby advancing our understanding of\nAI-generated text.", "published": "2025-06-15 13:30:38", "link": "http://arxiv.org/abs/2506.12843v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench", "abstract": "With the rapid development of LLM-based agents, increasing attention has been\ngiven to their social interaction and strategic reasoning capabilities.\nHowever, existing Werewolf-based benchmarking platforms suffer from overly\nsimplified game settings, incomplete evaluation metrics, and poor scalability.\nTo address these limitations, we propose WereWolf-Plus, a multi-model,\nmulti-dimensional, and multi-method benchmarking platform for evaluating\nmulti-agent strategic reasoning in the Werewolf game. The platform offers\nstrong extensibility, supporting customizable configurations for roles such as\nSeer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment\nand reasoning enhancement strategies for different roles. In addition, we\nintroduce a comprehensive set of quantitative evaluation metrics for all\nspecial roles, werewolves, and the sheriff, and enrich the assessment\ndimensions for agent reasoning ability, cooperation capacity, and social\ninfluence. WereWolf-Plus provides a more flexible and reliable environment for\nadvancing research on inference and strategic interaction within multi-agent\ncommunities. Our code is open sourced at\nhttps://github.com/MinstrelsyXia/WereWolfPlus.", "published": "2025-06-15 13:28:41", "link": "http://arxiv.org/abs/2506.12841v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems", "abstract": "This work presents an Argument Mining process that extracts argumentative\nentities from clinical texts and identifies their relationships using token\nclassification and Natural Language Inference techniques. Compared to\nstraightforward methods like text classification, this methodology demonstrates\nsuperior performance in data-scarce settings. By assessing the effectiveness of\nthese methods in identifying argumentative structures that support or refute\npossible diagnoses, this research lays the groundwork for future tools that can\nprovide evidence-based justifications for machine-generated clinical\nconclusions.", "published": "2025-06-15 12:10:28", "link": "http://arxiv.org/abs/2506.12823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surprise Calibration for Better In-Context Learning", "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for task\nadaptation in large language models (LLMs), where models infer underlying task\nstructures from a few demonstrations. However, ICL remains susceptible to\nbiases that arise from prior knowledge and contextual demonstrations, which can\ndegrade the performance of LLMs. Existing bias calibration methods typically\napply fixed class priors across all inputs, limiting their efficacy in dynamic\nICL settings where the context for each query differs. To address these\nlimitations, we adopt implicit sequential Bayesian inference as a framework for\ninterpreting ICL, identify \"surprise\" as an informative signal for class prior\nshift, and introduce a novel method--Surprise Calibration (SC). SC leverages\nthe notion of surprise to capture the temporal dynamics of class priors,\nproviding a more adaptive and computationally efficient solution for in-context\nlearning. We empirically demonstrate the superiority of SC over existing bias\ncalibration techniques across a range of benchmark natural language processing\ntasks.", "published": "2025-06-15 10:04:42", "link": "http://arxiv.org/abs/2506.12796v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models", "abstract": "As Large Language Models (LLMs) become increasingly integrated into everyday\nlife and information ecosystems, concerns about their implicit biases continue\nto persist. While prior work has primarily examined socio-demographic and\nleft--right political dimensions, little attention has been paid to how LLMs\nalign with broader geopolitical value systems, particularly the\ndemocracy--authoritarianism spectrum. In this paper, we propose a novel\nmethodology to assess such alignment, combining (1) the F-scale, a psychometric\ntool for measuring authoritarian tendencies, (2) FavScore, a newly introduced\nmetric for evaluating model favorability toward world leaders, and (3)\nrole-model probing to assess which figures are cited as general role-models by\nLLMs. We find that LLMs generally favor democratic values and leaders, but\nexhibit increases favorability toward authoritarian figures when prompted in\nMandarin. Further, models are found to often cite authoritarian figures as role\nmodels, even outside explicit political contexts. These results shed light on\nways LLMs may reflect and potentially reinforce global political ideologies,\nhighlighting the importance of evaluating bias beyond conventional\nsocio-political axes. Our code is available at:\nhttps://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs", "published": "2025-06-15 07:52:07", "link": "http://arxiv.org/abs/2506.12758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating-dominating partitions for some classes of graphs", "abstract": "A dominating set of a graph $G$ is a set $D \\subseteq V(G)$ such that every\nvertex in $V(G) \\setminus D$ is adjacent to at least one vertex in $D$. A set\n$L\\subseteq V(G)$ is a locating set of $G$ if every vertex in $V(G) \\setminus\nL$ has pairwise distinct open neighborhoods in $L$. A set $D\\subseteq V(G)$ is\na locating-dominating set of $G$ if $D$ is a dominating set and a locating set\nof $G$. The location-domination number of $G$, denoted by $\\gamma_{LD}(G)$, is\nthe minimum cardinality among all locating-dominating sets of $G$. A well-known\nconjecture in the study of locating-dominating sets is that if $G$ is an\nisolate-free and twin-free graph of order $n$, then $\\gamma_{LD}(G)\\le\n\\frac{n}{2}$. Recently, Bousquet et al. [Discrete Math. 348 (2025), 114297]\nproved that if $G$ is an isolate-free and twin-free graph of order $n$, then\n$\\gamma_{LD}(G)\\le \\lceil\\frac{5n}{8}\\rceil$ and posed the question whether the\nvertex set of such a graph can be partitioned into two locating sets. We answer\nthis question affirmatively for twin-free distance-hereditary graphs, maximal\nouterplanar graphs, split graphs, and co-bipartite graphs. In fact, we prove a\nstronger result that for any graph $G$ without isolated vertices and twin\nvertices, if $G$ is a distance-hereditary graph or a maximal outerplanar graph\nor a split graph or a co-bipartite graph, then the vertex set of $G$ can be\npartitioned into two locating-dominating sets. Consequently, this also confirms\nthe original conjecture for these graph classes.", "published": "2025-06-15 18:22:54", "link": "http://arxiv.org/abs/2506.12933v1", "categories": ["math.CO", "cs.DM", "05C69"], "primary_category": "math.CO"}
{"title": "Shortest Paths in a Weighted Simplicial Complex", "abstract": "Simplicial complexes are extensively studied in the field of algebraic\ntopology. They have gained attention in recent time due to their applications\nin fields like theoretical distributed computing and simplicial neural\nnetworks. Graphs are mono-dimensional simplicial complex. Graph theory has\napplication in topics like theoretical computer science, operations research,\nbioinformatics and social sciences. This makes it natural to try to adapt\ngraph-theoretic results for simplicial complexes, which can model more\nintricate and detailed structures appearing in real-world systems. In this\narticle, we define the concept of weighted simplicial complex and $d$-path in a\nsimplicial complex. Both these concepts have the potential to have numerous\nreal-life applications. Next, we provide a novel algorithm to find the shortest\npaths in a weighted simplicial complex. The core principles of our algorithm\nalign with those of Dijkstra$^\\prime$s algorithm for graphs. Hence, this work\nlays another brick for the sake of integrating graph-theoretic concepts with\nabstract simplicial complexes.", "published": "2025-06-15 17:36:30", "link": "http://arxiv.org/abs/2506.12921v1", "categories": ["cs.DM", "68R01, 68R10, 68Q25, 68W01"], "primary_category": "cs.DM"}
{"title": "On the Vertices of Delta-modular Polyhedra", "abstract": "Let $P$ be a polytope defined by the system $A x \\leq b$, where $A \\in R^{m\n\\times n}$, $b \\in R^m$, and $\\text{rank}(A) = n$. We give a short geometric\nproof of the following tight upper bound on the number of vertices of $P$: $$\n  n! \\cdot \\frac{\\Delta}{\\Delta_{\\text{average}}} \\cdot \\text{vol}(B_2) \\sim\n\\frac{1}{\\sqrt{\\pi n}} \\cdot \\left(\\frac{2 \\pi}{e}\\right)^{n/2} \\cdot n^{n/2}\n\\cdot \\frac{\\Delta}{\\Delta_{\\text{average}}}, $$ where $\\Delta$ is the maximum\nabsolute value of $n \\times n$ subdeterminants of $A$, and\n$\\Delta_{\\text{average}}$ is the average absolute value of subdeterminants of\n$A$ corresponding to a triangulation of $P$'s normal fan. Assuming that $A$ is\ninteger, such polyhedra are called $\\Delta$-modular polyhedra. Note that in the\ninteger case, the bound can be simplified via the inequality\n$\\Delta_{\\text{average}} \\geq \\Delta_{\\min} \\geq 1$, where $\\Delta_{\\min}$ is\nthe minimum absolute value of subdeterminants of $A$ corresponding to feasible\nbases of $A x \\leq b$. For this, we prove and use a symmetric variant of\nMacbeath's theorem.\n  Additionally, we give a direct argument based on prior results in the field,\nshowing that the graph diameter of $P$ is bounded by $O\\bigl(n^3 \\cdot\n\\frac{\\Delta}{\\Delta_{\\min}} \\cdot \\ln (n \\frac{\\Delta}{\\Delta_{\\min}})\n\\bigr)$. Thus, both characteristic of $P$ are linear in $\\Delta/\\Delta_{\\min}$.\n  From an algorithmic perspective, we demonstrate that:\n  Given $A \\in Q^{m \\times n}$, $b \\in Q^m$, and an initial feasible solution\nto $A x \\leq b$, the convex hull of $P$ can be constructed in $O(n)^{n/2} \\cdot\nm^2 \\cdot \\frac{\\Delta}{\\Delta_{\\text{average}}}$ operations. For simple\npolyhedra, the dependence on $m$ reduces to linear;\n  Given $A \\in Z^{m \\times n}$ and $b \\in Q^m$, the number $|P \\cap Z^n|$ can\nbe computed in $O(n)^n \\cdot \\frac{\\Delta^4}{\\Delta_{\\text{average}}}$\narithmetic operations.", "published": "2025-06-15 08:52:17", "link": "http://arxiv.org/abs/2506.12774v1", "categories": ["math.CO", "cs.CG", "cs.DM"], "primary_category": "math.CO"}
{"title": "Versatile and Fast Location-Based Private Information Retrieval with Fully Homomorphic Encryption over the Torus", "abstract": "Location-based services often require users to share sensitive locational\ndata, raising privacy concerns due to potential misuse or exploitation by\nuntrusted servers. In response, we present VeLoPIR, a versatile location-based\nprivate information retrieval (PIR) system designed to preserve user privacy\nwhile enabling efficient and scalable query processing. VeLoPIR introduces\nthree operational modes-interval validation, coordinate validation, and\nidentifier matching-that support a broad range of real-world applications,\nincluding information and emergency alerts. To enhance performance, VeLoPIR\nincorporates multi-level algorithmic optimizations with parallel structures,\nachieving significant scalability across both CPU and GPU platforms. We also\nprovide formal security and privacy proofs, confirming the system's robustness\nunder standard cryptographic assumptions. Extensive experiments on real-world\ndatasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a\nprior baseline. The implementation of VeLoPIR is publicly available at\nhttps://github.com/PrivStatBool/VeLoPIR.", "published": "2025-06-15 08:01:35", "link": "http://arxiv.org/abs/2506.12761v1", "categories": ["cs.CR", "cs.IR"], "primary_category": "cs.CR"}
{"title": "Hierarchical Group-wise Ranking Framework for Recommendation Models", "abstract": "In modern recommender systems, CTR/CVR models are increasingly trained with\nranking objectives to improve item ranking quality. While this shift aligns\ntraining more closely with serving goals, most existing methods rely on\nin-batch negative sampling, which predominantly surfaces easy negatives. This\nlimits the model's ability to capture fine-grained user preferences and weakens\noverall ranking performance. To address this, we propose a Hierarchical\nGroup-wise Ranking Framework with two key components. First, we apply residual\nvector quantization to user embeddings to generate hierarchical user codes that\npartition users into hierarchical, trie-structured clusters. Second, we apply\nlistwise ranking losses to user-item pairs at each level of the hierarchy,\nwhere shallow levels group loosely similar users and deeper levels group highly\nsimilar users, reinforcing learning-to-rank signals through progressively\nharder negatives. Since users with similar preferences and content exposure\ntend to yield more informative negatives, applying ranking losses within these\nhierarchical user groups serves as an effective approximation of hard negative\nmining. Our approach improves ranking performance without requiring complex\nreal-time context collection or retrieval infrastructure. Extensive experiments\ndemonstrate that the proposed framework consistently enhances both model\ncalibration and ranking accuracy, offering a scalable and practical solution\nfor industrial recommender systems.", "published": "2025-06-15 07:47:26", "link": "http://arxiv.org/abs/2506.12756v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation", "abstract": "The rapid growth of scientific literature demands robust tools for automated\nsurvey-generation. However, current large language model (LLM)-based methods\noften lack in-depth analysis, structural coherence, and reliable citations. To\naddress these limitations, we introduce SciSage, a multi-agent framework\nemploying a reflect-when-you-write paradigm. SciSage features a hierarchical\nReflector agent that critically evaluates drafts at outline, section, and\ndocument levels, collaborating with specialized agents for query\ninterpretation, content retrieval, and refinement. We also release SurveyScope,\na rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11\ncomputer science domains, with strict recency and citation-based quality\ncontrols. Evaluations demonstrate that SciSage outperforms state-of-the-art\nbaselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document\ncoherence and +32% in citation F1 scores. Human evaluations reveal mixed\noutcomes (3 wins vs. 7 losses against human-written surveys), but highlight\nSciSage's strengths in topical breadth and retrieval efficiency. Overall,\nSciSage offers a promising foundation for research-assistive writing tools.", "published": "2025-06-15 02:23:47", "link": "http://arxiv.org/abs/2506.12689v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Device-Cloud Collaborative Correction for On-Device Recommendation", "abstract": "With the rapid development of recommendation models and device computing\npower, device-based recommendation has become an important research area due to\nits better real-time performance and privacy protection. Previously,\nTransformer-based sequential recommendation models have been widely applied in\nthis field because they outperform Recurrent Neural Network (RNN)-based\nrecommendation models in terms of performance. However, as the length of\ninteraction sequences increases, Transformer-based models introduce\nsignificantly more space and computational overhead compared to RNN-based\nmodels, posing challenges for device-based recommendation. To balance real-time\nperformance and high performance on devices, we propose Device-Cloud\n\\underline{Co}llaborative \\underline{Corr}ection Framework for On-Device\n\\underline{Rec}ommendation (CoCorrRec). CoCorrRec uses a self-correction\nnetwork (SCN) to correct parameters with extremely low time cost. By updating\nmodel parameters during testing based on the input token, it achieves\nperformance comparable to current optimal but more complex Transformer-based\nmodels. Furthermore, to prevent SCN from overfitting, we design a global\ncorrection network (GCN) that processes hidden states uploaded from devices and\nprovides a global correction solution. Extensive experiments on multiple\ndatasets show that CoCorrRec outperforms existing Transformer-based and\nRNN-based device recommendation models in terms of performance, with fewer\nparameters and lower FLOPs, thereby achieving a balance between real-time\nperformance and high efficiency.", "published": "2025-06-15 02:20:18", "link": "http://arxiv.org/abs/2506.12687v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Optimal Reconstruction Codes with Given Reads in Multiple Burst-Substitutions Channels", "abstract": "We study optimal reconstruction codes over the multiple-burst substitution\nchannel. Our main contribution is establishing a trade-off between the\nerror-correction capability of the code, the number of reads used in the\nreconstruction process, and the decoding list size. We show that over a channel\nthat introduces at most $t$ bursts, we can use a length-$n$ code capable of\ncorrecting $\\epsilon$ errors, with $\\Theta(n^\\rho)$ reads, and decoding with a\nlist of size $O(n^\\lambda)$, where $t-1=\\epsilon+\\rho+\\lambda$. In the process\nof proving this, we establish sharp asymptotic bounds on the size of error\nballs in the burst metric. More precisely, we prove a Johnson-type lower bound\nvia Kahn's Theorem on large matchings in hypergraphs, and an upper bound via a\nnovel variant of Kleitman's Theorem under the burst metric, which might be of\nindependent interest.\n  Beyond this main trade-off, we derive several related results using a variety\nof combinatorial techniques. In particular, along with tools from recent\nadvances in discrete geometry, we improve the classical Gilbert-Varshamov bound\nin the asymptotic regime for multiple bursts, and determine the minimum\nredundancy required for reconstruction codes with polynomially many reads. We\nalso propose an efficient list-reconstruction algorithm that achieves the above\nguarantees, based on a majority-with-threshold decoding scheme.", "published": "2025-06-15 17:49:35", "link": "http://arxiv.org/abs/2506.12924v1", "categories": ["cs.IT", "math.CO", "math.IT"], "primary_category": "cs.IT"}
{"title": "Lempel-Ziv Complexity, Empirical Entropies, and Chain Rules", "abstract": "We derive upper and lower bounds on the overall compression ratio of the 1978\nLempel-Ziv (LZ78) algorithm, applied independently to $k$-blocks of a finite\nindividual sequence. Both bounds are given in terms of normalized empirical\nentropies of the given sequence. For the bounds to be tight and meaningful, the\norder of the empirical entropy should be small relative to $k$ in the upper\nbound, but large relative to $k$ in the lower bound. Several non-trivial\nconclusions arise from these bounds. One of them is a certain form of a chain\nrule of the Lempel-Ziv (LZ) complexity, which decomposes the joint LZ\ncomplexity of two sequences, say, $\\bx$ and $\\by$, into the sum of the LZ\ncomplexity of $\\bx$ and the conditional LZ complexity of $\\by$ given $\\bx$ (up\nto small terms). The price of this decomposition, however, is in changing the\nlength of the block. Additional conclusions are discussed as well.", "published": "2025-06-15 08:42:58", "link": "http://arxiv.org/abs/2506.12772v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Zero-shot denoising via neural compression: Theoretical and algorithmic framework", "abstract": "Zero-shot denoising aims to denoise observations without access to training\nsamples or clean reference images. This setting is particularly relevant in\npractical imaging scenarios involving specialized domains such as medical\nimaging or biology. In this work, we propose the Zero-Shot Neural Compression\nDenoiser (ZS-NCD), a novel denoising framework based on neural compression.\nZS-NCD treats a neural compression network as an untrained model, optimized\ndirectly on patches extracted from a single noisy image. The final\nreconstruction is then obtained by aggregating the outputs of the trained model\nover overlapping patches. Thanks to the built-in entropy constraints of\ncompression architectures, our method naturally avoids overfitting and does not\nrequire manual regularization or early stopping. Through extensive experiments,\nwe show that ZS-NCD achieves state-of-the-art performance among zero-shot\ndenoisers for both Gaussian and Poisson noise, and generalizes well to both\nnatural and non-natural images. Additionally, we provide new finite-sample\ntheoretical results that characterize upper bounds on the achievable\nreconstruction error of general maximum-likelihood compression-based denoisers.\nThese results further establish the theoretical foundations of\ncompression-based denoising. Our code is available at:\ngithub.com/Computational-Imaging-RU/ZS-NCDenoiser.", "published": "2025-06-15 02:29:32", "link": "http://arxiv.org/abs/2506.12693v1", "categories": ["eess.IV", "cs.CV", "cs.IT", "math.IT"], "primary_category": "eess.IV"}
{"title": "SIC-Free Rate-Splitting Multiple Access: Constellation-Constrained Optimization and Application to Large-Scale Systems", "abstract": "Rate-Splitting Multiple Access (RSMA) has been recognized as a promising\nmultiple access technique for future wireless communication systems. Recent\nresearch demonstrates that RSMA can maintain its superiority without relying on\nSuccessive Interference Cancellation (SIC) receivers. In practical systems,\nSIC-free receivers are more attractive than SIC receivers because of their low\ncomplexity and latency. This paper evaluates the theoretical limits of RSMA\nwith and without SIC receivers under finite constellations. We first derive the\nconstellation-constrained rate expressions for RSMA. We then design algorithms\nbased on projected subgradient ascent to optimize the precoders and maximize\nthe weighted sum-rate or max-min fairness (MMF) among users. To apply the\nproposed optimization algorithms to large-scale systems, one challenge lies in\nthe exponentially increasing computational complexity brought about by the\nconstellation-constrained rate expressions. In light of this, we propose\nmethods to avoid such computational burden. Numerical results show that, under\noptimized precoders, SIC-free RSMA leads to minor losses in weighted sum-rate\nand MMF performance in comparison to RSMA with SIC receivers, making it a\nviable option for future implementations.", "published": "2025-06-15 00:20:21", "link": "http://arxiv.org/abs/2506.12668v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Homeostatic Coupling for Prosocial Behavior", "abstract": "When regarding the suffering of others, we often experience personal distress\nand feel compelled to help\\footnote{Preprint. Under review.}. Inspired by\nliving systems, we investigate the emergence of prosocial behavior among\nautonomous agents that are motivated by homeostatic self-regulation. We perform\nmulti-agent reinforcement learning, treating each agent as a vulnerable\nhomeostat charged with maintaining its own well-being. We introduce an\nempathy-like mechanism to share homeostatic states between agents: an agent can\neither \\emph{observe} their partner's internal state ({\\bf cognitive empathy})\nor the agent's internal state can be \\emph{directly coupled} to that of their\npartner ({\\bf affective empathy}). In three simple multi-agent environments, we\nshow that prosocial behavior arises only under homeostatic coupling - when the\ndistress of a partner can affect one's own well-being. Additionally, we show\nthat empathy can be learned: agents can ``decode\" their partner's external\nemotive states to infer the partner's internal homeostatic states. Assuming\nsome level of physiological similarity, agents reference their own\nemotion-generation functions to invert the mapping from outward display to\ninternal state. Overall, we demonstrate the emergence of prosocial behavior\nwhen homeostatic agents learn to ``read\" the emotions of others and then to\nempathize, or feel as they feel.", "published": "2025-06-15 15:49:21", "link": "http://arxiv.org/abs/2506.12894v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Recovery of initial displacement and velocity in anisotropic elastic systems by the time dimensional reduction method", "abstract": "We introduce a time-dimensional reduction method for the inverse source\nproblem in linear elasticity, where the goal is to reconstruct the initial\ndisplacement and velocity fields from partial boundary measurements of elastic\nwave propagation. The key idea is to employ a novel spectral representation in\ntime, using an orthonormal basis composed of Legendre polynomials weighted by\nexponential functions. This Legendre polynomial-exponential basis enables a\nstable and accurate decomposition in the time variable, effectively reducing\nthe original space-time inverse problem to a sequence of coupled spatial\nelasticity systems that no longer depend on time. These resulting systems are\nsolved using the quasi-reversibility method. On the theoretical side, we\nestablish a convergence theorem ensuring the stability and consistency of the\nregularized solution obtained by the quasi-reversibility method as the noise\nlevel tends to zero. On the computational side, two-dimensional numerical\nexperiments confirm the theory and demonstrate the method's ability to\naccurately reconstruct both the geometry and amplitude of the initial data,\neven in the presence of substantial measurement noise. The results highlight\nthe effectiveness of the proposed framework as a robust and computationally\nefficient strategy for inverse elastic source problems.", "published": "2025-06-15 23:58:36", "link": "http://arxiv.org/abs/2506.13000v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Pointwise-in-time error bounds for semilinear and quasilinear fractional subdiffusion equations on graded meshes", "abstract": "Time-fractional semilinear and quasilinear parabolic equations with a Caputo\ntime derivative of order $\\alpha\\in(0,1)$ are considered, solutions of which\nexhibit a singular behaviour at an initial time of type $t^\\sigma$ for any\nfixed $\\sigma \\in (0,1) \\cup (1,2)$. The L1 scheme in time is combined with a\ngeneral class of discretizations for the semilinear term. For such\ndiscretizations, we obtain sharp pointwise-in-time error bounds on graded\ntemporal meshes with arbitrary degree of grading. Both semi-discretizations in\ntime and full discretizations using finite differences and finite elements in\nspace are addressed. The theoretcal findings are illustrated by numerical\nexperiments.", "published": "2025-06-15 19:58:52", "link": "http://arxiv.org/abs/2506.12954v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Geometric Multigrid Preconditioner for Discontinuous Galerkin Shifted Boundary Method", "abstract": "This paper introduces a geometric multigrid preconditioner for the Shifted\nBoundary Method (SBM) designed to solve PDEs on complex geometries. While SBM\nsimplifies mesh generation by using a non-conforming background grid, it often\nresults in non-symmetric and potentially ill-conditioned linear systems that\nare challenging to solve efficiently. Standard multigrid methods with pointwise\nsmoothers prove ineffective for such systems due to the localized perturbations\nintroduced by the shifted boundary conditions. To address this challenge, we\nintroduce a Discontinuous Galerkin (DG) formulation for SBM that enables the\ndesign of a cell-wise multiplicative smoother within an $hp$-multigrid\nframework. The element-local nature of DG methods naturally facilitates\ncell-wise correction, which can effectively handle the local complexities\narising from the boundary treatment. Numerical results for the Poisson equation\ndemonstrate favorable performance with mesh refinement for linear ($p=1$) and\nquadratic ($p=2$) elements in both 2D and 3D, with iteration counts showing\nmild growth. However, challenges emerge for cubic ($p=3$) elements,\nparticularly in 3D, where the current smoother shows reduced effectiveness.", "published": "2025-06-15 16:07:03", "link": "http://arxiv.org/abs/2506.12899v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "PDEfuncta: Spectrally-Aware Neural Representation for PDE Solution Modeling", "abstract": "Scientific machine learning often involves representing complex solution\nfields that exhibit high-frequency features such as sharp transitions,\nfine-scale oscillations, and localized structures. While implicit neural\nrepresentations (INRs) have shown promise for continuous function modeling,\ncapturing such high-frequency behavior remains a challenge-especially when\nmodeling multiple solution fields with a shared network. Prior work addressing\nspectral bias in INRs has primarily focused on single-instance settings,\nlimiting scalability and generalization. In this work, we propose Global\nFourier Modulation (GFM), a novel modulation technique that injects\nhigh-frequency information at each layer of the INR through Fourier-based\nreparameterization. This enables compact and accurate representation of\nmultiple solution fields using low-dimensional latent vectors. Building upon\nGFM, we introduce PDEfuncta, a meta-learning framework designed to learn\nmulti-modal solution fields and support generalization to new tasks. Through\nempirical studies on diverse scientific problems, we demonstrate that our\nmethod not only improves representational quality but also shows potential for\nforward and inverse inference tasks without the need for retraining.", "published": "2025-06-15 09:41:25", "link": "http://arxiv.org/abs/2506.12790v1", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.comp-ph"], "primary_category": "cs.LG"}
{"title": "Permutation-Avoiding FFT-Based Convolution", "abstract": "Fast Fourier Transform (FFT) libraries are widely used for evaluating\ndiscrete convolutions. Most FFT implementations follow some variant of the\nCooley-Tukey framework, in which the transform is decomposed into butterfly\noperations and index-reversal permutations. While butterfly operations dominate\nthe floating-point operation count, the memory access patterns induced by\nindex-reversal permutations significantly degrade the FFT's arithmetic\nintensity. In practice, discrete convolutions are often applied repeatedly with\na fixed filter. In such cases, we show that the index-reversal permutations\ninvolved in both the forward and backward transforms of standard FFT-based\nconvolution implementations can be avoided by deferring to a single offline\npermutation of the filter. We propose a multi-dimensional, permutation-avoiding\nconvolution procedure within a general radix Cooley-Tukey framework. We perform\nnumerical experiments to benchmark our algorithms against state-of-the-art\nFFT-based convolution implementations. Our results suggest that developers of\nFFT libraries should consider supporting permutation-avoiding convolution\nkernels.", "published": "2025-06-15 04:50:02", "link": "http://arxiv.org/abs/2506.12718v1", "categories": ["math.NA", "cs.DS", "cs.MS", "cs.NA"], "primary_category": "math.NA"}
{"title": "Computing the Bogoliubov-de Gennes excitations of two-component Bose-Einstein condensates", "abstract": "In this paper, we present an efficient and spectrally accurate numerical\nmethod to compute elementary/collective excitations in two-component\nBose-Einstein condensates (BEC), around their mean-field ground state, by\nsolving the associated Bogoliubov-de Gennes (BdG) equation. The BdG equation is\nessentially an eigenvalue problem for a non-Hermitian differential operator\nwith an eigenfunction normalization constraint. Firstly, we investigate its\nanalytical properties, including the exact eigenpairs, generalized nullspace\nstructure and bi-orthogonality of eigenspaces. Subsequently, by combining the\nFourier spectral method for spatial discretization and a stable modified\nGram-Schmidt bi-orthogonal algorithm, we propose a structure-preserving\niterative method for the resulting large-scale dense non-Hermitian discrete\neigenvalue problem. Our method is matrix-free, and the matrix-vector\nmultiplication (or the operator-function evaluation) is implemented with a\nnear-optimal complexity ${\\mathcal O}(N_{\\rm t}\\log(N_{\\rm t}))$, where $N_{\\rm\nt}$ is the total number of grid points, thanks to the utilization of the\ndiscrete Fast Fourier Transform (FFT). Therefore, it is memory-friendly,\nspectrally accurate, and highly efficient. Finally, we carry out a\ncomprehensive numerical investigation to showcase its superiority in terms of\naccuracy and efficiency, alongside some applications to compute the excitation\nspectrum and Bogoliubov amplitudes in one, two, and three-dimensional problems.", "published": "2025-06-15 02:21:58", "link": "http://arxiv.org/abs/2506.12688v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Implied Probabilities and Volatility in Credit Risk: A Merton-Based Approach with Binomial Trees", "abstract": "We explore credit risk pricing by modeling equity as a call option and debt\nas the difference between the firm's asset value and a put option, following\nthe structural framework of the Merton model. Our approach proceeds in two\nstages: first, we calibrate the asset volatility using the Black-Scholes-Merton\n(BSM) formula; second, we recover implied mean return and probability surfaces\nunder the physical measure. To achieve this, we construct a recombining\nbinomial tree under the real-world (natural) measure, assuming a fixed initial\nasset value. The volatility input is taken from a specific region of the\nimplied volatility surface - based on moneyness and maturity - which then\ninforms the calibration of drift and probability. A novel mapping is\nestablished between risk-neutral and physical parameters, enabling construction\nof implied surfaces that reflect the market's credit expectations and offer\npractical tools for stress testing and credit risk analysis.", "published": "2025-06-15 02:37:25", "link": "http://arxiv.org/abs/2506.12694v1", "categories": ["q-fin.RM", "q-fin.CP"], "primary_category": "q-fin.RM"}
{"title": "Distributional Training Data Attribution", "abstract": "Randomness is an unavoidable part of training deep learning models, yet\nsomething that traditional training data attribution algorithms fail to\nrigorously account for. They ignore the fact that, due to stochasticity in the\ninitialisation and batching, training on the same dataset can yield different\nmodels. In this paper, we address this shortcoming through introducing\ndistributional training data attribution (d-TDA), the goal of which is to\npredict how the distribution of model outputs (over training runs) depends upon\nthe dataset. We demonstrate the practical significance of d-TDA in experiments,\ne.g. by identifying training examples that drastically change the distribution\nof some target measurement without necessarily changing the mean. Intriguingly,\nwe also find that influence functions (IFs), a popular but poorly-understood\ndata attribution tool, emerge naturally from our distributional framework as\nthe limit to unrolled differentiation; without requiring restrictive convexity\nassumptions. This provides a new mathematical motivation for their efficacy in\ndeep learning, and helps to characterise their limitations.", "published": "2025-06-15 21:02:36", "link": "http://arxiv.org/abs/2506.12965v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Logit Dynamics in Softmax Policy Gradient Methods", "abstract": "We analyzes the logit dynamics of softmax policy gradient methods. We derive\nthe exact formula for the L2 norm of the logit update vector: $$ \\|\\Delta\n\\mathbf{z}\\|_2 \\propto \\sqrt{1-2P_c + C(P)} $$ This equation demonstrates that\nupdate magnitudes are determined by the chosen action's probability ($P_c$) and\nthe policy's collision probability ($C(P)$), a measure of concentration\ninversely related to entropy. Our analysis reveals an inherent self-regulation\nmechanism where learning vigor is automatically modulated by policy confidence,\nproviding a foundational insight into the stability and convergence of these\nmethods.", "published": "2025-06-15 17:02:39", "link": "http://arxiv.org/abs/2506.12912v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Variational Learning Finds Flatter Solutions at the Edge of Stability", "abstract": "Variational Learning (VL) has recently gained popularity for training deep\nneural networks and is competitive to standard learning methods. Part of its\nempirical success can be explained by theories such as PAC-Bayes bounds,\nminimum description length and marginal likelihood, but there are few tools to\nunravel the implicit regularization in play. Here, we analyze the implicit\nregularization of VL through the Edge of Stability (EoS) framework. EoS has\npreviously been used to show that gradient descent can find flat solutions and\nwe extend this result to VL to show that it can find even flatter solutions.\nThis is obtained by controlling the posterior covariance and the number of\nMonte Carlo samples from the posterior. These results are derived in a similar\nfashion as the standard EoS literature for deep learning, by first deriving a\nresult for a quadratic problem and then extending it to deep neural networks.\nWe empirically validate these findings on a wide variety of large networks,\nsuch as ResNet and ViT, to find that the theoretical results closely match the\nempirical ones. Ours is the first work to analyze the EoS dynamics in VL.", "published": "2025-06-15 16:33:02", "link": "http://arxiv.org/abs/2506.12903v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Uncovering Social Network Activity Using Joint User and Topic Interaction", "abstract": "The emergence of online social platforms, such as social networks and social\nmedia, has drastically affected the way people apprehend the information flows\nto which they are exposed. In such platforms, various information cascades\nspreading among users is the main force creating complex dynamics of opinion\nformation, each user being characterized by their own behavior adoption\nmechanism. Moreover, the spread of multiple pieces of information or beliefs in\na networked population is rarely uncorrelated. In this paper, we introduce the\nMixture of Interacting Cascades (MIC), a model of marked multidimensional\nHawkes processes with the capacity to model jointly non-trivial interaction\nbetween cascades and users. We emphasize on the interplay between information\ncascades and user activity, and use a mixture of temporal point processes to\nbuild a coupled user/cascade point process model. Experiments on synthetic and\nreal data highlight the benefits of this approach and demonstrate that MIC\nachieves superior performance to existing methods in modeling the spread of\ninformation cascades. Finally, we demonstrate how MIC can provide, through its\nlearned parameters, insightful bi-layered visualizations of real social network\nactivity data.", "published": "2025-06-15 13:30:22", "link": "http://arxiv.org/abs/2506.12842v1", "categories": ["cs.SI", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Fair Bayesian Model-Based Clustering", "abstract": "Fair clustering has become a socially significant task with the advancement\nof machine learning technologies and the growing demand for trustworthy AI.\nGroup fairness ensures that the proportions of each sensitive group are similar\nin all clusters. Most existing group-fair clustering methods are based on the\n$K$-means clustering and thus require the distance between instances and the\nnumber of clusters to be given in advance. To resolve this limitation, we\npropose a fair Bayesian model-based clustering called Fair Bayesian Clustering\n(FBC). We develop a specially designed prior which puts its mass only on fair\nclusters, and implement an efficient MCMC algorithm. Advantages of FBC are that\nit can infer the number of clusters and can be applied to any data type as long\nas the likelihood is defined (e.g., categorical data). Experiments on\nreal-world datasets show that FBC (i) reasonably infers the number of clusters,\n(ii) achieves a competitive utility-fairness trade-off compared to existing\nfair clustering methods, and (iii) performs well on categorical data.", "published": "2025-06-15 13:16:32", "link": "http://arxiv.org/abs/2506.12839v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "General and Estimable Learning Bound Unifying Covariate and Concept Shifts", "abstract": "Generalization under distribution shift remains a core challenge in modern\nmachine learning, yet existing learning bound theory is limited to narrow,\nidealized settings and is non-estimable from samples. In this paper, we bridge\nthe gap between theory and practical applications. We first show that existing\nbounds become loose and non-estimable because their concept shift definition\nbreaks when the source and target supports mismatch. Leveraging entropic\noptimal transport, we propose new support-agnostic definitions for covariate\nand concept shifts, and derive a novel unified error bound that applies to\nbroad loss functions, label spaces, and stochastic labeling. We further develop\nestimators for these shifts with concentration guarantees, and the DataShifts\nalgorithm, which can quantify distribution shifts and estimate the error bound\nin most applications -- a rigorous and general tool for analyzing learning\nerror under distribution shift.", "published": "2025-06-15 12:18:05", "link": "http://arxiv.org/abs/2506.12829v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Review of the Long Horizon Forecasting Problem in Time Series Analysis", "abstract": "The long horizon forecasting (LHF) problem has come up in the time series\nliterature for over the last 35 years or so. This review covers aspects of LHF\nin this period and how deep learning has incorporated variants of trend,\nseasonality, fourier and wavelet transforms, misspecification bias reduction\nand bandpass filters while contributing using convolutions, residual\nconnections, sparsity reduction, strided convolutions, attention masks, SSMs,\nnormalization methods, low-rank approximations and gating mechanisms. We\nhighlight time series decomposition techniques, input data preprocessing and\ndataset windowing schemes that improve performance. Multi-layer perceptron\nmodels, recurrent neural network hybrids, self-attention models that improve\nand/or address the performances of the LHF problem are described, with an\nemphasis on the feature space construction. Ablation studies are conducted over\nthe ETTm2 dataset in the multivariate and univariate high useful load (HUFL)\nforecasting contexts, evaluated over the last 4 months of the dataset. The\nheatmaps of MSE averages per time step over test set series in the horizon show\nthat there is a steady increase in the error proportionate to its length except\nwith xLSTM and Triformer models and motivate LHF as an error propagation\nproblem. The trained models are available here: https://bit.ly/LHFModelZoo", "published": "2025-06-15 10:49:50", "link": "http://arxiv.org/abs/2506.12809v1", "categories": ["cs.LG", "cs.ET", "cs.PF", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Single Index Bandits: Generalized Linear Contextual Bandits with Unknown Reward Functions", "abstract": "Generalized linear bandits have been extensively studied due to their broad\napplicability in real-world online decision-making problems. However, these\nmethods typically assume that the expected reward function is known to the\nusers, an assumption that is often unrealistic in practice. Misspecification of\nthis link function can lead to the failure of all existing algorithms. In this\nwork, we address this critical limitation by introducing a new problem of\ngeneralized linear bandits with unknown reward functions, also known as single\nindex bandits. We first consider the case where the unknown reward function is\nmonotonically increasing, and propose two novel and efficient algorithms, STOR\nand ESTOR, that achieve decent regrets under standard assumptions. Notably, our\nESTOR can obtain the nearly optimal regret bound $\\tilde{O}_T(\\sqrt{T})$ in\nterms of the time horizon $T$. We then extend our methods to the\nhigh-dimensional sparse setting and show that the same regret rate can be\nattained with the sparsity index. Next, we introduce GSTOR, an algorithm that\nis agnostic to general reward functions, and establish regret bounds under a\nGaussian design assumption. Finally, we validate the efficiency and\neffectiveness of our algorithms through experiments on both synthetic and\nreal-world datasets.", "published": "2025-06-15 07:19:00", "link": "http://arxiv.org/abs/2506.12751v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "On the attainment of the Wasserstein--Cramer--Rao lower bound", "abstract": "Recently, a Wasserstein analogue of the Cramer--Rao inequality has been\ndeveloped using the Wasserstein information matrix (Otto metric). This\ninequality provides a lower bound on the Wasserstein variance of an estimator,\nwhich quantifies its robustness against additive noise. In this study, we\ninvestigate conditions for an estimator to attain the Wasserstein--Cramer--Rao\nlower bound (asymptotically), which we call the (asymptotic) Wasserstein\nefficiency. We show a condition under which Wasserstein efficient estimators\nexist for one-parameter statistical models. This condition corresponds to a\nrecently proposed Wasserstein analogue of one-parameter exponential families\n(e-geodesics). We also show that the Wasserstein estimator, a Wasserstein\nanalogue of the maximum likelihood estimator based on the Wasserstein score\nfunction, is asymptotically Wasserstein efficient in location-scale families.", "published": "2025-06-15 05:56:12", "link": "http://arxiv.org/abs/2506.12732v1", "categories": ["math.ST", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Strategic Scaling of Test-Time Compute: A Bandit Learning Approach", "abstract": "Scaling test-time compute has emerged as an effective strategy for improving\nthe performance of large language models. However, existing methods typically\nallocate compute uniformly across all queries, overlooking variation in query\ndifficulty. To address this inefficiency, we formulate test-time compute\nallocation as a novel bandit learning problem and propose adaptive algorithms\nthat estimate query difficulty on the fly and allocate compute accordingly.\nCompared to uniform allocation, our algorithms allocate more compute to\nchallenging queries while maintaining accuracy on easier ones. Among\nchallenging queries, our algorithms further learn to prioritize solvable\ninstances, effectively reducing excessive computing on unsolvable queries. We\ntheoretically prove that our algorithms achieve better compute efficiency than\nuniform allocation and empirically validate their effectiveness on math and\ncode benchmarks. Specifically, our algorithms achieve up to an 11.10%\nperformance improvement (15.04% relative) on the MATH-500 dataset and up to a\n7.41% performance improvement (14.40% relative) on LiveCodeBench.", "published": "2025-06-15 04:55:49", "link": "http://arxiv.org/abs/2506.12721v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Effect Decomposition of Functional-Output Computer Experiments via Orthogonal Additive Gaussian Processes", "abstract": "Functional ANOVA (FANOVA) is a widely used variance-based sensitivity\nanalysis tool. However, studies on functional-output FANOVA remain relatively\nscarce, especially for black-box computer experiments, which often involve\ncomplex and nonlinear functional-output relationships with unknown data\ndistribution. Conventional approaches often rely on predefined basis functions\nor parametric structures that lack the flexibility to capture complex nonlinear\nrelationships. Additionally, strong assumptions about the underlying data\ndistributions further limit their ability to achieve a data-driven orthogonal\neffect decomposition. To address these challenges, this study proposes a\nfunctional-output orthogonal additive Gaussian process (FOAGP) to efficiently\nperform the data-driven orthogonal effect decomposition. By enforcing a\nconditional orthogonality constraint on the separable prior process, the\nproposed functional-output orthogonal additive kernel enables data-driven\northogonality without requiring prior distributional assumptions. The FOAGP\nframework also provides analytical formulations for local Sobol' indices and\nexpected conditional variance sensitivity indices, enabling comprehensive\nsensitivity analysis by capturing both global and local effect significance.\nValidation through two simulation studies and a real case study on fuselage\nshape control confirms the model's effectiveness in orthogonal effect\ndecomposition and variance decomposition, demonstrating its practical value in\nengineering applications.", "published": "2025-06-15 03:24:55", "link": "http://arxiv.org/abs/2506.12701v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Dependent Randomized Rounding for Budget Constrained Experimental Design", "abstract": "Policymakers in resource-constrained settings require experimental designs\nthat satisfy strict budget limits while ensuring precise estimation of\ntreatment effects. We propose a framework that applies a dependent randomized\nrounding procedure to convert assignment probabilities into binary treatment\ndecisions. Our proposed solution preserves the marginal treatment probabilities\nwhile inducing negative correlations among assignments, leading to improved\nestimator precision through variance reduction. We establish theoretical\nguarantees for the inverse propensity weighted and general linear estimators,\nand demonstrate through empirical studies that our approach yields efficient\nand accurate inference under fixed budget constraints.", "published": "2025-06-15 01:10:06", "link": "http://arxiv.org/abs/2506.12677v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding", "abstract": "As an emerging paradigm of brain-computer interfaces (BCIs), speech BCI has\nthe potential to directly reflect auditory perception and thoughts, offering a\npromising communication alternative for patients with aphasia. Chinese is one\nof the most widely spoken languages in the world, whereas there is very limited\nresearch on speech BCIs for Chinese language. This paper reports a\ntext-magnetoencephalography (MEG) dataset for non-invasive Chinese speech BCIs.\nIt also proposes a multi-modality assisted speech decoding (MASD) algorithm to\ncapture both text and acoustic information embedded in brain signals during\nspeech activities. Experiment results demonstrated the effectiveness of both\nour text-MEG dataset and our proposed MASD algorithm. To our knowledge, this is\nthe first study on modality-assisted decoding for non-invasive speech BCIs.", "published": "2025-06-15 11:34:06", "link": "http://arxiv.org/abs/2506.12817v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frequency Dynamic Convolutions for Sound Event Detection", "abstract": "Recent research in deep learning-based Sound Event Detection (SED) has\nprimarily focused on Convolutional Recurrent Neural Networks (CRNNs) and\nTransformer models. However, conventional 2D convolution-based models assume\nshift invariance along both the temporal and frequency axes, leadin to\ninconsistencies when dealing with frequency-dependent characteristics of\nacoustic signals. To address this issue, this study proposes Frequency Dynamic\nConvolution (FDY conv), which dynamically adjusts convolutional kernels based\non the frequency composition of the input signal to enhance SED performance.\nFDY conv constructs an optimal frequency response by adaptively weighting\nmultiple basis kernels based on frequency-specific attention weights.\nExperimental results show that applying FDY conv to CRNNs improves performance\non the DESED dataset by 7.56% compared to the baseline CRNN. However, FDY conv\nhas limitations in that it combines basis kernels of the same shape across all\nfrequencies, restricting its ability to capture diverse frequency-specific\ncharacteristics. Additionally, the $3\\times3$ basis kernel size is insufficient\nto capture a broader frequency range. To overcome these limitations, this study\nintroduces an extended family of FDY conv models. Dilated FDY conv (DFD conv)\napplies convolutional kernels with various dilation rates to expand the\nreceptive field along the frequency axis and enhance frequency-specific feature\nrepresentation. Experimental results show that DFD conv improves performance by\n9.27% over the baseline. Partial FDY conv (PFD conv) addresses the high\ncomputational cost of FDY conv, which results from performing all convolution\noperations with dynamic kernels. Since FDY conv may introduce unnecessary\nadaptivity for quasi-stationary sound events, PFD conv integrates standard 2D\nconvolutions with frequency-adaptive kernels to reduce computational complexity\nwhile maintaining performance. Experimental results demonstrate that PFD conv\nimproves performance by 7.80% over the baseline while reducing the number of\nparameters by 54.4% compared to FDY conv. Multi-Dilated FDY conv (MDFD conv)\nextends DFD conv by addressing its structural limitation of applying the same\ndilation across all frequencies. By utilizing multiple convolutional kernels\nwith different dilation rates, MDFD conv effectively captures diverse\nfrequency-dependent patterns. Experimental results indicate that MDFD conv\nachieves the highest performance, improving the baseline CRNN performance by\n10.98%. Furthermore, standard FDY conv employs Temporal Average Pooling, which\nassigns equal weight to all frames along the time axis, limiting its ability to\neffectively capture transient events. To overcome this, this study proposes\nTAP-FDY conv (TFD conv), which integrates Temporal Attention Pooling (TA) that\nfocuses on salient features, Velocity Attention Pooling (VA) that emphasizes\ntransient characteristics, and Average Pooling (AP) that captures stationary\nproperties. TAP-FDY conv achieves the same performance as MDFD conv but reduces\nthe number of parameters by approximately 30.01% (12.703M vs. 18.157M),\nachieving equivalent accuracy with lower computational complexity. Class-wise\nperformance analysis reveals that FDY conv improves detection of non-stationary\nevents, DFD conv is particularly effective for events with broad spectral\nfeatures, and PFD conv enhances the detection of quasi-stationary events.\nAdditionally, TFD conv (TFD-CRNN) demonstrates strong performance in detecting\ntransient events. In the case studies, PFD conv effectively captures stable\nsignal patterns in tank powertrain fault recognition, DFD conv recognizes wide\nharmonic spectral patterns on speed-varying motor fault recognition, while TFD\nconv outperforms other models in detecting transient signals in offshore arc\ndetection. These results suggest that frequency-adaptive convolutions and their\nextended variants provide a robust alternative to conventional 2D convolutions\nin deep learning-based audio processing.", "published": "2025-06-15 09:32:16", "link": "http://arxiv.org/abs/2506.12785v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration", "abstract": "Trouble hearing in noisy situations remains a common complaint for both\nindividuals with hearing loss and individuals with normal hearing. This is\nhypothesized to arise due to condition called: cochlear neural degeneration\n(CND) which can also result in significant variabilities in hearing aids\noutcomes. This paper uses computational models of auditory periphery to\nsimulate various hearing tasks. We present an objective method to quantify\nhearing loss and CND by comparing auditory nerve fiber responses using a\nNeurogram Similarity Index Measure (NSIM). Specifically study 1, shows that\nNSIM can be used to map performance of individuals with hearing loss on phoneme\nrecognition task with reasonable accuracy. In the study 2, we show that NSIM is\na sensitive measure that can also be used to capture the deficits resulting\nfrom CND and can be a candidate for noninvasive biomarker of auditory\nsynaptopathy.", "published": "2025-06-15 03:33:44", "link": "http://arxiv.org/abs/2506.12705v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition", "abstract": "We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an\nenhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT\nhandles overlapped speech, and we found the decoder performs implicit speaker\nseparation. We hypothesize this implicit separation is often insufficient due\nto ambiguous acoustic cues in overlapping regions. To address this, SC-SOT\nexplicitly conditions the decoder on speaker information, providing detailed\ninformation about \"who spoke when\". Specifically, we enhance the decoder by\nincorporating: (1) speaker embeddings, which allow the model to focus on the\nacoustic characteristics of the target speaker, and (2) speaker activity\ninformation, which guides the model to suppress non-target speakers. The\nspeaker embeddings are derived from a jointly trained E2E speaker diarization\nmodel, mitigating the need for speaker enrollment. Experimental results\ndemonstrate the effectiveness of our conditioning approach on overlapped\nspeech.", "published": "2025-06-15 00:37:27", "link": "http://arxiv.org/abs/2506.12672v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MORIC: CSI Delay-Doppler Decomposition for Robust Wi-Fi-based Human Activity Recognition", "abstract": "The newly established IEEE 802.11bf Task Group aims to amend the WLAN\nstandard to support advanced sensing applications such as human activity\nrecognition (HAR). Although studies have demonstrated the potential of sub-7\nGHz Wi-Fi Channel State Information (CSI) for HAR, no method currently performs\nreliably in real-world scenarios. This work tackles the poor generalization of\nWi-Fi-based HAR by introducing an innovative approach to extracting and\nutilizing movement-related representations, which makes it robust to noise and\nstatic environmental properties. This is achieved by transforming CSI signals\ninto the delay profile space and decomposing them into various Doppler\nvelocities, which serve as informative projections of a mobile point's velocity\nfrom different unknown random angles. To mitigate the impact of this\nrandomness, MORIC is introduced as a novel time series classification model\nbased on random convolutional kernels, designed to be invariant to the random\norder and repetition of input representations, thereby enabling robust Wi-Fi\nCSI-based activity classification. Experimental results on the collected\ndataset demonstrate that the proposed method outperforms state-of-the-art\napproaches in terms of generalization accuracy for hand motion recognition,\nparticularly for challenging gestures. Furthermore, incorporating a small\nnumber of calibration samples leads to a significant improvement in accuracy,\nenhancing the practicality of the method for real-world deployment.", "published": "2025-06-15 23:46:49", "link": "http://arxiv.org/abs/2506.12997v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Interference Mitigation in STAR-RIS-Aided Multi-User Networks with Statistical CSI", "abstract": "In this paper, we investigate real-time interference mitigation in multiuser\nwireless networks assisted by simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RISs). Unlike conventional methods\nthat rely on instantaneous channel state information (CSI), we consider a\npractical scenario where only statistical CSI is available, and the STAR-RIS\nphase shifts are impaired by random phase errors modeled via the Von Mises\ndistribution. To tackle the resulting nonconvex optimization problem induced by\nunit-modulus constraints and stochastic interference, we derive a closed-form\napproximation of the effective channel matrix using statistical expectations.\nWe then reformulate the interference minimization problem as an unconstrained\noptimization over a Riemannian manifold and propose a conjugate gradient\nalgorithm tailored to the complex circle manifold. The proposed solution\nenables efficient real-time computation of optimal phase shifts while\naccounting for hardware imperfections and limited CSI. Simulation results\nconfirm that our method significantly suppresses inter-user interference and\nachieves superior SINR performance and convergence speed compared to\nconventional baselines.", "published": "2025-06-15 20:57:04", "link": "http://arxiv.org/abs/2506.12964v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Low-Latency Terrestrial Interference Detection for Satellite-to-Device Communications", "abstract": "Direct satellite-to-device communication is a promising future direction due\nto its lower latency and enhanced efficiency. However, intermittent and\nunpredictable terrestrial interference significantly affects system reliability\nand performance. Continuously employing sophisticated interference mitigation\ntechniques is practically inefficient. Motivated by the periodic idle intervals\ncharacteristic of burst-mode satellite transmissions, this paper investigates\nonline interference detection frameworks specifically tailored for\nsatellite-to-device scenarios. We first rigorously formulate interference\ndetection as a binary hypothesis testing problem, leveraging differences\nbetween Rayleigh (no interference) and Rice (interference present)\ndistributions. Then, we propose a cumulative sum (CUSUM)-based online detector\nfor scenarios with known interference directions, explicitly characterizing the\ntrade-off between detection latency and false alarm rate, and establish its\nasymptotic optimality. For practical scenarios involving unknown interference\ndirection, we further propose a generalized likelihood ratio (GLR)-based\ndetection method, jointly estimating interference direction via the Root-MUSIC\nalgorithm. Numerical results validate our theoretical findings and demonstrate\nthat our proposed methods achieve high detection accuracy with remarkably low\nlatency, highlighting their practical applicability in future\nsatellite-to-device communication systems.", "published": "2025-06-15 16:53:45", "link": "http://arxiv.org/abs/2506.12908v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network", "abstract": "Integrated sensing and communication (ISAC) within sub-THz frequencies is\ncrucial for future air-ground networks, but unique propagation characteristics\nand hardware limitations present challenges in optimizing ISAC performance\nwhile increasing operational latency. This paper introduces a multi-modal\nsensing fusion framework inspired by synesthesia of machine (SoM) to enhance\nsub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz\nhardware and channels, the framework optimizes the radio-frequency environment.\nSquint-aware beam management is developed to improve air-ground network\nadaptability, enabling three-dimensional dynamic ISAC links. Leveraging\nmulti-modal information, the framework enhances ISAC performance and reduces\nlatency. Visual data rapidly localizes users and targets, while a customized\nmulti-modal learning algorithm optimizes the hybrid precoder. A new metric\nprovides comprehensive performance evaluation, and extensive experiments\ndemonstrate that the proposed scheme significantly improves ISAC efficiency.", "published": "2025-06-15 12:30:14", "link": "http://arxiv.org/abs/2506.12831v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Spatially Consistent Air-to-Ground Channel Modeling with Probabilistic LOS/NLOS Segmentation", "abstract": "In this paper, we present a spatially consistent A2G channel model based on\nprobabilistic LOS/NLOS segmentation to parameterize the deterministic path loss\nand stochastic shadow fading model. Motivated by the limitations of existing\nUnmanned Aerial Vehicle (UAV) channel models that overlook spatial correlation,\nour approach reproduces LOS/NLOS transitions along ground user trajectories in\nurban environments. This model captures environment-specific obstructions by\nmeans of azimuth and elevation-dependent LOS probabilities without requiring a\nfull detailed 3D representation of the surroundings. We validate our framework\nagainst a geometry-based simulator by evaluating it across various urban\nsettings. The results demonstrate its accuracy and computational efficiency,\nenabling further realistic derivations of path loss and shadow fading models\nand thorough outage analysis.", "published": "2025-06-15 09:52:14", "link": "http://arxiv.org/abs/2506.12794v1", "categories": ["cs.ET", "eess.SP"], "primary_category": "cs.ET"}
{"title": "Dynamic Scheduling for Enhanced Performance in RIS-assisted Cooperative Network with Interference", "abstract": "Reconfigurable Intelligent Surfaces (RIS) have emerged as transformative\ntechnologies, enhancing spectral efficiency and improving interference\nmanagement in multi-user cooperative communications. This paper investigates\nthe integration of RIS with Flexible-Duplex (FlexD) communication, featuring\ndynamic scheduling capabilities, to mitigate unintended external interference\nin multi-user wireless networks. By leveraging the reconfigurability of RIS and\ndynamic scheduling, we propose a user-pair selection scheme to maximize system\nthroughput when full channel state information (CSI) of interference is\nunavailable. We develop a mathematical framework to evaluate the throughput\noutage probability when RIS introduces spatial correlation. The derived\nanalytical results are used for asymptotic analysis, providing insights into\ndynamic user scheduling under interference based on statistical channel\nknowledge. Finally, we compare FlexD with traditional Full Duplex (FD) and Half\nDuplex (HD) systems against RIS-assisted FlexD. Our results show FlexD's\nsuperior throughput enhancement, energy efficiency and data management\ncapability in interference-affected networks, typical in current and\nnext-generation cooperative wireless applications like cellular and vehicular\ncommunications.", "published": "2025-06-15 09:08:27", "link": "http://arxiv.org/abs/2506.12778v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Conditional Diffusion Model-Driven Generative Channels for Double RIS-Aided Wireless Systems", "abstract": "With the development of the upcoming sixth-generation networks (6G),\nreconfigurable intelligent surfaces (RISs) have gained significant attention\ndue to its ability of reconfiguring wireless channels via smart reflections.\nHowever, traditional channel state information (CSI) acquisition techniques for\ndouble-RIS systems face challenges (e.g., high pilot overhead or multipath\ninterference). This paper proposes a new channel generation method in\ndouble-RIS communication systems based on the tool of conditional diffusion\nmodel (CDM). The CDM is trained on synthetic channel data to capture channel\ncharacteristics. It addresses the limitations of traditional CSI generation\nmethods, such as insufficient model understanding capability and poor\nenvironmental adaptability. We provide a detailed analysis of the diffusion\nprocess for channel generation, and it is validated through simulations. The\nsimulation results demonstrate that the proposed CDM based method outperforms\ntraditional channel acquisition methods in terms of normalized mean squared\nerror (NMSE). This method offers a new paradigm for channel acquisition in\ndouble-RIS systems, which is expected to improve the quality of channel\nacquisition with low pilot overhead.", "published": "2025-06-15 01:48:24", "link": "http://arxiv.org/abs/2506.12682v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
