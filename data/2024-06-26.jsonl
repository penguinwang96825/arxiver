{"title": "LSTM-ARIMA as a Hybrid Approach in Algorithmic Investment Strategies", "abstract": "This study focuses on building an algorithmic investment strategy employing a\nhybrid approach that combines LSTM and ARIMA models referred to as LSTM-ARIMA.\nThis unique algorithm uses LSTM to produce final predictions but boosts the\nresults of this RNN by adding the residuals obtained from ARIMA predictions\namong other inputs. The algorithm is tested across three equity indices (S&P\n500, FTSE 100, and CAC 40) using daily frequency data from January 2000 to\nAugust 2023. The testing architecture is based on the walk-forward procedure\nfor the hyperparameter tunning phase that uses Random Search and backtesting\nthe algorithms. The selection of the optimal model is determined based on\nadequately selected performance metrics focused on risk-adjusted return\nmeasures. We considered two strategies for each algorithm: Long-Only and\nLong-Short to present the situation of two various groups of investors with\ndifferent investment policy restrictions. For each strategy and equity index,\nwe compute the performance metrics and visualize the equity curve to identify\nthe best strategy with the highest modified information ratio. The findings\nconclude that the LSTM-ARIMA algorithm outperforms all the other algorithms\nacross all the equity indices which confirms the strong potential behind hybrid\nML-TS (machine learning - time series) models in searching for the optimal\nalgorithmic investment strategies.", "published": "2024-06-26 09:39:08", "link": "http://arxiv.org/abs/2406.18206v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "LLMs for Doctors: Leveraging Medical LLMs to Assist Doctors, Not Replace\n  Them", "abstract": "The recent success of Large Language Models (LLMs) has had a significant\nimpact on the healthcare field, providing patients with medical advice,\ndiagnostic information, and more. However, due to a lack of professional\nmedical knowledge, patients are easily misled by generated erroneous\ninformation from LLMs, which may result in serious medical problems. To address\nthis issue, we focus on tuning the LLMs to be medical assistants who\ncollaborate with more experienced doctors. We first conduct a two-stage survey\nby inspiration-feedback to gain a broad understanding of the real needs of\ndoctors for medical assistants. Based on this, we construct a Chinese medical\ndataset called DoctorFLAN to support the entire workflow of doctors, which\nincludes 92K Q\\&A samples from 22 tasks and 27 specialists. Moreover, we\nevaluate LLMs in doctor-oriented scenarios by constructing the\nDoctorFLAN-\\textit{test} containing 550 single-turn Q\\&A and DotaBench\ncontaining 74 multi-turn conversations. The evaluation results indicate that\nbeing a medical assistant still poses challenges for existing open-source\nmodels, but DoctorFLAN can help them significantly. It demonstrates that the\ndoctor-oriented dataset and benchmarks we construct can complement existing\npatient-oriented work and better promote medical LLMs research.", "published": "2024-06-26 03:08:24", "link": "http://arxiv.org/abs/2406.18034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n  Strong LLM Is All You Need", "abstract": "We present a comprehensive study of answer quality evaluation in\nRetrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel\ngrading system that is designed to assess correctness, completeness, and\nhonesty. We further map the grading of quality aspects aforementioned into a\nbinary score, indicating an accept or reject decision, mirroring the intuitive\n\"thumbs-up\" or \"thumbs-down\" gesture commonly used in chat applications. This\napproach suits factual business contexts where a clear decision opinion is\nessential. Our assessment applies vRAG-Eval to two Large Language Models\n(LLMs), evaluating the quality of answers generated by a vanilla RAG\napplication. We compare these evaluations with human expert judgments and find\na substantial alignment between GPT-4's assessments and those of human experts,\nreaching 83% agreement on accept or reject decisions. This study highlights the\npotential of LLMs as reliable evaluators in closed-domain, closed-ended\nsettings, particularly when human evaluations require significant resources.", "published": "2024-06-26 04:49:41", "link": "http://arxiv.org/abs/2406.18064v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Knowledge Graph Completion from Pretrained Language Models\n  with Knowledge Constraints", "abstract": "Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like\n(h, r, ?) in different languages by reasoning a tail entity t thus improving\nmultilingual knowledge graphs. Previous studies leverage multilingual\npretrained language models (PLMs) and the generative paradigm to achieve mKGC.\nAlthough multilingual pretrained language models contain extensive knowledge of\ndifferent languages, its pretraining tasks cannot be directly aligned with the\nmKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit\na pronounced English-centric bias. This makes it difficult for mKGC to achieve\ngood results, particularly in the context of low-resource languages. To\novercome previous problems, this paper introduces global and local knowledge\nconstraints for mKGC. The former is used to constrain the reasoning of answer\nentities, while the latter is used to enhance the representation of query\ncontexts. The proposed method makes the pretrained model better adapt to the\nmKGC task. Experimental results on public datasets demonstrate that our method\noutperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and\n16.03%, which indicates that our proposed method has significant enhancement on\nmKGC.", "published": "2024-06-26 05:46:35", "link": "http://arxiv.org/abs/2406.18085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shimo Lab at \"Discharge Me!\": Discharge Summarization by Prompt-Driven\n  Concatenation of Electronic Health Record Sections", "abstract": "In this paper, we present our approach to the shared task \"Discharge Me!\" at\nthe BioNLP Workshop 2024. The primary goal of this task is to reduce the time\nand effort clinicians spend on writing detailed notes in the electronic health\nrecord (EHR). Participants develop a pipeline to generate the \"Brief Hospital\nCourse\" and \"Discharge Instructions\" sections from the EHR. Our approach\ninvolves a first step of extracting the relevant sections from the EHR. We then\nadd explanatory prompts to these sections and concatenate them with separate\ntokens to create the input text. To train a text generation model, we perform\nLoRA fine-tuning on the ClinicalT5-large model. On the final test data, our\napproach achieved a ROUGE-1 score of $0.394$, which is comparable to the top\nsolutions.", "published": "2024-06-26 06:10:20", "link": "http://arxiv.org/abs/2406.18094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConvoCache: Smart Re-Use of Chatbot Responses", "abstract": "We present ConvoCache, a conversational caching system that solves the\nproblem of slow and expensive generative AI models in spoken chatbots.\nConvoCache finds a semantically similar prompt in the past and reuses the\nresponse. In this paper we evaluate ConvoCache on the DailyDialog dataset. We\nfind that ConvoCache can apply a UniEval coherence threshold of 90% and respond\nto 89% of prompts using the cache with an average latency of 214ms, replacing\nLLM and voice synthesis that can take over 1s. To further reduce latency we\ntest prefetching and find limited usefulness. Prefetching with 80% of a request\nleads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be\nused with any chatbot to reduce costs by reducing usage of generative AI by up\nto 89%.", "published": "2024-06-26 07:35:10", "link": "http://arxiv.org/abs/2406.18133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "abstract": "Retrieval-augmented generation has gained popularity as a framework to\nenhance large language models with external knowledge. However, its\neffectiveness hinges on the retrieval robustness of the model. If the model\nlacks retrieval robustness, its performance is constrained by the accuracy of\nthe retriever, resulting in significant compromises when the retrieved context\nis irrelevant. In this paper, we evaluate the \"implicit\" retrieval robustness\nof various large language models, instructing them to directly output the final\nanswer without explicitly judging the relevance of the retrieved context. Our\nfindings reveal that fine-tuning on a mix of gold and distracting context\nsignificantly enhances the model's robustness to retrieval inaccuracies, while\nstill maintaining its ability to extract correct answers when retrieval is\naccurate. This suggests that large language models can implicitly handle\nrelevant or irrelevant retrieved context by learning solely from the\nsupervision of the final answer in an end-to-end manner. Introducing an\nadditional process for explicit relevance judgment can be unnecessary and\ndisrupts the end-to-end approach.", "published": "2024-06-26 07:38:24", "link": "http://arxiv.org/abs/2406.18134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs", "abstract": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.", "published": "2024-06-26 08:44:36", "link": "http://arxiv.org/abs/2406.18173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative\n  Decoding", "abstract": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across\nvarious tasks, yet fall short of complex reasoning and planning tasks. The\ntree-search-based reasoning methods address this by surpassing the capabilities\nof chain-of-thought prompting, encouraging exploration of intermediate steps.\nHowever, such methods introduce significant inference latency due to the\nsystematic exploration and evaluation of multiple thought paths. This paper\nintroduces SeeD, a novel and efficient inference framework to optimize runtime\nspeed and GPU memory management concurrently. By employing a scheduled\nspeculative execution, SeeD efficiently handles multiple iterations for the\nthought generation and the state evaluation, leveraging a rounds-scheduled\nstrategy to manage draft model dispatching. Extensive experimental evaluations\non three reasoning datasets demonstrate superior speedup performance of SeeD,\nproviding a viable path for batched inference in training-free speculative\ndecoding.", "published": "2024-06-26 09:33:41", "link": "http://arxiv.org/abs/2406.18200v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak Reward Model Transforms Generative Models into Robust Causal Event\n  Extraction Systems", "abstract": "The inherent ambiguity of cause and effect boundaries poses a challenge in\nevaluating causal event extraction tasks. Traditional metrics like Exact Match\nand BertScore poorly reflect model performance, so we trained evaluation models\nto approximate human evaluation, achieving high agreement. We used them to\nperform Reinforcement Learning with extraction models to align them with human\npreference, prioritising semantic understanding. We successfully explored our\napproach through multiple datasets, including transferring an evaluator trained\non one dataset to another as a way to decrease the reliance on human-annotated\ndata. In that vein, we also propose a weak-to-strong supervision method that\nuses a fraction of the annotated data to train an evaluation model while still\nachieving high performance in training an RL model. Our code is available at\nhttps://github.com/oyarsa/event_extraction/tree/causal-event-extraction.", "published": "2024-06-26 10:48:14", "link": "http://arxiv.org/abs/2406.18245v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Llamipa: An Incremental Discourse Parser", "abstract": "This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.", "published": "2024-06-26 11:08:17", "link": "http://arxiv.org/abs/2406.18256v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Vorbe\u015fti Rom\u00e2ne\u015fte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions", "abstract": "In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages.", "published": "2024-06-26 11:39:51", "link": "http://arxiv.org/abs/2406.18266v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with\n  Repository-Level Pretrained Code LLMs", "abstract": "Some recently developed code large language models (Code LLMs) have been\npre-trained on repository-level code data (Repo-Code LLMs), enabling these\nmodels to recognize repository structures and utilize cross-file information\nfor code completion. However, in real-world development scenarios, simply\nconcatenating the entire code repository often exceeds the context window\nlimits of these Repo-Code LLMs, leading to significant performance degradation.\nIn this study, we conducted extensive preliminary experiments and analyses on\nsix Repo-Code LLMs. The results indicate that maintaining the topological\ndependencies of files and increasing the code file content in the completion\nprompts can improve completion accuracy; pruning the specific implementations\nof functions in all dependent files does not significantly reduce the accuracy\nof completions. Based on these findings, we proposed a strategy named\nHierarchical Context Pruning (HCP) to construct completion prompts with high\ninformational code content. The HCP models the code repository at the function\nlevel, maintaining the topological dependencies between code files while\nremoving a large amount of irrelevant code content, significantly reduces the\ninput length for repository-level code completion. We applied the HCP strategy\nin experiments with six Repo-Code LLMs, and the results demonstrate that our\nproposed method can significantly enhance completion accuracy while\nsubstantially reducing the length of input. Our code and data are available at\nhttps://github.com/Hambaobao/HCP-Coder.", "published": "2024-06-26 12:26:16", "link": "http://arxiv.org/abs/2406.18294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactFinders at CheckThat! 2024: Refining Check-worthy Statement\n  Detection with LLMs through Data Pruning", "abstract": "The rapid dissemination of information through social media and the Internet\nhas posed a significant challenge for fact-checking, among others in\nidentifying check-worthy claims that fact-checkers should pay attention to,\ni.e. filtering claims needing fact-checking from a large pool of sentences.\nThis challenge has stressed the need to focus on determining the priority of\nclaims, specifically which claims are worth to be fact-checked. Despite\nadvancements in this area in recent years, the application of large language\nmodels (LLMs), such as GPT, has only recently drawn attention in studies.\nHowever, many open-source LLMs remain underexplored. Therefore, this study\ninvestigates the application of eight prominent open-source LLMs with\nfine-tuning and prompt engineering to identify check-worthy statements from\npolitical transcriptions. Further, we propose a two-step data pruning approach\nto automatically identify high-quality training data instances for effective\nlearning. The efficiency of our approach is demonstrated through evaluations on\nthe English language dataset as part of the check-worthiness estimation task of\nCheckThat! 2024. Further, the experiments conducted with data pruning\ndemonstrate that competitive performance can be achieved with only about 44\\%\nof the training data. Our team ranked first in the check-worthiness estimation\ntask in the English language.", "published": "2024-06-26 12:31:31", "link": "http://arxiv.org/abs/2406.18297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar Assistance Using Syntactic Structures (GAUSS)", "abstract": "Automatic grammar coaching serves an important purpose of advising on\nstandard grammar varieties while not imposing social pressures or reinforcing\nestablished social roles. Such systems already exist but most of them are for\nEnglish and few of them offer meaningful feedback. Furthermore, they typically\nrely completely on neural methods and require huge computational resources\nwhich most of the world cannot afford. We propose a grammar coaching system for\nSpanish that relies on (i) a rich linguistic formalism capable of giving\ninformative feedback; and (ii) a faster parsing algorithm which makes using\nthis formalism practical in a real-world application. The approach is feasible\nfor any language for which there is a computerized grammar and is less reliant\non expensive and environmentally costly neural methods. We seek to contribute\nto Greener AI and to address global education challenges by raising the\nstandards of inclusivity and engagement in grammar coaching.", "published": "2024-06-26 13:35:10", "link": "http://arxiv.org/abs/2406.18340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Themis: A Reference-free NLG Evaluation Language Model with Flexibility\n  and Interpretability", "abstract": "The evaluation of natural language generation (NLG) tasks is a significant\nand longstanding research area. With the recent emergence of powerful large\nlanguage models (LLMs), some studies have turned to LLM-based automatic\nevaluation methods, which demonstrate great potential to become a new\nevaluation paradigm following traditional string-based and model-based metrics.\nHowever, despite the improved performance of existing methods, they still\npossess some deficiencies, such as dependency on references and limited\nevaluation flexibility. Therefore, in this paper, we meticulously construct a\nlarge-scale NLG evaluation corpus NLG-Eval with annotations from both human and\nGPT-4 to alleviate the lack of relevant data in this field. Furthermore, we\npropose Themis, an LLM dedicated to NLG evaluation, which has been trained with\nour designed multi-perspective consistency verification and rating-oriented\npreference alignment methods. Themis can conduct flexible and interpretable\nevaluations without references, and it exhibits superior evaluation performance\non various NLG tasks, simultaneously generalizing well to unseen tasks and\nsurpassing other evaluation models, including GPT-4.", "published": "2024-06-26 14:04:29", "link": "http://arxiv.org/abs/2406.18365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks", "abstract": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.", "published": "2024-06-26 14:56:13", "link": "http://arxiv.org/abs/2406.18403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks,\n  and Refusals of LLMs", "abstract": "We introduce WildGuard -- an open, light-weight moderation tool for LLM\nsafety that achieves three goals: (1) identifying malicious intent in user\nprompts, (2) detecting safety risks of model responses, and (3) determining\nmodel refusal rate. Together, WildGuard serves the increasing needs for\nautomatic safety moderation and evaluation of LLM interactions, providing a\none-stop tool with enhanced accuracy and broad coverage across 13 risk\ncategories. While existing open moderation tools such as Llama-Guard2 score\nreasonably well in classifying straightforward model interactions, they lag far\nbehind a prompted GPT-4, especially in identifying adversarial jailbreaks and\nin evaluating models' refusals, a key measure for evaluating safety behaviors\nin model responses.\n  To address these challenges, we construct WildGuardMix, a large-scale and\ncarefully balanced multi-task safety moderation dataset with 92K labeled\nexamples that cover vanilla (direct) prompts and adversarial jailbreaks, paired\nwith various refusal and compliance responses. WildGuardMix is a combination of\nWildGuardTrain, the training data of WildGuard, and WildGuardTest, a\nhigh-quality human-annotated moderation test set with 5K labeled items covering\nbroad risk scenarios. Through extensive evaluations on WildGuardTest and ten\nexisting public benchmarks, we show that WildGuard establishes state-of-the-art\nperformance in open-source safety moderation across all the three tasks\ncompared to ten strong existing open-source moderation models (e.g., up to\n26.4% improvement on refusal detection). Importantly, WildGuard matches and\nsometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt\nharmfulness identification). WildGuard serves as a highly effective safety\nmoderator in an LLM interface, reducing the success rate of jailbreak attacks\nfrom 79.8% to 2.4%.", "published": "2024-06-26 16:58:20", "link": "http://arxiv.org/abs/2406.18495v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is In-Context Learning a Type of Gradient-Based Learning? Evidence from\n  the Inverse Frequency Effect in Structural Priming", "abstract": "Large language models (LLMs) have shown the emergent capability of in-context\nlearning (ICL). One line of research has explained ICL as functionally\nperforming gradient descent. In this paper, we introduce a new way of\ndiagnosing whether ICL is functionally equivalent to gradient-based learning.\nOur approach is based on the inverse frequency effect (IFE) -- a phenomenon in\nwhich an error-driven learner is expected to show larger updates when trained\non infrequent examples than frequent ones. The IFE has previously been studied\nin psycholinguistics because humans show this effect in the context of\nstructural priming (the tendency for people to produce sentence structures they\nhave encountered recently); the IFE has been used as evidence that human\nstructural priming must involve error-driven learning mechanisms. In our\nexperiments, we simulated structural priming within ICL and found that LLMs\ndisplay the IFE, with the effect being stronger in larger models. We conclude\nthat ICL is indeed a type of gradient-based learning, supporting the hypothesis\nthat a gradient component is implicitly computed in the forward pass during\nICL. Our results suggest that both humans and LLMs make use of gradient-based,\nerror-driven processing mechanisms.", "published": "2024-06-26 17:06:41", "link": "http://arxiv.org/abs/2406.18501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)\n  Safer Language Models", "abstract": "We introduce WildTeaming, an automatic LLM safety red-teaming framework that\nmines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of\nnovel jailbreak tactics, and then composes multiple tactics for systematic\nexploration of novel jailbreaks. Compared to prior work that performed\nred-teaming via recruited human workers, gradient-based optimization, or\niterative revision with LLMs, our work investigates jailbreaks from chatbot\nusers who were not specifically instructed to break the system. WildTeaming\nreveals previously unidentified vulnerabilities of frontier LLMs, resulting in\nup to 4.6x more diverse and successful adversarial attacks compared to\nstate-of-the-art jailbreak methods.\n  While many datasets exist for jailbreak evaluation, very few open-source\ndatasets exist for jailbreak training, as safety training data has been closed\neven when model weights are open. With WildTeaming we create WildJailbreak, a\nlarge-scale open-source synthetic safety dataset with 262K vanilla (direct\nrequest) and adversarial (complex jailbreak) prompt-response pairs. To mitigate\nexaggerated safety behaviors, WildJailbreak provides two contrastive types of\nqueries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that\nresemble harmful queries in form but contain no harm. As WildJailbreak\nconsiderably upgrades the quality and scale of existing safety resources, it\nuniquely enables us to examine the scaling effects of data and the interplay of\ndata properties and model capabilities during safety training. Through\nextensive experiments, we identify the training properties that enable an ideal\nbalance of safety behaviors: appropriate safeguarding without over-refusal,\neffective handling of vanilla and adversarial queries, and minimal, if any,\ndecrease in general capabilities. All components of WildJailbeak contribute to\nachieving balanced safety behaviors of models.", "published": "2024-06-26 17:31:22", "link": "http://arxiv.org/abs/2406.18510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Is ChatGPT a Better Explainer than My Professor?\": Evaluating the\n  Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline", "abstract": "Explanations form the foundation of knowledge sharing and build upon\ncommunication principles, social dynamics, and learning theories. We focus\nspecifically on conversational approaches for explanations because the context\nis highly adaptive and interactive. Our research leverages previous work on\nexplanatory acts, a framework for understanding the different strategies that\nexplainers and explainees employ in a conversation to both explain, understand,\nand engage with the other party. We use the 5-Levels dataset was constructed\nfrom the WIRED YouTube series by Wachsmuth et al., and later annotated by\nBooshehri et al. with explanatory acts. These annotations provide a framework\nfor understanding how explainers and explainees structure their response when\ncrafting a response.\n  With the rise of generative AI in the past year, we hope to better understand\nthe capabilities of Large Language Models (LLMs) and how they can augment\nexpert explainer's capabilities in conversational settings. To achieve this\ngoal, the 5-Levels dataset (We use Booshehri et al.'s 2023 annotated dataset\nwith explanatory acts.) allows us to audit the ability of LLMs in engaging in\nexplanation dialogues. To evaluate the effectiveness of LLMs in generating\nexplainer responses, we compared 3 different strategies, we asked human\nannotators to evaluate 3 different strategies: human explainer response, GPT4\nstandard response, GPT4 response with Explanation Moves.", "published": "2024-06-26 17:33:51", "link": "http://arxiv.org/abs/2406.18512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine\n  Translation and Summarization Evaluation", "abstract": "Large language models (LLMs) have revolutionized NLP research. Notably,\nin-context learning enables their use as evaluation metrics for natural\nlanguage generation, making them particularly advantageous in low-resource\nscenarios and time-restricted applications. In this work, we introduce PrExMe,\na large-scale Prompt Exploration for Metrics, where we evaluate more than 720\nprompt templates for open-source LLM-based metrics on machine translation (MT)\nand summarization datasets, totalling over 6.6M evaluations. This extensive\ncomparison (1) benchmarks recent open-source LLMs as metrics and (2) explores\nthe stability and variability of different prompting strategies. We discover\nthat, on the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.", "published": "2024-06-26 17:56:29", "link": "http://arxiv.org/abs/2406.18528v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence Graph Network for Online Debate Analysis", "abstract": "Online debates involve a dynamic exchange of ideas over time, where\nparticipants need to actively consider their opponents' arguments, respond with\ncounterarguments, reinforce their own points, and introduce more compelling\narguments as the discussion unfolds. Modeling such a complex process is not a\nsimple task, as it necessitates the incorporation of both sequential\ncharacteristics and the capability to capture interactions effectively. To\naddress this challenge, we employ a sequence-graph approach. Building the\nconversation as a graph allows us to effectively model interactions between\nparticipants through directed edges. Simultaneously, the propagation of\ninformation along these edges in a sequential manner enables us to capture a\nmore comprehensive representation of context. We also introduce a Sequence\nGraph Attention layer to illustrate the proposed information update scheme. The\nexperimental results show that sequence graph networks achieve superior results\nto existing methods in online debates.", "published": "2024-06-26 18:58:23", "link": "http://arxiv.org/abs/2406.18696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Categorical Syllogisms Revisited: A Review of the Logical Reasoning\n  Abilities of LLMs for Analyzing Categorical Syllogism", "abstract": "There have been a huge number of benchmarks proposed to evaluate how large\nlanguage models (LLMs) behave for logic inference tasks. However, it remains an\nopen question how to properly evaluate this ability. In this paper, we provide\na systematic overview of prior works on the logical reasoning ability of LLMs\nfor analyzing categorical syllogisms. We first investigate all the possible\nvariations for the categorical syllogisms from a purely logical perspective and\nthen examine the underlying configurations (i.e., mood and figure) tested by\nthe existing datasets. Our results indicate that compared to template-based\nsynthetic datasets, crowdsourcing approaches normally sacrifice the coverage of\nconfigurations (i.e., mood and figure) of categorical syllogisms for more\nlanguage variations, thus bringing challenges to fully testing LLMs under\ndifferent situations. We then proceed to summarize the findings and\nobservations for the performances of LLMs to infer the validity of syllogisms\nfrom the current literature. The error rate breakdown analyses suggest that the\ninterpretation of the quantifiers seems to be the current bottleneck that\nlimits the performances of the LLMs and is thus worth more attention. Finally,\nwe discuss several points that might be worth considering when researchers plan\non the future release of categorical syllogism datasets. We hope our work will\nnot only provide a timely review of the current literature regarding\ncategorical syllogisms, but also motivate more interdisciplinary research\nbetween communities, specifically computational linguists and logicians.", "published": "2024-06-26 21:17:20", "link": "http://arxiv.org/abs/2406.18762v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Discourse Relation Classification For Nigerian Pidgin", "abstract": "Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.", "published": "2024-06-26 22:10:15", "link": "http://arxiv.org/abs/2406.18776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-step Inference over Unstructured Data", "abstract": "The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.", "published": "2024-06-26 00:00:45", "link": "http://arxiv.org/abs/2406.17987v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Catching Chameleons: Detecting Evolving Disinformation Generated using\n  Large Language Models", "abstract": "Despite recent advancements in detecting disinformation generated by large\nlanguage models (LLMs), current efforts overlook the ever-evolving nature of\nthis disinformation. In this work, we investigate a challenging yet practical\nresearch problem of detecting evolving LLM-generated disinformation.\nDisinformation evolves constantly through the rapid development of LLMs and\ntheir variants. As a consequence, the detection model faces significant\nchallenges. First, it is inefficient to train separate models for each\ndisinformation generator. Second, the performance decreases in scenarios when\nevolving LLM-generated disinformation is encountered in sequential order. To\naddress this problem, we propose DELD (Detecting Evolving LLM-generated\nDisinformation), a parameter-efficient approach that jointly leverages the\ngeneral fact-checking capabilities of pre-trained language models (PLM) and the\nindependent disinformation generation characteristics of various LLMs. In\nparticular, the learned characteristics are concatenated sequentially to\nfacilitate knowledge accumulation and transformation. DELD addresses the issue\nof label scarcity by integrating the semantic embeddings of disinformation with\ntrainable soft prompts to elicit model-specific knowledge. Our experiments show\nthat \\textit{DELD} significantly outperforms state-of-the-art methods.\nMoreover, our method provides critical insights into the unique patterns of\ndisinformation generation across different LLMs, offering valuable perspectives\nin this line of research.", "published": "2024-06-26 00:21:39", "link": "http://arxiv.org/abs/2406.17992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding with Limited Teacher Supervision Requires Understanding When to\n  Trust the Teacher", "abstract": "How can small-scale large language models (LLMs) efficiently utilize the\nsupervision of LLMs to improve their generative quality? This question has been\nwell studied in scenarios where there is no restriction on the number of LLM\nsupervisions one can use, giving birth to many decoding algorithms that utilize\nsupervision without further training. However, it is still unclear what is an\neffective strategy under the $\\textit{limited supervision}$ scenario, where we\nassume that no more than a few tokens can be generated by LLMs. To this end, we\ndevelop an algorithm to effectively aggregate the small-scale LLM and LLM\npredictions on initial tokens so that the generated tokens can more accurately\ncondition the subsequent token generation by small-scale LLM only. Critically,\nwe find that it is essential to adaptively overtrust or disregard the LLM\nprediction based on the confidence of the small-scale LLM. Through our\nexperiments on a wide range of models and datasets, we demonstrate that our\nmethod provides a consistent improvement over conventional decoding strategies.\n$\\small$ $\\textbf{Code:}$ https://github.com/HJ-Ok/DecLimSup", "published": "2024-06-26 01:16:12", "link": "http://arxiv.org/abs/2406.18002v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs", "abstract": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods.", "published": "2024-06-26 02:49:28", "link": "http://arxiv.org/abs/2406.18027v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical\n  and Chemistry", "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus tailored to the\nBio-Pharmaceutical and Chemical domains. Our evaluation shows that PharmaGPT\nsurpasses existing general models on specific-domain benchmarks such as NAPLEX,\ndemonstrating its exceptional capability in domain-specific tasks. Remarkably,\nthis performance is achieved with a model that has only a fraction, sometimes\njust one-tenth-of the parameters of general-purpose large models. This\nadvancement establishes a new benchmark for LLMs in the bio-pharmaceutical and\nchemical fields, addressing the existing gap in specialized language modeling.\nIt also suggests a promising path for enhanced research and development, paving\nthe way for more precise and effective NLP applications in these areas.", "published": "2024-06-26 03:43:09", "link": "http://arxiv.org/abs/2406.18045v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Entity Recognition Using Ensembles of Deep Learning and\n  Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction\n  from Multiple Sources", "abstract": "Adverse event (AE) extraction following COVID-19 vaccines from text data is\ncrucial for monitoring and analyzing the safety profiles of immunizations.\nTraditional deep learning models are adept at learning intricate feature\nrepresentations and dependencies in sequential data, but often require\nextensive labeled data. In contrast, large language models (LLMs) excel in\nunderstanding contextual information, but exhibit unstable performance on named\nentity recognition tasks, possibly due to their broad but unspecific training.\nThis study aims to evaluate the effectiveness of LLMs and traditional deep\nlearning models in AE extraction, and to assess the impact of ensembling these\nmodels on performance. In this study, we utilized reports and posts from the\nVAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal\nwas to extract three types of entities: \"vaccine\", \"shot\", and \"ae\". We\nexplored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,\nGPT-4, and Llama-2, as well as traditional deep learning models like RNN and\nBioBERT. To enhance performance, we created ensembles of the three models with\nthe best performance. For evaluation, we used strict and relaxed F1 scores to\nevaluate the performance for each entity type, and micro-average F1 was used to\nassess the overall performance. The ensemble model achieved the highest\nperformance in \"vaccine\", \"shot\", and \"ae\" with strict F1-scores of 0.878,\n0.930, and 0.925, respectively, along with a micro-average score of 0.903. In\nconclusion, this study demonstrates the effectiveness and robustness of\nensembling fine-tuned traditional deep learning models and LLMs, for extracting\nAE-related information. This study contributes to the advancement of biomedical\nnatural language processing, providing valuable insights into improving AE\nextraction from text data for pharmacovigilance and public health surveillance.", "published": "2024-06-26 03:56:21", "link": "http://arxiv.org/abs/2406.18049v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Energy-Based Models for Out-of-Distribution Detection in\n  Dialect Identification", "abstract": "The diverse nature of dialects presents challenges for models trained on\nspecific linguistic patterns, rendering them susceptible to errors when\nconfronted with unseen or out-of-distribution (OOD) data. This study introduces\na novel margin-enhanced joint energy model (MEJEM) tailored specifically for\nOOD detection in dialects. By integrating a generative model and the energy\nmargin loss, our approach aims to enhance the robustness of dialect\nidentification systems. Furthermore, we explore two OOD scores for OOD dialect\ndetection, and our findings conclusively demonstrate that the energy score\noutperforms the softmax score. Leveraging Sharpness-Aware Minimization to\noptimize the training process of the joint model, we enhance model\ngeneralization by minimizing both loss and sharpness. Experiments conducted on\ndialect identification tasks validate the efficacy of Energy-Based Models and\nprovide valuable insights into their performance.", "published": "2024-06-26 04:52:48", "link": "http://arxiv.org/abs/2406.18067v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad\n  Prediction", "abstract": "Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect\nterm, aspect category, opinion term, sentiment polarity) for a given review,\nwhich is the most representative and challenging task in aspect-based sentiment\nanalysis. A key challenge in the ASQP task is the scarcity of labeled data,\nwhich limits the performance of existing methods. To tackle this issue, we\npropose a self-training framework with a pseudo-label scorer, wherein a scorer\nassesses the match between reviews and their pseudo-labels, aiming to filter\nout mismatches and thereby enhance the effectiveness of self-training. We\nhighlight two critical aspects to ensure the scorer's effectiveness and\nreliability: the quality of the training dataset and its model architecture. To\nthis end, we create a human-annotated comparison dataset and train a generative\nmodel on it using ranking-based objectives. Extensive experiments on public\nASQP datasets reveal that using our scorer can greatly and consistently improve\nthe effectiveness of self-training. Moreover, we explore the possibility of\nreplacing humans with large language models for comparison dataset annotation,\nand experiments demonstrate its feasibility. We release our code and data at\nhttps://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .", "published": "2024-06-26 05:30:21", "link": "http://arxiv.org/abs/2406.18078v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Octo-planner: On-device Language Model for Planner-Action Agents", "abstract": "AI agents have become increasingly significant in various domains, enabling\nautonomous decision-making and problem-solving. To function effectively, these\nagents require a planning process that determines the best course of action and\nthen executes the planned actions. In this paper, we present an efficient\non-device Planner-Action framework that separates planning and action execution\ninto two distinct components: a planner agent based on Phi-3 Mini, a 3.8\nbillion parameter LLM optimized for edge devices, and an action agent using the\nOctopus model for function execution. The planner agent first responds to user\nqueries by decomposing tasks into a sequence of sub-steps, which are then\nexecuted by the action agent. To optimize performance on resource-constrained\ndevices, we employ model fine-tuning instead of in-context learning, reducing\ncomputational costs and energy consumption while improving response times. Our\napproach involves using GPT-4 to generate diverse planning queries and\nresponses based on available functions, with subsequent validations to ensure\ndata quality. We fine-tune the Phi-3 Mini model on this curated dataset,\nachieving a 97\\% success rate in our in-domain test environment. To address\nmulti-domain planning challenges, we developed a multi-LoRA training method\nthat merges weights from LoRAs trained on distinct function subsets. This\napproach enables flexible handling of complex, multi-domain queries while\nmaintaining computational efficiency on resource-constrained devices. To\nsupport further research, we have open-sourced our model weights at\n\\url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please\nrefer to \\url{https://www.nexa4ai.com/octo-planner}.", "published": "2024-06-26 05:40:10", "link": "http://arxiv.org/abs/2406.18082v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance", "abstract": "As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.", "published": "2024-06-26 07:15:44", "link": "http://arxiv.org/abs/2406.18118v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Poisoned LangChain: Jailbreak LLMs by LangChain", "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.", "published": "2024-06-26 07:21:02", "link": "http://arxiv.org/abs/2406.18122v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal\n  Long-Context Inference", "abstract": "Long-context Multimodal Large Language Models (MLLMs) demand substantial\ncomputational resources for inference as the growth of their multimodal\nKey-Value (KV) cache, in response to increasing input lengths, challenges\nmemory and time efficiency. Unlike single-modality LLMs that manage only\ntextual contexts, the KV cache of long-context MLLMs includes representations\nfrom multiple images with temporal and spatial relationships and related\ntextual contexts. The predominance of image tokens means traditional\noptimizations for LLMs' KV caches are unsuitable for multimodal long-context\nsettings, and no prior works have addressed this challenge. In this work, we\nintroduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently\nreduces the multimodal KV cache size while maintaining performance comparable\nto a full cache. We observe that during prompt prefill, the model prioritizes\nmore textual attention over image features, and based on the multimodal\ninteraction observation, a new proposed text-prior method is explored to\ncompress the KV cache. Furthermore, to mitigate the degradation of image\ncontextual information, we propose several compensatory strategies using KV\npairs merging. LOOK-M demonstrates that with a significant reduction in KV\nCache memory usage, such as reducing it by 80% in some cases, it not only\nachieves up to 1.5x faster decoding but also maintains or even enhances\nperformance across a variety of long context multimodal tasks.", "published": "2024-06-26 07:44:24", "link": "http://arxiv.org/abs/2406.18139v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Nebula: A discourse aware Minecraft Builder", "abstract": "When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset", "published": "2024-06-26 08:24:44", "link": "http://arxiv.org/abs/2406.18164v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Methodology of Adapting Large English Language Models for Specific\n  Cultural Contexts", "abstract": "The rapid growth of large language models(LLMs) has emerged as a prominent\ntrend in the field of artificial intelligence. However, current\nstate-of-the-art LLMs are predominantly based on English. They encounter\nlimitations when directly applied to tasks in specific cultural domains, due to\ndeficiencies in domain-specific knowledge and misunderstandings caused by\ndifferences in cultural values. To address this challenge, our paper proposes a\nrapid adaptation method for large models in specific cultural contexts, which\nleverages instruction-tuning based on specific cultural knowledge and safety\nvalues data. Taking Chinese as the specific cultural context and utilizing the\nLLaMA3-8B as the experimental English LLM, the evaluation results demonstrate\nthat the adapted LLM significantly enhances its capabilities in domain-specific\nknowledge and adaptability to safety values, while maintaining its original\nexpertise advantages.", "published": "2024-06-26 09:16:08", "link": "http://arxiv.org/abs/2406.18192v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Closer Look into Mixture-of-Experts in Large Language Models", "abstract": "Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree popular MoE-based models and reveal some intriguing observations,\nincluding 1) Neurons act like fine-grained experts; 2) The router of MoE\nusually selects experts with larger output norms; 3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier, which is\nfurther validated by an initial experiment. Based on the observations, we also\nprovide suggestions for a broad spectrum of MoE practitioners, such as router\ndesign and expert allocation. We hope this work could shed light on future\nresearch on the MoE framework and other modular architectures. Code is\navailable at https://github.com/kamanphoebe/Look-into-MoEs.", "published": "2024-06-26 10:07:57", "link": "http://arxiv.org/abs/2406.18219v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing", "abstract": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.", "published": "2024-06-26 10:08:47", "link": "http://arxiv.org/abs/2406.18221v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension", "abstract": "There are substantial instructional videos on the Internet, which provide us\ntutorials for completing various tasks. Existing instructional video datasets\nonly focus on specific steps at the video level, lacking experiential\nguidelines at the task level, which can lead to beginners struggling to learn\nnew tasks due to the lack of relevant experience. Moreover, the specific steps\nwithout guidelines are trivial and unsystematic, making it difficult to provide\na clear tutorial. To address these problems, we present the GUIDE\n(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional\ntasks in 8 domains related to our daily life. Specifically, we annotate each\ninstructional task with a guideline, representing a common pattern shared by\nall task-related videos. On this basis, we annotate systematic specific steps,\nincluding their associated guideline steps, specific step descriptions and\ntimestamps. Our proposed benchmark consists of three sub-tasks to evaluate\ncomprehension ability of models: (1) Step Captioning: models have to generate\ncaptions for specific steps from videos. (2) Guideline Summarization: models\nhave to mine the common pattern in task-related videos and summarize a\nguideline from them. (3) Guideline-Guided Captioning: models have to generate\ncaptions for specific steps under the guide of guideline. We evaluate plenty of\nfoundation models with GUIDE and perform in-depth analysis. Given the diversity\nand practicality of GUIDE, we believe that it can be used as a better benchmark\nfor instructional video comprehension.", "published": "2024-06-26 10:24:00", "link": "http://arxiv.org/abs/2406.18227v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets", "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.", "published": "2024-06-26 10:44:02", "link": "http://arxiv.org/abs/2406.18239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated", "abstract": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.", "published": "2024-06-26 11:11:47", "link": "http://arxiv.org/abs/2406.18259v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sanskrit Knowledge-based Systems: Annotation and Computational Tools", "abstract": "We address the challenges and opportunities in the development of knowledge\nsystems for Sanskrit, with a focus on question answering. By proposing a\nframework for the automated construction of knowledge graphs, introducing\nannotation tools for ontology-driven and general-purpose tasks, and offering a\ndiverse collection of web-interfaces, tools, and software libraries, we have\nmade significant contributions to the field of computational Sanskrit. These\ncontributions not only enhance the accessibility and accuracy of Sanskrit text\nanalysis but also pave the way for further advancements in knowledge\nrepresentation and language processing. Ultimately, this research contributes\nto the preservation, understanding, and utilization of the rich linguistic\ninformation embodied in Sanskrit texts.", "published": "2024-06-26 12:00:10", "link": "http://arxiv.org/abs/2406.18276v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "S3: A Simple Strong Sample-effective Multimodal Dialog System", "abstract": "In this work, we present a conceptually simple yet powerful baseline for the\nmultimodal dialog task, an S3 model, that achieves near state-of-the-art\nresults on two compelling leaderboards: MMMU and AI Journey Contest 2023. The\nsystem is based on a pre-trained large language model, pre-trained modality\nencoders for image and audio, and a trainable modality projector. The proposed\neffective data mixture for training such an architecture demonstrates that a\nmultimodal model based on a strong language model and trained on a small amount\nof multimodal data can perform efficiently in the task of multimodal dialog.", "published": "2024-06-26 12:45:43", "link": "http://arxiv.org/abs/2406.18305v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI-native Memory: A Pathway from LLMs Towards AGI", "abstract": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.", "published": "2024-06-26 12:51:37", "link": "http://arxiv.org/abs/2406.18312v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large\n  Language Models Using Odyssey Math Data", "abstract": "Large language models (LLMs) have significantly advanced natural language\nunderstanding and demonstrated strong problem-solving abilities. Despite these\nsuccesses, most LLMs still struggle with solving mathematical problems due to\nthe intricate reasoning required. This paper investigates the mathematical\nproblem-solving capabilities of LLMs using the newly developed \"MathOdyssey\"\ndataset. The dataset includes diverse mathematical problems at high school and\nuniversity levels, created by experts from notable institutions to rigorously\ntest LLMs in advanced problem-solving scenarios and cover a wider range of\nsubject areas. By providing the MathOdyssey dataset as a resource to the AI\ncommunity, we aim to contribute to the understanding and improvement of AI\ncapabilities in complex mathematical problem-solving. We conduct benchmarking\non open-source models, such as Llama-3 and DBRX-Instruct, and closed-source\nmodels from the GPT series and Gemini models. Our results indicate that while\nLLMs perform well on routine and moderately difficult tasks, they face\nsignificant challenges with Olympiad-level problems and complex\nuniversity-level questions. Our analysis shows a narrowing performance gap\nbetween open-source and closed-source models, yet substantial challenges\nremain, particularly with the most demanding problems. This study highlights\nthe ongoing need for research to enhance the mathematical reasoning of LLMs.\nThe dataset, results, and code are publicly available.", "published": "2024-06-26 13:02:35", "link": "http://arxiv.org/abs/2406.18321v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models", "abstract": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.", "published": "2024-06-26 13:12:40", "link": "http://arxiv.org/abs/2406.18326v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research on Information Extraction of LCSTS Dataset Based on an Improved\n  BERTSum-LSTM Model", "abstract": "With the continuous advancement of artificial intelligence, natural language\nprocessing technology has become widely utilized in various fields. At the same\ntime, there are many challenges in creating Chinese news summaries. First of\nall, the semantics of Chinese news is complex, and the amount of information is\nenormous. Extracting critical information from Chinese news presents a\nsignificant challenge. Second, the news summary should be concise and clear,\nfocusing on the main content and avoiding redundancy. In addition, the\nparticularity of the Chinese language, such as polysemy, word segmentation,\netc., makes it challenging to generate Chinese news summaries. Based on the\nabove, this paper studies the information extraction method of the LCSTS\ndataset based on an improved BERTSum-LSTM model. We improve the BERTSum-LSTM\nmodel to make it perform better in generating Chinese news summaries. The\nexperimental results show that the proposed method has a good effect on\ncreating news summaries, which is of great importance to the construction of\nnews summaries.", "published": "2024-06-26 14:04:15", "link": "http://arxiv.org/abs/2406.18364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons", "abstract": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN.", "published": "2024-06-26 14:57:38", "link": "http://arxiv.org/abs/2406.18406v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascading Large Language Models for Salient Event Graph Generation", "abstract": "Generating event graphs from long documents is challenging due to the\ninherent complexity of multiple tasks involved such as detecting events,\nidentifying their relationships, and reconciling unstructured input with\nstructured graphs. Recent studies typically consider all events with equal\nimportance, failing to distinguish salient events crucial for understanding\nnarratives. This paper presents CALLMSAE, a CAscading Large Language Model\nframework for SAlient Event graph generation, which leverages the capabilities\nof LLMs and eliminates the need for costly human annotations. We first identify\nsalient events by prompting LLMs to generate summaries, from which salient\nevents are identified. Next, we develop an iterative code refinement prompting\nstrategy to generate event relation graphs, removing hallucinated relations and\nrecovering missing edges. Powered by CALLMSAE, we present \\textit{NYT-SEG}, a\nlarge-scale automatically annotated event graph dataset which can serve as\ndistant supervision signals. Fine-tuning contextualised graph generation models\non \\textit{NYT-SEG} outperforms the models trained on CAEVO data. Results on a\nhuman-annotated test set show that the proposed method generates salient and\nmore accurate graphs, outperforming competitive baselines.", "published": "2024-06-26 15:53:54", "link": "http://arxiv.org/abs/2406.18449v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal\n  LLMs", "abstract": "Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/", "published": "2024-06-26 17:50:11", "link": "http://arxiv.org/abs/2406.18521v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation", "abstract": "We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on visual quality and textual relevance of\ngenerated videos, ChronoMagic-Bench focuses on the model's ability to generate\ntime-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.\n[Homepage](https://pku-yuangroup.github.io/ChronoMagic-Bench/).", "published": "2024-06-26 17:50:47", "link": "http://arxiv.org/abs/2406.18522v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Compositionality in Concept Learning", "abstract": "Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .", "published": "2024-06-26 17:59:30", "link": "http://arxiv.org/abs/2406.18534v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Copyright Takedown Methods for Language Models", "abstract": "Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.", "published": "2024-06-26 18:09:46", "link": "http://arxiv.org/abs/2406.18664v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling\n  Legislative Behavior and Bipartisanship", "abstract": "This study introduces a novel approach to simulating legislative processes\nusing LLM-driven virtual agents, focusing on the U.S. Senate Intelligence\nCommittee. We developed agents representing individual senators and placed them\nin simulated committee discussions. The agents demonstrated the ability to\nengage in realistic debate, provide thoughtful reflections, and find bipartisan\nsolutions under certain conditions. Notably, the simulation also showed promise\nin modeling shifts towards bipartisanship in response to external\nperturbations. Our results indicate that this LLM-driven approach could become\na valuable tool for understanding and potentially improving legislative\nprocesses, supporting a broader pattern of findings highlighting how LLM-based\nagents can usefully model real-world phenomena. Future works will focus on\nenhancing agent complexity, expanding the simulation scope, and exploring\napplications in policy testing and negotiation.", "published": "2024-06-26 19:10:51", "link": "http://arxiv.org/abs/2406.18702v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Learn it or Leave it: Module Composition and Pruning for Continual\n  Learning", "abstract": "In real-world environments, continual learning is essential for machine\nlearning models, as they need to acquire new knowledge incrementally without\nforgetting what they have already learned. While pretrained language models\nhave shown impressive capabilities on various static tasks, applying them to\ncontinual learning poses significant challenges, including avoiding\ncatastrophic forgetting, facilitating knowledge transfer, and maintaining\nparameter efficiency. In this paper, we introduce MoCL-P, a novel lightweight\ncontinual learning method that addresses these challenges simultaneously.\nUnlike traditional approaches that continuously expand parameters for newly\narriving tasks, MoCL-P integrates task representation-guided module composition\nwith adaptive pruning, effectively balancing knowledge integration and\ncomputational overhead. Our evaluation across three continual learning\nbenchmarks with up to 176 tasks shows that MoCL-P achieves state-of-the-art\nperformance and improves parameter efficiency by up to three times,\ndemonstrating its potential for practical applications where resource\nrequirements are constrained.", "published": "2024-06-26 19:18:28", "link": "http://arxiv.org/abs/2406.18708v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi", "abstract": "This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.", "published": "2024-06-26 19:48:48", "link": "http://arxiv.org/abs/2406.18725v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with\n  Large Language Models", "abstract": "Large Language Models (LLMs) have been revolutionizing a myriad of natural\nlanguage processing tasks with their diverse zero-shot capabilities. Indeed,\nexisting work has shown that LLMs can be used to great effect for many tasks,\nsuch as information retrieval (IR), and passage ranking. However, current\nstate-of-the-art results heavily lean on the capabilities of the LLM being\nused. Currently, proprietary, and very large LLMs such as GPT-4 are the highest\nperforming passage re-rankers. Hence, users without the resources to leverage\ntop of the line LLMs, or ones that are closed source, are at a disadvantage. In\nthis paper, we investigate the use of a pre-filtering step before passage\nre-ranking in IR. Our experiments show that by using a small number of human\ngenerated relevance scores, coupled with LLM relevance scoring, it is\neffectively possible to filter out irrelevant passages before re-ranking. Our\nexperiments also show that this pre-filtering then allows the LLM to perform\nsignificantly better at the re-ranking task. Indeed, our results show that\nsmaller models such as Mixtral can become competitive with much larger\nproprietary models (e.g., ChatGPT and GPT-4).", "published": "2024-06-26 20:12:24", "link": "http://arxiv.org/abs/2406.18740v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Psychological Profiling in Cybersecurity: A Look at LLMs and\n  Psycholinguistic Features", "abstract": "The increasing sophistication of cyber threats necessitates innovative\napproaches to cybersecurity. In this paper, we explore the potential of\npsychological profiling techniques, particularly focusing on the utilization of\nLarge Language Models (LLMs) and psycholinguistic features. We investigate the\nintersection of psychology and cybersecurity, discussing how LLMs can be\nemployed to analyze textual data for identifying psychological traits of threat\nactors. We explore the incorporation of psycholinguistic features, such as\nlinguistic patterns and emotional cues, into cybersecurity frameworks. Our\nresearch underscores the importance of integrating psychological perspectives\ninto cybersecurity practices to bolster defense mechanisms against evolving\nthreats.", "published": "2024-06-26 23:04:52", "link": "http://arxiv.org/abs/2406.18783v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Mixture of Experts in Large Language Models", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge advances in MoE\nresearch, we have established a resource repository at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.", "published": "2024-06-26 16:34:33", "link": "http://arxiv.org/abs/2407.06204v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explicit Diversity Conditions for Effective Question Answer Generation\n  with Large Language Models", "abstract": "Question Answer Generation (QAG) is an effective data augmentation technique\nto improve the accuracy of question answering systems, especially in\nlow-resource domains. While recent pretrained and large language model-based\nQAG methods have made substantial progress, they face the critical issue of\nredundant QA pair generation, affecting downstream QA systems. Implicit\ndiversity techniques such as sampling and diverse beam search are proven\neffective solutions but often yield smaller diversity. We present explicit\ndiversity conditions for QAG, focusing on spatial aspects, question types, and\nentities, substantially increasing diversity in QA generation. Our work\nemphasizes the need of explicit diversity conditions for generating diverse\nquestion-answer synthetic data by showing significant improvements in\ndownstream QA task over existing widely adopted implicit diversity techniques.\nIn particular, generated QA pairs from explicit diversity conditions when used\nto train the downstream QA model results in an average 4.1% exact match and\n4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our\nwork emphasizes the need for explicit diversity conditions even more in\nlow-resource datasets (SubjQA), where average downstream QA performance\nimprovements are around 12% EM.", "published": "2024-06-26 00:12:08", "link": "http://arxiv.org/abs/2406.17990v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for\n  Memory-Efficient Large Language Models Fine-Tuning", "abstract": "Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.", "published": "2024-06-26 04:33:13", "link": "http://arxiv.org/abs/2406.18060v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Cuffless Blood Pressure Measurement From\n  Wearable Biosignals", "abstract": "Large language models (LLMs) have captured significant interest from both\nacademia and industry due to their impressive performance across various\ntextual tasks. However, the potential of LLMs to analyze physiological\ntime-series data remains an emerging research field. Particularly, there is a\nnotable gap in the utilization of LLMs for analyzing wearable biosignals to\nachieve cuffless blood pressure (BP) measurement, which is critical for the\nmanagement of cardiovascular diseases. This paper presents the first work to\nexplore the capacity of LLMs to perform cuffless BP estimation based on\nwearable biosignals. We extracted physiological features from electrocardiogram\n(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced\nprompts by combining these features with BP domain knowledge and user\ninformation. Subsequently, we adapted LLMs to BP estimation tasks through\nfine-tuning. To evaluate the proposed approach, we conducted assessments of ten\nadvanced LLMs using a comprehensive public dataset of wearable biosignals from\n1,272 participants. The experimental results demonstrate that the optimally\nfine-tuned LLM significantly surpasses conventional task-specific baselines,\nachieving an estimation error of 0.00 $\\pm$ 9.25 mmHg for systolic BP and 1.29\n$\\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the\nbenefits of our context enhancement strategy, leading to an 8.9% reduction in\nmean absolute error for systolic BP estimation. This paper pioneers the\nexploration of LLMs for cuffless BP measurement, providing a potential solution\nto enhance the accuracy of cuffless BP measurement.", "published": "2024-06-26 04:54:45", "link": "http://arxiv.org/abs/2406.18069v3", "categories": ["eess.SP", "cs.AI", "cs.CL"], "primary_category": "eess.SP"}
{"title": "EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction\n  Using Large Language Multimodal Models", "abstract": "Traditional diagnosis of chronic diseases involves in-person consultations\nwith physicians to identify the disease. However, there is a lack of research\nfocused on predicting and developing application systems using clinical notes\nand blood test values. We collected five years of Electronic Health Records\n(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.\nFurthermore, we developed an EHR-based chronic disease prediction platform\nutilizing Large Language Multimodal Models (LLMMs), successfully integrating\nwith frontend web and mobile applications for prediction. This prediction\nplatform can also connect to the hospital's backend database, providing\nphysicians with real-time risk assessment diagnostics. The demonstration link\ncan be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.", "published": "2024-06-26 05:51:08", "link": "http://arxiv.org/abs/2406.18087v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "LLM-Driven Multimodal Opinion Expression Identification", "abstract": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.", "published": "2024-06-26 05:52:47", "link": "http://arxiv.org/abs/2406.18088v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Token-Weighted RNN-T for Learning from Flawed Data", "abstract": "ASR models are commonly trained with the cross-entropy criterion to increase\nthe probability of a target token sequence. While optimizing the probability of\nall tokens in the target sequence is sensible, one may want to de-emphasize\ntokens that reflect transcription errors. In this work, we propose a novel\ntoken-weighted RNN-T criterion that augments the RNN-T objective with\ntoken-specific weights. The new objective is used for mitigating accuracy loss\nfrom transcriptions errors in the training data, which naturally appear in two\nsettings: pseudo-labeling and human annotation errors. Experiments results show\nthat using our method for semi-supervised learning with pseudo-labels leads to\na consistent accuracy improvement, up to 38% relative. We also analyze the\naccuracy degradation resulting from different levels of WER in the reference\ntranscription, and show that token-weighted RNN-T is suitable for overcoming\nthis degradation, recovering 64%-99% of the accuracy loss.", "published": "2024-06-26 06:48:11", "link": "http://arxiv.org/abs/2406.18108v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BADGE: BADminton report Generation and Evaluation with LLM", "abstract": "Badminton enjoys widespread popularity, and reports on matches generally\ninclude details such as player names, game scores, and ball types, providing\naudiences with a comprehensive view of the games. However, writing these\nreports can be a time-consuming task. This challenge led us to explore whether\na Large Language Model (LLM) could automate the generation and evaluation of\nbadminton reports. We introduce a novel framework named BADGE, designed for\nthis purpose using LLM. Our method consists of two main phases: Report\nGeneration and Report Evaluation. Initially, badminton-related data is\nprocessed by the LLM, which then generates a detailed report of the match. We\ntested different Input Data Types, In-Context Learning (ICL), and LLM, finding\nthat GPT-4 performs best when using CSV data type and the Chain of Thought\nprompting. Following report generation, the LLM evaluates and scores the\nreports to assess their quality. Our comparisons between the scores evaluated\nby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.\nSince the application of LLM in badminton reporting remains largely unexplored,\nour research serves as a foundational step for future advancements in this\narea. Moreover, our method can be extended to other sports games, thereby\nenhancing sports promotion. For more details, please refer to\nhttps://github.com/AndyChiangSH/BADGE.", "published": "2024-06-26 07:07:52", "link": "http://arxiv.org/abs/2406.18116v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech\n  Recognition Using LLMs", "abstract": "Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of $56\\%$ in English\ntranslation over the state-of-the-art and $9.3\\%$ in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\n\\url{http://github.com/ahmedheakl/arazn-llm}}, Models:\n\\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.", "published": "2024-06-26 07:19:51", "link": "http://arxiv.org/abs/2406.18120v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets\n  and Large Language Models", "abstract": "The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.", "published": "2024-06-26 07:25:18", "link": "http://arxiv.org/abs/2406.18125v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition for Hindi", "abstract": "Automatic speech recognition (ASR) is a key area in computational\nlinguistics, focusing on developing technologies that enable computers to\nconvert spoken language into text. This field combines linguistics and machine\nlearning. ASR models, which map speech audio to transcripts through supervised\nlearning, require handling real and unrestricted text. Text-to-speech systems\ndirectly work with real text, while ASR systems rely on language models trained\non large text corpora. High-quality transcribed data is essential for training\npredictive models. The research involved two main components: developing a web\napplication and designing a web interface for speech recognition. The web\napplication, created with JavaScript and Node.js, manages large volumes of\naudio files and their transcriptions, facilitating collaborative human\ncorrection of ASR transcripts. It operates in real-time using a client-server\narchitecture. The web interface for speech recognition records 16 kHz mono\naudio from any device running the web app, performs voice activity detection\n(VAD), and sends the audio to the recognition engine. VAD detects human speech\npresence, aiding efficient speech processing and reducing unnecessary\nprocessing during non-speech intervals, thus saving computation and network\nbandwidth in VoIP applications. The final phase of the research tested a neural\nnetwork for accurately aligning the speech signal to hidden Markov model (HMM)\nstates. This included implementing a novel backpropagation method that utilizes\nprior statistics of node co-activations.", "published": "2024-06-26 07:39:20", "link": "http://arxiv.org/abs/2406.18135v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Selective Prompting Tuning for Personalized Conversations with LLMs", "abstract": "In conversational AI, personalizing dialogues with persona profiles and\ncontextual understanding is essential. Despite large language models' (LLMs)\nimproved response coherence, effective persona integration remains a challenge.\nIn this work, we first study two common approaches for personalizing LLMs:\ntextual prompting and direct fine-tuning. We observed that textual prompting\noften struggles to yield responses that are similar to the ground truths in\ndatasets, while direct fine-tuning tends to produce repetitive or overly\ngeneric replies. To alleviate those issues, we propose \\textbf{S}elective\n\\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for\npersonalized conversations in a selective way. Concretely, SPT initializes a\nset of soft prompts and uses a trainable dense retriever to adaptively select\nsuitable soft prompts for LLMs according to different input contexts, where the\nprompt retriever is dynamically updated through feedback from the LLMs.\nAdditionally, we propose context-prompt contrastive learning and prompt fusion\nlearning to encourage the SPT to enhance the diversity of personalized\nconversations. Experiments on the CONVAI2 dataset demonstrate that SPT\nsignificantly enhances response diversity by up to 90\\%, along with\nimprovements in other critical performance indicators. Those results highlight\nthe efficacy of SPT in fostering engaging and personalized dialogue generation.\nThe SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available\nfor further exploration.", "published": "2024-06-26 09:03:52", "link": "http://arxiv.org/abs/2406.18187v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of\n  Transcribed Audio for Speech Recognition Research", "abstract": "Recently, multilingual artificial intelligence assistants, exemplified by\nChatGPT, have gained immense popularity. As a crucial gateway to human-computer\ninteraction, multilingual automatic speech recognition (ASR) has also garnered\nsignificant attention, as evidenced by systems like Whisper. However, the\nproprietary nature of the training data has impeded researchers' efforts to\nstudy multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale\nmultilingual corpus for speech recognition research. The corpus is derived from\npublicly accessible videos on YouTube, comprising 15 languages and a total of\n86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K\ncorpus and other open-source corpora to train a robust multilingual ASR model\nthat is competitive with Whisper. MSR-86K will be publicly released on\nHuggingFace, and we believe that such a large corpus will pave new avenues for\nresearch in multilingual ASR.", "published": "2024-06-26 12:35:12", "link": "http://arxiv.org/abs/2406.18301v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Advancing Airport Tower Command Recognition: Integrating\n  Squeeze-and-Excitation and Broadcasted Residual Learning", "abstract": "Accurate recognition of aviation commands is vital for flight safety and\nefficiency, as pilots must follow air traffic control instructions precisely.\nThis paper addresses challenges in speech command recognition, such as noisy\nenvironments and limited computational resources, by advancing keyword spotting\ntechnology. We create a dataset of standardized airport tower commands,\nincluding routine and emergency instructions. We enhance broadcasted residual\nlearning with squeeze-and-excitation and time-frame frequency-wise\nsqueeze-and-excitation techniques, resulting in our BC-SENet model. This model\nfocuses on crucial information with fewer parameters. Our tests on five keyword\nspotting models, including BC-SENet, demonstrate superior accuracy and\nefficiency. These findings highlight the effectiveness of our model\nadvancements in improving speech command recognition for aviation safety and\nefficiency in noisy, high-stakes environments. Additionally, BC-SENet shows\ncomparable performance on the common Google Speech Command dataset.", "published": "2024-06-26 12:54:19", "link": "http://arxiv.org/abs/2406.18313v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic Data Pruning for Automatic Speech Recognition", "abstract": "The recent success of Automatic Speech Recognition (ASR) is largely\nattributed to the ever-growing amount of training data. However, this trend has\nmade model training prohibitively costly and imposed computational demands.\nWhile data pruning has been proposed to mitigate this issue by identifying a\nsmall subset of relevant data, its application in ASR has been barely explored,\nand existing works often entail significant overhead to achieve meaningful\nresults. To fill this gap, this paper presents the first investigation of\ndynamic data pruning for ASR, finding that we can reach the full-data\nperformance by dynamically selecting 70% of data. Furthermore, we introduce\nDynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained\npruning granularities specifically tailored for speech-related datasets, going\nbeyond the conventional pruning of entire time sequences. Our intensive\nexperiments show that DDP-ASR can save up to 1.6x training time with negligible\nperformance loss.", "published": "2024-06-26 14:17:36", "link": "http://arxiv.org/abs/2406.18373v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do LLMs dream of elephants (when told not to)? Latent concept\n  association and associative memory in transformers", "abstract": "Large Language Models (LLMs) have the capacity to store and recall facts.\nThrough experimentation with open-source models, we observe that this ability\nto retrieve facts can be easily manipulated by changing contexts, even without\naltering their factual meanings. These findings highlight that LLMs might\nbehave like an associative memory model where certain tokens in the contexts\nserve as clues to retrieving facts. We mathematically explore this property by\nstudying how transformers, the building blocks of LLMs, can complete such\nmemory tasks. We study a simple latent concept association problem with a\none-layer transformer and we show theoretically and empirically that the\ntransformer gathers information using self-attention and uses the value matrix\nfor associative memory.", "published": "2024-06-26 14:49:54", "link": "http://arxiv.org/abs/2406.18400v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain\n  Human-Machine Conversation", "abstract": "Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.", "published": "2024-06-26 16:10:53", "link": "http://arxiv.org/abs/2406.18460v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "abstract": "Can emergent language models faithfully model the intelligence of\ndecision-making agents? Though modern language models exhibit already some\nreasoning ability, and theoretically can potentially express any probable\ndistribution over tokens, it remains underexplored how the world knowledge\nthese pretrained models have memorized can be utilized to comprehend an agent's\nbehaviour in the physical world. This study empirically examines, for the first\ntime, how well large language models (LLMs) can build a mental model of agents,\ntermed agent mental modelling, by reasoning about an agent's behaviour and its\neffect on states from agent interaction history. This research may unveil the\npotential of leveraging LLMs for elucidating RL agent behaviour, addressing a\nkey challenge in eXplainable reinforcement learning (XRL). To this end, we\npropose specific evaluation metrics and test them on selected RL task datasets\nof varying complexity, reporting findings on agent mental model establishment.\nOur results disclose that LLMs are not yet capable of fully mental modelling\nagents through inference alone without further innovations. This work thus\nprovides new insights into the capabilities and limitations of modern LLMs.", "published": "2024-06-26 17:14:45", "link": "http://arxiv.org/abs/2406.18505v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse\n  Function-Calling Datasets", "abstract": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/", "published": "2024-06-26 17:49:11", "link": "http://arxiv.org/abs/2406.18518v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Symbolic Learning Enables Self-Evolving Agents", "abstract": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".", "published": "2024-06-26 17:59:18", "link": "http://arxiv.org/abs/2406.18532v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Large Language Model Aided Program Refinement", "abstract": "Program refinement involves correctness-preserving transformations from\nformal high-level specification statements into executable programs.\nTraditional verification tool support for program refinement is highly\ninteractive and lacks automation. On the other hand, the emergence of large\nlanguage models (LLMs) enables automatic code generations from informal natural\nlanguage specifications. However, code generated by LLMs is often unreliable.\nMoreover, the opaque procedure from specification to code provided by LLM is an\nuncontrolled black box. We propose LLM4PR, a tool that combines formal program\nrefinement techniques with informal LLM-based methods to (1) transform the\nspecification to preconditions and postconditions, (2) automatically build\nprompts based on refinement calculus, (3) interact with LLM to generate code,\nand finally, (4) verify that the generated code satisfies the conditions of\nrefinement calculus, thus guaranteeing the correctness of the code. We have\nimplemented our tool using GPT4, Coq, and Coqhammer, and evaluated it on the\nHumanEval and EvalPlus datasets.", "published": "2024-06-26 04:29:27", "link": "http://arxiv.org/abs/2406.18616v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "K.6.3"], "primary_category": "cs.SE"}
{"title": "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for\n  Biomedical Discovery", "abstract": "We present BioLunar, developed using the Lunar framework, as a tool for\nsupporting biological analyses, with a particular emphasis on molecular-level\nevidence enrichment for biomarker discovery in oncology. The platform\nintegrates Large Language Models (LLMs) to facilitate complex scientific\nreasoning across distributed evidence spaces, enhancing the capability for\nharmonizing and reasoning over heterogeneous data sources. Demonstrating its\nutility in cancer research, BioLunar leverages modular design, reusable data\naccess and data analysis components, and a low-code user interface, enabling\nresearchers of all programming levels to construct LLM-enabled scientific\nworkflows. By facilitating automatic scientific discovery and inference from\nheterogeneous evidence, BioLunar exemplifies the potential of the integration\nbetween LLMs, specialised databases and biomedical tools to support\nexpert-level knowledge synthesis and discovery.", "published": "2024-06-26 14:22:46", "link": "http://arxiv.org/abs/2406.18626v1", "categories": ["q-bio.QM", "cs.AI", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of\n  LLMs", "abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.", "published": "2024-06-26 17:43:06", "link": "http://arxiv.org/abs/2406.18629v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RouteLLM: Learning to Route LLMs with Preference Data", "abstract": "Large language models (LLMs) exhibit impressive capabilities across a wide\nrange of tasks, yet the choice of which model to use often involves a trade-off\nbetween performance and cost. More powerful models, though effective, come with\nhigher expenses, while less capable models are more cost-effective. To address\nthis dilemma, we propose several efficient router models that dynamically\nselect between a stronger and a weaker LLM during inference, aiming to optimize\nthe balance between cost and response quality. We develop a training framework\nfor these routers leveraging human preference data and data augmentation\ntechniques to enhance performance. Our evaluation on widely-recognized\nbenchmarks shows that our approach significantly reduces costs-by over 2 times\nin certain cases-without compromising the quality of responses. Interestingly,\nour router models also demonstrate significant transfer learning capabilities,\nmaintaining their performance even when the strong and weak models are changed\nat test time. This highlights the potential of these routers to provide a\ncost-effective yet high-performance solution for deploying LLMs.", "published": "2024-06-26 18:10:22", "link": "http://arxiv.org/abs/2406.18665v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human-AI Collaborative Taxonomy Construction: A Case Study in\n  Profession-Specific Writing Assistants", "abstract": "Large Language Models (LLMs) have assisted humans in several writing tasks,\nincluding text revision and story generation. However, their effectiveness in\nsupporting domain-specific writing, particularly in business contexts, is\nrelatively less explored. Our formative study with industry professionals\nrevealed the limitations in current LLMs' understanding of the nuances in such\ndomain-specific writing. To address this gap, we propose an approach of\nhuman-AI collaborative taxonomy development to perform as a guideline for\ndomain-specific writing assistants. This method integrates iterative feedback\nfrom domain experts and multiple interactions between these experts and LLMs to\nrefine the taxonomy. Through larger-scale experiments, we aim to validate this\nmethodology and thus improve LLM-powered writing assistance, tailoring it to\nmeet the unique requirements of different stakeholder needs.", "published": "2024-06-26 18:25:06", "link": "http://arxiv.org/abs/2406.18675v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.", "published": "2024-06-26 18:26:53", "link": "http://arxiv.org/abs/2406.18676v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Personalization of LLMs with Mis-aligned Responses", "abstract": "As the diversity of users increases, the capability of providing personalized\nresponses by large language models (LLMs) has become increasingly important.\nExisting approaches have only limited successes in LLM personalization, due to\nthe absence of personalized learning or the reliance on shared personal data.\nThis paper proposes a new approach for a few-shot personalization of LLMs with\ntheir mis-aligned responses (Fermi). Our key idea is to learn a set of\npersonalized prompts for each user by progressively improving the prompts using\nLLMs, based on user profile (e.g., demographic information) and a few examples\nof previous opinions. During an iterative process of prompt improvement, we\nincorporate the contexts of mis-aligned responses by LLMs, which are especially\ncrucial for the effective personalization of LLMs. In addition, we develop an\neffective inference method to further leverage the context of the test query\nand the personalized prompts. Our experimental results demonstrate that Fermi\nsignificantly improves performance across various benchmarks, compared to\nbest-performing baselines.", "published": "2024-06-26 18:29:12", "link": "http://arxiv.org/abs/2406.18678v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Speakers Unembedded: Embedding-free Approach to Long-form Neural\n  Diarization", "abstract": "End-to-end neural diarization (EEND) models offer significant improvements\nover traditional embedding-based Speaker Diarization (SD) approaches but falls\nshort on generalizing to long-form audio with large number of speakers.\nEEND-vector-clustering method mitigates this by combining local EEND with\nglobal clustering of speaker embeddings from local windows, but this requires\nan additional speaker embedding framework alongside the EEND module. In this\npaper, we propose a novel framework applying EEND both locally and globally for\nlong-form audio without separate speaker embeddings. This approach achieves\nsignificant relative DER reduction of 13% and 10% over the conventional 1-pass\nEEND on Callhome American English and RT03-CTS datasets respectively and\nmarginal improvements over EEND-vector-clustering without the need for\nadditional speaker embeddings. Furthermore, we discuss the computational\ncomplexity of our proposed framework and explore strategies for reducing\nprocessing times.", "published": "2024-06-26 18:32:16", "link": "http://arxiv.org/abs/2406.18679v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences\n  to Reduce Harm", "abstract": "A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.", "published": "2024-06-26 18:39:08", "link": "http://arxiv.org/abs/2406.18682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Correct for QA Reasoning with Black-box LLMs", "abstract": "An open challenge in recent machine learning is about how to improve the\nreasoning capability of large language models (LLMs) in a black-box setting,\ni.e., without access to detailed information such as output token\nprobabilities. Existing approaches either rely on accessibility (which is often\nunrealistic) or involve significantly increased train- and inference-time\ncosts. This paper addresses those limitations or shortcomings by proposing a\nnovel approach, namely CoBB (Correct for improving QA reasoning of Black-Box\nLLMs). It uses a trained adaptation model to perform a seq2seq mapping from the\noften-imperfect reasonings of the original black-box LLM to the correct or\nimproved reasonings. Specifically, the adaptation model is initialized with a\nrelatively small open-source LLM and adapted over a collection of sub-sampled\ntraining pairs. To select the representative pairs of correct and incorrect\nreasonings, we formulated the dataset construction as an optimization problem\nthat minimizes the statistical divergence between the sampled subset and the\nentire collection, and solved it via a genetic algorithm. We then train the\nadaptation model over the sampled pairs by contrasting the likelihoods of\ncorrect and incorrect reasonings. Our experimental results demonstrate that\nCoBB significantly improves reasoning accuracy across various QA benchmarks,\ncompared to the best-performing adaptation baselines.", "published": "2024-06-26 18:57:32", "link": "http://arxiv.org/abs/2406.18695v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech\n  Health Diagnostic Model", "abstract": "Speech is known to carry health-related attributes, which has emerged as a\nnovel venue for remote and long-term health monitoring. However, existing\nmodels are usually tailored for a specific type of disease, and have been shown\nto lack generalizability across datasets. Furthermore, concerns have been\nraised recently towards the leakage of speaker identity from health embeddings.\nTo mitigate these limitations, we propose WavRx, a speech health diagnostics\nmodel that captures the respiration and articulation related dynamics from a\nuniversal speech representation. Our in-domain and cross-domain experiments on\nsix pathological speech datasets demonstrate WavRx as a new state-of-the-art\nhealth diagnostic model. Furthermore, we show that the amount of speaker\nidentity entailed in the WavRx health embeddings is significantly reduced\nwithout extra guidance during training. An in-depth analysis of the model was\nperformed, thus providing physiological interpretation of its improved\ngeneralizability and privacy-preserving ability.", "published": "2024-06-26 19:59:21", "link": "http://arxiv.org/abs/2406.18731v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large\n  Language and Vision-Language Models", "abstract": "The rapid evolution of artificial intelligence (AI) through developments in\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) has brought\nsignificant advancements across various technological domains. While these\nmodels enhance capabilities in natural language processing and visual\ninteractive tasks, their growing adoption raises critical concerns regarding\nsecurity and ethical alignment. This survey provides an extensive review of the\nemerging field of jailbreaking--deliberately circumventing the ethical and\noperational boundaries of LLMs and VLMs--and the consequent development of\ndefense mechanisms. Our study categorizes jailbreaks into seven distinct types\nand elaborates on defense strategies that address these vulnerabilities.\nThrough this comprehensive examination, we identify research gaps and propose\ndirections for future studies to enhance the security frameworks of LLMs and\nVLMs. Our findings underscore the necessity for a unified perspective that\nintegrates both jailbreak strategies and defensive solutions to foster a\nrobust, secure, and reliable environment for the next generation of language\nmodels. More details can be found on our website:\n\\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.", "published": "2024-06-26 02:20:23", "link": "http://arxiv.org/abs/2407.01599v2", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clustering in pure-attention hardmax transformers and its role in\n  sentiment analysis", "abstract": "Transformers are extremely successful machine learning models whose\nmathematical properties remain poorly understood. Here, we rigorously\ncharacterize the behavior of transformers with hardmax self-attention and\nnormalization sublayers as the number of layers tends to infinity. By viewing\nsuch transformers as discrete-time dynamical systems describing the evolution\nof points in a Euclidean space, and thanks to a geometric interpretation of the\nself-attention mechanism based on hyperplane separation, we show that the\ntransformer inputs asymptotically converge to a clustered equilibrium\ndetermined by special points called leaders. We then leverage this theoretical\nunderstanding to solve sentiment analysis problems from language processing\nusing a fully interpretable transformer model, which effectively captures\n`context' by clustering meaningless words around leader words carrying the most\nmeaning. Finally, we outline remaining challenges to bridge the gap between the\nmathematical analysis of transformers and their real-life implementation.", "published": "2024-06-26 16:13:35", "link": "http://arxiv.org/abs/2407.01602v1", "categories": ["cs.CL", "cs.LG", "math.DS", "stat.ML", "68T07, 68T50"], "primary_category": "cs.CL"}
{"title": "A Review of Large Language Models and Autonomous Agents in Chemistry", "abstract": "Large language models (LLMs) have emerged as powerful tools in chemistry,\nsignificantly impacting molecule design, property prediction, and synthesis\noptimization. This review highlights LLM capabilities in these domains and\ntheir potential to accelerate scientific discovery through automation. We also\nreview LLM-based autonomous agents: LLMs with a broader set of tools to\ninteract with their surrounding environment. These agents perform diverse tasks\nsuch as paper scraping, interfacing with automated laboratories, and synthesis\nplanning. As agents are an emerging topic, we extend the scope of our review of\nagents beyond chemistry and discuss across any scientific domains. This review\ncovers the recent history, current capabilities, and design of LLMs and\nautonomous agents, addressing specific challenges, opportunities, and future\ndirections in chemistry. Key challenges include data quality and integration,\nmodel interpretability, and the need for standard benchmarks, while future\ndirections point towards more sophisticated multi-modal agents and enhanced\ncollaboration between agents and experimental methods. Due to the quick pace of\nthis field, a repository has been built to keep track of the latest studies:\nhttps://github.com/ur-whitelab/LLMs-in-science.", "published": "2024-06-26 17:33:21", "link": "http://arxiv.org/abs/2407.01603v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph"], "primary_category": "cs.LG"}
{"title": "MATE: Meet At The Embedding -- Connecting Images with Long Texts", "abstract": "While advancements in Vision Language Models (VLMs) have significantly\nimproved the alignment of visual and textual data, these models primarily focus\non aligning images with short descriptive captions. This focus limits their\nability to handle complex text interactions, particularly with longer texts\nsuch as lengthy captions or documents, which have not been extensively explored\nyet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach\nthat combines the capabilities of VLMs with Large Language Models (LLMs) to\novercome this challenge without the need for additional image-long text pairs.\nSpecifically, we replace the text encoder of the VLM with a pretrained\nLLM-based encoder that excels in understanding long texts. To bridge the gap\nbetween VLM and LLM, MATE incorporates a projection module that is trained in a\nmulti-stage manner. It starts by aligning the embeddings from the VLM text\nencoder with those from the LLM using extensive text pairs. This module is then\nemployed to seamlessly align image embeddings closely with LLM embeddings. We\npropose two new cross-modal retrieval benchmarks to assess the task of\nconnecting images with long texts (lengthy captions / documents). Extensive\nexperimental results demonstrate that MATE effectively connects images with\nlong texts, uncovering diverse semantic relationships.", "published": "2024-06-26 14:10:00", "link": "http://arxiv.org/abs/2407.09541v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Navigating the Minefield of MT Beam Search in Cascaded Streaming Speech\n  Translation", "abstract": "We adapt the well-known beam-search algorithm for machine translation to\noperate in a cascaded real-time speech translation system. This proved to be\nmore complex than initially anticipated, due to four key challenges: (1)\nreal-time processing of intermediate and final transcriptions with incomplete\nwords from ASR, (2) emitting intermediate and final translations with minimal\nuser perceived latency, (3) handling beam search hypotheses that have unequal\nlength and different model state, and (4) handling sentence boundaries.\nPrevious work in the field of simultaneous machine translation only implemented\ngreedy decoding. We present a beam-search realization that handles all of the\nabove, providing guidance through the minefield of challenges. Our approach\nincreases the BLEU score by 1 point compared to greedy search, reduces the CPU\ntime by up to 40% and character flicker rate by 20+% compared to a baseline\nheuristic that just retranslates input repeatedly.", "published": "2024-06-26 07:34:53", "link": "http://arxiv.org/abs/2407.11010v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Gender-Specific Speech Patterns in Automatic Suicide Risk\n  Assessment", "abstract": "In emergency medicine, timely intervention for patients at risk of suicide is\noften hindered by delayed access to specialised psychiatric care. To bridge\nthis gap, we introduce a speech-based approach for automatic suicide risk\nassessment. Our study involves a novel dataset comprising speech recordings of\n20 patients who read neutral texts. We extract four speech representations\nencompassing interpretable and deep features. Further, we explore the impact of\ngender-based modelling and phrase-level normalisation. By applying\ngender-exclusive modelling, features extracted from an emotion fine-tuned\nwav2vec2.0 model can be utilised to discriminate high- from low- suicide risk\nwith a balanced accuracy of 81%. Finally, our analysis reveals a discrepancy in\nthe relationship of speech characteristics and suicide risk between female and\nmale subjects. For men in our dataset, suicide risk increases together with\nagitation while voice characteristics of female subjects point the other way.", "published": "2024-06-26 12:51:28", "link": "http://arxiv.org/abs/2407.11012v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "68T10", "J.3"], "primary_category": "cs.CL"}
{"title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit\n  Reasoning and Precise Spatio-Temporal Retrieval", "abstract": "Large language models (LLMs) have shown promising results in learning and\ncontextualizing information from different forms of data. Recent advancements\nin foundational models, particularly those employing self-attention mechanisms,\nhave significantly enhanced our ability to comprehend the semantics of diverse\ndata types. One such area that could highly benefit from multi-modality is in\nunderstanding geospatial data, which inherently has multiple modalities.\nHowever, current Natural Language Processing (NLP) mechanisms struggle to\neffectively address geospatial queries. Existing pre-trained LLMs are\ninadequately equipped to meet the unique demands of geospatial data, lacking\nthe ability to retrieve precise spatio-temporal data in real-time, thus leading\nto significantly reduced accuracy in answering complex geospatial queries. To\naddress these limitations, we introduce Geode--a pioneering system designed to\ntackle zero-shot geospatial question-answering tasks with high precision using\nspatio-temporal data retrieval. Our approach represents a significant\nimprovement in addressing the limitations of current LLM models, demonstrating\nremarkable improvement in geospatial question-answering abilities compared to\nexisting state-of-the-art pre-trained models.", "published": "2024-06-26 21:59:54", "link": "http://arxiv.org/abs/2407.11014v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis", "abstract": "Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.", "published": "2024-06-26 12:33:34", "link": "http://arxiv.org/abs/2407.12013v2", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.DL"}
{"title": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS", "abstract": "This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully\nnon-autoregressive zero-shot text-to-speech system that offers human-level\nnaturalness and state-of-the-art speaker similarity and intelligibility. In the\nE2 TTS framework, the text input is converted into a character sequence with\nfiller tokens. The flow-matching-based mel spectrogram generator is then\ntrained based on the audio infilling task. Unlike many previous works, it does\nnot require additional components (e.g., duration model, grapheme-to-phoneme)\nor complex techniques (e.g., monotonic alignment search). Despite its\nsimplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that\nare comparable to or surpass previous works, including Voicebox and\nNaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the\ninput representation. We propose several variants of E2 TTS to improve\nusability during inference. See https://aka.ms/e2tts/ for demo samples.", "published": "2024-06-26 01:38:37", "link": "http://arxiv.org/abs/2406.18009v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Calibration of Speech Classification Models: Insights from\n  Energy-Based Model Investigations", "abstract": "For speech classification tasks, deep learning models often achieve high\naccuracy but exhibit shortcomings in calibration, manifesting as classifiers\nexhibiting overconfidence. The significance of calibration lies in its critical\nrole in guaranteeing the reliability of decision-making within deep learning\nsystems. This study explores the effectiveness of Energy-Based Models in\ncalibrating confidence for speech classification tasks by training a joint EBM\nintegrating a discriminative and a generative model, thereby enhancing the\nclassifiers calibration and mitigating overconfidence. Experimental evaluations\nconducted on three speech classification tasks specifically: age, emotion, and\nlanguage recognition. Our findings highlight the competitive performance of\nEBMs in calibrating the speech classification models. This research emphasizes\nthe potential of EBMs in speech classification tasks, demonstrating their\nability to enhance calibration without sacrificing accuracy.", "published": "2024-06-26 04:50:19", "link": "http://arxiv.org/abs/2406.18065v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and\n  Non-streaming Code-Switching ASR", "abstract": "In this work, we propose a Switch-Conformer-based MoE system named SC-MoE for\nunified streaming and non-streaming code-switching (CS) automatic speech\nrecognition (ASR), where we design a streaming MoE layer consisting of three\nlanguage experts, which correspond to Mandarin, English, and blank,\nrespectively, and equipped with a language identification (LID) network with a\nConnectionist Temporal Classification (CTC) loss as a router in the encoder of\nSC-MoE to achieve a real-time streaming CS ASR system. To further utilize the\nlanguage information embedded in text, we also incorporate MoE layers into the\ndecoder of SC-MoE. In addition, we introduce routers into every MoE layer of\nthe encoder and the decoder and achieve better recognition performance.\nExperimental results show that the SC-MoE significantly improves CS ASR\nperformances over baseline with comparable computational efficiency.", "published": "2024-06-26 02:32:59", "link": "http://arxiv.org/abs/2406.18021v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study on Synthesizing Expressive Violin Performances: Approaches and\n  Comparisons", "abstract": "Expressive music synthesis (EMS) for violin performance is a challenging task\ndue to the disagreement among music performers in the interpretation of\nexpressive musical terms (EMTs), scarcity of labeled recordings, and limited\ngeneralization ability of the synthesis model. These challenges create\ntrade-offs between model effectiveness, diversity of generated results, and\ncontrollability of the synthesis system, making it essential to conduct a\ncomparative study on EMS model design. This paper explores two violin EMS\napproaches. The end-to-end approach is a modification of a state-of-the-art\ntext-to-speech generator. The parameter-controlled approach is based on a\nsimple parameter sampling process that can render note lengths and other\nparameters compatible with MIDI-DDSP. We study these two approaches (in total,\nthree model variants) through objective and subjective experiments and discuss\nseveral key issues of EMS based on the results.", "published": "2024-06-26 05:54:07", "link": "http://arxiv.org/abs/2406.18089v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Deep Active Learning in Avian Bioacoustics", "abstract": "Passive acoustic monitoring (PAM) in avian bioacoustics enables\ncost-effective and extensive data collection with minimal disruption to natural\nhabitats. Despite advancements in computational avian bioacoustics, deep\nlearning models continue to encounter challenges in adapting to diverse\nenvironments in practical PAM scenarios. This is primarily due to the scarcity\nof annotations, which requires labor-intensive efforts from human experts.\nActive learning (AL) reduces annotation cost and speed ups adaption to diverse\nscenarios by querying the most informative instances for labeling. This paper\noutlines a deep AL approach, introduces key challenges, and conducts a\nsmall-scale pilot study.", "published": "2024-06-26 08:43:05", "link": "http://arxiv.org/abs/2406.18621v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using\n  Longitudinal Speech Transformer", "abstract": "Automatic prediction of amyotrophic lateral sclerosis (ALS) disease\nprogression provides a more efficient and objective alternative than manual\napproaches. We propose ALS longitudinal speech transformer (ALST), a neural\nnetwork-based automatic predictor of ALS disease progression from longitudinal\nspeech recordings of ALS patients. By taking advantage of high-quality\npretrained speech features and longitudinal information in the recordings, our\nbest model achieves 91.0\\% AUC, improving upon the previous best model by 5.6\\%\nrelative on the ALS TDI dataset. Careful analysis reveals that ALST is capable\nof fine-grained and interpretable predictions of ALS progression, especially\nfor distinguishing between rarer and more severe cases. Code is publicly\navailable.", "published": "2024-06-26 13:28:24", "link": "http://arxiv.org/abs/2406.18625v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PianoBART: Symbolic Piano Music Generation and Understanding with\n  Large-Scale Pre-Training", "abstract": "Learning musical structures and composition patterns is necessary for both\nmusic generation and understanding, but current methods do not make uniform use\nof learned features to generate and comprehend music simultaneously. In this\npaper, we propose PianoBART, a pre-trained model that uses BART for both\nsymbolic piano music generation and understanding. We devise a multi-level\nobject selection strategy for different pre-training tasks of PianoBART, which\ncan prevent information leakage or loss and enhance learning ability. The\nmusical semantics captured in pre-training are fine-tuned for music generation\nand understanding tasks. Experiments demonstrate that PianoBART efficiently\nlearns musical patterns and achieves outstanding performance in generating\nhigh-quality coherent pieces and comprehending music. Our code and\nsupplementary material are available at https://github.com/RS2002/PianoBart.", "published": "2024-06-26 03:35:54", "link": "http://arxiv.org/abs/2407.03361v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond\n  Four Stems", "abstract": "Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.", "published": "2024-06-26 20:25:53", "link": "http://arxiv.org/abs/2406.18747v2", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
