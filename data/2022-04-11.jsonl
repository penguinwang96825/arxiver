{"title": "HFL at SemEval-2022 Task 8: A Linguistics-inspired Regression Model with\n  Data Augmentation for Multilingual News Similarity", "abstract": "This paper describes our system designed for SemEval-2022 Task 8:\nMultilingual News Article Similarity. We proposed a linguistics-inspired model\ntrained with a few task-specific strategies. The main techniques of our system\nare: 1) data augmentation, 2) multi-label loss, 3) adapted R-Drop, 4) samples\nreconstruction with the head-tail combination. We also present a brief analysis\nof some negative methods like two-tower architecture. Our system ranked 1st on\nthe leaderboard while achieving a Pearson's Correlation Coefficient of 0.818 on\nthe official evaluation set.", "published": "2022-04-11 03:08:37", "link": "http://arxiv.org/abs/2204.04844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada", "abstract": "Over the last decade, Twitter has emerged as one of the most influential\nforums for social, political, and health discourse. In this paper, we introduce\na massive dataset of more than 45 million geo-located tweets posted between\n2015 and 2021 from US and Canada (TUSC), especially curated for natural\nlanguage analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to\ncapture patterns of emotions associated with tweets over time. We use TED and\nTUSC to explore the use of emotion-associated words across US and Canada;\nacross 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the\nsecond year of the pandemic); and across individual tweeters. We show that\nCanadian tweets tend to have higher valence, lower arousal, and higher\ndominance than the US tweets. Further, we show that the COVID-19 pandemic had a\nmarked impact on the emotional signature of tweets posted in 2020, when\ncompared to the adjoining years. Finally, we determine metrics of TED for\n170,000 tweeters to benchmark characteristics of TED metrics at an aggregate\nlevel. TUSC and the metrics for TED will enable a wide variety of research on\nstudying how we use language to express ourselves, persuade, communicate, and\ninfluence, with particularly promising applications in public health, affective\nscience, social science, and psychology.", "published": "2022-04-11 04:39:39", "link": "http://arxiv.org/abs/2204.04862v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Automatic Text Summarization using Synthetic Facts", "abstract": "Despite some recent advances, automatic text summarization remains\nunreliable, elusive, and of limited practical use in applications. Two main\nproblems with current summarization methods are well known: evaluation and\nfactual consistency. To address these issues, we propose a new automatic\nreference-less text summarization evaluation system that can measure the\nquality of any text summarization model with a set of generated facts based on\nfactual consistency, comprehensiveness, and compression rate. As far as we\nknow, our evaluation system is the first system that measures the overarching\nquality of the text summarization models based on factuality, information\ncoverage, and compression rate.", "published": "2022-04-11 05:10:37", "link": "http://arxiv.org/abs/2204.04869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias", "abstract": "Media news framing bias can increase political polarization and undermine\ncivil society. The need for automatic mitigation methods is therefore growing.\nWe propose a new task, a neutral summary generation from multiple news articles\nof the varying political leanings to facilitate balanced and unbiased news\nreading. In this paper, we first collect a new dataset, illustrate insights\nabout framing bias through a case study, and propose a new effective metric and\nmodel (NeuS-TITLE) for the task. Based on our discovery that title provides a\ngood signal for framing bias, we present NeuS-TITLE that learns to neutralize\nnews content in hierarchical order from title to article. Our hierarchical\nmulti-task learning is achieved by formatting our hierarchical data pair\n(title, article) sequentially with identifier-tokens (\"TITLE=>\", \"ARTICLE=>\")\nand fine-tuning the auto-regressive decoder with the standard negative\nlog-likelihood objective. We then analyze and point out the remaining\nchallenges and future directions. One of the most interesting observations is\nthat neural NLG models can hallucinate not only factually inaccurate or\nunverifiable content but also politically biased content.", "published": "2022-04-11 07:06:01", "link": "http://arxiv.org/abs/2204.04902v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Same Author or Just Same Topic? Towards Content-Independent Style\n  Representations", "abstract": "Linguistic style is an integral component of language. Recent advances in the\ndevelopment of style representations have increasingly used training objectives\nfrom authorship verification (AV): Do two texts have the same author? The\nassumption underlying the AV training task (same author approximates same\nwriting style) enables self-supervised and, thus, extensive training. However,\na good performance on the AV task does not ensure good \"general-purpose\" style\nrepresentations. For example, as the same author might typically write about\ncertain topics, representations trained on AV might also encode content\ninformation instead of style alone. We introduce a variation of the AV training\ntask that controls for content using conversation or domain labels. We evaluate\nwhether known style dimensions are represented and preferred over content\ninformation through an original variation to the recently proposed STEL\nframework. We find that representations trained by controlling for conversation\nare better than representations trained with domain or no content control at\nrepresenting style independent from content.", "published": "2022-04-11 07:15:06", "link": "http://arxiv.org/abs/2204.04907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Token-level Contrastive Framework for Sign Language Translation", "abstract": "Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.", "published": "2022-04-11 07:33:26", "link": "http://arxiv.org/abs/2204.04916v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text\n  Classification", "abstract": "Text classification struggles to generalize to unseen classes with very few\nlabeled text instances per class. In such a few-shot learning (FSL) setting,\nmetric-based meta-learning approaches have shown promising results. Previous\nstudies mainly aim to derive a prototype representation for each class.\nHowever, they neglect that it is challenging-yet-unnecessary to construct a\ncompact representation which expresses the entire meaning for each class. They\nalso ignore the importance to capture the inter-dependency between query and\nthe support set for few-shot text classification. To deal with these issues, we\npropose a meta-learning based method MGIMN which performs instance-wise\ncomparison followed by aggregation to generate class-wise matching vectors\ninstead of prototype learning. The key of instance-wise comparison is the\ninteractive matching within the class-specific context and episode-specific\ncontext. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms the existing state-of-the-art approaches, under both\nthe standard FSL and generalized FSL settings.", "published": "2022-04-11 08:58:55", "link": "http://arxiv.org/abs/2204.04952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Pre-trained Encoders for Low-Resource Named\n  Entity Recognition", "abstract": "Pre-trained language models (PLM) are effective components of few-shot named\nentity recognition (NER) approaches when augmented with continued pre-training\non task-specific out-of-domain data or fine-tuning on in-domain data. However,\ntheir performance in low-resource scenarios, where such data is not available,\nremains an open question. We introduce an encoder evaluation framework, and use\nit to systematically compare the performance of state-of-the-art pre-trained\nrepresentations on the task of low-resource NER. We analyze a wide range of\nencoders pre-trained with different strategies, model architectures,\nintermediate-task fine-tuning, and contrastive learning. Our experimental\nresults across ten benchmark NER datasets in English and German show that\nencoder performance varies significantly, suggesting that the choice of encoder\nfor a specific low-resource scenario needs to be carefully evaluated.", "published": "2022-04-11 09:48:26", "link": "http://arxiv.org/abs/2204.04980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRUE: Re-evaluating Factual Consistency Evaluation", "abstract": "Grounded text generation systems often generate text that contains factual\ninconsistencies, hindering their real-world applicability. Automatic factual\nconsistency evaluation may help alleviate this limitation by accelerating\nevaluation cycles, filtering inconsistent outputs and augmenting training data.\nWhile attracting increasing attention, such evaluation metrics are usually\ndeveloped and evaluated in silo for a single task or dataset, slowing their\nadoption. Moreover, previous meta-evaluation protocols focused on system-level\ncorrelations with human annotations, which leave the example-level accuracy of\nsuch metrics unclear. In this work, we introduce TRUE: a comprehensive survey\nand assessment of factual consistency metrics on a standardized collection of\nexisting texts from diverse tasks, manually annotated for factual consistency.\nOur standardization enables an example-level meta-evaluation protocol that is\nmore actionable and interpretable than previously reported correlations,\nyielding clearer quality measures. Across diverse state-of-the-art metrics and\n11 datasets we find that large-scale NLI and question\ngeneration-and-answering-based approaches achieve strong and complementary\nresults. We recommend those methods as a starting point for model and metric\ndevelopers, and hope TRUE will foster progress towards even better evaluation\nmethods.", "published": "2022-04-11 10:14:35", "link": "http://arxiv.org/abs/2204.04991v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resources for Turkish Natural Language Processing: A critical survey", "abstract": "This paper presents a comprehensive survey of corpora and lexical resources\navailable for Turkish. We review a broad range of resources, focusing on the\nones that are publicly available. In addition to providing information about\nthe available linguistic resources, we present a set of recommendations, and\nidentify gaps in the data available for conducting research and building\napplications in Turkish Linguistics and Natural Language Processing.", "published": "2022-04-11 12:23:07", "link": "http://arxiv.org/abs/2204.05042v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of\n  Lexical Gaps in Kinship", "abstract": "This paper describes a method to enrich lexical resources with content\nrelating to linguistic diversity, based on knowledge from the field of lexical\ntypology. We capture the phenomenon of diversity through the notions of lexical\ngap and language-specific word and use a systematic method to infer gaps\nsemi-automatically on a large scale. As a first result obtained for the domain\nof kinship terminology, known to be very diverse throughout the world, we\npublish a lexico-semantic resource consisting of 198 domain concepts, 1,911\nwords, and 37,370 gaps covering 699 languages. We see potential in the use of\nresources such as ours for the improvement of a variety of cross-lingual NLP\ntasks, which we demonstrate through a downstream application for the evaluation\nof machine translation systems.", "published": "2022-04-11 12:36:26", "link": "http://arxiv.org/abs/2204.05049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do complexity measures measure? Correlating and validating\n  corpus-based measures of morphological complexity", "abstract": "We present an analysis of eight measures used for quantifying morphological\ncomplexity of natural languages. The measures we study are corpus-based\nmeasures of morphological complexity with varying requirements for corpus\nannotation. We present similarities and differences between these measures\nvisually and through correlation analyses, as well as their relation to the\nrelevant typological variables. Our analysis focuses on whether these\n`measures' are measures of the same underlying variable, or whether they\nmeasure more than one dimension of morphological complexity. The principal\ncomponent analysis indicates that the first principal component explains 92.62\n% of the variation in eight measures, indicating a strong linear dependence\nbetween the complexity measures studied.", "published": "2022-04-11 12:49:26", "link": "http://arxiv.org/abs/2204.05056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Block-Segmentation Vectors for Arousal Prediction using Semi-supervised\n  Learning", "abstract": "To handle emotional expressions in computer applications, Russell's circum-\nplex model has been useful for representing emotions according to valence and\narousal. In SentiWordNet, the level of valence is automatically assigned to a\nlarge number of synsets (groups of synonyms in WordNet) using semi-supervised\nlearning. However, when assigning the level of arousal, the existing method\nproposed for SentiWordNet reduces the accuracy of sentiment prediction. In this\npaper, we propose a block-segmentation vector for predicting the arousal levels\nof many synsets from a small number of labeled words using semi-supervised\nlearning. We analyze the distribution of arousal and non-arousal words in a\ncorpus of sentences by comparing it with the distribution of valence words. We\naddress the problem that arousal level prediction fails when arousal and\nnon-arousal words are mixed together in some sentences. To capture the features\nof such arousal and non-arousal words, we generate word vectors based on\ninverted indexes by block IDs, where the corpus is divided into blocks in the\nflow of sentences. In the evaluation experiment, we show that the results of\narousal prediction with the block-segmentation vectors outperform the results\nof the previous method in SentiWordNet.", "published": "2022-04-11 13:55:21", "link": "http://arxiv.org/abs/2204.05096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gaining Insights into Unrecognized User Utterances in Task-Oriented\n  Dialog Systems", "abstract": "The rapidly growing market demand for automatic dialogue agents capable of\ngoal-oriented behavior has caused many tech-industry leaders to invest\nconsiderable efforts into task-oriented dialog systems. The success of these\nsystems is highly dependent on the accuracy of their intent identification --\nthe process of deducing the goal or meaning of the user's request and mapping\nit to one of the known intents for further processing. Gaining insights into\nunrecognized utterances -- user requests the systems fail to attribute to a\nknown intent -- is therefore a key process in continuous improvement of\ngoal-oriented dialog systems.\n  We present an end-to-end pipeline for processing unrecognized user\nutterances, deployed in a real-world, commercial task-oriented dialog system,\nincluding a specifically-tailored clustering algorithm, a novel approach to\ncluster representative extraction, and cluster naming. We evaluated the\nproposed components, demonstrating their benefits in the analysis of\nunrecognized user requests.", "published": "2022-04-11 14:45:55", "link": "http://arxiv.org/abs/2204.05158v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Biomedical Entity Linking via Knowledge Base-Guided\n  Pre-training and Synonyms-Aware Fine-tuning", "abstract": "Entities lie in the heart of biomedical natural language understanding, and\nthe biomedical entity linking (EL) task remains challenging due to the\nfine-grained and diversiform concept names. Generative methods achieve\nremarkable performances in general domain EL with less memory usage while\nrequiring expensive pre-training. Previous biomedical EL methods leverage\nsynonyms from knowledge bases (KB) which is not trivial to inject into a\ngenerative method. In this work, we use a generative approach to model\nbiomedical EL and propose to inject synonyms knowledge in it. We propose\nKB-guided pre-training by constructing synthetic samples with synonyms and\ndefinitions from KB and require the model to recover concept names. We also\npropose synonyms-aware fine-tuning to select concept names for training, and\npropose decoder prompt and multi-synonyms constrained prefix tree for\ninference. Our method achieves state-of-the-art results on several biomedical\nEL tasks without candidate selection which displays the effectiveness of\nproposed pre-training and fine-tuning strategies.", "published": "2022-04-11 14:50:51", "link": "http://arxiv.org/abs/2204.05164v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0", "abstract": "In this work, we explore whether the recently demonstrated zero-shot\nabilities of the T0 model extend to Named Entity Recognition for\nout-of-distribution languages and time periods. Using a historical newspaper\ncorpus in 3 languages as test-bed, we use prompts to extract possible named\nentities. Our results show that a naive approach for prompt-based zero-shot\nmultilingual Named Entity Recognition is error-prone, but highlights the\npotential of such an approach for historical languages lacking labeled\ndatasets. Moreover, we also find that T0-like models can be probed to predict\nthe publication date and language of a document, which could be very relevant\nfor the study of historical texts.", "published": "2022-04-11 15:56:13", "link": "http://arxiv.org/abs/2204.05211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-Turn Debate Does Not Help Humans Answer Hard\n  Reading-Comprehension Questions", "abstract": "Current QA systems can generate reasonable-sounding yet false answers without\nexplanation or evidence for the generated answer, which is especially\nproblematic when humans cannot readily check the model's answers. This presents\na challenge for building trust in machine learning systems. We take inspiration\nfrom real-world situations where difficult questions are answered by\nconsidering opposing sides (see Irving et al., 2018). For multiple-choice QA\nexamples, we build a dataset of single arguments for both a correct and\nincorrect answer option in a debate-style set-up as an initial step in training\nmodels to produce explanations for two candidate answers. We use long contexts\n-- humans familiar with the context write convincing explanations for\npre-selected correct and incorrect answers, and we test if those explanations\nallow humans who have not read the full context to more accurately determine\nthe correct answer. We do not find that explanations in our set-up improve\nhuman accuracy, but a baseline condition shows that providing human-selected\ntext snippets does improve accuracy. We use these findings to suggest ways of\nimproving the debate set up for future data collection efforts.", "published": "2022-04-11 15:56:34", "link": "http://arxiv.org/abs/2204.05212v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Universal Vulnerability of Prompt-based Learning Paradigm", "abstract": "Prompt-based learning paradigm bridges the gap between pre-training and\nfine-tuning, and works effectively under the few-shot setting. However, we find\nthat this learning paradigm inherits the vulnerability from the pre-training\nstage, where model predictions can be misled by inserting certain triggers into\nthe text. In this paper, we explore this universal vulnerability by either\ninjecting backdoor triggers or searching for adversarial triggers on\npre-trained language models using only plain text. In both scenarios, we\ndemonstrate that our triggers can totally control or severely decrease the\nperformance of prompt-based models fine-tuned on arbitrary downstream tasks,\nreflecting the universal vulnerability of the prompt-based learning paradigm.\nFurther experiments show that adversarial triggers have good transferability\namong language models. We also find conventional fine-tuning models are not\nvulnerable to adversarial triggers constructed from pre-trained language\nmodels. We conclude by proposing a potential solution to mitigate our attack\nmethods. Code and data are publicly available at\nhttps://github.com/leix28/prompt-universal-vulnerability", "published": "2022-04-11 16:34:10", "link": "http://arxiv.org/abs/2204.05239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis", "abstract": "Sentiment analysis is an important task in natural language processing. In\nrecent works, pre-trained language models are often used to achieve\nstate-of-the-art results, especially when training data is scarce. It is common\nto fine-tune on the downstream task, usually by adding task-specific layers on\ntop of the model. In this paper, we focus on aspect-based sentiment analysis,\nwhich involves extracting aspect term, category, and predicting their\ncorresponding polarities. In particular, we are interested in few-shot\nsettings. We propose to reformulate the extraction and prediction tasks into\nthe sequence generation task, using a generative language model with\nunidirectional attention (GPT2 is used unless stated otherwise). This way, the\nmodel learns to accomplish the tasks via language generation without the need\nof training task-specific layers. Our evaluation results on the single-task\npolarity prediction show that our approach outperforms the previous\nstate-of-the-art (based on BERT) on average performance by a large margins in\nfew-shot and full-shot settings. More importantly, our generative approach\nsignificantly reduces the model variance caused by low-resource data. We\nfurther demonstrate that the proposed generative language model can handle\njoint and multi-task settings, unlike previous work. We observe that the\nproposed sequence generation method achieves further improved performances on\npolarity prediction when the model is trained via joint and multi-task\nsettings. Further evaluation on similar sentiment analysis datasets, SST-2,\nSST- and OOS intent detection validates the superiority and noise robustness of\ngenerative language model in few-shot settings.", "published": "2022-04-11 18:31:53", "link": "http://arxiv.org/abs/2204.05356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Speech-Text Pre-training for Speech Translation and Recognition", "abstract": "We describe a method to jointly pre-train speech and text in an\nencoder-decoder modeling framework for speech translation and recognition. The\nproposed method incorporates four self-supervised and supervised subtasks for\ncross modality learning. A self-supervised speech subtask leverages unlabelled\nspeech data, and a (self-)supervised text to text subtask makes use of abundant\ntext training data. Two auxiliary supervised speech tasks are included to unify\nspeech and text modeling space. Our contribution lies in integrating linguistic\ninformation from the text corpus into the speech pre-training. Detailed\nanalysis reveals learning interference among subtasks. Two pre-training\nconfigurations for speech translation and recognition, respectively, are\npresented to alleviate subtask interference. Our experiments show the proposed\nmethod can effectively fuse speech and text information into one model. It\nachieves between 1.7 and 2.3 BLEU improvement above the state of the art on the\nMuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the\nLibrispeech speech recognition task.", "published": "2022-04-11 20:59:51", "link": "http://arxiv.org/abs/2204.05409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Call for Clarity in Beam Search: How It Works and When It Stops", "abstract": "Text generation with beam search has proven successful in a wide range of\napplications. We point out that, though largely overlooked in the literature,\nthe commonly-used implementation of beam decoding (e.g., Hugging Face\nTransformers and fairseq) uses a first come, first served heuristic: it keeps a\nset of already completed sequences over time steps and stops when the size of\nthis set reaches the beam size. Based on this finding, we introduce a patience\nfactor, a simple modification to this beam decoding implementation, that\ngeneralizes the stopping criterion and provides flexibility to the depth of\nsearch. Empirical results demonstrate that adjusting this patience factor\nimproves decoding performance of strong pretrained models on news text\nsummarization and machine translation over diverse language pairs, with a\nnegligible inference slowdown. Our approach only modifies one line of code and\ncan be thus readily incorporated in any implementation. Further, we find that\ndifferent versions of beam decoding result in large performance differences in\nsummarization, demonstrating the need for clarity in specifying the beam search\nimplementation in research work. Our code will be available upon publication.", "published": "2022-04-11 22:03:44", "link": "http://arxiv.org/abs/2204.05424v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Conservative are Language Models? Adapting to the Introduction of\n  Gender-Neutral Pronouns", "abstract": "Gender-neutral pronouns have recently been introduced in many languages to a)\ninclude non-binary people and b) as a generic singular. Recent results from\npsycholinguistics suggest that gender-neutral pronouns (in Swedish) are not\nassociated with human processing difficulties. This, we show, is in sharp\ncontrast with automated processing. We show that gender-neutral pronouns in\nDanish, English, and Swedish are associated with higher perplexity, more\ndispersed attention patterns, and worse downstream performance. We argue that\nsuch conservativity in language models may limit widespread adoption of\ngender-neutral pronouns and must therefore be resolved.", "published": "2022-04-11 09:42:02", "link": "http://arxiv.org/abs/2204.10281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embeddings Are Capable of Capturing Rhythmic Similarity of Words", "abstract": "Word embedding systems such as Word2Vec and GloVe are well-known in deep\nlearning approaches to NLP. This is largely due to their ability to capture\nsemantic relationships between words. In this work we investigated their\nusefulness in capturing rhythmic similarity of words instead. The results show\nthat vectors these embeddings assign to rhyming words are more similar to each\nother, compared to the other words. It is also revealed that GloVe performs\nrelatively better than Word2Vec in this regard. We also proposed a first of its\nkind metric for quantifying rhythmic similarity of a pair of words.", "published": "2022-04-11 02:33:23", "link": "http://arxiv.org/abs/2204.04833v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and\n  Challenges", "abstract": "Legal judgment prediction (LJP) applies Natural Language Processing (NLP)\ntechniques to predict judgment results based on fact descriptions\nautomatically. Recently, large-scale public datasets and advances in NLP\nresearch have led to increasing interest in LJP. Despite a clear gap between\nmachine and human performance, impressive results have been achieved in various\nbenchmark datasets. In this paper, to address the current lack of comprehensive\nsurvey of existing LJP tasks, datasets, models and evaluations, (1) we analyze\n31 LJP datasets in 6 languages, present their construction process and define a\nclassification method of LJP with 3 different attributes; (2) we summarize 14\nevaluation metrics under four categories for different outputs of LJP tasks;\n(3) we review 12 legal-domain pretrained models in 3 languages and highlight 3\nmajor research directions for LJP; (4) we show the state-of-art results for 8\nrepresentative datasets from different court cases and discuss the open\nchallenges. This paper can provide up-to-date and comprehensive reviews to help\nreaders understand the status of LJP. We hope to facilitate both NLP\nresearchers and legal professionals for further joint efforts in this problem.", "published": "2022-04-11 04:06:28", "link": "http://arxiv.org/abs/2204.04859v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-lingual Conversational Semantic Role Labeling", "abstract": "While conversational semantic role labeling (CSRL) has shown its usefulness\non Chinese conversational tasks, it is still under-explored in non-Chinese\nlanguages due to the lack of multilingual CSRL annotations for the parser\ntraining. To avoid expensive data collection and error-propagation of\ntranslation-based methods, we present a simple but effective approach to\nperform zero-shot cross-lingual CSRL. Our model implicitly learns\nlanguage-agnostic, conversational structure-aware and semantically rich\nrepresentations with the hierarchical encoders and elaborately designed\npre-training objectives. Experimental results show that our model outperforms\nall baselines by large margins on two newly collected English CSRL test sets.\nMore importantly, we confirm the usefulness of CSRL to non-Chinese\nconversational tasks such as the question-in-context rewriting task in English\nand the multi-turn dialogue response generation tasks in English, German and\nJapanese by incorporating the CSRL information into the downstream\nconversation-based models. We believe this finding is significant and will\nfacilitate the research of non-Chinese dialogue tasks which suffer the problems\nof ellipsis and anaphora.", "published": "2022-04-11 07:29:39", "link": "http://arxiv.org/abs/2204.04914v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessment of Massively Multilingual Sentiment Classifiers", "abstract": "Models are increasing in size and complexity in the hunt for SOTA. But what\nif those 2\\% increase in performance does not make a difference in a production\nuse case? Maybe benefits from a smaller, faster model outweigh those slight\nperformance gains. Also, equally good performance across languages in\nmultilingual tasks is more important than SOTA results on a single one. We\npresent the biggest, unified, multilingual collection of sentiment analysis\ndatasets. We use these to assess 11 models and 80 high-quality sentiment\ndatasets (out of 342 raw datasets collected) in 27 languages and included\nresults on the internally annotated datasets. We deeply evaluate multiple\nsetups, including fine-tuning transformer-based models for measuring\nperformance. We compare results in numerous dimensions addressing the imbalance\nin both languages coverage and dataset sizes. Finally, we present some best\npractices for working with such a massive collection of datasets and models\nfrom a multilingual perspective.", "published": "2022-04-11 08:22:05", "link": "http://arxiv.org/abs/2204.04937v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regularization-based Pruning of Irrelevant Weights in Deep Neural\n  Architectures", "abstract": "Deep neural networks exploiting millions of parameters are nowadays the norm\nin deep learning applications. This is a potential issue because of the great\namount of computational resources needed for training, and of the possible loss\nof generalization performance of overparametrized networks. We propose in this\npaper a method for learning sparse neural topologies via a regularization\ntechnique which identifies non relevant weights and selectively shrinks their\nnorm, while performing a classic update for relevant ones. This technique,\nwhich is an improvement of classical weight decay, is based on the definition\nof a regularization term which can be added to any loss functional regardless\nof its form, resulting in a unified general framework exploitable in many\ndifferent contexts. The actual elimination of parameters identified as\nirrelevant is handled by an iterative pruning algorithm. We tested the proposed\ntechnique on different image classification and Natural language generation\ntasks, obtaining results on par or better then competitors in terms of sparsity\nand metrics, while achieving strong models compression.", "published": "2022-04-11 09:44:16", "link": "http://arxiv.org/abs/2204.04977v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Team \u00daFAL at CMCL 2022 Shared Task: Figuring out the correct recipe\n  for predicting Eye-Tracking features using Pretrained Language Models", "abstract": "Eye-Tracking data is a very useful source of information to study cognition\nand especially language comprehension in humans. In this paper, we describe our\nsystems for the CMCL 2022 shared task on predicting eye-tracking information.\nWe describe our experiments with pretrained models like BERT and XLM and the\ndifferent ways in which we used those representations to predict four\neye-tracking features. Along with analysing the effect of using two different\nkinds of pretrained multilingual language models and different ways of pooling\nthe tokenlevel representations, we also explore how contextual information\naffects the performance of the systems. Finally, we also explore if factors\nlike augmenting linguistic information affect the predictions. Our submissions\nachieved an average MAE of 5.72 and ranked 5th in the shared task. The average\nMAE showed further reduction to 5.25 in post task evaluation.", "published": "2022-04-11 10:43:34", "link": "http://arxiv.org/abs/2204.04998v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linguistic communication as (inverse) reward design", "abstract": "Natural language is an intuitive and expressive way to communicate reward\ninformation to autonomous agents. It encompasses everything from concrete\ninstructions to abstract descriptions of the world. Despite this, natural\nlanguage is often challenging to learn from: it is difficult for machine\nlearning methods to make appropriate inferences from such a wide range of\ninput. This paper proposes a generalization of reward design as a unifying\nprinciple to ground linguistic communication: speakers choose utterances to\nmaximize expected rewards from the listener's future behaviors. We first extend\nreward design to incorporate reasoning about unknown future states in a linear\nbandit setting. We then define a speaker model which chooses utterances\naccording to this objective. Simulations show that short-horizon speakers\n(reasoning primarily about a single, known state) tend to use instructions,\nwhile long-horizon speakers (reasoning primarily about unknown, future states)\ntend to describe the reward function. We then define a pragmatic listener which\nperforms inverse reward design by jointly inferring the speaker's latent\nhorizon and rewards. Our findings suggest that this extension of reward design\nto linguistic communication, including the notion of a latent speaker horizon,\nis a promising direction for achieving more robust alignment outcomes from\nnatural language supervision.", "published": "2022-04-11 13:50:34", "link": "http://arxiv.org/abs/2204.05091v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards End-to-End Integration of Dialog History for Improved Spoken\n  Language Understanding", "abstract": "Dialog history plays an important role in spoken language understanding (SLU)\nperformance in a dialog system. For end-to-end (E2E) SLU, previous work has\nused dialog history in text form, which makes the model dependent on a cascaded\nautomatic speech recognizer (ASR). This rescinds the benefits of an E2E system\nwhich is intended to be compact and robust to ASR errors. In this paper, we\npropose a hierarchical conversation model that is capable of directly using\ndialog history in speech form, making it fully E2E. We also distill semantic\nknowledge from the available gold conversation transcripts by jointly training\na similar text-based conversation model with an explicit tying of acoustic and\nsemantic embeddings. We also propose a novel technique that we call DropFrame\nto deal with the long training time incurred by adding dialog history in an E2E\nmanner. On the HarperValleyBank dialog dataset, our E2E history integration\noutperforms a history independent baseline by 7.7% absolute F1 score on the\ntask of dialog action recognition. Our model performs competitively with the\nstate-of-the-art history based cascaded baseline, but uses 48% fewer\nparameters. In the absence of gold transcripts to fine-tune an ASR model, our\nmodel outperforms this baseline by a significant margin of 10% absolute F1\nscore.", "published": "2022-04-11 14:56:05", "link": "http://arxiv.org/abs/2204.05169v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uniform Complexity for Text Generation", "abstract": "Large language models (LLMs) have shown promising results in a wide array of\ngenerative NLP tasks, such as summarization and machine translation. In the\ncontext of narrative generation, however, existing models still do not capture\nfactors that contribute to producing consistent text. For instance, it is\nlogical that a piece of text or a story should be uniformly readable throughout\nand that this form of complexity should be controllable. As such, if the\ncomplexity of an input text prompt is rated first-grade reading level in the\nFlesch Reading Ease test, then the generated text continuing the plot should\nalso be within this range of complexity. With this in mind, we introduce\nUniform Complexity for Text Generation (UCTG), a new benchmark test which\nraises the challenge of making generative models observe uniform linguistic\nproperties with respect to prompts. We experiment with over 150+ linguistically\nand cognitively motivated features for evaluating text complexity in humans and\ngenerative models. From our results, we find that models such as GPT-2 struggle\nto preserve the complexity of input prompts used in its generations, even if\nfinetuned with professionally written texts.", "published": "2022-04-11 15:19:47", "link": "http://arxiv.org/abs/2204.05185v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"FIJO\": a French Insurance Soft Skill Detection Dataset", "abstract": "Understanding the evolution of job requirements is becoming more important\nfor workers, companies and public organizations to follow the fast\ntransformation of the employment market. Fortunately, recent natural language\nprocessing (NLP) approaches allow for the development of methods to\nautomatically extract information from job ads and recognize skills more\nprecisely. However, these efficient approaches need a large amount of annotated\ndata from the studied domain which is difficult to access, mainly due to\nintellectual property. This article proposes a new public dataset, FIJO,\ncontaining insurance job offers, including many soft skill annotations. To\nunderstand the potential of this dataset, we detail some characteristics and\nsome limitations. Then, we present the results of skill detection algorithms\nusing a named entity recognition approach and show that transformers-based\nmodels have good token-wise performances on this dataset. Lastly, we analyze\nsome errors made by our best model to emphasize the difficulties that may arise\nwhen applying NLP approaches.", "published": "2022-04-11 15:54:22", "link": "http://arxiv.org/abs/2204.05208v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap between Language Models and Cross-Lingual Sequence\n  Labeling", "abstract": "Large-scale cross-lingual pre-trained language models (xPLMs) have shown\neffectiveness in cross-lingual sequence labeling tasks (xSL), such as\ncross-lingual machine reading comprehension (xMRC) by transferring knowledge\nfrom a high-resource language to low-resource languages. Despite the great\nsuccess, we draw an empirical observation that there is a training objective\ngap between pre-training and fine-tuning stages: e.g., mask language modeling\nobjective requires local understanding of the masked token and the\nspan-extraction objective requires global understanding and reasoning of the\ninput passage/paragraph and question, leading to the discrepancy between\npre-training and xMRC. In this paper, we first design a pre-training task\ntailored for xSL named Cross-lingual Language Informative Span Masking (CLISM)\nto eliminate the objective gap in a self-supervised manner. Second, we present\nContrAstive-Consistency Regularization (CACR), which utilizes contrastive\nlearning to encourage the consistency between representations of input parallel\nsequences via unsupervised cross-lingual instance-wise training signals during\npre-training. By these means, our methods not only bridge the gap between\npretrain-finetune, but also enhance PLMs to better capture the alignment\nbetween different languages. Extensive experiments prove that our method\nachieves clearly superior results on multiple xSL benchmarks with limited\npre-training data. Our methods also surpass the previous state-of-the-art\nmethods by a large margin in few-shot data settings, where only a few hundred\ntraining examples are available.", "published": "2022-04-11 15:55:20", "link": "http://arxiv.org/abs/2204.05210v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Survey of Aspect-based Sentiment Analysis Datasets", "abstract": "Aspect-based sentiment analysis (ABSA) is a natural language processing\nproblem that requires analyzing user-generated reviews to determine: a) The\ntarget entity being reviewed, b) The high-level aspect to which it belongs, and\nc) The sentiment expressed toward the targets and the aspects. Numerous yet\nscattered corpora for ABSA make it difficult for researchers to identify\ncorpora best suited for a specific ABSA subtask quickly. This study aims to\npresent a database of corpora that can be used to train and assess autonomous\nABSA systems. Additionally, we provide an overview of the major corpora for\nABSA and its subtasks and highlight several features that researchers should\nconsider when selecting a corpus. Finally, we discuss the advantages and\ndisadvantages of current collection approaches and make recommendations for\nfuture corpora creation. This survey examines 65 publicly available ABSA\ndatasets covering over 25 domains, including 45 English and 20 other languages\ndatasets.", "published": "2022-04-11 16:23:36", "link": "http://arxiv.org/abs/2204.05232v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Downstream Task by Selectively Capturing Complementary\n  Knowledge from Multiple Self-supervisedly Learning Pretexts", "abstract": "Self-supervised learning (SSL), as a newly emerging unsupervised\nrepresentation learning paradigm, generally follows a two-stage learning\npipeline: 1) learning invariant and discriminative representations with\nauto-annotation pretext(s), then 2) transferring the representations to assist\ndownstream task(s). Such two stages are usually implemented separately, making\nthe learned representation learned agnostic to the downstream tasks. Currently,\nmost works are devoted to exploring the first stage. Whereas, it is less\nstudied on how to learn downstream tasks with limited labeled data using the\nalready learned representations. Especially, it is crucial and challenging to\nselectively utilize the complementary representations from diverse pretexts for\na downstream task. In this paper, we technically propose a novel solution by\nleveraging the attention mechanism to adaptively squeeze suitable\nrepresentations for the tasks. Meanwhile, resorting to information theory, we\ntheoretically prove that gathering representation from diverse pretexts is more\neffective than a single one. Extensive experiments validate that our scheme\nsignificantly exceeds current popular pretext-matching based methods in\ngathering knowledge and relieving negative transfer in downstream tasks.", "published": "2022-04-11 16:46:50", "link": "http://arxiv.org/abs/2204.05248v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Toward More Effective Human Evaluation for Machine Translation", "abstract": "Improvements in text generation technologies such as machine translation have\nnecessitated more costly and time-consuming human evaluation procedures to\nensure an accurate signal. We investigate a simple way to reduce cost by\nreducing the number of text segments that must be annotated in order to\naccurately predict a score for a complete test set. Using a sampling approach,\nwe demonstrate that information from document membership and automatic metrics\ncan help improve estimates compared to a pure random sampling baseline. We\nachieve gains of up to 20% in average absolute error by leveraging stratified\nsampling and control variates. Our techniques can improve estimates made from a\nfixed annotation budget, are easy to implement, and can be applied to any\nproblem with structure similar to the one we study.", "published": "2022-04-11 17:59:22", "link": "http://arxiv.org/abs/2204.05307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Scale Streaming End-to-End Speech Translation with Neural\n  Transducers", "abstract": "Neural transducers have been widely used in automatic speech recognition\n(ASR). In this paper, we introduce it to streaming end-to-end speech\ntranslation (ST), which aims to convert audio signals to texts in other\nlanguages directly. Compared with cascaded ST that performs ASR followed by\ntext-based machine translation (MT), the proposed Transformer transducer\n(TT)-based ST model drastically reduces inference latency, exploits speech\ninformation, and avoids error propagation from ASR to MT. To improve the\nmodeling capacity, we propose attention pooling for the joint network in TT. In\naddition, we extend TT-based ST to multilingual ST, which generates texts of\nmultiple languages at the same time. Experimental results on a large-scale 50\nthousand (K) hours pseudo-labeled training set show that TT-based ST not only\nsignificantly reduces inference time but also outperforms non-streaming\ncascaded ST for English-German translation.", "published": "2022-04-11 18:18:53", "link": "http://arxiv.org/abs/2204.05352v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Multilingual Perspective Towards the Evaluation of Attribution Methods\n  in Natural Language Inference", "abstract": "Most evaluations of attribution methods focus on the English language. In\nthis work, we present a multilingual approach for evaluating attribution\nmethods for the Natural Language Inference (NLI) task in terms of faithfulness\nand plausibility. First, we introduce a novel cross-lingual strategy to measure\nfaithfulness based on word alignments, which eliminates the drawbacks of\nerasure-based evaluations.We then perform a comprehensive evaluation of\nattribution methods, considering different output mechanisms and aggregation\nmethods. Finally, we augment the XNLI dataset with highlight-based\nexplanations, providing a multilingual NLI dataset with highlights, to support\nfuture exNLP studies. Our results show that attribution methods performing best\nfor plausibility and faithfulness are different.", "published": "2022-04-11 22:11:05", "link": "http://arxiv.org/abs/2204.05428v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Doctor XAvIer: Explainable Diagnosis on Physician-Patient Dialogues and\n  XAI Evaluation", "abstract": "We introduce Doctor XAvIer, a BERT-based diagnostic system that extracts\nrelevant clinical data from transcribed patient-doctor dialogues and explains\npredictions using feature attribution methods. We present a novel performance\nplot and evaluation metric for feature attribution methods: Feature Attribution\nDropping (FAD) curve and its Normalized Area Under the Curve (N-AUC). FAD curve\nanalysis shows that integrated gradients outperforms Shapley values in\nexplaining diagnosis classification. Doctor XAvIer outperforms the baseline\nwith 0.97 F1-score in named entity recognition and symptom pertinence\nclassification and 0.91 F1-score in diagnosis classification.", "published": "2022-04-11 18:38:22", "link": "http://arxiv.org/abs/2204.10178v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explanation Graph Generation via Pre-trained Language Models: An\n  Empirical Study with Contrastive Learning", "abstract": "Pre-trained sequence-to-sequence language models have led to widespread\nsuccess in many natural language generation tasks. However, there has been\nrelatively less work on analyzing their ability to generate structured outputs\nsuch as graphs. Unlike natural language, graphs have distinct structural and\nsemantic properties in the context of a downstream NLP task, e.g., generating a\ngraph that is connected and acyclic can be attributed to its structural\nconstraints, while the semantics of a graph can refer to how meaningfully an\nedge represents the relation between two node concepts. In this work, we study\npre-trained language models that generate explanation graphs in an end-to-end\nmanner and analyze their ability to learn the structural constraints and\nsemantics of such graphs. We first show that with limited supervision,\npre-trained language models often generate graphs that either violate these\nconstraints or are semantically incoherent. Since curating large amount of\nhuman-annotated graphs is expensive and tedious, we propose simple yet\neffective ways of graph perturbations via node and edge edit operations that\nlead to structurally and semantically positive and negative graphs. Next, we\nleverage these graphs in different contrastive learning models with Max-Margin\nand InfoNCE losses. Our methods lead to significant improvements in both\nstructural and semantic accuracy of explanation graphs and also generalize to\nother similar graph generation tasks. Lastly, we show that human errors are the\nbest negatives for contrastive learning and also that automatically generating\nmore such human-like negative graphs can lead to further improvements. Our code\nand models are publicly available at https://github.com/swarnaHub/ExplagraphGen", "published": "2022-04-11 00:58:27", "link": "http://arxiv.org/abs/2204.04813v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fusion of Self-supervised Learned Models for MOS Prediction", "abstract": "We participated in the mean opinion score (MOS) prediction challenge, 2022.\nThis challenge aims to predict MOS scores of synthetic speech on two tracks,\nthe main track and a more challenging sub-track: out-of-domain (OOD). To\nimprove the accuracy of the predicted scores, we have explored several model\nfusion-related strategies and proposed a fused framework in which seven\npretrained self-supervised learned (SSL) models have been engaged. These\npretrained SSL models are derived from three ASR frameworks, including Wav2Vec,\nHubert, and WavLM. For the OOD track, we followed the 7 SSL models selected on\nthe main track and adopted a semi-supervised learning method to exploit the\nunlabeled data. According to the official analysis results, our system has\nachieved 1st rank in 6 out of 16 metrics and is one of the top 3 systems for 13\nout of 16 metrics. Specifically, we have achieved the highest LCC, SRCC, and\nKTAU scores at the system level on main track, as well as the best performance\non the LCC, SRCC, and KTAU evaluation metrics at the utterance level on OOD\ntrack. Compared with the basic SSL models, the prediction accuracy of the fused\nsystem has been largely improved, especially on OOD sub-track.", "published": "2022-04-11 03:50:52", "link": "http://arxiv.org/abs/2204.04855v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adapting BigScience Multilingual Model to Unseen Languages", "abstract": "We benchmark different strategies of adding new languages (German and Korean)\ninto the BigScience's pretrained multilingual language model with 1.3 billion\nparameters that currently supports 13 languages. We investigate the factors\nthat affect the language adaptability of the model and the trade-offs between\ncomputational costs and expected performance.", "published": "2022-04-11 05:32:14", "link": "http://arxiv.org/abs/2204.04873v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Methods of Informational Trends Analytics and Fake News Detection on\n  Twitter", "abstract": "In the paper, different approaches for the analysis of news trends on Twitter\nhas been considered. For the analysis and case study, informational trends on\nTwitter caused by Russian invasion of Ukraine in 2022 year have been studied. A\ndeep learning approach for fake news detection has been analyzed. The use of\nthe theory of frequent itemsets and association rules, graph theory for news\ntrends analytics have been considered.", "published": "2022-04-11 06:26:10", "link": "http://arxiv.org/abs/2204.04891v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multistream neural architectures for cued-speech recognition using a\n  pre-trained visual feature extractor and constrained CTC decoding", "abstract": "This paper proposes a simple and effective approach for automatic recognition\nof Cued Speech (CS), a visual communication tool that helps people with hearing\nimpairment to understand spoken language with the help of hand gestures that\ncan uniquely identify the uttered phonemes in complement to lipreading. The\nproposed approach is based on a pre-trained hand and lips tracker used for\nvisual feature extraction and a phonetic decoder based on a multistream\nrecurrent neural network trained with connectionist temporal classification\nloss and combined with a pronunciation lexicon. The proposed system is\nevaluated on an updated version of the French CS dataset CSF18 for which the\nphonetic transcription has been manually checked and corrected. With a decoding\naccuracy at the phonetic level of 70.88%, the proposed system outperforms our\nprevious CNN-HMM decoder and competes with more complex baselines.", "published": "2022-04-11 09:30:08", "link": "http://arxiv.org/abs/2204.04965v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fine-grained Noise Control for Multispeaker Speech Synthesis", "abstract": "A text-to-speech (TTS) model typically factorizes speech attributes such as\ncontent, speaker and prosody into disentangled representations.Recent works aim\nto additionally model the acoustic conditions explicitly, in order to\ndisentangle the primary speech factors, i.e. linguistic content, prosody and\ntimbre from any residual factors, such as recording conditions and background\nnoise.This paper proposes unsupervised, interpretable and fine-grained noise\nand prosody modeling. We incorporate adversarial training, representation\nbottleneck and utterance-to-frame modeling in order to learn frame-level noise\nrepresentations. To the same end, we perform fine-grained prosody modeling via\na Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results\nin more expressive speech synthesis.", "published": "2022-04-11 13:13:55", "link": "http://arxiv.org/abs/2204.05070v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Speech Translation for Code Switched Speech", "abstract": "Code switching (CS) refers to the phenomenon of interchangeably using words\nand phrases from different languages. CS can pose significant accuracy\nchallenges to NLP, due to the often monolingual nature of the underlying\nsystems. In this work, we focus on CS in the context of English/Spanish\nconversations for the task of speech translation (ST), generating and\nevaluating both transcript and translation. To evaluate model performance on\nthis task, we create a novel ST corpus derived from existing public data sets.\nWe explore various ST architectures across two dimensions: cascaded (transcribe\nthen translate) vs end-to-end (jointly transcribe and translate) and\nunidirectional (source -> target) vs bidirectional (source <-> target). We show\nthat our ST architectures, and especially our bidirectional end-to-end\narchitecture, perform well on CS speech, even when no CS training data is used.", "published": "2022-04-11 13:25:30", "link": "http://arxiv.org/abs/2204.05076v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Building an ASR Error Robust Spoken Virtual Patient System in a Highly\n  Class-Imbalanced Scenario Without Speech Data", "abstract": "A Virtual Patient (VP) is a powerful tool for training medical students to\ntake patient histories, where responding to a diverse set of spoken questions\nis essential to simulate natural conversations with a student. The performance\nof such a Spoken Language Understanding system (SLU) can be adversely affected\nby both the presence of Automatic Speech Recognition (ASR) errors in the test\ndata and a high degree of class imbalance in the SLU training data. While these\ntwo issues have been addressed separately in prior work, we develop a novel\ntwo-step training methodology that tackles both these issues effectively in a\nsingle dialog agent. As it is difficult to collect spoken data from users\nwithout a functioning SLU system, our method does not rely on spoken data for\ntraining, rather we use an ASR error predictor to \"speechify\" the text data.\nOur method shows significant improvements over strong baselines on the VP\nintent classification task at various word error rate settings.", "published": "2022-04-11 15:13:33", "link": "http://arxiv.org/abs/2204.05183v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in\n  End-to-End Speech-to-Intent Systems", "abstract": "Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have\nbeen primarily due to effective pretraining of speech representations. One such\npretraining paradigm is the distillation of semantic knowledge from\nstate-of-the-art text-based models like BERT to speech encoder neural networks.\nThis work is a step towards doing the same in a much more efficient and\nfine-grained manner where we align speech embeddings and BERT embeddings on a\ntoken-by-token basis. We introduce a simple yet novel technique that uses a\ncross-modal attention mechanism to extract token-level contextual embeddings\nfrom a speech encoder such that these can be directly compared and aligned with\nBERT based contextual embeddings. This alignment is performed using a novel\ntokenwise contrastive loss. Fine-tuning such a pretrained model to perform\nintent recognition using speech directly yields state-of-the-art performance on\ntwo widely used SLU datasets. Our model improves further when fine-tuned with\nadditional regularization using SpecAugment especially when speech is noisy,\ngiving an absolute improvement as high as 8% over previous results.", "published": "2022-04-11 15:24:25", "link": "http://arxiv.org/abs/2204.05188v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Generalizable Semantic Product Search by Text Similarity\n  Pre-training on Search Click Logs", "abstract": "Recently, semantic search has been successfully applied to e-commerce product\nsearch and the learned semantic space(s) for query and product encoding are\nexpected to generalize to unseen queries or products. Yet, whether\ngeneralization can conveniently emerge has not been thoroughly studied in the\ndomain thus far. In this paper, we examine several general-domain and\ndomain-specific pre-trained Roberta variants and discover that general-domain\nfine-tuning does not help generalization, which aligns with the discovery of\nprior art. Proper domain-specific fine-tuning with clickstream data can lead to\nbetter model generalization, based on a bucketed analysis of a publicly\navailable manual annotated query-product pair da", "published": "2022-04-11 16:23:35", "link": "http://arxiv.org/abs/2204.05231v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "F.2.2; I.2.7"], "primary_category": "cs.IR"}
{"title": "Position-wise optimizer: A nature-inspired optimization algorithm", "abstract": "The human nervous system utilizes synaptic plasticity to solve optimization\nproblems. Previous studies have tried to add the plasticity factor to the\ntraining process of artificial neural networks, but most of those models\nrequire complex external control over the network or complex novel rules. In\nthis manuscript, a novel nature-inspired optimization algorithm is introduced\nthat imitates biological neural plasticity. Furthermore, the model is tested on\nthree datasets and the results are compared with gradient descent optimization.", "published": "2022-04-11 15:30:52", "link": "http://arxiv.org/abs/2204.05312v1", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors", "abstract": "We present ProtoTEx, a novel white-box NLP classification architecture based\non prototype networks. ProtoTEx faithfully explains model decisions based on\nprototype tensors that encode latent clusters of training examples. At\ninference time, classification decisions are based on the distances between the\ninput text and the prototype tensors, explained via the training examples most\nsimilar to the most influential prototypes. We also describe a novel\ninterleaved training algorithm that effectively handles classes characterized\nby the absence of indicative features. On a propaganda detection task, ProtoTEx\naccuracy matches BART-large and exceeds BERT-large with the added benefit of\nproviding faithful explanations. A user study also shows that prototype-based\nexplanations help non-experts to better recognize propaganda in online news.", "published": "2022-04-11 22:08:45", "link": "http://arxiv.org/abs/2204.05426v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Production federated keyword spotting via distillation, filtering, and\n  joint federated-centralized training", "abstract": "We trained a keyword spotting model using federated learning on real user\ndevices and observed significant improvements when the model was deployed for\ninference on phones. To compensate for data domains that are missing from\non-device training caches, we employed joint federated-centralized training.\nAnd to learn in the absence of curated labels on-device, we formulated a\nconfidence filtering strategy based on user-feedback signals for federated\ndistillation. These techniques created models that significantly improved\nquality metrics in offline evaluations and user-experience metrics in live A/B\nexperiments.", "published": "2022-04-11 18:16:41", "link": "http://arxiv.org/abs/2204.06322v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Correcting Robot Plans with Natural Language Feedback", "abstract": "When humans design cost or goal specifications for robots, they often produce\nspecifications that are ambiguous, underspecified, or beyond planners' ability\nto solve. In these cases, corrections provide a valuable tool for\nhuman-in-the-loop robot control. Corrections might take the form of new goal\nspecifications, new constraints (e.g. to avoid specific objects), or hints for\nplanning algorithms (e.g. to visit specific waypoints). Existing correction\nmethods (e.g. using a joystick or direct manipulation of an end effector)\nrequire full teleoperation or real-time interaction. In this paper, we explore\nnatural language as an expressive and flexible tool for robot correction. We\ndescribe how to map from natural language sentences to transformations of cost\nfunctions. We show that these transformations enable users to correct goals,\nupdate robot motions to accommodate additional user preferences, and recover\nfrom planning errors. These corrections can be leveraged to get 81% and 93%\nsuccess rates on tasks where the original planner failed, with either one or\ntwo language corrections. Our method makes it possible to compose multiple\nconstraints and generalizes to unseen scenes, objects, and sentences in\nsimulated environments and real-world environments.", "published": "2022-04-11 15:22:43", "link": "http://arxiv.org/abs/2204.05186v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Listen only to me! How well can target speech extraction handle false\n  alarms?", "abstract": "Target speech extraction (TSE) extracts the speech of a target speaker in a\nmixture given auxiliary clues characterizing the speaker, such as an enrollment\nutterance. TSE addresses thus the challenging problem of simultaneously\nperforming separation and speaker identification. There has been much progress\nin extraction performance following the recent development of neural networks\nfor speech enhancement and separation. Most studies have focused on processing\nmixtures where the target speaker is actively speaking. However, the target\nspeaker is sometimes silent in practice, i.e., inactive speaker (IS). A typical\nTSE system will tend to output a signal in IS cases, causing false alarms. It\nis a severe problem for the practical deployment of TSE systems. This paper\naims at understanding better how well TSE systems can handle IS cases. We\nconsider two approaches to deal with IS, (1) training a system to directly\noutput zero signals or (2) detecting IS with an extra speaker verification\nmodule. We perform an extensive experimental comparison of these schemes in\nterms of extraction performance and IS detection using the LibriMix dataset and\nreveal their pros and cons.", "published": "2022-04-11 00:52:20", "link": "http://arxiv.org/abs/2204.04811v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "How to Listen? Rethinking Visual Sound Localization", "abstract": "Localizing visual sounds consists on locating the position of objects that\nemit sound within an image. It is a growing research area with potential\napplications in monitoring natural and urban environments, such as wildlife\nmigration and urban traffic. Previous works are usually evaluated with datasets\nhaving mostly a single dominant visible object, and proposed models usually\nrequire the introduction of localization modules during training or dedicated\nsampling strategies, but it remains unclear how these design choices play a\nrole in the adaptability of these methods in more challenging scenarios. In\nthis work, we analyze various model choices for visual sound localization and\ndiscuss how their different components affect the model's performance, namely\nthe encoders' architecture, the loss function and the localization strategy.\nFurthermore, we study the interaction between these decisions, the model\nperformance, and the data, by digging into different evaluation datasets\nspanning different difficulties and characteristics, and discuss the\nimplications of such decisions in the context of real-world applications. Our\ncode and model weights are open-sourced and made available for further\napplications.", "published": "2022-04-11 14:41:35", "link": "http://arxiv.org/abs/2204.05156v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge", "abstract": "Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams\ncaused by data transmission failures in packet switched networks. This is a\ncommon problem, and of increasing importance as end-to-end VoIP telephony and\nteleconference systems become the default and ever more widely used form of\ncommunication in business as well as in personal usage. This paper presents the\nINTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an\noverview of the PLC problem, and introduce some classical approaches to PLC as\nwell as recent work. We then present the open source dataset released as part\nof this challenge as well as the evaluation methods and metrics used to\ndetermine the winner. We also briefly introduce PLCMOS, a novel data-driven\nmetric that can be used to quickly evaluate the performance PLC systems.\nFinally, we present the results of the INTERSPEECH 2022 Audio Deep PLC\nChallenge, and provide a summary of important takeaways.", "published": "2022-04-11 16:13:36", "link": "http://arxiv.org/abs/2204.05222v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Small Footprint Multi-channel ConvMixer for Keyword Spotting with\n  Centroid Based Awareness", "abstract": "It is critical for a keyword spotting model to have a small footprint as it\ntypically runs on-device with low computational resources. However, maintaining\nthe previous SOTA performance with reduced model size is challenging. In\naddition, a far-field and noisy environment with multiple signals interference\naggravates the problem causing the accuracy to degrade significantly. In this\npaper, we present a multi-channel ConvMixer for speech command recognitions.\nThe novel architecture introduces an additional audio channel mixing for\nchannel audio interaction in a multi-channel audio setting to achieve better\nnoise-robust features with more efficient computation. Besides, we proposed a\ncentroid based awareness component to enhance the system by equipping it with\nadditional spatial geometry information in the latent feature projection space.\nWe evaluate our model using the new MISP challenge 2021 dataset. Our model\nachieves significant improvement against the official baseline with a 55% gain\nin the competition score (0.152) on raw microphone array input and a 63%\n(0.126) boost upon front-end speech enhancement.", "published": "2022-04-11 23:41:25", "link": "http://arxiv.org/abs/2204.05445v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the pragmatism of using binary classifiers over data intensive neural\n  network classifiers for detection of COVID-19 from voice", "abstract": "Lately, there has been a global effort by multiple research groups to detect\nCOVID-19 from voice. Different researchers use different kinds of information\nfrom the voice signal to achieve this. Various types of phonated sounds and the\nsound of cough and breath have all been used with varying degree of success in\nautomated voice-based COVID-19 detection apps. In this paper, we show that\ndetecting COVID-19 from voice does not require custom-made non-standard\nfeatures or complicated neural network classifiers rather it can be\nsuccessfully done with just standard features and simple binary classifiers. In\nfact, we show that the latter is not only more accurate and interpretable but\nalso more computationally efficient in that they can be run locally on small\ndevices. We demonstrate this on a human-curated dataset of over 1000 subjects,\ncollected and calibrated in clinical settings.", "published": "2022-04-11 00:19:14", "link": "http://arxiv.org/abs/2204.04802v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The PartialSpoof Database and Countermeasures for the Detection of Short\n  Fake Speech Segments Embedded in an Utterance", "abstract": "Automatic speaker verification is susceptible to various manipulations and\nspoofing, such as text-to-speech synthesis, voice conversion, replay,\ntampering, adversarial attacks, and so on. We consider a new spoofing scenario\ncalled \"Partial Spoof\" (PS) in which synthesized or transformed speech segments\nare embedded into a bona fide utterance. While existing countermeasures (CMs)\ncan detect fully spoofed utterances, there is a need for their adaptation or\nextension to the PS scenario. We propose various improvements to construct a\nsignificantly more accurate CM that can detect and locate short-generated\nspoofed speech segments at finer temporal resolutions. First, we introduce\nnewly developed self-supervised pre-trained models as enhanced feature\nextractors. Second, we extend our PartialSpoof database by adding segment\nlabels for various temporal resolutions. Since the short spoofed speech\nsegments to be embedded by attackers are of variable length, six different\ntemporal resolutions are considered, ranging from as short as 20 ms to as large\nas 640 ms. Third, we propose a new CM that enables the simultaneous use of the\nsegment-level labels at different temporal resolutions as well as\nutterance-level labels to execute utterance- and segment-level detection at the\nsame time. We also show that the proposed CM is capable of detecting spoofing\nat the utterance level with low error rates in the PS scenario as well as in a\nrelated logical access (LA) scenario. The equal error rates of utterance-level\ndetection on the PartialSpoof database and ASVspoof 2019 LA database were 0.77\nand 0.90%, respectively.", "published": "2022-04-11 15:09:07", "link": "http://arxiv.org/abs/2204.05177v3", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
