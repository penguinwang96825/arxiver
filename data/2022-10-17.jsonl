{"title": "ConReader: Exploring Implicit Relations in Contracts for Contract Clause\n  Extraction", "abstract": "We study automatic Contract Clause Extraction (CCE) by modeling implicit\nrelations in legal contracts. Existing CCE methods mostly treat contracts as\nplain text, creating a substantial barrier to understanding contracts of high\ncomplexity. In this work, we first comprehensively analyze the complexity\nissues of contracts and distill out three implicit relations commonly found in\ncontracts, namely, 1) Long-range Context Relation that captures the\ncorrelations of distant clauses; 2) Term-Definition Relation that captures the\nrelation between important terms with their corresponding definitions; and 3)\nSimilar Clause Relation that captures the similarities between clauses of the\nsame type. Then we propose a novel framework ConReader to exploit the above\nthree relations for better contract understanding and improving CCE.\nExperimental results show that ConReader makes the prediction more\ninterpretable and achieves new state-of-the-art on two CCE tasks in both\nconventional and zero-shot settings.", "published": "2022-10-17 02:15:18", "link": "http://arxiv.org/abs/2210.08697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tencent AI Lab - Shanghai Jiao Tong University Low-Resource Translation\n  System for the WMT22 Translation Task", "abstract": "This paper describes Tencent AI Lab - Shanghai Jiao Tong University\n(TAL-SJTU) Low-Resource Translation systems for the WMT22 shared task. We\nparticipate in the general translation task on\nEnglish$\\Leftrightarrow$Livonian. Our system is based on M2M100 with novel\ntechniques that adapt it to the target language pair. (1) Cross-model word\nembedding alignment: inspired by cross-lingual word embedding alignment, we\nsuccessfully transfer a pre-trained word embedding to M2M100, enabling it to\nsupport Livonian. (2) Gradual adaptation strategy: we exploit Estonian and\nLatvian as auxiliary languages for many-to-many translation training and then\nadapt to English-Livonian. (3) Data augmentation: to enlarge the parallel data\nfor English-Livonian, we construct pseudo-parallel data with Estonian and\nLatvian as pivot languages. (4) Fine-tuning: to make the most of all available\ndata, we fine-tune the model with the validation set and online\nback-translation, further boosting the performance. In model evaluation: (1) We\nfind that previous work underestimated the translation performance of Livonian\ndue to inconsistent Unicode normalization, which may cause a discrepancy of up\nto 14.9 BLEU score. (2) In addition to the standard validation set, we also\nemploy round-trip BLEU to evaluate the models, which we find more appropriate\nfor this task. Finally, our unconstrained system achieves BLEU scores of 17.0\nand 30.4 for English to/from Livonian.", "published": "2022-10-17 04:34:09", "link": "http://arxiv.org/abs/2210.08742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Summary Candidates Fusion", "abstract": "Sequence-to-sequence deep neural models fine-tuned for abstractive\nsummarization can achieve great performance on datasets with enough human\nannotations. Yet, it has been shown that they have not reached their full\npotential, with a wide gap between the top beam search output and the oracle\nbeam. Recently, re-ranking methods have been proposed, to learn to select a\nbetter summary candidate. However, such methods are limited by the summary\nquality aspects captured by the first-stage candidates. To bypass this\nlimitation, we propose a new paradigm in second-stage abstractive summarization\ncalled SummaFusion that fuses several summary candidates to produce a novel\nabstractive second-stage summary. Our method works well on several\nsummarization datasets, improving both the ROUGE scores and qualitative\nproperties of fused summaries. It is especially good when the candidates to\nfuse are worse, such as in the few-shot setup where we set a new\nstate-of-the-art. We will make our code and checkpoints available at\nhttps://github.com/ntunlp/SummaFusion/.", "published": "2022-10-17 06:48:05", "link": "http://arxiv.org/abs/2210.08779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Topic Selection Model with Latent Variable for Topic-Grounded\n  Dialogue", "abstract": "Recently, topic-grounded dialogue system has attracted significant attention\ndue to its effectiveness in predicting the next topic to yield better responses\nvia the historical context and given topic sequence. However, almost all\nexisting topic prediction solutions focus on only the current conversation and\ncorresponding topic sequence to predict the next conversation topic, without\nexploiting other topic-guided conversations which may contain relevant\ntopic-transitions to current conversation. To address the problem, in this\npaper we propose a novel approach, named Sequential Global Topic Attention\n(SGTA) to exploit topic transition over all conversations in a subtle way for\nbetter modeling post-to-response topic-transition and guiding the response\ngeneration to the current conversation. Specifically, we introduce a latent\nspace modeled as a Multivariate Skew-Normal distribution with hybrid kernel\nfunctions to flexibly integrate the global-level information with\nsequence-level information, and predict the topic based on the distribution\nsampling results. We also leverage a topic-aware prior-posterior approach for\nsecondary selection of predicted topics, which is utilized to optimize the\nresponse generation task. Extensive experiments demonstrate that our model\noutperforms competitive baselines on prediction and generation tasks.", "published": "2022-10-17 07:34:14", "link": "http://arxiv.org/abs/2210.08801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event\n  Detection with Task-Adaptive Threshold", "abstract": "Conventional event detection models under supervised learning settings suffer\nfrom the inability of transfer to newly-emerged event types owing to lack of\nsufficient annotations. A commonly-adapted solution is to follow a\nidentify-then-classify manner, which first identifies the triggers and then\nconverts the classification task via a few-shot learning paradigm. However,\nthese methods still fall far short of expectations due to: (i) insufficient\nlearning of discriminative representations in low-resource scenarios, and (ii)\ntrigger misidentification caused by the overlap of the learned representations\nof triggers and non-triggers. To address the problems, in this paper, we\npropose a novel Hybrid Contrastive Learning method with a Task-Adaptive\nThreshold (abbreviated as HCLTAT), which enables discriminative representation\nlearning with a two-view contrastive loss (support-support and\nprototype-query), and devises a easily-adapted threshold to alleviate\nmisidentification of triggers. Extensive experiments on the benchmark dataset\nFewEvent demonstrate the superiority of our method to achieve better results\ncompared to the state-of-the-arts. All the code and data of this paper will be\navailable for online public access.", "published": "2022-10-17 07:37:38", "link": "http://arxiv.org/abs/2210.08806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust k-Nearest-Neighbor Machine Translation", "abstract": "k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research\ndirection of NMT in recent years. Its main idea is to retrieve useful key-value\npairs from an additional datastore to modify translations without updating the\nNMT model. However, the underlying retrieved noisy pairs will dramatically\ndeteriorate the model performance. In this paper, we conduct a preliminary\nstudy and find that this problem results from not fully exploiting the\nprediction of the NMT model. To alleviate the impact of noise, we propose a\nconfidence-enhanced kNN-MT model with robust training. Concretely, we introduce\nthe NMT confidence to refine the modeling of two important components of\nkNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two\ntypes of perturbations into the retrieved pairs for robust training.\nExperimental results on four benchmark datasets demonstrate that our model not\nonly achieves significant improvements over current kNN-MT models, but also\nexhibits better robustness. Our code is available at\nhttps://github.com/DeepLearnXMU/Robust-knn-mt.", "published": "2022-10-17 07:43:39", "link": "http://arxiv.org/abs/2210.08808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PACIFIC: Towards Proactive Conversational Question Answering over\n  Tabular and Textual Data in Finance", "abstract": "To facilitate conversational question answering (CQA) over hybrid contexts in\nfinance, we present a new dataset, named PACIFIC. Compared with existing CQA\ndatasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical\nreasoning, and (iii) hybrid context of tables and text. A new task is defined\naccordingly to study Proactive Conversational Question Answering (PCQA), which\ncombines clarification question generation and CQA. In addition, we propose a\nnovel method, namely UniPCQA, to adapt a hybrid format of input and output\ncontent in PCQA into the Seq2Seq problem, including the reformulation of the\nnumerical reasoning process as code generation. UniPCQA performs multi-task\nlearning over all sub-tasks in PCQA and incorporates a simple ensemble strategy\nto alleviate the error propagation issue in the multi-task learning by\ncross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC\ndataset with extensive baselines and provide comprehensive evaluations on each\nsub-task of PCQA.", "published": "2022-10-17 08:06:56", "link": "http://arxiv.org/abs/2210.08817v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Confidence Score Distribution for Out-of-Domain Intent\n  Detection with Energy-Based Learning", "abstract": "Detecting Out-of-Domain (OOD) or unknown intents from user queries is\nessential in a task-oriented dialog system. Traditional softmax-based\nconfidence scores are susceptible to the overconfidence issue. In this paper,\nwe propose a simple but strong energy-based score function to detect OOD where\nthe energy scores of OOD samples are higher than IND samples. Further, given a\nsmall set of labeled OOD samples, we introduce an energy-based margin objective\nfor supervised OOD detection to explicitly distinguish OOD samples from INDs.\nComprehensive experiments and analysis prove our method helps disentangle\nconfidence score distributions of IND and OOD data.\\footnote{Our code is\navailable at \\url{https://github.com/pris-nlp/EMNLP2022-energy_for_OOD/}.}", "published": "2022-10-17 08:19:01", "link": "http://arxiv.org/abs/2210.08830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PeerDA: Data Augmentation via Modeling Peer Relation for Span\n  Identification Tasks", "abstract": "Span identification aims at identifying specific text spans from text input\nand classifying them into pre-defined categories. Different from previous works\nthat merely leverage the Subordinate (SUB) relation (i.e. if a span is an\ninstance of a certain category) to train models, this paper for the first time\nexplores the Peer (PR) relation, which indicates that two spans are instances\nof the same category and share similar features. Specifically, a novel Peer\nData Augmentation (PeerDA) approach is proposed which employs span pairs with\nthe PR relation as the augmentation data for training. PeerDA has two unique\nadvantages: (1) There are a large number of PR span pairs for augmenting the\ntraining data. (2) The augmented data can prevent the trained model from\nover-fitting the superficial span-category mapping by pushing the model to\nleverage the span semantics. Experimental results on ten datasets over four\ndiverse tasks across seven domains demonstrate the effectiveness of PeerDA.\nNotably, PeerDA achieves state-of-the-art results on six of them.", "published": "2022-10-17 08:51:30", "link": "http://arxiv.org/abs/2210.08855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog\n  Systems", "abstract": "Recent advances in neural approaches greatly improve task-oriented dialogue\n(TOD) systems which assist users to accomplish their goals. However, such\nsystems rely on costly manually labeled dialogs which are not available in\npractical scenarios. In this paper, we present our models for Track 2 of the\nSereTOD 2022 challenge, which is the first challenge of building\nsemi-supervised and reinforced TOD systems on a large-scale real-world Chinese\nTOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate\ndialog history and local KB as input and predict the system response. And we\nperform semi-supervised pre-training both on the labeled and unlabeled data.\nOur system achieves the first place both in the automatic evaluation and human\ninteraction, especially with higher BLEU (+7.64) and Success (+13.6\\%) than the\nsecond place.", "published": "2022-10-17 09:10:03", "link": "http://arxiv.org/abs/2210.08873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning\n  Framework for OOD Intent Discovery", "abstract": "Discovering out-of-domain (OOD) intent is important for developing new skills\nin task-oriented dialogue systems. The key challenges lie in how to transfer\nprior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD\nrepresentations and cluster assignments. Previous methods suffer from in-domain\noverfitting problem, and there is a natural gap between representation learning\nand clustering objectives. In this paper, we propose a unified K-nearest\nneighbor contrastive learning framework to discover OOD intents. Specifically,\nfor IND pre-training stage, we propose a KCL objective to learn inter-class\ndiscriminative features, while maintaining intra-class diversity, which\nalleviates the in-domain overfitting problem. For OOD clustering stage, we\npropose a KCC method to form compact clusters by mining true hard negative\nsamples, which bridges the gap between clustering and representation learning.\nExtensive experiments on three benchmark datasets show that our method achieves\nsubstantial improvements over the state-of-the-art methods.", "published": "2022-10-17 10:04:55", "link": "http://arxiv.org/abs/2210.08909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mars: Modeling Context & State Representations with Contrastive Learning\n  for End-to-End Task-Oriented Dialog", "abstract": "Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.", "published": "2022-10-17 10:14:22", "link": "http://arxiv.org/abs/2210.08917v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Multilingual Knowledge Graph Completion and Alignment", "abstract": "Knowledge graph (KG) alignment and completion are usually treated as two\nindependent tasks. While recent work has leveraged entity and relation\nalignments from multiple KGs, such as alignments between multilingual KGs with\ncommon entities and relations, a deeper understanding of the ways in which\nmultilingual KG completion (MKGC) can aid the creation of multilingual KG\nalignments (MKGA) is still limited. Motivated by the observation that\nstructural inconsistencies -- the main challenge for MKGA models -- can be\nmitigated through KG completion methods, we propose a novel model for jointly\ncompleting and aligning knowledge graphs. The proposed model combines two\ncomponents that jointly accomplish KG completion and alignment. These two\ncomponents employ relation-aware graph neural networks that we propose to\nencode multi-hop neighborhood structures into entity and relation\nrepresentations. Moreover, we also propose (i) a structural inconsistency\nreduction mechanism to incorporate information from the completion into the\nalignment component, and (ii) an alignment seed enlargement and triple\ntransferring mechanism to enlarge alignment seeds and transfer triples during\nKGs alignment. Extensive experiments on a public multilingual benchmark show\nthat our proposed model outperforms existing competitive baselines, obtaining\nnew state-of-the-art results on both MKGC and MKGA tasks. We publicly release\nthe implementation of our model at https://github.com/vinhsuhi/JMAC", "published": "2022-10-17 10:25:10", "link": "http://arxiv.org/abs/2210.08922v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpanProto: A Two-stage Span-based Prototypical Network for Few-shot\n  Named Entity Recognition", "abstract": "Few-shot Named Entity Recognition (NER) aims to identify named entities with\nvery little annotated data. Previous methods solve this problem based on\ntoken-wise classification, which ignores the information of entity boundaries,\nand inevitably the performance is affected by the massive non-entity tokens. To\nthis end, we propose a seminal span-based prototypical network (SpanProto) that\ntackles few-shot NER via a two-stage approach, including span extraction and\nmention classification. In the span extraction stage, we transform the\nsequential tags into a global boundary matrix, enabling the model to focus on\nthe explicit boundary information. For mention classification, we leverage\nprototypical learning to capture the semantic representations for each labeled\nspan and make the model better adapt to novel-class entities. To further\nimprove the model performance, we split out the false positives generated by\nthe span extractor but not labeled in the current episode set, and then present\na margin-based loss to separate them from each prototype region. Experiments\nover multiple benchmarks demonstrate that our model outperforms strong\nbaselines by a large margin.", "published": "2022-10-17 12:59:33", "link": "http://arxiv.org/abs/2210.09049v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo-OOD training for robust language models", "abstract": "While pre-trained large-scale deep models have garnered attention as an\nimportant topic for many downstream natural language processing (NLP) tasks,\nsuch models often make unreliable predictions on out-of-distribution (OOD)\ninputs. As such, OOD detection is a key component of a reliable\nmachine-learning model for any industry-scale application. Common approaches\noften assume access to additional OOD samples during the training stage,\nhowever, outlier distribution is often unknown in advance. Instead, we propose\na post hoc framework called POORE - POsthoc pseudo-Ood REgularization, that\ngenerates pseudo-OOD samples using in-distribution (IND) data. The model is\nfine-tuned by introducing a new regularization loss that separates the\nembeddings of IND and OOD data, which leads to significant gains on the OOD\nprediction task during testing. We extensively evaluate our framework on three\nreal-world dialogue systems, achieving new state-of-the-art in OOD detection.", "published": "2022-10-17 14:32:02", "link": "http://arxiv.org/abs/2210.09132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting GPT-3 To Be Reliable", "abstract": "Large language models (LLMs) show impressive abilities via few-shot\nprompting. Commercialized APIs such as OpenAI GPT-3 further increase their use\nin real-world language applications. However, the crucial problem of how to\nimprove the reliability of GPT-3 is still under-explored. While reliability is\na broad and vaguely defined term, we decompose reliability into four main\nfacets that correspond to the existing framework of ML safety and are\nwell-recognized to be important: generalizability, social biases, calibration,\nand factuality. Our core contribution is to establish simple and effective\nprompts that improve GPT-3's reliability as it: 1) generalizes\nout-of-distribution, 2) balances demographic distribution and uses natural\nlanguage instructions to reduce social biases, 3) calibrates output\nprobabilities, and 4) updates the LLM's factual knowledge and reasoning chains.\nWith appropriate prompts, GPT-3 is more reliable than smaller-scale supervised\nmodels on all these facets. We release all processed datasets, evaluation\nscripts, and model predictions. Our systematic empirical study not only sheds\nnew insights on the reliability of prompting LLMs, but more importantly, our\nprompting strategies can help practitioners more reliably use LLMs like GPT-3.", "published": "2022-10-17 14:52:39", "link": "http://arxiv.org/abs/2210.09150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KPI-EDGAR: A Novel Dataset and Accompanying Metric for Relation\n  Extraction from Financial Documents", "abstract": "We introduce KPI-EDGAR, a novel dataset for Joint Named Entity Recognition\nand Relation Extraction building on financial reports uploaded to the\nElectronic Data Gathering, Analysis, and Retrieval (EDGAR) system, where the\nmain objective is to extract Key Performance Indicators (KPIs) from financial\ndocuments and link them to their numerical values and other attributes. We\nfurther provide four accompanying baselines for benchmarking potential future\nresearch. Additionally, we propose a new way of measuring the success of said\nextraction process by incorporating a word-level weighting scheme into the\nconventional F1 score to better model the inherently fuzzy borders of the\nentity pairs of a relation in this domain.", "published": "2022-10-17 15:06:20", "link": "http://arxiv.org/abs/2210.09163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do we get there? Evaluating transformer neural networks as cognitive\n  models for English past tense inflection", "abstract": "There is an ongoing debate on whether neural networks can grasp the\nquasi-regularities in languages like humans. In a typical quasi-regularity\ntask, English past tense inflections, the neural network model has long been\ncriticized that it learns only to generalize the most frequent pattern, but not\nthe regular pattern, thus can not learn the abstract categories of regular and\nirregular and is dissimilar to human performance. In this work, we train a set\nof transformer models with different settings to examine their behavior on this\ntask. The models achieved high accuracy on unseen regular verbs and some\naccuracy on unseen irregular verbs. The models' performance on the regulars is\nheavily affected by type frequency and ratio but not token frequency and ratio,\nand vice versa for the irregulars. The different behaviors on the regulars and\nirregulars suggest that the models have some degree of symbolic learning on the\nregularity of the verbs. In addition, the models are weakly correlated with\nhuman behavior on nonce verbs. Although the transformer model exhibits some\nlevel of learning on the abstract category of verb regularity, its performance\ndoes not fit human data well, suggesting that it might not be a good cognitive\nmodel.", "published": "2022-10-17 15:13:35", "link": "http://arxiv.org/abs/2210.09167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task\n  Generalization", "abstract": "Training language models to learn from human instructions for zero-shot\ncross-task generalization has attracted much attention in NLP communities.\nRecently, instruction tuning (IT), which fine-tunes a pre-trained language\nmodel on a massive collection of tasks described via human-craft instructions,\nhas been shown effective in instruction learning for unseen tasks. However, IT\nrelies on a large amount of human-annotated samples, which restricts its\ngeneralization. Unlike labeled data, unlabeled data are often massive and cheap\nto obtain. In this work, we study how IT can be improved with unlabeled data.\nWe first empirically explore the IT performance trends versus the number of\nlabeled data, instructions, and training tasks. We find it critical to enlarge\nthe number of training instructions, and the instructions can be underutilized\ndue to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented\nInstruction Tuning (UDIT) to take better advantage of the instructions during\nIT by constructing pseudo-labeled data from unlabeled plain texts. We conduct\nextensive experiments to show UDIT's effectiveness in various scenarios of\ntasks and datasets. We also comprehensively analyze the key factors of UDIT to\ninvestigate how to better improve IT with unlabeled data. The code is publicly\navailable at https://github.com/thu-coai/UDIT.", "published": "2022-10-17 15:25:24", "link": "http://arxiv.org/abs/2210.09175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Knowledge via Neighborhood-Aware Optimal Transport for\n  Low-Resource Hate Speech Detection", "abstract": "The concerning rise of hateful content on online platforms has increased the\nattention towards automatic hate speech detection, commonly formulated as a\nsupervised classification task. State-of-the-art deep learning-based approaches\nusually require a substantial amount of labeled resources for training.\nHowever, annotating hate speech resources is expensive, time-consuming, and\noften harmful to the annotators. This creates a pressing need to transfer\nknowledge from the existing labeled resources to low-resource hate speech\ncorpora with the goal of improving system performance. For this,\nneighborhood-based frameworks have been shown to be effective. However, they\nhave limited flexibility. In our paper, we propose a novel training strategy\nthat allows flexible modeling of the relative proximity of neighbors retrieved\nfrom a resource-rich corpus to learn the amount of transfer. In particular, we\nincorporate neighborhood information with Optimal Transport, which permits\nexploiting the geometry of the data embedding space. By aligning the joint\nembedding and label distributions of neighbors, we demonstrate substantial\nimprovements over strong baselines, in low-resource scenarios, on different\npublicly available hate speech corpora.", "published": "2022-10-17 18:07:01", "link": "http://arxiv.org/abs/2210.09340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossRE: A Cross-Domain Dataset for Relation Extraction", "abstract": "Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.", "published": "2022-10-17 18:33:14", "link": "http://arxiv.org/abs/2210.09345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Low-Resource Cross-lingual Parsing with Expected Statistic\n  Regularization", "abstract": "We present Expected Statistic Regularization (ESR), a novel regularization\ntechnique that utilizes low-order multi-task structural statistics to shape\nmodel distributions for semi-supervised learning on low-resource datasets. We\nstudy ESR in the context of cross-lingual transfer for syntactic analysis (POS\ntagging and labeled dependency parsing) and present several classes of\nlow-order statistic functions that bear on model behavior. Experimentally, we\nevaluate the proposed statistics with ESR for unsupervised transfer on 5\ndiverse target languages and show that all statistics, when estimated\naccurately, yield improvements to both POS and LAS, with the best statistic\nimproving POS by +7.0 and LAS by +8.5 on average. We also present\nsemi-supervised transfer and learning curve experiments that show ESR provides\nsignificant gains over strong cross-lingual-transfer-plus-fine-tuning baselines\nfor modest amounts of label data. These results indicate that ESR is a\npromising and complementary approach to model-transfer approaches for\ncross-lingual parsing.", "published": "2022-10-17 20:44:57", "link": "http://arxiv.org/abs/2210.09428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Emotion Dynamics in Song Lyrics with State Space Models", "abstract": "Most previous work in music emotion recognition assumes a single or a few\nsong-level labels for the whole song. While it is known that different emotions\ncan vary in intensity within a song, annotated data for this setup is scarce\nand difficult to obtain. In this work, we propose a method to predict emotion\ndynamics in song lyrics without song-level supervision. We frame each song as a\ntime series and employ a State Space Model (SSM), combining a sentence-level\nemotion predictor with an Expectation-Maximization (EM) procedure to generate\nthe full emotion dynamics. Our experiments show that applying our method\nconsistently improves the performance of sentence-level baselines without\nrequiring any annotated songs, making it ideal for limited training data\nscenarios. Further analysis through case studies shows the benefits of our\nmethod while also indicating the limitations and pointing to future directions.", "published": "2022-10-17 21:07:23", "link": "http://arxiv.org/abs/2210.09434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Generative User Simulator with GPT-based Architecture and Goal State\n  Tracking for Reinforced Multi-Domain Dialog Systems", "abstract": "Building user simulators (USs) for reinforcement learning (RL) of\ntask-oriented dialog systems (DSs) has gained more and more attention, which,\nhowever, still faces several fundamental challenges. First, it is unclear\nwhether we can leverage pretrained language models to design, for example,\nGPT-2 based USs, to catch up and interact with the recently advanced GPT-2\nbased DSs. Second, an important ingredient in a US is that the user goal can be\neffectively incorporated and tracked; but how to flexibly integrate goal state\ntracking and develop an end-to-end trainable US for multi-domains has remained\nto be a challenge. In this work, we propose a generative user simulator (GUS)\nwith GPT-2 based architecture and goal state tracking towards addressing the\nabove two challenges. Extensive experiments are conducted on MultiWOZ2.1.\nDifferent DSs are trained via RL with GUS, the classic agenda-based user\nsimulator (ABUS) and other ablation simulators respectively, and are compared\nfor cross-model evaluation, corpus-based evaluation and human evaluation. The\nGUS achieves superior results in all three evaluation tasks.", "published": "2022-10-17 01:57:50", "link": "http://arxiv.org/abs/2210.08692v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Unified Positive-Unlabeled Learning Framework for Document-Level\n  Relation Extraction with Different Levels of Labeling", "abstract": "Document-level relation extraction (RE) aims to identify relations between\nentities across multiple sentences. Most previous methods focused on\ndocument-level RE under full supervision. However, in real-world scenario, it\nis expensive and difficult to completely label all relations in a document\nbecause the number of entity pairs in document-level RE grows quadratically\nwith the number of entities. To solve the common incomplete labeling problem,\nwe propose a unified positive-unlabeled learning framework - shift and squared\nranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled\n(PU) learning on document-level RE for the first time. Considering that labeled\ndata of a dataset may lead to prior shift of unlabeled data, we introduce a PU\nlearning under prior shift of training data. Also, using none-class score as an\nadaptive threshold, we propose squared ranking loss and prove its Bayesian\nconsistency with multi-label ranking metrics. Extensive experiments demonstrate\nthat our method achieves an improvement of about 14 F1 points relative to the\nprevious baseline with incomplete labeling. In addition, it outperforms\nprevious state-of-the-art results under both fully supervised and extremely\nunlabeled settings as well.", "published": "2022-10-17 02:54:49", "link": "http://arxiv.org/abs/2210.08709v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keep Me Updated! Memory Management in Long-term Conversations", "abstract": "Remembering important information from the past and continuing to talk about\nit in the present are crucial in long-term conversations. However, previous\nliterature does not deal with cases where the memorized information is\noutdated, which may cause confusion in later conversations. To address this\nissue, we present a novel task and a corresponding dataset of memory management\nin long-term conversations, in which bots keep track of and bring up the latest\ninformation about users while conversing through multiple sessions. In order to\nsupport more precise and interpretable memory, we represent memory as\nunstructured text descriptions of key information and propose a new mechanism\nof memory management that selectively eliminates invalidated or redundant\ninformation. Experimental results show that our approach outperforms the\nbaselines that leave the stored memory unchanged in terms of engagingness and\nhumanness, with larger performance gap especially in the later sessions.", "published": "2022-10-17 05:06:38", "link": "http://arxiv.org/abs/2210.08750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Evaluation of Predictive Fairness", "abstract": "Mitigating bias in training on biased datasets is an important open problem.\nSeveral techniques have been proposed, however the typical evaluation regime is\nvery limited, considering very narrow data conditions. For instance, the effect\nof target class imbalance and stereotyping is under-studied. To address this\ngap, we examine the performance of various debiasing methods across multiple\ntasks, spanning binary classification (Twitter sentiment), multi-class\nclassification (profession prediction), and regression (valence prediction).\nThrough extensive experimentation, we find that data conditions have a strong\ninfluence on relative model performance, and that general conclusions cannot be\ndrawn about method efficacy when evaluating only on standard datasets, as is\ncurrent practice in fairness research.", "published": "2022-10-17 05:40:13", "link": "http://arxiv.org/abs/2210.08758v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Relation Extraction From Speech", "abstract": "Relation extraction typically aims to extract semantic relationships between\nentities from the unstructured text. One of the most essential data sources for\nrelation extraction is the spoken language, such as interviews and dialogues.\nHowever, the error propagation introduced in automatic speech recognition (ASR)\nhas been ignored in relation extraction, and the end-to-end speech-based\nrelation extraction method has been rarely explored. In this paper, we propose\na new listening information extraction task, i.e., speech relation extraction.\nWe construct the training dataset for speech relation extraction via\ntext-to-speech systems, and we construct the testing dataset via crowd-sourcing\nwith native English speakers. We explore speech relation extraction via two\napproaches: the pipeline approach conducting text-based extraction with a\npretrained ASR module, and the end2end approach via a new proposed\nencoder-decoder model, or what we called SpeechRE. We conduct comprehensive\nexperiments to distinguish the challenges in speech relation extraction, which\nmay shed light on future explorations. We share the code and data on\nhttps://github.com/wutong8023/SpeechRE.", "published": "2022-10-17 05:53:49", "link": "http://arxiv.org/abs/2210.08759v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "ReasonChainQA: Text-based Complex Question Answering with Explainable\n  Evidence Chains", "abstract": "The ability of reasoning over evidence has received increasing attention in\nquestion answering (QA). Recently, natural language database (NLDB) conducts\ncomplex QA in knowledge base with textual evidences rather than structured\nrepresentations, this task attracts a lot of attention because of the\nflexibility and richness of textual evidence. However, existing text-based\ncomplex question answering datasets fail to provide explicit reasoning process,\nwhile it's important for retrieval effectiveness and reasoning\ninterpretability. Therefore, we present a benchmark \\textbf{ReasonChainQA} with\nexplanatory and explicit evidence chains. ReasonChainQA consists of two\nsubtasks: answer generation and evidence chains extraction, it also contains\nhigher diversity for multi-hop questions with varying depths, 12 reasoning\ntypes and 78 relations. To obtain high-quality textual evidences for answering\ncomplex question. Additional experiment on supervised and unsupervised\nretrieval fully indicates the significance of ReasonChainQA. Dataset and codes\nwill be made publicly available upon accepted.", "published": "2022-10-17 06:07:39", "link": "http://arxiv.org/abs/2210.08763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph\n  Completion", "abstract": "Multimodal knowledge graph completion (MKGC) aims to predict missing entities\nin MKGs. Previous works usually share relation representation across\nmodalities. This results in mutual interference between modalities during\ntraining, since for a pair of entities, the relation from one modality probably\ncontradicts that from another modality. Furthermore, making a unified\nprediction based on the shared relation representation treats the input in\ndifferent modalities equally, while their importance to the MKGC task should be\ndifferent. In this paper, we propose MoSE, a Modality Split representation\nlearning and Ensemble inference framework for MKGC. Specifically, in the\ntraining phase, we learn modality-split relation embeddings for each modality\ninstead of a single modality-shared one, which alleviates the modality\ninterference. Based on these embeddings, in the inference phase, we first make\nmodality-split predictions and then exploit various ensemble methods to combine\nthe predictions with different weights, which models the modality importance\ndynamically. Experimental results on three KG datasets show that MoSE\noutperforms state-of-the-art MKGC methods. Codes are available at\nhttps://github.com/OreOZhao/MoSE4MKGC.", "published": "2022-10-17 08:09:54", "link": "http://arxiv.org/abs/2210.08821v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Social Biases in Automatic Evaluation Metrics for NLG", "abstract": "Many studies have revealed that word embeddings, language models, and models\nfor specific downstream tasks in NLP are prone to social biases, especially\ngender bias. Recently these techniques have been gradually applied to automatic\nevaluation metrics for text generation. In the paper, we propose an evaluation\nmethod based on Word Embeddings Association Test (WEAT) and Sentence Embeddings\nAssociation Test (SEAT) to quantify social biases in evaluation metrics and\ndiscover that social biases are also widely present in some model-based\nautomatic evaluation metrics. Moreover, we construct gender-swapped\nmeta-evaluation datasets to explore the potential impact of gender bias in\nimage caption and text summarization tasks. Results show that given\ngender-neutral references in the evaluation, model-based evaluation metrics may\nshow a preference for the male hypothesis, and the performance of them, i.e.\nthe correlation between evaluation metrics and human judgments, usually has\nmore significant variation after gender swapping.", "published": "2022-10-17 08:55:26", "link": "http://arxiv.org/abs/2210.08859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models", "abstract": "Recently, diffusion models have emerged as a new paradigm for generative\nmodels. Despite the success in domains using continuous signals such as vision\nand audio, adapting diffusion models to natural language is under-explored due\nto the discrete nature of texts, especially for conditional generation. We\ntackle this challenge by proposing DiffuSeq: a diffusion model designed for\nsequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation\nover a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or\neven better performance than six established baselines, including a\nstate-of-the-art model that is based on pre-trained language models. Apart from\nquality, an intriguing property of DiffuSeq is its high diversity during\ngeneration, which is desired in many Seq2Seq tasks. We further include a\ntheoretical analysis revealing the connection between DiffuSeq and\nautoregressive/non-autoregressive models. Bringing together theoretical\nanalysis and empirical evidence, we demonstrate the great potential of\ndiffusion models in complex conditional language generation tasks. Code is\navailable at \\url{https://github.com/Shark-NLP/DiffuSeq}", "published": "2022-10-17 10:49:08", "link": "http://arxiv.org/abs/2210.08933v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Table-To-Text generation and pre-training with TabT5", "abstract": "Encoder-only transformer models have been successfully applied to different\ntable understanding tasks, as in TAPAS (Herzig et al., 2020). A major\nlimitation of these architectures is that they are constrained to\nclassification-like tasks such as cell selection or entailment detection. We\npresent TABT5, an encoder-decoder model that generates natural language text\nbased on tables and textual inputs. TABT5 overcomes the encoder-only limitation\nby incorporating a decoder component and leverages the input structure with\ntable specific embeddings and pre-training. TABT5 achieves new state-of-the-art\nresults on several domains, including spreadsheet formula prediction with a 15%\nincrease in sequence accuracy, QA with a 2.5% increase in sequence accuracy and\ndata-to-text generation with a 2.5% increase in BLEU.", "published": "2022-10-17 15:05:53", "link": "http://arxiv.org/abs/2210.09162v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Ranking Socio-Political Texts with Transformer Language Models\n  to Reduce Close Reading Time", "abstract": "We approach the classification problem as an entailment problem and apply\nzero-shot ranking to socio-political texts. Documents that are ranked at the\ntop can be considered positively classified documents and this reduces the\nclose reading time for the information extraction process. We use Transformer\nLanguage Models to get the entailment probabilities and investigate different\ntypes of queries. We find that DeBERTa achieves higher mean average precision\nscores than RoBERTa and when declarative form of the class label is used as a\nquery, it outperforms dictionary definition of the class label. We show that\none can reduce the close reading time by taking some percentage of the ranked\ndocuments that the percentage depends on how much recall they want to achieve.\nHowever, our findings also show that percentage of the documents that should be\nread increases as the topic gets broader.", "published": "2022-10-17 15:28:54", "link": "http://arxiv.org/abs/2210.09179v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\nfocuses on tasks believed to be beyond the capabilities of current language\nmodels. Language models have already made good progress on this benchmark, with\nthe best model in the BIG-Bench paper outperforming average reported\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\non what tasks do language models fall short of average human-rater performance,\nand are those tasks actually unsolvable by current language models?\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\nevaluations did not outperform the average human-rater. We find that applying\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\naverage human-rater performance on 10 of the 23 tasks, and Codex\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\n2022), substantially underestimates the best performance and capabilities of\nlanguage models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH,\nfinding that CoT enables emergent task performance on several BBH tasks with\notherwise flat scaling curves.", "published": "2022-10-17 17:08:26", "link": "http://arxiv.org/abs/2210.09261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends", "abstract": "This paper surveys vision-language pre-training (VLP) methods for multimodal\nintelligence that have been developed in the last few years. We group these\napproaches into three categories: ($i$) VLP for image-text tasks, such as image\ncaptioning, image-text retrieval, visual question answering, and visual\ngrounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image\nclassification, object detection, and segmentation; and ($iii$) VLP for\nvideo-text tasks, such as video captioning, video-text retrieval, and video\nquestion answering. For each category, we present a comprehensive review of\nstate-of-the-art methods, and discuss the progress that has been made and\nchallenges still being faced, using specific systems and models as case\nstudies. In addition, for each category, we discuss advanced topics being\nactively explored in the research community, such as big foundation models,\nunified modeling, in-context few-shot learning, knowledge, robustness, and\ncomputer vision in the wild, to name a few.", "published": "2022-10-17 17:11:36", "link": "http://arxiv.org/abs/2210.09263v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Potrika: Raw and Balanced Newspaper Datasets in the Bangla Language with\n  Eight Topics and Five Attributes", "abstract": "Knowledge is central to human and scientific developments. Natural Language\nProcessing (NLP) allows automated analysis and creation of knowledge. Data is a\ncrucial NLP and machine learning ingredient. The scarcity of open datasets is a\nwell-known problem in machine and deep learning research. This is very much the\ncase for textual NLP datasets in English and other major world languages. For\nthe Bangla language, the situation is even more challenging and the number of\nlarge datasets for NLP research is practically nil. We hereby present Potrika,\na large single-label Bangla news article textual dataset curated for NLP\nresearch from six popular online news portals in Bangladesh (Jugantor,\nJaijaidin, Ittefaq, Kaler Kontho, Inqilab, and Somoyer Alo) for the period\n2014-2020. The articles are classified into eight distinct categories\n(National, Sports, International, Entertainment, Economy, Education, Politics,\nand Science \\& Technology) providing five attributes (News Article, Category,\nHeadline, Publication Date, and Newspaper Source). The raw dataset contains\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles. Moreover, using NLP augmentation techniques, we create from the raw\n(unbalanced) dataset another (balanced) dataset comprising 320,000 news\narticles with 40,000 articles in each of the eight news categories. Potrika\ncontains both the datasets (raw and balanced) to suit a wide range of NLP\nresearch. By far, to the best of our knowledge, Potrika is the largest and the\nmost extensive dataset for news classification.", "published": "2022-10-17 19:37:42", "link": "http://arxiv.org/abs/2210.09389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Bottleneck Adapters to Identify Cancer in Clinical Notes under\n  Low-Resource Constraints", "abstract": "Processing information locked within clinical health records is a challenging\ntask that remains an active area of research in biomedical NLP. In this work,\nwe evaluate a broad set of machine learning techniques ranging from simple RNNs\nto specialised transformers such as BioBERT on a dataset containing clinical\nnotes along with a set of annotations indicating whether a sample is\ncancer-related or not.\n  Furthermore, we specifically employ efficient fine-tuning methods from NLP,\nnamely, bottleneck adapters and prompt tuning, to adapt the models to our\nspecialised task. Our evaluations suggest that fine-tuning a frozen BERT model\npre-trained on natural language and with bottleneck adapters outperforms all\nother strategies, including full fine-tuning of the specialised BioBERT model.\nBased on our findings, we suggest that using bottleneck adapters in\nlow-resource situations with limited access to labelled data or processing\ncapacity could be a viable strategy in biomedical text mining. The code used in\nthe experiments are going to be made available at\nhttps://github.com/omidrohanian/bottleneck-adapters.", "published": "2022-10-17 21:22:23", "link": "http://arxiv.org/abs/2210.09440v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-granularity Argument Mining in Legal Texts", "abstract": "In this paper, we explore legal argument mining using multiple levels of\ngranularity. Argument mining has usually been conceptualized as a sentence\nclassification problem. In this work, we conceptualize argument mining as a\ntoken-level (i.e., word-level) classification problem. We use a Longformer\nmodel to classify the tokens. Results show that token-level text classification\nidentifies certain legal argument elements more accurately than sentence-level\ntext classification. Token-level classification also provides greater\nflexibility to analyze legal texts and to gain more insight into what the model\nfocuses on when processing a large amount of input data.", "published": "2022-10-17 23:28:22", "link": "http://arxiv.org/abs/2210.09472v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Leveraging Non-dialogue Summaries for Dialogue Summarization", "abstract": "To mitigate the lack of diverse dialogue summarization datasets in academia,\nwe present methods to utilize non-dialogue summarization data for enhancing\ndialogue summarization systems. We apply transformations to document\nsummarization data pairs to create training data that better befit dialogue\nsummarization. The suggested transformations also retain desirable properties\nof non-dialogue datasets, such as improved faithfulness to the source text. We\nconduct extensive experiments across both English and Korean to verify our\napproach. Although absolute gains in ROUGE naturally plateau as more dialogue\nsummarization samples are introduced, utilizing non-dialogue data for training\nsignificantly improves summarization performance in zero- and few-shot settings\nand enhances faithfulness across all training regimes.", "published": "2022-10-17 23:34:31", "link": "http://arxiv.org/abs/2210.09474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuned Sentiment Analysis of COVID-19 Vaccine-Related Social Media\n  Data: Comparative Study", "abstract": "This study investigated and compared public sentiment related to COVID-19\nvaccines expressed on two popular social media platforms, Reddit and Twitter,\nharvested from January 1, 2020, to March 1, 2022. To accomplish this task, we\ncreated a fine-tuned DistilRoBERTa model to predict sentiments of approximately\n9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our\nteam manually labeled the sentiment of 3600 Tweets and then augmented our\ndataset by the method of back-translation. Text sentiment for each social media\nplatform was then classified with our fine-tuned model using Python and the\nHuggingface sentiment analysis pipeline. Our results determined that the\naverage sentiment expressed on Twitter was more negative (52% positive) than\npositive and the sentiment expressed on Reddit was more positive than negative\n(53% positive). Though average sentiment was found to vary between these social\nmedia platforms, both displayed similar behavior related to sentiment shared at\nkey vaccine-related developments during the pandemic. Considering this similar\ntrend in shared sentiment demonstrated across social media platforms, Twitter\nand Reddit continue to be valuable data sources that public health officials\ncan utilize to strengthen vaccine confidence and combat misinformation. As the\nspread of misinformation poses a range of psychological and psychosocial risks\n(anxiety, fear, etc.), there is an urgency in understanding the public\nperspective and attitude toward shared falsities. Comprehensive educational\ndelivery systems tailored to the population's expressed sentiments that\nfacilitate digital literacy, health information-seeking behavior, and precision\nhealth promotion could aid in clarifying such misinformation.", "published": "2022-10-17 16:22:18", "link": "http://arxiv.org/abs/2211.15407v1", "categories": ["cs.CL", "cs.SI", "92-11", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Teacher Forcing Recovers Reward Functions for Text Generation", "abstract": "Reinforcement learning (RL) has been widely used in text generation to\nalleviate the exposure bias issue or to utilize non-parallel datasets. The\nreward function plays an important role in making RL training successful.\nHowever, previous reward functions are typically task-specific and sparse,\nrestricting the use of RL. In our work, we propose a task-agnostic approach\nthat derives a step-wise reward function directly from a model trained with\nteacher forcing. We additionally propose a simple modification to stabilize the\nRL training on non-parallel datasets with our induced reward function.\nEmpirical results show that our method outperforms self-training and reward\nregression methods on several text generation tasks, confirming the\neffectiveness of our reward function.", "published": "2022-10-17 02:48:58", "link": "http://arxiv.org/abs/2210.08708v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RARR: Researching and Revising What Language Models Say, Using Language\n  Models", "abstract": "Language models (LMs) now excel at many tasks such as few-shot learning,\nquestion answering, reasoning, and dialog. However, they sometimes generate\nunsupported or misleading content. A user cannot easily determine whether their\noutputs are trustworthy or not, because most LMs do not have any built-in\nmechanism for attribution to external evidence. To enable attribution while\nstill preserving all the powerful advantages of recent generation models, we\npropose RARR (Retrofit Attribution using Research and Revision), a system that\n1) automatically finds attribution for the output of any text generation model\nand 2) post-edits the output to fix unsupported content while preserving the\noriginal output as much as possible. When applied to the output of several\nstate-of-the-art LMs on a diverse set of generation tasks, we find that RARR\nsignificantly improves attribution while otherwise preserving the original\ninput to a much greater degree than previously explored edit models.\nFurthermore, the implementation of RARR requires only a handful of training\nexamples, a large language model, and standard web search.", "published": "2022-10-17 03:44:30", "link": "http://arxiv.org/abs/2210.08726v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MCP: Self-supervised Pre-training for Personalized Chatbots with\n  Multi-level Contrastive Sampling", "abstract": "Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.", "published": "2022-10-17 05:16:23", "link": "http://arxiv.org/abs/2210.08753v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Model Interpretability: On the Faithfulness and Adversarial\n  Robustness of Contrastive Textual Explanations", "abstract": "Contrastive explanation methods go beyond transparency and address the\ncontrastive aspect of explanations. Such explanations are emerging as an\nattractive option to provide actionable change to scenarios adversely impacted\nby classifiers' decisions. However, their extension to textual data is\nunder-explored and there is little investigation on their vulnerabilities and\nlimitations.\n  This work motivates textual counterfactuals by laying the ground for a novel\nevaluation scheme inspired by the faithfulness of explanations. Accordingly, we\nextend the computation of three metrics, proximity,connectedness and stability,\nto textual data and we benchmark two successful contrastive methods, POLYJUICE\nand MiCE, on our suggested metrics. Experiments on sentiment analysis data show\nthat the connectedness of counterfactuals to their original counterparts is not\nobvious in both models. More interestingly, the generated contrastive texts are\nmore attainable with POLYJUICE which highlights the significance of latent\nrepresentations in counterfactual search. Finally, we perform the first\nsemantic adversarial attack on textual recourse methods. The results\ndemonstrate the robustness of POLYJUICE and the role that latent input\nrepresentations play in robustness and reliability.", "published": "2022-10-17 09:50:02", "link": "http://arxiv.org/abs/2210.08902v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Language-agnostic Code-Switching in Sequence-To-Sequence Speech\n  Recognition", "abstract": "Code-Switching (CS) is referred to the phenomenon of alternately using words\nand phrases from different languages. While today's neural end-to-end (E2E)\nmodels deliver state-of-the-art performances on the task of automatic speech\nrecognition (ASR) it is commonly known that these systems are very\ndata-intensive. However, there is only a few transcribed and aligned CS speech\navailable. To overcome this problem and train multilingual systems which can\ntranscribe CS speech, we propose a simple yet effective data augmentation in\nwhich audio and corresponding labels of different source languages are\nconcatenated. By using this training data, our E2E model improves on\ntranscribing CS speech. It also surpasses monolingual models on monolingual\ntests. The results show that this augmentation technique can even improve the\nmodel's performance on inter-sentential language switches not seen during\ntraining by 5,03% WER.", "published": "2022-10-17 12:15:57", "link": "http://arxiv.org/abs/2210.08992v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Symbol Guided Hindsight Priors for Reward Learning from Human\n  Preferences", "abstract": "Specifying rewards for reinforcement learned (RL) agents is challenging.\nPreference-based RL (PbRL) mitigates these challenges by inferring a reward\nfrom feedback over sets of trajectories. However, the effectiveness of PbRL is\nlimited by the amount of feedback needed to reliably recover the structure of\nthe target reward. We present the PRIor Over Rewards (PRIOR) framework, which\nincorporates priors about the structure of the reward function and the\npreference feedback into the reward learning process. Imposing these priors as\nsoft constraints on the reward learning objective reduces the amount of\nfeedback required by half and improves overall reward recovery. Additionally,\nwe demonstrate that using an abstract state space for the computation of the\npriors further improves the reward learning and the agent's performance.", "published": "2022-10-17 14:57:06", "link": "http://arxiv.org/abs/2210.09151v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Impact of Temporal Concept Drift on Model Explanations", "abstract": "Explanation faithfulness of model predictions in natural language processing\nis typically evaluated on held-out data from the same temporal distribution as\nthe training data (i.e. synchronous settings). While model performance often\ndeteriorates due to temporal variation (i.e. temporal concept drift), it is\ncurrently unknown how explanation faithfulness is impacted when the time span\nof the target data is different from the data used to train the model (i.e.\nasynchronous settings). For this purpose, we examine the impact of temporal\nvariation on model explanations extracted by eight feature attribution methods\nand three select-then-predict models across six text classification tasks. Our\nexperiments show that (i)faithfulness is not consistent under temporal\nvariations across feature attribution methods (e.g. it decreases or increases\ndepending on the method), with an attention-based method demonstrating the most\nrobust faithfulness scores across datasets; and (ii) select-then-predict models\nare mostly robust in asynchronous settings with only small degradation in\npredictive performance. Finally, feature attribution methods show conflicting\nbehavior when used in FRESH (i.e. a select-and-predict model) and for measuring\nsufficiency/comprehensiveness (i.e. as post-hoc methods), suggesting that we\nneed more robust metrics to evaluate post-hoc explanation faithfulness.", "published": "2022-10-17 15:53:09", "link": "http://arxiv.org/abs/2210.09197v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Covertly Unsafe Text within Natural Language Systems", "abstract": "An increasingly prevalent problem for intelligent technologies is text\nsafety, as uncontrolled systems may generate recommendations to their users\nthat lead to injury or life-threatening consequences. However, the degree of\nexplicitness of a generated statement that can cause physical harm varies. In\nthis paper, we distinguish types of text that can lead to physical harm and\nestablish one particularly underexplored category: covertly unsafe text. Then,\nwe further break down this category with respect to the system's information\nand discuss solutions to mitigate the generation of text in each of these\nsubcategories. Ultimately, our work defines the problem of covertly unsafe\nlanguage that causes physical harm and argues that this subtle yet dangerous\nissue needs to be prioritized by stakeholders and regulators. We highlight\nmitigation strategies to inspire future researchers to tackle this challenging\nproblem and help improve safety within smart systems.", "published": "2022-10-17 17:59:49", "link": "http://arxiv.org/abs/2210.09306v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Deep Bidirectional Language-Knowledge Graph Pretraining", "abstract": "Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.", "published": "2022-10-17 18:02:52", "link": "http://arxiv.org/abs/2210.09338v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deepfake Text Detection: Limitations and Opportunities", "abstract": "Recent advances in generative models for language have enabled the creation\nof convincing synthetic text or deepfake text. Prior work has demonstrated the\npotential for misuse of deepfake text to mislead content consumers. Therefore,\ndeepfake text detection, the task of discriminating between human and\nmachine-generated text, is becoming increasingly critical. Several defenses\nhave been proposed for deepfake text detection. However, we lack a thorough\nunderstanding of their real-world applicability. In this paper, we collect\ndeepfake text from 4 online services powered by Transformer-based tools to\nevaluate the generalization ability of the defenses on content in the wild. We\ndevelop several low-cost adversarial attacks, and investigate the robustness of\nexisting defenses against an adaptive attacker. We find that many defenses show\nsignificant degradation in performance under our evaluation scenarios compared\nto their original claimed performance. Our evaluation shows that tapping into\nthe semantic information in the text content is a promising approach for\nimproving the robustness and generalization performance of deepfake text\ndetection schemes.", "published": "2022-10-17 20:40:14", "link": "http://arxiv.org/abs/2210.09421v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Affective Idiosyncratic Responses to Music", "abstract": "Affective responses to music are highly personal. Despite consensus that\nidiosyncratic factors play a key role in regulating how listeners emotionally\nrespond to music, precisely measuring the marginal effects of these variables\nhas proved challenging. To address this gap, we develop computational methods\nto measure affective responses to music from over 403M listener comments on a\nChinese social music platform. Building on studies from music psychology in\nsystematic and quasi-causal analyses, we test for musical, lyrical, contextual,\ndemographic, and mental health effects that drive listener affective responses.\nFinally, motivated by the social phenomenon known as w\\v{a}ng-y\\`i-y\\'un, we\nidentify influencing factors of platform user self-disclosures, the social\nsupport they receive, and notable differences in discloser user activity.", "published": "2022-10-17 19:57:46", "link": "http://arxiv.org/abs/2210.09396v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "spatial-dccrn: dccrn equipped with frame-level angle feature and hybrid\n  filtering for multi-channel speech enhancement", "abstract": "Recently, multi-channel speech enhancement has drawn much interest due to the\nuse of spatial information to distinguish target speech from interfering\nsignal. To make full use of spatial information and neural network based\nmasking estimation, we propose a multi-channel denoising neural network --\nSpatial DCCRN. Firstly, we extend S-DCCRN to multi-channel scenario, aiming at\nperforming cascaded sub-channel and full-channel processing strategy, which can\nmodel different channels separately. Moreover, instead of only adopting\nmulti-channel spectrum or concatenating first-channel's magnitude and IPD as\nthe model's inputs, we apply an angle feature extraction module (AFE) to\nextract frame-level angle feature embeddings, which can help the model to\napparently perceive spatial information. Finally, since the phenomenon of\nresidual noise will be more serious when the noise and speech exist in the same\ntime frequency (TF) bin, we particularly design a masking and mapping filtering\nmethod to substitute the traditional filter-and-sum operation, with the purpose\nof cascading coarsely denoising, dereverberation and residual noise\nsuppression. The proposed model, Spatial-DCCRN, has surpassed EaBNet, FasNet as\nwell as several competitive models on the L3DAS22 Challenge dataset. Not only\nthe 3D scenario, Spatial-DCCRN outperforms state-of-the-art (SOTA) model\nMIMO-UNet by a large margin in multiple evaluation metrics on the multi-channel\nConferencingSpeech2021 Challenge dataset. Ablation studies also demonstrate the\neffectiveness of different contributions.", "published": "2022-10-17 07:35:26", "link": "http://arxiv.org/abs/2210.08802v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Visual onoma-to-wave: environmental sound synthesis from visual\n  onomatopoeias and sound-source images", "abstract": "We propose a method for synthesizing environmental sounds from visually\nrepresented onomatopoeias and sound sources. An onomatopoeia is a word that\nimitates a sound structure, i.e., the text representation of sound. From this\nperspective, onoma-to-wave has been proposed to synthesize environmental sounds\nfrom the desired onomatopoeia texts. Onomatopoeias have another representation:\nvisual-text representations of sounds in comics, advertisements, and virtual\nreality. A visual onomatopoeia (visual text of onomatopoeia) contains rich\ninformation that is not present in the text, such as a long-short duration of\nthe image, so the use of this representation is expected to synthesize diverse\nsounds. Therefore, we propose visual onoma-to-wave for environmental sound\nsynthesis from visual onomatopoeia. The method can transfer visual concepts of\nthe visual text and sound-source image to the synthesized sound. We also\npropose a data augmentation method focusing on the repetition of onomatopoeias\nto enhance the performance of our method. An experimental evaluation shows that\nthe methods can synthesize diverse environmental sounds from visual text and\nsound-source images.", "published": "2022-10-17 15:19:51", "link": "http://arxiv.org/abs/2210.09173v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How to Leverage DNN-based speech enhancement for multi-channel speaker\n  verification?", "abstract": "Speaker verification (SV) suffers from unsatisfactory performance in\nfar-field scenarios due to environmental noise andthe adverse impact of room\nreverberation. This work presents a benchmark of multichannel speech\nenhancement for far-fieldspeaker verification. One approach is a deep neural\nnetwork-based, and the other is a combination of deep neural network andsignal\nprocessing. We integrated a DNN architecture with signal processing techniques\nto carry out various experiments. Ourapproach is compared to the existing\nstate-of-the-art approaches. We examine the importance of enrollment in\npre-processing,which has been largely overlooked in previous studies.\nExperimental evaluation shows that pre-processing can improve the SVperformance\nas long as the enrollment files are processed similarly to the test data and\nthat test and enrollment occur within similarSNR ranges. Considerable\nimprovement is obtained on the generated and all the noise conditions of the\nVOiCES dataset.", "published": "2022-10-17 08:22:08", "link": "http://arxiv.org/abs/2210.08834v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sub-8-bit quantization for on-device speech recognition: a\n  regularization-free approach", "abstract": "For on-device automatic speech recognition (ASR), quantization aware training\n(QAT) is ubiquitous to achieve the trade-off between model predictive\nperformance and efficiency. Among existing QAT methods, one major drawback is\nthat the quantization centroids have to be predetermined and fixed. To overcome\nthis limitation, we introduce a regularization-free, \"soft-to-hard\" compression\nmechanism with self-adjustable centroids in a mu-Law constrained space,\nresulting in a simpler yet more versatile quantization scheme, called General\nQuantizer (GQ). We apply GQ to ASR tasks using Recurrent Neural Network\nTransducer (RNN-T) and Conformer architectures on both LibriSpeech and\nde-identified far-field datasets. Without accuracy degradation, GQ can compress\nboth RNN-T and Conformer into sub-8-bit, and for some RNN-T layers, to 1-bit\nfor fast and accurate inference. We observe a 30.73% memory footprint saving\nand 31.75% user-perceived latency reduction compared to 8-bit QAT via physical\ndevice benchmarking.", "published": "2022-10-17 15:42:26", "link": "http://arxiv.org/abs/2210.09188v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TorchDIVA: An Extensible Computational Model of Speech Production built\n  on an Open-Source Machine Learning Library", "abstract": "The DIVA model is a computational model of speech motor control that combines\na simulation of the brain regions responsible for speech production with a\nmodel of the human vocal tract. The model is currently implemented in Matlab\nSimulink; however, this is less than ideal as most of the development in speech\ntechnology research is done in Python. This means there is a wealth of machine\nlearning tools which are freely available in the Python ecosystem that cannot\nbe easily integrated with DIVA. We present TorchDIVA, a full rebuild of DIVA in\nPython using PyTorch tensors. DIVA source code was directly translated from\nMatlab to Python, and built-in Simulink signal blocks were implemented from\nscratch. After implementation, the accuracy of each module was evaluated via\nsystematic block-by-block validation. The TorchDIVA model is shown to produce\noutputs that closely match those of the original DIVA model, with a negligible\ndifference between the two. We additionally present an example of the\nextensibility of TorchDIVA as a research platform. Speech quality enhancement\nin TorchDIVA is achieved through an integration with an existing PyTorch\ngenerative vocoder called DiffWave. A modified DiffWave mel-spectrum upsampler\nwas trained on human speech waveforms and conditioned on the TorchDIVA speech\nproduction. The results indicate improved speech quality metrics in the\nDiffWave-enhanced output as compared to the baseline. This enhancement would\nhave been difficult or impossible to accomplish in the original Matlab\nimplementation. This proof-of-concept demonstrates the value TorchDIVA will\nbring to the research community. Researchers can download the new\nimplementation at: https://github.com/skinahan/DIVA_PyTorch", "published": "2022-10-17 18:00:52", "link": "http://arxiv.org/abs/2210.09334v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
