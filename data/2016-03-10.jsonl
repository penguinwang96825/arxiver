{"title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X =\n  Language, Domain, Genre)", "abstract": "Recent research has shown great progress on fine-grained entity typing. Most\nexisting methods require pre-defining a set of types and training a multi-class\nclassifier from a large labeled data set based on multi-level linguistic\nfeatures. They are thus limited to certain domains, genres and languages. In\nthis paper, we propose a novel unsupervised entity typing framework by\ncombining symbolic and distributional semantics. We start from learning general\nembeddings for each entity mention, compose the embeddings of specific contexts\nusing linguistic structures, link the mention to knowledge bases and learn its\nrelated knowledge representations. Then we develop a novel joint hierarchical\nclustering and linking algorithm to type all mentions using these\nrepresentations. This framework doesn't rely on any annotated data, predefined\ntyping schema, or hand-crafted features, therefore it can be quickly adapted to\na new domain, genre and language. Furthermore, it has great flexibility at\nincorporating linguistic structures (e.g., Abstract Meaning Representation\n(AMR), dependency relations) to improve specific context representation.\nExperiments on genres (news and discussion forum) show comparable performance\nwith state-of-the-art supervised typing systems trained from a large amount of\nlabeled data. Results on various languages (English, Chinese, Japanese, Hausa,\nand Yoruba) and domains (general and biomedical) demonstrate the portability of\nour framework.", "published": "2016-03-10 00:33:28", "link": "http://arxiv.org/abs/1603.03112v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Part-of-Speech Tagging for Historical English", "abstract": "As more historical texts are digitized, there is interest in applying natural\nlanguage processing tools to these archives. However, the performance of these\ntools is often unsatisfactory, due to language change and genre differences.\nSpelling normalization heuristics are the dominant solution for dealing with\nhistorical texts, but this approach fails to account for changes in usage and\nvocabulary. In this empirical paper, we assess the capability of domain\nadaptation techniques to cope with historical texts, focusing on the classic\nbenchmark task of part-of-speech tagging. We evaluate several domain adaptation\nmethods on the task of tagging Early Modern English and Modern British English\ntexts in the Penn Corpora of Historical English. We demonstrate that the\nFeature Embedding method for unsupervised domain adaptation outperforms word\nembeddings and Brown clusters, showing the importance of embedding the entire\nfeature space, rather than just individual words. Feature Embeddings also give\nbetter performance than spelling normalization, but the combination of the two\nmethods is better still, yielding a 5% raw improvement in tagging accuracy on\nEarly Modern English texts.", "published": "2016-03-10 04:27:15", "link": "http://arxiv.org/abs/1603.03144v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Zipf's law emerges asymptotically during phase transitions in\n  communicative systems", "abstract": "Zipf's law predicts a power-law relationship between word rank and frequency\nin language communication systems, and is widely reported in texts yet remains\nenigmatic as to its origins. Computer simulations have shown that language\ncommunication systems emerge at an abrupt phase transition in the fidelity of\nmappings between symbols and objects. Since the phase transition approximates\nthe Heaviside or step function, we show that Zipfian scaling emerges\nasymptotically at high rank based on the Laplace transform. We thereby\ndemonstrate that Zipf's law gradually emerges from the moment of phase\ntransition in communicative systems. We show that this power-law scaling\nbehavior explains the emergence of natural languages at phase transitions. We\nfind that the emergence of Zipf's law during language communication suggests\nthat the use of rare words in a lexicon is critical for the construction of an\neffective communicative system at the phase transition.", "published": "2016-03-10 06:01:28", "link": "http://arxiv.org/abs/1603.03153v2", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Data fluidity in DARIAH -- pushing the agenda forward", "abstract": "This paper provides both an update concerning the setting up of the European\nDARIAH infrastructure and a series of strong action lines related to the\ndevelopment of a data centred strategy for the humanities in the coming years.\nIn particular we tackle various aspect of data management: data hosting, the\nsetting up of a DARIAH seal of approval, the establishment of a charter between\ncultural heritage institutions and scholars and finally a specific view on\ncertification mechanisms for data.", "published": "2016-03-10 07:43:15", "link": "http://arxiv.org/abs/1603.03170v2", "categories": ["cs.CY", "cs.CL", "cs.DL"], "primary_category": "cs.CY"}
{"title": "Personalized Speech recognition on mobile devices", "abstract": "We describe a large vocabulary speech recognition system that is accurate,\nhas low latency, and yet has a small enough memory and computational footprint\nto run faster than real-time on a Nexus 5 Android smartphone. We employ a\nquantized Long Short-Term Memory (LSTM) acoustic model trained with\nconnectionist temporal classification (CTC) to directly predict phoneme\ntargets, and further reduce its memory footprint using an SVD-based compression\nscheme. Additionally, we minimize our memory footprint by using a single\nlanguage model for both dictation and voice command domains, constructed using\nBayesian interpolation. Finally, in order to properly handle device-specific\ninformation, such as proper names and other context-dependent information, we\ninject vocabulary items into the decoder graph and bias the language model\non-the-fly. Our system achieves 13.5% word error rate on an open-ended\ndictation task, running with a median speed that is seven times faster than\nreal-time.", "published": "2016-03-10 08:51:51", "link": "http://arxiv.org/abs/1603.03185v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
