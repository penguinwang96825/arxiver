{"title": "Publicly Available Clinical BERT Embeddings", "abstract": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.", "published": "2019-04-06 00:34:39", "link": "http://arxiv.org/abs/1904.03323v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for\n  out-of-domain samples", "abstract": "This paper describes our system, Joint Encoders for Stable Suggestion\nInference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online\nReviews and Forums. JESSI is a combination of two sentence encoders: (a) one\nusing multiple pre-trained word embeddings learned from log-bilinear regression\n(GloVe) and translation (CoVe) models, and (b) one on top of word encodings\nfrom a pre-trained deep bidirectional transformer (BERT). We include a domain\nadversarial training module when training for out-of-domain samples. Our\nexperiments show that while BERT performs exceptionally well for in-domain\nsamples, several runs of the model show that it is unstable for out-of-domain\nsamples. The problem is mitigated tremendously by (1) combining BERT with a\nnon-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our\nfinal models obtained second place with 77.78\\% F-Score on Subtask A (i.e.\nin-domain) and achieved an F-Score of 79.59\\% on Subtask B (i.e.\nout-of-domain), even without using any additional external data.", "published": "2019-04-06 02:24:30", "link": "http://arxiv.org/abs/1904.03339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Steep Road to Happily Ever After: An Analysis of Current Visual\n  Storytelling Models", "abstract": "Visual storytelling is an intriguing and complex task that only recently\nentered the research arena. In this work, we survey relevant work to date, and\nconduct a thorough error analysis of three very recent approaches to visual\nstorytelling. We categorize and provide examples of common types of errors, and\nidentify key shortcomings in current work. Finally, we make recommendations for\naddressing these limitations in the future.", "published": "2019-04-06 05:42:19", "link": "http://arxiv.org/abs/1904.03366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallelizable Stack Long Short-Term Memory", "abstract": "Stack Long Short-Term Memory (StackLSTM) is useful for various applications\nsuch as parsing and string-to-tree neural machine translation, but it is also\nknown to be notoriously difficult to parallelize for GPU training due to the\nfact that the computations are dependent on discrete operations. In this paper,\nwe tackle this problem by utilizing state access patterns of StackLSTM to\nhomogenize computations with regard to different discrete operations. Our\nparsing experiments show that the method scales up almost linearly with\nincreasing batch size, and our parallelized PyTorch implementation trains\nsignificantly faster compared to the Dynet C++ implementation.", "published": "2019-04-06 10:12:27", "link": "http://arxiv.org/abs/1904.03409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speeding Up Natural Language Parsing by Reusing Partial Results", "abstract": "This paper proposes a novel technique that applies case-based reasoning in\norder to generate templates for reusable parse tree fragments, based on PoS\ntags of bigrams and trigrams that demonstrate low variability in their\nsyntactic analyses from prior data. The aim of this approach is to improve the\nspeed of dependency parsers by avoiding redundant calculations. This can be\nresolved by applying the predefined templates that capture results of previous\nsyntactic analyses and directly assigning the stored structure to a new n-gram\nthat matches one of the templates, instead of parsing a similar text fragment\nagain. The study shows that using a heuristic approach to select and reuse the\npartial results increases parsing speed by reducing the input length to be\nprocessed by a parser. The increase in parsing speed comes at some expense of\naccuracy. Experiments on English show promising results: the input dimension\ncan be reduced by more than 20% at the cost of less than 3 points of Unlabeled\nAttachment Score.", "published": "2019-04-06 10:55:11", "link": "http://arxiv.org/abs/1904.03417v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "UM-IU@LING at SemEval-2019 Task 6: Identifying Offensive Tweets Using\n  BERT and SVMs", "abstract": "This paper describes the UM-IU@LING's system for the SemEval 2019 Task 6:\nOffensEval. We take a mixed approach to identify and categorize hate speech in\nsocial media. In subtask A, we fine-tuned a BERT based classifier to detect\nabusive content in tweets, achieving a macro F1 score of 0.8136 on the test\ndata, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C,\nwe used a linear SVM with selected character n-gram features. For subtask C,\nour system could identify the target of abuse with a macro F1 score of 0.5243,\nranking it 27th out of 65 submissions.", "published": "2019-04-06 14:02:13", "link": "http://arxiv.org/abs/1904.03450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Integrated Approach for Keyphrase Generation via Exploring the Power\n  of Retrieval and Extraction", "abstract": "In this paper, we present a novel integrated approach for keyphrase\ngeneration (KG). Unlike previous works which are purely extractive or\ngenerative, we first propose a new multi-task learning framework that jointly\nlearns an extractive model and a generative model. Besides extracting\nkeyphrases, the output of the extractive model is also employed to rectify the\ncopy probability distribution of the generative model, such that the generative\nmodel can better identify important contents from the given document. Moreover,\nwe retrieve similar documents with the given document from training data and\nuse their associated keyphrases as external knowledge for the generative model\nto produce more accurate keyphrases. For further exploiting the power of\nextraction and retrieval, we propose a neural-based merging module to combine\nand re-rank the predicted keyphrases from the enhanced generative model, the\nextractive model, and the retrieved keyphrases. Experiments on the five KG\nbenchmarks demonstrate that our integrated approach outperforms the\nstate-of-the-art methods.", "published": "2019-04-06 14:18:25", "link": "http://arxiv.org/abs/1904.03454v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking Discrete and Continuous Entity State for Process Understanding", "abstract": "Procedural text, which describes entities and their interactions as they\nundergo some process, depicts entities in a uniquely nuanced way. First, each\nentity may have some observable discrete attributes, such as its state or\nlocation; modeling these involves imposing global structure and enforcing\nconsistency. Second, an entity may have properties which are not made explicit\nbut can be effectively induced and tracked by neural networks. In this paper,\nwe propose a structured neural architecture that reflects this dual nature of\nentity evolution. The model tracks each entity recurrently, updating its hidden\ncontinuous representation at each step to contain relevant state information.\nThe global discrete state structure is explicitly modeled with a neural CRF\nover the changing hidden representation of the entity. This CRF can explicitly\ncapture constraints on entity states over time, enforcing that, for example, an\nentity cannot move to a location after it is destroyed. We evaluate the\nperformance of our proposed model on QA tasks over process paragraphs in the\nProPara dataset and find that our model achieves state-of-the-art results.", "published": "2019-04-06 19:56:14", "link": "http://arxiv.org/abs/1904.03518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Coherence in Dialogue Systems using Entailment", "abstract": "Evaluating open-domain dialogue systems is difficult due to the diversity of\npossible correct answers. Automatic metrics such as BLEU correlate weakly with\nhuman annotations, resulting in a significant bias across different models and\ndatasets. Some researchers resort to human judgment experimentation for\nassessing response quality, which is expensive, time consuming, and not\nscalable. Moreover, judges tend to evaluate a small number of dialogues,\nmeaning that minor differences in evaluation configuration may lead to\ndissimilar results. In this paper, we present interpretable metrics for\nevaluating topic coherence by making use of distributed sentence\nrepresentations. Furthermore, we introduce calculable approximations of human\njudgment based on conversational coherence by adopting state-of-the-art\nentailment techniques. Results show that our metrics can be used as a surrogate\nfor human judgment, making it easy to evaluate dialogue systems on large-scale\ndatasets and allowing an unbiased estimate for the quality of the responses.", "published": "2019-04-06 06:06:11", "link": "http://arxiv.org/abs/1904.03371v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Step-by-Step: Separating Planning from Realization in Neural\n  Data-to-Text Generation", "abstract": "Data-to-text generation can be conceptually divided into two parts: ordering\nand structuring the information (planning), and generating fluent language\ndescribing the information (realization). Modern neural generation systems\nconflate these two steps into a single end-to-end differentiable system. We\npropose to split the generation process into a symbolic text-planning stage\nthat is faithful to the input, followed by a neural generation stage that\nfocuses only on realization. For training a plan-to-text generator, we present\na method for matching reference texts to their corresponding text plans. For\ninference time, we describe a method for selecting high-quality text plans for\nnew inputs. We implement and evaluate our approach on the WebNLG benchmark. Our\nresults demonstrate that decoupling text planning from neural realization\nindeed improves the system's reliability and adequacy while maintaining fluent\noutput. We observe improvements both in BLEU scores and in manual evaluations.\nAnother benefit of our approach is the ability to output diverse realizations\nof the same input, paving the way to explicit control over the generated text\nstructure.", "published": "2019-04-06 09:25:32", "link": "http://arxiv.org/abs/1904.03396v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion", "abstract": "Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech\nrecognition and text-to-speech systems. Recently, G2P conversion is viewed as a\nsequence to sequence task and modeled by RNN or CNN based encoder-decoder\nframework. However, previous works do not consider the practical issues when\ndeploying G2P model in the production system, such as how to leverage\nadditional unlabeled data to boost the accuracy, as well as reduce model size\nfor online deployment. In this work, we propose token-level ensemble\ndistillation for G2P conversion, which can (1) boost the accuracy by distilling\nthe knowledge from additional unlabeled data, and (2) reduce the model size but\nmaintain the high accuracy, both of which are very practical and helpful in the\nonline production system. We use token-level knowledge distillation, which\nresults in better accuracy than the sequence-level counterpart. What is more,\nwe adopt the Transformer instead of RNN or CNN based models to further boost\nthe accuracy of G2P conversion. Experiments on the publicly available CMUDict\ndataset and an internal English dataset demonstrate the effectiveness of our\nproposed method. Particularly, our method achieves 19.88% WER on CMUDict\ndataset, outperforming the previous works by more than 4.22% WER, and setting\nthe new state-of-the-art results.", "published": "2019-04-06 13:49:16", "link": "http://arxiv.org/abs/1904.03446v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Embodied Question Answering in Photorealistic Environments with Point\n  Cloud Perception", "abstract": "To help bridge the gap between internet vision-style problems and the goal of\nvision for embodied perception we instantiate a large-scale navigation task --\nEmbodied Question Answering [1] in photo-realistic environments (Matterport\n3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB\nimages, or their combination. Our analysis of these models reveals several key\nfindings. We find that two seemingly naive navigation baselines, forward-only\nand random, are strong navigators and challenging to outperform, due to the\nspecific choice of the evaluation setting presented by [1]. We find a novel\nloss-weighting scheme we call Inflection Weighting to be important when\ntraining recurrent models for navigation with behavior cloning and are able to\nout perform the baselines with this technique. We find that point clouds\nprovide a richer signal than RGB images for learning obstacle avoidance,\nmotivating the use (and continued study) of 3D deep learning models for\nembodied navigation.", "published": "2019-04-06 14:50:11", "link": "http://arxiv.org/abs/1904.03461v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Mathematics of Text Structure", "abstract": "In previous work we gave a mathematical foundation, referred to as DisCoCat,\nfor how words interact in a sentence in order to produce the meaning of that\nsentence. To do so, we exploited the perfect structural match of grammar and\ncategories of meaning spaces. Here, we give a mathematical foundation, referred\nto as DisCoCirc, for how sentences interact in texts in order to produce the\nmeaning of that text. First we revisit DisCoCat. While in DisCoCat all meanings\nare fixed as states (i.e. have no input), in DisCoCirc word meanings correspond\nto a type, or system, and the states of this system can evolve. Sentences are\ngates within a circuit which update the variable meanings of those words. Like\nin DisCoCat, word meanings can live in a variety of spaces e.g. propositional,\nvectorial, or cognitive. The compositional structure are string diagrams\nrepresenting information flows, and an entire text yields a single string\ndiagram in which word meanings lift to the meaning of an entire text. While the\ndevelopments in this paper are independent of a physical embodiment (cf.\nclassical vs. quantum computing), both the compositional formalism and\nsuggested meaning model are highly quantum-inspired, and implementation on a\nquantum computer would come with a range of benefits. We also praise Jim Lambek\nfor his role in mathematical linguistics in general, and the development of the\nDisCo program more specifically.", "published": "2019-04-06 15:47:13", "link": "http://arxiv.org/abs/1904.03478v2", "categories": ["cs.CL", "math.CT", "quant-ph"], "primary_category": "cs.CL"}
{"title": "VATEX: A Large-Scale, High-Quality Multilingual Dataset for\n  Video-and-Language Research", "abstract": "We present a new large-scale multilingual video description dataset, VATEX,\nwhich contains over 41,250 videos and 825,000 captions in both English and\nChinese. Among the captions, there are over 206,000 English-Chinese parallel\ntranslation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is\nmultilingual, larger, linguistically complex, and more diverse in terms of both\nvideo and natural language descriptions. We also introduce two tasks for\nvideo-and-language research based on VATEX: (1) Multilingual Video Captioning,\naimed at describing a video in various languages with a compact unified\ncaptioning model, and (2) Video-guided Machine Translation, to translate a\nsource language description into the target language using the video\ninformation as additional spatiotemporal context. Extensive experiments on the\nVATEX dataset show that, first, the unified multilingual model can not only\nproduce both English and Chinese descriptions for a video more efficiently, but\nalso offer improved performance over the monolingual models. Furthermore, we\ndemonstrate that the spatiotemporal video context can be effectively utilized\nto align source and target languages and thus assist machine translation. In\nthe end, we discuss the potentials of using VATEX for other video-and-language\nresearch.", "published": "2019-04-06 16:50:31", "link": "http://arxiv.org/abs/1904.03493v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Team QCRI-MIT at SemEval-2019 Task 4: Propaganda Analysis Meets\n  Hyperpartisan News Detection", "abstract": "In this paper, we describe our submission to SemEval-2019 Task 4 on\nHyperpartisan News Detection. Our system relies on a variety of engineered\nfeatures originally used to detect propaganda. This is based on the assumption\nthat biased messages are propagandistic in the sense that they promote a\nparticular political cause or viewpoint. We trained a logistic regression model\nwith features ranging from simple bag-of-words to vocabulary richness and text\nreadability features. Our system achieved 72.9% accuracy on the test data that\nis annotated manually and 60.8% on the test data that is annotated with distant\nsupervision. Additional experiments showed that significant performance\nimprovements can be achieved with better feature pre-processing.", "published": "2019-04-06 19:04:29", "link": "http://arxiv.org/abs/1904.03513v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Cross-task learning for audio tagging, sound event detection and spatial\n  localization: DCASE 2019 baseline systems", "abstract": "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019\nchallenge focuses on audio tagging, sound event detection and spatial\nlocalisation. DCASE 2019 consists of five tasks: 1) acoustic scene\nclassification, 2) audio tagging with noisy labels and minimal supervision, 3)\nsound event localisation and detection, 4) sound event detection in domestic\nenvironments, and 5) urban sound tagging. In this paper, we propose generic\ncross-task baseline systems based on convolutional neural networks (CNNs). The\nmotivation is to investigate the performance of a variety of models across\nseveral audio recognition tasks without exploiting the specific characteristics\nof the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the\noptimal architecture is task-dependent. For the systems we considered, we found\nthat the 9-layer CNN with average pooling after convolutional layers is a good\nmodel for a majority of the DCASE 2019 tasks.", "published": "2019-04-06 15:37:40", "link": "http://arxiv.org/abs/1904.03476v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Margin Softmax Loss for Speaker Verification", "abstract": "In neural network based speaker verification, speaker embedding is expected\nto be discriminative between speakers while the intra-speaker distance should\nremain small. A variety of loss functions have been proposed to achieve this\ngoal. In this paper, we investigate the large margin softmax loss with\ndifferent configurations in speaker verification. Ring loss and minimum\nhyperspherical energy criterion are introduced to further improve the\nperformance. Results on VoxCeleb show that our best system outperforms the\nbaseline approach by 15\\% in EER, and by 13\\%, 33\\% in minDCF08 and minDCF10,\nrespectively.", "published": "2019-04-06 15:53:43", "link": "http://arxiv.org/abs/1904.03479v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems", "abstract": "This paper introduces a new database of voice recordings with the goal of\nsupporting research on vulnerabilities and protection of voice-controlled\nsystems (VCSs). In contrast to prior efforts, the proposed database contains\nboth genuine voice commands and replayed recordings of such commands, collected\nin realistic VCSs usage scenarios and using modern voice assistant development\nkits. Specifically, the database contains recordings from four systems (each\nwith a different microphone array) in a variety of environmental conditions\nwith different forms of background noise and relative positions between speaker\nand device. To the best of our knowledge, this is the first publicly available\ndatabase that has been specifically designed for the protection of\nstate-of-the-art voice-controlled systems against various replay attacks in\nvarious conditions and environments.", "published": "2019-04-06 05:42:18", "link": "http://arxiv.org/abs/1904.03365v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Learning Problem-agnostic Speech Representations from Multiple\n  Self-supervised Tasks", "abstract": "Learning good representations without supervision is still an open issue in\nmachine learning, and is particularly challenging for speech signals, which are\noften characterized by long sequences with a complex hierarchical structure.\nSome recent works, however, have shown that it is possible to derive useful\nspeech representations by employing a self-supervised encoder-discriminator\napproach. This paper proposes an improved self-supervised method, where a\nsingle neural encoder is followed by multiple workers that jointly solve\ndifferent self-supervised tasks. The needed consensus across different tasks\nnaturally imposes meaningful constraints to the encoder, contributing to\ndiscover general representations and to minimize the risk of learning\nsuperficial ones. Experiments show that the proposed approach can learn\ntransferable, robust, and problem-agnostic features that carry on relevant\ninformation from the speech signal, such as speaker identity, phonemes, and\neven higher-level features such as emotional cues. In addition, a number of\ndesign choices make the encoder easily exportable, facilitating its direct\nusage or adaptation to different problems.", "published": "2019-04-06 10:51:25", "link": "http://arxiv.org/abs/1904.03416v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Generalized Speech Enhancement with Generative Adversarial\n  Networks", "abstract": "The speech enhancement task usually consists of removing additive noise or\nreverberation that partially mask spoken utterances, affecting their\nintelligibility. However, little attention is drawn to other, perhaps more\naggressive signal distortions like clipping, chunk elimination, or\nfrequency-band removal. Such distortions can have a large impact not only on\nintelligibility, but also on naturalness or even speaker identity, and require\nof careful signal reconstruction. In this work, we give full consideration to\nthis generalized speech enhancement task, and show it can be tackled with a\ntime-domain generative adversarial network (GAN). In particular, we extend a\nprevious GAN-based speech enhancement system to deal with mixtures of four\ntypes of aggressive distortions. Firstly, we propose the addition of an\nadversarial acoustic regression loss that promotes a richer feature extraction\nat the discriminator. Secondly, we also make use of a two-step adversarial\ntraining schedule, acting as a warm up-and-fine-tune sequence. Both objective\nand subjective evaluations show that these two additions bring improved speech\nreconstructions that better match the original speaker identity and\nnaturalness.", "published": "2019-04-06 10:58:17", "link": "http://arxiv.org/abs/1904.03418v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Taco-VC: A Single Speaker Tacotron based Voice Conversion with Limited\n  Data", "abstract": "This paper introduces Taco-VC, a novel architecture for voice conversion\nbased on Tacotron synthesizer, which is a sequence-to-sequence with attention\nmodel. The training of multi-speaker voice conversion systems requires a large\nnumber of resources, both in training and corpus size. Taco-VC is implemented\nusing a single speaker Tacotron synthesizer based on Phonetic PosteriorGrams\n(PPGs) and a single speaker WaveNet vocoder conditioned on mel spectrograms. To\nenhance the converted speech quality, and to overcome over-smoothing, the\noutputs of Tacotron are passed through a novel speechenhancement network, which\nis composed of a combination of the phoneme recognition and Tacotron networks.\nOur system is trained just with a single speaker corpus and adapts to new\nspeakers using only a few minutes of training data. Using mid-size public\ndatasets, our method outperforms the baseline in the VCC 2018 SPOKE\nnon-parallel voice conversion task and achieves competitive results compared to\nmulti-speaker networks trained on large private datasets.", "published": "2019-04-06 20:19:07", "link": "http://arxiv.org/abs/1904.03522v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatio-Temporal Attention Pooling for Audio Scene Classification", "abstract": "Acoustic scenes are rich and redundant in their content. In this work, we\npresent a spatio-temporal attention pooling layer coupled with a convolutional\nrecurrent neural network to learn from patterns that are discriminative while\nsuppressing those that are irrelevant for acoustic scene classification. The\nconvolutional layers in this network learn invariant features from\ntime-frequency input. The bidirectional recurrent layers are then able to\nencode the temporal dynamics of the resulting convolutional features.\nAfterwards, a two-dimensional attention mask is formed via the outer product of\nthe spatial and temporal attention vectors learned from two designated\nattention layers to weigh and pool the recurrent output into a final feature\nvector for classification. The network is trained with between-class examples\ngenerated from between-class data augmentation. Experiments demonstrate that\nthe proposed method not only outperforms a strong convolutional neural network\nbaseline but also sets new state-of-the-art performance on the LITIS Rouen\ndataset.", "published": "2019-04-06 22:49:20", "link": "http://arxiv.org/abs/1904.03543v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
