{"title": "Word and Document Embeddings based on Neural Network Approaches", "abstract": "Data representation is a fundamental task in machine learning. The\nrepresentation of data affects the performance of the whole machine learning\nsystem. In a long history, the representation of data is done by feature\nengineering, and researchers aim at designing better features for specific\ntasks. Recently, the rapid development of deep learning and representation\nlearning has brought new inspiration to various domains.\n  In natural language processing, the most widely used feature representation\nis the Bag-of-Words model. This model has the data sparsity problem and cannot\nkeep the word order information. Other features such as part-of-speech tagging\nor more complex syntax features can only fit for specific tasks in most cases.\nThis thesis focuses on word representation and document representation. We\ncompare the existing systems and present our new model.\n  First, for generating word embeddings, we make comprehensive comparisons\namong existing word embedding models. In terms of theory, we figure out the\nrelationship between the two most important models, i.e., Skip-gram and GloVe.\nIn our experiments, we analyze three key points in generating word embeddings,\nincluding the model construction, the training corpus and parameter design. We\nevaluate word embeddings with three types of tasks, and we argue that they\ncover the existing use of word embeddings. Through theory and practical\nexperiments, we present some guidelines for how to generate a good word\nembedding.\n  Second, in Chinese character or word representation. We introduce the joint\ntraining of Chinese character and word. ...\n  Third, for document representation, we analyze the existing document\nrepresentation models, including recursive NNs, recurrent NNs and convolutional\nNNs. We point out the drawbacks of these models and present our new model, the\nrecurrent convolutional neural networks. ...", "published": "2016-11-18 03:21:28", "link": "http://arxiv.org/abs/1611.05962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Properties of European Languages and Voynich Manuscript\n  Analysis", "abstract": "The statistical properties of letters frequencies in European literature\ntexts are investigated. The determination of logarithmic dependence of letters\nsequence for one-language and two-language texts are examined. The pare of\nlanguages is suggested for Voynich Manuscript. The internal structure of\nManuscript is considered. The spectral portraits of two-letters distribution\nare constructed.", "published": "2016-11-18 03:48:04", "link": "http://arxiv.org/abs/1611.09122v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Variable Computation in Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been used extensively and with\nincreasing success to model various types of sequential data. Much of this\nprogress has been achieved through devising recurrent units and architectures\nwith the flexibility to capture complex statistics in the data, such as long\nrange dependency or localized attention phenomena. However, while many\nsequential data (such as video, speech or language) can have highly variable\ninformation flow, most recurrent models still consume input features at a\nconstant rate and perform a constant number of computations per time step,\nwhich can be detrimental to both speed and model capacity. In this paper, we\nexplore a modification to existing recurrent units which allows them to learn\nto vary the amount of computation they perform at each step, without prior\nknowledge of the sequence's time structure. We show experimentally that not\nonly do our models require fewer operations, they also lead to better\nperformance overall on evaluation tasks.", "published": "2016-11-18 18:13:46", "link": "http://arxiv.org/abs/1611.06188v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Visualizing and Understanding Curriculum Learning for Long Short-Term\n  Memory Networks", "abstract": "Curriculum Learning emphasizes the order of training instances in a\ncomputational learning setup. The core hypothesis is that simpler instances\nshould be learned early as building blocks to learn more complex ones. Despite\nits usefulness, it is still unknown how exactly the internal representation of\nmodels are affected by curriculum learning. In this paper, we study the effect\nof curriculum learning on Long Short-Term Memory (LSTM) networks, which have\nshown strong competency in many Natural Language Processing (NLP) problems. Our\nexperiments on sentiment analysis task and a synthetic task similar to sequence\nprediction tasks in NLP show that curriculum learning has a positive effect on\nthe LSTM's internal states by biasing the model towards building constructive\nrepresentations i.e. the internal representation at the previous timesteps are\nused as building blocks for the final prediction. We also find that smaller\nmodels significantly improves when they are trained with curriculum learning.\nLastly, we show that curriculum learning helps more when the amount of training\ndata is limited.", "published": "2016-11-18 19:38:59", "link": "http://arxiv.org/abs/1611.06204v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Generative Deep Neural Networks for Dialogue: A Short Review", "abstract": "Researchers have recently started investigating deep neural networks for\ndialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)\nmodels have shown promising results for unstructured tasks, such as word-level\ndialogue response generation. The hope is that such models will be able to\nleverage massive amounts of data to learn meaningful natural language\nrepresentations and response generation strategies, while requiring a minimum\namount of domain knowledge and hand-crafting. An important challenge is to\ndevelop models that can effectively incorporate dialogue context and generate\nmeaningful and diverse responses. In support of this goal, we review recently\nproposed models based on generative encoder-decoder neural network\narchitectures, and show that these models have better ability to incorporate\nlong-term dialogue history, to model uncertainty and ambiguity in dialogue, and\nto generate responses with high-level compositional structure.", "published": "2016-11-18 20:11:51", "link": "http://arxiv.org/abs/1611.06216v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "I.5.1; I.2.7"], "primary_category": "cs.CL"}
