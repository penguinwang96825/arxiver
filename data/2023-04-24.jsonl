{"title": "Directed Acyclic Transformer Pre-training for High-quality\n  Non-autoregressive Text Generation", "abstract": "Non-AutoRegressive (NAR) text generation models have drawn much attention\nbecause of their significantly faster decoding speed and good generation\nquality in machine translation. However, in a wider range of text generation\ntasks, existing NAR models lack proper pre-training, making them still far\nbehind the pre-trained autoregressive models. In this paper, we propose\nPre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task\nto promote prediction consistency in NAR generation. Experiments on five text\ngeneration tasks show that our PreDAT remarkably outperforms existing\npre-trained NAR models (+4.2 scores on average) and even achieves better\nresults than pre-trained autoregressive baselines in n-gram-based metrics,\nalong with 17 times speedup in throughput. Further analysis shows that PreDAT\nbenefits from the unbiased prediction order that alleviates the error\naccumulation problem in autoregressive generation, which provides new insights\ninto the advantages of NAR generation.", "published": "2023-04-24 02:30:33", "link": "http://arxiv.org/abs/2304.11791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts", "abstract": "The powerful ability of ChatGPT has caused widespread concern in the academic\ncommunity. Malicious users could synthesize dummy academic content through\nChatGPT, which is extremely harmful to academic rigor and originality. The need\nto develop ChatGPT-written content detection algorithms call for large-scale\ndatasets. In this paper, we initially investigate the possible negative impact\nof ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract\ndataset (CHEAT) to support the development of detection algorithms. In\nparticular, the ChatGPT-written abstract dataset contains 35,304 synthetic\nabstracts, with Generation, Polish, and Mix as prominent representatives. Based\non these data, we perform a thorough analysis of the existing text synthesis\ndetection algorithms. We show that ChatGPT-written abstracts are detectable,\nwhile the detection difficulty increases with human involvement.Our dataset is\navailable in https://github.com/botianzhe/CHEAT.", "published": "2023-04-24 11:19:33", "link": "http://arxiv.org/abs/2304.12008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SocialDial: A Benchmark for Socially-Aware Dialogue Systems", "abstract": "Dialogue systems have been widely applied in many scenarios and are now more\npowerful and ubiquitous than ever before. With large neural models and massive\navailable data, current dialogue systems have access to more knowledge than any\npeople in their life. However, current dialogue systems still do not perform at\na human level. One major gap between conversational agents and humans lies in\ntheir abilities to be aware of social norms. The development of socially-aware\ndialogue systems is impeded due to the lack of resources. In this paper, we\npresent the first socially-aware dialogue corpus - SocialDial, based on Chinese\nsocial culture. SocialDial consists of two parts: 1,563 multi-turn dialogues\nbetween two human speakers with fine-grained labels, and 4,870 synthetic\nconversations generated by ChatGPT. The human corpus covers five categories of\nsocial norms, which have 14 sub-categories in total. Specifically, it contains\nsocial factor annotations including social relation, context, social distance,\nand social norms. However, collecting sufficient socially-aware dialogues is\ncostly. Thus, we harness the power of ChatGPT and devise an ontology-based\nsynthetic data generation framework. This framework is able to generate\nsynthetic data at scale. To ensure the quality of synthetic dialogues, we\ndesign several mechanisms for quality control during data collection. Finally,\nwe evaluate our dataset using several pre-trained models, such as BERT and\nRoBERTa. Comprehensive empirical results based on state-of-the-art neural\nmodels demonstrate that modeling of social norms for dialogue systems is a\npromising research direction. To the best of our knowledge, SocialDial is the\nfirst socially-aware dialogue dataset that covers multiple social factors and\nhas fine-grained labels.", "published": "2023-04-24 11:55:22", "link": "http://arxiv.org/abs/2304.12026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of\n  LLMs with Self-Information-Based Content Filtering", "abstract": "Large language models (LLMs) have received significant attention by achieving\nremarkable performance across various tasks. However, their fixed context\nlength poses challenges when processing long documents or maintaining extended\nconversations. This paper proposes a method called \\textit{Selective Context}\nthat employs self-information to filter out less informative content, thereby\nenhancing the efficiency of the fixed context length. We demonstrate the\neffectiveness of our approach on tasks of summarisation and question answering\nacross different data sources, including academic papers, news articles, and\nconversation transcripts.", "published": "2023-04-24 13:55:47", "link": "http://arxiv.org/abs/2304.12102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAXQA: Generating Cross-lingual Question Answering Examples at Training\n  Scale", "abstract": "Existing question answering (QA) systems owe much of their success to large,\nhigh-quality training data. Such annotation efforts are costly, and the\ndifficulty compounds in the cross-lingual setting. Therefore, prior\ncross-lingual QA work has focused on releasing evaluation datasets, and then\napplying zero-shot methods as baselines. This work proposes a synthetic data\ngeneration method for cross-lingual QA which leverages indirect supervision\nfrom existing parallel corpora. Our method termed PAXQA (Projecting annotations\nfor cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First,\nwe apply a question generation (QG) model to the English side. Second, we apply\nannotation projection to translate both the questions and answers. To better\ntranslate questions, we propose a novel use of lexically-constrained machine\ntranslation, in which constrained entities are extracted from the parallel\nbitexts.\n  We apply PAXQA to generate cross-lingual QA examples in 4 languages (662K\nexamples total), and perform human evaluation on a subset to create validation\nand test splits. We then show that models fine-tuned on these datasets\noutperform prior synthetic data generation models over several extractive QA\ndatasets. The largest performance gains are for directions with non-English\nquestions and English contexts. Ablation studies show that our dataset\ngeneration method is relatively robust to noise from automatic word alignments,\nshowing the sufficient quality of our generations. To facilitate follow-up\nwork, we release our code and datasets at https://github.com/manestay/paxqa .", "published": "2023-04-24 15:46:26", "link": "http://arxiv.org/abs/2304.12206v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Source Code with Contextual Data for Code Completion Models:\n  An Empirical Study", "abstract": "Transformer-based pre-trained models have recently achieved great results in\nsolving many software engineering tasks including automatic code completion\nwhich is a staple in a developer's toolkit. While many have striven to improve\nthe code-understanding abilities of such models, the opposite -- making the\ncode easier to understand -- has not been properly investigated. In this study,\nwe aim to answer whether making code easier to understand through using\ncontextual data improves the performance of pre-trained code language models\nfor the task of code completion. We consider type annotations and comments as\ntwo common forms of additional contextual information that often help\ndevelopers understand code better. For the experiments, we study code\ncompletion in two granularity levels; token and line completion and take three\nrecent and large-scale language models for source code: UniXcoder, CodeGPT, and\nInCoder with five evaluation metrics. Finally, we perform the Wilcoxon Signed\nRank test to gauge significance and measure the effect size. Contrary to our\nexpectations, all models perform better if type annotations are removed (albeit\nthe effect sizes are small). For comments, we find that the models perform\nbetter in the presence of multi-line comments (again with small effect sizes).\nBased on our observations, we recommend making proper design choices when\ntraining, fine-tuning, or simply selecting such models given the intended data\nand application. Better evaluations and multi-modal techniques can also be\nfurther investigated to improve the practicality and accuracy of\nauto-completions.", "published": "2023-04-24 17:09:14", "link": "http://arxiv.org/abs/2304.12269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Question-Answering Models on a Budget", "abstract": "Low-rank adaptation (LoRA) and question-answer datasets from large language\nmodels have made it much easier for much smaller models to be finetuned to the\npoint where they display sophisticated conversational abilities. In this paper,\nwe present Eluwa, a family of LoRA models that use the Stanford Alpaca dataset\nand massively improve the capabilities of Facebook's OPT 1.3B, 2.7B and 6.7B\nmodels. We benchmark these models in multiple ways, including letting GPT-4\njudge their answers to prompts that span general knowledge, writing,\nprogramming and other tasks. We show that smaller models here can be fine-tuned\nto be as performant as models 3x larger - all for as little as 40 USD in\ncompute.", "published": "2023-04-24 18:06:27", "link": "http://arxiv.org/abs/2304.12370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Tokenizer for Enhanced Natural Language Processing", "abstract": "Traditionally, NLP performance improvement has been focused on improving\nmodels and increasing the number of model parameters. NLP vocabulary\nconstruction has remained focused on maximizing the number of words represented\nthrough subword regularization. We present a novel tokenizer that uses\nsemantics to drive vocabulary construction. The tokenizer includes a trainer\nthat uses stemming to enhance subword formation. Further optimizations and\nadaptations are implemented to minimize the number of words that cannot be\nencoded. The encoder is updated to integrate with the trainer. The tokenizer is\nimplemented as a drop-in replacement for the SentencePiece tokenizer. The new\ntokenizer more than doubles the number of wordforms represented in the\nvocabulary. The enhanced vocabulary significantly improves NLP model\nconvergence, and improves quality of word and sentence embeddings. Our\nexperimental results show top performance on two Glue tasks using BERT-base,\nimproving on models more than 50X in size.", "published": "2023-04-24 19:33:41", "link": "http://arxiv.org/abs/2304.12404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Predicting Human Label Variation in Natural Language\n  Inference through Explanation", "abstract": "Human label variation (Plank 2022), or annotation disagreement, exists in\nmany natural language processing (NLP) tasks. To be robust and trusted, NLP\nmodels need to identify such variation and be able to explain it. To this end,\nwe created the first ecologically valid explanation dataset with diverse\nreasoning, LiveNLI. LiveNLI contains annotators' highlights and free-text\nexplanations for the label(s) of their choice for 122 English Natural Language\nInference items, each with at least 10 annotations. We used its explanations\nfor chain-of-thought prompting, and found there is still room for improvement\nin GPT-3's ability to predict label distribution with in-context learning.", "published": "2023-04-24 20:45:09", "link": "http://arxiv.org/abs/2304.12443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generation-driven Contrastive Self-training for Zero-shot Text\n  Classification with Instruction-following LLM", "abstract": "The remarkable performance of large language models (LLMs) in zero-shot\nlanguage understanding has garnered significant attention. However, employing\nLLMs for large-scale inference or domain-specific fine-tuning requires immense\ncomputational resources due to their substantial model size. To overcome these\nlimitations, we introduce a novel method, namely GenCo, which leverages the\nstrong generative power of LLMs to assist in training a smaller and more\nadaptable language model. In our method, an LLM plays an important role in the\nself-training loop of a smaller model in two important ways. Firstly, the LLM\nis used to augment each input instance with a variety of possible\ncontinuations, enriching its semantic context for better understanding.\nSecondly, it helps crafting additional high-quality training pairs, by\nrewriting input texts conditioned on predicted labels. This ensures the\ngenerated texts are highly relevant to the predicted labels, alleviating the\nprediction error during pseudo-labeling, while reducing the dependency on large\nvolumes of unlabeled text. In our experiments, GenCo outperforms previous\nstate-of-the-art methods when only limited ($<5\\%$ of original) in-domain text\ndata is available. Notably, our approach surpasses the performance of Alpaca-7B\nwith human prompts, highlighting the potential of leveraging LLM for\nself-training.", "published": "2023-04-24 07:35:38", "link": "http://arxiv.org/abs/2304.11872v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Development of a Trust-Aware User Simulator for Statistical Proactive\n  Dialog Modeling in Human-AI Teams", "abstract": "The concept of a Human-AI team has gained increasing attention in recent\nyears. For effective collaboration between humans and AI teammates, proactivity\nis crucial for close coordination and effective communication. However, the\ndesign of adequate proactivity for AI-based systems to support humans is still\nan open question and a challenging topic. In this paper, we present the\ndevelopment of a corpus-based user simulator for training and testing proactive\ndialog policies. The simulator incorporates informed knowledge about proactive\ndialog and its effect on user trust and simulates user behavior and personal\ninformation, including socio-demographic features and personality traits. Two\ndifferent simulation approaches were compared, and a task-step-based approach\nyielded better overall results due to enhanced modeling of sequential\ndependencies. This research presents a promising avenue for exploring and\nevaluating appropriate proactive strategies in a dialog game setting for\nimproving Human-AI teams.", "published": "2023-04-24 08:42:51", "link": "http://arxiv.org/abs/2304.11913v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "KInITVeraAI at SemEval-2023 Task 3: Simple yet Powerful Multilingual\n  Fine-Tuning for Persuasion Techniques Detection", "abstract": "This paper presents the best-performing solution to the SemEval 2023 Task 3\non the subtask 3 dedicated to persuasion techniques detection. Due to a high\nmultilingual character of the input data and a large number of 23 predicted\nlabels (causing a lack of labelled data for some language-label combinations),\nwe opted for fine-tuning pre-trained transformer-based language models.\nConducting multiple experiments, we find the best configuration, which consists\nof large multilingual model (XLM-RoBERTa large) trained jointly on all input\ndata, with carefully calibrated confidence thresholds for seen and surprise\nlanguages separately. Our final system performed the best on 6 out of 9\nlanguages (including two surprise languages) and achieved highly competitive\nresults on the remaining three languages.", "published": "2023-04-24 09:06:43", "link": "http://arxiv.org/abs/2304.11924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Constructing Tree-based Index for Efficient and Effective Dense\n  Retrieval", "abstract": "Recent studies have shown that Dense Retrieval (DR) techniques can\nsignificantly improve the performance of first-stage retrieval in IR systems.\nDespite its empirical effectiveness, the application of DR is still limited. In\ncontrast to statistic retrieval models that rely on highly efficient inverted\nindex solutions, DR models build dense embeddings that are difficult to be\npre-processed with most existing search indexing systems. To avoid the\nexpensive cost of brute-force search, the Approximate Nearest Neighbor (ANN)\nalgorithm and corresponding indexes are widely applied to speed up the\ninference process of DR models. Unfortunately, while ANN can improve the\nefficiency of DR models, it usually comes with a significant price on retrieval\nperformance.\n  To solve this issue, we propose JTR, which stands for Joint optimization of\nTRee-based index and query encoding. Specifically, we design a new unified\ncontrastive learning loss to train tree-based index and query encoder in an\nend-to-end manner. The tree-based negative sampling strategy is applied to make\nthe tree have the maximum heap property, which supports the effectiveness of\nbeam search well. Moreover, we treat the cluster assignment as an optimization\nproblem to update the tree-based index that allows overlapped clustering. We\nevaluate JTR on numerous popular retrieval benchmarks. Experimental results\nshow that JTR achieves better retrieval performance while retaining high system\nefficiency compared with widely-adopted baselines. It provides a potential\nsolution to balance efficiency and effectiveness in neural retrieval system\ndesigns.", "published": "2023-04-24 09:25:39", "link": "http://arxiv.org/abs/2304.11943v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam\n  and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted\n  Medical Education and Decision Making in Radiation Oncology", "abstract": "The potential of large language models in medicine for education and decision\nmaking purposes has been demonstrated as they achieve decent scores on medical\nexams such as the United States Medical Licensing Exam (USMLE) and the MedQA\nexam. In this work, we evaluate the performance of ChatGPT-4 in the specialized\nfield of radiation oncology using the 38th American College of Radiology (ACR)\nradiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone\ncases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of\n63.65% and 74.57%, respectively, highlighting the advantage of the latest\nChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in\nradiation oncology are identified to some extent. Specifically, ChatGPT-4\ndemonstrates better knowledge of statistics, CNS & eye, pediatrics, biology,\nand physics than knowledge of bone & soft tissue and gynecology, as per the ACR\nknowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in\ndiagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks\nproficiency in in-depth details of clinical trials. For the Gray Zone cases,\nChatGPT-4 is able to suggest a personalized treatment approach to each case\nwith high correctness and comprehensiveness. Importantly, it provides novel\ntreatment aspects for many cases, which are not suggested by any human experts.\nBoth evaluations demonstrate the potential of ChatGPT-4 in medical education\nfor the general public and cancer patients, as well as the potential to aid\nclinical decision-making, while acknowledging its limitations in certain\ndomains. Because of the risk of hallucination, facts provided by ChatGPT always\nneed to be verified.", "published": "2023-04-24 09:50:39", "link": "http://arxiv.org/abs/2304.11957v4", "categories": ["physics.med-ph", "cs.CL"], "primary_category": "physics.med-ph"}
{"title": "WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions", "abstract": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM", "published": "2023-04-24 16:31:06", "link": "http://arxiv.org/abs/2304.12244v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AMR Parsing with Instruction Fine-tuned Pre-trained Language Models", "abstract": "Instruction fine-tuned language models on a collection of instruction\nannotated datasets (FLAN) have shown highly effective to improve model\nperformance and generalization to unseen tasks. However, a majority of standard\nparsing tasks including abstract meaning representation (AMR), universal\ndependency (UD), semantic role labeling (SRL) has been excluded from the FLAN\ncollections for both model training and evaluations. In this paper, we take one\nof such instruction fine-tuned pre-trained language models, i.e. FLAN-T5, and\nfine-tune them for AMR parsing. Our extensive experiments on various AMR\nparsing tasks including AMR2.0, AMR3.0 and BioAMR indicate that FLAN-T5\nfine-tuned models out-perform previous state-of-the-art models across all\ntasks. In addition, full fine-tuning followed by the parameter efficient\nfine-tuning, LoRA, further improves the model performances, setting new\nstate-of-the-arts in Smatch on AMR2.0 (86.4), AMR3.0 (84.9) and BioAMR (82.3).", "published": "2023-04-24 17:12:17", "link": "http://arxiv.org/abs/2304.12272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis\n  [Experiment, Analysis & Benchmark]", "abstract": "Many recent works on Entity Resolution (ER) leverage Deep Learning techniques\ninvolving language models to improve effectiveness. This is applied to both\nmain steps of ER, i.e., blocking and matching. Several pre-trained embeddings\nhave been tested, with the most popular ones being fastText and variants of the\nBERT model. However, there is no detailed analysis of their pros and cons. To\ncover this gap, we perform a thorough experimental analysis of 12 popular\nlanguage models over 17 established benchmark datasets. First, we assess their\nvectorization overhead for converting all input entities into dense embeddings\nvectors. Second, we investigate their blocking performance, performing a\ndetailed scalability analysis, and comparing them with the state-of-the-art\ndeep learning-based blocking method. Third, we conclude with their relative\nperformance for both supervised and unsupervised matching. Our experimental\nresults provide novel insights into the strengths and weaknesses of the main\nlanguage models, facilitating researchers and practitioners to select the most\nsuitable ones in practice.", "published": "2023-04-24 08:53:54", "link": "http://arxiv.org/abs/2304.12329v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "USTEP: Structuration des logs en flux gr{\u00e2}ce {\u00e0} un arbre de\n  recherche {\u00e9}volutif", "abstract": "Logs record valuable system information at runtime. They are widely used by\ndata-driven approaches for development and monitoring purposes. Parsing log\nmessages to structure their format is a classic preliminary step for log-mining\ntasks. As they appear upstream, parsing operations can become a processing time\nbottleneck for downstream applications. The quality of parsing also has a\ndirect influence on their efficiency. Here, we propose USTEP, an online log\nparsing method based on an evolving tree structure. Evaluation results on a\nwide panel of datasets coming from different real-world systems demonstrate\nUSTEP superiority in terms of both effectiveness and robustness when compared\nto other online methods.", "published": "2023-04-24 09:12:00", "link": "http://arxiv.org/abs/2304.12331v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in\n  Research", "abstract": "Perception of toxicity evolves over time and often differs between\ngeographies and cultural backgrounds. Similarly, black-box commercially\navailable APIs for detecting toxicity, such as the Perspective API, are not\nstatic, but frequently retrained to address any unattended weaknesses and\nbiases. We evaluate the implications of these changes on the reproducibility of\nfindings that compare the relative merits of models and methods that aim to\ncurb toxicity. Our findings suggest that research that relied on inherited\nautomatic toxicity scores to compare models and techniques may have resulted in\ninaccurate findings. Rescoring all models from HELM, a widely respected living\nbenchmark, for toxicity with the recent version of the API led to a different\nranking of widely used foundation models. We suggest caution in applying\napples-to-apples comparisons between studies and lay recommendations for a more\nstructured approach to evaluating toxicity over time. Code and data are\navailable at https://github.com/for-ai/black-box-api-challenges.", "published": "2023-04-24 19:11:51", "link": "http://arxiv.org/abs/2304.12397v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topological properties and organizing principles of semantic networks", "abstract": "Interpreting natural language is an increasingly important task in computer\nalgorithms due to the growing availability of unstructured textual data.\nNatural Language Processing (NLP) applications rely on semantic networks for\nstructured knowledge representation. The fundamental properties of semantic\nnetworks must be taken into account when designing NLP algorithms, yet they\nremain to be structurally investigated. We study the properties of semantic\nnetworks from ConceptNet, defined by 7 semantic relations from 11 different\nlanguages. We find that semantic networks have universal basic properties: they\nare sparse, highly clustered, and many exhibit power-law degree distributions.\nOur findings show that the majority of the considered networks are scale-free.\nSome networks exhibit language-specific properties determined by grammatical\nrules, for example networks from highly inflected languages, such as e.g.\nLatin, German, French and Spanish, show peaks in the degree distribution that\ndeviate from a power law. We find that depending on the semantic relation type\nand the language, the link formation in semantic networks is guided by\ndifferent principles. In some networks the connections are similarity-based,\nwhile in others the connections are more complementarity-based. Finally, we\ndemonstrate how knowledge of similarity and complementarity in semantic\nnetworks can improve NLP algorithms in missing link inference.", "published": "2023-04-24 11:12:21", "link": "http://arxiv.org/abs/2304.12940v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "ChatLLM Network: More brains, More intelligence", "abstract": "Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.", "published": "2023-04-24 08:29:14", "link": "http://arxiv.org/abs/2304.12998v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AI, write an essay for me: A large-scale comparison of human-written\n  versus ChatGPT-generated essays", "abstract": "Background: Recently, ChatGPT and similar generative AI models have attracted\nhundreds of millions of users and become part of the public discourse. Many\nbelieve that such models will disrupt society and will result in a significant\nchange in the education system and information generation in the future. So\nfar, this belief is based on either colloquial evidence or benchmarks from the\nowners of the models -- both lack scientific rigour.\n  Objective: Through a large-scale study comparing human-written versus\nChatGPT-generated argumentative student essays, we systematically assess the\nquality of the AI-generated content.\n  Methods: A large corpus of essays was rated using standard criteria by a\nlarge number of human experts (teachers). We augment the analysis with a\nconsideration of the linguistic characteristics of the generated essays.\n  Results: Our results demonstrate that ChatGPT generates essays that are rated\nhigher for quality than human-written essays. The writing style of the AI\nmodels exhibits linguistic characteristics that are different from those of the\nhuman-written essays, e.g., it is characterized by fewer discourse and\nepistemic markers, but more nominalizations and greater lexical diversity.\n  Conclusions: Our results clearly demonstrate that models like ChatGPT\noutperform humans in generating argumentative essays. Since the technology is\nreadily available for anyone to use, educators must act immediately. We must\nre-invent homework and develop teaching concepts that utilize these AI models\nin the same way as math utilized the calculator: teach the general concepts\nfirst and then use AI tools to free up time for other learning objectives.", "published": "2023-04-24 12:58:28", "link": "http://arxiv.org/abs/2304.14276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Topic Pages for Scientific Concepts Using Scientific\n  Publications", "abstract": "In this paper, we describe Topic Pages, an inventory of scientific concepts\nand information around them extracted from a large collection of scientific\nbooks and journals. The main aim of Topic Pages is to provide all the necessary\ninformation to the readers to understand scientific concepts they come across\nwhile reading scholarly content in any scientific domain. Topic Pages are a\ncollection of automatically generated information pages using NLP and ML, each\ncorresponding to a scientific concept. Each page contains three pieces of\ninformation: a definition, related concepts, and the most relevant snippets,\nall extracted from scientific peer-reviewed publications. In this paper, we\ndiscuss the details of different components to extract each of these elements.\nThe collection of pages in production contains over 360,000 Topic Pages across\n20 different scientific domains with an average of 23 million unique visits per\nmonth, constituting it a popular source for scientific information.", "published": "2023-04-24 09:03:14", "link": "http://arxiv.org/abs/2304.11922v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain", "abstract": "Publicly available information contains valuable information for Cyber Threat\nIntelligence (CTI). This can be used to prevent attacks that have already taken\nplace on other systems. Ideally, only the initial attack succeeds and all\nsubsequent ones are detected and stopped. But while there are different\nstandards to exchange this information, a lot of it is shared in articles or\nblog posts in non-standardized ways. Manually scanning through multiple online\nportals and news pages to discover new threats and extracting them is a\ntime-consuming task. To automize parts of this scanning process, multiple\npapers propose extractors that use Natural Language Processing (NLP) to extract\nIndicators of Compromise (IOCs) from documents. However, while this already\nsolves the problem of extracting the information out of documents, the search\nfor these documents is rarely considered. In this paper, a new focused crawler\nis proposed called ThreatCrawl, which uses Bidirectional Encoder\nRepresentations from Transformers (BERT)-based models to classify documents and\nadapt its crawling path dynamically. While ThreatCrawl has difficulties to\nclassify the specific type of Open Source Intelligence (OSINT) named in texts,\ne.g., IOC content, it can successfully find relevant documents and modify its\npath accord ingly. It yields harvest rates of up to 52%, which are, to the best\nof our knowledge, better than the current state of the art. The results and\nsource code will be made publicly available upon acceptance.", "published": "2023-04-24 09:53:33", "link": "http://arxiv.org/abs/2304.11960v4", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by\n  Identifying Important Nodes with Bridgeness", "abstract": "Node representation learning in a network is an important machine learning\ntechnique for encoding relational information in a continuous vector space\nwhile preserving the inherent properties and structures of the network.\nRecently, unsupervised node embedding methods such as DeepWalk, LINE,\nstruc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model\nand perform better performance in several downstream tasks such as node\nclassification and link prediction than the existing relational models.\nHowever, providing post-hoc explanations of Skip-gram-based embeddings remains\na challenging problem because of the lack of explanation methods and\ntheoretical studies applicable for embeddings. In this paper, we first show\nthat global explanations to the Skip-gram-based embeddings can be found by\ncomputing bridgeness under a spectral cluster-aware local perturbation.\nMoreover, a novel gradient-based explanation method, which we call GRAPH-wGD,\nis proposed that allows the top-q global explanations about learned graph\nembedding vectors more efficiently. Experiments show that the ranking of nodes\nby scores using GRAPH-wGD is highly correlated with true bridgeness scores. We\nalso observe that the top-q node-level explanations selected by GRAPH-wGD have\nhigher importance scores and produce more changes in class label prediction\nwhen perturbed, compared with the nodes selected by recent alternatives, using\nfive real-world graphs.", "published": "2023-04-24 12:25:35", "link": "http://arxiv.org/abs/2304.12036v3", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Extreme Classification for Answer Type Prediction in Question Answering", "abstract": "Semantic answer type prediction (SMART) is known to be a useful step towards\neffective question answering (QA) systems. The SMART task involves predicting\nthe top-$k$ knowledge graph (KG) types for a given natural language question.\nThis is challenging due to the large number of types in KGs. In this paper, we\npropose use of extreme multi-label classification using Transformer models\n(XBERT) by clustering KG types using structural and semantic features based on\nquestion text. We specifically improve the clustering stage of the XBERT\npipeline using textual and structural features derived from KGs. We show that\nthese features can improve end-to-end performance for the SMART task, and yield\nstate-of-the-art results.", "published": "2023-04-24 19:08:51", "link": "http://arxiv.org/abs/2304.12395v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PEFT-Ref: A Modular Reference Architecture and Typology for\n  Parameter-Efficient Finetuning Techniques", "abstract": "Recent parameter-efficient finetuning (PEFT) techniques aim to improve over\nthe considerable cost of fully finetuning large pretrained language models\n(PLM). As different PEFT techniques proliferate, it is becoming difficult to\ncompare them, in particular in terms of (i) the structure and functionality\nthey add to the PLM, (ii) the different types and degrees of efficiency\nimprovements achieved, (iii) performance at different downstream tasks, and\n(iv) how differences in structure and functionality relate to efficiency and\ntask performance. To facilitate such comparisons, this paper presents a\nreference architecture which standardises aspects shared by different PEFT\ntechniques, while isolating differences to specific locations and interactions\nwith the standard components. Through this process of standardising and\nisolating differences, a modular view of PEFT techniques emerges, supporting\nnot only direct comparison of different techniques and their efficiency and\ntask performance, but also systematic exploration of reusability and\ncomposability of the different types of finetuned modules. We demonstrate how\nthe reference architecture can be applied to understand properties and relative\nadvantages of PEFT techniques, hence to inform selection of techniques for\nspecific tasks, and design choices for new PEFT techniques.", "published": "2023-04-24 19:43:11", "link": "http://arxiv.org/abs/2304.12410v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TIGTEC : Token Importance Guided TExt Counterfactuals", "abstract": "Counterfactual examples explain a prediction by highlighting changes of\ninstance that flip the outcome of a classifier. This paper proposes TIGTEC, an\nefficient and modular method for generating sparse, plausible and diverse\ncounterfactual explanations for textual data. TIGTEC is a text editing\nheuristic that targets and modifies words with high contribution using local\nfeature importance. A new attention-based local feature importance is proposed.\nCounterfactual candidates are generated and assessed with a cost function\nintegrating semantic distance, while the solution space is efficiently explored\nin a beam search fashion. The conducted experiments show the relevance of\nTIGTEC in terms of success rate, sparsity, diversity and plausibility. This\nmethod can be used in both model-specific or model-agnostic way, which makes it\nvery convenient for generating counterfactual explanations.", "published": "2023-04-24 20:11:58", "link": "http://arxiv.org/abs/2304.12425v1", "categories": ["cs.LG", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent\n  Diffusion Model", "abstract": "The immense scale of the recent large language models (LLM) allows many\ninteresting properties, such as, instruction- and chain-of-thought-based\nfine-tuning, that has significantly improved zero- and few-shot performance in\nmany natural language processing (NLP) tasks. Inspired by such successes, we\nadopt such an instruction-tuned LLM Flan-T5 as the text encoder for\ntext-to-audio (TTA) generation -- a task where the goal is to generate an audio\nfrom its textual description. The prior works on TTA either pre-trained a joint\ntext-audio encoder or used a non-instruction-tuned model, such as, T5.\nConsequently, our latent diffusion model (LDM)-based approach TANGO outperforms\nthe state-of-the-art AudioLDM on most metrics and stays comparable on the rest\non AudioCaps test set, despite training the LDM on a 63 times smaller dataset\nand keeping the text encoder frozen. This improvement might also be attributed\nto the adoption of audio pressure level-based sound mixing for training set\naugmentation, whereas the prior methods take a random mix.", "published": "2023-04-24 07:45:28", "link": "http://arxiv.org/abs/2304.13731v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-regularised Minimum Latency Training for Streaming\n  Transformer-based Speech Recognition", "abstract": "This paper proposes a self-regularised minimum latency training (SR-MLT)\nmethod for streaming Transformer-based automatic speech recognition (ASR)\nsystems. In previous works, latency was optimised by truncating the online\nattention weights based on the hard alignments obtained from conventional ASR\nmodels, without taking into account the potential loss of ASR accuracy. On the\ncontrary, here we present a strategy to obtain the alignments as a part of the\nmodel training without external supervision. The alignments produced by the\nproposed method are dynamically regularised on the training data, such that the\nlatency reduction does not result in the loss of ASR accuracy. SR-MLT is\napplied as a fine-tuning step on the pre-trained Transformer models that are\nbased on either monotonic chunkwise attention (MoChA) or cumulative attention\n(CA) algorithms for online decoding. ASR experiments on the AIShell-1 and\nLibrispeech datasets show that when applied on a decent pre-trained MoChA or CA\nbaseline model, SR-MLT can effectively reduce the latency with the relative\ngains ranging from 11.8% to 39.5%. Furthermore, we also demonstrate that under\ncertain accuracy levels, the models trained with SR-MLT can achieve lower\nlatency when compared to those supervised using external hard alignments.", "published": "2023-04-24 10:38:09", "link": "http://arxiv.org/abs/2304.11985v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Audio-Visual Singing Voice Transcription based on Self-Supervised\n  Learning Models", "abstract": "Singing voice transcription converts recorded singing audio to musical\nnotation. Sound contamination (such as accompaniment) and lack of annotated\ndata make singing voice transcription an extremely difficult task. We take two\napproaches to tackle the above challenges: 1) introducing multimodal learning\nfor singing voice transcription together with a new multimodal singing dataset,\nN20EMv2, enhancing noise robustness by utilizing video information (lip\nmovements to predict the onset/offset of notes), and 2) adapting\nself-supervised learning models from the speech domain to the singing voice\ntranscription task, significantly reducing annotated data requirements while\npreserving pretrained features. We build a self-supervised learning based\naudio-only singing voice transcription system, which not only outperforms\ncurrent state-of-the-art technologies as a strong baseline, but also\ngeneralizes well to out-of-domain singing data. We then develop a\nself-supervised learning based video-only singing voice transcription system\nthat detects note onsets and offsets with an accuracy of about 80\\%. Finally,\nbased on the powerful acoustic and visual representations extracted by the\nabove two systems as well as the feature fusion design, we create an\naudio-visual singing voice transcription system that improves the noise\nrobustness significantly under different acoustic environments compared to the\naudio-only systems.", "published": "2023-04-24 13:25:22", "link": "http://arxiv.org/abs/2304.12082v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pre-Training Strategies Using Contrastive Learning and Playlist\n  Information for Music Classification and Similarity", "abstract": "In this work, we investigate an approach that relies on contrastive learning\nand music metadata as a weak source of supervision to train music\nrepresentation models. Recent studies show that contrastive learning can be\nused with editorial metadata (e.g., artist or album name) to learn audio\nrepresentations that are useful for different classification tasks. In this\npaper, we extend this idea to using playlist data as a source of music\nsimilarity information and investigate three approaches to generate anchor and\npositive track pairs. We evaluate these approaches by fine-tuning the\npre-trained models for music multi-label classification tasks (genre, mood, and\ninstrument tagging) and music similarity. We find that creating anchor and\npositive track pairs by relying on co-occurrences in playlists provides better\nmusic similarity and competitive classification results compared to choosing\ntracks from the same artist as in previous works. Additionally, our best\npre-training approach based on playlists provides superior classification\nperformance for most datasets.", "published": "2023-04-24 16:49:58", "link": "http://arxiv.org/abs/2304.12257v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing underwater acoustic target recognition via adaptive data\n  pruning and smoothness-inducing regularization", "abstract": "Underwater acoustic recognition for ship-radiated signals has high practical\napplication value due to the ability to recognize non-line-of-sight targets.\nHowever, due to the difficulty of data acquisition, the collected signals are\nscarce in quantity and mainly composed of mechanical periodic noise. According\nto the experiments, we observe that the repeatability of periodic signals leads\nto a double-descent phenomenon, which indicates a significant local bias toward\nrepeated samples. To address this issue, we propose a strategy based on\ncross-entropy to prune excessively similar segments in training data.\nFurthermore, to compensate for the reduction of training data, we generate\nnoisy samples and apply smoothness-inducing regularization based on KL\ndivergence to mitigate overfitting. Experiments show that our proposed data\npruning and regularization strategy can bring stable benefits and our framework\nsignificantly outperforms the state-of-the-art in low-resource scenarios.", "published": "2023-04-24 08:30:41", "link": "http://arxiv.org/abs/2304.11907v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Zero-shot text-to-speech synthesis conditioned using self-supervised\n  speech representation model", "abstract": "This paper proposes a zero-shot text-to-speech (TTS) conditioned by a\nself-supervised speech-representation model acquired through self-supervised\nlearning (SSL). Conventional methods with embedding vectors from x-vector or\nglobal style tokens still have a gap in reproducing the speaker characteristics\nof unseen speakers. A novel point of the proposed method is the direct use of\nthe SSL model to obtain embedding vectors from speech representations trained\nwith a large amount of data. We also introduce the separate conditioning of\nacoustic features and a phoneme duration predictor to obtain the disentangled\nembeddings between rhythm-based speaker characteristics and\nacoustic-feature-based ones. The disentangled embeddings will enable us to\nachieve better reproduction performance for unseen speakers and rhythm transfer\nconditioned by different speeches. Objective and subjective evaluations showed\nthat the proposed method can synthesize speech with improved similarity and\nachieve speech-rhythm transfer.", "published": "2023-04-24 10:15:58", "link": "http://arxiv.org/abs/2304.11976v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-channel Speech Separation Using Spatially Selective Deep\n  Non-linear Filters", "abstract": "In a multi-channel separation task with multiple speakers, we aim to recover\nall individual speech signals from the mixture. In contrast to single-channel\napproaches, which rely on the different spectro-temporal characteristics of the\nspeech signals, multi-channel approaches should additionally utilize the\ndifferent spatial locations of the sources for a more powerful separation\nespecially when the number of sources increases. To enhance the spatial\nprocessing in a multi-channel source separation scenario, in this work, we\npropose a deep neural network (DNN) based spatially selective filter (SSF) that\ncan be spatially steered to extract the speaker of interest by initializing a\nrecurrent neural network layer with the target direction. We compare the\nproposed SSF with a common end-to-end direct separation (DS) approach trained\nusing utterance-wise permutation invariant training (PIT), which only\nimplicitly learns to perform spatial filtering. We show that the SSF has a\nclear advantage over a DS approach with the same underlying network\narchitecture when there are more than two speakers in the mixture, which can be\nattributed to a better use of the spatial information. Furthermore, we find\nthat the SSF generalizes much better to additional noise sources that were not\nseen during training and to scenarios with speakers positioned at a similar\nangle.", "published": "2023-04-24 11:44:00", "link": "http://arxiv.org/abs/2304.12023v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The ACCompanion: Combining Reactivity, Robustness, and Musical\n  Expressivity in an Automatic Piano Accompanist", "abstract": "This paper introduces the ACCompanion, an expressive accompaniment system.\nSimilarly to a musician who accompanies a soloist playing a given musical\npiece, our system can produce a human-like rendition of the accompaniment part\nthat follows the soloist's choices in terms of tempo, dynamics, and\narticulation. The ACCompanion works in the symbolic domain, i.e., it needs a\nmusical instrument capable of producing and playing MIDI data, with explicitly\nencoded onset, offset, and pitch for each played note. We describe the\ncomponents that go into such a system, from real-time score following and\nprediction to expressive performance generation and online adaptation to the\nexpressive choices of the human player. Based on our experience with repeated\nlive demonstrations in front of various audiences, we offer an analysis of the\nchallenges of combining these components into a system that is highly reactive\nand precise, while still a reliable musical partner, robust to possible\nperformance errors and responsive to expressive variations.", "published": "2023-04-24 05:19:52", "link": "http://arxiv.org/abs/2304.12939v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Generative NMF for Single Channel Source Separation", "abstract": "The idea of adversarial learning of regularization functionals has recently\nbeen introduced in the wider context of inverse problems. The intuition behind\nthis method is the realization that it is not only necessary to learn the basic\nfeatures that make up a class of signals one wants to represent, but also, or\neven more so, which features to avoid in the representation. In this paper, we\nwill apply this approach to the problem of source separation by means of\nnon-negative matrix factorization (NMF) and present a new method for the\nadversarial training of NMF bases. We show in numerical experiments, both for\nimage and audio separation, that this leads to a clear improvement of the\nreconstructed signals, in particular in the case where little or no strong\nsupervision data is available.", "published": "2023-04-24 09:26:43", "link": "http://arxiv.org/abs/2305.01758v1", "categories": ["eess.AS", "cs.LG", "cs.NA", "math.NA", "stat.ML", "94A12 (primary), 47A52, 94A08 (secondary)"], "primary_category": "eess.AS"}
