{"title": "NMT-based Cross-lingual Document Embeddings", "abstract": "This paper investigates a cross-lingual document embedding method that\nimproves the current Neural machine Translation framework based Document Vector\n(NTDV or simply NV). NV is developed with a self-attention mechanism under the\nneural machine translation (NMT) framework. In NV, each pair of parallel\ndocuments in different languages are projected to the same shared layer in the\nmodel. However, the pair of NV embeddings are not guaranteed to be similar.\nThis paper further adds a distance constraint to the training objective\nfunction of NV so that the two embeddings of a parallel document are required\nto be as close as possible. The new method will be called constrained NV (cNV).\nIn a cross-lingual document classification task, the new cNV performs as well\nas NV and outperforms other published studies that require forward-pass\ndecoding. Compared with the previous NV, cNV does not need a translator during\ntesting, and so the method is lighter and more flexible.", "published": "2018-07-29 13:49:00", "link": "http://arxiv.org/abs/1807.11057v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Gated Recurrent Units for Medical Relation Classification", "abstract": "Convolutional neural network (CNN) and recurrent neural network (RNN) models\nhave become the mainstream methods for relation classification. We propose a\nunified architecture, which exploits the advantages of CNN and RNN\nsimultaneously, to identify medical relations in clinical records, with only\nword embedding features. Our model learns phrase-level features through a CNN\nlayer, and these feature representations are directly fed into a bidirectional\ngated recurrent unit (GRU) layer to capture long-term feature dependencies. We\nevaluate our model on two clinical datasets, and experiments demonstrate that\nour model performs significantly better than previous single-model methods on\nboth datasets.", "published": "2018-07-29 16:43:06", "link": "http://arxiv.org/abs/1807.11082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Opinion Spam Recognition Method for Online Reviews using Ontological\n  Features", "abstract": "Nowadays, there are a lot of people using social media opinions to make their\ndecision on buying products or services. Opinion spam detection is a hard\nproblem because fake reviews can be made by organizations as well as\nindividuals for different purposes. They write fake reviews to mislead readers\nor automated detection system by promoting or demoting target products to\npromote them or to damage their reputations. In this paper, we pro-pose a new\napproach using knowledge-based Ontology to detect opinion spam with high\naccuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,\nOntology.", "published": "2018-07-29 09:05:21", "link": "http://arxiv.org/abs/1807.11024v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Microsoft Dialogue Challenge: Building End-to-End Task-Completion\n  Dialogue Systems", "abstract": "This proposal introduces a Dialogue Challenge for building end-to-end\ntask-completion dialogue systems, with the goal of encouraging the dialogue\nresearch community to collaborate and benchmark on standard datasets and\nunified experimental environment. In this special session, we will release\nhuman-annotated conversational data in three domains (movie-ticket booking,\nrestaurant reservation, and taxi booking), as well as an experiment platform\nwith built-in simulators in each domain, for training and evaluation purposes.\nThe final submitted systems will be evaluated both in simulated setting and by\nhuman judges.", "published": "2018-07-29 23:51:08", "link": "http://arxiv.org/abs/1807.11125v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Speech Identification from Vocal Tract Shape Dynamics\n  in Real-time MRI", "abstract": "Vocal tract configurations play a vital role in generating distinguishable\nspeech sounds, by modulating the airflow and creating different resonant\ncavities in speech production. They contain abundant information that can be\nutilized to better understand the underlying speech production mechanism. As a\nstep towards automatic mapping of vocal tract shape geometry to acoustics, this\npaper employs effective video action recognition techniques, like Long-term\nRecurrent Convolutional Networks (LRCN) models, to identify different\nvowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract.\nSuch a model typically combines a CNN based deep hierarchical visual feature\nextractor with Recurrent Networks, that ideally makes the network\nspatio-temporally deep enough to learn the sequential dynamics of a short video\nclip for video classification tasks. We use a database consisting of 2D\nreal-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The\ncomparative performances of this class of algorithms under various parameter\nsettings and for various classification tasks are discussed. Interestingly, the\nresults show a marked difference in the model performance in the context of\nspeech classification with respect to generic sequence or video classification\ntasks.", "published": "2018-07-29 17:36:08", "link": "http://arxiv.org/abs/1807.11089v1", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards End-to-End Acoustic Localization using Deep Learning: from Audio\n  Signal to Source Position Coordinates", "abstract": "This paper presents a novel approach for indoor acoustic source localization\nusing microphone arrays and based on a Convolutional Neural Network (CNN). The\nproposed solution is, to the best of our knowledge, the first published work in\nwhich the CNN is designed to directly estimate the three dimensional position\nof an acoustic source, using the raw audio signal as the input information\navoiding the use of hand crafted audio features. Given the limited amount of\navailable localization data, we propose in this paper a training strategy based\non two steps. We first train our network using semi-synthetic data, generated\nfrom close talk speech recordings, and where we simulate the time delays and\ndistortion suffered in the signal that propagates from the source to the array\nof microphones. We then fine tune this network using a small amount of real\ndata. Our experimental results show that this strategy is able to produce\nnetworks that significantly improve existing localization methods based on\n\\textit{SRP-PHAT} strategies. In addition, our experiments show that our CNN\nmethod exhibits better resistance against varying gender of the speaker and\ndifferent window sizes compared with the other methods.", "published": "2018-07-29 18:22:38", "link": "http://arxiv.org/abs/1807.11094v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Recognition from Raw Waveform with SincNet", "abstract": "Deep learning is progressively gaining popularity as a viable alternative to\ni-vectors for speaker recognition. Promising results have been recently\nobtained with Convolutional Neural Networks (CNNs) when fed by raw speech\nsamples directly. Rather than employing standard hand-crafted features, the\nlatter CNNs learn low-level speech representations from waveforms, potentially\nallowing the network to better capture important narrow-band speaker\ncharacteristics such as pitch and formants. Proper design of the neural network\nis crucial to achieve this goal. This paper proposes a novel CNN architecture,\ncalled SincNet, that encourages the first convolutional layer to discover more\nmeaningful filters. SincNet is based on parametrized sinc functions, which\nimplement band-pass filters. In contrast to standard CNNs, that learn all\nelements of each filter, only low and high cutoff frequencies are directly\nlearned from data with the proposed method. This offers a very compact and\nefficient way to derive a customized filter bank specifically tuned for the\ndesired application. Our experiments, conducted on both speaker identification\nand speaker verification tasks, show that the proposed architecture converges\nfaster and performs better than a standard CNN on raw waveforms.", "published": "2018-07-29 16:27:19", "link": "http://arxiv.org/abs/1808.00158v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
