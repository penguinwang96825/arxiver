{"title": "Paraphrase Detection on Noisy Subtitles in Six Languages", "abstract": "We perform automatic paraphrase detection on subtitle data from the\nOpusparcus corpus comprising six European languages: German, English, Finnish,\nFrench, Russian, and Swedish. We train two types of supervised sentence\nembedding models: a word-averaging (WA) model and a gated recurrent averaging\nnetwork (GRAN) model. We find out that GRAN outperforms WA and is more robust\nto noisy training data. Better results are obtained with more and noisier data\nthan less and cleaner data. Additionally, we experiment on other datasets,\nwithout reaching the same level of performance, because of domain mismatch\nbetween training and test data.", "published": "2018-09-21 08:18:12", "link": "http://arxiv.org/abs/1809.07978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Convolutional Neural Networks for Text Classification", "abstract": "We present an analysis into the inner workings of Convolutional Neural\nNetworks (CNNs) for processing text. CNNs used for computer vision can be\ninterpreted by projecting filters into image space, but for discrete sequence\ninputs CNNs remain a mystery. We aim to understand the method by which the\nnetworks process and classify text. We examine common hypotheses to this\nproblem: that filters, accompanied by global max-pooling, serve as ngram\ndetectors. We show that filters may capture several different semantic classes\nof ngrams by using different activation patterns, and that global max-pooling\ninduces behavior which separates important ngrams from the rest. Finally, we\nshow practical use cases derived from our findings in the form of model\ninterpretability (explaining a trained model by deriving a concrete identity\nfor each filter, bridging the gap between visualization tools in vision tasks\nand NLP) and prediction interpretability (explaining predictions). Code\nimplementation is available online at\ngithub.com/sayaendo/interpreting-cnn-for-text.", "published": "2018-09-21 11:03:48", "link": "http://arxiv.org/abs/1809.08037v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting the Usefulness of Amazon Reviews Using Off-The-Shelf\n  Argumentation Mining", "abstract": "Internet users generate content at unprecedented rates. Building intelligent\nsystems capable of discriminating useful content within this ocean of\ninformation is thus becoming a urgent need. In this paper, we aim to predict\nthe usefulness of Amazon reviews, and to do this we exploit features coming\nfrom an off-the-shelf argumentation mining system. We argue that the usefulness\nof a review, in fact, is strictly related to its argumentative content, whereas\nthe use of an already trained system avoids the costly need of relabeling a\nnovel dataset. Results obtained on a large publicly available corpus support\nthis hypothesis.", "published": "2018-09-21 14:31:57", "link": "http://arxiv.org/abs/1809.08145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automated Factchecking: Developing an Annotation Schema and\n  Benchmark for Consistent Automated Claim Detection", "abstract": "In an effort to assist factcheckers in the process of factchecking, we tackle\nthe claim detection task, one of the necessary stages prior to determining the\nveracity of a claim. It consists of identifying the set of sentences, out of a\nlong text, deemed capable of being factchecked. This paper is a collaborative\nwork between Full Fact, an independent factchecking charity, and academic\npartners. Leveraging the expertise of professional factcheckers, we develop an\nannotation schema and a benchmark for automated claim detection that is more\nconsistent across time, topics and annotators than previous approaches. Our\nannotation schema has been used to crowdsource the annotation of a dataset with\nsentences from UK political TV shows. We introduce an approach based on\nuniversal sentence representations to perform the classification, achieving an\nF1 score of 0.83, with over 5% relative improvement over the state-of-the-art\nmethods ClaimBuster and ClaimRank. The system was deployed in production and\nreceived positive user feedback.", "published": "2018-09-21 16:24:37", "link": "http://arxiv.org/abs/1809.08193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Exploiting Background Knowledge for Building Conversation\n  Systems", "abstract": "Existing dialog datasets contain a sequence of utterances and responses\nwithout any explicit background knowledge associated with them. This has\nresulted in the development of models which treat conversation as a\nsequence-to-sequence generation task i.e, given a sequence of utterances\ngenerate the response sequence). This is not only an overly simplistic view of\nconversation but it is also emphatically different from the way humans converse\nby heavily relying on their background knowledge about the topic (as opposed to\nsimply relying on the previous sequence of utterances). For example, it is\ncommon for humans to (involuntarily) produce utterances which are copied or\nsuitably modified from background articles they have read about the topic. To\nfacilitate the development of such natural conversation models which mimic the\nhuman process of conversing, we create a new dataset containing movie chats\nwherein each response is explicitly generated by copying and/or modifying\nsentences from unstructured background knowledge such as plots, comments and\nreviews about the movie. We establish baseline results on this dataset (90K\nutterances from 9K conversations) using three different models: (i) pure\ngeneration based models which ignore the background knowledge (ii) generation\nbased models which learn to copy information from the background knowledge when\nrequired and (iii) span prediction based models which predict the appropriate\nresponse span in the background knowledge.", "published": "2018-09-21 16:50:17", "link": "http://arxiv.org/abs/1809.08205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Approaches to Conversational AI", "abstract": "The present paper surveys neural approaches to conversational AI that have\nbeen developed in the last few years. We group conversational systems into\nthree categories: (1) question answering agents, (2) task-oriented dialogue\nagents, and (3) chatbots. For each category, we present a review of\nstate-of-the-art neural approaches, draw the connection between them and\ntraditional approaches, and discuss the progress that has been made and\nchallenges still being faced, using specific systems and models as case\nstudies.", "published": "2018-09-21 18:42:24", "link": "http://arxiv.org/abs/1809.08267v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do you correct run-on sentences it's not as easy as it seems", "abstract": "Run-on sentences are common grammatical mistakes but little research has\ntackled this problem to date. This work introduces two machine learning models\nto correct run-on sentences that outperform leading methods for related tasks,\npunctuation restoration and whole-sentence grammatical error correction. Due to\nthe limited annotated data for this error, we experiment with artificially\ngenerating training data from clean newswire text. Our findings suggest\nartificial training data is viable for this task. We discuss implications for\ncorrecting run-ons and other types of mistakes that have low coverage in\nerror-annotated corpora.", "published": "2018-09-21 20:13:41", "link": "http://arxiv.org/abs/1809.08298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CollaboNet: collaboration of deep neural networks for biomedical named\n  entity recognition", "abstract": "Background: Finding biomedical named entities is one of the most essential\ntasks in biomedical text mining. Recently, deep learning-based approaches have\nbeen applied to biomedical named entity recognition (BioNER) and showed\npromising results. However, as deep learning approaches need an abundant amount\nof training data, a lack of data can hinder performance. BioNER datasets are\nscarce resources and each dataset covers only a small subset of entity types.\nFurthermore, many bio entities are polysemous, which is one of the major\nobstacles in named entity recognition. Results: To address the lack of data and\nthe entity type misclassification problem, we propose CollaboNet which utilizes\na combination of multiple NER models. In CollaboNet, models trained on a\ndifferent dataset are connected to each other so that a target model obtains\ninformation from other collaborator models to reduce false positives. Every\nmodel is an expert on their target entity type and takes turns serving as a\ntarget and a collaborator model during training time. The experimental results\nshow that CollaboNet can be used to greatly reduce the number of false\npositives and misclassified entities including polysemous words. CollaboNet\nachieved state-of-the-art performance in terms of precision, recall and F1\nscore. Conclusions: We demonstrated the benefits of combining multiple models\nfor BioNER. Our model has successfully reduced the number of misclassified\nentities and improved the performance by leveraging multiple datasets annotated\nfor different entity types. Given the state-of-the-art performance of our\nmodel, we believe that CollaboNet can improve the accuracy of downstream\nbiomedical text mining applications such as bio-entity relation extraction.", "published": "2018-09-21 05:48:54", "link": "http://arxiv.org/abs/1809.07950v2", "categories": ["cs.CL", "cs.LG", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Lexical Bias In Essay Level Prediction", "abstract": "Automatically predicting the level of non-native English speakers given their\nwritten essays is an interesting machine learning problem. In this work I\npresent the system \"balikasg\" that achieved the state-of-the-art performance in\nthe CAp 2018 data science challenge among 14 systems. I detail the feature\nextraction, feature engineering and model selection steps and I evaluate how\nthese decisions impact the system's performance. The paper concludes with\nremarks for future work.", "published": "2018-09-21 12:04:44", "link": "http://arxiv.org/abs/1809.08935v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention-based Encoder-Decoder Networks for Spelling and Grammatical\n  Error Correction", "abstract": "Automatic spelling and grammatical correction systems are one of the most\nwidely used tools within natural language applications. In this thesis, we\nassume the task of error correction as a type of monolingual machine\ntranslation where the source sentence is potentially erroneous and the target\nsentence should be the corrected form of the input. Our main focus in this\nproject is building neural network models for the task of error correction. In\nparticular, we investigate sequence-to-sequence and attention-based models\nwhich have recently shown a higher performance than the state-of-the-art of\nmany language processing problems. We demonstrate that neural machine\ntranslation models can be successfully applied to the task of error correction.\n  While the experiments of this research are performed on an Arabic corpus, our\nmethods in this thesis can be easily applied to any language.", "published": "2018-09-21 23:47:42", "link": "http://arxiv.org/abs/1810.00660v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Dual Attention Memory for Video Story Question Answering", "abstract": "We propose a video story question-answering (QA) architecture, Multimodal\nDual Attention Memory (MDAM). The key idea is to use a dual attention mechanism\nwith late fusion. MDAM uses self-attention to learn the latent concepts in\nscene frames and captions. Given a question, MDAM uses the second attention\nover these latent concepts. Multimodal fusion is performed after the dual\nattention processes (late fusion). Using this processing pipeline, MDAM learns\nto infer a high-level vision-language joint representation from an abstraction\nof the full video content. We evaluate MDAM on PororoQA and MovieQA datasets\nwhich have large-scale QA annotations on cartoon videos and movies,\nrespectively. For both datasets, MDAM achieves new state-of-the-art results\nwith significant margins compared to the runner-up models. We confirm the best\nperformance of the dual attention mechanism combined with late fusion by\nablation studies. We also perform qualitative analysis by visualizing the\ninference mechanisms of MDAM.", "published": "2018-09-21 09:19:12", "link": "http://arxiv.org/abs/1809.07999v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Opacity, Obscurity, and the Geometry of Question-Asking", "abstract": "Asking questions is a pervasive human activity, but little is understood\nabout what makes them difficult to answer. An analysis of a pair of large\ndatabases, of New York Times crosswords and questions from the quiz-show\nJeopardy, establishes two orthogonal dimensions of question difficulty:\nobscurity (the rarity of the answer) and opacity (the indirectness of question\ncues, operationalized with word2vec). The importance of opacity, and the role\nof synergistic information in resolving it, suggests that accounts of\ndifficulty in terms of prior expectations captures only a part of the\nquestion-asking process. A further regression analysis shows the presence of\nadditional dimensions to question-asking: question complexity, the answer's\nlocal network density, cue intersection, and the presence of signal words. Our\nwork shows how question-askers can help their interlocutors by using contextual\ncues, or, conversely, how a particular kind of unfamiliarity with the domain in\nquestion can make it harder for individuals to learn from others. Taken\ntogether, these results suggest how Bayesian models of question difficulty can\nbe supplemented by process models and accounts of the heuristics individuals\nuse to navigate conceptual spaces.", "published": "2018-09-21 20:01:30", "link": "http://arxiv.org/abs/1809.08291v1", "categories": ["cs.CL", "cs.AI", "physics.soc-ph", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Adversarial Training in Affective Computing and Sentiment Analysis:\n  Recent Advances and Perspectives", "abstract": "Over the past few years, adversarial training has become an extremely active\nresearch topic and has been successfully applied to various Artificial\nIntelligence (AI) domains. As a potentially crucial technique for the\ndevelopment of the next generation of emotional AI systems, we herein provide a\ncomprehensive overview of the application of adversarial training to affective\ncomputing and sentiment analysis. Various representative adversarial training\nalgorithms are explained and discussed accordingly, aimed at tackling diverse\nchallenges associated with emotional AI systems. Further, we highlight a range\nof potential future research directions. We expect that this overview will help\nfacilitate the development of adversarial training for affective computing and\nsentiment analysis in both the academic and industrial communities.", "published": "2018-09-21 08:27:01", "link": "http://arxiv.org/abs/1809.08927v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Perfect match: Improved cross-modal embeddings for audio-visual\n  synchronisation", "abstract": "This paper proposes a new strategy for learning powerful cross-modal\nembeddings for audio-to-video synchronization. Here, we set up the problem as\none of cross-modal retrieval, where the objective is to find the most relevant\naudio segment given a short video clip. The method builds on the recent\nadvances in learning representations from cross-modal self-supervision.\n  The main contributions of this paper are as follows: (1) we propose a new\nlearning strategy where the embeddings are learnt via a multi-way matching\nproblem, as opposed to a binary classification (matching or non-matching)\nproblem as proposed by recent papers; (2) we demonstrate that performance of\nthis method far exceeds the existing baselines on the synchronization task; (3)\nwe use the learnt embeddings for visual speech recognition in self-supervision,\nand show that the performance matches the representations learnt end-to-end in\na fully-supervised manner.", "published": "2018-09-21 09:24:37", "link": "http://arxiv.org/abs/1809.08001v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
