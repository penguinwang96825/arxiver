{"title": "An Investigation of Supervised Learning Methods for Authorship\n  Attribution in Short Hinglish Texts using Char & Word N-grams", "abstract": "The writing style of a person can be affirmed as a unique identity indicator;\nthe words used, and the structuring of the sentences are clear measures which\ncan identify the author of a specific work. Stylometry and its subset -\nAuthorship Attribution, have a long history beginning from the 19th century,\nand we can still find their use in modern times. The emergence of the Internet\nhas shifted the application of attribution studies towards non-standard texts\nthat are comparatively shorter to and different from the long texts on which\nmost research has been done. The aim of this paper focuses on the study of\nshort online texts, retrieved from messaging application called WhatsApp and\nstudying the distinctive features of a macaronic language (Hinglish), using\nsupervised learning methods and then comparing the models. Various features\nsuch as word n-gram and character n-gram are compared via methods viz., Naive\nBayes Classifier, Support Vector Machine, Conditional Tree, and Random Forest,\nto find the best discriminator for such corpora. Our results showed that SVM\nattained a test accuracy of up to 95.079% while similarly, Naive Bayes attained\nan accuracy of up to 94.455% for the dataset. Conditional Tree & Random Forest\nfailed to perform as well as expected. We also found that word unigram and\ncharacter 3-grams features were more likely to distinguish authors accurately\nthan other features.", "published": "2018-12-26 10:54:52", "link": "http://arxiv.org/abs/1812.10281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantized-Dialog Language Model for Goal-Oriented Conversational Systems", "abstract": "We propose a novel methodology to address dialog learning in the context of\ngoal-oriented conversational systems. The key idea is to quantize the dialog\nspace into clusters and create a language model across the clusters, thus\nallowing for an accurate choice of the next utterance in the conversation. The\nlanguage model relies on n-grams associated with clusters of utterances. This\nquantized-dialog language model methodology has been applied to the end-to-end\ngoal-oriented track of the latest Dialog System Technology Challenges (DSTC6).\nThe objective is to find the correct system utterance from a pool of candidates\nin order to complete a dialog between a user and an automated\nrestaurant-reservation system. Our results show that the technique proposed in\nthis paper achieves high accuracy regarding selection of the correct candidate\nutterance, and outperforms other state-of-the-art approaches based on neural\nnetworks.", "published": "2018-12-26 16:09:38", "link": "http://arxiv.org/abs/1812.10356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Refine Source Representations for Neural Machine Translation", "abstract": "Neural machine translation (NMT) models generally adopt an encoder-decoder\narchitecture for modeling the entire translation process. The encoder\nsummarizes the representation of input sentence from scratch, which is\npotentially a problem if the sentence is ambiguous. When translating a text,\nhumans often create an initial understanding of the source sentence and then\nincrementally refine it along the translation on the target side. Starting from\nthis intuition, we propose a novel encoder-refiner-decoder framework, which\ndynamically refines the source representations based on the generated\ntarget-side information at each decoding step. Since the refining operations\nare time-consuming, we propose a strategy, leveraging the power of\nreinforcement learning models, to decide when to refine at specific decoding\nsteps. Experimental results on both Chinese-English and English-German\ntranslation tasks show that the proposed approach significantly and\nconsistently improves translation performance over the standard encoder-decoder\nframework. Furthermore, when refining strategy is applied, results still show\nreasonable improvement over the baseline without much decrease in decoding\nspeed.", "published": "2018-12-26 05:17:03", "link": "http://arxiv.org/abs/1812.10230v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Investigation of Few-Shot Learning in Spoken Term Classification", "abstract": "In this paper, we investigate the feasibility of applying few-shot learning\nalgorithms to a speech task. We formulate a user-defined scenario of spoken\nterm classification as a few-shot learning problem. In most few-shot learning\nstudies, it is assumed that all the N classes are new in a N-way problem. We\nsuggest that this assumption can be relaxed and define a N+M-way problem where\nN and M are the number of new classes and fixed classes respectively. We\npropose a modification to the Model-Agnostic Meta-Learning (MAML) algorithm to\nsolve the problem. Experiments on the Google Speech Commands dataset show that\nour approach outperforms the conventional supervised learning approach and the\noriginal MAML.", "published": "2018-12-26 05:43:23", "link": "http://arxiv.org/abs/1812.10233v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction\n  Corpus", "abstract": "In the past decade, the DBpedia community has put significant amount of\neffort on developing technical infrastructure and methods for efficient\nextraction of structured information from Wikipedia. These efforts have been\nprimarily focused on harvesting, refinement and publishing semi-structured\ninformation found in Wikipedia articles, such as information from infoboxes,\ncategorization information, images, wikilinks and citations. Nevertheless,\nstill vast amount of valuable information is contained in the unstructured\nWikipedia article texts. In this paper, we present DBpedia NIF - a large-scale\nand multilingual knowledge extraction corpus. The aim of the dataset is\ntwo-fold: to dramatically broaden and deepen the amount of structured\ninformation in DBpedia, and to provide large-scale and multilingual language\nresource for development of various NLP and IR task. The dataset provides the\ncontent of all articles for 128 Wikipedia languages. We describe the dataset\ncreation process and the NLP Interchange Format (NIF) used to model the\ncontent, links and the structure the information of the Wikipedia articles. The\ndataset has been further enriched with about 25% more links and selected\npartitions published as Linked Data. Finally, we describe the maintenance and\nsustainability plans, and selected use cases of the dataset from the TextExt\nknowledge extraction challenge.", "published": "2018-12-26 13:50:50", "link": "http://arxiv.org/abs/1812.10315v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A New Concept of Deep Reinforcement Learning based Augmented General\n  Sequence Tagging System", "abstract": "In this paper, a new deep reinforcement learning based augmented general\nsequence tagging system is proposed. The new system contains two parts: a deep\nneural network (DNN) based sequence tagging model and a deep reinforcement\nlearning (DRL) based augmented tagger. The augmented tagger helps improve\nsystem performance by modeling the data with minority tags. The new system is\nevaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-2003\nbenchmark datasets, to demonstrate the new system's outstanding performance on\ngeneral tagging tasks. Evaluated by F1 scores, it shows that the new system\noutperforms the current state-of-the-art model on ATIS dataset by 1.9% and that\non CoNLL-2003 dataset by 1.4%.", "published": "2018-12-26 05:54:34", "link": "http://arxiv.org/abs/1812.10234v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection\n  and Slot Filling", "abstract": "Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding(SLU) system. Multiple deep learning based models have\ndemonstrated good results on these tasks . The most effective algorithms are\nbased on the structures of sequence to sequence models (or \"encoder-decoder\"\nmodels), and generate the intents and semantic tags either using separate\nmodels or a joint model. Most of the previous studies, however, either treat\nthe intent detection and slot filling as two separate parallel tasks, or use a\nsequence to sequence model to generate both semantic tags and intent. Most of\nthese approaches use one (joint) NN based model (including encoder-decoder\nstructure) to model two tasks, hence may not fully take advantage of the\ncross-impact between them. In this paper, new Bi-model based RNN semantic frame\nparsing network structures are designed to perform the intent detection and\nslot filling tasks jointly, by considering their cross-impact to each other\nusing two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a\ndecoder achieves state-of-the-art result on the benchmark ATIS data, with about\n0.5$\\%$ intent accuracy improvement and 0.9 $\\%$ slot filling improvement.", "published": "2018-12-26 05:55:42", "link": "http://arxiv.org/abs/1812.10235v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual\n  Transfer and Beyond", "abstract": "We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER", "published": "2018-12-26 18:58:39", "link": "http://arxiv.org/abs/1812.10464v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multiversion Programming Inspired Approach to Detecting Audio\n  Adversarial Examples", "abstract": "Adversarial examples (AEs) are crafted by adding human-imperceptible\nperturbations to inputs such that a machine-learning based classifier\nincorrectly labels them. They have become a severe threat to the\ntrustworthiness of machine learning. While AEs in the image domain have been\nwell studied, audio AEs are less investigated. Recently, multiple techniques\nare proposed to generate audio AEs, which makes countermeasures against them an\nurgent task. Our experiments show that, given an AE, the transcription results\nby different Automatic Speech Recognition (ASR) systems differ significantly,\nas they use different architectures, parameters, and training datasets.\nInspired by Multiversion Programming, we propose a novel audio AE detection\napproach, which utilizes multiple off-the-shelf ASR systems to determine\nwhether an audio input is an AE. The evaluation shows that the detection\nachieves accuracies over 98.6%.", "published": "2018-12-26 01:46:53", "link": "http://arxiv.org/abs/1812.10199v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The CORAL+ Algorithm for Unsupervised Domain Adaptation of PLDA", "abstract": "State-of-the-art speaker recognition systems comprise an x-vector (or\ni-vector) speaker embedding front-end followed by a probabilistic linear\ndiscriminant analysis (PLDA) backend. The effectiveness of these components\nrelies on the availability of a large collection of labeled training data. In\npractice, it is common that the domains (e.g., language, demographic) in which\nthe system are deployed differs from that we trained the system. To close the\ngap due to the domain mismatch, we propose an unsupervised PLDA adaptation\nalgorithm to learn from a small amount of unlabeled in-domain data. The\nproposed method was inspired by a prior work on feature-based domain adaptation\ntechnique known as the correlation alignment (CORAL). We refer to the\nmodel-based adaptation technique proposed in this paper as CORAL+. The efficacy\nof the proposed technique is experimentally validated on the recent NIST 2016\nand 2018 Speaker Recognition Evaluation (SRE'16, SRE'18) datasets.", "published": "2018-12-26 08:38:57", "link": "http://arxiv.org/abs/1812.10260v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
