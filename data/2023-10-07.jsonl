{"title": "EMO: Earth Mover Distance Optimization for Auto-Regressive Language\n  Modeling", "abstract": "Neural language models are probabilistic models of human text. They are\npredominantly trained using maximum likelihood estimation (MLE), which is\nequivalent to minimizing the forward cross-entropy between the empirical data\ndistribution and the model distribution. However, various degeneration\nphenomena are still widely observed when decoding from the distributions\nlearned by such models. We establish that the forward cross-entropy is\nsuboptimal as a distance metric for aligning human and model distribution due\nto its (1) recall-prioritization (2) negative diversity ignorance and (3)\ntrain-test mismatch. In this paper, we propose Earth Mover Distance\nOptimization (EMO) for auto-regressive language modeling. EMO capitalizes on\nthe inherent properties of earth mover distance to address the aforementioned\nchallenges. Due to the high complexity of direct computation, we further\nintroduce a feasible upper bound for EMO to ease end-to-end training. Upon\nextensive evaluation of language models trained using EMO and MLE. We find that\nEMO demonstrates a consistently better language modeling performance than MLE\nacross domains. Moreover, EMO demonstrates noteworthy enhancements in\ndownstream performance with minimal fine-tuning on merely 25,000 sentences.\nThis highlights the tremendous potential of EMO as a lightweight calibration\nmethod for enhancing large-scale pre-trained language models.", "published": "2023-10-07 05:37:41", "link": "http://arxiv.org/abs/2310.04691v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-lingual Transfer without Parallel Corpus", "abstract": "Recently, although pre-trained language models have achieved great success on\nmultilingual NLP (Natural Language Processing) tasks, the lack of training data\non many tasks in low-resource languages still limits their performance. One\neffective way of solving that problem is to transfer knowledge from\nrich-resource languages to low-resource languages. However, many previous works\non cross-lingual transfer rely heavily on the parallel corpus or translation\nmodels, which are often difficult to obtain. We propose a novel approach to\nconduct zero-shot cross-lingual transfer with a pre-trained model. It consists\nof a Bilingual Task Fitting module that applies task-related bilingual\ninformation alignment; a self-training module generates pseudo soft and hard\nlabels for unlabeled data and utilizes them to conduct self-training. We got\nthe new SOTA on different tasks without any dependencies on the parallel corpus\nor translation models.", "published": "2023-10-07 07:54:22", "link": "http://arxiv.org/abs/2310.04726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning\n  in Large Language Models", "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.", "published": "2023-10-07 08:56:28", "link": "http://arxiv.org/abs/2310.04743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Reliability of Large Language Models by Leveraging\n  Uncertainty-Aware In-Context Learning", "abstract": "In recent years, large-scale language models (LLMs) have gained attention for\ntheir impressive text generation capabilities. However, these models often face\nthe challenge of \"hallucination,\" which undermines their reliability. In this\nstudy, we introduce an uncertainty-aware in-context learning framework to\nempower the model to enhance or reject its output in response to uncertainty.\nHuman-defined methods for estimating uncertainty typically assume that\n\"uncertainty is lower when the model's response is correct compared to when it\nis incorrect.\" However, setting a precise threshold to distinguish correctness\nis challenging. Therefore, we introduce uncertainty information as an\nintermediary variable that implicitly influences the model's behavior. Our\ninnovative uncertainty-aware in-context learning framework involves fine-tuning\nthe LLM using a calibration dataset. Our aim is to improve the model's\nresponses by filtering out answers with high uncertainty while considering the\nmodel's knowledge limitations. We evaluate the model's knowledge by examining\nmultiple responses to the same question for the presence of a correct answer.\nWhen the model lacks relevant knowledge, the response should indicate that the\nquestion cannot be answered. Conversely, when the model has relevant knowledge,\nthe response should provide the correct answer. Extensive experiments confirm\nthe effectiveness of our framework, leading to two key findings. First, the\nlogit output values of the LLM partly reflect inherent uncertainty. Second, our\nmodel autonomously recognizes uncertainty, resulting in improved responses.", "published": "2023-10-07 12:06:53", "link": "http://arxiv.org/abs/2310.04782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following\n  and Model Alignment in New Languages", "abstract": "Recently, the development of open-source large language models (LLMs) has\nadvanced rapidly. Nevertheless, due to data constraints, the capabilities of\nmost open-source LLMs are primarily focused on English. To address this issue,\nwe introduce the concept of $\\textit{chat vector}$ to equip pre-trained\nlanguage models with instruction following and human value alignment via simple\nmodel arithmetic. The chat vector is derived by subtracting the weights of a\npre-trained base model (e.g. LLaMA2) from those of its corresponding chat model\n(e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained\nmodel's weights, we can endow the model with chat capabilities in new languages\nwithout the need for further training. Our empirical studies demonstrate the\nsuperior efficacy of the chat vector from three different aspects: instruction\nfollowing, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase\nthe adaptability of our approach, we extend our experiments to encompass\nvarious languages, base models, and chat vectors. The results underscore the\nchat vector's simplicity, effectiveness, and wide applicability, making it a\ncompelling solution for efficiently enabling conversational capabilities in\npre-trained language models. Our code is available at\nhttps://github.com/aqweteddy/ChatVector.", "published": "2023-10-07 13:34:21", "link": "http://arxiv.org/abs/2310.04799v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameterizing Context: Unleashing the Power of Parameter-Efficient\n  Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing", "abstract": "Continual table semantic parsing aims to train a parser on a sequence of\ntasks, where each task requires the parser to translate natural language into\nSQL based on task-specific tables but only offers limited training examples.\nConventional methods tend to suffer from overfitting with limited supervision,\nas well as catastrophic forgetting due to parameter updates. Despite recent\nadvancements that partially alleviate these issues through semi-supervised data\naugmentation and retention of a few past examples, the performance is still\nlimited by the volume of unsupervised data and stored examples. To overcome\nthese challenges, this paper introduces a novel method integrating\n\\textit{parameter-efficient fine-tuning} (PEFT) and \\textit{in-context tuning}\n(ICT) for training a continual table semantic parser. Initially, we present a\ntask-adaptive PEFT framework capable of fully circumventing catastrophic\nforgetting, which is achieved by freezing the pre-trained model backbone and\nfine-tuning small-scale prompts. Building on this, we propose a teacher-student\nframework-based solution. The teacher addresses the few-shot problem using ICT,\nwhich procures contextual information by demonstrating a few training examples.\nIn turn, the student leverages the proposed PEFT framework to learn from the\nteacher's output distribution, and subsequently compresses and saves the\ncontextual information to the prompts, eliminating the need to store any\ntraining examples. Experimental evaluations on two benchmarks affirm the\nsuperiority of our method over prevalent few-shot and continual learning\nbaselines across various metrics.", "published": "2023-10-07 13:40:41", "link": "http://arxiv.org/abs/2310.04801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Only Pass Primary School Exams in Indonesia: A\n  Comprehensive Test on IndoMMLU", "abstract": "Although large language models (LLMs) are often pre-trained on large-scale\nmultilingual texts, their reasoning abilities and real-world knowledge are\nmainly evaluated based on English datasets. Assessing LLM capabilities beyond\nEnglish is increasingly vital but hindered due to the lack of suitable\ndatasets. In this work, we introduce IndoMMLU, the first multi-task language\nunderstanding benchmark for Indonesian culture and languages, which consists of\nquestions from primary school to university entrance exams in Indonesia. By\nemploying professional teachers, we obtain 14,981 questions across 64 tasks and\neducation levels, with 46% of the questions focusing on assessing proficiency\nin the Indonesian language and knowledge of nine local languages and cultures\nin Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass\nthe Indonesian primary school level, with limited knowledge of local Indonesian\nlanguages and culture. Other smaller models such as BLOOMZ and Falcon perform\nat even lower levels.", "published": "2023-10-07 21:49:38", "link": "http://arxiv.org/abs/2310.04928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based\n  Queries", "abstract": "In scientific research, the ability to effectively retrieve relevant\ndocuments based on complex, multifaceted queries is critical. Existing\nevaluation datasets for this task are limited, primarily due to the high cost\nand effort required to annotate resources that effectively represent complex\nqueries. To address this, we propose a novel task, Scientific DOcument\nRetrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed\nto handle the complex nature of user queries in scientific research. We\ndeveloped a benchmark dataset within the field of computer science, consisting\nof 100 human-authored complex query cases. For each complex query, we assembled\na collection of 100 relevant documents and produced annotated relevance scores\nfor ranking them. Recognizing the significant labor of expert annotation, we\nalso introduce Anno-GPT, a scalable framework for validating the performance of\nLarge Language Models (LLMs) on expert-level dataset annotation tasks. LLM\nannotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,\nwithout compromising quality. Furthermore, due to the multi-tiered structure of\nthese complex queries, the DORIS-MAE dataset can be extended to over 4,000\nsub-query test cases without requiring additional annotation. We evaluated 17\nrecent retrieval methods on DORIS-MAE, observing notable performance drops\ncompared to traditional datasets. This highlights the need for better\napproaches to handle complex, multifaceted queries in scientific research. Our\ndataset and codebase are available at\nhttps://github.com/Real-Doris-Mae/Doris-Mae-Dataset.", "published": "2023-10-07 03:25:06", "link": "http://arxiv.org/abs/2310.04678v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language\n  Models in Financial Datasets", "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the\npotential of GPT-based models for the financial sector is increasingly evident.\nHowever, the integration of these models with financial datasets presents\nchallenges, notably in determining their adeptness and relevance. This paper\nintroduces a distinctive approach anchored in the Instruction Tuning paradigm\nfor open-source large language models, specifically adapted for financial\ncontexts. Through this methodology, we capitalize on the interoperability of\nopen-source models, ensuring a seamless and transparent integration. We begin\nby explaining the Instruction Tuning paradigm, highlighting its effectiveness\nfor immediate integration. The paper presents a benchmarking scheme designed\nfor end-to-end training and testing, employing a cost-effective progression.\nFirstly, we assess basic competencies and fundamental tasks, such as Named\nEntity Recognition (NER) and sentiment analysis to enhance specialization.\nNext, we delve into a comprehensive model, executing multi-task operations by\namalgamating all instructional tunings to examine versatility. Finally, we\nexplore the zero-shot capabilities by earmarking unseen tasks and incorporating\nnovel datasets to understand adaptability in uncharted terrains. Such a\nparadigm fortifies the principles of openness and reproducibility, laying a\nrobust foundation for future investigations in open-source financial large\nlanguage models (FinLLMs).", "published": "2023-10-07 12:52:58", "link": "http://arxiv.org/abs/2310.04793v2", "categories": ["cs.CL", "q-fin.TR"], "primary_category": "cs.CL"}
{"title": "End-to-End Lip Reading in Romanian with Cross-Lingual Domain Adaptation\n  and Lateral Inhibition", "abstract": "Lip reading or visual speech recognition has gained significant attention in\nrecent years, particularly because of hardware development and innovations in\ncomputer vision. While considerable progress has been obtained, most models\nhave only been tested on a few large-scale datasets. This work addresses this\nshortcoming by analyzing several architectures and optimizations on the\nunderrepresented, short-scale Romanian language dataset called Wild LRRo. Most\nnotably, we compare different backend modules, demonstrating the effectiveness\nof adding ample regularization methods. We obtain state-of-the-art results\nusing our proposed method, namely cross-lingual domain adaptation and unlabeled\nvideos from English and German datasets to help the model learn\nlanguage-invariant features. Lastly, we assess the performance of adding a\nlayer inspired by the neural inhibition mechanism.", "published": "2023-10-07 15:36:58", "link": "http://arxiv.org/abs/2310.04858v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GradXKG: A Universal Explain-per-use Temporal Knowledge Graph Explainer", "abstract": "Temporal knowledge graphs (TKGs) have shown promise for reasoning tasks by\nincorporating a temporal dimension to represent how facts evolve over time.\nHowever, existing TKG reasoning (TKGR) models lack explainability due to their\nblack-box nature. Recent work has attempted to address this through customized\nmodel architectures that generate reasoning paths, but these recent approaches\nhave limited generalizability and provide sparse explanatory output. To enable\ninterpretability for most TKGR models, we propose GradXKG, a novel two-stage\ngradient-based approach for explaining Relational Graph Convolution Network\n(RGCN)-based TKGR models. First, a Grad-CAM-inspired RGCN explainer tracks\ngradients to quantify each node's contribution across timesteps in an efficient\n\"explain-per-use\" fashion. Second, an integrated gradients explainer\nconsolidates importance scores for RGCN outputs, extending compatibility across\ndiverse TKGR architectures based on RGCN. Together, the two explainers\nhighlight the most critical nodes at each timestep for a given prediction. Our\nextensive experiments demonstrated that, by leveraging gradient information,\nGradXKG provides insightful explanations grounded in the model's logic in a\ntimely manner for most RGCN-based TKGR models. This helps address the lack of\ninterpretability in existing TKGR models and provides a universal explanation\napproach applicable across various models.", "published": "2023-10-07 18:21:35", "link": "http://arxiv.org/abs/2310.04889v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in\n  Commonsense Question Answering", "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used\nin commonsense question answering, but generating faithful explanations remains\nchallenging. Current methods often overlook path decoding faithfulness, leading\nto divergence between graph encoder outputs and model predictions. We identify\nconfounding effects and LM-KG misalignment as key factors causing spurious\nexplanations. To address this, we introduce the LM-KG Fidelity metric to assess\nKG representation reliability and propose the LM-KG Distribution-aware\nAlignment (\\textit{LKDA}) algorithm to improve explanation faithfulness.\nWithout ground truth, we evaluate KG explanations using the proposed\nFidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA\nshow that LKDA significantly enhances explanation fidelity and model\nperformance, highlighting the need to address distributional misalignment for\nreliable commonsense reasoning.", "published": "2023-10-07 20:29:45", "link": "http://arxiv.org/abs/2310.04910v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern\n  Tuning and Data Strategy", "abstract": "This paper introduces a multifaceted methodology for fine-tuning and\nevaluating large language models (LLMs) for specialized monetization tasks. The\ngoal is to balance general language proficiency with domain-specific skills.\nThe methodology has three main components: 1) Carefully blending in-domain and\ngeneral-purpose data during fine-tuning to achieve an optimal balance between\ngeneral and specialized capabilities; 2) Designing a comprehensive evaluation\nframework with 45 questions tailored to assess performance on functionally\nrelevant dimensions like reliability, consistency, and business impact; 3)\nAnalyzing how model size and continual training influence metrics to guide\nefficient resource allocation during fine-tuning. The paper details the design,\ndata collection, analytical techniques, and results validating the proposed\nframeworks. It aims to provide businesses and researchers with actionable\ninsights on effectively adapting LLMs for specialized contexts. We also intend\nto make public the comprehensive evaluation framework, which includes the 45\ntailored questions and their respective scoring guidelines, to foster\ntransparency and collaboration in adapting LLMs for specialized tasks.", "published": "2023-10-07 23:29:00", "link": "http://arxiv.org/abs/2310.04945v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Evaluation of State-of-the-Art Large Language Models for Sarcasm\n  Detection", "abstract": "Sarcasm, as defined by Merriam-Webster, is the use of words by someone who\nmeans the opposite of what he is trying to say. In the field of sentimental\nanalysis of Natural Language Processing, the ability to correctly identify\nsarcasm is necessary for understanding people's true opinions. Because the use\nof sarcasm is often context-based, previous research has used language\nrepresentation models, such as Support Vector Machine (SVM) and Long Short-Term\nMemory (LSTM), to identify sarcasm with contextual-based information. Recent\ninnovations in NLP have provided more possibilities for detecting sarcasm. In\nBERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding, Jacob Devlin et al. (2018) introduced a new language\nrepresentation model and demonstrated higher precision in interpreting\ncontextualized language. As proposed by Hazarika et al. (2018), CASCADE is a\ncontext-driven model that produces good results for detecting sarcasm. This\nstudy analyzes a Reddit corpus using these two state-of-the-art models and\nevaluates their performance against baseline models to find the ideal approach\nto sarcasm detection.", "published": "2023-10-07 14:45:43", "link": "http://arxiv.org/abs/2312.03706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Anonymization of Swiss Federal Supreme Court Rulings", "abstract": "Releasing court decisions to the public relies on proper anonymization to\nprotect all involved parties, where necessary. The Swiss Federal Supreme Court\nrelies on an existing system that combines different traditional computational\nmethods with human experts. In this work, we enhance the existing anonymization\nsoftware using a large dataset annotated with entities to be anonymized. We\ncompared BERT-based models with models pre-trained on in-domain data. Our\nresults show that using in-domain data to pre-train the models further improves\nthe F1-score by more than 5\\% compared to existing models. Our work\ndemonstrates that combining existing anonymization methods, such as regular\nexpressions, with machine learning can further reduce manual labor and enhance\nautomatic suggestions.", "published": "2023-10-07 00:56:49", "link": "http://arxiv.org/abs/2310.04632v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Do self-supervised speech and language models extract similar\n  representations as human brain?", "abstract": "Speech and language models trained through self-supervised learning (SSL)\ndemonstrate strong alignment with brain activity during speech and language\nperception. However, given their distinct training modalities, it remains\nunclear whether they correlate with the same neural aspects. We directly\naddress this question by evaluating the brain prediction performance of two\nrepresentative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and\nlanguage tasks. Our findings reveal that both models accurately predict speech\nresponses in the auditory cortex, with a significant correlation between their\nbrain predictions. Notably, shared speech contextual information between\nWav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain\nactivity, surpassing static semantic and lower-level acoustic-phonetic\ninformation. These results underscore the convergence of speech contextual\nrepresentations in SSL models and their alignment with the neural network\nunderlying speech perception, offering valuable insights into both SSL models\nand the neural basis of speech and language processing.", "published": "2023-10-07 01:39:56", "link": "http://arxiv.org/abs/2310.04645v2", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Label-free Node Classification on Graphs with Large Language Models\n  (LLMS)", "abstract": "In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.", "published": "2023-10-07 03:14:11", "link": "http://arxiv.org/abs/2310.04668v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates\n  before In-Context Learning", "abstract": "How does scaling the number of parameters in large language models (LLMs)\naffect their core capabilities? We study two natural scaling techniques --\nweight pruning and simply training a smaller or larger model, which we refer to\nas dense scaling -- and their effects on two core capabilities of LLMs: (a)\nrecalling facts presented during pre-training and (b) processing information\npresented in-context during inference. By curating a suite of tasks that help\ndisentangle these two capabilities, we find a striking difference in how these\ntwo abilities evolve due to scaling. Reducing the model size by more than 30\\%\n(via either scaling approach) significantly decreases the ability to recall\nfacts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the\nvarious ways the model can process in-context information, ranging from\nretrieving answers from a long context to learning parameterized functions from\nin-context exemplars. The fact that both dense scaling and weight pruning\nexhibit this behavior suggests that scaling model size has an inherently\ndisparate effect on fact recall and in-context learning.", "published": "2023-10-07 03:36:39", "link": "http://arxiv.org/abs/2310.04680v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Contrastive Learning into a Multitask Transformer Model for\n  Effective Domain Adaptation", "abstract": "While speech emotion recognition (SER) research has made significant\nprogress, achieving generalization across various corpora continues to pose a\nproblem. We propose a novel domain adaptation technique that embodies a\nmultitask framework with SER as the primary task, and contrastive learning and\ninformation maximisation loss as auxiliary tasks, underpinned by fine-tuning of\ntransformers pre-trained on large language models. Empirical results obtained\nthrough experiments on well-established datasets like IEMOCAP and MSP-IMPROV,\nillustrate that our proposed model achieves state-of-the-art performance in SER\nwithin cross-corpus scenarios.", "published": "2023-10-07 06:41:29", "link": "http://arxiv.org/abs/2310.04703v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "Speech Emotion Recognition, Domain adaptation"], "primary_category": "cs.CL"}
{"title": "A New Dataset for End-to-End Sign Language Translation: The Greek\n  Elementary School Dataset", "abstract": "Automatic Sign Language Translation (SLT) is a research avenue of great\nsocietal impact. End-to-End SLT facilitates the interaction of Hard-of-Hearing\n(HoH) with hearing people, thus improving their social life and opportunities\nfor participation in social life. However, research within this frame of\nreference is still in its infancy, and current resources are particularly\nlimited. Existing SLT methods are either of low translation ability or are\ntrained and evaluated on datasets of restricted vocabulary and questionable\nreal-world value. A characteristic example is Phoenix2014T benchmark dataset,\nwhich only covers weather forecasts in German Sign Language. To address this\nshortage of resources, we introduce a newly constructed collection of 29653\nGreek Sign Language video-translation pairs which is based on the official\nsyllabus of Greek Elementary School. Our dataset covers a wide range of\nsubjects. We use this novel dataset to train recent state-of-the-art\nTransformer-based methods widely used in SLT research. Our results demonstrate\nthe potential of our introduced dataset to advance SLT research by offering a\nfavourable balance between usability and real-world value.", "published": "2023-10-07 09:18:33", "link": "http://arxiv.org/abs/2310.04753v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding", "abstract": "Developing text mining approaches to mine aspects from customer reviews has\nbeen well-studied due to its importance in understanding customer needs and\nproduct attributes. In contrast, it remains unclear how to predict the future\nemerging aspects of a new product that currently has little review information.\nThis task, which we named product aspect forecasting, is critical for\nrecommending new products, but also challenging because of the missing reviews.\nHere, we propose ForeSeer, a novel textual mining and product embedding\napproach progressively trained on temporal product graphs for this novel\nproduct aspect forecasting task. ForeSeer transfers reviews from similar\nproducts on a large product graph and exploits these reviews to predict aspects\nthat might emerge in future reviews. A key novelty of our method is to jointly\nprovide review, product, and aspect embeddings that are both time-sensitive and\nless affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer\non a real-world product review system containing 11,536,382 reviews and 11,000\nproducts over 3 years. We observe that ForeSeer substantially outperformed\nexisting approaches with at least 49.1\\% AUPRC improvement under the real\nsetting where aspect associations are not given. ForeSeer further improves\nfuture link prediction on the product graph and the review aspect association\nprediction. Collectively, Foreseer offers a novel framework for review\nforecasting by effectively integrating review text, product network, and\ntemporal information, opening up new avenues for online shopping recommendation\nand e-commerce applications.", "published": "2023-10-07 16:21:04", "link": "http://arxiv.org/abs/2310.04865v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine\n  Conversations", "abstract": "Multimodal Vision-Language Models (VLMs) enable powerful applications from\ntheir fused understanding of images and language, but many perform poorly on UI\ntasks due to the lack of UI training data. In this paper, we adapt a recipe for\ngenerating paired text-image training data for VLMs to the UI domain by\ncombining existing pixel-based methods with a Large Language Model (LLM).\nUnlike prior art, our method requires no human-provided annotations, and it can\nbe applied to any dataset of UI screenshots. We generate a dataset of 335K\nconversational examples paired with UIs that cover Q&A, UI descriptions, and\nplanning, and use it to fine-tune a conversational VLM for UI tasks. To assess\nthe performance of our model, we benchmark it on UI element detection tasks,\nevaluate response quality, and showcase its applicability to multi-step UI\nnavigation and planning.", "published": "2023-10-07 16:32:34", "link": "http://arxiv.org/abs/2310.04869v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Analyzing Zero-Shot Abilities of Vision-Language Models on Video\n  Understanding Tasks", "abstract": "Foundational multimodal models pre-trained on large scale image-text pairs or\nvideo-text pairs or both have shown strong generalization abilities on\ndownstream tasks. However unlike image-text models, pretraining video-text\nmodels is always not feasible due to the difficulty in collecting large-scale\nclean and aligned data, and exponential computational costs involved in the\npretraining phase. Therefore, the pertinent question to ask is: Can image-text\nmodels be adapted to video tasks and is there any benefit to using these models\nover pretraining directly on videos? In this work, we focus on this question by\nproposing a detailed study on the generalization abilities of image-text models\nwhen evaluated on video understanding tasks in a zero-shot setting. We\ninvestigate 9 foundational image-text models on a diverse set of video tasks\nthat include video action recognition (video AR), video retrieval (video RT),\nvideo question answering (video QA), video multiple choice (video MC) and video\ncaptioning (video CP). Our experiments show that image-text models exhibit\nimpressive performance on video AR, video RT and video MC. Furthermore, they\nperform moderately on video captioning and poorly on video QA. These findings\nshed a light on the benefits of adapting foundational image-text models to an\narray of video tasks while avoiding the costly pretraining step.", "published": "2023-10-07 20:57:54", "link": "http://arxiv.org/abs/2310.04914v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback", "abstract": "Extensive work has shown that the performance and interpretability of\ncommonsense reasoning can be improved via knowledge-augmented reasoning\nmethods, where the knowledge that underpins the reasoning process is explicitly\nverbalized and utilized. However, existing implementations, including\n\"chain-of-thought\" and its variants, fall short in capturing the introspective\nnature of knowledge required in commonsense reasoning, and in accounting for\nthe mutual adaptation between the generation and utilization of knowledge. We\npropose a novel method to develop an introspective commonsense reasoner,\nCrystal. To tackle commonsense problems, it first introspects for knowledge\nstatements related to the given question, and subsequently makes an informed\nprediction that is grounded in the previously introspected knowledge. The\nknowledge introspection and knowledge-grounded reasoning modes of the model are\ntuned via reinforcement learning to mutually adapt, where the reward derives\nfrom the feedback given by the model itself. Experiments show that Crystal\nsignificantly outperforms both the standard supervised finetuning and\nchain-of-thought distilled methods, and enhances the transparency of the\ncommonsense reasoning process. Our work ultimately validates the feasibility\nand potential of reinforcing a neural model with self-feedback.", "published": "2023-10-07 21:23:58", "link": "http://arxiv.org/abs/2310.04921v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Data-Centric Financial Large Language Models", "abstract": "Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.", "published": "2023-10-07 04:53:31", "link": "http://arxiv.org/abs/2310.17784v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-to-OS (P2OS): Revolutionizing Operating Systems and\n  Human-Computer Interaction with Integrated AI Generative Models", "abstract": "In this paper, we present a groundbreaking paradigm for human-computer\ninteraction that revolutionizes the traditional notion of an operating system.\n  Within this innovative framework, user requests issued to the machine are\nhandled by an interconnected ecosystem of generative AI models that seamlessly\nintegrate with or even replace traditional software applications. At the core\nof this paradigm shift are large generative models, such as language and\ndiffusion models, which serve as the central interface between users and\ncomputers. This pioneering approach leverages the abilities of advanced\nlanguage models, empowering users to engage in natural language conversations\nwith their computing devices. Users can articulate their intentions, tasks, and\ninquiries directly to the system, eliminating the need for explicit commands or\ncomplex navigation. The language model comprehends and interprets the user's\nprompts, generating and displaying contextual and meaningful responses that\nfacilitate seamless and intuitive interactions.\n  This paradigm shift not only streamlines user interactions but also opens up\nnew possibilities for personalized experiences. Generative models can adapt to\nindividual preferences, learning from user input and continuously improving\ntheir understanding and response generation. Furthermore, it enables enhanced\naccessibility, as users can interact with the system using speech or text,\naccommodating diverse communication preferences.\n  However, this visionary concept raises significant challenges, including\nprivacy, security, trustability, and the ethical use of generative models.\nRobust safeguards must be in place to protect user data and prevent potential\nmisuse or manipulation of the language model.\n  While the full realization of this paradigm is still far from being achieved,\nthis paper serves as a starting point for envisioning this transformative\npotential.", "published": "2023-10-07 17:16:34", "link": "http://arxiv.org/abs/2310.04875v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.HC", "cs.OS"], "primary_category": "cs.LG"}
{"title": "Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech\n  Recognition", "abstract": "The attention-based deep contextual biasing method has been demonstrated to\neffectively improve the recognition performance of end-to-end automatic speech\nrecognition (ASR) systems on given contextual phrases. However, unlike shallow\nfusion methods that directly bias the posterior of the ASR model, deep biasing\nmethods implicitly integrate contextual information, making it challenging to\ncontrol the degree of bias. In this study, we introduce a spike-triggered deep\nbiasing method that simultaneously supports both explicit and implicit bias.\nMoreover, both bias approaches exhibit significant improvements and can be\ncascaded with shallow fusion methods for better results. Furthermore, we\npropose a context sampling enhancement strategy and improve the contextual\nphrase filtering algorithm. Experiments on the public WenetSpeech Mandarin\nbiased-word dataset show a 32.0% relative CER reduction compared to the\nbaseline model, with an impressively 68.6% relative CER reduction on contextual\nphrases.", "published": "2023-10-07 02:31:01", "link": "http://arxiv.org/abs/2310.04657v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Exploration of Task-decoupling on Two-stage Neural Post Filter for\n  Real-time Personalized Acoustic Echo Cancellation", "abstract": "Deep learning based techniques have been popularly adopted in acoustic echo\ncancellation (AEC). Utilization of speaker representation has extended the\nfrontier of AEC, thus attracting many researchers' interest in personalized\nacoustic echo cancellation (PAEC). Meanwhile, task-decoupling strategies are\nwidely adopted in speech enhancement. To further explore the task-decoupling\napproach, we propose to use a two-stage task-decoupling post-filter (TDPF) in\nPAEC. Furthermore, a multi-scale local-global speaker representation is applied\nto improve speaker extraction in PAEC. Experimental results indicate that the\ntask-decoupling model can yield better performance than a single joint network.\nThe optimal approach is to decouple the echo cancellation from noise and\ninterference speech suppression. Based on the task-decoupling sequence, optimal\ntraining strategies for the two-stage model are explored afterwards.", "published": "2023-10-07 07:14:53", "link": "http://arxiv.org/abs/2310.04715v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-objective Progressive Clustering for Semi-supervised Domain\n  Adaptation in Speaker Verification", "abstract": "Utilizing the pseudo-labeling algorithm with large-scale unlabeled data\nbecomes crucial for semi-supervised domain adaptation in speaker verification\ntasks. In this paper, we propose a novel pseudo-labeling method named\nMulti-objective Progressive Clustering (MoPC), specifically designed for\nsemi-supervised domain adaptation. Firstly, we utilize limited labeled data\nfrom the target domain to derive domain-specific descriptors based on multiple\ndistinct objectives, namely within-graph denoising, intra-class denoising and\ninter-class denoising. Then, the Infomap algorithm is adopted for embedding\nclustering, and the descriptors are leveraged to further refine the target\ndomain's pseudo-labels. Moreover, to further improve the quality of pseudo\nlabels, we introduce the subcenter-purification and progressive-merging\nstrategy for label denoising. Our proposed MoPC method achieves 4.95% EER and\nranked the 1$^{st}$ place on the evaluation set of VoxSRC 2023 track 3. We also\nconduct additional experiments on the FFSVC dataset and yield promising\nresults.", "published": "2023-10-07 09:46:07", "link": "http://arxiv.org/abs/2310.04760v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR", "abstract": "Joint modeling of multi-speaker ASR and speaker diarization has recently\nshown promising results in speaker-attributed automatic speech recognition\n(SA-ASR).Although being able to obtain state-of-the-art (SOTA) performance,\nmost of the studies are based on an autoregressive (AR) decoder which generates\ntokens one-by-one and results in a large real-time factor (RTF). To speed up\ninference, we introduce a recently proposed non-autoregressive model Paraformer\nas an acoustic model in the SA-ASR model.Paraformer uses a single-step decoder\nto enable parallel generation, obtaining comparable performance to the SOTA AR\ntransformer models. Besides, we propose a speaker-filling strategy to reduce\nspeaker identification errors and adopt an inter-CTC strategy to enhance the\nencoder's ability in acoustic modeling. Experiments on the AliMeeting corpus\nshow that our model outperforms the cascaded SA-ASR model by a 6.1% relative\nspeaker-dependent character error rate (SD-CER) reduction on the test set.\nMoreover, our model achieves a comparable SD-CER of 34.8% with only 1/10 RTF\ncompared with the SOTA joint AR SA-ASR model.", "published": "2023-10-07 16:07:42", "link": "http://arxiv.org/abs/2310.04863v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech\n  Reconstruction", "abstract": "Reconstructing natural speech from neural activity is vital for enabling\ndirect communication via brain-computer interfaces. Previous efforts have\nexplored the conversion of neural recordings into speech using complex deep\nneural network (DNN) models trained on extensive neural recording data, which\nis resource-intensive under regular clinical constraints. However, achieving\nsatisfactory performance in reconstructing speech from limited-scale neural\nrecordings has been challenging, mainly due to the complexity of speech\nrepresentations and the neural data constraints. To overcome these challenges,\nwe propose a novel transfer learning framework for neural-driven speech\nreconstruction, called Neural2Speech, which consists of two distinct training\nphases. First, a speech autoencoder is pre-trained on readily available speech\ncorpora to decode speech waveforms from the encoded speech representations.\nSecond, a lightweight adaptor is trained on the small-scale neural recordings\nto align the neural activity and the speech representation for decoding.\nRemarkably, our proposed Neural2Speech demonstrates the feasibility of\nneural-driven speech reconstruction even with only 20 minutes of intracranial\ndata, which significantly outperforms existing baseline methods in terms of\nspeech fidelity and intelligibility.", "published": "2023-10-07 01:39:14", "link": "http://arxiv.org/abs/2310.04644v2", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "VoiceExtender: Short-utterance Text-independent Speaker Verification\n  with Guided Diffusion Model", "abstract": "Speaker verification (SV) performance deteriorates as utterances become\nshorter. To this end, we propose a new architecture called VoiceExtender which\nprovides a promising solution for improving SV performance when handling\nshort-duration speech signals. We use two guided diffusion models, the built-in\nand the external speaker embedding (SE) guided diffusion model, both of which\nutilize a diffusion model-based sample generator that leverages SE guidance to\naugment the speech features based on a short utterance. Extensive experimental\nresults on the VoxCeleb1 dataset show that our method outperforms the baseline,\nwith relative improvements in equal error rate (EER) of 46.1%, 35.7%, 10.4%,\nand 5.7% for the short utterance conditions of 0.5, 1.0, 1.5, and 2.0 seconds,\nrespectively.", "published": "2023-10-07 03:42:19", "link": "http://arxiv.org/abs/2310.04681v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Holistic Evaluation of Piano Sound Quality", "abstract": "This paper aims to develop a holistic evaluation method for piano sound\nquality to assist in purchasing decisions. Unlike previous studies that focused\non the effect of piano performance techniques on sound quality, this study\nevaluates the inherent sound quality of different pianos. To derive quality\nevaluation systems, the study uses subjective questionnaires based on a piano\nsound quality dataset. The method selects the optimal piano classification\nmodels by comparing the fine-tuning results of different pre-training models of\nConvolutional Neural Networks (CNN). To improve the interpretability of the\nmodels, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The\nresults reveal that musically trained individuals are better able to\ndistinguish between the sound quality differences of different pianos. The best\nfine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the\npiano classifier. However, the dataset is limited, and the audio is sliced to\nincrease its quantity, resulting in a lack of diversity and balance, so we use\nfocal loss to reduce the impact of data imbalance. To optimize the method, the\ndataset will be expanded, or few-shot learning techniques will be employed in\nfuture research.", "published": "2023-10-07 07:51:34", "link": "http://arxiv.org/abs/2310.04722v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional Diffusion Model for Target Speaker Extraction", "abstract": "We propose DiffSpEx, a generative target speaker extraction method based on\nscore-based generative modelling through stochastic differential equations.\nDiffSpEx deploys a continuous-time stochastic diffusion process in the complex\nshort-time Fourier transform domain, starting from the target speaker source\nand converging to a Gaussian distribution centred on the mixture of sources.\nFor the reverse-time process, a parametrised score function is conditioned on a\ntarget speaker embedding to extract the target speaker from the mixture of\nsources. We utilise ECAPA-TDNN target speaker embeddings and condition the\nscore function alternately on the SDE time embedding and the target speaker\nembedding. The potential of DiffSpEx is demonstrated with the WSJ0-2mix\ndataset, achieving an SI-SDR of 12.9 dB and a NISQA score of 3.56. Moreover, we\nshow that fine-tuning a pre-trained DiffSpEx model to a specific speaker\nfurther improves performance, enabling personalisation in target speaker\nextraction.", "published": "2023-10-07 12:48:54", "link": "http://arxiv.org/abs/2310.04791v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT", "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks, and have shown great\npotential as backbones for audio-and-text large language models (LLMs).\nPrevious mainstream audio-and-text LLMs use discrete audio tokens to represent\nboth input and output audio; however, they suffer from performance degradation\non tasks such as automatic speech recognition, speech-to-text translation, and\nspeech enhancement over models using continuous speech features. In this paper,\nwe propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio\nrecognition, understanding, and generation. LauraGPT is a versatile LLM that\ncan process both audio and text inputs and generate outputs in either\nmodalities. We propose a novel data representation that combines continuous and\ndiscrete features for audio: LauraGPT encodes input audio into continuous\nrepresentations using an audio encoder and generates output audio from discrete\ncodec codes. We propose a one-step codec vocoder to overcome the prediction\nchallenge caused by the multimodal distribution of codec tokens. We fine-tune\nLauraGPT using supervised multi-task learning. Extensive experiments show that\nLauraGPT consistently achieves comparable to superior performance compared to\nstrong baselines on a wide range of audio tasks related to content, semantics,\nparalinguistics, and audio-signal analysis, such as automatic speech\nrecognition, speech-to-text translation, text-to-speech synthesis, speech\nenhancement, automated audio captioning, speech emotion recognition, and spoken\nlanguage understanding.", "published": "2023-10-07 03:17:59", "link": "http://arxiv.org/abs/2310.04673v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FM Tone Transfer with Envelope Learning", "abstract": "Tone Transfer is a novel deep-learning technique for interfacing a sound\nsource with a synthesizer, transforming the timbre of audio excerpts while\nkeeping their musical form content. Due to its good audio quality results and\ncontinuous controllability, it has been recently applied in several audio\nprocessing tools. Nevertheless, it still presents several shortcomings related\nto poor sound diversity, and limited transient and dynamic rendering, which we\nbelieve hinder its possibilities of articulation and phrasing in a real-time\nperformance context.\n  In this work, we present a discussion on current Tone Transfer architectures\nfor the task of controlling synthetic audio with musical instruments and\ndiscuss their challenges in allowing expressive performances. Next, we\nintroduce Envelope Learning, a novel method for designing Tone Transfer\narchitectures that map musical events using a training objective at the\nsynthesis parameter level. Our technique can render note beginnings and endings\naccurately and for a variety of sounds; these are essential steps for improving\nmusical articulation, phrasing, and sound diversity with Tone Transfer.\nFinally, we implement a VST plugin for real-time live use and discuss\npossibilities for improvement.", "published": "2023-10-07 14:03:25", "link": "http://arxiv.org/abs/2310.04811v1", "categories": ["cs.SD", "cs.NE", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.SD"}
