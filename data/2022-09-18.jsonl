{"title": "Improving Topic Segmentation by Injecting Discourse Dependencies", "abstract": "Recent neural supervised topic segmentation models achieve distinguished\nsuperior effectiveness over unsupervised methods, with the availability of\nlarge-scale training corpora sampled from Wikipedia. These models may, however,\nsuffer from limited robustness and transferability caused by exploiting simple\nlinguistic cues for prediction, but overlooking more important inter-sentential\ntopical consistency. To address this issue, we present a discourse-aware neural\ntopic segmentation model with the injection of above-sentence discourse\ndependency structures to encourage the model make topic boundary prediction\nbased more on the topical consistency between sentences. Our empirical study on\nEnglish evaluation datasets shows that injecting above-sentence discourse\nstructures to a neural topic segmenter with our proposed strategy can\nsubstantially improve its performances on intra-domain and out-of-domain data,\nwith little increase of model's complexity.", "published": "2022-09-18 18:22:25", "link": "http://arxiv.org/abs/2209.08626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Global Memory for Document-level Argument Extraction", "abstract": "Extracting informative arguments of events from news articles is a\nchallenging problem in information extraction, which requires a global\ncontextual understanding of each document. While recent work on document-level\nextraction has gone beyond single-sentence and increased the cross-sentence\ninference capability of end-to-end models, they are still restricted by certain\ninput sequence length constraints and usually ignore the global context between\nevents. To tackle this issue, we introduce a new global neural generation-based\nframework for document-level event argument extraction by constructing a\ndocument memory store to record the contextual event information and leveraging\nit to implicitly and explicitly help with decoding of arguments for later\nevents. Empirical results show that our framework outperforms prior methods\nsubstantially and it is more robust to adversarially annotated examples with\nour constrained decoding design. (Our code and resources are available at\nhttps://github.com/xinyadu/memory_docie for research purpose.)", "published": "2022-09-18 23:45:25", "link": "http://arxiv.org/abs/2209.08679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Classification-based Source-specific Term Penalization for Domain\n  Adaptation in Hate-speech Detection", "abstract": "State-of-the-art approaches for hate-speech detection usually exhibit poor\nperformance in out-of-domain settings. This occurs, typically, due to\nclassifiers overemphasizing source-specific information that negatively impacts\nits domain invariance. Prior work has attempted to penalize terms related to\nhate-speech from manually curated lists using feature attribution methods,\nwhich quantify the importance assigned to input terms by the classifier when\nmaking a prediction. We, instead, propose a domain adaptation approach that\nautomatically extracts and penalizes source-specific terms using a domain\nclassifier, which learns to differentiate between domains, and\nfeature-attribution scores for hate-speech classes, yielding consistent\nimprovements in cross-domain evaluation.", "published": "2022-09-18 23:52:22", "link": "http://arxiv.org/abs/2209.08681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Understanding and Generating Dialogue between Characters\n  in Stories", "abstract": "Many classical fairy tales, fiction, and screenplays leverage dialogue to\nadvance story plots and establish characters. We present the first study to\nexplore whether machines can understand and generate dialogue in stories, which\nrequires capturing traits of different characters and the relationships between\nthem. To this end, we propose two new tasks including Masked Dialogue\nGeneration and Dialogue Speaker Recognition, i.e., generating missing dialogue\nturns and predicting speakers for specified dialogue turns, respectively. We\nbuild a new dataset DialStory, which consists of 105k Chinese stories with a\nlarge amount of dialogue weaved into the plots to support the evaluation. We\nshow the difficulty of the proposed tasks by testing existing models with\nautomatic and manual evaluation on DialStory. Furthermore, we propose to learn\nexplicit character representations to improve performance on these tasks.\nExtensive experiments and case studies show that our approach can generate more\ncoherent and informative dialogue, and achieve higher speaker recognition\naccuracy than strong baselines.", "published": "2022-09-18 10:19:04", "link": "http://arxiv.org/abs/2209.08524v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Overcoming Language Priors in Visual Question Answering via\n  Distinguishing Superficially Similar Instances", "abstract": "Despite the great progress of Visual Question Answering (VQA), current VQA\nmodels heavily rely on the superficial correlation between the question type\nand its corresponding frequent answers (i.e., language priors) to make\npredictions, without really understanding the input. In this work, we define\nthe training instances with the same question type but different answers as\n\\textit{superficially similar instances}, and attribute the language priors to\nthe confusion of VQA model on such instances. To solve this problem, we propose\na novel training framework that explicitly encourages the VQA model to\ndistinguish between the superficially similar instances. Specifically, for each\ntraining instance, we first construct a set that contains its superficially\nsimilar counterparts. Then we exploit the proposed distinguishing module to\nincrease the distance between the instance and its counterparts in the answer\nspace. In this way, the VQA model is forced to further focus on the other parts\nof the input beyond the question type, which helps to overcome the language\npriors. Experimental results show that our method achieves the state-of-the-art\nperformance on VQA-CP v2. Codes are available at\n\\href{https://github.com/wyk-nku/Distinguishing-VQA.git}{Distinguishing-VQA}.", "published": "2022-09-18 10:30:44", "link": "http://arxiv.org/abs/2209.08529v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document\n  Understanding", "abstract": "Recent efforts of multimodal Transformers have improved Visually Rich\nDocument Understanding (VrDU) tasks via incorporating visual and textual\ninformation. However, existing approaches mainly focus on fine-grained elements\nsuch as words and document image patches, making it hard for them to learn from\ncoarse-grained elements, including natural lexical units like phrases and\nsalient visual regions like prominent image regions. In this paper, we attach\nmore importance to coarse-grained elements containing high-density information\nand consistent semantics, which are valuable for document understanding. At\nfirst, a document graph is proposed to model complex relationships among\nmulti-grained multimodal elements, in which salient visual regions are detected\nby a cluster-based method. Then, a multi-grained multimodal Transformer called\nmmLayout is proposed to incorporate coarse-grained information into existing\npre-trained fine-grained multimodal Transformers based on the graph. In\nmmLayout, coarse-grained information is aggregated from fine-grained, and then,\nafter further processing, is fused back into fine-grained for final prediction.\nFurthermore, common sense enhancement is introduced to exploit the semantic\ninformation of natural lexical units. Experimental results on four tasks,\nincluding information extraction and document question answering, show that our\nmethod can improve the performance of multimodal Transformers based on\nfine-grained elements and achieve better performance with fewer parameters.\nQualitative analyses show that our method can capture consistent semantics in\ncoarse-grained elements.", "published": "2022-09-18 13:46:56", "link": "http://arxiv.org/abs/2209.08569v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Ambisonic Encoding of Signals From Spherical Microphone Arrays", "abstract": "This document illustrates how to process the signals from the microphones of\na rigid-sphere higher-order ambisonic microphone array so that they are encoded\nwith N3D normalization and ACN channel order and thereby can be used with the\nstandard ambisonic software tools such as SPARTA and the IEM Plugin Suite. A\nMATLAB script is provided.", "published": "2022-09-18 18:58:58", "link": "http://arxiv.org/abs/2211.00583v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ambisonic Encoding of Signals From Equatorial Microphone Arrays", "abstract": "The equatorial microphone array presented in (Ahrens et al., 2021) computes a\nspherical harmonic (SH) representation of a sound field based on pressure\nsensors along the equator of a rigid spherical baffle. The original formulation\nuses complex-valued SH basis functions. This is inconvenient if the SH\nrepresentation of the captured sound field is intended to be stored in time\ndomain by means of real-valued audio signals as it is common in the spatial\naudio format of ambisonics. The present document summarizes the modifications\nthat need to be applied to the mathematical formulation from (Ahrens et al.,\n2021) to produce an ambisonic representation of the captured sound field that\nis compatible with the established ambisonic software tools like SPARTA and the\nIEM Plugin Suite. An example MATLAB script that implements this formulation is\nprovided.", "published": "2022-09-18 18:57:43", "link": "http://arxiv.org/abs/2211.00584v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
