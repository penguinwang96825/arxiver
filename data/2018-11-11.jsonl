{"title": "Multi-labeled Relation Extraction with Attentive Capsule Network", "abstract": "To disclose overlapped multiple relations from a sentence still keeps\nchallenging. Most current works in terms of neural models inconveniently\nassuming that each sentence is explicitly mapped to a relation label, cannot\nhandle multiple relations properly as the overlapped features of the relations\nare either ignored or very difficult to identify. To tackle with the new issue,\nwe propose a novel approach for multi-labeled relation extraction with capsule\nnetwork which acts considerably better than current convolutional or recurrent\nnet in identifying the highly overlapped relations within an individual\nsentence. To better cluster the features and precisely extract the relations,\nwe further devise attention-based routing algorithm and sliding-margin loss\nfunction, and embed them into our capsule network. The experimental results\nshow that the proposed approach can indeed extract the highly overlapped\nfeatures and achieve significant performance improvement for relation\nextraction comparing to the state-of-the-art works.", "published": "2018-11-11 05:29:17", "link": "http://arxiv.org/abs/1811.04354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Product Title Refinement via Multi-Modal Generative Adversarial Learning", "abstract": "Nowadays, an increasing number of customers are in favor of using E-commerce\nApps to browse and purchase products. Since merchants are usually inclined to\nemploy redundant and over-informative product titles to attract customers'\nattention, it is of great importance to concisely display short product titles\non limited screen of cell phones. Previous researchers mainly consider textual\ninformation of long product titles and lack of human-like view during training\nand evaluation procedure. In this paper, we propose a Multi-Modal Generative\nAdversarial Network (MM-GAN) for short product title generation, which\ninnovatively incorporates image information, attribute tags from the product\nand the textual information from original long titles. MM-GAN treats short\ntitles generation as a reinforcement learning process, where the generated\ntitles are evaluated by the discriminator in a human-like view.", "published": "2018-11-11 22:37:38", "link": "http://arxiv.org/abs/1811.04498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Vocabulary Learning for Neural Chinese Pinyin IME", "abstract": "Pinyin-to-character (P2C) conversion is the core component of pinyin-based\nChinese input method engine (IME). However, the conversion is seriously\ncompromised by the ambiguities of Chinese characters corresponding to pinyin as\nwell as the predefined fixed vocabularies. To alleviate such inconveniences, we\npropose a neural P2C conversion model augmented by an online updated vocabulary\nwith a sampling mechanism to support open vocabulary learning during IME\nworking. Our experiments show that the proposed method outperforms commercial\nIMEs and state-of-the-art traditional models on standard corpus and true\ninputting history dataset in terms of multiple metrics and thus the online\nupdated vocabulary indeed helps our IME effectively follows user inputting\nbehavior.", "published": "2018-11-11 05:07:25", "link": "http://arxiv.org/abs/1811.04352v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "End-to-end Structure-Aware Convolutional Networks for Knowledge Base\n  Completion", "abstract": "Knowledge graph embedding has been an active research topic for knowledge\nbase completion, with progressive improvement from the initial TransE, TransH,\nDistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution\nover embeddings and multiple layers of nonlinear features to model knowledge\ngraphs. The model can be efficiently trained and scalable to large knowledge\ngraphs. However, there is no structure enforcement in the embedding space of\nConvE. The recent graph convolutional network (GCN) provides another way of\nlearning graph node embedding by successfully utilizing graph connectivity\nstructure. In this work, we propose a novel end-to-end Structure-Aware\nConvolutional Network (SACN) that takes the benefit of GCN and ConvE together.\nSACN consists of an encoder of a weighted graph convolutional network (WGCN),\nand a decoder of a convolutional network called Conv-TransE. WGCN utilizes\nknowledge graph node structure, node attributes and edge relation types. It has\nlearnable weights that adapt the amount of information from neighbors used in\nlocal aggregation, leading to more accurate embeddings of graph nodes. Node\nattributes in the graph are represented as additional nodes in the WGCN. The\ndecoder Conv-TransE enables the state-of-the-art ConvE to be translational\nbetween entities and relations while keeps the same link prediction performance\nas ConvE. We demonstrate the effectiveness of the proposed SACN on standard\nFB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over\nthe state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.", "published": "2018-11-11 18:07:44", "link": "http://arxiv.org/abs/1811.04441v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "User Modeling for Task Oriented Dialogues", "abstract": "We introduce end-to-end neural network based models for simulating users of\ntask-oriented dialogue systems. User simulation in dialogue systems is crucial\nfrom two different perspectives: (i) automatic evaluation of different dialogue\nmodels, and (ii) training task-oriented dialogue systems. We design a\nhierarchical sequence-to-sequence model that first encodes the initial user\ngoal and system turns into fixed length representations using Recurrent Neural\nNetworks (RNN). It then encodes the dialogue history using another RNN layer.\nAt each turn, user responses are decoded from the hidden representations of the\ndialogue level RNN. This hierarchical user simulator (HUS) approach allows the\nmodel to capture undiscovered parts of the user goal without the need of an\nexplicit dialogue state tracking. We further develop several variants by\nutilizing a latent variable model to inject random variations into user\nresponses to promote diversity in simulated user responses and a novel goal\nregularization mechanism to penalize divergence of user responses from the\ninitial user goal. We evaluate the proposed models on movie ticket booking\ndomain by systematically interacting each user simulator with various dialogue\nsystem policies trained with different objectives and users.", "published": "2018-11-11 08:21:55", "link": "http://arxiv.org/abs/1811.04369v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReDecode Framework for Iterative Improvement in Paraphrase Generation", "abstract": "Generating paraphrases, that is, different variations of a sentence conveying\nthe same meaning, is an important yet challenging task in NLP. Automatically\ngenerating paraphrases has its utility in many NLP tasks like question\nanswering, information retrieval, conversational systems to name a few. In this\npaper, we introduce iterative refinement of generated paraphrases within VAE\nbased generation framework. Current sequence generation models lack the\ncapability to (1) make improvements once the sentence is generated; (2) rectify\nerrors made while decoding. We propose a technique to iteratively refine the\noutput using multiple decoders, each one attending on the output sentence\ngenerated by the previous decoder. We improve current state of the art results\nsignificantly - with over 9% and 28% absolute increase in METEOR scores on\nQuora question pairs and MSCOCO datasets respectively. We also show\nqualitatively through examples that our re-decoding approach generates better\nparaphrases compared to a single decoder by rectifying errors and making\nimprovements in paraphrase structure, inducing variations and introducing new\nbut semantically coherent information.", "published": "2018-11-11 19:02:50", "link": "http://arxiv.org/abs/1811.04454v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multi-modal Deep Neural Network approach to Bird-song identification", "abstract": "We present a multi-modal Deep Neural Network (DNN) approach for bird song\nidentification. The presented approach takes both audio samples and metadata as\ninput. The audio is fed into a Convolutional Neural Network (CNN) using four\nconvolutional layers. The additionally provided metadata is processed using\nfully connected layers. The flattened convolutional layers and the fully\nconnected layer of the metadata are joined and fed into a fully connected\nlayer. The resulting architecture achieved 2., 3. and 4. rank in the\nBirdCLEF2017 task in various training configurations.", "published": "2018-11-11 18:58:30", "link": "http://arxiv.org/abs/1811.04448v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PerformanceNet: Score-to-Audio Music Generation with Multi-Band\n  Convolutional Residual Network", "abstract": "Music creation is typically composed of two parts: composing the musical\nscore, and then performing the score with instruments to make sounds. While\nrecent work has made much progress in automatic music generation in the\nsymbolic domain, few attempts have been made to build an AI model that can\nrender realistic music audio from musical scores. Directly synthesizing audio\nwith sound sample libraries often leads to mechanical and deadpan results,\nsince musical scores do not contain performance-level information, such as\nsubtle changes in timing and dynamics. Moreover, while the task may sound like\na text-to-speech synthesis problem, there are fundamental differences since\nmusic audio has rich polyphonic sounds. To build such an AI performer, we\npropose in this paper a deep convolutional model that learns in an end-to-end\nmanner the score-to-audio mapping between a symbolic representation of music\ncalled the piano rolls and an audio representation of music called the\nspectrograms. The model consists of two subnets: the ContourNet, which uses a\nU-Net structure to learn the correspondence between piano rolls and\nspectrograms and to give an initial result; and the TextureNet, which further\nuses a multi-band residual network to refine the result by adding the spectral\ntexture of overtones and timbre. We train the model to generate music clips of\nthe violin, cello, and flute, with a dataset of moderate size. We also present\nthe result of a user study that shows our model achieves higher mean opinion\nscore (MOS) in naturalness and emotional expressivity than a WaveNet-based\nmodel and two commercial sound libraries. We open our source code at\nhttps://github.com/bwang514/PerformanceNet", "published": "2018-11-11 05:55:39", "link": "http://arxiv.org/abs/1811.04357v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Temporal Resolution Convolutional Neural Networks for Acoustic\n  Scene Classification", "abstract": "In this paper we present a Deep Neural Network architecture for the task of\nacoustic scene classification which harnesses information from increasing\ntemporal resolutions of Mel-Spectrogram segments. This architecture is composed\nof separated parallel Convolutional Neural Networks which learn spectral and\ntemporal representations for each input resolution. The resolutions are chosen\nto cover fine-grained characteristics of a scene's spectral texture as well as\nits distribution of acoustic events. The proposed model shows a 3.56% absolute\nimprovement of the best performing single resolution model and 12.49% of the\nDCASE 2017 Acoustic Scenes Classification task baseline.", "published": "2018-11-11 14:05:52", "link": "http://arxiv.org/abs/1811.04419v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
