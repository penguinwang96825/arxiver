{"title": "Tag Recommendation by Word-Level Tag Sequence Modeling", "abstract": "In this paper, we transform tag recommendation into a word-based text\ngeneration problem and introduce a sequence-to-sequence model. The model\ninherits the advantages of LSTM-based encoder for sequential modeling and\nattention-based decoder with local positional encodings for learning relations\nglobally. Experimental results on Zhihu datasets illustrate the proposed model\noutperforms other state-of-the-art text classification based methods.", "published": "2019-11-30 02:12:31", "link": "http://arxiv.org/abs/1912.00113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Creation of Text Corpora for Low-Resource Languages from the\n  Internet: The Case of Swiss German", "abstract": "This paper presents SwissCrawl, the largest Swiss German text corpus to date.\nComposed of more than half a million sentences, it was generated using a\ncustomized web scraping tool that could be applied to other low-resource\nlanguages as well. The approach demonstrates how freely available web pages can\nbe used to construct comprehensive text corpora, which are of fundamental\nimportance for natural language processing. In an experimental evaluation, we\nshow that using the new corpus leads to significant improvements for the task\nof language modeling. To capture new content, our approach will run\ncontinuously to keep increasing the corpus over time.", "published": "2019-11-30 08:42:25", "link": "http://arxiv.org/abs/1912.00159v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural language modeling of free word order argument structure", "abstract": "Neural language models trained with a predictive or masked objective have\nproven successful at capturing short and long distance syntactic dependencies.\nHere, we focus on verb argument structure in German, which has the interesting\nproperty that verb arguments may appear in a relatively free order in\nsubordinate clauses. Therefore, checking that the verb argument structure is\ncorrect cannot be done in a strictly sequential fashion, but rather requires to\nkeep track of the arguments' cases irrespective of their orders. We introduce a\nnew probing methodology based on minimal variation sets and show that both\nTransformers and LSTM achieve a score substantially better than chance on this\ntest. As humans, they also show graded judgments preferring canonical word\norders and plausible case assignments. However, we also found unexpected\ndiscrepancies in the strength of these effects, the LSTMs having difficulties\nrejecting ungrammatical sentences containing frequent argument structure types\n(double nominatives), and the Transformers tending to overgeneralize, accepting\nsome infrequent word orders or implausible sentences that humans barely accept.", "published": "2019-11-30 17:11:07", "link": "http://arxiv.org/abs/1912.00239v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Graph Contextualized Knowledge into Pre-trained Language\n  Models", "abstract": "Complex node interactions are common in knowledge graphs, and these\ninteractions also contain rich knowledge information. However, traditional\nmethods usually treat a triple as a training unit during the knowledge\nrepresentation learning (KRL) procedure, neglecting contextualized information\nof the nodes in knowledge graphs (KGs). We generalize the modeling object to a\nvery general form, which theoretically supports any subgraph extracted from the\nknowledge graph, and these subgraphs are fed into a novel transformer-based\nmodel to learn the knowledge embeddings. To broaden usage scenarios of\nknowledge, pre-trained language models are utilized to build a model that\nincorporates the learned knowledge representations. Experimental results\ndemonstrate that our model achieves the state-of-the-art performance on several\nmedical NLP tasks, and improvement above TransE indicates that our KRL method\ncaptures the graph contextualized information effectively.", "published": "2019-11-30 07:13:25", "link": "http://arxiv.org/abs/1912.00147v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Fluency and Faithfulness for Diverse Neural Machine Translation", "abstract": "Neural machine translation models usually adopt the teacher forcing strategy\nfor training which requires the predicted sequence matches ground truth word by\nword and forces the probability of each prediction to approach a 0-1\ndistribution. However, the strategy casts all the portion of the distribution\nto the ground truth word and ignores other words in the target vocabulary even\nwhen the ground truth word cannot dominate the distribution. To address the\nproblem of teacher forcing, we propose a method to introduce an evaluation\nmodule to guide the distribution of the prediction. The evaluation module\naccesses each prediction from the perspectives of fluency and faithfulness to\nencourage the model to generate the word which has a fluent connection with its\npast and future translation and meanwhile tends to form a translation\nequivalent in meaning to the source. The experiments on multiple translation\ntasks show that our method can achieve significant improvements over strong\nbaselines.", "published": "2019-11-30 10:30:46", "link": "http://arxiv.org/abs/1912.00178v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing the Robustness of Visual Question Answering Models", "abstract": "Deep neural networks have been playing an essential role in the task of\nVisual Question Answering (VQA). Until recently, their accuracy has been the\nmain focus of research. Now there is a trend toward assessing the robustness of\nthese models against adversarial attacks by evaluating the accuracy of these\nmodels under increasing levels of noisiness in the inputs of VQA models. In\nVQA, the attack can target the image and/or the proposed query question, dubbed\nmain question, and yet there is a lack of proper analysis of this aspect of\nVQA. In this work, we propose a new method that uses semantically related\nquestions, dubbed basic questions, acting as noise to evaluate the robustness\nof VQA models. We hypothesize that as the similarity of a basic question to the\nmain question decreases, the level of noise increases. To generate a reasonable\nnoise level for a given main question, we rank a pool of basic questions based\non their similarity with this main question. We cast this ranking problem as a\nLASSO optimization problem. We also propose a novel robustness measure Rscore\nand two large-scale basic question datasets in order to standardize robustness\nanalysis of VQA models. The experimental results demonstrate that the proposed\nevaluation method is able to effectively analyze the robustness of VQA models.\nTo foster the VQA research, we will publish our proposed datasets.", "published": "2019-11-30 09:32:38", "link": "http://arxiv.org/abs/1912.01452v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Free Lunch in Generating Datasets: Building a VQG and VQA System with\n  Attention and Humans in the Loop", "abstract": "Despite their importance in training artificial intelligence systems, large\ndatasets remain challenging to acquire. For example, the ImageNet dataset\nrequired fourteen million labels of basic human knowledge, such as whether an\nimage contains a chair. Unfortunately, this knowledge is so simple that it is\ntedious for human annotators but also tacit enough such that they are\nnecessary. However, human collaborative efforts for tasks like labeling massive\namounts of data are costly, inconsistent, and prone to failure, and this method\ndoes not resolve the issue of the resulting dataset being static in nature.\nWhat if we asked people questions they want to answer and collected their\nresponses as data? This would mean we could gather data at a much lower cost,\nand expanding a dataset would simply become a matter of asking more questions.\nWe focus on the task of Visual Question Answering (VQA) and propose a system\nthat uses Visual Question Generation (VQG) to produce questions, asks them to\nsocial media users, and collects their responses. We present two models that\ncan then parse clean answers from the noisy human responses significantly\nbetter than our baselines, with the goal of eventually incorporating the\nanswers into a Visual Question Answering (VQA) dataset. By demonstrating how\nour system can collect large amounts of data at little to no cost, we envision\nsimilar systems being used to improve performance on other tasks in the future.", "published": "2019-11-30 03:45:17", "link": "http://arxiv.org/abs/1912.00124v2", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.LG", "I.2, I.4, I.6, I.7", "I.2; I.4; I.6; I.7"], "primary_category": "cs.CV"}
{"title": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique", "abstract": "Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification.", "published": "2019-11-30 04:00:31", "link": "http://arxiv.org/abs/1912.00127v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Predominant Musical Instrument Classification based on Spectral Features", "abstract": "This work aims to examine one of the cornerstone problems of Musical\nInstrument Retrieval (MIR), in particular, instrument classification. IRMAS\n(Instrument recognition in Musical Audio Signals) data set is chosen for this\npurpose. The data includes musical clips recorded from various sources in the\nlast century, thus having a wide variety of audio quality. We have presented a\nvery concise summary of past work in this domain. Having implemented various\nsupervised learning algorithms for this classification task, SVM classifier has\noutperformed the other state-of-the-art models with an accuracy of 79%. We also\nimplemented Unsupervised techniques out of which Hierarchical Clustering has\nperformed well.", "published": "2019-11-30 07:43:24", "link": "http://arxiv.org/abs/1912.02606v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
