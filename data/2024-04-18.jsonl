{"title": "AdvisorQA: Towards Helpful and Harmless Advice-seeking Question\n  Answering with Collective Intelligence", "abstract": "As the integration of large language models into daily life is on the rise,\nthere is a clear gap in benchmarks for advising on subjective and personal\ndilemmas. To address this, we introduce AdvisorQA, the first benchmark\ndeveloped to assess LLMs' capability in offering advice for deeply personalized\nconcerns, utilizing the LifeProTips subreddit forum. This forum features a\ndynamic interaction where users post advice-seeking questions, receiving an\naverage of 8.9 advice per query, with 164.2 upvotes from hundreds of users,\nembodying a collective intelligence framework. Therefore, we've completed a\nbenchmark encompassing daily life questions, diverse corresponding responses,\nand majority vote ranking to train our helpfulness metric. Baseline experiments\nvalidate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and\nhuman evaluation, analyzing phenomena beyond the trade-off between helpfulness\nand harmlessness. AdvisorQA marks a significant leap in enhancing QA systems\nfor providing personalized, empathetic advice, showcasing LLMs' improved\nunderstanding of human subjectivity.", "published": "2024-04-18 01:15:41", "link": "http://arxiv.org/abs/2404.11826v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NALA: an Effective and Interpretable Entity Alignment Method", "abstract": "Entity alignment (EA) aims to find equivalent entities between two Knowledge\nGraphs. Existing embedding-based EA methods usually encode entities as\nembeddings, triples as embeddings' constraint and learn to align the\nembeddings. However, the details of the underlying logical inference steps\namong the alignment process are usually omitted, resulting in inadequate\ninference process. In this paper, we introduce NALA, an entity alignment method\nthat captures three types of logical inference paths with Non-Axiomatic Logic\n(NAL). Type 1&2 align the entity pairs and type 3 aligns relations. NALA\niteratively aligns entities and relations by integrating the conclusions of the\ninference paths. Our method is logically interpretable and extensible by\nintroducing NAL, and thus suitable for various EA settings. Experimental\nresults show that NALA outperforms state-of-the-art methods in terms of Hits@1,\nachieving 0.98+ on all three datasets of DBP15K with both supervised and\nunsupervised settings. We offer a pioneering in-depth analysis of the\nfundamental principles of entity alignment, approaching the subject from a\nunified and logical perspective. Our code is available at\nhttps://github.com/13998151318/NALA.", "published": "2024-04-18 07:55:02", "link": "http://arxiv.org/abs/2404.11968v2", "categories": ["cs.CL", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Aligning Language Models to Explicitly Handle Ambiguity", "abstract": "In interactions between users and language model agents, user utterances\nfrequently exhibit ellipsis (omission of words or phrases) or imprecision (lack\nof exactness) to prioritize efficiency. This can lead to varying\ninterpretations of the same input based on different assumptions or background\nknowledge. It is thus crucial for agents to adeptly handle the inherent\nambiguity in queries to ensure reliability. However, even state-of-the-art\nlarge language models (LLMs) still face challenges in such scenarios, primarily\ndue to the following hurdles: (1) LLMs are not explicitly trained to deal with\nambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may\nvary depending on the possessed knowledge. To address these issues, we propose\nAlignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to\nmanage ambiguous queries by leveraging their own assessment of ambiguity (i.e.,\nperceived ambiguity). Experimental results on question-answering datasets\ndemonstrate that APA empowers LLMs to explicitly detect and manage ambiguous\nqueries while retaining the ability to answer clear questions. Furthermore, our\nfinding proves that APA excels beyond training with gold-standard labels,\nespecially in out-of-distribution scenarios. The data and code are available at\nhttps://github.com/heyjoonkim/APA.", "published": "2024-04-18 07:59:53", "link": "http://arxiv.org/abs/2404.11972v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EVIT: Event-Oriented Instruction Tuning for Event Reasoning", "abstract": "Events refer to specific occurrences, incidents, or happenings that take\nplace under a particular background. Event reasoning aims to infer events\naccording to certain relations and predict future events. The cutting-edge\ntechniques for event reasoning play a crucial role in various natural language\nprocessing applications. Large language models (LLMs) have made significant\nadvancements in event reasoning owing to their wealth of knowledge and\nreasoning capabilities. However, smaller instruction-tuned models currently in\nuse do not consistently demonstrate exceptional proficiency in managing these\ntasks. This discrepancy arises from the absence of explicit modeling of events\nand the interconnections of them within their instruction data. Consequently,\nthese models face challenges in comprehending event structures and semantics\nwhile struggling to bridge the gap between their interpretations and human\nunderstanding of events. Additionally, their limitations in grasping event\nrelations lead to constrained event reasoning abilities to effectively deduce\nand incorporate pertinent event knowledge. In this paper, we propose\nEvent-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we\nfirst propose a novel structure named event quadruple which contains the\nstructure and semantics of events and is complete in the event representation.\nWe then design event-relation learning based on the structures. We encapsulate\nthe learning into the instruction-tuning formulation to better stimulate the\nevent reasoning capacity of our model. We design a heuristic unsupervised\nmethod to mine event quadruple from a large-scale corpus. At last, we finetune\na Llama model on our Event-Oriented Instruction Tuning. We conduct extensive\nexperiments on event reasoning tasks on several datasets. Automatic and human\nevaluations demonstrate EvIT achieves competitive performances on event\nreasoning.", "published": "2024-04-18 08:14:53", "link": "http://arxiv.org/abs/2404.11978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Multi-Modal Hypergraph Attention Network for Multi-Modal\n  Relation Extraction", "abstract": "Multi-modal relation extraction (MMRE) is a challenging task that aims to\nidentify relations between entities in text leveraging image information.\nExisting methods are limited by their neglect of the multiple entity pairs in\none sentence sharing very similar contextual information (ie, the same text and\nimage), resulting in increased difficulty in the MMRE task. To address this\nlimitation, we propose the Variational Multi-Modal Hypergraph Attention Network\n(VM-HAN) for multi-modal relation extraction. Specifically, we first construct\na multi-modal hypergraph for each sentence with the corresponding image, to\nestablish different high-order intra-/inter-modal correlations for different\nentity pairs in each sentence. We further design the Variational Hypergraph\nAttention Networks (V-HAN) to obtain representational diversity among different\nentity pairs using Gaussian distribution and learn a better hypergraph\nstructure via variational attention. VM-HAN achieves state-of-the-art\nperformance on the multi-modal relation extraction task, outperforming existing\nmethods in terms of accuracy and efficiency.", "published": "2024-04-18 08:56:47", "link": "http://arxiv.org/abs/2404.12006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Compositional Generalization in Multimodal Models", "abstract": "The rise of large-scale multimodal models has paved the pathway for\ngroundbreaking advances in generative modeling and reasoning, unlocking\ntransformative applications in a variety of complex tasks. However, a pressing\nquestion that remains is their genuine capability for stronger forms of\ngeneralization, which has been largely underexplored in the multimodal setting.\nOur study aims to address this by examining sequential compositional\ngeneralization using \\textsc{CompAct} (\\underline{Comp}ositional\n\\underline{Act}ivities)\\footnote{Project Page:\n\\url{http://cyberiada.github.io/CompAct}}, a carefully constructed,\nperceptually grounded dataset set within a rich backdrop of egocentric kitchen\nactivity videos. Each instance in our dataset is represented with a combination\nof raw video footage, naturally occurring sound, and crowd-sourced step-by-step\ndescriptions. More importantly, our setup ensures that the individual concepts\nare consistently distributed across training and evaluation sets, while their\ncompositions are novel in the evaluation set. We conduct a comprehensive\nassessment of several unimodal and multimodal models. Our findings reveal that\nbi-modal and tri-modal models exhibit a clear edge over their text-only\ncounterparts. This highlights the importance of multimodality while charting a\ntrajectory for future research in this domain.", "published": "2024-04-18 09:04:15", "link": "http://arxiv.org/abs/2404.12013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Decoding via Hidden Transfer for Lossless Large Language Model\n  Acceleration", "abstract": "Large language models (LLMs) have recently shown remarkable performance\nacross a wide range of tasks. However, the substantial number of parameters in\nLLMs contributes to significant latency during model inference. This is\nparticularly evident when utilizing autoregressive decoding methods, which\ngenerate one token in a single forward process, thereby not fully capitalizing\non the parallel computing capabilities of GPUs. In this paper, we propose a\nnovel parallel decoding approach, namely \\textit{hidden transfer}, which\ndecodes multiple successive tokens simultaneously in a single forward pass. The\nidea is to transfer the intermediate hidden states of the previous context to\nthe \\textit{pseudo} hidden states of the future tokens to be generated, and\nthen the pseudo hidden states will pass the following transformer layers\nthereby assimilating more semantic information and achieving superior\npredictive accuracy of the future tokens.\n  Besides, we use the novel tree attention mechanism to simultaneously generate\nand verify multiple candidates of output sequences, which ensure the lossless\ngeneration and further improves the generation efficiency of our method.\nExperiments demonstrate the effectiveness of our method. We conduct a lot of\nanalytic experiments to prove our motivation. In terms of acceleration metrics,\nwe outperform all the single-model acceleration techniques, including Medusa\nand Self-Speculative decoding.", "published": "2024-04-18 09:17:06", "link": "http://arxiv.org/abs/2404.12022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector", "abstract": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV.", "published": "2024-04-18 09:46:25", "link": "http://arxiv.org/abs/2404.12038v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Boundaries and Intensities in Offensive and Hate Speech:\n  Unveiling the Complex Spectrum of Social Media Discourse", "abstract": "The prevalence of digital media and evolving sociopolitical dynamics have\nsignificantly amplified the dissemination of hateful content. Existing studies\nmainly focus on classifying texts into binary categories, often overlooking the\ncontinuous spectrum of offensiveness and hatefulness inherent in the text. In\nthis research, we present an extensive benchmark dataset for Amharic,\ncomprising 8,258 tweets annotated for three distinct tasks: category\nclassification, identification of hate targets, and rating offensiveness and\nhatefulness intensities. Our study highlights that a considerable majority of\ntweets belong to the less offensive and less hate intensity levels,\nunderscoring the need for early interventions by stakeholders. The prevalence\nof ethnic and political hatred targets, with significant overlaps in our\ndataset, emphasizes the complex relationships within Ethiopia's sociopolitical\nlandscape. We build classification and regression models and investigate the\nefficacy of models in handling these tasks. Our results reveal that hate and\noffensive speech can not be addressed by a simplistic binary classification,\ninstead manifesting as variables across a continuous range of values. The\nAfro-XLMR-large model exhibits the best performances achieving F1-scores of\n75.30%, 70.59%, and 29.42% for the category, target, and regression tasks,\nrespectively. The 80.22% correlation coefficient of the Afro-XLMR-large model\nindicates strong alignments.", "published": "2024-04-18 09:52:50", "link": "http://arxiv.org/abs/2404.12042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "emrQA-msquad: A Medical Dataset Structured with the SQuAD V2.0\n  Framework, Enriched with emrQA Medical Information", "abstract": "Machine Reading Comprehension (MRC) holds a pivotal role in shaping Medical\nQuestion Answering Systems (QAS) and transforming the landscape of accessing\nand applying medical information. However, the inherent challenges in the\nmedical field, such as complex terminology and question ambiguity, necessitate\ninnovative solutions. One key solution involves integrating specialized medical\ndatasets and creating dedicated datasets. This strategic approach enhances the\naccuracy of QAS, contributing to advancements in clinical decision-making and\nmedical research. To address the intricacies of medical terminology, a\nspecialized dataset was integrated, exemplified by a novel Span extraction\ndataset derived from emrQA but restructured into 163,695 questions and 4,136\nmanually obtained answers, this new dataset was called emrQA-msquad dataset.\nAdditionally, for ambiguous questions, a dedicated medical dataset for the Span\nextraction task was introduced, reinforcing the system's robustness. The\nfine-tuning of models such as BERT, RoBERTa, and Tiny RoBERTa for medical\ncontexts significantly improved response accuracy within the F1-score range of\n0.75 to 1.00 from 10.1% to 37.4%, 18.7% to 44.7% and 16.0% to 46.8%,\nrespectively. Finally, emrQA-msquad dataset is publicy available at\nhttps://huggingface.co/datasets/Eladio/emrqa-msquad.", "published": "2024-04-18 10:06:00", "link": "http://arxiv.org/abs/2404.12050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Parsing by Searching for Frequent Word Sequences among\n  Sentences with Equivalent Predicate-Argument Structures", "abstract": "Unsupervised constituency parsing focuses on identifying word sequences that\nform a syntactic unit (i.e., constituents) in target sentences. Linguists\nidentify the constituent by evaluating a set of Predicate-Argument Structure\n(PAS) equivalent sentences where we find the constituent appears more\nfrequently than non-constituents (i.e., the constituent corresponds to a\nfrequent word sequence within the sentence set). However, such frequency\ninformation is unavailable in previous parsing methods that identify the\nconstituent by observing sentences with diverse PAS. In this study, we\nempirically show that constituents correspond to frequent word sequences in the\nPAS-equivalent sentence set. We propose a frequency-based parser span-overlap\nthat (1) computes the span-overlap score as the word sequence's frequency in\nthe PAS-equivalent sentence set and (2) identifies the constituent structure by\nfinding a constituent tree with the maximum span-overlap score. The parser\nachieves state-of-the-art level parsing accuracy, outperforming existing\nunsupervised parsers in eight out of ten languages. Additionally, we discover a\nmultilingual phenomenon: participant-denoting constituents tend to have higher\nspan-overlap scores than equal-length event-denoting constituents, meaning that\nthe former tend to appear more frequently in the PAS-equivalent sentence set\nthan the latter. The phenomenon indicates a statistical difference between the\ntwo constituent types, laying the foundation for future labeled unsupervised\nparsing research.", "published": "2024-04-18 10:17:04", "link": "http://arxiv.org/abs/2404.12059v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FecTek: Enhancing Term Weight in Lexicon-Based Retrieval with Feature\n  Context and Term-level Knowledge", "abstract": "Lexicon-based retrieval has gained siginificant popularity in text retrieval\ndue to its efficient and robust performance. To further enhance performance of\nlexicon-based retrieval, researchers have been diligently incorporating\nstate-of-the-art methodologies like Neural retrieval and text-level contrastive\nlearning approaches. Nonetheless, despite the promising outcomes, current\nlexicon-based retrieval methods have received limited attention in exploring\nthe potential benefits of feature context representations and term-level\nknowledge guidance. In this paper, we introduce an innovative method by\nintroducing FEature Context and TErm-level Knowledge modules(FecTek). To\neffectively enrich the feature context representations of term weight, the\nFeature Context Module (FCM) is introduced, which leverages the power of BERT's\nrepresentation to determine dynamic weights for each element in the embedding.\nAdditionally, we develop a term-level knowledge guidance module (TKGM) for\neffectively utilizing term-level knowledge to intelligently guide the modeling\nprocess of term weight. Evaluation of the proposed method on MS Marco benchmark\ndemonstrates its superiority over the previous state-of-the-art approaches.", "published": "2024-04-18 12:58:36", "link": "http://arxiv.org/abs/2404.12152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation\n  Guidelines?", "abstract": "The increasing threat of disinformation calls for automating parts of the\nfact-checking pipeline. Identifying text segments requiring fact-checking is\nknown as claim detection (CD) and claim check-worthiness detection (CW), the\nlatter incorporating complex domain-specific criteria of worthiness and often\nframed as a ranking task. Zero- and few-shot LLM prompting is an attractive\noption for both tasks, as it bypasses the need for labeled datasets and allows\nverbalized claim and worthiness criteria to be directly used for prompting. We\nevaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets\nfrom diverse domains, each utilizing a different worthiness criterion. We\ninvestigate two key aspects: (1) how best to distill factuality and worthiness\ncriteria into a prompt and (2) what amount of context to provide for each\nclaim. To this end, we experiment with varying the level of prompt verbosity\nand the amount of contextual information provided to the model. Our results\nshow that optimal prompt verbosity is domain-dependent, adding context does not\nimprove performance, and confidence scores can be directly used to produce\nreliable check-worthiness rankings.", "published": "2024-04-18 13:31:05", "link": "http://arxiv.org/abs/2404.12174v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque", "abstract": "The widespread availability of Question Answering (QA) datasets in English\nhas greatly facilitated the advancement of the Natural Language Processing\n(NLP) field. However, the scarcity of such resources for minority languages,\nsuch as Basque, poses a substantial challenge for these communities. In this\ncontext, the translation and alignment of existing QA datasets plays a crucial\nrole in narrowing this technological gap. This work presents EuSQuAD, the first\ninitiative dedicated to automatically translating and aligning SQuAD2.0 into\nBasque, resulting in more than 142k QA examples. We demonstrate EuSQuAD's value\nthrough extensive qualitative analysis and QA experiments supported with\nEuSQuAD as training data. These experiments are evaluated with a new\nhuman-annotated dataset.", "published": "2024-04-18 13:31:57", "link": "http://arxiv.org/abs/2404.12177v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Length Generalization of Causal Transformers without Position Encoding", "abstract": "Generalizing to longer sentences is important for recent Transformer-based\nlanguage models. Besides algorithms manipulating explicit position features,\nthe success of Transformers without position encodings (NoPE) provides a new\nway to overcome the challenge. In this paper, we study the length\ngeneralization property of NoPE. We find that although NoPE can extend to\nlonger sequences than the commonly used explicit position encodings, it still\nhas a limited context length. We identify a connection between the failure of\nNoPE's generalization and the distraction of attention distributions. We\npropose a parameter-efficient tuning for searching attention heads' best\ntemperature hyper-parameters, which substantially expands NoPE's context size.\nExperiments on long sequence language modeling, the synthetic passkey retrieval\ntask and real-world long context tasks show that NoPE can achieve competitive\nperformances with state-of-the-art length generalization algorithms. The source\ncode is publicly accessible", "published": "2024-04-18 14:38:32", "link": "http://arxiv.org/abs/2404.12224v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMNEE: A Large-Scale Document-Level Event Extraction Dataset based on\n  Open-Source Chinese Military News", "abstract": "Extracting structured event knowledge, including event triggers and\ncorresponding arguments, from military texts is fundamental to many\napplications, such as intelligence analysis and decision assistance. However,\nevent extraction in the military field faces the data scarcity problem, which\nimpedes the research of event extraction models in this domain. To alleviate\nthis problem, we propose CMNEE, a large-scale, document-level open-source\nChinese Military News Event Extraction dataset. It contains 17,000 documents\nand 29,223 events, which are all manually annotated based on a pre-defined\nschema for the military domain including 8 event types and 11 argument role\ntypes. We designed a two-stage, multi-turns annotation strategy to ensure the\nquality of CMNEE and reproduced several state-of-the-art event extraction\nmodels with a systematic evaluation. The experimental results on CMNEE fall\nshorter than those on other domain datasets obviously, which demonstrates that\nevent extraction for military domain poses unique challenges and requires\nfurther research efforts. Our code and data can be obtained from\nhttps://github.com/Mzzzhu/CMNEE.", "published": "2024-04-18 15:02:35", "link": "http://arxiv.org/abs/2404.12242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Embedding Performance through Large Language Model-based Text\n  Enrichment and Rewriting", "abstract": "Embedding models are crucial for various natural language processing tasks\nbut can be limited by factors such as limited vocabulary, lack of context, and\ngrammatical errors. This paper proposes a novel approach to improve embedding\nperformance by leveraging large language models (LLMs) to enrich and rewrite\ninput text before the embedding process. By utilizing ChatGPT 3.5 to provide\nadditional context, correct inaccuracies, and incorporate metadata, the\nproposed method aims to enhance the utility and accuracy of embedding models.\nThe effectiveness of this approach is evaluated on three datasets:\nBanking77Classification, TwitterSemEval 2015, and Amazon Counter-factual\nClassification. Results demonstrate significant improvements over the baseline\nmodel on the TwitterSemEval 2015 dataset, with the best-performing prompt\nachieving a score of 85.34 compared to the previous best of 81.52 on the\nMassive Text Embedding Benchmark (MTEB) Leaderboard. However, performance on\nthe other two datasets was less impressive, highlighting the importance of\nconsidering domain-specific characteristics. The findings suggest that\nLLM-based text enrichment has shown promising results to improve embedding\nperformance, particularly in certain domains. Hence, numerous limitations in\nthe process of embedding can be avoided.", "published": "2024-04-18 15:58:56", "link": "http://arxiv.org/abs/2404.12283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resilience through Scene Context in Visual Referring Expression\n  Generation", "abstract": "Scene context is well known to facilitate humans' perception of visible\nobjects. In this paper, we investigate the role of context in Referring\nExpression Generation (REG) for objects in images, where existing research has\noften focused on distractor contexts that exert pressure on the generator. We\ntake a new perspective on scene context in REG and hypothesize that contextual\ninformation can be conceived of as a resource that makes REG models more\nresilient and facilitates the generation of object descriptions, and object\ntypes in particular. We train and test Transformer-based REG models with target\nrepresentations that have been artificially obscured with noise to varying\ndegrees. We evaluate how properties of the models' visual context affect their\nprocessing and performance. Our results show that even simple scene contexts\nmake models surprisingly resilient to perturbations, to the extent that they\ncan identify referent types even when visual information about the target is\ncompletely missing.", "published": "2024-04-18 16:10:38", "link": "http://arxiv.org/abs/2404.12289v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual\n  Alignment", "abstract": "Aligning language models (LMs) based on human-annotated preference data is a\ncrucial step in obtaining practical and performant LM-based systems. However,\nmultilingual human preference data are difficult to obtain at scale, making it\nchallenging to extend this framework to diverse languages. In this work, we\nevaluate a simple approach for zero-shot cross-lingual alignment, where a\nreward model is trained on preference data in one source language and directly\napplied to other target languages. On summarization and open-ended dialog\ngeneration, we show that this method is consistently successful under\ncomprehensive evaluation settings, including human evaluation: cross-lingually\naligned models are preferred by humans over unaligned models on up to >70% of\nevaluation instances. We moreover find that a different-language reward model\nsometimes yields better aligned models than a same-language reward model. We\nalso identify best practices when there is no language-specific data for even\nsupervised finetuning, another component in alignment.", "published": "2024-04-18 16:52:36", "link": "http://arxiv.org/abs/2404.12318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Targeted Sentiment Analysis", "abstract": "In this paper we investigate the use of decoder-based generative transformers\nfor extracting sentiment towards the named entities in Russian news articles.\nWe study sentiment analysis capabilities of instruction-tuned large language\nmodels (LLMs). We consider the dataset of RuSentNE-2023 in our study. The first\ngroup of experiments was aimed at the evaluation of zero-shot capabilities of\nLLMs with closed and open transparencies. The second covers the fine-tuning of\nFlan-T5 using the \"chain-of-thought\" (CoT) three-hop reasoning framework\n(THoR). We found that the results of the zero-shot approaches are similar to\nthe results achieved by baseline fine-tuned encoder-based transformers\n(BERT-base). Reasoning capabilities of the fine-tuned Flan-T5 models with THoR\nachieve at least 5% increment with the base-size model compared to the results\nof the zero-shot experiment. The best results of sentiment analysis on\nRuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed the\nresults of previous state-of-the-art transformer-based classifiers. Our CoT\napplication framework is publicly available:\nhttps://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework", "published": "2024-04-18 17:16:16", "link": "http://arxiv.org/abs/2404.12342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AmbigDocs: Reasoning across Documents on Different Entities under the\n  Same Name", "abstract": "Different entities with the same name can be difficult to distinguish.\nHandling confusing entity mentions is a crucial skill for language models\n(LMs). For example, given the question \"Where was Michael Jordan educated?\" and\na set of documents discussing different people named Michael Jordan, can LMs\ndistinguish entity mentions to generate a cohesive answer to the question? To\ntest this ability, we introduce a new benchmark, AmbigDocs. By leveraging\nWikipedia's disambiguation pages, we identify a set of documents, belonging to\ndifferent entities who share an ambiguous name. From these documents, we\ngenerate questions containing an ambiguous name and their corresponding sets of\nanswers. Our analysis reveals that current state-of-the-art models often yield\nambiguous answers or incorrectly merge information belonging to different\nentities. We establish an ontology categorizing four types of incomplete\nanswers and automatic evaluation metrics to identify such categories. We lay\nthe foundation for future work on reasoning across multiple documents with\nambiguous entities.", "published": "2024-04-18 18:12:01", "link": "http://arxiv.org/abs/2404.12447v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing LLM Abstention Behavior in Science QA with Context\n  Perturbations", "abstract": "The correct model response in the face of uncertainty is to abstain from\nanswering a question so as not to mislead the user. In this work, we study the\nability of LLMs to abstain from answering context-dependent science questions\nwhen provided insufficient or incorrect context. We probe model sensitivity in\nseveral settings: removing gold context, replacing gold context with irrelevant\ncontext, and providing additional context beyond what is given. In experiments\non four QA datasets with six LLMs, we show that performance varies greatly\nacross models, across the type of context provided, and also by question type;\nin particular, many LLMs seem unable to abstain from answering boolean\nquestions using standard QA prompts. Our analysis also highlights the\nunexpected impact of abstention performance on QA task accuracy.\nCounter-intuitively, in some settings, replacing gold context with irrelevant\ncontext or adding irrelevant context to gold context can improve abstention\nperformance in a way that results in improvements in task performance. Our\nresults imply that changes are needed in QA dataset design and evaluation to\nmore effectively assess the correctness and downstream impacts of model\nabstention.", "published": "2024-04-18 18:26:43", "link": "http://arxiv.org/abs/2404.12452v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models", "abstract": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.", "published": "2024-04-18 18:48:50", "link": "http://arxiv.org/abs/2404.12464v9", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction for Code-Switched Sentences by Learners of\n  English", "abstract": "Code-switching (CSW) is a common phenomenon among multilingual speakers where\nmultiple languages are used in a single discourse or utterance. Mixed language\nutterances may still contain grammatical errors however, yet most existing\nGrammar Error Correction (GEC) systems have been trained on monolingual data\nand not developed with CSW in mind. In this work, we conduct the first\nexploration into the use of GEC systems on CSW text. Through this exploration,\nwe propose a novel method of generating synthetic CSW GEC datasets by\ntranslating different spans of text within existing GEC corpora. We then\ninvestigate different methods of selecting these spans based on CSW ratio,\nswitch-point factor and linguistic constraints, and identify how they affect\nthe performance of GEC systems on CSW text. Our best model achieves an average\nincrease of 1.57 $F_{0.5}$ across 3 CSW test sets (English-Chinese,\nEnglish-Korean and English-Japanese) without affecting the model's performance\non a monolingual dataset. We furthermore discovered that models trained on one\nCSW language generalise relatively well to other typologically similar CSW\nlanguages.", "published": "2024-04-18 20:05:30", "link": "http://arxiv.org/abs/2404.12489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models", "abstract": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision-making and planning\ntasks. Current large language models (LLMs) are insufficient for accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30% better than those provided directly by LLM baselines. These estimates\nfurther contribute to better and more trustworthy decision making.", "published": "2024-04-18 20:17:23", "link": "http://arxiv.org/abs/2404.12494v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Concept-based Explanation of NLP Models", "abstract": "Interpreting and understanding the predictions made by deep learning models\nposes a formidable challenge due to their inherently opaque nature. Many\nprevious efforts aimed at explaining these predictions rely on input features,\nspecifically, the words within NLP models. However, such explanations are often\nless informative due to the discrete nature of these words and their lack of\ncontextual verbosity. To address this limitation, we introduce the Latent\nConcept Attribution method (LACOAT), which generates explanations for\npredictions based on latent concepts. Our foundational intuition is that a word\ncan exhibit multiple facets, contingent upon the context in which it is used.\nTherefore, given a word in context, the latent space derived from our training\nprocess reflects a specific facet of that word. LACOAT functions by mapping the\nrepresentations of salient input words into the training latent space, allowing\nit to provide latent context-based explanations of the prediction.", "published": "2024-04-18 23:50:50", "link": "http://arxiv.org/abs/2404.12545v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharing Parameter by Conjugation for Knowledge Graph Embeddings in\n  Complex Space", "abstract": "A Knowledge Graph (KG) is the directed graphical representation of entities\nand relations in the real world. KG can be applied in diverse Natural Language\nProcessing (NLP) tasks where knowledge is required. The need to scale up and\ncomplete KG automatically yields Knowledge Graph Embedding (KGE), a shallow\nmachine learning model that is suffering from memory and training time\nconsumption issues. To mitigate the computational load, we propose a\nparameter-sharing method, i.e., using conjugate parameters for complex numbers\nemployed in KGE models. Our method improves memory efficiency by 2x in relation\nembedding while achieving comparable performance to the state-of-the-art\nnon-conjugate models, with faster, or at least comparable, training time. We\ndemonstrated the generalizability of our method on two best-performing KGE\nmodels $5^{\\bigstar}\\mathrm{E}$ and $\\mathrm{ComplEx}$ on five benchmark\ndatasets.", "published": "2024-04-18 00:05:02", "link": "http://arxiv.org/abs/2404.11809v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenging Negative Gender Stereotypes: A Study on the Effectiveness of\n  Automated Counter-Stereotypes", "abstract": "Gender stereotypes are pervasive beliefs about individuals based on their\ngender that play a significant role in shaping societal attitudes, behaviours,\nand even opportunities. Recognizing the negative implications of gender\nstereotypes, particularly in online communications, this study investigates\neleven strategies to automatically counter-act and challenge these views. We\npresent AI-generated gender-based counter-stereotypes to (self-identified) male\nand female study participants and ask them to assess their offensiveness,\nplausibility, and potential effectiveness. The strategies of counter-facts and\nbroadening universals (i.e., stating that anyone can have a trait regardless of\ngroup membership) emerged as the most robust approaches, while humour,\nperspective-taking, counter-examples, and empathy for the speaker were\nperceived as less effective. Also, the differences in ratings were more\npronounced for stereotypes about the different targets than between the genders\nof the raters. Alarmingly, many AI-generated counter-stereotypes were perceived\nas offensive and/or implausible. Our analysis and the collected dataset offer\nfoundational insight into counter-stereotype generation, guiding future efforts\nto develop strategies that effectively challenge gender stereotypes in online\ninteractions.", "published": "2024-04-18 01:48:28", "link": "http://arxiv.org/abs/2404.11845v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing Length Extrapolation in Sequential Models with\n  Pointer-Augmented Neural Memory", "abstract": "We propose Pointer-Augmented Neural Memory (PANM) to help neural networks\nunderstand and apply symbol processing to new, longer sequences of data. PANM\nintegrates an external neural memory that uses novel physical addresses and\npointer manipulation techniques to mimic human and computer symbol processing\nabilities. PANM facilitates pointer assignment, dereference, and arithmetic by\nexplicitly using physical pointers to access memory content. Remarkably, it can\nlearn to perform these operations through end-to-end training on sequence data,\npowering various sequential models. Our experiments demonstrate PANM's\nexceptional length extrapolating capabilities and improved performance in tasks\nthat require symbol processing, such as algorithmic reasoning and Dyck language\nrecognition. PANM helps Transformer achieve up to 100% generalization accuracy\nin compositional learning tasks and significantly better results in\nmathematical reasoning, question answering and machine translation tasks.", "published": "2024-04-18 03:03:46", "link": "http://arxiv.org/abs/2404.11870v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding", "abstract": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.", "published": "2024-04-18 05:25:54", "link": "http://arxiv.org/abs/2404.11912v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skeleton: A New Framework for Accelerating Language Models via Task\n  Neuron Localized Prompt Tuning", "abstract": "Prompt tuning methods have shown comparable performance to general training\nmethods as parameter-efficient fine-tuning (PEFT) methods in various natural\nlanguage understanding tasks. However, existing prompt tuning methods still\nutilize the entire model architecture even when solving a specific task, which\nprevents them from accelerating inference speed during the application\nprocedure. In this paper, we propose a novel prompt tuning framework called\nSkeleton to efficiently utilize a language model in terms of memory and time\ncomplexity for solving various tasks, retaining only task-relevant neurons by\nusing an explainability method. From our framework, we can efficiently solve\nvarious tasks by using only task-relevant neurons and prepending adequate\ntask-specific prompt tokens with only a single language model. Experiments\nreveal that our method significantly enhances inference efficiency (at most x\n1.73 speed up) for various widely used benchmarks, showing comparable\nperformance to the prompt tuning method. Moreover, our method is applicable\nacross various transformer-based architectures, confirming its practicality and\nscalability.", "published": "2024-04-18 05:43:50", "link": "http://arxiv.org/abs/2404.11916v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment", "abstract": "Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.", "published": "2024-04-18 06:20:50", "link": "http://arxiv.org/abs/2404.11932v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token-level Direct Preference Optimization", "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.", "published": "2024-04-18 08:49:38", "link": "http://arxiv.org/abs/2404.11999v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhance Robustness of Language Models Against Variation Attack through\n  Graph Integration", "abstract": "The widespread use of pre-trained language models (PLMs) in natural language\nprocessing (NLP) has greatly improved performance outcomes. However, these\nmodels' vulnerability to adversarial attacks (e.g., camouflaged hints from drug\ndealers), particularly in the Chinese language with its rich character\ndiversity/variation and complex structures, hatches vital apprehension. In this\nstudy, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE),\nto increase the robustness of PLMs against character variation attacks in\nChinese content. CHANGE presents a novel approach for incorporating a Chinese\ncharacter variation graph into the PLMs. Through designing different\nsupplementary tasks utilizing the graph structure, CHANGE essentially enhances\nPLMs' interpretation of adversarially manipulated text. Experiments conducted\nin a multitude of NLP tasks show that CHANGE outperforms current language\nmodels in combating against adversarial attacks and serves as a valuable\ncontribution to robust language model research. These findings contribute to\nthe groundwork on robust language models and highlight the substantial\npotential of graph-guided pre-training strategies for real-world applications.", "published": "2024-04-18 09:04:39", "link": "http://arxiv.org/abs/2404.12014v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Can We Catch the Elephant? A Survey of the Evolvement of Hallucination\n  Evaluation on Natural Language Generation", "abstract": "Hallucination in Natural Language Generation (NLG) is like the elephant in\nthe room, obvious but often overlooked until recent achievements significantly\nimproved the fluency and grammaticality of generated text. As the capabilities\nof text generation models have improved, researchers have begun to pay more\nattention to the phenomenon of hallucination. Despite significant progress in\nthis field in recent years, the evaluation system for hallucination is complex\nand diverse, lacking clear organization. We are the first to comprehensively\nsurvey how various evaluation methods have evolved with the development of text\ngeneration models from three dimensions, including hallucinated fact\ngranularity, evaluator design principles, and assessment facets. This survey\naims to help researchers identify current limitations in hallucination\nevaluation and highlight future research directions.", "published": "2024-04-18 09:52:18", "link": "http://arxiv.org/abs/2404.12041v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAM: Towards an Ever-Improving Memory System by Learning from\n  Communications", "abstract": "We introduce an innovative RAG-based framework with an ever-improving memory.\nInspired by humans'pedagogical process, RAM utilizes recursively\nreasoning-based retrieval and experience reflections to continually update the\nmemory and learn from users' communicative feedback, namely communicative\nlearning. Extensive experiments with both simulated and real users demonstrate\nsignificant improvements over traditional RAG and self-knowledge methods,\nparticularly excelling in handling false premise and multi-hop questions.\nFurthermore, RAM exhibits promising adaptability to various feedback and\nretrieval methods, showcasing its potential for advancing AI capabilities in\ndynamic knowledge acquisition and lifelong learning.", "published": "2024-04-18 09:58:51", "link": "http://arxiv.org/abs/2404.12045v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LongEmbed: Extending Embedding Models for Long Context Retrieval", "abstract": "Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.", "published": "2024-04-18 11:29:23", "link": "http://arxiv.org/abs/2404.12096v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Form(s) to Meaning: Probing the Semantic Depths of Language Models\n  Using Multisense Consistency", "abstract": "The staggering pace with which the capabilities of large language models\n(LLMs) are increasing, as measured by a range of commonly used natural language\nunderstanding (NLU) benchmarks, raises many questions regarding what\n\"understanding\" means for a language model and how it compares to human\nunderstanding. This is especially true since many LLMs are exclusively trained\non text, casting doubt on whether their stellar benchmark performances are\nreflective of a true understanding of the problems represented by these\nbenchmarks, or whether LLMs simply excel at uttering textual forms that\ncorrelate with what someone who understands the problem would say. In this\nphilosophically inspired work, we aim to create some separation between form\nand meaning, with a series of tests that leverage the idea that world\nunderstanding should be consistent across presentational modes - inspired by\nFregean senses - of the same meaning. Specifically, we focus on consistency\nacross languages as well as paraphrases. Taking GPT-3.5 as our object of study,\nwe evaluate multisense consistency across five different languages and various\ntasks. We start the evaluation in a controlled setting, asking the model for\nsimple facts, and then proceed with an evaluation on four popular NLU\nbenchmarks. We find that the model's multisense consistency is lacking and run\nseveral follow-up analyses to verify that this lack of consistency is due to a\nsense-dependent task understanding. We conclude that, in this aspect, the\nunderstanding of LLMs is still quite far from being consistent and human-like,\nand deliberate on how this impacts their utility in the context of learning\nabout human language and understanding.", "published": "2024-04-18 12:48:17", "link": "http://arxiv.org/abs/2404.12145v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aligning language models with human preferences", "abstract": "Language models (LMs) trained on vast quantities of text data can acquire\nsophisticated skills such as generating summaries, answering questions or\ngenerating code. However, they also manifest behaviors that violate human\npreferences, e.g., they can generate offensive content, falsehoods or\nperpetuate social biases. In this thesis, I explore several approaches to\naligning LMs with human preferences. First, I argue that aligning LMs can be\nseen as Bayesian inference: conditioning a prior (base, pretrained LM) on\nevidence about human preferences (Chapter 2). Conditioning on human preferences\ncan be implemented in numerous ways. In Chapter 3, I investigate the relation\nbetween two approaches to finetuning pretrained LMs using feedback given by a\nscoring function: reinforcement learning from human feedback (RLHF) and\ndistribution matching. I show that RLHF can be seen as a special case of\ndistribution matching but distributional matching is strictly more general. In\nchapter 4, I show how to extend the distribution matching to conditional\nlanguage models. Finally, in chapter 5 I explore a different root: conditioning\nan LM on human preferences already during pretraining. I show that involving\nhuman feedback from the very start tends to be more effective than using it\nonly during supervised finetuning. Overall, these results highlight the room\nfor alignment techniques different from and complementary to RLHF.", "published": "2024-04-18 12:55:18", "link": "http://arxiv.org/abs/2404.12150v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Stance Detection on Social Media with Fine-Tuned Large Language Models", "abstract": "Stance detection, a key task in natural language processing, determines an\nauthor's viewpoint based on textual analysis. This study evaluates the\nevolution of stance detection methods, transitioning from early machine\nlearning approaches to the groundbreaking BERT model, and eventually to modern\nLarge Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While\nChatGPT's closed-source nature and associated costs present challenges, the\nopen-source models like LLaMa-2 and Mistral-7B offers an encouraging\nalternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2,\nand Mistral-7B using several publicly available datasets. Subsequently, to\nprovide a comprehensive comparison, we assess the performance of these models\nin zero-shot and few-shot learning scenarios. The results underscore the\nexceptional ability of LLMs in accurately detecting stance, with all tested\nmodels surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B\ndemonstrate remarkable efficiency and potential for stance detection, despite\ntheir smaller sizes compared to ChatGPT. This study emphasizes the potential of\nLLMs in stance detection and calls for more extensive research in this field.", "published": "2024-04-18 13:25:29", "link": "http://arxiv.org/abs/2404.12171v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\n  Instruction Data", "abstract": "Instruction fine-tuning pretrained LLMs for diverse downstream tasks has\ndemonstrated remarkable success and has captured the interest of both academics\nand practitioners. To ensure such fine-tuned LLMs align with human preferences,\ntechniques such as RLHF and DPO have emerged. At the same time, there is\nincreasing interest in smaller parameter counts for models. In this work, using\nOpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the\nOpenBezoar family of models. In this recipe: We first generate synthetic\ninstruction fine-tuning data using an open and commercially non-restrictive\ninstruction fine-tuned variant of the Falcon-40B model under three schemes\nbased on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a\nseed dataset) and Orca (with the Flan Collection as a seed dataset), then\nfilter these generations using GPT-4 as a human proxy. We then perform\ncost-effective QLoRA-based supervised fine-tuning sequentially with each\nscheme. The resulting checkpoint is further fine-tuned with a subset of the\nHH-RLHF dataset to minimize distribution shift prior to using the DPO loss to\nobtain the final checkpoint. Evaluation is done with the LM Eval Harness\ntasks/metrics as well as on MT-Bench using the \"LLM-as-a-judge\" framework with\nClaude 2.1, with the finding that the final checkpoint,\n\"OpenBezoar-HH-RLHF-DPO\", demonstrates superior performance over many models at\nthe 3B parameter scale, even outperforming the top model in one of the\ncategories on the Huggingface Open LLM Leaderboard. We release\n\"OpenBezoar-SFT\", \"OpenBezoar-HH-RLHF-SFT\", \"OpenBezoar-HH-RLHF-DPO\"\ncheckpoints, alongside our generated datasets on HuggingFace at\nhttps://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc\nand our codebase at\nhttps://bitbucket.org/paladinanalytics/workspace/projects/OP.", "published": "2024-04-18 13:57:18", "link": "http://arxiv.org/abs/2404.12195v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons", "abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created\nby the MLCommons AI Safety Working Group. The AI Safety Benchmark has been\ndesigned to assess the safety risks of AI systems that use chat-tuned language\nmodels. We introduce a principled approach to specifying and constructing the\nbenchmark, which for v0.5 covers only a single use case (an adult chatting to a\ngeneral-purpose assistant in English), and a limited set of personas (i.e.,\ntypical users, malicious users, and vulnerable users). We created a new\ntaxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark.\nWe plan to release version 1.0 of the AI Safety Benchmark by the end of 2024.\nThe v1.0 benchmark will provide meaningful insights into the safety of AI\nsystems. However, the v0.5 benchmark should not be used to assess the safety of\nAI systems. We have sought to fully document the limitations, flaws, and\nchallenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes\n(1) a principled approach to specifying and constructing the benchmark, which\ncomprises use cases, types of systems under test (SUTs), language and context,\npersonas, tests, and test items; (2) a taxonomy of 13 hazard categories with\ndefinitions and subcategories; (3) tests for seven of the hazard categories,\neach comprising a unique set of test items, i.e., prompts. There are 43,090\ntest items in total, which we created with templates; (4) a grading system for\nAI systems against the benchmark; (5) an openly available platform, and\ndownloadable tool, called ModelBench that can be used to evaluate the safety of\nAI systems on the benchmark; (6) an example evaluation report which benchmarks\nthe performance of over a dozen openly available chat-tuned language models;\n(7) a test specification for the benchmark.", "published": "2024-04-18 15:01:00", "link": "http://arxiv.org/abs/2404.12241v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing", "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs.", "published": "2024-04-18 15:21:34", "link": "http://arxiv.org/abs/2404.12253v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing the Robustness of Large Language Models through Self-Denoised\n  Smoothing", "abstract": "Although large language models (LLMs) have achieved significant success,\ntheir vulnerability to adversarial perturbations, including recent jailbreak\nattacks, has raised considerable concerns. However, the increasing size of\nthese models and their limited access make improving their robustness a\nchallenging task. Among various defense strategies, randomized smoothing has\nshown great potential for LLMs, as it does not require full access to the\nmodel's parameters or fine-tuning via adversarial training. However, randomized\nsmoothing involves adding noise to the input before model prediction, and the\nfinal model's robustness largely depends on the model's performance on these\nnoise corrupted data. Its effectiveness is often limited by the model's\nsub-optimal performance on noisy data. To address this issue, we propose to\nleverage the multitasking nature of LLMs to first denoise the noisy inputs and\nthen to make predictions based on these denoised versions. We call this\nprocedure self-denoised smoothing. Unlike previous denoised smoothing\ntechniques in computer vision, which require training a separate model to\nenhance the robustness of LLMs, our method offers significantly better\nefficiency and flexibility. Our experimental results indicate that our method\nsurpasses existing methods in both empirical and certified robustness in\ndefending against adversarial attacks for both downstream tasks and human\nalignments (i.e., jailbreak attacks). Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise", "published": "2024-04-18 15:47:00", "link": "http://arxiv.org/abs/2404.12274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Augmenting emotion features in irony detection with Large language\n  modeling", "abstract": "This study introduces a novel method for irony detection, applying Large\nLanguage Models (LLMs) with prompt-based learning to facilitate emotion-centric\ntext augmentation. Traditional irony detection techniques typically fall short\ndue to their reliance on static linguistic features and predefined knowledge\nbases, often overlooking the nuanced emotional dimensions integral to irony. In\ncontrast, our methodology augments the detection process by integrating subtle\nemotional cues, augmented through LLMs, into three benchmark pre-trained NLP\nmodels - BERT, T5, and GPT-2 - which are widely recognized as foundational in\nirony detection. We assessed our method using the SemEval-2018 Task 3 dataset\nand observed substantial enhancements in irony detection capabilities.", "published": "2024-04-18 16:11:17", "link": "http://arxiv.org/abs/2404.12291v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language\n  Models", "abstract": "We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .", "published": "2024-04-18 17:59:48", "link": "http://arxiv.org/abs/2404.12387v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "mOthello: When Do Cross-Lingual Representation Alignment and\n  Cross-Lingual Transfer Emerge in Multilingual Models?", "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability,\nwhich is often attributed to a learned language-neutral representation during\npretraining. However, it remains unclear what factors contribute to the\nlearning of a language-neutral representation, and whether the learned\nlanguage-neutral representation suffices to facilitate cross-lingual transfer.\nWe propose a synthetic task, Multilingual Othello (mOthello), as a testbed to\ndelve into these two questions. We find that: (1) models trained with naive\nmultilingual pretraining fail to learn a language-neutral representation across\nall input languages; (2) the introduction of \"anchor tokens\" (i.e., lexical\nitems that are identical across languages) helps cross-lingual representation\nalignment; and (3) the learning of a language-neutral representation alone is\nnot sufficient to facilitate cross-lingual transfer. Based on our findings, we\npropose a novel approach - multilingual pretraining with unified output space -\nthat both induces the learning of language-neutral representation and\nfacilitates cross-lingual transfer.", "published": "2024-04-18 18:03:08", "link": "http://arxiv.org/abs/2404.12444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GraphER: A Structure-aware Text-to-Graph Model for Entity and Relation\n  Extraction", "abstract": "Information extraction (IE) is an important task in Natural Language\nProcessing (NLP), involving the extraction of named entities and their\nrelationships from unstructured text. In this paper, we propose a novel\napproach to this task by formulating it as graph structure learning (GSL). By\nformulating IE as GSL, we enhance the model's ability to dynamically refine and\noptimize the graph structure during the extraction process. This formulation\nallows for better interaction and structure-informed decisions for entity and\nrelation prediction, in contrast to previous models that have separate or\nuntied predictions for these tasks. When compared against state-of-the-art\nbaselines on joint entity and relation extraction benchmarks, our model,\nGraphER, achieves competitive results.", "published": "2024-04-18 20:09:37", "link": "http://arxiv.org/abs/2404.12491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EnriCo: Enriched Representation and Globally Constrained Inference for\n  Entity and Relation Extraction", "abstract": "Joint entity and relation extraction plays a pivotal role in various\napplications, notably in the construction of knowledge graphs. Despite recent\nprogress, existing approaches often fall short in two key aspects: richness of\nrepresentation and coherence in output structure. These models often rely on\nhandcrafted heuristics for computing entity and relation representations,\npotentially leading to loss of crucial information. Furthermore, they disregard\ntask and/or dataset-specific constraints, resulting in output structures that\nlack coherence. In our work, we introduce EnriCo, which mitigates these\nshortcomings. Firstly, to foster rich and expressive representation, our model\nleverage attention mechanisms that allow both entities and relations to\ndynamically determine the pertinent information required for accurate\nextraction. Secondly, we introduce a series of decoding algorithms designed to\ninfer the highest scoring solutions while adhering to task and dataset-specific\nconstraints, thus promoting structured and coherent outputs. Our model\ndemonstrates competitive performance compared to baselines when evaluated on\nJoint IE datasets.", "published": "2024-04-18 20:15:48", "link": "http://arxiv.org/abs/2404.12493v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Solve Real-World Planning Rigorously with\n  Formal Verification Tools", "abstract": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning.", "published": "2024-04-18 04:36:37", "link": "http://arxiv.org/abs/2404.11891v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\n  with High-Quality Lexical and Syntactic Diversity", "abstract": "Paraphrase generation is a pivotal task in natural language processing (NLP).\nExisting datasets in the domain lack syntactic and lexical diversity, resulting\nin paraphrases that closely resemble the source sentences. Moreover, these\ndatasets often contain hate speech and noise, and may unintentionally include\nnon-English language sentences. This research introduces ParaFusion, a\nlarge-scale, high-quality English paraphrase dataset developed using Large\nLanguage Models (LLM) to address these challenges. ParaFusion augments existing\ndatasets with high-quality data, significantly enhancing both lexical and\nsyntactic diversity while maintaining close semantic similarity. It also\nmitigates the presence of hate speech and reduces noise, ensuring a cleaner and\nmore focused English dataset. Results show that ParaFusion offers at least a\n25% improvement in both syntactic and lexical diversity, measured across\nseveral metrics for each data source. The paper also aims to set a gold\nstandard for paraphrase evaluation as it contains one of the most comprehensive\nevaluation strategies to date. The results underscore the potential of\nParaFusion as a valuable resource for improving NLP applications.", "published": "2024-04-18 09:02:45", "link": "http://arxiv.org/abs/2404.12010v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\n  Models", "abstract": "The burgeoning landscape of text-to-image models, exemplified by innovations\nsuch as Midjourney and DALLE 3, has revolutionized content creation across\ndiverse sectors. However, these advancements bring forth critical ethical\nconcerns, particularly with the misuse of open-source models to generate\ncontent that violates societal norms. Addressing this, we introduce\nEthical-Lens, a framework designed to facilitate the value-aligned usage of\ntext-to-image tools without necessitating internal model revision. Ethical-Lens\nensures value alignment in text-to-image models across toxicity and bias\ndimensions by refining user commands and rectifying model outputs. Systematic\nevaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess\nalignment capability. Our experiments reveal that Ethical-Lens enhances\nalignment capabilities to levels comparable with or superior to commercial\nmodels like DALLE 3, ensuring user-generated content adheres to ethical\nstandards while maintaining image quality. This study indicates the potential\nof Ethical-Lens to ensure the sustainable development of open-source\ntext-to-image tools and their beneficial integration into society. Our code is\navailable at https://github.com/yuzhu-cai/Ethical-Lens.", "published": "2024-04-18 11:38:25", "link": "http://arxiv.org/abs/2404.12104v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Non-Invasive Suicide Risk Prediction Through Speech Analysis", "abstract": "The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.", "published": "2024-04-18 12:33:57", "link": "http://arxiv.org/abs/2404.12132v3", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2"], "primary_category": "cs.SD"}
{"title": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\n  Tasks with Collective Wisdom", "abstract": "Federated Learning (FL) has emerged as a promising solution for collaborative\ntraining of large language models (LLMs). However, the integration of LLMs into\nFL introduces new challenges, particularly concerning the evaluation of LLMs.\nTraditional evaluation methods that rely on labeled test sets and\nsimilarity-based metrics cover only a subset of the acceptable answers, thereby\nfailing to accurately reflect the performance of LLMs on generative tasks.\nMeanwhile, although automatic evaluation methods that leverage advanced LLMs\npresent potential, they face critical risks of data leakage due to the need to\ntransmit data to external servers and suboptimal performance on downstream\ntasks due to the lack of domain knowledge. To address these issues, we propose\na Federated Evaluation framework of Large Language Models, named FedEval-LLM,\nthat provides reliable performance measurements of LLMs on downstream tasks\nwithout the reliance on labeled test sets and external tools, thus ensuring\nstrong privacy-preserving capability. FedEval-LLM leverages a consortium of\npersonalized LLMs from participants as referees to provide domain knowledge and\ncollective evaluation capability, thus aligning to the respective downstream\ntasks and mitigating uncertainties and biases associated with a single referee.\nExperimental results demonstrate a significant improvement in the evaluation\ncapability of personalized evaluation models on downstream tasks. When applied\nto FL, these evaluation models exhibit strong agreement with human preference\nand RougeL-score on meticulously curated test sets. FedEval-LLM effectively\novercomes the limitations of traditional metrics and the reliance on external\nservices, making it a promising framework for the evaluation of LLMs within\ncollaborative training scenarios.", "published": "2024-04-18 15:46:26", "link": "http://arxiv.org/abs/2404.12273v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes", "abstract": "We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.", "published": "2024-04-18 17:48:05", "link": "http://arxiv.org/abs/2404.12365v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BLINK: Multimodal Large Language Models Can See but Not Perceive", "abstract": "We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.", "published": "2024-04-18 17:59:54", "link": "http://arxiv.org/abs/2404.12390v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has shown significant improvements in\nvarious natural language processing tasks by integrating the strengths of large\nlanguage models (LLMs) and external knowledge databases. However, RAG\nintroduces long sequence generation and leads to high computation and memory\ncosts. We propose RAGCache, a novel multilevel dynamic caching system tailored\nfor RAG. Our analysis benchmarks current RAG systems, pinpointing the\nperformance bottleneck (i.e., long sequence due to knowledge injection) and\noptimization opportunities (i.e., caching knowledge's intermediate states).\nBased on these insights, we design RAGCache, which organizes the intermediate\nstates of retrieved knowledge in a knowledge tree and caches them in the GPU\nand host memory hierarchy. RAGCache proposes a replacement policy that is aware\nof LLM inference characteristics and RAG retrieval patterns. It also\ndynamically overlaps the retrieval and inference steps to minimize the\nend-to-end latency. We implement RAGCache and evaluate it on vLLM, a\nstate-of-the-art LLM inference system and Faiss, a state-of-the-art vector\ndatabase. The experimental results show that RAGCache reduces the time to first\ntoken (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to\nvLLM integrated with Faiss.", "published": "2024-04-18 18:32:30", "link": "http://arxiv.org/abs/2404.12457v2", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "UIClip: A Data-driven Model for Assessing User Interface Design", "abstract": "User interface (UI) design is a difficult yet important task for ensuring the\nusability, accessibility, and aesthetic qualities of applications. In our\npaper, we develop a machine-learned model, UIClip, for assessing the design\nquality and visual relevance of a UI given its screenshot and natural language\ndescription. To train UIClip, we used a combination of automated crawling,\nsynthetic augmentation, and human ratings to construct a large-scale dataset of\nUIs, collated by description and ranked by design quality. Through training on\nthe dataset, UIClip implicitly learns properties of good and bad designs by i)\nassigning a numerical score that represents a UI design's relevance and quality\nand ii) providing design suggestions. In an evaluation that compared the\noutputs of UIClip and other baselines to UIs rated by 12 human designers, we\nfound that UIClip achieved the highest agreement with ground-truth rankings.\nFinally, we present three example applications that demonstrate how UIClip can\nfacilitate downstream applications that rely on instantaneous assessment of UI\ndesign quality: i) UI code generation, ii) UI design tips generation, and iii)\nquality-aware UI example search.", "published": "2024-04-18 20:43:08", "link": "http://arxiv.org/abs/2404.12500v1", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Adaptive Memory Replay for Continual Learning", "abstract": "Foundation Models (FMs) have become the hallmark of modern AI, however, these\nmodels are trained on massive data, leading to financially expensive training.\nUpdating FMs as new data becomes available is important, however, can lead to\n`catastrophic forgetting', where models underperform on tasks related to data\nsub-populations observed too long ago. This continual learning (CL) phenomenon\nhas been extensively studied, but primarily in a setting where only a small\namount of past data can be stored. We advocate for the paradigm where memory is\nabundant, allowing us to keep all previous data, but computational resources\nare limited. In this setting, traditional replay-based CL approaches are\noutperformed by a simple baseline which replays past data selected uniformly at\nrandom, indicating that this setting necessitates a new approach. We address\nthis by introducing a framework of adaptive memory replay for continual\nlearning, where sampling of past data is phrased as a multi-armed bandit\nproblem. We utilize Bolzmann sampling to derive a method which dynamically\nselects past data for training conditioned on the current task, assuming full\ndata access and emphasizing training efficiency. Through extensive evaluations\non both vision and language pre-training tasks, we demonstrate the\neffectiveness of our approach, which maintains high performance while reducing\nforgetting by up to 10% at no training efficiency cost.", "published": "2024-04-18 22:01:56", "link": "http://arxiv.org/abs/2404.12526v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Is There No Such Thing as a Bad Question? H4R: HalluciBot For\n  Ratiocination, Rewriting, Ranking, and Routing", "abstract": "Hallucination continues to be one of the most critical challenges in the\ninstitutional adoption journey of Large Language Models (LLMs). While prior\nstudies have primarily focused on the post-generation analysis and refinement\nof outputs, this paper centers on the effectiveness of queries in eliciting\naccurate responses from LLMs. We present HalluciBot, a model that estimates the\nquery's propensity to hallucinate before generation, without invoking any LLMs\nduring inference. HalluciBot can serve as a proxy reward model for query\nrewriting, offering a general framework to estimate query quality based on\naccuracy and consensus. In essence, HalluciBot investigates how poorly\nconstructed queries can lead to erroneous outputs - moreover, by employing\nquery rewriting guided by HalluciBot's empirical estimates, we demonstrate that\n95.7% output accuracy can be achieved for Multiple Choice questions. The\ntraining procedure for HalluciBot consists of perturbing 369,837 queries n\ntimes, employing n+1 independent LLM agents, sampling an output from each\nquery, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs,\nand training an encoder classifier. The idea of perturbation is the outcome of\nour ablation studies that measures the increase in output diversity (+12.5\nagreement spread) by perturbing a query in lexically different but semantically\nsimilar ways. Therefore, HalluciBot paves the way to ratiocinate (76.0% test F1\nscore, 46.6% in saved computation on hallucinatory queries), rewrite (+30.2%\npositive class transition from hallucinatory to non-hallucinatory), rank\n(+50.6% positive class transition from hallucinatory to non-hallucinatory), and\nroute queries to effective pipelines.", "published": "2024-04-18 22:56:57", "link": "http://arxiv.org/abs/2404.12535v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Monitoring Critical Infrastructure Facilities During Disasters Using\n  Large Language Models", "abstract": "Critical Infrastructure Facilities (CIFs), such as healthcare and\ntransportation facilities, are vital for the functioning of a community,\nespecially during large-scale emergencies. In this paper, we explore a\npotential application of Large Language Models (LLMs) to monitor the status of\nCIFs affected by natural disasters through information disseminated in social\nmedia networks. To this end, we analyze social media data from two disaster\nevents in two different countries to identify reported impacts to CIFs as well\nas their impact severity and operational status. We employ state-of-the-art\nopen-source LLMs to perform computational tasks including retrieval,\nclassification, and inference, all in a zero-shot setting. Through extensive\nexperimentation, we report the results of these tasks using standard evaluation\nmetrics and reveal insights into the strengths and weaknesses of LLMs. We note\nthat although LLMs perform well in classification tasks, they encounter\nchallenges with inference tasks, especially when the context/prompt is complex\nand lengthy. Additionally, we outline various potential directions for future\nexploration that can be beneficial during the initial adoption phase of LLMs\nfor disaster response tasks.", "published": "2024-04-18 19:41:05", "link": "http://arxiv.org/abs/2404.14432v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in\n  Logical Fallacy Detection", "abstract": "Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.", "published": "2024-04-18 00:20:48", "link": "http://arxiv.org/abs/2405.02318v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political\n  Fact-Checking using Multimodal Large Language Models", "abstract": "The escalating challenge of misinformation, particularly in political\ndiscourse, requires advanced fact-checking solutions; this is even clearer in\nthe more complex scenario of multimodal claims. We tackle this issue using a\nmultimodal large language model in conjunction with retrieval-augmented\ngeneration (RAG), and introduce two novel reasoning techniques: Chain of RAG\n(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by\nextracting both textual and image content, retrieving external information, and\nreasoning subsequent questions to be answered based on prior evidence. We\nachieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique\nby 0.14 points. Human evaluation confirms that the vast majority of our\ngenerated fact-check explanations contain all information from gold standard\ndata.", "published": "2024-04-18 10:25:42", "link": "http://arxiv.org/abs/2404.12065v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.ET", "cs.MA"], "primary_category": "cs.CL"}
{"title": "TIMIT Speaker Profiling: A Comparison of Multi-task learning and\n  Single-task learning Approaches", "abstract": "This study employs deep learning techniques to explore four speaker profiling\ntasks on the TIMIT dataset, namely gender classification, accent\nclassification, age estimation, and speaker identification, highlighting the\npotential and challenges of multi-task learning versus single-task models. The\nmotivation for this research is twofold: firstly, to empirically assess the\nadvantages and drawbacks of multi-task learning over single-task models in the\ncontext of speaker profiling; secondly, to emphasize the undiminished\nsignificance of skillful feature engineering for speaker recognition tasks. The\nfindings reveal challenges in accent classification, and multi-task learning is\nfound advantageous for tasks of similar complexity. Non-sequential features are\nfavored for speaker recognition, but sequential ones can serve as starting\npoints for complex models. The study underscores the necessity of meticulous\nexperimentation and parameter tuning for deep learning models.", "published": "2024-04-18 10:59:54", "link": "http://arxiv.org/abs/2404.12077v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simultaneous Interpretation Corpus Construction by Large Language Models\n  in Distant Language Pair", "abstract": "In Simultaneous Machine Translation (SiMT) systems, training with a\nsimultaneous interpretation (SI) corpus is an effective method for achieving\nhigh-quality yet low-latency systems. However, it is very challenging to curate\nsuch a corpus due to limitations in the abilities of annotators, and hence,\nexisting SI corpora are limited. Therefore, we propose a method to convert\nexisting speech translation corpora into interpretation-style data, maintaining\nthe original word order and preserving the entire source content using Large\nLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in\ntext-to-text and speech-to-text settings with the LLM-SI-Corpus reduces\nlatencies while maintaining the same level of quality as the models trained\nwith offline datasets. The LLM-SI-Corpus is available at\n\\url{https://github.com/yusuke1997/LLM-SI-Corpus}.", "published": "2024-04-18 16:24:12", "link": "http://arxiv.org/abs/2404.12299v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Models: From Notes to Musical Form", "abstract": "While many topics of the learning-based approach to automated music\ngeneration are under active research, musical form is under-researched. In\nparticular, recent methods based on deep learning models generate music that,\nat the largest time scale, lacks any structure. In practice, music longer than\none minute generated by such models is either unpleasantly repetitive or\ndirectionless. Adapting a recent music generation model, this paper proposes a\nnovel method to generate music with form. The experimental results show that\nthe proposed method can generate 2.5-minute-long music that is considered as\npleasant as the music used to train the model. The paper first reviews a recent\nmusic generation method based on language models (transformer architecture). We\ndiscuss why learning musical form by such models is infeasible. Then we discuss\nour proposed method and the experiments.", "published": "2024-04-18 08:07:42", "link": "http://arxiv.org/abs/2404.11976v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy\n  Preservation in Multimodal Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment\ntendencies in multimodal video content, raising serious concerns about privacy\nrisks associated with multimodal data, such as voiceprints and facial images.\nRecent distributed collaborative learning has been verified as an effective\nparadigm for privacy preservation in multimodal tasks. However, they often\noverlook the privacy distinctions among different modalities, struggling to\nstrike a balance between performance and privacy preservation. Consequently, it\nposes an intriguing question of maximizing multimodal utilization to improve\nperformance while simultaneously protecting necessary modalities. This paper\nforms the first attempt at modality-specified (i.e., audio and visual) privacy\npreservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality\ncGAN framework (HyDiscGAN), which learns multimodality alignment to generate\nfake audio and visual features conditioned on shareable de-identified textual\ndata. The objective is to leverage the fake features to approximate real audio\nand visual content to guarantee privacy preservation while effectively\nenhancing performance. Extensive experiments show that compared with the\nstate-of-the-art MSA model, HyDiscGAN can achieve superior or competitive\nperformance while preserving privacy.", "published": "2024-04-18 06:38:02", "link": "http://arxiv.org/abs/2404.11938v1", "categories": ["cs.MM", "cs.DC", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "MIDGET: Music Conditioned 3D Dance Generation", "abstract": "In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model,\nnamed MIDGET based on Dance motion Vector Quantised Variational AutoEncoder\n(VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate\nvibrant and highquality dances that match the music rhythm. To tackle\nchallenges in the field, we introduce three new components: 1) a pre-trained\nmemory codebook based on the Motion VQ-VAE model to store different human pose\ncodes, 2) employing Motion GPT model to generate pose codes with music and\nmotion Encoders, 3) a simple framework for music feature extraction. We compare\nwith existing state-of-the-art models and perform ablation experiments on\nAIST++, the largest publicly available music-dance dataset. Experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\non motion quality and its alignment with the music.", "published": "2024-04-18 10:20:37", "link": "http://arxiv.org/abs/2404.12062v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic Modality and View Selection for Multimodal Emotion Recognition\n  with Missing Modalities", "abstract": "The study of human emotions, traditionally a cornerstone in fields like\npsychology and neuroscience, has been profoundly impacted by the advent of\nartificial intelligence (AI). Multiple channels, such as speech (voice) and\nfacial expressions (image), are crucial in understanding human emotions.\nHowever, AI's journey in multimodal emotion recognition (MER) is marked by\nsubstantial technical challenges. One significant hurdle is how AI models\nmanage the absence of a particular modality - a frequent occurrence in\nreal-world situations. This study's central focus is assessing the performance\nand resilience of two strategies when confronted with the lack of one modality:\na novel multimodal dynamic modality and view selection and a cross-attention\nmechanism. Results on the RECOLA dataset show that dynamic selection-based\nmethods are a promising approach for MER. In the missing modalities scenarios,\nall dynamic selection-based methods outperformed the baseline. The study\nconcludes by emphasizing the intricate interplay between audio and video\nmodalities in emotion prediction, showcasing the adaptability of dynamic\nselection methods in handling missing modalities.", "published": "2024-04-18 15:18:14", "link": "http://arxiv.org/abs/2404.12251v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Artificial Neural Networks to Recognize Speakers Division from\n  Continuous Bengali Speech", "abstract": "Voice based applications are ruling over the era of automation because speech\nhas a lot of factors that determine a speakers information as well as speech.\nModern Automatic Speech Recognition (ASR) is a blessing in the field of\nHuman-Computer Interaction (HCI) for efficient communication among humans and\ndevices using Artificial Intelligence technology. Speech is one of the easiest\nmediums of communication because it has a lot of identical features for\ndifferent speakers. Nowadays it is possible to determine speakers and their\nidentity using their speech in terms of speaker recognition. In this paper, we\npresented a method that will provide a speakers geographical identity in a\ncertain region using continuous Bengali speech. We consider eight different\ndivisions of Bangladesh as the geographical region. We applied the Mel\nFrequency Cepstral Coefficient (MFCC) and Delta features on an Artificial\nNeural Network to classify speakers division. We performed some preprocessing\ntasks like noise reduction and 8-10 second segmentation of raw audio before\nfeature extraction. We used our dataset of more than 45 hours of audio data\nfrom 633 individual male and female speakers. We recorded the highest accuracy\nof 85.44%.", "published": "2024-04-18 10:17:20", "link": "http://arxiv.org/abs/2404.15168v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
