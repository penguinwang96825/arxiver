{"title": "Resolving Word Vagueness with Scenario-guided Adapter for Natural\n  Language Inference", "abstract": "Natural Language Inference (NLI) is a crucial task in natural language\nprocessing that involves determining the relationship between two sentences,\ntypically referred to as the premise and the hypothesis. However, traditional\nNLI models solely rely on the semantic information inherent in independent\nsentences and lack relevant situational visual information, which can hinder a\ncomplete understanding of the intended meaning of the sentences due to the\nambiguity and vagueness of language. To address this challenge, we propose an\ninnovative ScenaFuse adapter that simultaneously integrates large-scale\npre-trained linguistic knowledge and relevant visual information for NLI tasks.\nSpecifically, we first design an image-sentence interaction module to\nincorporate visuals into the attention mechanism of the pre-trained model,\nallowing the two modalities to interact comprehensively. Furthermore, we\nintroduce an image-sentence fusion module that can adaptively integrate visual\ninformation from images and semantic information from sentences. By\nincorporating relevant visual information and leveraging linguistic knowledge,\nour approach bridges the gap between language and vision, leading to improved\nunderstanding and inference capabilities in NLI tasks. Extensive benchmark\nexperiments demonstrate that our proposed ScenaFuse, a scenario-guided\napproach, consistently boosts NLI performance.", "published": "2024-05-21 01:19:52", "link": "http://arxiv.org/abs/2405.12434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot\n  Dialogue State Tracking", "abstract": "We demonstrate substantial performance gains in zero-shot dialogue state\ntracking (DST) by enhancing training data diversity through synthetic data\ngeneration. Existing DST datasets are severely limited in the number of\napplication domains and slot types they cover due to the high costs of data\ncollection, restricting their adaptability to new domains. This work addresses\nthis challenge with a novel, fully automatic data generation approach that\ncreates synthetic zero-shot DST datasets. Distinguished from previous methods,\nour approach can generate dialogues across a massive range of application\ndomains, complete with silver-standard dialogue state annotations and slot\ndescriptions. This technique is used to create the D0T dataset for training\nzero-shot DST models, encompassing an unprecedented 1,000+ domains. Experiments\non the MultiWOZ benchmark show that training models on diverse synthetic data\nimproves Joint Goal Accuracy by 6.7%, achieving results competitive with models\n13.5 times larger than ours.", "published": "2024-05-21 03:04:14", "link": "http://arxiv.org/abs/2405.12468v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SirLLM: Streaming Infinite Retentive LLM", "abstract": "As Large Language Models (LLMs) become increasingly prevalent in various\ndomains, their ability to process inputs of any length and maintain a degree of\nmemory becomes essential. However, the one-off input of overly long texts is\nlimited, as studies have shown that when input lengths exceed the LLMs'\npre-trained text length, there is a dramatic decline in text generation\ncapabilities. Moreover, simply extending the length of pre-training texts is\nimpractical due to the difficulty in obtaining long text data and the\nsubstantial memory consumption costs this would entail for LLMs. Recent efforts\nhave employed streaming inputs to alleviate the pressure of excessively long\ntext inputs, but this approach can significantly impair the model's long-term\nmemory capabilities.\n  Motivated by this challenge, we introduce Streaming Infinite Retentive LLM\n(SirLLM), which allows LLMs to maintain longer memory during infinite-length\ndialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy\nmetric and a memory decay mechanism to filter key phrases, endowing LLMs with\nboth long-lasting and flexible memory. We designed three distinct tasks and\nconstructed three datasets to measure the effectiveness of SirLLM from various\nangles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our\nexperimental results robustly demonstrate that SirLLM can achieve stable and\nsignificant improvements across different LLMs and tasks, compellingly proving\nits effectiveness. When having a coversation, \"A sir could forget himself,\" but\nSirLLM never does! Our code is publicly available at\nhttps://github.com/Zoeyyao27/SirLLM", "published": "2024-05-21 06:37:03", "link": "http://arxiv.org/abs/2405.12528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM\n  Inference", "abstract": "Large Language Models (LLMs) have shown remarkable comprehension abilities\nbut face challenges in GPU memory usage during inference, hindering their\nscalability for real-time applications like chatbots. To accelerate inference,\nwe store computed keys and values (KV cache) in the GPU memory. Existing\nmethods study the KV cache compression to reduce memory by pruning the\npre-computed KV cache. However, they neglect the inter-layer dependency between\nlayers and huge memory consumption in pre-computation. To explore these\ndeficiencies, we find that the number of crucial keys and values that influence\nfuture generations decreases layer by layer and we can extract them by the\nconsistency in attention weights. Based on the findings, we propose\nPyramidInfer, a method that compresses the KV cache by layer-wise retaining\ncrucial context. PyramidInfer saves significant memory by computing fewer keys\nand values without sacrificing performance. Experimental results show\nPyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU\nmemory reduction in KV cache.", "published": "2024-05-21 06:46:37", "link": "http://arxiv.org/abs/2405.12532v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining the Explainability and Generalization: Fact Verification Based on\n  Self-Instruction", "abstract": "Fact-checking based on commercial LLMs has become mainstream. Although these\nmethods offer high explainability, it falls short in accuracy compared to\ntraditional fine-tuning approaches, and data security is also a significant\nconcern. In this paper, we propose a self-instruction based fine-tuning\napproach for fact-checking that balances accuracy and explainability. Our\nmethod consists of Data Augmentation and Improved DPO fine-tuning. The former\nstarts by instructing the model to generate both positive and negative\nexplanations based on claim-evidence pairs and labels, then sampling the\ndataset according to our customized difficulty standards. The latter employs\nour proposed improved DPO to fine-tune the model using the generated samples.\nWe fine-tune the smallest-scale LLaMA-7B model and evaluate it on the\nchallenging fact-checking datasets FEVEROUS and HOVER, utilizing four\nfine-tuning methods and three few-shot learning methods for comparison. The\nexperiments demonstrate that our approach not only retains accuracy comparable\nto, or even surpassing, traditional fine-tuning methods, but also generates\nfluent explanation text. Moreover, it also exhibit high generalization\nperformance. Our method is the first to leverage self-supervised learning for\nfact-checking and innovatively combines contrastive learning and improved DPO\nin fine-tuning LLMs, as shown in the experiments.", "published": "2024-05-21 08:23:54", "link": "http://arxiv.org/abs/2405.12579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for\n  KV Cache Compression", "abstract": "Key-value~(KV) caching is an important technique to accelerate the inference\nof large language models~(LLMs), but incurs significant memory overhead. To\ncompress the size of KV cache, existing methods often compromise precision or\nrequire extra data for calibration, limiting their practicality in LLM\ndeployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free\nlow-bit quantization technique based on tensor decomposition methods, to\neffectively compress KV cache. Our core idea is to adjust the outlier\ndistribution of the original matrix by performing tensor decomposition, so that\nthe quantization difficulties are migrated from the matrix to decomposed local\ntensors. Specially, we find that outliers mainly concentrate on small local\ntensors, while large tensors tend to have a narrower value range. Based on this\nfinding, we propose to apply low-bit quantization to the large tensor, while\nmaintaining high-precision representation for the small tensor. Furthermore, we\nutilize the proposed quantization method to compress the KV cache of LLMs to\naccelerate the inference and develop an efficient dequantization kernel\ntailored specifically for DecoQuant. Through extensive experiments, DecoQuant\ndemonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\%\nreduction in memory footprint while maintaining comparable generation quality.", "published": "2024-05-21 08:35:10", "link": "http://arxiv.org/abs/2405.12591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental\n  Healthcare", "abstract": "Mental health disorders significantly impact people globally, regardless of\nbackground, education, or socioeconomic status. However, access to adequate\ncare remains a challenge, particularly for underserved communities with limited\nresources. Text mining tools offer immense potential to support mental\nhealthcare by assisting professionals in diagnosing and treating patients. This\nstudy addresses the scarcity of Arabic mental health resources for developing\nsuch tools. We introduce MentalQA, a novel Arabic dataset featuring\nconversational-style question-and-answer (QA) interactions. To ensure data\nquality, we conducted a rigorous annotation process using a well-defined schema\nwith quality control measures. Data was collected from a question-answering\nmedical platform. The annotation schema for mental health questions and\ncorresponding answers draws upon existing classification schemes with some\nmodifications. Question types encompass six distinct categories: diagnosis,\ntreatment, anatomy \\& physiology, epidemiology, healthy lifestyle, and provider\nchoice. Answer strategies include information provision, direct guidance, and\nemotional support. Three experienced annotators collaboratively annotated the\ndata to ensure consistency. Our findings demonstrate high inter-annotator\nagreement, with Fleiss' Kappa of $0.61$ for question types and $0.98$ for\nanswer strategies. In-depth analysis revealed insightful patterns, including\nvariations in question preferences across age groups and a strong correlation\nbetween question types and answer strategies. MentalQA offers a valuable\nfoundation for developing Arabic text mining tools capable of supporting mental\nhealth professionals and individuals seeking information.", "published": "2024-05-21 09:16:38", "link": "http://arxiv.org/abs/2405.12619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Multi-modal Machine Translation: Tasks, Methods and\n  Challenges", "abstract": "In recent years, multi-modal machine translation has attracted significant\ninterest in both academia and industry due to its superior performance. It\ntakes both textual and visual modalities as inputs, leveraging visual context\nto tackle the ambiguities in source texts. In this paper, we begin by offering\nan exhaustive overview of 99 prior works, comprehensively summarizing\nrepresentative studies from the perspectives of dominant models, datasets, and\nevaluation metrics. Afterwards, we analyze the impact of various factors on\nmodel performance and finally discuss the possible research directions for this\ntask in the future. Over time, multi-modal machine translation has developed\nmore types to meet diverse needs. Unlike previous surveys confined to the early\nstage of multi-modal machine translation, our survey thoroughly concludes these\nemerging types from different aspects, so as to provide researchers with a\nbetter understanding of its current state.", "published": "2024-05-21 10:34:47", "link": "http://arxiv.org/abs/2405.12669v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Echoes of Multilinguality: Tracing Cultural Value Shifts during LM\n  Fine-tuning", "abstract": "Texts written in different languages reflect different culturally-dependent\nbeliefs of their writers. Thus, we expect multilingual LMs (MLMs), that are\njointly trained on a concatenation of text in multiple languages, to encode\ndifferent cultural values for each language. Yet, as the 'multilinguality' of\nthese LMs is driven by cross-lingual sharing, we also have reason to belief\nthat cultural values bleed over from one language into another. This limits the\nuse of MLMs in practice, as apart from being proficient in generating text in\nmultiple languages, creating language technology that can serve a community\nalso requires the output of LMs to be sensitive to their biases (Naous et al.,\n2023). Yet, little is known about how cultural values emerge and evolve in MLMs\n(Hershcovich et al., 2022a). We are the first to study how languages can exert\ninfluence on the cultural values encoded for different test languages, by\nstudying how such values are revised during fine-tuning. Focusing on the\nfine-tuning stage allows us to study the interplay between value shifts when\nexposed to new linguistic experience from different data sources and languages.\nLastly, we use a training data attribution method to find patterns in the\nfine-tuning examples, and the languages that they come from, that tend to\ninstigate value shifts.", "published": "2024-05-21 12:55:15", "link": "http://arxiv.org/abs/2405.12744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Have We Achieved on Non-autoregressive Translation?", "abstract": "Recent advances have made non-autoregressive (NAT) translation comparable to\nautoregressive methods (AT). However, their evaluation using BLEU has been\nshown to weakly correlate with human annotations. Limited research compares\nnon-autoregressive translation and autoregressive translation comprehensively,\nleaving uncertainty about the true proximity of NAT to AT. To address this gap,\nwe systematically evaluate four representative NAT methods across various\ndimensions, including human evaluation. Our empirical results demonstrate that\ndespite narrowing the performance gap, state-of-the-art NAT still underperforms\nAT under more reliable evaluation metrics. Furthermore, we discover that\nexplicitly modeling dependencies is crucial for generating natural language and\ngeneralizing to out-of-distribution sequences.", "published": "2024-05-21 13:38:15", "link": "http://arxiv.org/abs/2405.12788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "G-DIG: Towards Gradient-based Diverse and High-quality Instruction Data\n  Selection for Machine Translation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in\ngeneral scenarios. Instruction finetuning empowers them to align with humans in\nvarious tasks. Nevertheless, the Diversity and Quality of the instruction data\nremain two main challenges for instruction finetuning. With regard to this, in\nthis paper, we propose a novel gradient-based method to automatically select\nhigh-quality and diverse instruction finetuning data for machine translation.\nOur key innovation centers around analyzing how individual training examples\ninfluence the model during training. Specifically, we select training examples\nthat exert beneficial influences on the model as high-quality ones by means of\nInfluence Function plus a small high-quality seed dataset. Moreover, to enhance\nthe diversity of the training data we maximize the variety of influences they\nhave on the model by clustering on their gradients and resampling. Extensive\nexperiments on WMT22 and FLORES translation tasks demonstrate the superiority\nof our methods, and in-depth analysis further validates their effectiveness and\ngeneralization.", "published": "2024-05-21 16:38:13", "link": "http://arxiv.org/abs/2405.12915v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-mixed Sentiment and Hate-speech Prediction", "abstract": "Code-mixed discourse combines multiple languages in a single text. It is\ncommonly used in informal discourse in countries with several official\nlanguages, but also in many other countries in combination with English or\nneighboring languages. As recently large language models have dominated most\nnatural language processing tasks, we investigated their performance in\ncode-mixed settings for relevant tasks. We first created four new bilingual\npre-trained masked language models for English-Hindi and English-Slovene\nlanguages, specifically aimed to support informal language. Then we performed\nan evaluation of monolingual, bilingual, few-lingual, and massively\nmultilingual models on several languages, using two tasks that frequently\ncontain code-mixed text, in particular, sentiment analysis and offensive\nlanguage detection in social media texts. The results show that the most\nsuccessful classifiers are fine-tuned bilingual models and multilingual models,\nspecialized for social media texts, followed by non-specialized massively\nmultilingual and monolingual models, while huge generative models are not\ncompetitive. For our affective problems, the models mostly perform slightly\nbetter on code-mixed data compared to non-code-mixed data.", "published": "2024-05-21 16:56:36", "link": "http://arxiv.org/abs/2405.12929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer\n  Selection in Large Language Models", "abstract": "Recent advancements in Chain-of-Thought prompting have facilitated\nsignificant breakthroughs for Large Language Models (LLMs) in complex reasoning\ntasks. Current research enhances the reasoning performance of LLMs by sampling\nmultiple reasoning chains and ensembling based on the answer frequency.\nHowever, this approach fails in scenarios where the correct answers are in the\nminority. We identify this as a primary factor constraining the reasoning\ncapabilities of LLMs, a limitation that cannot be resolved solely based on the\npredicted answers. To address this shortcoming, we introduce a hierarchical\nreasoning aggregation framework AoR (Aggregation of Reasoning), which selects\nanswers based on the evaluation of reasoning chains. Additionally, AoR\nincorporates dynamic sampling, adjusting the number of reasoning chains in\naccordance with the complexity of the task. Experimental results on a series of\ncomplex reasoning tasks show that AoR outperforms prominent ensemble methods.\nFurther analysis reveals that AoR not only adapts various LLMs but also\nachieves a superior performance ceiling when compared to current methods.", "published": "2024-05-21 17:12:19", "link": "http://arxiv.org/abs/2405.12939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atomic Self-Consistency for Better Long Form Generations", "abstract": "Recent work has aimed to improve LLM generations by filtering out\nhallucinations, thereby improving the precision of the information in\nresponses. Correctness of a long-form response, however, also depends on the\nrecall of multiple pieces of information relevant to the question. In this\npaper, we introduce Atomic Self-Consistency (ASC), a technique for improving\nthe recall of relevant information in an LLM response. ASC follows recent work,\nUniversal Self-Consistency (USC) in using multiple stochastic samples from an\nLLM to improve the long-form response. Unlike USC which only focuses on\nselecting the best single generation, ASC picks authentic subparts from the\nsamples and merges them into a superior composite answer. Through extensive\nexperiments and ablations, we show that merging relevant subparts of multiple\nsamples performs significantly better than picking a single sample. ASC\ndemonstrates significant gains over USC on multiple factoids and open-ended QA\ndatasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama2. Our analysis\nalso reveals untapped potential for enhancing long-form generations using\napproach of merging multiple samples.", "published": "2024-05-21 18:05:44", "link": "http://arxiv.org/abs/2405.13131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented\n  Generation and Readability Control for Layman Summarization of Biomedical\n  Texts", "abstract": "This paper introduces the RAG-RLRC-LaySum framework, designed to make complex\nbiomedical research understandable to laymen through advanced Natural Language\nProcessing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,\nenhanced by a reranking method, utilizes multiple knowledge sources to ensure\nthe precision and pertinence of lay summaries. Additionally, our Reinforcement\nLearning for Readability Control (RLRC) strategy improves readability, making\nscientific content comprehensible to non-specialists. Evaluations using the\npublicly accessible PLOS and eLife datasets show that our methods surpass Plain\nGemini model, demonstrating a 20% increase in readability scores, a 15%\nimprovement in ROUGE-2 relevance scores, and a 10% enhancement in factual\naccuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific\nknowledge, enhancing public engagement with biomedical discoveries.", "published": "2024-05-21 20:03:40", "link": "http://arxiv.org/abs/2405.13179v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equipping Transformer with Random-Access Reading for Long-Context\n  Understanding", "abstract": "Long-context modeling presents a significant challenge for transformer-based\nlarge language models (LLMs) due to the quadratic complexity of the\nself-attention mechanism and issues with length extrapolation caused by\npretraining exclusively on short inputs. Existing methods address computational\ncomplexity through techniques such as text chunking, the kernel approach, and\nstructured attention, and tackle length extrapolation problems through\npositional encoding, continued pretraining, and data engineering. These\napproaches typically require $\\textbf{sequential access}$ to the document,\nnecessitating reading from the first to the last token. We contend that for\ngoal-oriented reading of long documents, such sequential access is not\nnecessary, and a proficiently trained model can learn to omit hundreds of less\npertinent tokens. Inspired by human reading behaviors and existing empirical\nobservations, we propose $\\textbf{random access}$, a novel reading strategy\nthat enables transformers to efficiently process long documents without\nexamining every token. Experimental results from pretraining, fine-tuning, and\ninference phases validate the efficacy of our method.", "published": "2024-05-21 21:41:07", "link": "http://arxiv.org/abs/2405.13216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MELD-ST: An Emotion-aware Speech Translation Dataset", "abstract": "Emotion plays a crucial role in human conversation. This paper underscores\nthe significance of considering emotion in speech translation. We present the\nMELD-ST dataset for the emotion-aware speech translation task, comprising\nEnglish-to-Japanese and English-to-German language pairs. Each language pair\nincludes about 10,000 utterances annotated with emotion labels from the MELD\ndataset. Baseline experiments using the SeamlessM4T model on the dataset\nindicate that fine-tuning with emotion labels can enhance translation\nperformance in some settings, highlighting the need for further research in\nemotion-aware speech translation systems.", "published": "2024-05-21 22:40:38", "link": "http://arxiv.org/abs/2405.13233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification\n  in Language Models", "abstract": "This paper introduces an efficient and robust method for discovering\ninterpretable circuits in large language models using discrete sparse\nautoencoders. Our approach addresses key limitations of existing techniques,\nnamely computational complexity and sensitivity to hyperparameters. We propose\ntraining sparse autoencoders on carefully designed positive and negative\nexamples, where the model can only correctly predict the next token for the\npositive examples. We hypothesise that learned representations of attention\nhead outputs will signal when a head is engaged in specific computations. By\ndiscretising the learned representations into integer codes and measuring the\noverlap between codes unique to positive examples for each head, we enable\ndirect identification of attention heads involved in circuits without the need\nfor expensive ablations or architectural modifications. On three well-studied\ntasks - indirect object identification, greater-than comparisons, and docstring\ncompletion - the proposed method achieves higher precision and recall in\nrecovering ground-truth circuits compared to state-of-the-art baselines, while\nreducing runtime from hours to seconds. Notably, we require only 5-10 text\nexamples for each task to learn robust representations. Our findings highlight\nthe promise of discrete sparse autoencoders for scalable and efficient\nmechanistic interpretability, offering a new direction for analysing the inner\nworkings of large language models.", "published": "2024-05-21 06:26:10", "link": "http://arxiv.org/abs/2405.12522v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model\n  Against LLM Red-Teaming", "abstract": "With the proliferation of red-teaming strategies for Large Language Models\n(LLMs), the deficiency in the literature about improving the safety and\nrobustness of LLM defense strategies is becoming increasingly pronounced. This\npaper introduces the LLM-based \\textbf{sentinel} model as a plug-and-play\nprefix module designed to reconstruct the input prompt with just a few ($<30$)\nadditional tokens, effectively reducing toxicity in responses from target LLMs.\nThe sentinel model naturally overcomes the \\textit{parameter inefficiency} and\n\\textit{limited model accessibility} for fine-tuning large target models. We\nemploy an interleaved training regimen using Proximal Policy Optimization (PPO)\nto optimize both red team and sentinel models dynamically, incorporating a\nvalue head-sharing mechanism inspired by the multi-agent centralized critic to\nmanage the complex interplay between agents. Our extensive experiments across\ntext-to-text and text-to-image demonstrate the effectiveness of our approach in\nmitigating toxic outputs, even when dealing with larger models like\n\\texttt{Llama-2}, \\texttt{GPT-3.5} and \\texttt{Stable-Diffusion}, highlighting\nthe potential of our framework in enhancing safety and robustness in various\napplications.", "published": "2024-05-21 08:57:44", "link": "http://arxiv.org/abs/2405.12604v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying Semantic Emergence in Language Models", "abstract": "Large language models (LLMs) are widely recognized for their exceptional\ncapacity to capture semantics meaning. Yet, there remains no established metric\nto quantify this capability. In this work, we introduce a quantitative metric,\nInformation Emergence (IE), designed to measure LLMs' ability to extract\nsemantics from input tokens. We formalize ``semantics'' as the meaningful\ninformation abstracted from a sequence of tokens and quantify this by comparing\nthe entropy reduction observed for a sequence of tokens (macro-level) and\nindividual tokens (micro-level). To achieve this, we design a lightweight\nestimator to compute the mutual information at each transformer layer, which is\nagnostic to different tasks and language model architectures. We apply IE in\nboth synthetic in-context learning (ICL) scenarios and natural sentence\ncontexts. Experiments demonstrate informativeness and patterns about semantics.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.", "published": "2024-05-21 09:12:20", "link": "http://arxiv.org/abs/2405.12617v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploration of Masked and Causal Language Modelling for Text Generation", "abstract": "Large Language Models (LLMs) have revolutionised the field of Natural\nLanguage Processing (NLP) and have achieved state-of-the-art performance in\npractically every task in this field. However, the prevalent approach used in\ntext generation, Causal Language Modelling (CLM), which generates text\nsequentially from left to right, inherently limits the freedom of the model,\nwhich does not decide when and where each token is generated. In contrast,\nMasked Language Modelling (MLM), primarily used for language understanding\ntasks, can generate tokens anywhere in the text and any order. This paper\nconducts an extensive comparison of MLM and CLM approaches for text generation\ntasks. To do so, we pre-train several language models of comparable sizes on\nthree different datasets, namely 1) medical discharge summaries, 2) movie plot\nsynopses, and 3) authorship verification datasets. To assess the quality of the\ngenerations, we first employ quantitative metrics and then perform a\nqualitative human evaluation to analyse coherence and grammatical correctness.\nIn addition, we evaluate the usefulness of the generated texts by using them in\nthree different downstream tasks: 1) Entity Recognition, 2) Text\nClassification, and 3) Authorship Verification. The results show that MLM\nconsistently outperforms CLM in text generation across all datasets, with\nhigher quantitative scores and better coherence in the generated text. The\nstudy also finds \\textit{no strong correlation} between the quality of the\ngenerated text and the performance of the models in the downstream tasks. With\nthis study, we show that MLM for text generation has great potential for future\nresearch and provides direction for future studies in this area.", "published": "2024-05-21 09:33:31", "link": "http://arxiv.org/abs/2405.12630v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge\n  Graph Link Prediction", "abstract": "Extrapolation in Large language models (LLMs) for open-ended inquiry\nencounters two pivotal issues: (1) hallucination and (2) expensive training\ncosts. These issues present challenges for LLMs in specialized domains and\npersonalized data, requiring truthful responses and low fine-tuning costs.\nExisting works attempt to tackle the problem by augmenting the input of a\nsmaller language model with information from a knowledge graph (KG). However,\nthey have two limitations: (1) failing to extract relevant information from a\nlarge one-hop neighborhood in KG and (2) applying the same augmentation\nstrategy for KGs with different characteristics that may result in low\nperformance. Moreover, open-ended inquiry typically yields multiple responses,\nfurther complicating extrapolation. We propose a new task, the extreme\nmulti-label KG link prediction task, to enable a model to perform extrapolation\nwith multiple responses using structured real-world knowledge. Our retriever\nidentifies relevant one-hop neighbors by considering entity, relation, and\ntextual data together. Our experiments demonstrate that (1) KGs with different\ncharacteristics require different augmenting strategies, and (2) augmenting the\nlanguage model's input with textual data improves task performance\nsignificantly. By incorporating the retrieval-augmented framework with KG, our\nframework, with a small parameter size, is able to extrapolate based on a given\nKG. The code can be obtained on GitHub:\nhttps://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git", "published": "2024-05-21 10:10:56", "link": "http://arxiv.org/abs/2405.12656v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text", "abstract": "AI-generated text detection has attracted increasing attention as powerful\nlanguage models approach human-level generation. Limited work is devoted to\ndetecting (partially) AI-paraphrased texts. However, AI paraphrasing is\ncommonly employed in various application scenarios for text refinement and\ndiversity. To this end, we propose a novel detection framework, paraphrased\ntext span detection (PTD), aiming to identify paraphrased text spans within a\ntext. Different from text-level detection, PTD takes in the full text and\nassigns each of the sentences with a score indicating the paraphrasing degree.\nWe construct a dedicated dataset, PASTED, for paraphrased text span detection.\nBoth in-distribution and out-of-distribution results demonstrate the\neffectiveness of PTD models in identifying AI-paraphrased text spans.\nStatistical and model analysis explains the crucial role of the surrounding\ncontext of the paraphrased text spans. Extensive experiments show that PTD\nmodels can generalize to versatile paraphrasing prompts and multiple\nparaphrased text spans. We release our resources at\nhttps://github.com/Linzwcs/PASTED.", "published": "2024-05-21 11:22:27", "link": "http://arxiv.org/abs/2405.12689v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "abstract": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.", "published": "2024-05-21 11:50:16", "link": "http://arxiv.org/abs/2405.12701v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RecGPT: Generative Pre-training for Text-based Recommendation", "abstract": "We present the first domain-adapted and fully-trained large language model,\nRecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for\ntext-based recommendation. Experimental results on rating prediction and\nsequential recommendation tasks show that our model, RecGPT-7B-Instruct,\noutperforms previous strong baselines. We are releasing our RecGPT models as\nwell as their pre-training and fine-tuning datasets to facilitate future\nresearch and downstream applications in text-based recommendation. Public\n\"huggingface\" links to our RecGPT models and datasets are available at:\nhttps://github.com/VinAIResearch/RecGPT", "published": "2024-05-21 12:16:20", "link": "http://arxiv.org/abs/2405.12715v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large Language Models Meet NLP: A Survey", "abstract": "While large language models (LLMs) like ChatGPT have shown impressive\ncapabilities in Natural Language Processing (NLP) tasks, a systematic\ninvestigation of their potential in this field remains largely unexplored. This\nstudy aims to address this gap by exploring the following questions: (1) How\nare LLMs currently applied to NLP tasks in the literature? (2) Have traditional\nNLP tasks already been solved with LLMs? (3) What is the future of the LLMs for\nNLP? To answer these questions, we take the first step to provide a\ncomprehensive overview of LLMs in NLP. Specifically, we first introduce a\nunified taxonomy including (1) parameter-frozen application and (2)\nparameter-tuning application to offer a unified perspective for understanding\nthe current progress of LLMs in NLP. Furthermore, we summarize the new\nfrontiers and the associated challenges, aiming to inspire further\ngroundbreaking advancements. We hope this work offers valuable insights into\nthe {potential and limitations} of LLMs in NLP, while also serving as a\npractical guide for building effective LLMs in NLP.", "published": "2024-05-21 14:24:01", "link": "http://arxiv.org/abs/2405.12819v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in\n  Remote Sensing Images", "abstract": "Remote sensing image change captioning (RSICC) aims at generating human-like\nlanguage to describe the semantic changes between bi-temporal remote sensing\nimage pairs. It provides valuable insights into environmental dynamics and land\nmanagement. Unlike conventional change captioning task, RSICC involves not only\nretrieving relevant information across different modalities and generating\nfluent captions, but also mitigating the impact of pixel-level differences on\nterrain change localization. The pixel problem due to long time span decreases\nthe accuracy of generated caption. Inspired by the remarkable generative power\nof diffusion model, we propose a probabilistic diffusion model for RSICC to\nsolve the aforementioned problems. In training process, we construct a noise\npredictor conditioned on cross modal features to learn the distribution from\nthe real caption distribution to the standard Gaussian distribution under the\nMarkov chain. Meanwhile, a cross-mode fusion and a stacking self-attention\nmodule are designed for noise predictor in the reverse process. In testing\nphase, the well-trained noise predictor helps to estimate the mean value of the\ndistribution and generate change captions step by step. Extensive experiments\non the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and\nits individual components. The quantitative results showcase superior\nperformance over existing methods across both traditional and newly augmented\nmetrics. The code and materials will be available online at\nhttps://github.com/Fay-Y/Diffusion-RSCC.", "published": "2024-05-21 15:44:31", "link": "http://arxiv.org/abs/2405.12875v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Investigating Persuasion Techniques in Arabic: An Empirical Study\n  Leveraging Large Language Models", "abstract": "In the current era of digital communication and widespread use of social\nmedia, it is crucial to develop an understanding of persuasive techniques\nemployed in written text. This knowledge is essential for effectively\ndiscerning accurate information and making informed decisions. To address this\nneed, this paper presents a comprehensive empirical study focused on\nidentifying persuasive techniques in Arabic social media content. To achieve\nthis objective, we utilize Pre-trained Language Models (PLMs) and leverage the\nArAlEval dataset, which encompasses two tasks: binary classification to\ndetermine the presence or absence of persuasion techniques, and multi-label\nclassification to identify the specific types of techniques employed in the\ntext. Our study explores three different learning approaches by harnessing the\npower of PLMs: feature extraction, fine-tuning, and prompt engineering\ntechniques. Through extensive experimentation, we find that the fine-tuning\napproach yields the highest results on the aforementioned dataset, achieving an\nf1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our\nanalysis sheds light on an interesting finding. While the performance of the\nGPT model is relatively lower compared to the other approaches, we have\nobserved that by employing few-shot learning techniques, we can enhance its\nresults by up to 20\\%. This offers promising directions for future research and\nexploration in this topic\\footnote{Upon Acceptance, the source code will be\nreleased on GitHub.}.", "published": "2024-05-21 15:55:09", "link": "http://arxiv.org/abs/2405.12884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with\n  Minimal Impact on Coherence and Evasiveness in Dialogue Agents", "abstract": "Recent advancements in open-domain dialogue systems have been propelled by\nthe emergence of high-quality large language models (LLMs) and various\neffective training methodologies. Nevertheless, the presence of toxicity within\nthese models presents a significant challenge that can potentially diminish the\nuser experience. In this study, we introduce an innovative training algorithm,\nan improvement upon direct preference optimization (DPO), called adversarial\nDPO (ADPO). The ADPO algorithm is designed to train models to assign higher\nprobability distributions to preferred responses and lower distributions to\nunsafe responses, which are self-generated using the toxic control token. We\ndemonstrate that ADPO enhances the model's resilience against harmful\nconversations while minimizing performance degradation. Furthermore, we\nillustrate that ADPO offers a more stable training procedure compared to the\ntraditional DPO. To the best of our knowledge, this is the first adaptation of\nthe DPO algorithm that directly incorporates harmful data into the generative\nmodel, thereby reducing the need to artificially create safe dialogue data.", "published": "2024-05-21 16:14:55", "link": "http://arxiv.org/abs/2405.12900v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention", "abstract": "Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible", "published": "2024-05-21 17:59:29", "link": "http://arxiv.org/abs/2405.12981v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented\n  Generation (FutureDial-RAG)", "abstract": "Recently, increasing research interests have focused on retrieval augmented\ngeneration (RAG) to mitigate hallucination for large language models (LLMs).\nFollowing this trend, we launch the FutureDial-RAG challenge at SLT 2024, which\naims at promoting the study of RAG for dialog systems. The challenge builds\nupon the MobileCS2 dataset, a real-life customer service datasets with nearly\n3000 high-quality dialogs containing annotations for knowledge base query and\ncorresponding results. Over the dataset, we define two tasks, track 1 for\nknowledge retrieval and track 2 for response generation, which are core\nresearch questions in dialog systems with RAG. We build baseline systems for\nthe two tracks and design metrics to measure whether the systems can perform\naccurate retrieval and generate informative and coherent response. The baseline\nresults show that it is very challenging to perform well on the two tasks,\nwhich encourages the participating teams and the community to study how to make\nbetter use of RAG for real-life dialog systems.", "published": "2024-05-21 07:35:21", "link": "http://arxiv.org/abs/2405.13084v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-domain Knowledge Graph Collaborative Pre-training and Prompt\n  Tuning for Diverse Downstream Tasks", "abstract": "Knowledge graphs (KGs) provide reliable external knowledge for a wide variety\nof AI tasks in the form of structured triples. Knowledge graph pre-training\n(KGP) aims to pre-train neural networks on large-scale KGs and provide unified\ninterfaces to enhance different downstream tasks, which is a key direction for\nKG management, maintenance, and applications. Existing works often focus on\npurely research questions in open domains, or they are not open source due to\ndata security and privacy in real scenarios. Meanwhile, existing studies have\nnot explored the training efficiency and transferability of KGP models in\ndepth. To address these problems, We propose a framework MuDoK to achieve\nmulti-domain collaborative pre-training and efficient prefix prompt tuning to\nserve diverse downstream tasks like recommendation and text understanding. Our\ndesign is a plug-and-play prompt learning approach that can be flexibly adapted\nto different downstream task backbones. In response to the lack of open-source\nbenchmarks, we constructed a new multi-domain KGP benchmark called KPI with two\nlarge-scale KGs and six different sub-domain tasks to evaluate our method and\nopen-sourced it for subsequent research. We evaluated our approach based on\nconstructed KPI benchmarks using diverse backbone models in heterogeneous\ndownstream tasks. The experimental results show that our framework brings\nsignificant performance gains, along with its generality, efficiency, and\ntransferability.", "published": "2024-05-21 08:22:14", "link": "http://arxiv.org/abs/2405.13085v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Presentations are not always linear! GNN meets LLM for\n  Document-to-Presentation Transformation with Attribution", "abstract": "Automatically generating a presentation from the text of a long document is a\nchallenging and useful problem. In contrast to a flat summary, a presentation\nneeds to have a better and non-linear narrative, i.e., the content of a slide\ncan come from different and non-contiguous parts of the given document.\nHowever, it is difficult to incorporate such non-linear mapping of content to\nslides and ensure that the content is faithful to the document. LLMs are prone\nto hallucination and their performance degrades with the length of the input\ndocument. Towards this, we propose a novel graph based solution where we learn\na graph from the input document and use a combination of graph neural network\nand LLM to generate a presentation with attribution of content for each slide.\nWe conduct thorough experiments to show the merit of our approach compared to\ndirectly using LLMs for this task.", "published": "2024-05-21 13:52:33", "link": "http://arxiv.org/abs/2405.13095v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dataset Mention Extraction in Scientific Articles Using Bi-LSTM-CRF\n  Model", "abstract": "Datasets are critical for scientific research, playing an important role in\nreplication, reproducibility, and efficiency. Researchers have recently shown\nthat datasets are becoming more important for science to function properly,\neven serving as artifacts of study themselves. However, citing datasets is not\na common or standard practice in spite of recent efforts by data repositories\nand funding agencies. This greatly affects our ability to track their usage and\nimportance. A potential solution to this problem is to automatically extract\ndataset mentions from scientific articles. In this work, we propose to achieve\nsuch extraction by using a neural network based on a Bi-LSTM-CRF architecture.\nOur method achieves F1 = 0.885 in social science articles released as part of\nthe Rich Context Dataset. We discuss the limitations of the current datasets\nand propose modifications to the model to be done in the future.", "published": "2024-05-21 18:12:37", "link": "http://arxiv.org/abs/2405.13135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs for Mathematical Modeling: Towards Bridging the Gap between Natural\n  and Mathematical Languages", "abstract": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious natural language processing tasks, yet their proficiency in\nmathematical reasoning remains a key challenge. Addressing the gap between\nnatural and mathematical language requires advanced reasoning capabilities,\napproaching those of Artificial General Intelligence (AGI). However, the\nevaluation remains challenging, as perfectly representing reality is inherently\nelusive, and traditional methods like manual or direct comparison of\nmathematical statements (Ramamonjison et al., 2023) are insufficient for\nassessing true modeling ability. We propose a process-oriented framework to\nevaluate LLMs' ability to construct mathematical models, using solvers to\ncompare outputs with ground truth. Introducing Mamo, a benchmark with 1,209\nquestions covering ordinary differential equations, linear programming, and\nmixed-integer linear programming, we enable automatic evaluation of modeling\naccuracy. The results show that existing LLMs struggle with complex\nmathematical modeling tasks, with larger models demonstrating superior\nperformance, while open-source models remain competitive in simpler cases but\nstill fall short of proprietary models in more challenging problems.", "published": "2024-05-21 18:29:54", "link": "http://arxiv.org/abs/2405.13144v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Comparative Analysis of Different Efficient Fine Tuning Methods of Large\n  Language Models (LLMs) in Low-Resource Setting", "abstract": "In the domain of large language models (LLMs), arXiv:2305.16938 showed that\nfew-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and\nPattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize\nsimilarly on Out-Of-Domain (OOD) datasets, but vary in terms of task\nadaptation. However, they both pose challenges, especially in term of memory\nrequirements. In this paper, we further try to push the understanding of\ndifferent fine-tuning strategies for LLM and aim to bring a myriad of these on\nthe same pedestal for an elaborate comparison with full-model fine-tuning on\ntwo diverse datasets. To that end, we conducted a series of experiments,\nbeginning with state-of-the-art methods like vanilla fine-tuning and\nPattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets,\nCOLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of\nLoRA adapters in a few-shot setting. Finally, we also compare an alternative\napproach that has gained recent popularity -- context distillation -- with the\nvanilla FT and PBFT with and without few-shot setup.\n  Our findings suggest that these alternative strategies that we explored can\nexhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.\nPBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the\nneed for effective prompts. Further, our adaptive-fine tuning and LoRA\nexperiments perform comparable or slightly worse than the standard fine-tunings\nas anticipated, since standard fine-tunings involve tuning the entire model.\nFinally, our context distillation experiments out-perform the standard\nfine-tuning methods. These findings underscore that eventually the choice of an\nappropriate fine-tuning method depends on the available resources (memory,\ncompute, data) and task adaptability.", "published": "2024-05-21 20:08:52", "link": "http://arxiv.org/abs/2405.13181v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Real-Time Interactive Conversations as Timed Diarized\n  Transcripts", "abstract": "Chatbots built upon language models have exploded in popularity, but they\nhave largely been limited to synchronous, turn-by-turn dialogues. In this paper\nwe present a simple yet general method to simulate real-time interactive\nconversations using pretrained text-only language models, by modeling timed\ndiarized transcripts and decoding them with causal rejection sampling. We\ndemonstrate the promise of this method with two case studies: instant messenger\ndialogues and spoken conversations, which require generation at about 30 tok/s\nand 20 tok/s respectively to maintain real-time interactivity. These\ncapabilities can be added into language models using relatively little data and\nrun on commodity hardware.", "published": "2024-05-21 21:14:31", "link": "http://arxiv.org/abs/2405.13203v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Investigating Symbolic Capabilities of Large Language Models", "abstract": "Prompting techniques have significantly enhanced the capabilities of Large\nLanguage Models (LLMs) across various complex tasks, including reasoning,\nplanning, and solving math word problems. However, most research has\npredominantly focused on language-based reasoning and word problems, often\noverlooking the potential of LLMs in handling symbol-based calculations and\nreasoning. This study aims to bridge this gap by rigorously evaluating LLMs on\na series of symbolic tasks, such as addition, multiplication, modulus\narithmetic, numerical precision, and symbolic counting. Our analysis\nencompasses eight LLMs, including four enterprise-grade and four open-source\nmodels, of which three have been pre-trained on mathematical tasks. The\nassessment framework is anchored in Chomsky's Hierarchy, providing a robust\nmeasure of the computational abilities of these models. The evaluation employs\nminimally explained prompts alongside the zero-shot Chain of Thoughts\ntechnique, allowing models to navigate the solution process autonomously. The\nfindings reveal a significant decline in LLMs' performance on context-free and\ncontext-sensitive symbolic tasks as the complexity, represented by the number\nof symbols, increases. Notably, even the fine-tuned GPT3.5 exhibits only\nmarginal improvements, mirroring the performance trends observed in other\nmodels. Across the board, all models demonstrated a limited generalization\nability on these symbol-intensive tasks. This research underscores LLMs'\nchallenges with increasing symbolic complexity and highlights the need for\nspecialized training, memory and architectural adjustments to enhance their\nproficiency in symbol-based reasoning tasks.", "published": "2024-05-21 21:24:34", "link": "http://arxiv.org/abs/2405.13209v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Reliable AI Chatbots are for Disease Prediction from Patient\n  Complaints?", "abstract": "Artificial Intelligence (AI) chatbots leveraging Large Language Models (LLMs)\nare gaining traction in healthcare for their potential to automate patient\ninteractions and aid clinical decision-making. This study examines the\nreliability of AI chatbots, specifically GPT 4.0, Claude 3 Opus, and Gemini\nUltra 1.0, in predicting diseases from patient complaints in the emergency\ndepartment. The methodology includes few-shot learning techniques to evaluate\nthe chatbots' effectiveness in disease prediction. We also fine-tune the\ntransformer-based model BERT and compare its performance with the AI chatbots.\nResults suggest that GPT 4.0 achieves high accuracy with increased few-shot\ndata, while Gemini Ultra 1.0 performs well with fewer examples, and Claude 3\nOpus maintains consistent performance. BERT's performance, however, is lower\nthan all the chatbots, indicating limitations due to limited labeled data.\nDespite the chatbots' varying accuracy, none of them are sufficiently reliable\nfor critical medical decision-making, underscoring the need for rigorous\nvalidation and human oversight. This study reflects that while AI chatbots have\npotential in healthcare, they should complement, not replace, human expertise\nto ensure patient safety. Further refinement and research are needed to improve\nAI-based healthcare applications' reliability for disease prediction.", "published": "2024-05-21 22:00:13", "link": "http://arxiv.org/abs/2405.13219v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length\n  Curriculum", "abstract": "Large language models (LLMs) are commonly trained on datasets consisting of\nfixed-length token sequences. These datasets are created by randomly\nconcatenating documents of various lengths and then chunking them into\nsequences of a predetermined target length (concat-and-chunk). Recent attention\nimplementations mask cross-document attention, reducing the effective length of\na chunk of tokens. Additionally, training on long sequences becomes\ncomputationally prohibitive due to the quadratic cost of attention. In this\nstudy, we introduce dataset decomposition, a novel variable sequence length\ntraining technique, to tackle these challenges. We decompose a dataset into a\nunion of buckets, each containing sequences of the same size extracted from a\nunique document. During training, we use variable sequence length and\nbatch-size, sampling simultaneously from all buckets with a curriculum. In\ncontrast to the concat-and-chunk baseline, which incurs a fixed attention cost\nat every step of training, our proposed method incurs a computational cost\nproportional to the actual document lengths at each step, resulting in\nsignificant savings in training time. We train an 8k context-length 1B model at\nthe same cost as a 2k context-length model trained with the baseline approach.\nExperiments on a web-scale corpus demonstrate that our approach significantly\nenhances performance on standard language evaluations and long-context\nbenchmarks, reaching target accuracy with up to 6x faster training compared to\nthe baseline. Our method not only enables efficient pretraining on long\nsequences but also scales effectively with dataset size. Lastly, we shed light\non a critical yet less studied aspect of training large language models: the\ndistribution and curriculum of sequence lengths, which results in a\nnon-negligible difference in performance.", "published": "2024-05-21 22:26:01", "link": "http://arxiv.org/abs/2405.13226v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing Biases towards Minoritized Populations in Medical Curricular\n  Content via Artificial Intelligence for Fairer Health Outcomes", "abstract": "Biased information (recently termed bisinformation) continues to be taught in\nmedical curricula, often long after having been debunked. In this paper, we\nintroduce BRICC, a firstin-class initiative that seeks to mitigate medical\nbisinformation using machine learning to systematically identify and flag text\nwith potential biases, for subsequent review in an expert-in-the-loop fashion,\nthus greatly accelerating an otherwise labor-intensive process. A gold-standard\nBRICC dataset was developed throughout several years, and contains over 12K\npages of instructional materials. Medical experts meticulously annotated these\ndocuments for bias according to comprehensive coding guidelines, emphasizing\ngender, sex, age, geography, ethnicity, and race. Using this labeled dataset,\nwe trained, validated, and tested medical bias classifiers. We test three\nclassifier approaches: a binary type-specific classifier, a general bias\nclassifier; an ensemble combining bias type-specific classifiers\nindependently-trained; and a multitask learning (MTL) model tasked with\npredicting both general and type-specific biases. While MTL led to some\nimprovement on race bias detection in terms of F1-score, it did not outperform\nbinary classifiers trained specifically on each task. On general bias\ndetection, the binary classifier achieves up to 0.923 of AUC, a 27.8%\nimprovement over the baseline. This work lays the foundations for debiasing\nmedical curricula by exploring a novel dataset and evaluating different\ntraining model strategies. Hence, it offers new pathways for more nuanced and\neffective mitigation of bisinformation.", "published": "2024-05-21 04:11:18", "link": "http://arxiv.org/abs/2407.12680v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "CoCo Matrix: Taxonomy of Cognitive Contributions in Co-writing with\n  Intelligent Agents", "abstract": "In recent years, there has been a growing interest in employing intelligent\nagents in writing. Previous work emphasizes the evaluation of the quality of\nend product-whether it was coherent and polished, overlooking the journey that\nled to the product, which is an invaluable dimension of the creative process.\nTo understand how to recognize human efforts in co-writing with intelligent\nwriting systems, we adapt Flower and Hayes' cognitive process theory of writing\nand propose CoCo Matrix, a two-dimensional taxonomy of entropy and information\ngain, to depict the new human-agent co-writing model. We define four quadrants\nand situate thirty-four published systems within the taxonomy. Our research\nfound that low entropy and high information gain systems are under-explored,\nyet offer promising future directions in writing tasks that benefit from the\nagent's divergent planning and the human's focused translation. CoCo Matrix,\nnot only categorizes different writing systems but also deepens our\nunderstanding of the cognitive processes in human-agent co-writing. By\nanalyzing minimal changes in the writing process, CoCo Matrix serves as a proxy\nfor the writer's mental model, allowing writers to reflect on their\ncontributions. This reflection is facilitated through the measured metrics of\ninformation gain and entropy, which provide insights irrespective of the\nwriting system used.", "published": "2024-05-21 01:31:17", "link": "http://arxiv.org/abs/2405.12438v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding", "abstract": "Language Models (LMs) excel in understanding textual descriptions of\nproteins, as evident in biomedical question-answering tasks. However, their\ncapability falters with raw protein data, such as amino acid sequences, due to\na deficit in pretraining on such data. Conversely, Protein Language Models\n(PLMs) can understand and convert protein data into high-quality\nrepresentations, but struggle to process texts. To address their limitations,\nwe introduce ProtT3, a framework for Protein-to-Text Generation for Text-based\nProtein Understanding. ProtT3 empowers an LM to understand protein sequences of\namino acids by incorporating a PLM as its protein understanding module,\nenabling effective protein-to-text generation. This collaboration between PLM\nand LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges\nthe modality gap between the PLM's representation space and the LM's input\nspace. Unlike previous studies focusing on protein property prediction and\nprotein-text retrieval, we delve into the largely unexplored field of\nprotein-to-text generation. To facilitate comprehensive benchmarks and promote\nfuture research, we establish quantitative evaluations for protein-text\nmodeling tasks, including protein captioning, protein question-answering, and\nprotein-text retrieval. Our experiments show that ProtT3 substantially\nsurpasses current baselines, with ablation studies further highlighting the\nefficacy of its core components. Our code is available at\nhttps://github.com/acharkq/ProtT3.", "published": "2024-05-21 08:06:13", "link": "http://arxiv.org/abs/2405.12564v1", "categories": ["q-bio.QM", "cs.CL", "cs.MM"], "primary_category": "q-bio.QM"}
{"title": "Tagengo: A Multilingual Chat Dataset", "abstract": "Open source large language models (LLMs) have shown great improvements in\nrecent times. However, many of these models are focused solely on popular\nspoken languages. We present a high quality dataset of more than 70k\nprompt-response pairs in 74 languages which consist of human generated prompts\nand synthetic responses. We use this dataset to train a state-of-the-art open\nsource English LLM to chat multilingually. We evaluate our model on MT-Bench\nchat benchmarks in 6 languages, finding that our multilingual model outperforms\nprevious state-of-the-art open source LLMs across each language. We further\nfind that training on more multilingual data is beneficial to the performance\nin a chosen target language (Japanese) compared to simply training on only data\nin that language. These results indicate the necessity of training on large\namounts of high quality multilingual data to make a more accessible LLM.", "published": "2024-05-21 09:06:36", "link": "http://arxiv.org/abs/2405.12612v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Adaptive Inference for Document Image Classification with\n  Anytime Early Exiting", "abstract": "This work addresses the need for a balanced approach between performance and\nefficiency in scalable production environments for visually-rich document\nunderstanding (VDU) tasks. Currently, there is a reliance on large document\nfoundation models that offer advanced capabilities but come with a heavy\ncomputational burden. In this paper, we propose a multimodal early exit (EE)\nmodel design that incorporates various training strategies, exit layer types\nand placements. Our goal is to achieve a Pareto-optimal balance between\npredictive performance and efficiency for multimodal document image\nclassification. Through a comprehensive set of experiments, we compare our\napproach with traditional exit policies and showcase an improved\nperformance-efficiency trade-off. Our multimodal EE design preserves the\nmodel's predictive capabilities, enhancing both speed and latency. This is\nachieved through a reduction of over 20% in latency, while fully retaining the\nbaseline accuracy. This research represents the first exploration of multimodal\nEE design within the VDU community, highlighting as well the effectiveness of\ncalibration in improving confidence scores for exiting at different layers.\nOverall, our findings contribute to practical VDU applications by enhancing\nboth performance and efficiency.", "published": "2024-05-21 11:52:14", "link": "http://arxiv.org/abs/2405.12705v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From Human-to-Human to Human-to-Bot Conversations in Software\n  Engineering", "abstract": "Software developers use natural language to interact not only with other\nhumans, but increasingly also with chatbots. These interactions have different\nproperties and flow differently based on what goal the developer wants to\nachieve and who they interact with. In this paper, we aim to understand the\ndynamics of conversations that occur during modern software development after\nthe integration of AI and chatbots, enabling a deeper recognition of the\nadvantages and disadvantages of including chatbot interactions in addition to\nhuman conversations in collaborative work. We compile existing conversation\nattributes with humans and NLU-based chatbots and adapt them to the context of\nsoftware development. Then, we extend the comparison to include LLM-powered\nchatbots based on an observational study. We present similarities and\ndifferences between human-to-human and human-to-bot conversations, also\ndistinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how\nunderstanding the differences among the conversation styles guides the\ndeveloper on how to shape their expectations from a conversation and\nconsequently support the communication within a software team. We conclude that\nthe recent conversation styles that we observe with LLM-chatbots can not\nreplace conversations with humans due to certain attributes regarding social\naspects despite their ability to support productivity and decrease the\ndevelopers' mental load.", "published": "2024-05-21 12:04:55", "link": "http://arxiv.org/abs/2405.12712v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.SE"}
{"title": "Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal\n  Utterances", "abstract": "Discovering the semantics of multimodal utterances is essential for\nunderstanding human language and enhancing human-machine interactions. Existing\nmethods manifest limitations in leveraging nonverbal information for discerning\ncomplex semantics in unsupervised scenarios. This paper introduces a novel\nunsupervised multimodal clustering method (UMC), making a pioneering\ncontribution to this field. UMC introduces a unique approach to constructing\naugmentation views for multimodal data, which are then used to perform\npre-training to establish well-initialized representations for subsequent\nclustering. An innovative strategy is proposed to dynamically select\nhigh-quality samples as guidance for representation learning, gauged by the\ndensity of each sample's nearest neighbors. Besides, it is equipped to\nautomatically determine the optimal value for the top-$K$ parameter in each\ncluster to refine sample selection. Finally, both high- and low-quality samples\nare used to learn representations conducive to effective clustering. We build\nbaselines on benchmark multimodal intent and dialogue act datasets. UMC shows\nremarkable improvements of 2-6\\% scores in clustering metrics over\nstate-of-the-art methods, marking the first successful endeavor in this domain.\nThe complete code and data are available at https://github.com/thuiar/UMC.", "published": "2024-05-21 13:24:07", "link": "http://arxiv.org/abs/2405.12775v1", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple\n  Candidates for Efficient and Effective Retrieval", "abstract": "A common retrieve-and-rerank paradigm involves retrieving relevant candidates\nfrom a broad set using a fast bi-encoder (BE), followed by applying expensive\nbut accurate cross-encoders (CE) to a limited candidate set. However, relying\non this small subset is often susceptible to error propagation from the\nbi-encoders, which limits the overall performance. To address these issues, we\npropose the Comparing Multiple Candidates (CMC) framework. CMC compares a query\nand multiple embeddings of similar candidates (i.e., neighbors) through shallow\nself-attention layers, delivering rich representations contextualized to each\nother. Furthermore, CMC is scalable enough to handle multiple comparisons\nsimultaneously. For example, comparing ~10K candidates with CMC takes a similar\namount of time as comparing 16 candidates with CE. Experimental results on the\nZeSHEL dataset demonstrate that CMC, when plugged in between bi-encoders and\ncross-encoders as a seamless intermediate reranker (BE-CMC-CE), can effectively\nimprove recall@k (+4.8%-p, +3.5%-p for R@16, R@64) compared to using only\nbi-encoders (BE-CE), with negligible slowdown (<7%). Additionally, to verify\nCMC's effectiveness as the final-stage reranker in improving top-1 accuracy, we\nconduct experiments on downstream tasks such as entity, passage, and dialogue\nranking. The results indicate that CMC is not only faster (11x) but also often\nmore effective than CE, with improved prediction accuracy in Wikipedia entity\nlinking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p).", "published": "2024-05-21 13:51:48", "link": "http://arxiv.org/abs/2405.12801v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural\n  Language", "abstract": "Machine learning practitioners often face significant challenges in formally\nintegrating their prior knowledge and beliefs into predictive models, limiting\nthe potential for nuanced and context-aware analyses. Moreover, the expertise\nneeded to integrate this prior knowledge into probabilistic modeling typically\nlimits the application of these models to specialists. Our goal is to build a\nregression model that can process numerical data and make probabilistic\npredictions at arbitrary locations, guided by natural language text which\ndescribes a user's prior knowledge. Large Language Models (LLMs) provide a\nuseful starting point for designing such a tool since they 1) provide an\ninterface where users can incorporate expert insights in natural language and\n2) provide an opportunity for leveraging latent problem-relevant knowledge\nencoded in LLMs that users may not have themselves. We start by exploring\nstrategies for eliciting explicit, coherent numerical predictive distributions\nfrom LLMs. We examine these joint predictive distributions, which we call LLM\nProcesses, over arbitrarily-many quantities in settings such as forecasting,\nmulti-dimensional regression, black-box optimization, and image modeling. We\ninvestigate the practical details of prompting to elicit coherent predictive\ndistributions, and demonstrate their effectiveness at regression. Finally, we\ndemonstrate the ability to usefully incorporate text into numerical\npredictions, improving predictive performance and giving quantitative structure\nthat reflects qualitative descriptions. This lets us begin to explore the rich,\ngrounded hypothesis space that LLMs implicitly encode.", "published": "2024-05-21 15:13:12", "link": "http://arxiv.org/abs/2405.12856v5", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Topic Classification of Case Law Using a Large Language Model and a New\n  Taxonomy for UK Law: AI Insights into Summary Judgment", "abstract": "This paper addresses a critical gap in legal analytics by developing and\napplying a novel taxonomy for topic classification of summary judgment cases in\nthe United Kingdom. Using a curated dataset of summary judgment cases, we use\nthe Large Language Model Claude 3 Opus to explore functional topics and trends.\nWe find that Claude 3 Opus correctly classified the topic with an accuracy of\n87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the\napplication of summary judgments across various legal domains. As case law in\nthe United Kingdom is not originally labelled with keywords or a topic\nfiltering option, the findings not only refine our understanding of the\nthematic underpinnings of summary judgments but also illustrate the potential\nof combining traditional and AI-driven approaches in legal classification.\nTherefore, this paper provides a new and general taxonomy for UK law. The\nimplications of this work serve as a foundation for further research and policy\ndiscussions in the field of judicial administration and computational legal\nresearch methodologies.", "published": "2024-05-21 16:30:25", "link": "http://arxiv.org/abs/2405.12910v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in\n  LLMs", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tasks such\nas summarization, arithmetic reasoning, and question answering. However, they\nencounter significant challenges in the domain of moral reasoning and ethical\ndecision-making, especially in complex scenarios with multiple stakeholders.\nThis paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing\nmoral reasoning in LLMs by exploring decisions' consequences from multiple\nstakeholder perspectives. Central to SKIG's mechanism is simulating\naccountability for actions, which, alongside empathy exercises and risk\nassessment, is pivotal to its effectiveness. We validate SKIG's performance\nacross various moral reasoning benchmarks with proprietary and opensource LLMs,\nand investigate its crucial components through extensive ablation analyses.", "published": "2024-05-21 17:04:44", "link": "http://arxiv.org/abs/2405.12933v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "abstract": "Research on jailbreaking has been valuable for testing and understanding the\nsafety and security issues of large language models (LLMs). In this paper, we\nintroduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach\nthat leverages the reflective capabilities of LLMs for jailbreaking with only\nblack-box access. Unlike previous methods, IRIS simplifies the jailbreaking\nprocess by using a single model as both the attacker and target. This method\nfirst iteratively refines adversarial prompts through self-explanation, which\nis crucial for ensuring that even well-aligned LLMs obey adversarial\ninstructions. IRIS then rates and enhances the output given the refined prompt\nto increase its harmfulness. We find that IRIS achieves jailbreak success rates\nof 98% on GPT-4, 92% on GPT-4 Turbo, and 94% on Llama-3.1-70B in under 7\nqueries. It significantly outperforms prior approaches in automatic, black-box,\nand interpretable jailbreaking, while requiring substantially fewer queries,\nthereby establishing a new standard for interpretable jailbreaking methods.", "published": "2024-05-21 03:16:35", "link": "http://arxiv.org/abs/2405.13077v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Towards Retrieval-Augmented Architectures for Image Captioning", "abstract": "The objective of image captioning models is to bridge the gap between the\nvisual and linguistic modalities by generating natural language descriptions\nthat accurately reflect the content of input images. In recent years,\nresearchers have leveraged deep learning-based models and made advances in the\nextraction of visual features and the design of multimodal connections to\ntackle this task. This work presents a novel approach towards developing image\ncaptioning models that utilize an external kNN memory to improve the generation\nprocess. Specifically, we propose two model variants that incorporate a\nknowledge retriever component that is based on visual similarities, a\ndifferentiable encoder to represent input images, and a kNN-augmented language\nmodel to predict tokens based on contextual cues and text retrieved from the\nexternal memory. We experimentally validate our approach on COCO and nocaps\ndatasets and demonstrate that incorporating an explicit external memory can\nsignificantly enhance the quality of captions, especially with a larger\nretrieval corpus. This work provides valuable insights into retrieval-augmented\ncaptioning models and opens up new avenues for improving image captioning at a\nlarger scale.", "published": "2024-05-21 18:02:07", "link": "http://arxiv.org/abs/2405.13127v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Survey of Robotic Language Grounding: Tradeoffs between Symbols and\n  Embeddings", "abstract": "With large language models, robots can understand language more flexibly and\nmore capable than ever before. This survey reviews and situates recent\nliterature into a spectrum with two poles: 1) mapping between language and some\nmanually defined formal representation of meaning, and 2) mapping between\nlanguage and high-dimensional vector spaces that translate directly to\nlow-level robot policy. Using a formal representation allows the meaning of the\nlanguage to be precisely represented, limits the size of the learning problem,\nand leads to a framework for interpretability and formal safety guarantees.\nMethods that embed language and perceptual data into high-dimensional spaces\navoid this manually specified symbolic structure and thus have the potential to\nbe more general when fed enough data but require more data and computing to\ntrain. We discuss the benefits and tradeoffs of each approach and finish by\nproviding directions for future work that achieves the best of both worlds.", "published": "2024-05-21 23:12:03", "link": "http://arxiv.org/abs/2405.13245v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Mamba in Speech: Towards an Alternative to Self-Attention", "abstract": "Transformer and its derivatives have achieved success in diverse tasks across\ncomputer vision, natural language processing, and speech processing. To reduce\nthe complexity of computations within the multi-head self-attention mechanism\nin Transformer, Selective State Space Models (i.e., Mamba) were proposed as an\nalternative. Mamba exhibited its effectiveness in natural language processing\nand computer vision tasks, but its superiority has rarely been investigated in\nspeech signal processing. This paper explores solutions for applying Mamba to\nspeech processing by discussing two typical speech processing tasks: speech\nrecognition, which requires semantic and sequential information, and speech\nenhancement, which focuses primarily on sequential patterns. The experimental\nresults show the superiority of bidirectional Mamba~(BiMamba) for speech\nprocessing to vanilla Mamba. Moreover, experiments demonstrate the\neffectiveness of BiMamba as an alternative to the self-attention module in\nTransformer and its derivates, particularly for the semantic-aware task. The\ncrucial technologies for transferring Mamba to speech are then summarized in\nablation studies and the discussion section to offer insights for future\nresearch.", "published": "2024-05-21 09:04:48", "link": "http://arxiv.org/abs/2405.12609v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey of Integrating Wireless Technology into Active Noise Control", "abstract": "Active Noise Control (ANC) is a widely adopted technology for reducing\nenvironmental noise across various scenarios. This paper focuses on enhancing\nnoise reduction performance, particularly through the refinement of signal\nquality fed into ANC systems. We discuss the main wireless technique integrated\ninto the ANC system, equipped with some innovative algorithms, in diverse\nenvironments. Instead of using microphone arrays, which increase the\ncomputation complexity of the ANC system, to isolate multiple noise sources to\nimprove noise reduction performance, the application of the wireless technique\navoids extra computation demand. Wireless transmissions of reference, error,\nand control signals are also applied to improve the convergence performance of\nthe ANC system. Furthermore, this paper lists some wireless ANC applications,\nsuch as earbuds, headphones, windows, and headrests, underscoring their\nadaptability and efficiency in various settings.", "published": "2024-05-21 04:53:39", "link": "http://arxiv.org/abs/2405.12496v1", "categories": ["eess.AS", "cs.NI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SYMPLEX: Controllable Symbolic Music Generation using Simplex Diffusion\n  with Vocabulary Priors", "abstract": "We present a new approach for fast and controllable generation of symbolic\nmusic based on the simplex diffusion, which is essentially a diffusion process\noperating on probabilities rather than the signal space. This objective has\nbeen applied in domains such as natural language processing but here we apply\nit to generating 4-bar multi-instrument music loops using an orderless\nrepresentation. We show that our model can be steered with vocabulary priors,\nwhich affords a considerable level control over the music generation process,\nfor instance, infilling in time and pitch and choice of instrumentation -- all\nwithout task-specific model adaptation or applying extrinsic control.", "published": "2024-05-21 10:27:34", "link": "http://arxiv.org/abs/2405.12666v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Separation of Vibration Sources using Deep Learning and\n  Deconvolution", "abstract": "Vibrations of rotating machinery primarily originate from two sources, both\nof which are distorted by the machine's transfer function on their way to the\nsensor: the dominant gear-related vibrations and a low-energy signal linked to\nbearing faults. The proposed method facilitates the blind separation of\nvibration sources, eliminating the need for any information about the monitored\nequipment or external measurements. This method estimates both sources in two\nstages: initially, the gear signal is isolated using a dilated CNN, followed by\nthe estimation of the bearing fault signal using the squared log envelope of\nthe residual. The effect of the transfer function is removed from both sources\nusing a novel whitening-based deconvolution method (WBD). Both simulation and\nexperimental results demonstrate the method's ability to detect bearing\nfailures early when no additional information is available. This study\nconsiders both local and distributed bearing faults, assuming that the\nvibrations are recorded under stable operating conditions.", "published": "2024-05-21 13:24:05", "link": "http://arxiv.org/abs/2405.12774v1", "categories": ["cs.LG", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "On a time-frequency blurring operator with applications in data\n  augmentation", "abstract": "Inspired by the success of recent data augmentation methods for signals which\nact on time-frequency representations, we introduce an operator which convolves\nthe short-time Fourier transform of a signal with a specified kernel.\nAnalytical properties including boundedness, compactness and positivity are\ninvestigated from the perspective of time-frequency analysis. A convolutional\nneural network and a vision transformer are trained to classify audio signals\nusing spectrograms with different augmentation setups, including the above\nmentioned time-frequency blurring operator, with results indicating that the\noperator can significantly improve test performance, especially in the\ndata-starved regime.", "published": "2024-05-21 16:14:16", "link": "http://arxiv.org/abs/2405.12899v1", "categories": ["math.FA", "cs.SD", "eess.AS"], "primary_category": "math.FA"}
{"title": "Non-autoregressive real-time Accent Conversion model with voice cloning", "abstract": "Currently, the development of Foreign Accent Conversion (FAC) models utilizes\ndeep neural network architectures, as well as ensembles of neural networks for\nspeech recognition and speech generation. The use of these models is limited by\narchitectural features, which does not allow flexible changes in the timbre of\nthe generated speech and requires the accumulation of context, leading to\nincreased delays in generation and makes these systems unsuitable for use in\nreal-time multi-user communication scenarios. We have developed the\nnon-autoregressive model for real-time accent conversion with voice cloning.\nThe model generates native-sounding L1 speech with minimal latency based on\ninput L2 accented speech. The model consists of interconnected modules for\nextracting accent, gender, and speaker embeddings, converting speech,\ngenerating spectrograms, and decoding the resulting spectrogram into an audio\nsignal. The model has the ability to save, clone and change the timbre, gender\nand accent of the speaker's voice in real time. The results of the objective\nassessment show that the model improves speech quality, leading to enhanced\nrecognition performance in existing ASR systems. The results of subjective\ntests show that the proposed accent and gender encoder improves the generation\nquality. The developed model demonstrates high-quality low-latency accent\nconversion, voice cloning, and speech enhancement capabilities, making it\nsuitable for real-time multi-user communication scenarios.", "published": "2024-05-21 19:07:26", "link": "http://arxiv.org/abs/2405.13162v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FairLENS: Assessing Fairness in Law Enforcement Speech Recognition", "abstract": "Automatic speech recognition (ASR) techniques have become powerful tools,\nenhancing efficiency in law enforcement scenarios. To ensure fairness for\ndemographic groups in different acoustic environments, ASR engines must be\ntested across a variety of speakers in realistic settings. However, describing\nthe fairness discrepancies between models with confidence remains a challenge.\nMeanwhile, most public ASR datasets are insufficient to perform a satisfying\nfairness evaluation. To address the limitations, we built FairLENS - a\nsystematic fairness evaluation framework. We propose a novel and adaptable\nevaluation method to examine the fairness disparity between different models.\nWe also collected a fairness evaluation dataset covering multiple scenarios and\ndemographic dimensions. Leveraging this framework, we conducted fairness\nassessments on 1 open-source and 11 commercially available state-of-the-art ASR\nmodels. Our results reveal that certain models exhibit more biases than others,\nserving as a fairness guideline for users to make informed choices when\nselecting ASR models for a given real-world scenario. We further explored model\nbiases towards specific demographic groups and observed that shifts in the\nacoustic domain can lead to the emergence of new biases.", "published": "2024-05-21 19:23:40", "link": "http://arxiv.org/abs/2405.13166v2", "categories": ["eess.AS", "cs.AI", "cs.CY"], "primary_category": "eess.AS"}
{"title": "A Dataset and Baselines for Measuring and Predicting the Music Piece\n  Memorability", "abstract": "Nowadays, humans are constantly exposed to music, whether through voluntary\nstreaming services or incidental encounters during commercial breaks. Despite\nthe abundance of music, certain pieces remain more memorable and often gain\ngreater popularity. Inspired by this phenomenon, we focus on measuring and\npredicting music memorability. To achieve this, we collect a new music piece\ndataset with reliable memorability labels using a novel interactive\nexperimental procedure. We then train baselines to predict and analyze music\nmemorability, leveraging both interpretable features and audio mel-spectrograms\nas inputs. To the best of our knowledge, we are the first to explore music\nmemorability using data-driven deep learning-based methods. Through a series of\nexperiments and ablation studies, we demonstrate that while there is room for\nimprovement, predicting music memorability with limited data is possible.\nCertain intrinsic elements, such as higher valence, arousal, and faster tempo,\ncontribute to memorable music. As prediction techniques continue to evolve,\nreal-life applications like music recommendation systems and music style\ntransfer will undoubtedly benefit from this new area of research.", "published": "2024-05-21 14:57:04", "link": "http://arxiv.org/abs/2405.12847v1", "categories": ["cs.IR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
