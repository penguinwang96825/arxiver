{"title": "LeafAI: query generator for clinical cohort discovery rivaling a human\n  programmer", "abstract": "Objective: Identifying study-eligible patients within clinical databases is a\ncritical step in clinical research. However, accurate query design typically\nrequires extensive technical and biomedical expertise. We sought to create a\nsystem capable of generating data model-agnostic queries while also providing\nnovel logical reasoning capabilities for complex clinical trial eligibility\ncriteria.\n  Materials and Methods: The task of query creation from eligibility criteria\nrequires solving several text-processing problems, including named entity\nrecognition and relation extraction, sequence-to-sequence transformation,\nnormalization, and reasoning. We incorporated hybrid deep learning and\nrule-based modules for these, as well as a knowledge base of the Unified\nMedical Language System (UMLS) and linked ontologies. To enable data-model\nagnostic query creation, we introduce a novel method for tagging database\nschema elements using UMLS concepts. To evaluate our system, called LeafAI, we\ncompared the capability of LeafAI to a human database programmer to identify\npatients who had been enrolled in 8 clinical trials conducted at our\ninstitution. We measured performance by the number of actual enrolled patients\nmatched by generated queries.\n  Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligible\nacross 8 clinical trials, compared to 27% matched and 14,587 eligible in\nqueries by a human database programmer. The human programmer spent 26 total\nhours crafting queries compared to several minutes by LeafAI.\n  Conclusions: Our work contributes a state-of-the-art data model-agnostic\nquery generation system capable of conditional reasoning using a knowledge\nbase. We demonstrate that LeafAI can rival an experienced human programmer in\nfinding patients eligible for clinical trials.", "published": "2023-04-13 00:34:32", "link": "http://arxiv.org/abs/2304.06203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LasUIE: Unifying Information Extraction with Latent Adaptive\n  Structure-aware Generative Language Model", "abstract": "Universally modeling all typical information extraction tasks (UIE) with one\ngenerative language model (GLM) has revealed great potential by the latest\nstudy, where various IE predictions are unified into a linearized hierarchical\nexpression under a GLM. Syntactic structure information, a type of effective\nfeature which has been extensively utilized in IE community, should also be\nbeneficial to UIE. In this work, we propose a novel structure-aware GLM, fully\nunleashing the power of syntactic knowledge for UIE. A heterogeneous structure\ninductor is explored to unsupervisedly induce rich heterogeneous structural\nrepresentations by post-training an existing GLM. In particular, a structural\nbroadcaster is devised to compact various latent trees into explicit high-order\nforests, helping to guide a better generation during decoding. We finally\nintroduce a task-oriented structure fine-tuning mechanism, further adjusting\nthe learned structures to most coincide with the end-task's need. Over 12 IE\nbenchmarks across 7 tasks our system shows significant improvements over the\nbaseline UIE system. Further in-depth analyses show that our GLM learns rich\ntask-adaptive structural bias that greatly resolves the UIE crux, the\nlong-range dependence issue and boundary identifying. Source codes are open at\nhttps://github.com/ChocoWu/LasUIE.", "published": "2023-04-13 04:01:14", "link": "http://arxiv.org/abs/2304.06248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rule-based detection of access to education and training in Germany", "abstract": "As a result of transformation processes, the German labor market is highly\ndependent on vocational training, retraining and continuing education. To match\ntraining seekers and offers, we present a novel approach towards the automated\ndetection of access to education and training in German training offers and\nadvertisements. We will in particular focus on (a) general school and education\ndegrees and schoolleaving certificates, (b) professional experience, (c) a\nprevious apprenticeship and (d) a list of skills provided by the German Federal\nEmployment Agency. This novel approach combines several methods: First, we\nprovide a mapping of synonyms in education combining different qualifications\nand adding deprecated terms. Second, we provide a rule-based matching to\nidentify the need for professional experience or apprenticeship. However, not\nall access requirements can be matched due to incompatible data schemata or\nnon-standardizes requirements, e.g initial tests or interviews. While we can\nidentify several shortcomings, the presented approach offers promising results\nfor two data sets: training and re-training advertisements.", "published": "2023-04-13 07:32:09", "link": "http://arxiv.org/abs/2304.06307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational modeling of semantic change", "abstract": "In this chapter we provide an overview of computational modeling for semantic\nchange using large and semi-large textual corpora. We aim to provide a key for\nthe interpretation of relevant methods and evaluation techniques, and also\nprovide insights into important aspects of the computational study of semantic\nchange. We discuss the pros and cons of different classes of models with\nrespect to the properties of the data from which one wishes to model semantic\nchange, and which avenues are available to evaluate the results.", "published": "2023-04-13 08:33:50", "link": "http://arxiv.org/abs/2304.06337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs All You Need for Task-Oriented Dialogue?", "abstract": "Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.", "published": "2023-04-13 14:03:14", "link": "http://arxiv.org/abs/2304.06556v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the State of the Art in Legal QA Systems", "abstract": "Answering questions related to the legal domain is a complex task, primarily\ndue to the intricate nature and diverse range of legal document systems.\nProviding an accurate answer to a legal query typically necessitates\nspecialized knowledge in the relevant domain, which makes this task all the\nmore challenging, even for human experts. Question answering (QA) systems are\ndesigned to generate answers to questions asked in human languages. QA uses\nnatural language processing to understand questions and search through\ninformation to find relevant answers. QA has various practical applications,\nincluding customer service, education, research, and cross-lingual\ncommunication. However, QA faces challenges such as improving natural language\nunderstanding and handling complex and ambiguous questions. Answering questions\nrelated to the legal domain is a complex task, primarily due to the intricate\nnature and diverse range of legal document systems. Providing an accurate\nanswer to a legal query typically necessitates specialized knowledge in the\nrelevant domain, which makes this task all the more challenging, even for human\nexperts. At this time, there is a lack of surveys that discuss legal question\nanswering. To address this problem, we provide a comprehensive survey that\nreviews 14 benchmark datasets for question-answering in the legal field as well\nas presents a comprehensive review of the state-of-the-art Legal Question\nAnswering deep learning models. We cover the different architectures and\ntechniques used in these studies and the performance and limitations of these\nmodels. Moreover, we have established a public GitHub repository where we\nregularly upload the most recent articles, open data, and source code. The\nrepository is available at:\n\\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.", "published": "2023-04-13 15:48:01", "link": "http://arxiv.org/abs/2304.06623v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2023 Task 12: Sentiment Analysis for African Languages\n  (AfriSenti-SemEval)", "abstract": "We present the first Africentric SemEval Shared task, Sentiment Analysis for\nAfrican Languages (AfriSenti-SemEval) - The dataset is available at\nhttps://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval\nis a sentiment classification challenge in 14 African languages: Amharic,\nAlgerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican\nPortuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and\nYor\\`ub\\'a (Muhammad et al., 2023), using data labeled with 3 sentiment\nclasses. We present three subtasks: (1) Task A: monolingual classification,\nwhich received 44 submissions; (2) Task B: multilingual classification, which\nreceived 32 submissions; and (3) Task C: zero-shot classification, which\nreceived 34 submissions. The best performance for tasks A and B was achieved by\nNLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP\nachieved the best average score for task C with 58.15 weighted F1. We describe\nthe various approaches adopted by the top 10 systems and their approaches.", "published": "2023-04-13 22:26:10", "link": "http://arxiv.org/abs/2304.06845v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "abstract": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.", "published": "2023-04-13 09:39:30", "link": "http://arxiv.org/abs/2304.06364v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sign Language Translation from Instructional Videos", "abstract": "The advances in automatic sign language translation (SLT) to spoken languages\nhave been mostly benchmarked with datasets of limited size and restricted\ndomains. Our work advances the state of the art by providing the first baseline\nresults on How2Sign, a large and broad dataset.\n  We train a Transformer over I3D video features, using the reduced BLEU as a\nreference metric for validation, instead of the widely used BLEU score. We\nreport a result of 8.03 on the BLEU score, and publish the first open-source\nimplementation of its kind to promote further advances.", "published": "2023-04-13 09:50:43", "link": "http://arxiv.org/abs/2304.06371v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards hypergraph cognitive networks as feature-rich models of\n  knowledge", "abstract": "Semantic networks provide a useful tool to understand how related concepts\nare retrieved from memory. However, most current network approaches use\npairwise links to represent memory recall patterns. Pairwise connections\nneglect higher-order associations, i.e. relationships between more than two\nconcepts at a time. These higher-order interactions might covariate with (and\nthus contain information about) how similar concepts are along psycholinguistic\ndimensions like arousal, valence, familiarity, gender and others. We overcome\nthese limits by introducing feature-rich cognitive hypergraphs as quantitative\nmodels of human memory where: (i) concepts recalled together can all engage in\nhyperlinks involving also more than two concepts at once (cognitive hypergraph\naspect), and (ii) each concept is endowed with a vector of psycholinguistic\nfeatures (feature-rich aspect). We build hypergraphs from word association data\nand use evaluation methods from machine learning features to predict concept\nconcreteness. Since concepts with similar concreteness tend to cluster together\nin human memory, we expect to be able to leverage this structure. Using word\nassociation data from the Small World of Words dataset, we compared a pairwise\nnetwork and a hypergraph with N=3586 concepts/nodes. Interpretable artificial\nintelligence models trained on (1) psycholinguistic features only, (2)\npairwise-based feature aggregations, and on (3) hypergraph-based aggregations\nshow significant differences between pairwise and hypergraph links.\nSpecifically, our results show that higher-order and feature-rich hypergraph\nmodels contain richer information than pairwise networks leading to improved\nprediction of word concreteness. The relation with previous studies about\nconceptual clustering and compartmentalisation in associative knowledge and\nhuman memory are discussed.", "published": "2023-04-13 09:56:45", "link": "http://arxiv.org/abs/2304.06375v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PDFVQA: A New Dataset for Real-World VQA on PDF Documents", "abstract": "Document-based Visual Question Answering examines the document understanding\nof document images in conditions of natural language questions. We proposed a\nnew document-based VQA dataset, PDF-VQA, to comprehensively examine the\ndocument understanding from various aspects, including document element\nrecognition, document layout structural understanding as well as contextual\nunderstanding and key information extraction. Our PDF-VQA dataset extends the\ncurrent scale of document understanding that limits on the single document page\nto the new scale that asks questions over the full document of multiple pages.\nWe also propose a new graph-based VQA model that explicitly integrates the\nspatial and hierarchically structural relationships between different document\nelements to boost the document structural understanding. The performances are\ncompared with several baselines over different question types and\ntasks\\footnote{The full dataset will be released after paper acceptance.", "published": "2023-04-13 12:28:14", "link": "http://arxiv.org/abs/2304.06447v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using\n  Afro-centric Language Models and Adapters for Low-resource African Languages", "abstract": "AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform\nmonolingual sentiment classification (sub-task A) for 12 African languages,\nmultilingual sentiment classification (sub-task B), and zero-shot sentiment\nclassification (task C). For sub-task A, we conducted experiments using\nclassical machine learning classifiers, Afro-centric language models, and\nlanguage-specific models. For task B, we fine-tuned multilingual pre-trained\nlanguage models that support many of the languages in the task. For task C, we\nused we make use of a parameter-efficient Adapter approach that leverages\nmonolingual texts in the target language for effective zero-shot transfer. Our\nfindings suggest that using pre-trained Afro-centric language models improves\nperformance for low-resource African languages. We also ran experiments using\nadapters for zero-shot tasks, and the results suggest that we can obtain\npromising results by using adapters with a limited amount of resources.", "published": "2023-04-13 12:54:29", "link": "http://arxiv.org/abs/2304.06459v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph2topic: an opensource topic modeling framework based on sentence\n  embedding and community detection", "abstract": "It has been reported that clustering-based topic models, which cluster\nhigh-quality sentence embeddings with an appropriate word selection method, can\ngenerate better topics than generative probabilistic topic models. However,\nthese approaches suffer from the inability to select appropriate parameters and\nincomplete models that overlook the quantitative relation between words with\ntopics and topics with text. To solve these issues, we propose graph to topic\n(G2T), a simple but effective framework for topic modelling. The framework is\ncomposed of four modules. First, document representation is acquired using\npretrained language models. Second, a semantic graph is constructed according\nto the similarity between document representations. Third, communities in\ndocument semantic graphs are identified, and the relationship between topics\nand documents is quantified accordingly. Fourth, the word--topic distribution\nis computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\nachieved state-of-the-art performance on both English and Chinese documents\nwith different lengths.", "published": "2023-04-13 16:28:07", "link": "http://arxiv.org/abs/2304.06653v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergence of Symbols in Neural Networks for Semantic Understanding and\n  Communication", "abstract": "The capacity to generate meaningful symbols and effectively employ them for\nadvanced cognitive processes, such as communication, reasoning, and planning,\nconstitutes a fundamental and distinctive aspect of human intelligence.\nExisting deep neural networks still notably lag human capabilities in terms of\ngenerating symbols for higher cognitive functions. Here, we propose a solution\n(symbol emergence artificial network (SEA-net)) to endow neural networks with\nthe ability to create symbols, understand semantics, and achieve communication.\nSEA-net generates symbols that dynamically configure the network to perform\nspecific tasks. These symbols capture compositional semantic information that\nallows the system to acquire new functions purely by symbolic manipulation or\ncommunication. In addition, these self-generated symbols exhibit an intrinsic\nstructure resembling that of natural language, suggesting a common framework\nunderlying the generation and understanding of symbols in both human brains and\nartificial neural networks. We believe that the proposed framework will be\ninstrumental in producing more capable systems that can synergize the strengths\nof connectionist and symbolic approaches for artificial intelligence (AI).", "published": "2023-04-13 10:13:00", "link": "http://arxiv.org/abs/2304.06377v3", "categories": ["cs.AI", "cs.CL", "cs.SC", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "SpectFormer: Frequency and Attention is what you need in a Vision\n  Transformer", "abstract": "Vision transformers have been applied successfully for image recognition\ntasks. There have been either multi-headed self-attention based (ViT\n\\cite{dosovitskiy2020image}, DeIT, \\cite{touvron2021training}) similar to the\noriginal work in textual models or more recently based on spectral layers\n(Fnet\\cite{lee2021fnet}, GFNet\\cite{rao2021global},\nAFNO\\cite{guibas2021efficient}). We hypothesize that both spectral and\nmulti-headed attention plays a major role. We investigate this hypothesis\nthrough this work and observe that indeed combining spectral and multi-headed\nattention layers provides a better transformer architecture. We thus propose\nthe novel Spectformer architecture for transformers that combines spectral and\nmulti-headed attention layers. We believe that the resulting representation\nallows the transformer to capture the feature representation appropriately and\nit yields improved performance over other transformer representations. For\ninstance, it improves the top-1 accuracy by 2\\% on ImageNet compared to both\nGFNet-H and LiT. SpectFormer-S reaches 84.25\\% top-1 accuracy on ImageNet-1K\n(state of the art for small version). Further, Spectformer-L achieves 85.7\\%\nthat is the state of the art for the comparable base version of the\ntransformers. We further ensure that we obtain reasonable results in other\nscenarios such as transfer learning on standard datasets such as CIFAR-10,\nCIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate\nits use in downstream tasks such of object detection and instance segmentation\non the MS-COCO dataset and observe that Spectformer shows consistent\nperformance that is comparable to the best backbones and can be further\noptimized and improved. Hence, we believe that combined spectral and attention\nlayers are what are needed for vision transformers.", "published": "2023-04-13 12:27:17", "link": "http://arxiv.org/abs/2304.06446v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political\n  Twitter Messages with Zero-Shot Learning", "abstract": "This paper assesses the accuracy, reliability and bias of the Large Language\nModel (LLM) ChatGPT-4 on the text analysis task of classifying the political\naffiliation of a Twitter poster based on the content of a tweet. The LLM is\ncompared to manual annotation by both expert classifiers and crowd workers,\ngenerally considered the gold standard for such tasks. We use Twitter messages\nfrom United States politicians during the 2020 election, providing a ground\ntruth against which to measure accuracy. The paper finds that ChatGPT-4 has\nachieves higher accuracy, higher reliability, and equal or lower bias than the\nhuman classifiers. The LLM is able to correctly annotate messages that require\nreasoning on the basis of contextual knowledge, and inferences around the\nauthor's intentions - traditionally seen as uniquely human abilities. These\nfindings suggest that LLM will have substantial impact on the use of textual\ndata in the social sciences, by enabling interpretive research at a scale.", "published": "2023-04-13 14:51:40", "link": "http://arxiv.org/abs/2304.06588v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "PGTask: Introducing the Task of Profile Generation from Dialogues", "abstract": "Recent approaches have attempted to personalize dialogue systems by\nleveraging profile information into models. However, this knowledge is scarce\nand difficult to obtain, which makes the extraction/generation of profile\ninformation from dialogues a fundamental asset. To surpass this limitation, we\nintroduce the Profile Generation Task (PGTask). We contribute with a new\ndataset for this problem, comprising profile sentences aligned with related\nutterances, extracted from a corpus of dialogues. Furthermore, using\nstate-of-the-art methods, we provide a benchmark for profile generation on this\nnovel dataset. Our experiments disclose the challenges of profile generation,\nand we hope that this introduces a new research direction.", "published": "2023-04-13 16:02:19", "link": "http://arxiv.org/abs/2304.06634v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How Useful are Educational Questions Generated by Large Language Models?", "abstract": "Controllable text generation (CTG) by large language models has a huge\npotential to transform education for teachers and students alike. Specifically,\nhigh quality and diverse question generation can dramatically reduce the load\non teachers and improve the quality of their educational content. Recent work\nin this domain has made progress with generation, but fails to show that real\nteachers judge the generated questions as sufficiently useful for the classroom\nsetting; or if instead the questions have errors and/or pedagogically unhelpful\ncontent. We conduct a human evaluation with teachers to assess the quality and\nusefulness of outputs from combining CTG and question taxonomies (Bloom's and a\ndifficulty taxonomy). The results demonstrate that the questions generated are\nhigh quality and sufficiently useful, showing their promise for widespread use\nin the classroom setting.", "published": "2023-04-13 16:05:25", "link": "http://arxiv.org/abs/2304.06638v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image\n  Generation", "abstract": "Spatial control is a core capability in controllable image generation.\nAdvancements in layout-guided image generation have shown promising results on\nin-distribution (ID) datasets with similar spatial configurations. However, it\nis unclear how these models perform when facing out-of-distribution (OOD)\nsamples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,\na diagnostic benchmark for layout-guided image generation that examines four\ncategories of spatial control skills: number, position, size, and shape. We\nbenchmark two recent representative layout-guided image generation methods and\nobserve that the good ID layout control may not generalize well to arbitrary\nlayouts in the wild (e.g., objects at the boundary). Next, we propose\nIterInpaint, a new baseline that generates foreground and background regions\nstep-by-step via inpainting, demonstrating stronger generalizability than\nexisting models on OOD layouts in LayoutBench. We perform quantitative and\nqualitative evaluation and fine-grained analysis on the four LayoutBench skills\nto pinpoint the weaknesses of existing models. We show comprehensive ablation\nstudies on IterInpaint, including training task ratio, crop&paste vs. repaint,\nand generation order. Lastly, we evaluate the zero-shot performance of\ndifferent pretrained layout-guided image generation models on LayoutBench-COCO,\nour new benchmark for OOD layouts with real objects, where our IterInpaint\nconsistently outperforms SOTA baselines in all four splits. Project website:\nhttps://layoutbench.github.io", "published": "2023-04-13 16:58:33", "link": "http://arxiv.org/abs/2304.06671v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Verbs in Action: Improving verb understanding in video-language models", "abstract": "Understanding verbs is crucial to modelling how people and objects interact\nwith each other and the environment through space and time. Recently,\nstate-of-the-art video-language models based on CLIP have been shown to have\nlimited verb understanding and to rely extensively on nouns, restricting their\nperformance in real-world video applications that require action and temporal\nunderstanding. In this work, we improve verb understanding for CLIP-based\nvideo-language models by proposing a new Verb-Focused Contrastive (VFC)\nframework. This consists of two main components: (1) leveraging pretrained\nlarge language models (LLMs) to create hard negatives for cross-modal\ncontrastive learning, together with a calibration strategy to balance the\noccurrence of concepts in positive and negative pairs; and (2) enforcing a\nfine-grained, verb phrase alignment loss. Our method achieves state-of-the-art\nresults for zero-shot performance on three downstream tasks that focus on verb\nunderstanding: video-text matching, video question-answering and video\nclassification. To the best of our knowledge, this is the first work which\nproposes a method to alleviate the verb understanding problem, and does not\nsimply highlight it.", "published": "2023-04-13 17:57:01", "link": "http://arxiv.org/abs/2304.06708v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A\n  Comprehensive Study", "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of\nperplexity by retrieval (e.g., RETRO), but its impact on text generation\nquality and downstream task accuracy is unclear. Thus, it is still an open\nquestion: shall we pretrain large autoregressive LMs with retrieval? To answer\nit, we perform a comprehensive study on a scalable pre-trained\nretrieval-augmented LM (i.e., RETRO) compared with standard GPT and\nretrieval-augmented GPT incorporated at fine-tuning or inference stages. We\nfirst provide the recipe to reproduce RETRO up to 9.5B parameters while\nretrieving a text corpus with 330B tokens. Based on that, we have the following\nnovel findings: i) RETRO outperforms GPT on text generation with much less\ndegeneration (i.e., repetition), moderately higher factual accuracy, and\nslightly lower toxicity with a nontoxic retrieval database. ii) On the LM\nEvaluation Harness benchmark, RETRO largely outperforms GPT on\nknowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,\nwe introduce a simple variant of the model, RETRO++, which largely improves\nopen-domain QA results of original RETRO (e.g., EM score +8.6 on Natural\nQuestion) and significantly outperforms retrieval-augmented GPT in both\nfine-tuning and zero-shot evaluation settings. Our findings highlight the\npromising direction of pretraining autoregressive LMs with retrieval as future\nfoundation models. We release our code and model at:\nhttps://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md", "published": "2023-04-13 18:04:19", "link": "http://arxiv.org/abs/2304.06762v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Sequence Transduction by Jointly Predicting Tokens and\n  Durations", "abstract": "This paper introduces a novel Token-and-Duration Transducer (TDT)\narchitecture for sequence-to-sequence tasks. TDT extends conventional\nRNN-Transducer architectures by jointly predicting both a token and its\nduration, i.e. the number of input frames covered by the emitted token. This is\nachieved by using a joint network with two outputs which are independently\nnormalized to generate distributions over tokens and durations. During\ninference, TDT models can skip input frames guided by the predicted duration\noutput, which makes them significantly faster than conventional Transducers\nwhich process the encoder output frame by frame. TDT models achieve both better\naccuracy and significantly faster inference than conventional Transducers on\ndifferent sequence transduction tasks. TDT models for Speech Recognition\nachieve better accuracy and up to 2.82X faster inference than conventional\nTransducers. TDT models for Speech Translation achieve an absolute gain of over\n1 BLEU on the MUST-C test compared with conventional Transducers, and its\ninference is 2.27X faster. In Speech Intent Classification and Slot Filling\ntasks, TDT models improve the intent accuracy by up to over 1% (absolute) over\nconventional Transducers, while running up to 1.28X faster. Our implementation\nof the TDT model will be open-sourced with the NeMo\n(https://github.com/NVIDIA/NeMo) toolkit.", "published": "2023-04-13 19:38:27", "link": "http://arxiv.org/abs/2304.06795v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Opportunities and Challenges of Foundation Models for Geospatial\n  Artificial Intelligence", "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained\nin a task-agnostic manner on large-scale data and can be adapted to a wide\nrange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.\nDespite their successes in language and vision tasks, we have yet seen an\nattempt to develop foundation models for geospatial artificial intelligence\n(GeoAI). In this work, we explore the promises and challenges of developing\nmultimodal foundation models for GeoAI. We first investigate the potential of\nmany existing FMs by testing their performances on seven tasks across multiple\ngeospatial subdomains including Geospatial Semantics, Health Geography, Urban\nGeography, and Remote Sensing. Our results indicate that on several geospatial\ntasks that only involve text modality such as toponym recognition, location\ndescription recognition, and US state-level/county-level dementia time series\nforecasting, these task-agnostic LLMs can outperform task-specific\nfully-supervised models in a zero-shot or few-shot learning setting. However,\non other geospatial tasks, especially tasks that involve multiple data\nmodalities (e.g., POI-based urban function classification, street view\nimage-based urban noise intensity classification, and remote sensing image\nscene classification), existing foundation models still underperform\ntask-specific models. Based on these observations, we propose that one of the\nmajor challenges of developing a FM for GeoAI is to address the multimodality\nnature of geospatial tasks. After discussing the distinct challenges of each\ngeospatial data modality, we suggest the possibility of a multimodal foundation\nmodel which can reason over various types of geospatial data through geospatial\nalignments. We conclude this paper by discussing the unique risks and\nchallenges to develop such a model for GeoAI.", "published": "2023-04-13 19:50:17", "link": "http://arxiv.org/abs/2304.06798v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.0; I.2.4; I.2.7; I.2.10; I.5.1"], "primary_category": "cs.AI"}
{"title": "Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter", "abstract": "Vaccine hesitancy continues to be a main challenge for public health\nofficials during the COVID-19 pandemic. As this hesitancy undermines vaccine\ncampaigns, many researchers have sought to identify its root causes, finding\nthat the increasing volume of anti-vaccine misinformation on social media\nplatforms is a key element of this problem. We explored Twitter as a source of\nmisleading content with the goal of extracting overlapping cultural and\npolitical beliefs that motivate the spread of vaccine misinformation. To do\nthis, we have collected a data set of vaccine-related Tweets and annotated them\nwith the help of a team of annotators with a background in communications and\njournalism. Ultimately we hope this can lead to effective and targeted public\nhealth communication strategies for reaching individuals with anti-vaccine\nbeliefs. Moreover, this information helps with developing Machine Learning\nmodels to automatically detect vaccine misinformation posts and combat their\nnegative impacts. In this paper, we present Vax-Culture, a novel Twitter\nCOVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an\nextensive set of human-provided annotations including vaccine-hesitancy stance,\nindication of any misinformation in tweets, the entities criticized and\nsupported in each tweet and the communicated message of each tweet. Moreover,\nwe define five baseline tasks including four classification and one sequence\ngeneration tasks, and report the results of a set of recent transformer-based\nmodels for them. The dataset and code are publicly available at\nhttps://github.com/mrzarei5/Vax-Culture.", "published": "2023-04-13 23:04:30", "link": "http://arxiv.org/abs/2304.06858v3", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Evaluation of Social Biases in Recent Large Pre-Trained Models", "abstract": "Large pre-trained language models are widely used in the community. These\nmodels are usually trained on unmoderated and unfiltered data from open sources\nlike the Internet. Due to this, biases that we see in platforms online which\nare a reflection of those in society are in turn captured and learned by these\nmodels. These models are deployed in applications that affect millions of\npeople and their inherent biases are harmful to the targeted social groups. In\nthis work, we study the general trend in bias reduction as newer pre-trained\nmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)\nare chosen and evaluated against two bias benchmarks, StereoSet and\nCrowS-Pairs. They are compared to the baseline of BERT using the associated\nmetrics. We explore whether as advancements are made and newer, faster, lighter\nmodels are released: are they being developed responsibly such that their\ninherent social biases have been reduced compared to their older counterparts?\nThe results are compiled and we find that all the models under study do exhibit\nbiases but have generally improved as compared to BERT.", "published": "2023-04-13 23:29:58", "link": "http://arxiv.org/abs/2304.06861v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Instructed Reinforcement Learning for Human-AI Coordination", "abstract": "One of the fundamental quests of AI is to produce agents that coordinate well\nwith humans. This problem is challenging, especially in domains that lack high\nquality human behavioral data, because multi-agent reinforcement learning (RL)\noften converges to different equilibria from the ones that humans prefer. We\npropose a novel framework, instructRL, that enables humans to specify what kind\nof strategies they expect from their AI partners through natural language\ninstructions. We use pretrained large language models to generate a prior\npolicy conditioned on the human instruction and use the prior to regularize the\nRL objective. This leads to the RL agent converging to equilibria that are\naligned with human preferences. We show that instructRL converges to human-like\npolicies that satisfy the given instructions in a proof-of-concept environment\nas well as the challenging Hanabi benchmark. Finally, we show that knowing the\nlanguage instruction significantly boosts human-AI coordination performance in\nhuman evaluations in Hanabi.", "published": "2023-04-13 04:47:31", "link": "http://arxiv.org/abs/2304.07297v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A Reference Architecture for Designing Foundation Model based Systems", "abstract": "The release of ChatGPT, Gemini, and other large language model has drawn huge\ninterests on foundations models. There is a broad consensus that foundations\nmodels will be the fundamental building blocks for future AI systems. However,\nthere is a lack of systematic guidance on the architecture design.\nParticularly, the the rapidly growing capabilities of foundations models can\neventually absorb other components of AI systems, posing challenges of moving\nboundary and interface evolution in architecture design. Furthermore,\nincorporating foundations models into AI systems raises significant concerns\nabout responsible and safe AI due to their opaque nature and rapidly advancing\nintelligence. To address these challenges, the paper first presents an\narchitecture evolution of AI systems in the era of foundation models,\ntransitioning from \"foundation-model-as-a-connector\" to\n\"foundation-model-as-a-monolithic architecture\". The paper then identifies key\ndesign decisions and proposes a pattern-oriented reference architecture for\ndesigning responsible foundation-model-based systems. The patterns can enable\nthe potential of foundation models while ensuring associated risks.", "published": "2023-04-13 05:01:03", "link": "http://arxiv.org/abs/2304.11090v5", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses", "abstract": "In recent years, a proliferation of cyber-security threats and diversity has\nbeen on the rise culminating in an increase in their reporting and analysis. To\ncounter that, many non-profit organizations have emerged in this domain, such\nas MITRE and OSWAP, which have been actively tracking vulnerabilities, and\npublishing defense recommendations in standardized formats. As producing data\nin such formats manually is very time-consuming, there have been some proposals\nto automate the process. Unfortunately, a major obstacle to adopting supervised\nmachine learning for this problem has been the lack of publicly available\nspecialized datasets. Here, we aim to bridge this gap. In particular, we focus\non mapping CVE records into MITRE CWE Weaknesses, and we release to the\nresearch community a manually annotated dataset of 4,012 records for this task.\nWith a human-in-the-loop framework in mind, we approach the problem as a\nranking task and aim to incorporate reinforced learning to make use of the\nhuman feedback in future work. Our experimental results using fine-tuned deep\nlearning models, namely Sentence-BERT and rankT5, show sizable performance\ngains over BM25, BERT, and RoBERTa, which demonstrates the need for an\narchitecture capable of good semantic understanding for this task.", "published": "2023-04-13 19:46:54", "link": "http://arxiv.org/abs/2304.11130v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CR"}
{"title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review", "abstract": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability", "published": "2023-04-13 16:01:28", "link": "http://arxiv.org/abs/2305.03123v4", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment", "abstract": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.", "published": "2023-04-13 18:22:40", "link": "http://arxiv.org/abs/2304.06767v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Context-aware Coherent Speaking Style Prediction with Hierarchical\n  Transformers for Audiobook Speech Synthesis", "abstract": "Recent advances in text-to-speech have significantly improved the\nexpressiveness of synthesized speech. However, it is still challenging to\ngenerate speech with contextually appropriate and coherent speaking style for\nmulti-sentence text in audiobooks. In this paper, we propose a context-aware\ncoherent speaking style prediction method for audiobook speech synthesis. To\npredict the style embedding of the current utterance, a hierarchical\ntransformer-based context-aware style predictor with a mixture attention mask\nis designed, considering both text-side context information and speech-side\nstyle information of previous speeches. Based on this, we can generate\nlong-form speech with coherent style and prosody sentence by sentence.\nObjective and subjective evaluations on a Mandarin audiobook dataset\ndemonstrate that our proposed model can generate speech with more expressive\nand coherent speaking style than baselines, for both single-sentence and\nmulti-sentence test.", "published": "2023-04-13 09:26:04", "link": "http://arxiv.org/abs/2304.06359v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The future of hearing aid technology", "abstract": "Background. Hearing aid technology has proven successful in the\nrehabilitation of hearing loss, but its performance is still limited in\ndifficult everyday conditions characterized by noise and reverberation.\n  Objectives. Introduction to the current state of hearing aid technology and\npresentation of the current state of research and future development.\n  Methods. Current literature is analyzed and several specific new developments\nare presented.\n  Results. Both objective and subjective data from empirical studies show the\nlimitation of current technology. Examples of current research show the\npotential of machine-learning based algorithms and multi-modal signal\nprocessing for improving speech processing and perception, of using virtual\nreality for improving hearing device fitting and of mobile health technology\nfor improving hearing-health services.\n  Conclusions. Hearing device technology will remain a key factor in the\nrehabilitation of hearing impairment. New technology such as machine learning,\nand multi-modal signal processing, virtual reality and mobile health technology\nwill improve speech enhancement, individual fitting and communication training.", "published": "2023-04-13 19:11:52", "link": "http://arxiv.org/abs/2304.06786v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Brain Connectivity Features-based Age Group Classification using\n  Temporal Asynchrony Audio-Visual Integration Task", "abstract": "The process of integration of inputs from several sensory modalities in the\nhuman brain is referred to as multisensory integration. Age-related cognitive\ndecline leads to a loss in the ability of the brain to conceive multisensory\ninputs. There has been considerable work done in the study of such cognitive\nchanges for the old age groups. However, in the case of middle age groups, such\nanalysis is limited. Motivated by this, in the current work, EEG-based\nfunctional connectivity during audiovisual temporal asynchrony integration task\nfor middle-aged groups is explored. Investigation has been carried out during\ndifferent tasks such as: unimodal audio, unimodal visual, and variations of\naudio-visual stimulus. A correlation-based functional connectivity analysis is\ndone, and the changes among different age groups including: young (18-25\nyears), transition from young to middle age (25-33 years), and medium (33-41\nyears), are observed. Furthermore, features extracted from the connectivity\ngraphs have been used to classify among the different age groups.\nClassification accuracies of $89.4\\%$ and $88.4\\%$ are obtained for the Audio\nand Audio-50-Visual stimuli cases with a Random Forest based classifier,\nthereby validating the efficacy of the proposed method.", "published": "2023-04-13 07:53:14", "link": "http://arxiv.org/abs/2304.06315v2", "categories": ["eess.SP", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "eess.SP"}
{"title": "Level generation for rhythm VR games", "abstract": "Ragnarock is a virtual reality (VR) rhythm game in which you play a Viking\ncaptain competing in a longship race. With two hammers, the task is to crush\nthe incoming runes in sync with epic Viking music. The runes are defined by a\nbeat map which the player can manually create. The creation of beat maps takes\nhours. This work aims to automate the process of beat map creation, also known\nas the task of learning to choreograph. The assignment is broken down into\nthree parts: determining the timing of the beats (action placement),\ndetermining where in space the runes connected with the chosen beats should be\nplaced (action selection) and web-application creation. For the first task of\naction placement, extraction of predominant local pulse (PLP) information from\nmusic recordings is used. This approach allows to learn where and how many\nbeats are supposed to be placed. For the second task of action selection,\nRecurrent Neural Networks (RNN) are used, specifically Gated recurrent unit\n(GRU) to learn sequences of beats, and their patterns to be able to recreate\nthose rules and receive completely new levels. Then the last task was to build\na solution for non-technical players, the task was to combine the results of\nthe first and the second parts into a web application for easy use. For this\ntask the frontend was built using JavaScript and React and the backend - python\nand FastAPI.", "published": "2023-04-13 20:24:51", "link": "http://arxiv.org/abs/2304.06809v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
