{"title": "A Comprehensive Analysis of the Effectiveness of Large Language Models\n  as Automatic Dialogue Evaluators", "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.", "published": "2023-12-24 04:50:57", "link": "http://arxiv.org/abs/2312.15407v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Group Fairness Lens for Large Language Models", "abstract": "The rapid advancement of large language models has revolutionized various\napplications but also raised crucial concerns about their potential to\nperpetuate biases and unfairness when deployed in social media contexts.\nEvaluating LLMs' potential biases and fairness has become crucial, as existing\nmethods rely on limited prompts focusing on just a few groups, lacking a\ncomprehensive categorical perspective. In this paper, we propose evaluating LLM\nbiases from a group fairness lens using a novel hierarchical schema\ncharacterizing diverse social groups. Specifically, we construct a dataset,\nGFair, encapsulating target-attribute combinations across multiple dimensions.\nIn addition, we introduce statement organization, a new open-ended text\ngeneration task, to uncover complex biases in LLMs. Extensive evaluations of\npopular LLMs reveal inherent safety concerns. To mitigate the biases of LLM\nfrom a group fairness perspective, we pioneer a novel chain-of-thought method\nGF-Think to mitigate biases of LLMs from a group fairness perspective.\nExperimental results demonstrate its efficacy in mitigating bias in LLMs to\nachieve fairness.", "published": "2023-12-24 13:25:15", "link": "http://arxiv.org/abs/2312.15478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Large Language Models A Better Foundation For Dense Retrieval", "abstract": "Dense retrieval needs to learn discriminative text embeddings to represent\nthe semantic relationship between query and document. It may benefit from the\nusing of large language models (LLMs), given LLMs' strong capability on\nsemantic understanding. However, the LLMs are pre-trained by text generation\ntasks, whose working pattern is completely different from representing texts as\nembeddings. As a result, it is imperative to study how to adapt LLMs properly\nso that they can be effectively initialized as the backbone encoder for dense\nretrieval.\n  In this paper, we propose a novel approach, called LLaRA (LLM adapted for\ndense RetrievAl), which works as a post-hoc adaptation of LLM for the dense\nretrieval application. LLaRA consists of two pretext tasks: EBAE\n(Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression),\nwhere the text embeddings from LLM are used to reconstruct the tokens for the\ninput sentence and predict the tokens for the next sentence, respectively.\nLLaRA turns out to be simple, lightweight, and highly effective. It is applied\nto adapt LLaMA-2-7B (base) on the Wikipedia corpus, where it substantially\nimproves the model's fine-tuned performances on a variety of dense retrieval\nbenchmarks, like MSMARCO and BEIR. Our model and code will be made publicly\navailable at BGE repository.", "published": "2023-12-24 15:10:35", "link": "http://arxiv.org/abs/2312.15503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Consistent Language Models Using Declarative Constraints", "abstract": "Large language models have shown unprecedented abilities in generating\nlinguistically coherent and syntactically correct natural language output.\nHowever, they often return incorrect and inconsistent answers to input\nquestions. Due to the complexity and uninterpretability of the internally\nlearned representations, it is challenging to modify language models such that\nthey provide correct and consistent results. The data management community has\ndeveloped various methods and tools for providing consistent answers over\ninconsistent datasets. In these methods, users specify the desired properties\nof data in a domain in the form of high-level declarative constraints. This\napproach has provided usable and scalable methods to delivering consistent\ninformation from inconsistent datasets. We aim to build upon this success and\nleverage these methods to modify language models such that they deliver\nconsistent and accurate results. We investigate the challenges of using these\nideas to obtain consistent and relevant answers from language models and report\nsome preliminary empirical studies.", "published": "2023-12-24 12:53:07", "link": "http://arxiv.org/abs/2312.15472v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal\n  Information Extraction", "abstract": "The difficulty of the information extraction task lies in dealing with the\ntask-specific label schemas and heterogeneous data structures. Recent work has\nproposed methods based on large language models to uniformly model different\ninformation extraction tasks. However, these existing methods are deficient in\ntheir information extraction capabilities for Chinese languages other than\nEnglish. In this paper, we propose an end-to-end chat-enhanced instruction\ntuning framework for universal information extraction (YAYI-UIE), which\nsupports both Chinese and English. Specifically, we utilize dialogue data and\ninformation extraction data to enhance the information extraction performance\njointly. Experimental results show that our proposed framework achieves\nstate-of-the-art performance on Chinese datasets while also achieving\ncomparable performance on English datasets under both supervised settings and\nzero-shot settings.", "published": "2023-12-24 21:33:03", "link": "http://arxiv.org/abs/2312.15548v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP", "abstract": "The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.", "published": "2023-12-24 23:01:00", "link": "http://arxiv.org/abs/2312.15561v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Classification of Teaching Activities from University Lecture\n  Recordings", "abstract": "The way of understanding online higher education has greatly changed due to\nthe worldwide pandemic situation. Teaching is undertaken remotely, and the\nfaculty incorporate lecture audio recordings as part of the teaching material.\nThis new online teaching-learning setting has largely impacted university\nclasses. While online teaching technology that enriches virtual classrooms has\nbeen abundant over the past two years, the same has not occurred in supporting\nstudents during online learning. {To overcome this limitation, our aim is to\nwork toward enabling students to easily access the piece of the lesson\nrecording in which the teacher explains a theoretical concept, solves an\nexercise, or comments on organizational issues of the course. To that end, we\npresent a multimodal classification algorithm that identifies the type of\nactivity that is being carried out at any time of the lesson by using a\ntransformer-based language model that exploits features from the audio file and\nfrom the automated lecture transcription. The experimental results will show\nthat some academic activities are more easily identifiable with the audio\nsignal while resorting to the text transcription is needed to identify others.\nAll in all, our contribution aims to recognize the academic activities of a\nteacher during a lesson.", "published": "2023-12-24 08:33:30", "link": "http://arxiv.org/abs/2312.17262v1", "categories": ["cs.CL", "cs.LG", "68T05 (Primary), 97P80 (Secondary)", "I.2.6; I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "Prompt Valuation Based on Shapley Values", "abstract": "Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt.", "published": "2023-12-24 03:37:11", "link": "http://arxiv.org/abs/2312.15395v2", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fairness-Aware Structured Pruning in Transformers", "abstract": "The increasing size of large language models (LLMs) has introduced challenges\nin their training and inference. Removing model components is perceived as a\nsolution to tackle the large model sizes, however, existing pruning methods\nsolely focus on performance, without considering an essential aspect for the\nresponsible use of LLMs: model fairness. It is crucial to address the fairness\nof LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish\ncommunities, among others, as they are being deployed and available to a wide\naudience. In this work, first, we investigate how attention heads impact\nfairness and performance in pre-trained transformer-based language models. We\nthen propose a novel method to prune the attention heads that negatively impact\nfairness while retaining the heads critical for performance, i.e. language\nmodeling capabilities. Our approach is practical in terms of time and\nresources, as it does not require fine-tuning the final pruned, and fairer,\nmodel. Our findings demonstrate a reduction in gender bias by 19%, 19.5%,\n39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different\nsizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased\nmodel, with only a slight decrease in performance.", "published": "2023-12-24 03:57:52", "link": "http://arxiv.org/abs/2312.15398v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Persuasive Power of Large Language Models", "abstract": "The increasing capability of Large Language Models to act as human-like\nsocial agents raises two important questions in the area of opinion dynamics.\nFirst, whether these agents can generate effective arguments that could be\ninjected into the online discourse to steer the public opinion. Second, whether\nartificial agents can interact with each other to reproduce dynamics of\npersuasion typical of human social systems, opening up opportunities for\nstudying synthetic social systems as faithful proxies for opinion dynamics in\nhuman populations. To address these questions, we designed a synthetic\npersuasion dialogue scenario on the topic of climate change, where a\n'convincer' agent generates a persuasive argument for a 'skeptic' agent, who\nsubsequently assesses whether the argument changed its internal opinion state.\nDifferent types of arguments were generated to incorporate different linguistic\ndimensions underpinning psycho-linguistic theories of opinion change. We then\nasked human judges to evaluate the persuasiveness of machine-generated\narguments. Arguments that included factual knowledge, markers of trust,\nexpressions of support, and conveyed status were deemed most effective\naccording to both humans and agents, with humans reporting a marked preference\nfor knowledge-based arguments. Our experimental framework lays the groundwork\nfor future in-silico studies of opinion dynamics, and our findings suggest that\nartificial agents have the potential of playing an important role in collective\nprocesses of opinion formation in online social media.", "published": "2023-12-24 16:21:11", "link": "http://arxiv.org/abs/2312.15523v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "Multi-level biomedical NER through multi-granularity embeddings and\n  enhanced labeling", "abstract": "Biomedical Named Entity Recognition (NER) is a fundamental task of Biomedical\nNatural Language Processing for extracting relevant information from biomedical\ntexts, such as clinical records, scientific publications, and electronic health\nrecords. The conventional approaches for biomedical NER mainly use traditional\nmachine learning techniques, such as Conditional Random Fields and Support\nVector Machines or deep learning-based models like Recurrent Neural Networks\nand Convolutional Neural Networks. Recently, Transformer-based models,\nincluding BERT, have been used in the domain of biomedical NER and have\ndemonstrated remarkable results. However, these models are often based on\nword-level embeddings, limiting their ability to capture character-level\ninformation, which is effective in biomedical NER due to the high variability\nand complexity of biomedical texts. To address these limitations, this paper\nproposes a hybrid approach that integrates the strengths of multiple models. In\nthis paper, we proposed an approach that leverages fine-tuned BERT to provide\ncontextualized word embeddings, a pre-trained multi-channel CNN for\ncharacter-level information capture, and following by a BiLSTM + CRF for\nsequence labelling and modelling dependencies between the words in the text. In\naddition, also we propose an enhanced labelling method as part of\npre-processing to enhance the identification of the entity's beginning word and\nthus improve the identification of multi-word entities, a common challenge in\nbiomedical NER. By integrating these models and the pre-processing method, our\nproposed model effectively captures both contextual information and detailed\ncharacter-level information. We evaluated our model on the benchmark i2b2/2010\ndataset, achieving an F1-score of 90.11. These results illustrate the\nproficiency of our proposed model in performing biomedical Named Entity\nRecognition.", "published": "2023-12-24 21:45:36", "link": "http://arxiv.org/abs/2312.15550v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 68T07", "J.3; I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Exploring data augmentation in bias mitigation against\n  non-native-accented speech", "abstract": "Automatic speech recognition (ASR) should serve every speaker, not only the\nmajority ``standard'' speakers of a language. In order to build inclusive ASR,\nmitigating the bias against speaker groups who speak in a ``non-standard'' or\n``diverse'' way is crucial. We aim to mitigate the bias against\nnon-native-accented Flemish in a Flemish ASR system. Since this is a\nlow-resource problem, we investigate the optimal type of data augmentation,\ni.e., speed/pitch perturbation, cross-lingual voice conversion-based methods,\nand SpecAugment, applied to both native Flemish and non-native-accented\nFlemish, for bias mitigation. The results showed that specific types of data\naugmentation applied to both native and non-native-accented speech improve\nnon-native-accented ASR while applying data augmentation to the\nnon-native-accented speech is more conducive to bias reduction. Combining both\ngave the largest bias reduction for human-machine interaction (HMI) as well as\nread-type speech.", "published": "2023-12-24 14:58:41", "link": "http://arxiv.org/abs/2312.15499v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Consistent and Relevant: Rethink the Query Embedding in General Sound\n  Separation", "abstract": "The query-based audio separation usually employs specific queries to extract\ntarget sources from a mixture of audio signals. Currently, most query-based\nseparation models need additional networks to obtain query embedding. In this\nway, separation model is optimized to be adapted to the distribution of query\nembedding. However, query embedding may exhibit mismatches with separation\nmodels due to inconsistent structures and independent information. In this\npaper, we present CaRE-SEP, a consistent and relevant embedding network for\ngeneral sound separation to encourage a comprehensive reconsideration of query\nusage in audio separation. CaRE-SEP alleviates the potential mismatch between\nqueries and separation in two aspects, including sharing network structure and\nsharing feature information. First, a Swin-Unet model with a shared encoder is\nconducted to unify query encoding and sound separation into one model,\neliminating the network architecture difference and generating consistent\ndistribution of query and separation features. Second, by initializing CaRE-SEP\nwith a pretrained classification network and allowing gradient backpropagation,\nthe query embedding is optimized to be relevant to the separation feature,\nfurther alleviating the feature mismatch problem. Experimental results indicate\nthe proposed CaRE-SEP model substantially improves the performance of\nseparation tasks. Moreover, visualizations validate the potential mismatch and\nhow CaRE-SEP solves it.", "published": "2023-12-24 11:47:16", "link": "http://arxiv.org/abs/2312.15463v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combinatorial music generation model with song structure graph analysis", "abstract": "In this work, we propose a symbolic music generation model with the song\nstructure graph analysis network. We construct a graph that uses information\nsuch as note sequence and instrument as node features, while the correlation\nbetween note sequences acts as the edge feature. We trained a Graph Neural\nNetwork to obtain node representation in the graph, then we use node\nrepresentation as input of Unet to generate CONLON pianoroll image latent. The\noutcomes of our experimental results show that the proposed model can generate\na comprehensive form of music. Our approach represents a promising and\ninnovative method for symbolic music generation and holds potential\napplications in various fields in Music Information Retreival, including music\ncomposition, music classification, and music inpainting systems.", "published": "2023-12-24 04:09:30", "link": "http://arxiv.org/abs/2312.15400v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deformable Audio Transformer for Audio Event Detection", "abstract": "Transformers have achieved promising results on a variety of tasks. However,\nthe quadratic complexity in self-attention computation has limited the\napplications, especially in low-resource settings and mobile or edge devices.\nExisting works have proposed to exploit hand-crafted attention patterns to\nreduce computation complexity. However, such hand-crafted patterns are\ndata-agnostic and may not be optimal. Hence, it is likely that relevant keys or\nvalues are being reduced, while less important ones are still preserved. Based\non this key insight, we propose a novel deformable audio Transformer for audio\nrecognition, named DATAR, where a deformable attention equipping with a pyramid\ntransformer backbone is constructed and learnable. Such an architecture has\nbeen proven effective in prediction tasks,~\\textit{e.g.}, event classification.\nMoreover, we identify that the deformable attention map computation may\nover-simplify the input feature, which can be further enhanced. Hence, we\nintroduce a learnable input adaptor to alleviate this issue, and DATAR achieves\nstate-of-the-art performance.", "published": "2023-12-24 18:27:22", "link": "http://arxiv.org/abs/2312.16228v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
