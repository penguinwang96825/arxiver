{"title": "Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System", "abstract": "Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks\nby tracking dialogue states and generating appropriate responses to help users\nachieve defined goals. Recently, end-to-end dialogue models pre-trained based\non large datasets have shown promising performance in the conversational\nsystem. However, they share the same parameters to train tasks of the dialogue\nsystem (NLU, DST, NLG), so debugging each task is challenging. Also, they\nrequire a lot of effort to fine-tune large parameters to create a task-oriented\nchatbot, making it difficult for non-experts to handle. Therefore, we intend to\ntrain relatively lightweight and fast models compared to PLM. In this paper, we\npropose an End-to-end TOD system with Task-Optimized Adapters which learn\nindependently per task, adding only small number of parameters after fixed\nlayers of pre-trained network. We also enhance the performance of the DST and\nNLG modules through reinforcement learning, overcoming the learning curve that\nhas lacked at the adapter learning and enabling the natural and consistent\nresponse generation that is appropriate for the goal. Our method is a\nmodel-agnostic approach and does not require prompt-tuning as only input data\nwithout a prompt. As results of the experiment, our method shows competitive\nperformance on the MultiWOZ benchmark compared to the existing end-to-end\nmodels. In particular, we attain state-of-the-art performance on the DST task\nof 2.2 dataset.", "published": "2023-05-04 00:17:49", "link": "http://arxiv.org/abs/2305.02468v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Abstractive Summarization by Tri-agent Generation Pipeline", "abstract": "Tailoring outputs from large language models, like ChatGPT, to implicit user\npreferences remains a challenge despite their impressive generative\ncapabilities. In this paper, we propose a tri-agent generation pipeline\ncomprising a generator, an instructor, and an editor to enhance output\npersonalization. The generator produces an initial output, the instructor\nautomatically generates editing instructions based on user preferences, and the\neditor refines the output to align with those preferences. The inference-only\nlarge language model (ChatGPT) serves as both the generator and editor, with a\nsmaller model acting as the instructor to guide output generation. We train the\ninstructor using editor-steered reinforcement learning, leveraging feedback\nfrom a large-scale editor model to optimize instruction generation.\nExperimental results on two abstractive summarization datasets demonstrate the\neffectiveness of our approach in generating outputs that better meet user\nexpectations. Code is available at\n\\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}", "published": "2023-05-04 01:12:35", "link": "http://arxiv.org/abs/2305.02483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual\n  Adaptation of Gazetteer for Multilingual Complex NER", "abstract": "This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER\nII). A method named Statistical Construction and Dual Adaptation of Gazetteer\n(SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a\nstatistics-based approach to construct a gazetteer. Secondly, the\nrepresentations of gazetteer networks and language models are adapted by\nminimizing the KL divergence between them at both the sentence-level and\nentity-level. Finally, these two networks are then integrated for supervised\nnamed entity recognition (NER) training. The proposed method is applied to\nXLM-R with a gazetteer built from Wikidata, and shows great generalization\nability across different tracks. Experimental results and detailed analysis\nverify the effectiveness of the proposed method. The official results show that\nour system ranked 1st on one track (Hindi) in this task.", "published": "2023-05-04 03:00:46", "link": "http://arxiv.org/abs/2305.02517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful Question Answering with Monte-Carlo Planning", "abstract": "Although large language models demonstrate remarkable question-answering\nperformances, revealing the intermediate reasoning steps that the models\nfaithfully follow remains challenging. In this paper, we propose FAME (FAithful\nquestion answering with MontE-carlo planning) to answer questions based on\nfaithful reasoning steps. The reasoning steps are organized as a structured\nentailment tree, which shows how premises are used to produce intermediate\nconclusions that can prove the correctness of the answer. We formulate the task\nas a discrete decision-making problem and solve it through the interaction of a\nreasoning environment and a controller. The environment is modular and contains\nseveral basic task-oriented modules, while the controller proposes actions to\nassemble the modules. Since the search space could be large, we introduce a\nMonte-Carlo planning algorithm to do a look-ahead search and select actions\nthat will eventually lead to high-quality steps. FAME achieves state-of-the-art\nperformance on the standard benchmark. It can produce valid and faithful\nreasoning steps compared with large language models with a much smaller model\nsize.", "published": "2023-05-04 05:21:36", "link": "http://arxiv.org/abs/2305.02556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training\n  Retrieval-Oriented Language Models", "abstract": "To better support information retrieval tasks such as web search and\nopen-domain question answering, growing effort is made to develop\nretrieval-oriented language models, e.g., RetroMAE and many others. Most of the\nexisting works focus on improving the semantic representation capability for\nthe contextualized embedding of the [CLS] token. However, recent study shows\nthat the ordinary tokens besides [CLS] may provide extra information, which\nhelp to produce a better representation effect. As such, it's necessary to\nextend the current methods where all contextualized embeddings can be jointly\npre-trained for the retrieval tasks. In this work, we propose a novel\npre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is\ndesigned to improve the quality of semantic representation where all\ncontextualized embeddings of the pre-trained model can be leveraged. It takes\nadvantage of two complementary auto-encoding tasks: one reconstructs the input\nsentence on top of the [CLS] embedding; the other one predicts the bag-of-words\nfeature of the input sentence based on the ordinary tokens' embeddings. The two\ntasks are jointly conducted to train a unified encoder, where the whole\ncontextualized embeddings are aggregated in a compact way to produce the final\nsemantic representation. DupMAE is simple but empirically competitive: it\nsubstantially improves the pre-trained model's representation capability and\ntransferability, where superior retrieval performances can be achieved on\npopular benchmarks, like MS MARCO and BEIR.", "published": "2023-05-04 05:37:22", "link": "http://arxiv.org/abs/2305.02564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for\n  Long-Turn Open-Domain Dialogue Pre-training", "abstract": "Pre-training on large-scale open-domain dialogue data can substantially\nimprove the performance of dialogue models. However, the pre-trained dialogue\nmodel's ability to utilize long-range context is limited due to the scarcity of\nlong-turn dialogue sessions. Most dialogues in existing pre-training corpora\ncontain fewer than three turns of dialogue. To alleviate this issue, we propose\nthe Retrieve, Reorganize and Rescale framework (Re$^3$Dial), which can\nautomatically construct billion-scale long-turn dialogues by reorganizing\nexisting short-turn ones. Given a short-turn session, Re$^3$Dial first employs\na session retriever to retrieve coherent consecutive sessions. To this end, we\ntrain the retriever to capture semantic and discourse relations within\nmulti-turn dialogues through contrastive training. Next, Re$^3$Dial samples a\nsession from retrieved results following a diversity sampling strategy, which\nis designed to penalize repetitive or generic sessions. A longer session is\nthen derived by concatenating the original session and the sampled session. By\nrepeating the above process, Re$^3$Dial can yield a coherent long-turn\ndialogue. Extensive experiments on multiple multi-turn dialogue benchmarks\ndemonstrate that Re$^3$Dial significantly improves the dialogue model's ability\nto utilize long-range context and thus generate more sensible and informative\nresponses. Finally, we build a toolkit for efficiently rescaling conversations\nwith Re$^3$Dial, which enables us to construct a corpus containing 1B Chinese\ndialogue sessions with 11.3 turns on average (5$\\times$ longer than the\noriginal corpus). Our retriever model, code, and data is publicly available at\n\\url{https://github.com/thu-coai/Re3Dial}.", "published": "2023-05-04 07:28:23", "link": "http://arxiv.org/abs/2305.02606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DN at SemEval-2023 Task 12: Low-Resource Language Text Classification\n  via Multilingual Pretrained Language Model Fine-tuning", "abstract": "In recent years, sentiment analysis has gained significant importance in\nnatural language processing. However, most existing models and datasets for\nsentiment analysis are developed for high-resource languages, such as English\nand Chinese, leaving low-resource languages, particularly African languages,\nlargely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this\ngap by evaluating sentiment analysis models on low-resource African languages.\nIn this paper, we present our solution to the shared task, where we employed\ndifferent multilingual XLM-R models with classification head trained on various\ndata, including those retrained in African dialects and fine-tuned on target\nlanguages. Our team achieved the third-best results in Subtask B, Track 16:\nMultilingual, demonstrating the effectiveness of our approach. While our model\nshowed relatively good results on multilingual data, it performed poorly in\nsome languages. Our findings highlight the importance of developing more\ncomprehensive datasets and models for low-resource African languages to advance\nsentiment analysis research. We also provided the solution on the github\nrepository.", "published": "2023-05-04 07:28:45", "link": "http://arxiv.org/abs/2305.02607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Elephant in the Room: Analyzing the Presence of Big Tech in Natural\n  Language Processing Research", "abstract": "Recent advances in deep learning methods for natural language processing\n(NLP) have created new business opportunities and made NLP research critical\nfor industry development. As one of the big players in the field of NLP,\ntogether with governments and universities, it is important to track the\ninfluence of industry on research. In this study, we seek to quantify and\ncharacterize industry presence in the NLP community over time. Using a corpus\nwith comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP\npublication authors, we explore the industry presence in the field since the\nearly 90s. We find that industry presence among NLP authors has been steady\nbefore a steep increase over the past five years (180% growth from 2017 to\n2022). A few companies account for most of the publications and provide funding\nto academic researchers through grants and internships. Our study shows that\nthe presence and impact of the industry on natural language processing research\nare significant and fast-growing. This work calls for increased transparency of\nindustry influence in the field.", "published": "2023-05-04 12:57:18", "link": "http://arxiv.org/abs/2305.02797v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Space Grounded Weighted Decoding for Multi-Attribute\n  Controllable Dialogue Generation", "abstract": "Controlling chatbot utterance generation with multiple attributes such as\npersonalities, emotions and dialogue acts is a practically useful but\nunder-studied problem. We propose a novel framework called DASC that possesses\nstrong controllability with a weighted decoding paradigm, while improving\ngeneration quality with the grounding in an attribute semantics space.\nGeneration with multiple attributes is then intuitively implemented with an\ninterpolation of multiple attribute embeddings, which results in substantial\nreduction in the model sizes. Experiments show that DASC can achieve high\ncontrol accuracy in generation task with the simultaneous control of 3 aspects\nwhile also producing interesting and reasonably sensible responses, even in an\nout-of-distribution robustness test.", "published": "2023-05-04 13:35:27", "link": "http://arxiv.org/abs/2305.02820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masked Structural Growth for 2x Faster Language Model Pre-training", "abstract": "Accelerating large language model pre-training is a critical issue in present\nresearch. In this paper, we focus on speeding up pre-training by progressively\ngrowing from a small Transformer structure to a large one. There are two main\nresearch problems associated with progressive growth: determining the optimal\ngrowth schedule, and designing efficient growth operators. In terms of growth\nschedule, the impact of each single dimension on a schedule's efficiency is\nunder-explored by existing work. Regarding the growth operators, existing\nmethods rely on the initialization of new weights to inherit knowledge, and\nachieve only non-strict function preservation, limiting further improvements on\ntraining dynamics. To address these issues, we propose Masked Structural Growth\n(MSG), including (i) growth schedules involving all possible dimensions and\n(ii) strictly function-preserving growth operators that is independent of the\ninitialization of new weights. Experiments show that MSG is significantly\nfaster than related work: we achieve up to 2.2x speedup in pre-training\ndifferent types of language models while maintaining comparable or better\ndownstream performances. Code is publicly available at\nhttps://github.com/cofe-ai/MSG.", "published": "2023-05-04 14:28:39", "link": "http://arxiv.org/abs/2305.02869v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NatCS: Eliciting Natural Customer Support Dialogues", "abstract": "Despite growing interest in applications based on natural customer support\nconversations, there exist remarkably few publicly available datasets that\nreflect the expected characteristics of conversations in these settings.\nExisting task-oriented dialogue datasets, which were collected to benchmark\ndialogue systems mainly in written human-to-bot settings, are not\nrepresentative of real customer support conversations and do not provide\nrealistic benchmarks for systems that are applied to natural data. To address\nthis gap, we introduce NatCS, a multi-domain collection of spoken customer\nservice conversations. We describe our process for collecting synthetic\nconversations between customers and agents based on natural language phenomena\nobserved in real conversations. Compared to previous dialogue datasets, the\nconversations collected with our approach are more representative of real\nhuman-to-human conversations along multiple metrics. Finally, we demonstrate\npotential uses of NatCS, including dialogue act classification and intent\ninduction from conversations as potential applications, showing that dialogue\nact annotations in NatCS provide more effective training data for modeling real\nconversations compared to existing synthetic written datasets. We publicly\nrelease NatCS to facilitate research in natural dialog systems", "published": "2023-05-04 17:25:24", "link": "http://arxiv.org/abs/2305.03007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLM Already Serve as A Database Interface? A BIg Bench for\n  Large-Scale Database Grounded Text-to-SQLs", "abstract": "Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.", "published": "2023-05-04 19:02:29", "link": "http://arxiv.org/abs/2305.03111v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Human Explanations Always Helpful? Towards Objective Evaluation of\n  Human Natural Language Explanations", "abstract": "Human-annotated labels and explanations are critical for training explainable\nNLP models. However, unlike human-annotated labels whose quality is easier to\ncalibrate (e.g., with a majority vote), human-crafted free-form explanations\ncan be quite subjective. Before blindly using them as ground truth to train ML\nmodels, a vital question needs to be asked: How do we evaluate a\nhuman-annotated explanation's quality? In this paper, we build on the view that\nthe quality of a human-annotated explanation can be measured based on its\nhelpfulness (or impairment) to the ML models' performance for the desired NLP\ntasks for which the annotations were collected. In comparison to the commonly\nused Simulatability score, we define a new metric that can take into\nconsideration the helpfulness of an explanation for model performance at both\nfine-tuning and inference. With the help of a unified dataset format, we\nevaluated the proposed metric on five datasets (e.g., e-SNLI) against two model\narchitectures (T5 and BART), and the results show that our proposed metric can\nobjectively evaluate the quality of human-annotated explanations, while\nSimulatability falls short.", "published": "2023-05-04 19:31:50", "link": "http://arxiv.org/abs/2305.03117v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Skills: A Configurable Model for Open-domain Question Answering", "abstract": "The retrieval model is an indispensable component for real-world\nknowledge-intensive tasks, e.g., open-domain question answering (ODQA). As\nseparate retrieval skills are annotated for different datasets, recent work\nfocuses on customized methods, limiting the model transferability and\nscalability. In this work, we propose a modular retriever where individual\nmodules correspond to key skills that can be reused across datasets. Our\napproach supports flexible skill configurations based on the target domain to\nboost performance. To mitigate task interference, we design a novel\nmodularization parameterization inspired by sparse Transformer. We demonstrate\nthat our model can benefit from self-supervised pretraining on Wikipedia and\nfine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our\napproach outperforms recent self-supervised retrievers in zero-shot evaluations\nand achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA\nand OTT-QA.", "published": "2023-05-04 20:19:39", "link": "http://arxiv.org/abs/2305.03130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Global and Local Context in Named Entity Recognition", "abstract": "Pre-trained transformer-based models have recently shown great performance\nwhen applied to Named Entity Recognition (NER). As the complexity of their\nself-attention mechanism prevents them from processing long documents at once,\nthese models are usually applied in a sequential fashion. Such an approach\nunfortunately only incorporates local context and prevents leveraging global\ndocument context in long documents such as novels, which might hinder\nperformance. In this article, we explore the impact of global document context,\nand its relationships with local context. We find that correctly retrieving\nglobal document context has a greater impact on performance than only\nleveraging local context, prompting for further research on how to better\nretrieve that context.", "published": "2023-05-04 20:22:18", "link": "http://arxiv.org/abs/2305.03132v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging BERT Language Model for Arabic Long Document Classification", "abstract": "Given the number of Arabic speakers worldwide and the notably large amount of\ncontent in the web today in some fields such as law, medicine, or even news,\ndocuments of considerable length are produced regularly. Classifying those\ndocuments using traditional learning models is often impractical since extended\nlength of the documents increases computational requirements to an\nunsustainable level. Thus, it is necessary to customize these models\nspecifically for long textual documents. In this paper we propose two simple\nbut effective models to classify long length Arabic documents. We also\nfine-tune two different models-namely, Longformer and RoBERT, for the same task\nand compare their results to our models. Both of our models outperform the\nLongformer and RoBERT in this task over two different datasets.", "published": "2023-05-04 13:56:32", "link": "http://arxiv.org/abs/2305.03519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction\n  For QA Domain Adaptation", "abstract": "Existing Question Answering (QA) systems limited by the capability of\nanswering questions from unseen domain or any out-of-domain distributions\nmaking them less reliable for deployment to real scenarios. Most importantly\nall the existing QA domain adaptation methods are either based on generating\nsynthetic data or pseudo labeling the target domain data. The domain adaptation\nmethods based on synthetic data and pseudo labeling suffers either from the\nrequirement of computational resources or an extra overhead of carefully\nselecting the confidence threshold to separate the noisy examples from being in\nthe training dataset. In this paper, we propose the unsupervised domain\nadaptation for unlabeled target domain by transferring the target\nrepresentation near to source domain while still using the supervision from\nsource domain. Towards that we proposed the idea of domain invariant fine\ntuning along with adversarial label correction to identify the target instances\nwhich lie far apart from the source domain, so that the feature encoder can be\nlearnt to minimize the distance between such target instances and source\ninstances class wisely, removing the possibility of learning the features of\ntarget domain which are still near to source support but are ambiguous.\nEvaluation of our QA domain adaptation method namely, DomainInv on multiple\ntarget QA dataset reveal the performance improvement over the strongest\nbaseline.", "published": "2023-05-04 18:13:17", "link": "http://arxiv.org/abs/2305.05589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward the Automated Construction of Probabilistic Knowledge Graphs for\n  the Maritime Domain", "abstract": "International maritime crime is becoming increasingly sophisticated, often\nassociated with wider criminal networks. Detecting maritime threats by means of\nfusing data purely related to physical movement (i.e., those generated by\nphysical sensors, or hard data) is not sufficient. This has led to research and\ndevelopment efforts aimed at combining hard data with other types of data\n(especially human-generated or soft data). Existing work often assumes that\ninput soft data is available in a structured format, or is focused on\nextracting certain relevant entities or concepts to accompany or annotate hard\ndata. Much less attention has been given to extracting the rich knowledge about\nthe situations of interest implicitly embedded in the large amount of soft data\nexisting in unstructured formats (such as intelligence reports and news\narticles). In order to exploit the potentially useful and rich information from\nsuch sources, it is necessary to extract not only the relevant entities and\nconcepts but also their semantic relations, together with the uncertainty\nassociated with the extracted knowledge (i.e., in the form of probabilistic\nknowledge graphs). This will increase the accuracy of and confidence in, the\nextracted knowledge and facilitate subsequent reasoning and learning. To this\nend, we propose Maritime DeepDive, an initial prototype for the automated\nconstruction of probabilistic knowledge graphs from natural language data for\nthe maritime domain. In this paper, we report on the current implementation of\nMaritime DeepDive, together with preliminary results on extracting\nprobabilistic events from maritime piracy incidents. This pipeline was\nevaluated on a manually crafted gold standard, yielding promising results.", "published": "2023-05-04 00:24:30", "link": "http://arxiv.org/abs/2305.02471v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning\n  over Untrimmed Videos", "abstract": "Building benchmarks to systemically analyze different capabilities of video\nquestion answering (VideoQA) models is challenging yet crucial. Existing\nbenchmarks often use non-compositional simple questions and suffer from\nlanguage biases, making it difficult to diagnose model weaknesses incisively. A\nrecent benchmark AGQA poses a promising paradigm to generate QA pairs\nautomatically from pre-annotated scene graphs, enabling it to measure diverse\nreasoning abilities with granular control. However, its questions have\nlimitations in reasoning about the fine-grained semantics in videos as such\ninformation is absent in its scene graphs. To this end, we present ANetQA, a\nlarge-scale benchmark that supports fine-grained compositional reasoning over\nthe challenging untrimmed videos from ActivityNet. Similar to AGQA, the QA\npairs in ANetQA are automatically generated from annotated video scene graphs.\nThe fine-grained properties of ANetQA are reflected in the following: (i)\nuntrimmed videos with fine-grained semantics; (ii) spatio-temporal scene graphs\nwith fine-grained taxonomies; and (iii) diverse questions generated from\nfine-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million\nbalanced QA pairs, which is an order of magnitude larger than AGQA with a\nsimilar number of videos. Comprehensive experiments are performed for\nstate-of-the-art methods. The best model achieves 44.5% accuracy while human\nperformance tops out at 84.5%, leaving sufficient room for improvement.", "published": "2023-05-04 03:04:59", "link": "http://arxiv.org/abs/2305.02519v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can LLMs Capture Human Preferences?", "abstract": "We explore the viability of Large Language Models (LLMs), specifically\nOpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting\npreferences, with a focus on intertemporal choices. Leveraging the extensive\nliterature on intertemporal discounting for benchmarking, we examine responses\nfrom LLMs across various languages and compare them to human responses,\nexploring preferences between smaller, sooner, and larger, later rewards. Our\nfindings reveal that both GPT models demonstrate less patience than humans,\nwith GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike\nhuman decision-makers. Though GPT-4 does not display lexicographic preferences,\nits measured discount rates are still considerably larger than those found in\nhumans. Interestingly, GPT models show greater patience in languages with weak\nfuture tense references, such as German and Mandarin, aligning with existing\nliterature that suggests a correlation between language structure and\nintertemporal preferences. We demonstrate how prompting GPT to explain its\ndecisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but\ndoes not eliminate, discrepancies between LLM and human responses. While\ndirectly eliciting preferences using LLMs may yield misleading results,\ncombining chain-of-thought conjoint with topic modeling aids in hypothesis\ngeneration, enabling researchers to explore the underpinnings of preferences.\nChain-of-thought conjoint provides a structured framework for marketers to use\nLLMs to identify potential attributes or factors that can explain preference\nheterogeneity across different customers and contexts.", "published": "2023-05-04 03:51:31", "link": "http://arxiv.org/abs/2305.02531v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Hong Kong's Legal Judgments from a Computational Linguistics\n  point-of-view", "abstract": "Analysis and extraction of useful information from legal judgments using\ncomputational linguistics was one of the earliest problems posed in the domain\nof information retrieval. Presently, several commercial vendors exist who\nautomate such tasks. However, a crucial bottleneck arises in the form of\nexorbitant pricing and lack of resources available in analysis of judgements\nmete out by Hong Kong's Legal System. This paper attempts to bridge this gap by\nproviding several statistical, machine learning, deep learning and zero-shot\nlearning based methods to effectively analyze legal judgments from Hong Kong's\nCourt System. The methods proposed consists of: (1) Citation Network Graph\nGeneration, (2) PageRank Algorithm, (3) Keyword Analysis and Summarization, (4)\nSentiment Polarity, and (5) Paragrah Classification, in order to be able to\nextract key insights from individual as well a group of judgments together.\nThis would make the overall analysis of judgments in Hong Kong less tedious and\nmore automated in order to extract insights quickly using fast inferencing. We\nalso provide an analysis of our results by benchmarking our results using Large\nLanguage Models making robust use of the HuggingFace ecosystem.", "published": "2023-05-04 05:23:11", "link": "http://arxiv.org/abs/2305.02558v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "From Statistical Methods to Deep Learning, Automatic Keyphrase\n  Prediction: A Survey", "abstract": "Keyphrase prediction aims to generate phrases (keyphrases) that highly\nsummarizes a given document. Recently, researchers have conducted in-depth\nstudies on this task from various perspectives. In this paper, we\ncomprehensively summarize representative studies from the perspectives of\ndominant models, datasets and evaluation metrics. Our work analyzes up to 167\nprevious works, achieving greater coverage of this task than previous surveys.\nParticularly, we focus highly on deep learning-based keyphrase prediction,\nwhich attracts increasing attention of this task in recent years. Afterwards,\nwe conduct several groups of experiments to carefully compare representative\nmodels. To the best of our knowledge, our work is the first attempt to compare\nthese models using the identical commonly-used datasets and evaluation metric,\nfacilitating in-depth analyses of their disadvantages and advantages. Finally,\nwe discuss the possible research directions of this task in the future.", "published": "2023-05-04 06:22:50", "link": "http://arxiv.org/abs/2305.02579v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective\n  Reasoning", "abstract": "Our investigation into the Affective Reasoning in Conversation (ARC) task\nhighlights the challenge of causal discrimination. Almost all existing models,\nincluding large language models (LLMs), excel at capturing semantic\ncorrelations within utterance embeddings but fall short in determining the\nspecific causal relationships. To overcome this limitation, we propose the\nincorporation of \\textit{i.i.d.} noise terms into the conversation process,\nthereby constructing a structural causal model (SCM). It explores how distinct\ncausal relationships of fitted embeddings can be discerned through independent\nconditions. To facilitate the implementation of deep learning, we introduce the\ncogn frameworks to handle unstructured conversation data, and employ an\nautoencoder architecture to regard the unobservable noise as learnable\n\"implicit causes.\" Moreover, we curate a synthetic dataset that includes i.i.d.\nnoise. Through comprehensive experiments, we validate the effectiveness and\ninterpretability of our approach. Our code is available in\nhttps://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.", "published": "2023-05-04 07:45:49", "link": "http://arxiv.org/abs/2305.02615v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conformal Nucleus Sampling", "abstract": "Language models generate text based on successively sampling the next word. A\ndecoding procedure based on nucleus (top-$p$) sampling chooses from the\nsmallest possible set of words whose cumulative probability exceeds the\nprobability $p$. In this work, we assess whether a top-$p$ set is indeed\naligned with its probabilistic meaning in various linguistic contexts. We\nemploy conformal prediction, a calibration procedure that focuses on the\nconstruction of minimal prediction sets according to a desired confidence\nlevel, to calibrate the parameter $p$ as a function of the entropy of the next\nword distribution. We find that OPT models are overconfident, and that\ncalibration shows a moderate inverse scaling with model size.", "published": "2023-05-04 08:11:57", "link": "http://arxiv.org/abs/2305.02633v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Language-Specific Layers for Multilingual Machine Translation", "abstract": "Multilingual Machine Translation promises to improve translation quality\nbetween non-English languages. This is advantageous for several reasons, namely\nlower latency (no need to translate twice), and reduced error cascades (e.g.,\navoiding losing gender and formality information when translating through\nEnglish). On the downside, adding more languages reduces model capacity per\nlanguage, which is usually countered by increasing the overall model size,\nmaking training harder and inference slower. In this work, we introduce\nLanguage-Specific Transformer Layers (LSLs), which allow us to increase model\ncapacity, while keeping the amount of computation and the number of parameters\nused in the forward pass constant. The key idea is to have some layers of the\nencoder be source or target language-specific, while keeping the remaining\nlayers shared. We study the best way to place these layers using a neural\narchitecture search inspired approach, and achieve an improvement of 1.3 chrF\n(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and\n1.9 chrF (2.2 spBLEU) on a shared decoder one.", "published": "2023-05-04 09:18:05", "link": "http://arxiv.org/abs/2305.02665v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neighboring Words Affect Human Interpretation of Saliency Explanations", "abstract": "Word-level saliency explanations (\"heat maps over words\") are often used to\ncommunicate feature-attribution in text-based models. Recent studies found that\nsuperficial factors such as word length can distort human interpretation of the\ncommunicated saliency scores. We conduct a user study to investigate how the\nmarking of a word's neighboring words affect the explainee's perception of the\nword's importance in the context of a saliency explanation. We find that\nneighboring words have significant effects on the word's importance rating.\nConcretely, we identify that the influence changes based on neighboring\ndirection (left vs. right) and a-priori linguistic and computational measures\nof phrases and collocations (vs. unrelated neighboring words). Our results\nquestion whether text-based saliency explanations should be continued to be\ncommunicated at word level, and inform future research on alternative saliency\nexplanation methods.", "published": "2023-05-04 09:50:25", "link": "http://arxiv.org/abs/2305.02679v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance\n  Representation", "abstract": "Dialogue Topic Segmentation (DTS) plays an essential role in a variety of\ndialogue modeling tasks. Previous DTS methods either focus on semantic\nsimilarity or dialogue coherence to assess topic similarity for unsupervised\ndialogue segmentation. However, the topic similarity cannot be fully identified\nvia semantic similarity or dialogue coherence. In addition, the unlabeled\ndialogue data, which contains useful clues of utterance relationships, remains\nunderexploited. In this paper, we propose a novel unsupervised DTS framework,\nwhich learns topic-aware utterance representations from unlabeled dialogue data\nthrough neighboring utterance matching and pseudo-segmentation. Extensive\nexperiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial)\ndemonstrate that our method significantly outperforms the strong baseline\nmethods. For reproducibility, we provide our code and data\nat:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.", "published": "2023-05-04 11:35:23", "link": "http://arxiv.org/abs/2305.02747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects", "abstract": "Proactive dialogue systems, related to a wide range of real-world\nconversational applications, equip the conversational agent with the capability\nof leading the conversation direction towards achieving pre-defined targets or\nfulfilling certain goals from the system side. It is empowered by advanced\ntechniques to progress to more complicated tasks that require strategical and\nmotivational interactions. In this survey, we provide a comprehensive overview\nof the prominent problems and advanced designs for conversational agent's\nproactivity in different types of dialogues. Furthermore, we discuss challenges\nthat meet the real-world application needs but require a greater research focus\nin the future. We hope that this first survey of proactive dialogue systems can\nprovide the community with a quick access and an overall picture to this\npractical problem, and stimulate more progresses on conversational AI to the\nnext level.", "published": "2023-05-04 11:38:49", "link": "http://arxiv.org/abs/2305.02750v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Model Learning for Various Neural Machine Translation", "abstract": "Existing neural machine translation (NMT) studies mainly focus on developing\ndataset-specific models based on data from different tasks (e.g., document\ntranslation and chat translation). Although the dataset-specific models have\nachieved impressive performance, it is cumbersome as each dataset demands a\nmodel to be designed, trained, and stored. In this work, we aim to unify these\ntranslation tasks into a more general setting. Specifically, we propose a\n``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that\nworks with data from different tasks, and can translate well in multiple\nsettings simultaneously, and theoretically it can be as many as possible.\nThrough unified learning, UMLNMT is able to jointly train across multiple\ntasks, implementing intelligent on-demand translation. On seven widely-used\ntranslation tasks, including sentence translation, document translation, and\nchat translation, our UMLNMT results in substantial improvements over\ndataset-specific models with significantly reduced model deployment costs.\nFurthermore, UMLNMT can achieve competitive or better performance than\nstate-of-the-art dataset-specific methods. Human evaluation and in-depth\nanalysis also demonstrate the superiority of our approach on generating diverse\nand high-quality translations. Additionally, we provide a new genre translation\ndataset about famous aphorisms with 186k Chinese->English sentence pairs.", "published": "2023-05-04 12:21:52", "link": "http://arxiv.org/abs/2305.02777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BranchNorm: Robustly Scaling Extremely Deep Transformers", "abstract": "Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000\nlayers) and reveals the promising potential of deep scaling. To stabilize the\ntraining of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the\nmodel update to a constant value. Although applying such a constraint can\nbenefit the early stage of model training, it may lead to undertrained models\nduring the whole training procedure. In this paper, we propose BranchNorm,\nwhich dynamically rescales the non-residual branch of Transformer in accordance\nwith the training period. BranchNorm not only theoretically stabilizes the\ntraining with smooth gradient norms at the early stage, but also encourages\nbetter convergence in the subsequent training stage. Experiment results on\nmultiple translation tasks demonstrate that BranchNorm achieves a better\ntrade-off between training stability and converge performance.", "published": "2023-05-04 12:46:12", "link": "http://arxiv.org/abs/2305.02790v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interpretable Sentence Representation with Variational Autoencoders and\n  Attention", "abstract": "In this thesis, we develop methods to enhance the interpretability of recent\nrepresentation learning techniques in natural language processing (NLP) while\naccounting for the unavailability of annotated data. We choose to leverage\nVariational Autoencoders (VAEs) due to their efficiency in relating\nobservations to latent generative factors and their effectiveness in\ndata-efficient learning and interpretable representation learning. As a first\ncontribution, we identify and remove unnecessary components in the functioning\nscheme of semi-supervised VAEs making them faster, smaller and easier to\ndesign. Our second and main contribution is to use VAEs and Transformers to\nbuild two models with inductive bias to separate information in latent\nrepresentations into understandable concepts without annotated data. The first\nmodel, Attention-Driven VAE (ADVAE), is able to separately represent and\ncontrol information about syntactic roles in sentences. The second model,\nQKVAE, uses separate latent variables to form keys and values for its\nTransformer decoder and is able to separate syntactic and semantic information\nin its neural representations. In transfer experiments, QKVAE has competitive\nperformance compared to supervised models and equivalent performance to a\nsupervised model using 50K annotated samples. Additionally, QKVAE displays\nimproved syntactic role disentanglement capabilities compared to ADVAE.\nOverall, we demonstrate that it is possible to enhance the interpretability of\nstate-of-the-art deep learning architectures for language modeling with\nunannotated data in situations where text data is abundant but annotations are\nscarce.", "published": "2023-05-04 13:16:15", "link": "http://arxiv.org/abs/2305.02810v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReMask: A Robust Information-Masking Approach for Domain Counterfactual\n  Generation", "abstract": "Domain shift is a big challenge in NLP, thus, many approaches resort to\nlearning domain-invariant features to mitigate the inference phase domain\nshift. Such methods, however, fail to leverage the domain-specific nuances\nrelevant to the task at hand. To avoid such drawbacks, domain counterfactual\ngeneration aims to transform a text from the source domain to a given target\ndomain. However, due to the limited availability of data, such frequency-based\nmethods often miss and lead to some valid and spurious domain-token\nassociations. Hence, we employ a three-step domain obfuscation approach that\ninvolves frequency and attention norm-based masking, to mask domain-specific\ncues, and unmasking to regain the domain generic context. Our experiments\nempirically show that the counterfactual samples sourced from our masked text\nlead to improved domain transfer on 10 out of 12 domain sentiment\nclassification settings, with an average of 2% accuracy improvement over the\nstate-of-the-art for unsupervised domain adaptation (UDA). Further, our model\noutperforms the state-of-the-art by achieving 1.4% average accuracy improvement\nin the adversarial domain adaptation (ADA) setting. Moreover, our model also\nshows its domain adaptation efficacy on a large multi-domain intent\nclassification dataset where it attains state-of-the-art results. We release\nthe codes publicly at \\url{https://github.com/declare-lab/remask}.", "published": "2023-05-04 14:19:02", "link": "http://arxiv.org/abs/2305.02858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing", "abstract": "Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics,\nis getting increasing attention for its detrimental effect on the\ngeneralization ability of NLU models. Existing works focus on eliminating\ndataset bias by down-weighting problematic data in the training process, which\ninduce the omission of valid feature information while mitigating bias. In this\nwork, We analyze the causes of dataset bias from the perspective of causal\ninference and propose CausalAPM, a generalizable literal disentangling\nframework to ameliorate the bias problem from feature granularity. The proposed\napproach projects literal and semantic information into independent feature\nsubspaces, and constrains the involvement of literal information in subsequent\npredictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and\nQQP) demonstrate that our proposed framework significantly improves the OOD\ngeneralization performance while maintaining ID performance.", "published": "2023-05-04 14:22:26", "link": "http://arxiv.org/abs/2305.02865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An automatically discovered chain-of-thought prompt generalizes to novel\n  models and datasets", "abstract": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improve\nperformance and explainability of large language models (LLMs). However,\nuncertainties remain about how reasoning strategies formulated for previous\nmodel generations generalize to new model generations and different datasets.\nIn this small-scale study, we compare different reasoning strategies induced by\nzero-shot prompting across six recently released LLMs (davinci-002,\ndavinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a\nmixture of six question-answering datasets, including datasets from scientific\nand medical domains. Our findings demonstrate that while some variations in\neffectiveness occur, gains from CoT reasoning strategies remain robust across\ndifferent models and datasets. GPT-4 has the most benefit from current\nstate-of-the-art reasoning strategies and exhibits the best performance by\napplying a prompt previously discovered through automated discovery.", "published": "2023-05-04 15:07:20", "link": "http://arxiv.org/abs/2305.02897v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence Embedding Leaks More Information than You Expect: Generative\n  Embedding Inversion Attack to Recover the Whole Sentence", "abstract": "Sentence-level representations are beneficial for various natural language\nprocessing tasks. It is commonly believed that vector representations can\ncapture rich linguistic properties. Currently, large language models (LMs)\nachieve state-of-the-art performance on sentence embedding. However, some\nrecent works suggest that vector representations from LMs can cause information\nleakage. In this work, we further investigate the information leakage issue and\npropose a generative embedding inversion attack (GEIA) that aims to reconstruct\ninput sequences based only on their sentence embeddings. Given the black-box\naccess to a language model, we treat sentence embeddings as initial tokens'\nrepresentations and train or fine-tune a powerful decoder model to decode the\nwhole sequences directly. We conduct extensive experiments to demonstrate that\nour generative inversion attack outperforms previous embedding inversion\nattacks in classification metrics and generates coherent and contextually\nsimilar sentences as the original inputs.", "published": "2023-05-04 17:31:41", "link": "http://arxiv.org/abs/2305.03010v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Panda LLM: Training Data and Evaluation for Open-Sourced Chinese\n  Instruction-Following Large Language Models", "abstract": "This project focuses on enhancing open-source large language models through\ninstruction-tuning and providing comprehensive evaluations of their\nperformance. We explore how various training data factors, such as quantity,\nquality, and linguistic distribution, influence the performance of\ninstruction-tuned models trained on publicly accessible high-quality\ninstruction datasets for both English and Chinese languages. Our goal is to\nsupplement evaluation with quantitative analyses, providing valuable insights\nfor the continued advancement of open-source chat models. Our model, data, and\ncode are publicly available for others to use and build upon.", "published": "2023-05-04 17:49:09", "link": "http://arxiv.org/abs/2305.03025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What changes when you randomly choose BPE merge operations? Not much", "abstract": "We introduce three simple randomized variants of byte pair encoding (BPE) and\nexplore whether randomizing the selection of merge operations substantially\naffects a downstream machine translation task. We focus on translation into\nmorphologically rich languages, hypothesizing that this task may show\nsensitivity to the method of choosing subwords. Analysis using a Bayesian\nlinear model indicates that two of the variants perform nearly\nindistinguishably compared to standard BPE while the other degrades performance\nless than we anticipated. We conclude that although standard BPE is widely\nused, there exists an interesting universe of potential variations on it worth\ninvestigating. Our code is available at: https://github.com/bltlab/random-bpe.", "published": "2023-05-04 17:53:27", "link": "http://arxiv.org/abs/2305.03029v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational\n  Question Generation", "abstract": "Conversational Question Generation (CQG) is a critical task for machines to\nassist humans in fulfilling their information needs through conversations. The\ntask is generally cast into two different settings: answer-aware and\nanswer-unaware. While the former facilitates the models by exposing the\nexpected answer, the latter is more realistic and receiving growing attentions\nrecently. What-to-ask and how-to-ask are the two main challenges in the\nanswer-unaware setting. To address the first challenge, existing methods mainly\nselect sequential sentences in context as the rationales. We argue that the\nconversation generated using such naive heuristics may not be natural enough as\nin reality, the interlocutors often talk about the relevant contents that are\nnot necessarily sequential in context. Additionally, previous methods decide\nthe type of question to be generated (boolean/span-based) implicitly. Modeling\nthe question type explicitly is crucial as the answer, which hints the models\nto generate a boolean or span-based question, is unavailable. To this end, we\npresent SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a\nsentence is selected as the rationale from a semantic graph that we construct,\nand extract the answer span from it. For the how-to-ask stage, a classifier\ndetermines the target answer type of the question via two explicit control\nsignals before generating and filtering. In addition, we propose Conv-Distinct,\na novel evaluation metric for CQG, to evaluate the diversity of the generated\nconversation from a context. Compared with the existing answer-unaware CQG\nmodels, the proposed SG-CQG achieves state-of-the-art performance.", "published": "2023-05-04 18:06:48", "link": "http://arxiv.org/abs/2305.03088v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Curating corpora with classifiers: A case study of clean energy\n  sentiment online", "abstract": "Well curated, large-scale corpora of social media posts containing broad\npublic opinion offer an alternative data source to complement traditional\nsurveys. While surveys are effective at collecting representative samples and\nare capable of achieving high accuracy, they can be both expensive to run and\nlag public opinion by days or weeks. Both of these drawbacks could be overcome\nwith a real-time, high volume data stream and fast analysis pipeline. A central\nchallenge in orchestrating such a data pipeline is devising an effective method\nfor rapidly selecting the best corpus of relevant documents for analysis.\nQuerying with keywords alone often includes irrelevant documents that are not\neasily disambiguated with bag-of-words natural language processing methods.\nHere, we explore methods of corpus curation to filter irrelevant tweets using\npre-trained transformer-based models, fine-tuned for our binary classification\ntask on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.\nThe low cost and high performance of fine-tuning such a model suggests that our\napproach could be of broad benefit as a pre-processing step for social media\ndatasets with uncertain corpus boundaries.", "published": "2023-05-04 18:15:45", "link": "http://arxiv.org/abs/2305.03092v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing Pashto Text Classification using Language Processing\n  Techniques for Single And Multi-Label Analysis", "abstract": "Text classification has become a crucial task in various fields, leading to a\nsignificant amount of research on developing automated text classification\nsystems for national and international languages. However, there is a growing\nneed for automated text classification systems that can handle local languages.\nThis study aims to establish an automated classification system for Pashto\ntext. To achieve this goal, we constructed a dataset of Pashto documents and\napplied various models, including statistical and neural machine learning\nmodels such as DistilBERT-base-multilingual-cased, Multilayer Perceptron,\nSupport Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\\\"ive\nBayes, multinomial na\\\"ive Bayes, random forest, and logistic regression, to\nidentify the most effective approach. We also evaluated two different feature\nextraction methods, bag of words and Term Frequency Inverse Document Frequency.\nThe study achieved an average testing accuracy rate of 94% using the MLP\nclassification algorithm and TFIDF feature extraction method in single-label\nmulticlass classification. Similarly, MLP+TFIDF yielded the best results, with\nan F1-measure of 0.81. Furthermore, the use of pre-trained language\nrepresentation models, such as DistilBERT, showed promising results for Pashto\ntext classification; however, the study highlights the importance of developing\na specific tokenizer for a particular language to achieve reasonable results.", "published": "2023-05-04 23:11:31", "link": "http://arxiv.org/abs/2305.03201v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation", "abstract": "We propose a new two-stage pre-training framework for video-to-text\ngeneration tasks such as video captioning and video question answering: A\ngenerative encoder-decoder model is first jointly pre-trained on massive\nimage-text data to learn fundamental vision-language concepts, and then adapted\nto video data in an intermediate video-text pre-training stage to learn\nvideo-specific skills such as spatio-temporal reasoning. As a result, our\nVideoOFA model achieves new state-of-the-art performance on four Video\nCaptioning benchmarks, beating prior art by an average of 9.7 points in CIDEr\nscore. It also outperforms existing models on two open-ended Video Question\nAnswering datasets, showcasing its generalization capability as a universal\nvideo-to-text model.", "published": "2023-05-04 23:27:21", "link": "http://arxiv.org/abs/2305.03204v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Investigating Lexical Sharing in Multilingual Machine Translation for\n  Indian Languages", "abstract": "Multilingual language models have shown impressive cross-lingual transfer\nability across a diverse set of languages and tasks. To improve the\ncross-lingual ability of these models, some strategies include transliteration\nand finer-grained segmentation into characters as opposed to subwords. In this\nwork, we investigate lexical sharing in multilingual machine translation (MT)\nfrom Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist\nin translation performance between data sampling and vocabulary size, and we\nexplore whether transliteration is useful in encouraging cross-script\ngeneralisation. We also verify how the different settings generalise to unseen\nlanguages (Marathi and Bengali). We find that transliteration does not give\npronounced improvements and our analysis suggests that our multilingual MT\nmodels trained on original scripts seem to already be robust to cross-script\ndifferences even for relatively low-resource languages", "published": "2023-05-04 23:35:15", "link": "http://arxiv.org/abs/2305.03207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Domain-Adaptive Visually-fused Event Detection from Text", "abstract": "Incorporating auxiliary modalities such as images into event detection models\nhas attracted increasing interest over the last few years. The complexity of\nnatural language in describing situations has motivated researchers to leverage\nthe related visual context to improve event detection performance. However,\ncurrent approaches in this area suffer from data scarcity, where a large amount\nof labelled text-image pairs are required for model training. Furthermore,\nlimited access to the visual context at inference time negatively impacts the\nperformance of such models, which makes them practically ineffective in\nreal-world scenarios. In this paper, we present a novel domain-adaptive\nvisually-fused event detection approach that can be trained on a few labelled\nimage-text paired data points. Specifically, we introduce a visual imaginator\nmethod that synthesises images from text in the absence of visual context.\nMoreover, the imaginator can be customised to a specific domain. In doing so,\nour model can leverage the capabilities of pre-trained vision-language models\nand can be trained in a few-shot setting. This also allows for effective\ninference where only single-modality data (i.e. text) is available. The\nexperimental evaluation on the benchmark M2E2 dataset shows that our model\noutperforms existing state-of-the-art models, by up to 11 points.", "published": "2023-05-04 00:10:57", "link": "http://arxiv.org/abs/2305.03517v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Black-box Prompt Tuning with Subspace Learning", "abstract": "Black-box prompt tuning employs derivative-free optimization algorithms to\nlearn prompts within low-dimensional subspaces rather than back-propagating\nthrough the network of Large Language Models (LLMs). Recent studies reveal that\nblack-box prompt tuning lacks versatility across tasks and LLMs, which we\nbelieve is related to the suboptimal choice of subspaces. In this paper, we\nintroduce Black-box prompt tuning with Subspace Learning (BSL) to enhance the\nversatility of black-box prompt tuning. Based on the assumption that nearly\noptimal prompts for similar tasks reside in a common subspace, we propose\nidentifying such subspaces through meta-learning on a collection of similar\nsource tasks. Consequently, for a target task that shares similarities with the\nsource tasks, we expect that optimizing within the identified subspace can\nyield a prompt that performs well on the target task. Experimental results\nconfirm that our BSL framework consistently achieves competitive performance\nacross various downstream tasks and LLMs.", "published": "2023-05-04 01:04:25", "link": "http://arxiv.org/abs/2305.03518v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversational Semantic Parsing using Dynamic Context Graphs", "abstract": "In this paper we consider the task of conversational semantic parsing over\ngeneral purpose knowledge graphs (KGs) with millions of entities, and thousands\nof relation-types. We focus on models which are capable of interactively\nmapping user utterances into executable logical forms (e.g., Sparql) in the\ncontext of the conversational history. Our key idea is to represent information\nabout an utterance and its context via a subgraph which is created dynamically,\ni.e., the number of nodes varies per utterance. Rather than treating the\nsubgraph as a sequence, we exploit its underlying structure and encode it with\na graph neural network which further allows us to represent a large number of\n(unseen) nodes. Experimental results show that dynamic context modeling is\nsuperior to static approaches, delivering performance improvements across the\nboard (i.e., for simple and complex questions). Our results further confirm\nthat modeling the structure of context is better at processing discourse\ninformation, (i.e., at handling ellipsis and resolving coreference) and longer\ninteractions.", "published": "2023-05-04 16:04:41", "link": "http://arxiv.org/abs/2305.06164v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitive Reframing of Negative Thoughts through Human-Language Model\n  Interaction", "abstract": "A proven therapeutic technique to overcome negative thoughts is to replace\nthem with a more hopeful \"reframed thought.\" Although therapy can help people\npractice and learn this Cognitive Reframing of Negative Thoughts, clinician\nshortages and mental health stigma commonly limit people's access to therapy.\nIn this paper, we conduct a human-centered study of how language models may\nassist people in reframing negative thoughts. Based on psychology literature,\nwe define a framework of seven linguistic attributes that can be used to\nreframe a thought. We develop automated metrics to measure these attributes and\nvalidate them with expert judgements from mental health practitioners. We\ncollect a dataset of 600 situations, thoughts and reframes from practitioners\nand use it to train a retrieval-enhanced in-context learning model that\neffectively generates reframed thoughts and controls their linguistic\nattributes. To investigate what constitutes a \"high-quality\" reframe, we\nconduct an IRB-approved randomized field study on a large mental health website\nwith over 2,000 participants. Amongst other findings, we show that people\nprefer highly empathic or specific reframes, as opposed to reframes that are\noverly positive. Our findings provide key implications for the use of LMs to\nassist people in overcoming negative thoughts.", "published": "2023-05-04 00:12:52", "link": "http://arxiv.org/abs/2305.02466v1", "categories": ["cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "PersonaLLM: Investigating the Ability of Large Language Models to\n  Express Personality Traits", "abstract": "Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents which we refer to as LLM personas and present a case study with GPT-3.5\nand GPT-4 to investigate whether LLMs can generate content that aligns with\ntheir assigned personality profiles. To this end, we simulate distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, LLM personas' writings have emerging representative\nlinguistic patterns for personality traits when compared with a human writing\ncorpus. Furthermore, human evaluation shows that humans can perceive some\npersonality traits with an accuracy of up to 80%. Interestingly, the accuracy\ndrops significantly when the annotators were informed of AI authorship.", "published": "2023-05-04 04:58:00", "link": "http://arxiv.org/abs/2305.02547v5", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "FormNetV2: Multimodal Graph Contrastive Learning for Form Document\n  Information Extraction", "abstract": "The recent advent of self-supervised pre-training techniques has led to a\nsurge in the use of multimodal learning in form document understanding.\nHowever, existing approaches that extend the mask language modeling to other\nmodalities require careful multi-task tuning, complex reconstruction target\ndesigns, or additional pre-training data. In FormNetV2, we introduce a\ncentralized multimodal graph contrastive learning strategy to unify\nself-supervised pre-training for all modalities in one loss. The graph\ncontrastive objective maximizes the agreement of multimodal representations,\nproviding a natural interplay for all modalities without special customization.\nIn addition, we extract image features within the bounding box that joins a\npair of tokens connected by a graph edge, capturing more targeted visual cues\nwithout loading a sophisticated and separately pre-trained image embedder.\nFormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE\nand Payment benchmarks with a more compact model size.", "published": "2023-05-04 05:02:04", "link": "http://arxiv.org/abs/2305.02549v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A framework for the emergence and analysis of language in social\n  learning agents", "abstract": "Artificial neural networks (ANNs) are increasingly used as research models,\nbut questions remain about their generalizability and representational\ninvariance. Biological neural networks under social constraints evolved to\nenable communicable representations, demonstrating generalization capabilities.\nThis study proposes a communication protocol between cooperative agents to\nanalyze the formation of individual and shared abstractions and their impact on\ntask performance. This communication protocol aims to mimic language features\nby encoding high-dimensional information through low-dimensional\nrepresentation. Using grid-world mazes and reinforcement learning, teacher ANNs\npass a compressed message to a student ANN for better task completion. Through\nthis, the student achieves a higher goal-finding rate and generalizes the goal\nlocation across task worlds. Further optimizing message content to maximize\nstudent reward improves information encoding, suggesting that an accurate\nrepresentation in the space of messages requires bi-directional input. This\nhighlights the role of language as a common representation between agents and\nits implications on generalization capabilities.", "published": "2023-05-04 08:11:01", "link": "http://arxiv.org/abs/2305.02632v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Weakly-Supervised Hate Speech Classification Across Datasets", "abstract": "As pointed out by several scholars, current research on hate speech (HS)\nrecognition is characterized by unsystematic data creation strategies and\ndiverging annotation schemata. Subsequently, supervised-learning models tend to\ngeneralize poorly to datasets they were not trained on, and the performance of\nthe models trained on datasets labeled using different HS taxonomies cannot be\ncompared. To ease this problem, we propose applying extremely weak supervision\nthat only relies on the class name rather than on class samples from the\nannotated data. We demonstrate the effectiveness of a state-of-the-art\nweakly-supervised text classification model in various in-dataset and\ncross-dataset settings. Furthermore, we conduct an in-depth quantitative and\nqualitative analysis of the source of poor generalizability of HS\nclassification models.", "published": "2023-05-04 08:15:40", "link": "http://arxiv.org/abs/2305.02637v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Big Data and Large Numbers. Interpreting Zipf's Law", "abstract": "It turns out that some empirical facts in Big Data are the effects of\nproperties of large numbers. Zipf's law 'noise' is an example of such an\nartefact. We expose several properties of the power law distributions and of\nsimilar distribution that occur when the population is finite and the rank and\ncounts of elements in the population are natural numbers. We are particularly\nconcerned with the low-rank end of the graph of the law, the potential of noise\nin the law, and with the approximation of the number of types of objects at\nvarious ranks. Approximations instead of exact solutions are the center of\nattention. Consequences in the interpretation of Zipf's law are discussed.", "published": "2023-05-04 10:03:37", "link": "http://arxiv.org/abs/2305.02687v2", "categories": ["physics.soc-ph", "cs.CL", "math.ST", "stat.TH", "11, 62", "G.2; G.3; I.2.7; C.2"], "primary_category": "physics.soc-ph"}
{"title": "An Asynchronous Updating Reinforcement Learning Framework for\n  Task-oriented Dialog System", "abstract": "Reinforcement learning has been applied to train the dialog systems in many\nworks. Previous approaches divide the dialog system into multiple modules\nincluding DST (dialog state tracking) and DP (dialog policy), and train these\nmodules simultaneously. However, different modules influence each other during\ntraining. The errors from DST might misguide the dialog policy, and the system\naction brings extra difficulties for the DST module. To alleviate this problem,\nwe propose Asynchronous Updating Reinforcement Learning framework (AURL) that\nupdates the DST module and the DP module asynchronously under a cooperative\nsetting. Furthermore, curriculum learning is implemented to address the problem\nof unbalanced data distribution during reinforcement learning sampling, and\nmultiple user models are introduced to increase the dialog diversity. Results\non the public SSD-PHONE dataset show that our method achieves a compelling\nresult with a 31.37% improvement on the dialog success rate. The code is\npublicly available via https://github.com/shunjiu/AURL.", "published": "2023-05-04 10:39:17", "link": "http://arxiv.org/abs/2305.02718v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "VendorLink: An NLP approach for Identifying & Linking Vendor Migrants &\n  Potential Aliases on Darknet Markets", "abstract": "The anonymity on the Darknet allows vendors to stay undetected by using\nmultiple vendor aliases or frequently migrating between markets. Consequently,\nillegal markets and their connections are challenging to uncover on the\nDarknet. To identify relationships between illegal markets and their vendors,\nwe propose VendorLink, an NLP-based approach that examines writing patterns to\nverify, identify, and link unique vendor accounts across text advertisements\n(ads) on seven public Darknet markets. In contrast to existing literature,\nVendorLink utilizes the strength of supervised pre-training to perform\nclosed-set vendor verification, open-set vendor identification, and\nlow-resource market adaption tasks. Through VendorLink, we uncover (i) 15\nmigrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17\nmigrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii)\n75 migrants and 10 potential aliases in the Traderoute-Agora dataset.\nAltogether, our approach can help Law Enforcement Agencies (LEA) make more\ninformed decisions by verifying and identifying migrating vendors and their\npotential aliases on existing and Low-Resource (LR) emerging Darknet markets.", "published": "2023-05-04 12:04:33", "link": "http://arxiv.org/abs/2305.02763v1", "categories": ["cs.CY", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "The Politics of Language Choice: How the Russian-Ukrainian War\n  Influences Ukrainians' Language Use on Twitter", "abstract": "The use of language is innately political and often a vehicle of cultural\nidentity as well as the basis for nation building. Here, we examine language\nchoice and tweeting activity of Ukrainian citizens based on more than 4 million\ngeo-tagged tweets from over 62,000 users before and during the\nRussian-Ukrainian War, from January 2020 to October 2022. Using statistical\nmodels, we disentangle sample effects, arising from the in- and outflux of\nusers on Twitter, from behavioural effects, arising from behavioural changes of\nthe users. We observe a steady shift from the Russian language towards the\nUkrainian language already before the war, which drastically speeds up with its\noutbreak. We attribute these shifts in large part to users' behavioural\nchanges. Notably, we find that more than half of the Russian-tweeting users\nshift towards Ukrainian as a result of the war.", "published": "2023-05-04 12:15:54", "link": "http://arxiv.org/abs/2305.02770v3", "categories": ["cs.CY", "cs.CL", "stat.AP"], "primary_category": "cs.CY"}
{"title": "End-to-end spoken language understanding using joint CTC loss and\n  self-supervised, pretrained acoustic encoders", "abstract": "It is challenging to extract semantic meanings directly from audio signals in\nspoken language understanding (SLU), due to the lack of textual information.\nPopular end-to-end (E2E) SLU models utilize sequence-to-sequence automatic\nspeech recognition (ASR) models to extract textual embeddings as input to infer\nsemantics, which, however, require computationally expensive auto-regressive\ndecoding. In this work, we leverage self-supervised acoustic encoders\nfine-tuned with Connectionist Temporal Classification (CTC) to extract textual\nembeddings and use joint CTC and SLU losses for utterance-level SLU tasks.\nExperiments show that our model achieves 4% absolute improvement over the the\nstate-of-the-art (SOTA) dialogue act classification model on the DSTC2 dataset\nand 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.", "published": "2023-05-04 15:36:37", "link": "http://arxiv.org/abs/2305.02937v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for\n  Clinical Trial Data", "abstract": "This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence\nNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2\ntasks, a Natural Language Inference (NLI) task, and an evidence selection task\non clinical trial data. The proposed challenges require multi-hop biomedical\nand numerical reasoning, which are of significant importance to the development\nof systems capable of large-scale interpretation and retrieval of medical\nevidence, to provide personalized evidence-based care.\n  Task 1, the entailment task, received 643 submissions from 40 participants,\nand Task 2, the evidence selection task, received 364 submissions from 23\nparticipants. The tasks are challenging, with the majority of submitted systems\nfailing to significantly outperform the majority class baseline on the\nentailment task, and we observe significantly better performance on the\nevidence selection task than on the entailment task. Increasing the number of\nmodel parameters leads to a direct increase in performance, far more\nsignificant than the effect of biomedical pre-training. Future works could\nexplore the limitations of large models for generalization and numerical\ninference, and investigate methods to augment clinical datasets to allow for\nmore rigorous testing and to facilitate fine-tuning.\n  We envisage that the dataset, models, and results of this task will be useful\nto the biomedical NLI and evidence retrieval communities. The dataset,\ncompetition leaderboard, and website are publicly available.", "published": "2023-05-04 16:58:19", "link": "http://arxiv.org/abs/2305.02993v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accuracy on the Curve: On the Nonlinear Correlation of ML Performance\n  Between Data Subpopulations", "abstract": "Understanding the performance of machine learning (ML) models across diverse\ndata distributions is critically important for reliable applications. Despite\nrecent empirical studies positing a near-perfect linear correlation between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically\ndemonstrate that this correlation is more nuanced under subpopulation shifts.\nThrough rigorous experimentation and analysis across a variety of datasets,\nmodels, and training epochs, we demonstrate that OOD performance often has a\nnonlinear correlation with ID performance in subpopulation shifts. Our\nfindings, which contrast previous studies that have posited a linear\ncorrelation in model performance during distribution shifts, reveal a \"moon\nshape\" correlation (parabolic uptrend curve) between the test performance on\nthe majority subpopulation and the minority subpopulation. This non-trivial\nnonlinear correlation holds across model architectures, hyperparameters,\ntraining durations, and the imbalance between subpopulations. Furthermore, we\nfound that the nonlinearity of this \"moon shape\" is causally influenced by the\ndegree of spurious correlations in the training data. Our controlled\nexperiments show that stronger spurious correlation in the training data\ncreates more nonlinear performance correlation. We provide complementary\nexperimental and theoretical analyses for this phenomenon, and discuss its\nimplications for ML reliability and fairness. Our work highlights the\nimportance of understanding the nonlinear effects of model improvement on\nperformance in different subpopulations, and has the potential to inform the\ndevelopment of more equitable and responsible machine learning models.", "published": "2023-05-04 17:00:17", "link": "http://arxiv.org/abs/2305.02995v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR\n  Decomposition", "abstract": "Cross-encoder models, which jointly encode and score a query-item pair, are\nprohibitively expensive for direct k-nearest neighbor (k-NN) search.\nConsequently, k-NN search typically employs a fast approximate retrieval (e.g.\nusing BM25 or dual-encoder vectors), followed by reranking with a\ncross-encoder; however, the retrieval approximation often has detrimental\nrecall regret. This problem is tackled by ANNCUR (Yadav et al., 2022), a recent\nwork that employs a cross-encoder only, making search efficient using a\nrelatively small number of anchor items, and a CUR matrix factorization. While\nANNCUR's one-time selection of anchors tends to approximate the cross-encoder\ndistances on average, doing so forfeits the capacity to accurately estimate\ndistances to items near the query, leading to regret in the crucial end-task:\nrecall of top-k items. In this paper, we propose ADACUR, a method that\nadaptively, iteratively, and efficiently minimizes the approximation error for\nthe practically important top-k neighbors. It does so by iteratively performing\nk-NN search using the anchors available so far, then adding these retrieved\nnearest neighbors to the anchor set for the next round. Empirically, on\nmultiple datasets, in comparison to previous traditional and state-of-the-art\nmethods such as ANNCUR and dual-encoder-based retrieve-and-rerank, our proposed\napproach ADACUR consistently reduces recall error-by up to 70% on the important\nk = 1 setting-while using no more compute than its competitors.", "published": "2023-05-04 17:01:17", "link": "http://arxiv.org/abs/2305.02996v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Principle-Driven Self-Alignment of Language Models from Scratch with\n  Minimal Human Supervision", "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including < 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.", "published": "2023-05-04 17:59:28", "link": "http://arxiv.org/abs/2305.03047v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for\n  Speech-to-Text Tasks", "abstract": "Transducer and Attention based Encoder-Decoder (AED) are two widely used\nframeworks for speech-to-text tasks. They are designed for different purposes\nand each has its own benefits and drawbacks for speech-to-text tasks. In order\nto leverage strengths of both modeling methods, we propose a solution by\ncombining Transducer and Attention based Encoder-Decoder (TAED) for\nspeech-to-text tasks. The new method leverages AED's strength in non-monotonic\nsequence to sequence learning while retaining Transducer's streaming property.\nIn the proposed framework, Transducer and AED share the same speech encoder.\nThe predictor in Transducer is replaced by the decoder in the AED model, and\nthe outputs of the decoder are conditioned on the speech inputs instead of\noutputs from an unconditioned language model. The proposed solution ensures\nthat the model is optimized by covering all possible read/write scenarios and\ncreates a matched environment for streaming applications. We evaluate the\nproposed approach on the \\textsc{MuST-C} dataset and the findings demonstrate\nthat TAED performs significantly better than Transducer for offline automatic\nspeech recognition (ASR) and speech-to-text translation (ST) tasks. In the\nstreaming case, TAED outperforms Transducer in the ASR task and one ST\ndirection while comparable results are achieved in another translation\ndirection.", "published": "2023-05-04 18:34:50", "link": "http://arxiv.org/abs/2305.03101v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Influence of various text embeddings on clustering performance in NLP", "abstract": "With the advent of e-commerce platforms, reviews are crucial for customers to\nassess the credibility of a product. The star ratings do not always match the\nreview text written by the customer. For example, a three star rating (out of\nfive) may be incongruous with the review text, which may be more suitable for a\nfive star review. A clustering approach can be used to relabel the correct star\nratings by grouping the text reviews into individual groups. In this work, we\nexplore the task of choosing different text embeddings to represent these\nreviews and also explore the impact the embedding choice has on the performance\nof various classes of clustering algorithms. We use contextual (BERT) and\nnon-contextual (Word2Vec) text embeddings to represent the text and measure\ntheir impact of three classes on clustering algorithms - partitioning based\n(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN\nand HDBSCAN), each with various experimental settings. We use the silhouette\nscore, adjusted rand index score, and cluster purity score metrics to evaluate\nthe performance of the algorithms and discuss the impact of different\nembeddings on the clustering performance. Our results indicate that the type of\nembedding chosen drastically affects the performance of the algorithm, the\nperformance varies greatly across different types of clustering algorithms, no\nembedding type is better than the other, and DBSCAN outperforms KMeans and\nsingle linkage agglomerative clustering but also labels more data points as\noutliers. We provide a thorough comparison of the performances of different\nalgorithms and provide numerous ideas to foster further research in the domain\nof text clustering.", "published": "2023-05-04 20:53:19", "link": "http://arxiv.org/abs/2305.03144v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Gpt-4: A Review on Advancements and Opportunities in Natural Language\n  Processing", "abstract": "Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation\nlanguage model in the GPT series, developed by OpenAI, which promises\nsignificant advancements in the field of natural language processing (NLP). In\nthis research article, we have discussed the features of GPT-4, its potential\napplications, and the challenges that it might face. We have also compared\nGPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one\ntrillion), better multilingual capabilities, improved contextual understanding,\nand reasoning capabilities than GPT-3. Some of the potential applications of\nGPT-4 include chatbots, personal assistants, language translation, text\nsummarization, and question-answering. However, GPT-4 poses several challenges\nand limitations such as computational requirements, data requirements, and\nethical concerns.", "published": "2023-05-04 22:46:43", "link": "http://arxiv.org/abs/2305.03195v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Employing Hybrid Deep Neural Networks on Dari Speech", "abstract": "This paper is an extension of our previous conference paper. In recent years,\nthere has been a growing interest among researchers in developing and improving\nspeech recognition systems to facilitate and enhance human-computer\ninteraction. Today, Automatic Speech Recognition (ASR) systems have become\nubiquitous, used in everything from games to translation systems, robots, and\nmore. However, much research is still needed on speech recognition systems for\nlow-resource languages. This article focuses on the recognition of individual\nwords in the Dari language using the Mel-frequency cepstral coefficients\n(MFCCs) feature extraction method and three different deep neural network\nmodels: Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and\nMultilayer Perceptron (MLP), as well as two hybrid models combining CNN and\nRNN. We evaluate these models using an isolated Dari word corpus that we have\ncreated, consisting of 1000 utterances for 20 short Dari terms. Our study\nachieved an impressive average accuracy of 98.365%.", "published": "2023-05-04 23:10:53", "link": "http://arxiv.org/abs/2305.03200v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AttentionViz: A Global View of Transformer Attention", "abstract": "Transformer models are revolutionizing machine learning, but their inner\nworkings remain mysterious. In this work, we present a new visualization\ntechnique designed to help researchers understand the self-attention mechanism\nin transformers that allows these models to learn rich, contextual\nrelationships between elements of a sequence. The main idea behind our method\nis to visualize a joint embedding of the query and key vectors used by\ntransformer models to compute attention. Unlike previous attention\nvisualization techniques, our approach enables the analysis of global patterns\nacross multiple input sequences. We create an interactive visualization tool,\nAttentionViz (demo: http://attentionviz.com), based on these joint query-key\nembeddings, and use it to study attention mechanisms in both language and\nvision transformers. We demonstrate the utility of our approach in improving\nmodel understanding and offering new insights about query-key interactions\nthrough several application scenarios and expert feedback.", "published": "2023-05-04 23:46:49", "link": "http://arxiv.org/abs/2305.03210v2", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "abstract": "Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.", "published": "2023-05-04 15:15:22", "link": "http://arxiv.org/abs/2305.03495v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention\n  for Emotion Recognition in Conversation", "abstract": "Emotion Recognition in Conversation~(ERC) across modalities is of vital\nimportance for a variety of applications, including intelligent healthcare,\nartificial intelligence for conversation, and opinion mining over chat history.\nThe crux of ERC is to model both cross-modality and cross-time interactions\nthroughout the conversation. Previous methods have made progress in learning\nthe time series information of conversation while lacking the ability to trace\ndown the different emotional states of each speaker in a conversation. In this\npaper, we propose a recurrent structure called Speaker Information Enhanced\nLong-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states\nof the distinct speaker can be tracked in a sequential way to enhance the\nlearning of the emotion in conversation. Further, to improve the learning of\nmultimodal features in ERC, we utilize a cross-modal attention component to\nfuse the features between different modalities and model the interaction of the\nimportant information from different modalities. Experimental results on two\nbenchmark datasets demonstrate the superiority of the proposed SI-LSTM against\nthe state-of-the-art baseline methods in the ERC task on multimodal data.", "published": "2023-05-04 10:13:15", "link": "http://arxiv.org/abs/2305.03506v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable\n  Diffusion", "abstract": "Diffusion-based generative models' impressive ability to create convincing\nimages has garnered global attention. However, their complex structures and\noperations often pose challenges for non-experts to grasp. We present Diffusion\nExplainer, the first interactive visualization tool that explains how Stable\nDiffusion transforms text prompts into images. Diffusion Explainer tightly\nintegrates a visual overview of Stable Diffusion's complex structure with\nexplanations of the underlying operations. By comparing image generation of\nprompt variants, users can discover the impact of keyword changes on image\ngeneration. A 56-participant user study demonstrates that Diffusion Explainer\noffers substantial learning benefits to non-experts. Our tool has been used by\nover 10,300 users from 124 countries at\nhttps://poloclub.github.io/diffusion-explainer/.", "published": "2023-05-04 16:14:43", "link": "http://arxiv.org/abs/2305.03509v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tuning Traditional Language Processing Approaches for Pashto Text\n  Classification", "abstract": "Today text classification becomes critical task for concerned individuals for\nnumerous purposes. Hence, several researches have been conducted to develop\nautomatic text classification for national and international languages.\nHowever, the need for an automatic text categorization system for local\nlanguages is felt. The main aim of this study is to establish a Pashto\nautomatic text classification system. In order to pursue this work, we built a\nPashto corpus which is a collection of Pashto documents due to the\nunavailability of public datasets of Pashto text documents. Besides, this study\ncompares several models containing both statistical and neural network machine\nlearning techniques including Multilayer Perceptron (MLP), Support Vector\nMachine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\\\"ive Bayes,\nmultinomial na\\\"ive Bayes, random forest, and logistic regression to discover\nthe most effective approach. Moreover, this investigation evaluates two\ndifferent feature extraction methods including unigram, and Time Frequency\nInverse Document Frequency (IFIDF). Subsequently, this research obtained\naverage testing accuracy rate 94% using MLP classification algorithm and TFIDF\nfeature extraction method in this context.", "published": "2023-05-04 22:57:45", "link": "http://arxiv.org/abs/2305.03737v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-grained Hypergraph Interest Modeling for Conversational\n  Recommendation", "abstract": "Conversational recommender system (CRS) interacts with users through\nmulti-turn dialogues in natural language, which aims to provide high-quality\nrecommendations for user's instant information need. Although great efforts\nhave been made to develop effective CRS, most of them still focus on the\ncontextual information from the current dialogue, usually suffering from the\ndata scarcity issue. Therefore, we consider leveraging historical dialogue data\nto enrich the limited contexts of the current dialogue session.\n  In this paper, we propose a novel multi-grained hypergraph interest modeling\napproach to capture user interest beneath intricate historical data from\ndifferent perspectives. As the core idea, we employ hypergraph to represent\ncomplicated semantic relations underlying historical dialogues. In our\napproach, we first employ the hypergraph structure to model users' historical\ndialogue sessions and form a session-based hypergraph, which captures\ncoarse-grained, session-level relations. Second, to alleviate the issue of data\nscarcity, we use an external knowledge graph and construct a knowledge-based\nhypergraph considering fine-grained, entity-level semantics. We further conduct\nmulti-grained hypergraph convolution on the two kinds of hypergraphs, and\nutilize the enhanced representations to develop interest-aware CRS. Extensive\nexperiments on two benchmarks ReDial and TG-ReDial validate the effectiveness\nof our approach on both recommendation and conversation tasks. Code is\navailable at: https://github.com/RUCAIBox/MHIM.", "published": "2023-05-04 13:13:44", "link": "http://arxiv.org/abs/2305.04798v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Analysis of Visual Question Answering Algorithms with attention model", "abstract": "Visual question answering (VQA) usesimage processing algorithms to process\nthe image and natural language processing methods to understand and answer the\nquestion. VQA is helpful to a visually impaired person, can be used for the\nsecurity surveillance system and online chatbots that learn from the web. It\nuses NLP methods to learn the semantic of the question and to derive the\ntextual features. Computer vision techniques are used for generating image\nrepresentation in such a way that they can identify the objects about which\nquestion is asked. The Attention model tries to mimic the human behavior of\ngiving attention to a different region of an image according to our\nunderstanding of its context. This paper critically examines and reviews\nmethods of VQA algorithm such as generation of semantics of text,\nidentification of objects and answer classification techniques that use the\nco-attention approach.", "published": "2023-05-04 20:10:37", "link": "http://arxiv.org/abs/2305.09782v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AutoML-GPT: Automatic Machine Learning with GPT", "abstract": "AI tasks encompass a wide range of domains and fields. While numerous AI\nmodels have been designed for specific tasks and applications, they often\nrequire considerable human efforts in finding the right model architecture,\noptimization algorithm, and hyperparameters. Recent advances in large language\nmodels (LLMs) like ChatGPT show remarkable capabilities in various aspects of\nreasoning, comprehension, and interaction. Consequently, we propose developing\ntask-oriented prompts and automatically utilizing LLMs to automate the training\npipeline. To implement this concept, we present the AutoML-GPT, which employs\nGPT as the bridge to diverse AI models and dynamically trains models with\noptimized hyperparameters. AutoML-GPT dynamically takes user requests from the\nmodel and data cards and composes the corresponding prompt paragraph.\nUltimately, with this prompt paragraph, AutoML-GPT will automatically conduct\nthe experiments from data processing to model architecture, hyperparameter\ntuning, and predicted training log. By leveraging {\\ours}'s robust language\ncapabilities and the available AI models, AutoML-GPT can tackle numerous\nintricate AI tasks across various tasks and datasets. This approach achieves\nremarkable results in computer vision, natural language processing, and other\nchallenging areas. Extensive experiments and ablation studies demonstrate that\nour method can be general, effective, and beneficial for many AI tasks.", "published": "2023-05-04 02:09:43", "link": "http://arxiv.org/abs/2305.02499v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Personalize Segment Anything Model with One Shot", "abstract": "Driven by large-data pre-training, Segment Anything Model (SAM) has been\ndemonstrated as a powerful and promptable framework, revolutionizing the\nsegmentation models. Despite the generality, customizing SAM for specific\nvisual concepts without man-powered prompting is under explored, e.g.,\nautomatically segmenting your pet dog in different images. In this paper, we\npropose a training-free Personalization approach for SAM, termed as PerSAM.\nGiven only a single image with a reference mask, PerSAM first localizes the\ntarget concept by a location prior, and segments it within other images or\nvideos via three techniques: target-guided attention, target-semantic\nprompting, and cascaded post-refinement. In this way, we effectively adapt SAM\nfor private use without any training. To further alleviate the mask ambiguity,\nwe present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the\nentire SAM, we introduce two learnable weights for multi-scale masks, only\ntraining 2 parameters within 10 seconds for improved performance. To\ndemonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for\npersonalized evaluation, and test our methods on video object segmentation with\ncompetitive performance. Besides, our approach can also enhance DreamBooth to\npersonalize Stable Diffusion for text-to-image generation, which discards the\nbackground disturbance for better target appearance learning. Code is released\nat https://github.com/ZrrSkywalker/Personalize-SAM", "published": "2023-05-04 17:59:36", "link": "http://arxiv.org/abs/2305.03048v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic\n  Howling Suppression", "abstract": "Deep learning has been recently introduced for efficient acoustic howling\nsuppression (AHS). However, the recurrent nature of howling creates a mismatch\nbetween offline training and streaming inference, limiting the quality of\nenhanced speech. To address this limitation, we propose a hybrid method that\ncombines a Kalman filter with a self-attentive recurrent neural network (SARNN)\nto leverage their respective advantages for robust AHS. During offline\ntraining, a pre-processed signal obtained from the Kalman filter and an ideal\nmicrophone signal generated via teacher-forced training strategy are used to\ntrain the deep neural network (DNN). During streaming inference, the DNN's\nparameters are fixed while its output serves as a reference signal for updating\nthe Kalman filter. Evaluation in both offline and streaming inference scenarios\nusing simulated and real-recorded data shows that the proposed method\nefficiently suppresses howling and consistently outperforms baselines.", "published": "2023-05-04 06:36:35", "link": "http://arxiv.org/abs/2305.02583v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Acoustic Simulation Framework to Support Indoor Positioning and Data\n  Driven Signal Processing Assessments", "abstract": "We present an indoor acoustic simulation framework that supports both\nultrasonic and audible signaling. The framework opens the opportunity for fast\nindoor acoustic data generation and positioning development. The improved\nPyroomacoustics-based physical model includes both an image-source model (ISM)\nand ray tracing method to simulate acoustic signaling in geometric spaces that\nextend typical shoe-box rooms. Moreover, it offers the convenience to\nfacilitate multiple speakers and microphones with different directivity\npatterns. In addition to temperature and air absorption, the room reverberation\nis taken into account characterized by the RT60 value or the combination of\nbuilding materials. Additional noise sources can be added by means of post\nprocessing and/or extra speakers. Indoor positioning methods assessed in\nsimulation are compared with real measurements in a testbed, called 'Techtile'.\nThis analysis confirms that the simulation results are close to the\nmeasurements and form a realistic representation of the reality. The simulation\nframework is constructed in a modular way, and parts can be replaced or\nmodified to support different application domains. The code is made available\nopen source.", "published": "2023-05-04 10:37:34", "link": "http://arxiv.org/abs/2305.02715v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio\n  Codec", "abstract": "Audio codec models are widely used in audio communication as a crucial\ntechnique for compressing audio into discrete representations. Nowadays, audio\ncodec models are increasingly utilized in generation fields as intermediate\nrepresentations. For instance, AudioLM is an audio generation model that uses\nthe discrete representation of SoundStream as a training target, while VALL-E\nemploys the Encodec model as an intermediate feature to aid TTS tasks. Despite\ntheir usefulness, two challenges persist: (1) training these audio codec models\ncan be difficult due to the lack of publicly available training processes and\nthe need for large-scale data and GPUs; (2) achieving good reconstruction\nperformance requires many codebooks, which increases the burden on generation\nmodels. In this study, we propose a group-residual vector quantization (GRVQ)\ntechnique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio\nCodec model, HiFi-Codec, which only requires 4 codebooks. We train all the\nmodels using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and\nmore, with a total duration of over 1000 hours, using 8 GPUs. Our experimental\nresults show that HiFi-Codec outperforms Encodec in terms of reconstruction\nperformance despite requiring only 4 codebooks. To facilitate research in audio\ncodec and generation, we introduce AcademiCodec, the first open-source audio\ncodec toolkit that offers training codes and pre-trained models for Encodec,\nSoundStream, and HiFi-Codec. Code and pre-trained model can be found on:\n\\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}", "published": "2023-05-04 12:11:13", "link": "http://arxiv.org/abs/2305.02765v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
