{"title": "Depth-Gated LSTM", "abstract": "In this short note, we present an extension of long short-term memory (LSTM)\nneural networks to using a depth gate to connect memory cells of adjacent\nlayers. Doing so introduces a linear dependence between lower and upper layer\nrecurrent units. Importantly, the linear dependence is gated through a gating\nfunction, which we call depth gate. This gate is a function of the lower layer\nmemory cell, the input to and the past memory cell of this layer. We conducted\nexperiments and verified that this new architecture of LSTMs was able to\nimprove machine translation and language modeling performances.", "published": "2015-08-16 04:31:37", "link": "http://arxiv.org/abs/1508.03790v4", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite\n  Solution", "abstract": "Most existing word embedding methods can be categorized into Neural Embedding\nModels and Matrix Factorization (MF)-based methods. However some models are\nopaque to probabilistic interpretation, and MF-based methods, typically solved\nusing Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as\ntopics, sentiments or writing styles, into the word embedding model. Since\ngenerative models provide a principled way to incorporate latent factors, we\npropose a generative word embedding model, which is easy to interpret, and can\nserve as a basis of more sophisticated latent factor models. The model\ninference reduces to a low rank weighted positive semidefinite approximation\nproblem. Its optimization is approached by eigendecomposition on a submatrix,\nfollowed by online blockwise regression, which is scalable and avoids the\ninformation loss in SVD. In experiments on 7 common benchmark datasets, our\nvectors are competitive to word2vec, and better than other MF-based methods.", "published": "2015-08-16 14:12:17", "link": "http://arxiv.org/abs/1508.03826v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Online Representation Learning in Recurrent Neural Language Models", "abstract": "We investigate an extension of continuous online learning in recurrent neural\nnetwork language models. The model keeps a separate vector representation of\nthe current unit of text being processed and adaptively adjusts it after each\nprediction. The initial experiments give promising results, indicating that the\nmethod is able to increase language modelling accuracy, while also decreasing\nthe parameters needed to store the model along with the computation required at\neach step.", "published": "2015-08-16 18:27:25", "link": "http://arxiv.org/abs/1508.03854v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Visual Affect Around the World: A Large-scale Multilingual Visual\n  Sentiment Ontology", "abstract": "Every culture and language is unique. Our work expressly focuses on the\nuniqueness of culture and language in relation to human affect, specifically\nsentiment and emotion semantics, and how they manifest in social multimedia. We\ndevelop sets of sentiment- and emotion-polarized visual concepts by adapting\nsemantic structures called adjective-noun pairs, originally introduced by Borth\net al. (2013), but in a multilingual context. We propose a new\nlanguage-dependent method for automatic discovery of these adjective-noun\nconstructs. We show how this pipeline can be applied on a social multimedia\nplatform for the creation of a large-scale multilingual visual sentiment\nconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our\nunified ontology is organized hierarchically by multilingual clusters of\nvisually detectable nouns and subclusters of emotionally biased versions of\nthese nouns. In addition, we present an image-based prediction task to show how\ngeneralizable language-specific models are in a multilingual context. A new,\npublicly available dataset of >15.6K sentiment-biased visual concepts across 12\nlanguages with language-specific detector banks, >7.36M images and their\nmetadata is also released.", "published": "2015-08-16 21:43:59", "link": "http://arxiv.org/abs/1508.03868v3", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.IR", "H.1.2; H.5.1; H.5.4; I.2.10"], "primary_category": "cs.MM"}
