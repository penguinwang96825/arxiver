{"title": "S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet\n  Entity Linking", "abstract": "Non-linear models recently receive a lot of attention as people are starting\nto discover the power of statistical and embedding features. However,\ntree-based models are seldom studied in the context of structured learning\ndespite their recent success on various classification and ranking tasks. In\nthis paper, we propose S-MART, a tree-based structured learning framework based\non multiple additive regression trees. S-MART is especially suitable for\nhandling tasks with dense features, and can be used to learn many different\nstructures under various loss functions.\n  We apply S-MART to the task of tweet entity linking --- a core component of\ntweet information extraction, which aims to identify and link name mentions to\nentities in a knowledge base. A novel inference algorithm is proposed to handle\nthe special structure of the task. The experimental results show that S-MART\nsignificantly outperforms state-of-the-art tweet entity linking systems.", "published": "2016-09-26 17:01:03", "link": "http://arxiv.org/abs/1609.08075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Socially-Infused Information Extraction: Embedding Authors,\n  Mentions, and Entities", "abstract": "Entity linking is the task of identifying mentions of entities in text, and\nlinking them to entries in a knowledge base. This task is especially difficult\nin microblogs, as there is little additional text to provide disambiguating\ncontext; rather, authors rely on an implicit common ground of shared knowledge\nwith their readers. In this paper, we attempt to capture some of this implicit\ncontext by exploiting the social network structure in microblogs. We build on\nthe theory of homophily, which implies that socially linked individuals share\ninterests, and are therefore likely to mention the same sorts of entities. We\nimplement this idea by encoding authors, mentions, and entities in a continuous\nvector space, which is constructed so that socially-connected authors have\nsimilar vector representations. These vectors are incorporated into a neural\nstructured prediction model, which captures structural constraints that are\ninherent in the entity linking task. Together, these design decisions yield F1\nimprovements of 1%-5% on benchmark datasets, as compared to the previous\nstate-of-the-art.", "published": "2016-09-26 17:19:07", "link": "http://arxiv.org/abs/1609.08084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating Causal Embeddings for Question Answering with Minimal\n  Supervision", "abstract": "A common model for question answering (QA) is that a good answer is one that\nis closely related to the question, where relatedness is often determined using\ngeneral-purpose lexical models such as word embeddings. We argue that a better\napproach is to look for answers that are related to the question in a relevant\nway, according to the information need of the question, which may be determined\nthrough task-specific embeddings. With causality as a use case, we implement\nthis insight in three steps. First, we generate causal embeddings\ncost-effectively by bootstrapping cause-effect pairs extracted from free text\nusing a small set of seed patterns. Second, we train dedicated embeddings over\nthis data, by using task-specific contexts, i.e., the context of a cause is its\neffect. Finally, we extend a state-of-the-art reranking approach for QA to\nincorporate these causal embeddings. We evaluate the causal embedding models\nboth directly with a casual implication task, and indirectly, in a downstream\ncausal QA task using data from Yahoo! Answers. We show that explicitly modeling\ncausality improves performance in both tasks. In the QA task our best model\nachieves 37.3% P@1, significantly outperforming a strong baseline by 7.7%\n(relative).", "published": "2016-09-26 17:50:15", "link": "http://arxiv.org/abs/1609.08097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Probability Model for Speech-to-Translation Alignment of\n  Low-Resource Languages", "abstract": "For many low-resource languages, spoken language resources are more likely to\nbe annotated with translations than with transcriptions. Translated speech data\nis potentially valuable for documenting endangered languages or for training\nspeech translation systems. A first step towards making use of such data would\nbe to automatically align spoken words with their translations. We present a\nmodel that combines Dyer et al.'s reparameterization of IBM Model 2\n(fast-align) and k-means clustering using Dynamic Time Warping as a distance\nmetric. The two components are trained jointly using expectation-maximization.\nIn an extremely low-resource scenario, our model performs significantly better\nthan both a neural model and a strong baseline.", "published": "2016-09-26 19:50:59", "link": "http://arxiv.org/abs/1609.08139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved\ntheir best language modeling performance only with very large hidden states and\nlarge vocabularies. Even then they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer\nsentinel mixture architecture for neural sequence models which has the ability\nto either reproduce a word from the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinel-LSTM model achieves state of\nthe art language modeling performance on the Penn Treebank (70.9 perplexity)\nwhile using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit longer contexts and deal with\nmore realistic vocabularies and larger corpora we also introduce the freely\navailable WikiText corpus.", "published": "2016-09-26 04:06:13", "link": "http://arxiv.org/abs/1609.07843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and\n  Signer Adaptation", "abstract": "We study the problem of recognizing video sequences of fingerspelled letters\nin American Sign Language (ASL). Fingerspelling comprises a significant but\nrelatively understudied part of ASL. Recognizing fingerspelling is challenging\nfor a number of reasons: It involves quick, small motions that are often highly\ncoarticulated; it exhibits significant variation between signers; and there has\nbeen a dearth of continuous fingerspelling data collected. In this work we\ncollect and annotate a new data set of continuous fingerspelling videos,\ncompare several types of recognizers, and explore the problem of signer\nvariation. Our best-performing models are segmental (semi-Markov) conditional\nrandom fields using deep neural network-based features. In the signer-dependent\nsetting, our recognizers achieve up to about 92% letter accuracy. The\nmulti-signer setting is much more challenging, but with neural network\nadaptation we achieve up to 83% letter accuracies in this setting.", "published": "2016-09-26 07:34:24", "link": "http://arxiv.org/abs/1609.07876v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning to Translate for Multilingual Question Answering", "abstract": "In multilingual question answering, either the question needs to be\ntranslated into the document language, or vice versa. In addition to direction,\nthere are multiple methods to perform the translation, four of which we explore\nin this paper: word-based, 10-best, context-based, and grammar-based. We build\na feature for each combination of translation direction and method, and train a\nmodel that learns optimal feature weights. On a large forum dataset consisting\nof posts in English, Arabic, and Chinese, our novel learn-to-translate approach\nwas more effective than a strong baseline (p<0.05): translating all text into\nEnglish, then training a classifier based only on English (original or\ntranslated) text.", "published": "2016-09-26 22:12:50", "link": "http://arxiv.org/abs/1609.08210v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.", "published": "2016-09-26 19:59:55", "link": "http://arxiv.org/abs/1609.08144v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Online Segment to Segment Neural Transduction", "abstract": "We introduce an online neural sequence to sequence model that learns to\nalternate between encoding and decoding segments of the input as it is read. By\nindependently tracking the encoding and decoding representations our algorithm\npermits exact polynomial marginalization of the latent segmentation during\ntraining, and during decoding beam search is employed to find the best\nalignment path together with the predicted output sequence. Our model tackles\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\nentire input sequence in their fixed-length hidden states before producing any\noutput. It is different from previous attentive models in that, instead of\ntreating the attention weights as output of a deterministic function, our model\nassigns attention weights to a sequential latent variable which can be\nmarginalized out and permits online generation. Experiments on abstractive\nsentence summarization and morphological inflection show significant\nperformance gains over the baseline encoder-decoders.", "published": "2016-09-26 21:13:49", "link": "http://arxiv.org/abs/1609.08194v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
