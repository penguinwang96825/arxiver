{"title": "Automatic Answerability Evaluation for Question Generation", "abstract": "Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed\nfor natural language generation (NLG) tasks, are based on measuring the n-gram\noverlap between the generated and reference text. These simple metrics may be\ninsufficient for more complex tasks, such as question generation (QG), which\nrequires generating questions that are answerable by the reference answers.\nDeveloping a more sophisticated automatic evaluation metric, thus, remains an\nurgent problem in QG research. This work proposes PMAN (Prompting-based Metric\non ANswerability), a novel automatic evaluation metric to assess whether the\ngenerated questions are answerable by the reference answers for the QG tasks.\nExtensive experiments demonstrate that its evaluation results are reliable and\nalign with human evaluations. We further apply our metric to evaluate the\nperformance of QG models, which shows that our metric complements conventional\nmetrics. Our implementation of a GPT-based QG model achieves state-of-the-art\nperformance in generating answerable questions.", "published": "2023-09-22 00:13:07", "link": "http://arxiv.org/abs/2309.12546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is it Possible to Modify Text to a Target Readability Level? An Initial\n  Investigation Using Zero-Shot Large Language Models", "abstract": "Text simplification is a common task where the text is adapted to make it\neasier to understand. Similarly, text elaboration can make a passage more\nsophisticated, offering a method to control the complexity of reading\ncomprehension tests. However, text simplification and elaboration tasks are\nlimited to only relatively alter the readability of texts. It is useful to\ndirectly modify the readability of any text to an absolute target readability\nlevel to cater to a diverse audience. Ideally, the readability of\nreadability-controlled generated text should be independent of the source text.\nTherefore, we propose a novel readability-controlled text modification task.\nThe task requires the generation of 8 versions at various target readability\nlevels for each input text. We introduce novel readability-controlled text\nmodification metrics. The baselines for this task use ChatGPT and Llama-2, with\nan extension approach introducing a two-step process (generating paraphrases by\npassing through the language model twice). The zero-shot approaches are able to\npush the readability of the paraphrases in the desired direction but the final\nreadability remains correlated with the original text's readability. We also\nfind greater drops in semantic and lexical similarity between the source and\ntarget texts with greater shifts in the readability.", "published": "2023-09-22 00:47:18", "link": "http://arxiv.org/abs/2309.12551v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Model Insights: A Dataset for Automated Model Card Generation", "abstract": "Language models (LMs) are no longer restricted to ML community, and\ninstruction-tuned LMs have led to a rise in autonomous AI agents. As the\naccessibility of LMs grows, it is imperative that an understanding of their\ncapabilities, intended usage, and development cycle also improves. Model cards\nare a popular practice for documenting detailed information about an ML model.\nTo automate model card generation, we introduce a dataset of 500\nquestion-answer pairs for 25 ML models that cover crucial aspects of the model,\nsuch as its training configurations, datasets, biases, architecture details,\nand training resources. We employ annotators to extract the answers from the\noriginal paper. Further, we explore the capabilities of LMs in generating model\ncards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa,\nand Galactica showcase a significant gap in the understanding of research\npapers by these aforementioned LMs as well as generating factual textual\nresponses. We posit that our dataset can be used to train models to automate\nthe generation of model cards from paper text and reduce human effort in the\nmodel card curation process. The complete dataset is available on\nhttps://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37", "published": "2023-09-22 04:46:11", "link": "http://arxiv.org/abs/2309.12616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Diversify Neural Text Generation via Degenerative Model", "abstract": "Neural language models often fail to generate diverse and informative texts,\nlimiting their applicability in real-world problems. While previous approaches\nhave proposed to address these issues by identifying and penalizing undesirable\nbehaviors (e.g., repetition, overuse of frequent words) from language models,\nwe propose an alternative approach based on an observation: models primarily\nlearn attributes within examples that are likely to cause degeneration\nproblems. Based on this observation, we propose a new approach to prevent\ndegeneration problems by training two models. Specifically, we first train a\nmodel that is designed to amplify undesirable patterns. We then enhance the\ndiversity of the second model by focusing on patterns that the first model\nfails to learn. Extensive experiments on two tasks, namely language modeling\nand dialogue generation, demonstrate the effectiveness of our approach.", "published": "2023-09-22 04:57:10", "link": "http://arxiv.org/abs/2309.12619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Emotional Experiences in Dyadic Conversations of Married\n  Couples: Leveraging Semantic Similarity through Sentence Embedding", "abstract": "Recent advancements in Natural Language Processing (NLP) have highlighted the\npotential of sentence embeddings in measuring semantic similarity (hereafter\nsimilarity). Yet, whether this approach can be used to analyze real-world\ndyadic interactions and predict people's emotional experiences in response to\nthese interactions remains largely uncharted. To bridge this gap, the present\nstudy analyzes verbal conversations of 50 married couples who engage in\nnaturalistic 10-minute conflict and 10-minute positive conversations.\nTransformer-based model General Text Embeddings-Large is employed to obtain the\nembeddings of the utterances from each speaker. The overall similarity of the\nconversations is then quantified by the average cosine similarity between the\nembeddings of adjacent utterances. Results show that lower similarity is\nassociated with greater positive emotional experiences in the positive (but not\nconflict) conversation. Follow-up analyses show that (a) findings remain stable\nwhen controlling for marital satisfaction and the number of utterance pairs and\n(b) the similarity measure is valid in capturing critical features of a dyadic\nconversation. The present study underscores the potency of sentence embeddings\nin understanding links between interpersonal dynamics and individual emotional\nexperiences, paving the way for innovative applications of NLP tools in\naffective and relationship science.", "published": "2023-09-22 06:37:45", "link": "http://arxiv.org/abs/2309.12646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text\n  Hybrid Question Answering", "abstract": "Answering numerical questions over hybrid contents from the given tables and\ntext(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)\nhave gained significant attention in the NLP community. With the emergence of\nlarge language models, In-Context Learning and Chain-of-Thought prompting have\nbecome two particularly popular research topics in this field. In this paper,\nwe introduce a new prompting strategy called Hybrid prompt strategy and\nRetrieval of Thought for TextTableQA. Through In-Context Learning, we prompt\nthe model to develop the ability of retrieval thinking when dealing with hybrid\ndata. Our method achieves superior performance compared to the fully-supervised\nSOTA on the MultiHiertt dataset in the few-shot setting.", "published": "2023-09-22 07:26:17", "link": "http://arxiv.org/abs/2309.12669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JCoLA: Japanese Corpus of Linguistic Acceptability", "abstract": "Neural language models have exhibited outstanding performance in a range of\ndownstream tasks. However, there is limited understanding regarding the extent\nto which these models internalize syntactic knowledge, so that various datasets\nhave recently been constructed to facilitate syntactic evaluation of language\nmodels across languages. In this paper, we introduce JCoLA (Japanese Corpus of\nLinguistic Acceptability), which consists of 10,020 sentences annotated with\nbinary acceptability judgments. Specifically, those sentences are manually\nextracted from linguistics textbooks, handbooks and journal articles, and split\ninto in-domain data (86 %; relatively simple acceptability judgments extracted\nfrom textbooks and handbooks) and out-of-domain data (14 %; theoretically\nsignificant acceptability judgments extracted from journal articles), the\nlatter of which is categorized by 12 linguistic phenomena. We then evaluate the\nsyntactic knowledge of 9 different types of Japanese language models on JCoLA.\nThe results demonstrated that several models could surpass human performance\nfor the in-domain data, while no models were able to exceed human performance\nfor the out-of-domain data. Error analyses by linguistic phenomena further\nrevealed that although neural language models are adept at handling local\nsyntactic dependencies like argument structure, their performance wanes when\nconfronted with long-distance syntactic dependencies like verbal agreement and\nNPI licensing.", "published": "2023-09-22 07:35:45", "link": "http://arxiv.org/abs/2309.12676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Furthest Reasoning with Plan Assessment: Stable Reasoning Path with\n  Retrieval-Augmented Large Language Models", "abstract": "Large Language Models (LLMs), acting as a powerful reasoner and generator,\nexhibit extraordinary performance across various natural language tasks, such\nas question answering (QA). Among these tasks, Multi-Hop Question Answering\n(MHQA) stands as a widely discussed category, necessitating seamless\nintegration between LLMs and the retrieval of external knowledge. Existing\nmethods employ LLM to generate reasoning paths and plans, and utilize IR to\niteratively retrieve related knowledge, but these approaches have inherent\nflaws. On one hand, Information Retriever (IR) is hindered by the low quality\nof generated queries by LLM. On the other hand, LLM is easily misguided by the\nirrelevant knowledge by IR. These inaccuracies, accumulated by the iterative\ninteraction between IR and LLM, lead to a disaster in effectiveness at the end.\nTo overcome above barriers, in this paper, we propose a novel pipeline for MHQA\ncalled Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved\nframework (Furthest Reasoning) and an attached module (Plan Assessor). 1)\nFurthest reasoning operates by masking previous reasoning path and generated\nqueries for LLM, encouraging LLM generating chain of thought from scratch in\neach iteration. This approach enables LLM to break the shackle built by\nprevious misleading thoughts and queries (if any). 2) The Plan Assessor is a\ntrained evaluator that selects an appropriate plan from a group of candidate\nplans proposed by LLM. Our methods are evaluated on three highly recognized\npublic multi-hop question answering datasets and outperform state-of-the-art on\nmost metrics (achieving a 10%-12% in answer accuracy).", "published": "2023-09-22 10:15:13", "link": "http://arxiv.org/abs/2309.12767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatPRCS: A Personalized Support System for English Reading\n  Comprehension based on ChatGPT", "abstract": "As a common approach to learning English, reading comprehension primarily\nentails reading articles and answering related questions. However, the\ncomplexity of designing effective exercises results in students encountering\nstandardized questions, making it challenging to align with individualized\nlearners' reading comprehension ability. By leveraging the advanced\ncapabilities offered by large language models, exemplified by ChatGPT, this\npaper presents a novel personalized support system for reading comprehension,\nreferred to as ChatPRCS, based on the Zone of Proximal Development theory.\nChatPRCS employs methods including reading comprehension proficiency\nprediction, question generation, and automatic evaluation, among others, to\nenhance reading comprehension instruction. First, we develop a new algorithm\nthat can predict learners' reading comprehension abilities using their\nhistorical data as the foundation for generating questions at an appropriate\nlevel of difficulty. Second, a series of new ChatGPT prompt patterns is\nproposed to address two key aspects of reading comprehension objectives:\nquestion generation, and automated evaluation. These patterns further improve\nthe quality of generated questions. Finally, by integrating personalized\nability and reading comprehension prompt patterns, ChatPRCS is systematically\nvalidated through experiments. Empirical results demonstrate that it provides\nlearners with high-quality reading comprehension questions that are broadly\naligned with expert-crafted questions at a statistical level.", "published": "2023-09-22 11:46:44", "link": "http://arxiv.org/abs/2309.12808v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyloMetrix: An Open-Source Multilingual Tool for Representing\n  Stylometric Vectors", "abstract": "This work aims to provide an overview on the open-source multilanguage tool\ncalled StyloMetrix. It offers stylometric text representations that cover\nvarious aspects of grammar, syntax and lexicon. StyloMetrix covers four\nlanguages: Polish as the primary language, English, Ukrainian and Russian. The\nnormalized output of each feature can become a fruitful course for machine\nlearning models and a valuable addition to the embeddings layer for any deep\nlearning algorithm. We strive to provide a concise, but exhaustive overview on\nthe application of the StyloMetrix vectors as well as explain the sets of the\ndeveloped linguistic features. The experiments have shown promising results in\nsupervised content classification with simple algorithms as Random Forest\nClassifier, Voting Classifier, Logistic Regression and others. The deep\nlearning assessments have unveiled the usefulness of the StyloMetrix vectors at\nenhancing an embedding layer extracted from Transformer architectures. The\nStyloMetrix has proven itself to be a formidable source for the machine\nlearning and deep learning algorithms to execute different classification\ntasks.", "published": "2023-09-22 11:53:47", "link": "http://arxiv.org/abs/2309.12810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affect Recognition in Conversations Using Large Language Models", "abstract": "Affect recognition, encompassing emotions, moods, and feelings, plays a\npivotal role in human communication. In the realm of conversational artificial\nintelligence, the ability to discern and respond to human affective cues is a\ncritical factor for creating engaging and empathetic interactions. This study\ninvestigates the capacity of large language models (LLMs) to recognise human\naffect in conversations, with a focus on both open-domain chit-chat dialogues\nand task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP\n(Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al.,\n2014), covering a spectrum of dialogues from casual conversations to clinical\ninterviews, we evaluate and compare LLMs' performance in affect recognition.\nOur investigation explores the zero-shot and few-shot capabilities of LLMs\nthrough in-context learning as well as their model capacities through\ntask-specific fine-tuning. Additionally, this study takes into account the\npotential impact of automatic speech recognition errors on LLM predictions.\nWith this work, we aim to shed light on the extent to which LLMs can replicate\nhuman-like affect recognition capabilities in conversations.", "published": "2023-09-22 14:11:23", "link": "http://arxiv.org/abs/2309.12881v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with\n  Diverse Writing Styles", "abstract": "Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as deepfake texts.\nThere are currently over 72K text generation models in the huggingface model\nrepo. As such, users with malicious intent can easily use these open-sourced\nLLMs to generate harmful texts and dis/misinformation at scale. To mitigate\nthis problem, a computational method to determine if a given text is a deepfake\ntext or not is desired--i.e., Turing Test (TT). In particular, in this work, we\ninvestigate the more general version of the problem, known as Authorship\nAttribution (AA), in a multi-class setting--i.e., not only determining if a\ngiven text is a deepfake text or not but also being able to pinpoint which LLM\nis the author. We propose TopFormer to improve existing AA solutions by\ncapturing more linguistic patterns in deepfake texts by including a Topological\nData Analysis (TDA) layer in the Transformer-based model. We show the benefits\nof having a TDA layer when dealing with imbalanced, and multi-style datasets,\nby extracting TDA features from the reshaped $pooled\\_output$ of our backbone\nas input. This Transformer-based model captures contextual representations\n(i.e., semantic and syntactic linguistic features), while TDA captures the\nshape and structure of data (i.e., linguistic structures). Finally, TopFormer,\noutperforms all baselines in all 3 datasets, achieving up to 7\\% increase in\nMacro F1 score. Our code and datasets are available at:\nhttps://github.com/AdaUchendu/topformer", "published": "2023-09-22 15:32:49", "link": "http://arxiv.org/abs/2309.12934v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nested Event Extraction upon Pivot Element Recogniton", "abstract": "Nested Event Extraction (NEE) aims to extract complex event structures where\nan event contains other events as its arguments recursively. Nested events\ninvolve a kind of Pivot Elements (PEs) that simultaneously act as arguments of\nouter-nest events and as triggers of inner-nest events, and thus connect them\ninto nested structures. This special characteristic of PEs brings challenges to\nexisting NEE methods, as they cannot well cope with the dual identities of PEs.\nTherefore, this paper proposes a new model, called PerNee, which extracts\nnested events mainly based on recognizing PEs. Specifically, PerNee first\nrecognizes the triggers of both inner-nest and outer-nest events and further\nrecognizes the PEs via classifying the relation type between trigger pairs. The\nmodel uses prompt learning to incorporate information from both event types and\nargument roles for better trigger and argument representations to improve NEE\nperformance. Since existing NEE datasets (e.g., Genia11) are limited to\nspecific domains and contain a narrow range of event types with nested\nstructures, we systematically categorize nested events in the generic domain\nand construct a new NEE dataset, called ACE2005-Nest. Experimental results\ndemonstrate that PerNee consistently achieves state-of-the-art performance on\nACE2005-Nest, Genia11, and Genia13. The ACE2005-Nest dataset and the code of\nthe PerNee model are available at https://github.com/waysonren/PerNee.", "published": "2023-09-22 15:58:06", "link": "http://arxiv.org/abs/2309.12960v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cardiovascular Disease Risk Prediction via Social Media", "abstract": "Researchers use Twitter and sentiment analysis to predict Cardiovascular\nDisease (CVD) risk. We developed a new dictionary of CVD-related keywords by\nanalyzing emotions expressed in tweets. Tweets from eighteen US states,\nincluding the Appalachian region, were collected. Using the VADER model for\nsentiment analysis, users were classified as potentially at CVD risk. Machine\nLearning (ML) models were employed to classify individuals' CVD risk and\napplied to a CDC dataset with demographic information to make the comparison.\nPerformance evaluation metrics such as Test Accuracy, Precision, Recall, F1\nscore, Mathew's Correlation Coefficient (MCC), and Cohen's Kappa (CK) score\nwere considered. Results demonstrated that analyzing tweets' emotions surpassed\nthe predictive power of demographic data alone, enabling the identification of\nindividuals at potential risk of developing CVD. This research highlights the\npotential of Natural Language Processing (NLP) and ML techniques in using\ntweets to identify individuals with CVD risks, providing an alternative\napproach to traditional demographic information for public health monitoring.", "published": "2023-09-22 19:03:34", "link": "http://arxiv.org/abs/2309.13147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP", "abstract": "Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.", "published": "2023-09-22 20:29:34", "link": "http://arxiv.org/abs/2309.13173v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Distillation of Table-based Reasoning Ability from LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their enormous\nparameter size and extremely high requirements for compute power pose\nchallenges for their practical deployment. Recent research has revealed that\nspecific capabilities of LLMs, such as numerical reasoning, can be transferred\nto smaller models through distillation. Some studies explore the potential of\nleveraging LLMs to perform table-based reasoning. However, there has been no\nprior work focusing on table reasoning skills in smaller models specifically\ntailored for scientific table-to-text generation tasks. In this paper, we\npropose a novel table-based reasoning distillation approach, with the aim of\ndistilling LLMs into tailored smaller models. Our experimental results have\nshown that a 220 million parameter model (Flan-T5-base) fine-tuned using\ndistilled data, not only achieves a significant improvement compared to\ntraditionally fine-tuned baselines, but also surpasses specific LLMs on a\nscientific table-to-text generation dataset. Our code is available at\nhttps://github.com/Bernard-Yang/DistillTableCoT.", "published": "2023-09-22 21:15:28", "link": "http://arxiv.org/abs/2309.13182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for\n  Hospitalized Patients", "abstract": "In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is\npivotal, but its assignment process is inefficient. The study introduces\nDRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes\nto enhance DRGs assignment. Utilizing LLaMA as the foundational model and\noptimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge\nsummaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1\nscore of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area\nUnder the Curve (AUC) of 0.986, with a maximum input token length of 512. This\nmodel surpassed the performance of prior leading models in DRG prediction,\nshowing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score\ncompared to ClinicalBERT and CAML, respectively. Applied to base DRG and\ncomplication or comorbidity (CC)/major complication or comorbidity (MCC)\nprediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%,\nrespectively. Additionally, our findings indicate that DRG-LLaMA's performance\ncorrelates with increased model parameters and input context lengths.", "published": "2023-09-22 05:18:54", "link": "http://arxiv.org/abs/2309.12625v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Construction contract risk identification based on knowledge-augmented\n  language model", "abstract": "Contract review is an essential step in construction projects to prevent\npotential losses. However, the current methods for reviewing construction\ncontracts lack effectiveness and reliability, leading to time-consuming and\nerror-prone processes. While large language models (LLMs) have shown promise in\nrevolutionizing natural language processing (NLP) tasks, they struggle with\ndomain-specific knowledge and addressing specialized issues. This paper\npresents a novel approach that leverages LLMs with construction contract\nknowledge to emulate the process of contract review by human experts. Our\ntuning-free approach incorporates construction contract domain knowledge to\nenhance language models for identifying construction contract risks. The use of\na natural language when building the domain knowledge base facilitates\npractical implementation. We evaluated our method on real construction\ncontracts and achieved solid performance. Additionally, we investigated how\nlarge language models employ logical thinking during the task and provide\ninsights and recommendations for future research.", "published": "2023-09-22 05:27:06", "link": "http://arxiv.org/abs/2309.12626v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AMPLIFY:Attention-based Mixup for Performance Improvement and Label\n  Smoothing in Transformer", "abstract": "Mixup is an effective data augmentation method that generates new augmented\nsamples by aggregating linear combinations of different original samples.\nHowever, if there are noises or aberrant features in the original samples,\nMixup may propagate them to the augmented samples, leading to over-sensitivity\nof the model to these outliers . To solve this problem, this paper proposes a\nnew Mixup method called AMPLIFY. This method uses the Attention mechanism of\nTransformer itself to reduce the influence of noises and aberrant values in the\noriginal samples on the prediction results, without increasing additional\ntrainable parameters, and the computational cost is very low, thereby avoiding\nthe problem of high resource consumption in common Mixup methods such as\nSentence Mixup . The experimental results show that, under a smaller\ncomputational resource cost, AMPLIFY outperforms other Mixup methods in text\nclassification tasks on 7 benchmark datasets, providing new ideas and new ways\nto further improve the performance of pre-trained models based on the Attention\nmechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at\nhttps://github.com/kiwi-lilo/AMPLIFY.", "published": "2023-09-22 08:02:45", "link": "http://arxiv.org/abs/2309.12689v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantic similarity prediction is better than other semantic similarity\n  measures", "abstract": "Semantic similarity between natural language texts is typically measured\neither by looking at the overlap between subsequences (e.g., BLEU) or by using\nembeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we\nare only interested in measuring the semantic similarity, it is better to\ndirectly predict the similarity using a fine-tuned model for such a task. Using\na fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B)\nfrom the GLUE benchmark, we define the STSScore approach and show that the\nresulting similarity is better aligned with our expectations on a robust\nsemantic similarity measure than other approaches.", "published": "2023-09-22 08:11:01", "link": "http://arxiv.org/abs/2309.12697v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-context Interference in Chat-based Large Language Models", "abstract": "Large language models (LLMs) have had a huge impact on society due to their\nimpressive capabilities and vast knowledge of the world. Various applications\nand tools have been created that allow users to interact with these models in a\nblack-box scenario. However, one limitation of this scenario is that users\ncannot modify the internal knowledge of the model, and the only way to add or\nmodify internal knowledge is by explicitly mentioning it to the model during\nthe current interaction. This learning process is called in-context training,\nand it refers to training that is confined to the user's current session or\ncontext. In-context learning has significant applications, but also has\nlimitations that are seldom studied. In this paper, we present a study that\nshows how the model can suffer from interference between information that\ncontinually flows in the context, causing it to forget previously learned\nknowledge, which can reduce the model's performance. Along with showing the\nproblem, we propose an evaluation benchmark based on the bAbI dataset.", "published": "2023-09-22 09:18:55", "link": "http://arxiv.org/abs/2309.12727v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Domain Adaptation for Arabic Machine Translation: The Case of Financial\n  Texts", "abstract": "Neural machine translation (NMT) has shown impressive performance when\ntrained on large-scale corpora. However, generic NMT systems have demonstrated\npoor performance on out-of-domain translation. To mitigate this issue, several\ndomain adaptation methods have recently been proposed which often lead to\nbetter translation quality than genetic NMT systems. While there has been some\ncontinuous progress in NMT for English and other European languages, domain\nadaption in Arabic has received little attention in the literature. The current\nstudy, therefore, aims to explore the effectiveness of domain-specific\nadaptation for Arabic MT (AMT), in yet unexplored domain, financial news\narticles. To this end, we developed carefully a parallel corpus for\nArabic-English (AR- EN) translation in the financial domain for benchmarking\ndifferent domain adaptation methods. We then fine-tuned several pre-trained NMT\nand Large Language models including ChatGPT-3.5 Turbo on our dataset. The\nresults showed that the fine-tuning is successful using just a few well-aligned\nin-domain AR-EN segments. The quality of ChatGPT translation was superior than\nother models based on automatic and human evaluations. To the best of our\nknowledge, this is the first work on fine-tuning ChatGPT towards financial\ndomain transfer learning. To contribute to research in domain translation, we\nmade our datasets and fine-tuned models available at\nhttps://huggingface.co/asas-ai/.", "published": "2023-09-22 13:37:19", "link": "http://arxiv.org/abs/2309.12863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation\n  Extraction", "abstract": "Event Relation Extraction (ERE) aims to extract multiple kinds of relations\namong events in texts. However, existing methods singly categorize event\nrelations as different classes, which are inadequately capturing the intrinsic\nsemantics of these relations. To comprehensively understand their intrinsic\nsemantics, in this paper, we obtain prototype representations for each type of\nevent relation and propose a Prototype-Enhanced Matching (ProtoEM) framework\nfor the joint extraction of multiple kinds of event relations. Specifically,\nProtoEM extracts event relations in a two-step manner, i.e., prototype\nrepresenting and prototype matching. In the first step, to capture the\nconnotations of different event relations, ProtoEM utilizes examples to\nrepresent the prototypes corresponding to these relations. Subsequently, to\ncapture the interdependence among event relations, it constructs a dependency\ngraph for the prototypes corresponding to these relations and utilized a Graph\nNeural Network (GNN)-based module for modeling. In the second step, it obtains\nthe representations of new event pairs and calculates their similarity with\nthose prototypes obtained in the first step to evaluate which types of event\nrelations they belong to. Experimental results on the MAVEN-ERE dataset\ndemonstrate that the proposed ProtoEM framework can effectively represent the\nprototypes of event relations and further obtain a significant improvement over\nbaseline models.", "published": "2023-09-22 14:26:06", "link": "http://arxiv.org/abs/2309.12892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Explanation Prompting Improves Dialogue Understanding in Large\n  Language Models", "abstract": "Task-oriented dialogue (TOD) systems facilitate users in executing various\nactivities via multi-turn dialogues, but Large Language Models (LLMs) often\nstruggle to comprehend these intricate contexts. In this study, we propose a\nnovel \"Self-Explanation\" prompting strategy to enhance the comprehension\nabilities of LLMs in multi-turn dialogues. This task-agnostic approach requires\nthe model to analyze each dialogue utterance before task execution, thereby\nimproving performance across various dialogue-centric tasks. Experimental\nresults from six benchmark datasets confirm that our method consistently\noutperforms other zero-shot prompts and matches or exceeds the efficacy of\nfew-shot prompts, demonstrating its potential as a powerful tool in enhancing\nLLMs' comprehension in complex dialogue tasks.", "published": "2023-09-22 15:41:34", "link": "http://arxiv.org/abs/2309.12940v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Audience-specific Explanations for Machine Translation", "abstract": "In machine translation, a common problem is that the translation of certain\nwords even if translated can cause incomprehension of the target language\naudience due to different cultural backgrounds. A solution to solve this\nproblem is to add explanations for these words. In a first step, we therefore\nneed to identify these words or phrases. In this work we explore techniques to\nextract example explanations from a parallel corpus. However, the sparsity of\nsentences containing words that need to be explained makes building the\ntraining dataset extremely difficult. In this work, we propose a semi-automatic\ntechnique to extract these explanations from a large parallel corpus.\nExperiments on English->German language pair show that our method is able to\nextract sentence so that more than 10% of the sentences contain explanation,\nwhile only 1.9% of the original sentences contain explanations. In addition,\nexperiments on English->French and English->Chinese language pairs also show\nsimilar conclusions. This is therefore an essential first automatic step to\ncreate a explanation dataset. Furthermore we show that the technique is robust\nfor all three language pairs.", "published": "2023-09-22 17:00:45", "link": "http://arxiv.org/abs/2309.12998v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Also Good Prototypical Commonsense Reasoners", "abstract": "Commonsense reasoning is a pivotal skill for large language models, yet it\npresents persistent challenges in specific tasks requiring this competence.\nTraditional fine-tuning approaches can be resource-intensive and potentially\ncompromise a model's generalization capacity. Furthermore, state-of-the-art\nlanguage models like GPT-3.5 and Claude are primarily accessible through API\ncalls, which makes fine-tuning models challenging. To address these challenges,\nwe draw inspiration from the outputs of large models for tailored tasks and\nsemi-automatically developed a set of novel prompts from several perspectives,\nincluding task-relevance, supportive evidence generation (e.g. chain-of-thought\nand knowledge), diverse path decoding to aid the model. Experimental results on\nProtoQA dataset demonstrate that with better designed prompts we can achieve\nthe new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max\nAnswer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for the\nfirst time) compared to the previous SOTA model and achieved an improvement on\nStrategyQA and CommonsenseQA2.0 (3% and 1%, respectively). Furthermore, with\nthe generated Chain-of-Thought and knowledge, we can improve the\ninterpretability of the model while also surpassing the previous SOTA models.\nWe hope that our work can provide insight for the NLP community to develop\nbetter prompts and explore the potential of large language models for more\ncomplex reasoning tasks.", "published": "2023-09-22 20:07:24", "link": "http://arxiv.org/abs/2309.13165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Document Understanding for Healthcare Referrals", "abstract": "Reliance on scanned documents and fax communication for healthcare referrals\nleads to high administrative costs and errors that may affect patient care. In\nthis work we propose a hybrid model leveraging LayoutLMv3 along with\ndomain-specific rules to identify key patient, physician, and exam-related\nentities in faxed referral documents. We explore some of the challenges in\napplying a document understanding model to referrals, which have formats\nvarying by medical practice, and evaluate model performance using MUC-5 metrics\nto obtain appropriate metrics for the practical use case. Our analysis shows\nthe addition of domain-specific rules to the transformer model yields greatly\nincreased precision and F1 scores, suggesting a hybrid model trained on a\ncurated dataset can increase efficiency in referral management.", "published": "2023-09-22 21:19:47", "link": "http://arxiv.org/abs/2309.13184v1", "categories": ["cs.CL", "cs.IR", "I.2.7; I.7.5"], "primary_category": "cs.CL"}
{"title": "Investigating Large Language Models and Control Mechanisms to Improve\n  Text Readability of Biomedical Abstracts", "abstract": "Biomedical literature often uses complex language and inaccessible\nprofessional terminologies. That is why simplification plays an important role\nin improving public health literacy. Applying Natural Language Processing (NLP)\nmodels to automate such tasks allows for quick and direct accessibility for lay\nreaders. In this work, we investigate the ability of state-of-the-art large\nlanguage models (LLMs) on the task of biomedical abstract simplification, using\nthe publicly available dataset for plain language adaptation of biomedical\nabstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and\nprompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and\nBART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT,\nand 3) Control-token mechanisms on BART-based models. We used a range of\nautomatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and\nalso conducted human evaluations. BART-Large with Control Token (BART-L-w-CT)\nmechanisms reported the highest SARI score of 46.54 and T5-base reported the\nhighest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better\nsimplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better\nmeaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised\nthe system outputs with examples, hoping this will shed some light for future\nresearch on this task. Our code, fine-tuned models, and data splits are\navailable at \\url{https://github.com/HECTA-UoM/PLABA-MU} \\begin{IEEEkeywords}\nLarge Language Models, Text Simplification, Biomedical NLP, Control Mechanisms,\nHealth Informatics \\end{IEEEkeywords}", "published": "2023-09-22 22:47:32", "link": "http://arxiv.org/abs/2309.13202v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PlanFitting: Tailoring Personalized Exercise Plans with Large Language\n  Models", "abstract": "A personally tailored exercise regimen is crucial to ensuring sufficient\nphysical activities, yet challenging to create as people have complex schedules\nand considerations and the creation of plans often requires iterations with\nexperts. We present PlanFitting, a conversational AI that assists in\npersonalized exercise planning. Leveraging generative capabilities of large\nlanguage models, PlanFitting enables users to describe various constraints and\nqueries in natural language, thereby facilitating the creation and refinement\nof their weekly exercise plan to suit their specific circumstances while\nstaying grounded in foundational principles. Through a user study where\nparticipants (N=18) generated a personalized exercise plan using PlanFitting\nand expert planners (N=3) evaluated these plans, we identified the potential of\nPlanFitting in generating personalized, actionable, and evidence-based exercise\nplans. We discuss future design opportunities for AI assistants in creating\nplans that better comply with exercise principles and accommodate personal\nconstraints.", "published": "2023-09-22 00:55:52", "link": "http://arxiv.org/abs/2309.12555v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Creativity Support in the Age of Large Language Models: An Empirical\n  Study Involving Emerging Writers", "abstract": "The development of large language models (LLMs) capable of following\ninstructions and engaging in conversational interactions sparked increased\ninterest in their utilization across various support tools. We investigate the\nutility of modern LLMs in assisting professional writers via an empirical user\nstudy (n=30). The design of our collaborative writing interface is grounded in\nthe cognitive process model of writing that views writing as a goal-oriented\nthinking process encompassing non-linear cognitive activities: planning,\ntranslating, and reviewing. Participants are asked to submit a post-completion\nsurvey to provide feedback on the potential and pitfalls of LLMs as writing\ncollaborators. Upon analyzing the writer-LLM interactions, we find that while\nwriters seek LLM's help across all three types of cognitive activities, they\nfind LLMs more helpful in translation and reviewing. Our findings from\nanalyzing both the interactions and the survey responses highlight future\nresearch directions in creative writing assistance using LLMs.", "published": "2023-09-22 01:49:36", "link": "http://arxiv.org/abs/2309.12570v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language\n  augmentation for Low Resource Self-Supervised Speech Models", "abstract": "Self-supervised representation learning (SSRL) has demonstrated superior\nperformance than supervised models for tasks including phoneme recognition.\nTraining SSRL models poses a challenge for low-resource languages where\nsufficient pre-training data may not be available. A common approach is\ncross-lingual pre-training. Instead, we propose to use audio augmentation\ntechniques, namely: pitch variation, noise addition, accented target language\nand other language speech to pre-train SSRL models in a low resource condition\nand evaluate phoneme recognition. Our comparisons found that a combined\nsynthetic augmentations (noise/pitch) strategy outperformed accent and language\nknowledge transfer. Furthermore, we examined the scaling factor of augmented\ndata to achieve equivalent performance to model pre-trained with target domain\nspeech. Our findings suggest that for resource-constrained languages, combined\naugmentations can be a viable option than other augmentations.", "published": "2023-09-22 10:09:09", "link": "http://arxiv.org/abs/2309.12763v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language\n  Segmentation in Echocardiography", "abstract": "Accurate segmentation is essential for echocardiography-based assessment of\ncardiovascular diseases (CVDs). However, the variability among sonographers and\nthe inherent challenges of ultrasound images hinder precise segmentation. By\nleveraging the joint representation of image and text modalities,\nVision-Language Segmentation Models (VLSMs) can incorporate rich contextual\ninformation, potentially aiding in accurate and explainable segmentation.\nHowever, the lack of readily available data in echocardiography hampers the\ntraining of VLSMs. In this study, we explore using synthetic datasets from\nSemantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography\nsegmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)\nusing seven different kinds of language prompts derived from several\nattributes, automatically extracted from echocardiography images, segmentation\nmasks, and their metadata. Our results show improved metrics and faster\nconvergence when pretraining VLSMs on SDM-generated synthetic images before\nfinetuning on real images. The code, configs, and prompts are available at\nhttps://github.com/naamiinepal/synthetic-boost.", "published": "2023-09-22 12:36:30", "link": "http://arxiv.org/abs/2309.12829v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AnglE-optimized Text Embeddings", "abstract": "High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.", "published": "2023-09-22 13:52:42", "link": "http://arxiv.org/abs/2309.12871v9", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Separate Normalization in Self-supervised Transformers", "abstract": "Self-supervised training methods for transformers have demonstrated\nremarkable performance across various domains. Previous transformer-based\nmodels, such as masked autoencoders (MAE), typically utilize a single\nnormalization layer for both the [CLS] symbol and the tokens. We propose in\nthis paper a simple modification that employs separate normalization layers for\nthe tokens and the [CLS] symbol to better capture their distinct\ncharacteristics and enhance downstream task performance. Our method aims to\nalleviate the potential negative effects of using the same normalization\nstatistics for both token types, which may not be optimally aligned with their\nindividual roles. We empirically show that by utilizing a separate\nnormalization layer, the [CLS] embeddings can better encode the global\ncontextual information and are distributed more uniformly in its anisotropic\nspace. When replacing the conventional normalization layer with the two\nseparate layers, we observe an average 2.7% performance improvement over the\nimage, natural language, and graph domains.", "published": "2023-09-22 15:30:53", "link": "http://arxiv.org/abs/2309.12931v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among\n  Diverse LLMs", "abstract": "Large Language Models (LLMs) still struggle with natural language reasoning\ntasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile,\na multi-model multi-agent framework designed as a round table conference among\ndiverse LLM agents. ReConcile enhances collaborative reasoning between LLM\nagents via multiple rounds of discussion, learning to convince other agents to\nimprove their answers, and employing a confidence-weighted voting mechanism\nthat leads to a better consensus. In each round, ReConcile initiates discussion\nbetween agents via a 'discussion prompt' that consists of (a) grouped answers\nand explanations generated by each agent in the previous round, (b) their\nconfidence scores, and (c) demonstrations of answer-rectifying human\nexplanations, used for convincing other agents. Experiments on seven benchmarks\ndemonstrate that ReConcile significantly improves LLMs' reasoning -- both\nindividually and as a team -- surpassing prior single-agent and multi-agent\nbaselines by up to 11.4% and even outperforming GPT-4 on three datasets.\nReConcile also flexibly incorporates different combinations of agents,\nincluding API-based, open-source, and domain-specific models, leading to an 8%\nimprovement on MATH. Finally, we analyze the individual components of\nReConcile, demonstrating that the diversity originating from different models\nis critical to its superior performance. Code:\nhttps://github.com/dinobby/ReConcile", "published": "2023-09-22 17:12:45", "link": "http://arxiv.org/abs/2309.13007v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient\n  Pruning of A Multilingual ASR Model", "abstract": "Neural network pruning offers an effective method for compressing a\nmultilingual automatic speech recognition (ASR) model with minimal performance\nloss. However, it entails several rounds of pruning and re-training needed to\nbe run for each language. In this work, we propose the use of an adaptive\nmasking approach in two scenarios for pruning a multilingual ASR model\nefficiently, each resulting in sparse monolingual models or a sparse\nmultilingual model (named as Dynamic ASR Pathways). Our approach dynamically\nadapts the sub-network, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning methods when\ntargeting sparse monolingual models. Further, we illustrate that Dynamic ASR\nPathways jointly discovers and trains better sub-networks (pathways) of a\nsingle multilingual model by adapting from different sub-network\ninitializations, thereby reducing the need for language-specific pruning.", "published": "2023-09-22 17:30:28", "link": "http://arxiv.org/abs/2309.13018v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "abstract": "The remarkable advancements in large language models (LLMs) have brought\nabout significant improvements in Natural Language Processing(NLP) tasks. This\npaper presents a comprehensive review of in-context learning techniques,\nfocusing on different types of prompts, including discrete, continuous,\nfew-shot, and zero-shot, and their impact on LLM performance. We explore\nvarious approaches to prompt design, such as manual design, optimization\nalgorithms, and evaluation methods, to optimize LLM performance across diverse\ntasks. Our review covers key research studies in prompt engineering, discussing\ntheir methodologies and contributions to the field. We also delve into the\nchallenges faced in evaluating prompt performance, given the absence of a\nsingle \"best\" prompt and the importance of considering multiple metrics. In\nconclusion, the paper highlights the critical role of prompt design in\nharnessing the full potential of LLMs and provides insights into the\ncombination of manual design, optimization techniques, and rigorous evaluation\nfor more effective and efficient use of LLMs in various NLP tasks.", "published": "2023-09-22 23:00:34", "link": "http://arxiv.org/abs/2309.13205v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PopBERT. Detecting populism and its host ideologies in the German\n  Bundestag", "abstract": "The rise of populism concerns many political scientists and practitioners,\nyet the detection of its underlying language remains fragmentary. This paper\naims to provide a reliable, valid, and scalable approach to measure populist\nstances. For that purpose, we created an annotated dataset based on\nparliamentary speeches of the German Bundestag (2013 to 2021). Following the\nideational definition of populism, we label moralizing references to the\nvirtuous people or the corrupt elite as core dimensions of populist language.\nTo identify, in addition, how the thin ideology of populism is thickened, we\nannotate how populist statements are attached to left-wing or right-wing host\nideologies. We then train a transformer-based model (PopBERT) as a multilabel\nclassifier to detect and quantify each dimension. A battery of validation\nchecks reveals that the model has a strong predictive accuracy, provides high\nqualitative face validity, matches party rankings of expert surveys, and\ndetects out-of-sample text snippets correctly. PopBERT enables dynamic analyses\nof how German-speaking politicians and parties use populist language as a\nstrategic device. Furthermore, the annotator-level data may also be applied in\ncross-domain applications or to develop related classifiers.", "published": "2023-09-22 14:48:02", "link": "http://arxiv.org/abs/2309.14355v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ICASSP 2023 Acoustic Echo Cancellation Challenge", "abstract": "The ICASSP 2023 Acoustic Echo Cancellation Challenge is intended to stimulate\nresearch in acoustic echo cancellation (AEC), which is an important area of\nspeech enhancement and is still a top issue in audio communication. This is the\nfourth AEC challenge and it is enhanced by adding a second track for\npersonalized acoustic echo cancellation, reducing the algorithmic + buffering\nlatency to 20ms, as well as including a full-band version of AECMOS. We open\nsource two large datasets to train AEC models under both single talk and double\ntalk scenarios. These datasets consist of recordings from more than 10,000 real\naudio devices and human speakers in real environments, as well as a synthetic\ndataset. We open source an online subjective test framework and provide an\nobjective metric for researchers to quickly test their results. The winners of\nthis challenge were selected based on the average mean opinion score (MOS)\nachieved across all scenarios and the word accuracy (WAcc) rate.", "published": "2023-09-22 00:51:19", "link": "http://arxiv.org/abs/2309.12553v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SPGM: Prioritizing Local Features for enhanced speech separation\n  performance", "abstract": "Dual-path is a popular architecture for speech separation models (e.g.\nSepformer) which splits long sequences into overlapping chunks for its intra-\nand inter-blocks that separately model intra-chunk local features and\ninter-chunk global relationships. However, it has been found that inter-blocks,\nwhich comprise half a dual-path model's parameters, contribute minimally to\nperformance. Thus, we propose the Single-Path Global Modulation (SPGM) block to\nreplace inter-blocks. SPGM is named after its structure consisting of a\nparameter-free global pooling module followed by a modulation module comprising\nonly 2% of the model's total parameters. The SPGM block allows all transformer\nlayers in the model to be dedicated to local feature modelling, making the\noverall model single-path. SPGM achieves 22.1 dB SI-SDRi on WSJ0-2Mix and 20.4\ndB SI-SDRi on Libri2Mix, exceeding the performance of Sepformer by 0.5 dB and\n0.3 dB respectively and matches the performance of recent SOTA models with up\nto 8 times fewer parameters. Model and weights are available at\nhuggingface.co/yipjiaqi/spgm", "published": "2023-09-22 03:48:50", "link": "http://arxiv.org/abs/2309.12608v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NTT speaker diarization system for CHiME-7: multi-domain,\n  multi-microphone End-to-end and vector clustering diarization", "abstract": "This paper details our speaker diarization system designed for multi-domain,\nmulti-microphone casual conversations. The proposed diarization pipeline uses\nweighted prediction error (WPE)-based dereverberation as a front end, then\napplies end-to-end neural diarization with vector clustering (EEND-VC) to each\nchannel separately. It integrates the diarization result obtained from each\nchannel using diarization output voting error reduction plus overlap\n(DOVER-LAP). To harness the knowledge from the target domain and results\nintegrated across all channels, we apply self-supervised adaptation for each\nsession by retraining the EEND-VC with pseudo-labels derived from DOVER-LAP.\nThe proposed system was incorporated into NTT's submission for the distant\nautomatic speech recognition task in the CHiME-7 challenge. Our system achieved\n65 % and 62 % relative improvements on development and eval sets compared to\nthe organizer-provided VC-based baseline diarization system, securing third\nplace in diarization performance.", "published": "2023-09-22 06:53:34", "link": "http://arxiv.org/abs/2309.12656v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CrossSinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice\n  Synthesizer Trained on Monolingual Singers", "abstract": "It is challenging to build a multi-singer high-fidelity singing voice\nsynthesis system with cross-lingual ability by only using monolingual singers\nin the training stage. In this paper, we propose CrossSinger, which is a\ncross-lingual singing voice synthesizer based on Xiaoicesing2. Specifically, we\nutilize International Phonetic Alphabet to unify the representation for all\nlanguages of the training data. Moreover, we leverage conditional layer\nnormalization to incorporate the language information into the model for better\npronunciation when singers meet unseen languages. Additionally, gradient\nreversal layer (GRL) is utilized to remove singer biases included in lyrics\nsince all singers are monolingual, which indicates singer's identity is\nimplicitly associated with the text. The experiment is conducted on a\ncombination of three singing voice datasets containing Japanese Kiritan\ndataset, English NUS-48E dataset, and one internal Chinese dataset. The result\nshows CrossSinger can synthesize high-fidelity songs for various singers with\ncross-lingual ability, including code-switch cases.", "published": "2023-09-22 07:29:10", "link": "http://arxiv.org/abs/2309.12672v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study on Incorporating Whisper for Robust Speech Assessment", "abstract": "This research introduces an enhanced version of the multi-objective speech\nassessment model--MOSA-Net+, by leveraging the acoustic features from Whisper,\na large-scaled weakly supervised model. We first investigate the effectiveness\nof Whisper in deploying a more robust speech assessment model. After that, we\nexplore combining representations from Whisper and SSL models. The experimental\nresults reveal that Whisper's embedding features can contribute to more\naccurate prediction performance. Moreover, combining the embedding features\nfrom Whisper and SSL models only leads to marginal improvement. As compared to\nintrusive methods, MOSA-Net, and other SSL-based speech assessment models,\nMOSA-Net+ yields notable improvements in estimating subjective quality and\nintelligibility scores across all evaluation metrics in Taiwan Mandarin Hearing\nIn Noise test - Quality & Intelligibility (TMHINT-QI) dataset. To further\nvalidate its robustness, MOSA-Net+ was tested in the noisy-and-enhanced track\nof the VoiceMOS Challenge 2023, where it obtained the top-ranked performance\namong nine systems.", "published": "2023-09-22 10:11:05", "link": "http://arxiv.org/abs/2309.12766v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DurIAN-E: Duration Informed Attention Network For Expressive\n  Text-to-Speech Synthesis", "abstract": "This paper introduces an improved duration informed attention neural network\n(DurIAN-E) for expressive and high-fidelity text-to-speech (TTS) synthesis.\nInherited from the original DurIAN model, an auto-regressive model structure in\nwhich the alignments between the input linguistic information and the output\nacoustic features are inferred from a duration model is adopted. Meanwhile the\nproposed DurIAN-E utilizes multiple stacked SwishRNN-based Transformer blocks\nas linguistic encoders. Style-Adaptive Instance Normalization (SAIN) layers are\nexploited into frame-level encoders to improve the modeling ability of\nexpressiveness. A denoiser incorporating both denoising diffusion probabilistic\nmodel (DDPM) for mel-spectrograms and SAIN modules is conducted to further\nimprove the synthetic speech quality and expressiveness. Experimental results\nprove that the proposed expressive TTS model in this paper can achieve better\nperformance than the state-of-the-art approaches in both subjective mean\nopinion score (MOS) and preference tests.", "published": "2023-09-22 11:06:04", "link": "http://arxiv.org/abs/2309.12792v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VIC-KD: Variance-Invariance-Covariance Knowledge Distillation to Make\n  Keyword Spotting More Robust Against Adversarial Attacks", "abstract": "Keyword spotting (KWS) refers to the task of identifying a set of predefined\nwords in audio streams. With the advances seen recently with deep neural\nnetworks, it has become a popular technology to activate and control small\ndevices, such as voice assistants. Relying on such models for edge devices,\nhowever, can be challenging due to hardware constraints. Moreover, as\nadversarial attacks have increased against voice-based technologies, developing\nsolutions robust to such attacks has become crucial. In this work, we propose\nVIC-KD, a robust distillation recipe for model compression and adversarial\nrobustness. Using self-supervised speech representations, we show that imposing\ngeometric priors to the latent representations of both Teacher and Student\nmodels leads to more robust target models. Experiments on the Google Speech\nCommands datasets show that the proposed methodology improves upon current\nstate-of-the-art robust distillation methods, such as ARD and RSLAD, by 12% and\n8% in robust accuracy, respectively.", "published": "2023-09-22 15:03:41", "link": "http://arxiv.org/abs/2309.12914v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Massive End-to-end Models for Short Search Queries", "abstract": "In this work, we investigate two popular end-to-end automatic speech\nrecognition (ASR) models, namely Connectionist Temporal Classification (CTC)\nand RNN-Transducer (RNN-T), for offline recognition of voice search queries,\nwith up to 2B model parameters. The encoders of our models use the neural\narchitecture of Google's universal speech model (USM), with additional funnel\npooling layers to significantly reduce the frame rate and speed up training and\ninference. We perform extensive studies on vocabulary size, time reduction\nstrategy, and its generalization performance on long-form test sets. Despite\nthe speculation that, as the model size increases, CTC can be as good as RNN-T\nwhich builds label dependency into the prediction, we observe that a 900M RNN-T\nclearly outperforms a 1.8B CTC and is more tolerant to severe time reduction,\nalthough the WER gap can be largely removed by LM shallow fusion.", "published": "2023-09-22 16:00:50", "link": "http://arxiv.org/abs/2309.12963v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sampling-Frequency-Independent Universal Sound Separation", "abstract": "This paper proposes a universal sound separation (USS) method capable of\nhandling untrained sampling frequencies (SFs). The USS aims at separating\narbitrary sources of different types and can be the key technique to realize a\nsource separator that can be universally used as a preprocessor for any\ndownstream tasks. To realize a universal source separator, there are two\nessential properties: universalities with respect to source types and recording\nconditions. The former property has been studied in the USS literature, which\nhas greatly increased the number of source types that can be handled by a\nsingle neural network. However, the latter property (e.g., SF) has received\nless attention despite its necessity. Since the SF varies widely depending on\nthe downstream tasks, the universal source separator must handle a wide variety\nof SFs. In this paper, to encompass the two properties, we propose an\nSF-independent (SFI) extension of a computationally efficient USS network,\nSuDoRM-RF. The proposed network uses our previously proposed SFI convolutional\nlayers, which can handle various SFs by generating convolutional kernels in\naccordance with an input SF. Experiments show that signal resampling can\ndegrade the USS performance and the proposed method works more consistently\nthan signal-resampling-based methods for various SFs.", "published": "2023-09-22 02:16:37", "link": "http://arxiv.org/abs/2309.12581v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Big model only for hard audios: Sample dependent Whisper model selection\n  for efficient inferences", "abstract": "Recent progress in Automatic Speech Recognition (ASR) has been coupled with a\nsubstantial increase in the model sizes, which may now contain billions of\nparameters, leading to slow inferences even with adapted hardware. In this\ncontext, several ASR models exist in various sizes, with different inference\ncosts leading to different performance levels. Based on the observation that\nsmaller models perform optimally on large parts of testing corpora, we propose\nto train a decision module, that would allow, given an audio sample, to use the\nsmallest sufficient model leading to a good transcription. We apply our\napproach to two Whisper models with different sizes. By keeping the decision\nprocess computationally efficient, we build a decision module that allows\nsubstantial computational savings with reduced performance drops.", "published": "2023-09-22 08:50:58", "link": "http://arxiv.org/abs/2309.12712v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Representations Improve Supervised Learning in Speech\n  Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) plays a pivotal role in enhancing\nhuman-computer interaction by enabling a deeper understanding of emotional\nstates across a wide range of applications, contributing to more empathetic and\neffective communication. This study proposes an innovative approach that\nintegrates self-supervised feature extraction with supervised classification\nfor emotion recognition from small audio segments. In the preprocessing step,\nto eliminate the need of crafting audio features, we employed a self-supervised\nfeature extractor, based on the Wav2Vec model, to capture acoustic features\nfrom audio data. Then, the output featuremaps of the preprocessing step are fed\nto a custom designed Convolutional Neural Network (CNN)-based model to perform\nemotion classification. Utilizing the ShEMO dataset as our testing ground, the\nproposed method surpasses two baseline methods, i.e. support vector machine\nclassifier and transfer learning of a pretrained CNN. comparing the propose\nmethod to the state-of-the-art methods in SER task indicates the superiority of\nthe proposed method. Our findings underscore the pivotal role of deep\nunsupervised feature learning in elevating the landscape of SER, offering\nenhanced emotional comprehension in the realm of human-computer interactions.", "published": "2023-09-22 08:54:06", "link": "http://arxiv.org/abs/2309.12714v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deepfake audio as a data augmentation technique for training automatic\n  speech to text transcription models", "abstract": "To train transcriptor models that produce robust results, a large and diverse\nlabeled dataset is required. Finding such data with the necessary\ncharacteristics is a challenging task, especially for languages less popular\nthan English. Moreover, producing such data requires significant effort and\noften money. Therefore, a strategy to mitigate this problem is the use of data\naugmentation techniques. In this work, we propose a framework that approaches\ndata augmentation based on deepfake audio. To validate the produced framework,\nexperiments were conducted using existing deepfake and transcription models. A\nvoice cloner and a dataset produced by Indians (in English) were selected,\nensuring the presence of a single accent in the dataset. Subsequently, the\naugmented data was used to train speech to text models in various scenarios.", "published": "2023-09-22 11:33:03", "link": "http://arxiv.org/abs/2309.12802v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.6; I.2.0; E.0"], "primary_category": "cs.SD"}
{"title": "Memory-augmented conformer for improved end-to-end long-form ASR", "abstract": "Conformers have recently been proposed as a promising modelling approach for\nautomatic speech recognition (ASR), outperforming recurrent neural\nnetwork-based approaches and transformers. Nevertheless, in general, the\nperformance of these end-to-end models, especially attention-based models, is\nparticularly degraded in the case of long utterances. To address this\nlimitation, we propose adding a fully-differentiable memory-augmented neural\nnetwork between the encoder and decoder of a conformer. This external memory\ncan enrich the generalization for longer utterances since it allows the system\nto store and retrieve more information recurrently. Notably, we explore the\nneural Turing machine (NTM) that results in our proposed Conformer-NTM model\narchitecture for ASR. Experimental results using Librispeech train-clean-100\nand train-960 sets show that the proposed system outperforms the baseline\nconformer without memory for long utterances.", "published": "2023-09-22 17:44:58", "link": "http://arxiv.org/abs/2309.13029v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Importance of Smoothness Induced by Optimizers in FL4ASR: Towards\n  Understanding Federated Learning for End-to-End ASR", "abstract": "In this paper, we start by training End-to-End Automatic Speech Recognition\n(ASR) models using Federated Learning (FL) and examining the fundamental\nconsiderations that can be pivotal in minimizing the performance gap in terms\nof word error rate between models trained using FL versus their centralized\ncounterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii)\nloss characteristics via altering Connectionist Temporal Classification (CTC)\nweight, (iii) model initialization through seed start, (iv) carrying over\nmodeling setup from experiences in centralized training to FL, e.g., pre-layer\nor post-layer normalization, and (v) FL-specific hyperparameters, such as\nnumber of local epochs, client sampling size, and learning rate scheduler,\nspecifically for ASR under heterogeneous data distribution. We shed light on\nhow some optimizers work better than others via inducing smoothness. We also\nsummarize the applicability of algorithms, trends, and propose best practices\nfrom prior works in FL (in general) toward End-to-End ASR models.", "published": "2023-09-22 17:23:01", "link": "http://arxiv.org/abs/2309.13102v1", "categories": ["eess.AS", "cs.DC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Invisible Watermarking for Audio Generation Diffusion Models", "abstract": "Diffusion models have gained prominence in the image domain for their\ncapabilities in data generation and transformation, achieving state-of-the-art\nperformance in various tasks in both image and audio domains. In the rapidly\nevolving field of audio-based machine learning, safeguarding model integrity\nand establishing data copyright are of paramount importance. This paper\npresents the first watermarking technique applied to audio diffusion models\ntrained on mel-spectrograms. This offers a novel approach to the aforementioned\nchallenges. Our model excels not only in benign audio generation, but also\nincorporates an invisible watermarking trigger mechanism for model\nverification. This watermark trigger serves as a protective layer, enabling the\nidentification of model ownership and ensuring its integrity. Through extensive\nexperiments, we demonstrate that invisible watermark triggers can effectively\nprotect against unauthorized modifications while maintaining high utility in\nbenign audio generation tasks.", "published": "2023-09-22 20:10:46", "link": "http://arxiv.org/abs/2309.13166v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
