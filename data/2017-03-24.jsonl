{"title": "Crowdsourcing Universal Part-Of-Speech Tags for Code-Switching", "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between\nmultiple languages during communication. The importance of developing language\ntechnologies for codeswitching data is immense, given the large populations\nthat routinely code-switch. High-quality linguistic annotations are extremely\nvaluable for any NLP task, and performance is often limited by the amount of\nhigh-quality labeled data. However, little such data exists for code-switching.\nIn this paper, we describe crowd-sourcing universal part-of-speech tags for the\nMiami Bangor Corpus of Spanish-English code-switched speech. We split the\nannotation task into three subtasks: one in which a subset of tokens are\nlabeled automatically, one in which questions are specifically designed to\ndisambiguate a subset of high frequency words, and a more general cascaded\napproach for the remaining data in which questions are displayed to the worker\nfollowing a decision tree structure. Each subtask is extended and adapted for a\nmultilingual setting and the universal tagset. The quality of the annotation\nprocess is measured using hidden check questions annotated with gold labels.\nThe overall agreement between gold standard labels and the majority vote is\nbetween 0.95 and 0.96 for just three labels and the average recall across\npart-of-speech tags is between 0.87 and 0.99, depending on the task.", "published": "2017-03-24 17:55:33", "link": "http://arxiv.org/abs/1703.08537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interacting Conceptual Spaces I : Grammatical Composition of Concepts", "abstract": "The categorical compositional approach to meaning has been successfully\napplied in natural language processing, outperforming other models in\nmainstream empirical language processing tasks. We show how this approach can\nbe generalized to conceptual space models of cognition. In order to do this,\nfirst we introduce the category of convex relations as a new setting for\ncategorical compositional semantics, emphasizing the convex structure important\nto conceptual space applications. We then show how to construct conceptual\nspaces for various types such as nouns, adjectives and verbs. Finally we show\nby means of examples how concepts can be systematically combined to establish\nthe meanings of composite phrases from the meanings of their constituent parts.\nThis provides the mathematical underpinnings of a new compositional approach to\ncognition.", "published": "2017-03-24 08:46:48", "link": "http://arxiv.org/abs/1703.08314v2", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Batch-normalized joint training for DNN-based distant speech recognition", "abstract": "Improving distant speech recognition is a crucial step towards flexible\nhuman-machine interfaces. Current technology, however, still exhibits a lack of\nrobustness, especially when adverse acoustic conditions are met. Despite the\nsignificant progress made in the last years on both speech enhancement and\nspeech recognition, one potential limitation of state-of-the-art technology\nlies in composing modules that are not well matched because they are not\ntrained jointly. To address this concern, a promising approach consists in\nconcatenating a speech enhancement and a speech recognition deep neural network\nand to jointly update their parameters as if they were within a single bigger\nnetwork. Unfortunately, joint training can be difficult because the output\ndistribution of the speech enhancement system may change substantially during\nthe optimization procedure. The speech recognition module would have to deal\nwith an input distribution that is non-stationary and unnormalized. To mitigate\nthis issue, we propose a joint training approach based on a fully\nbatch-normalized architecture. Experiments, conducted using different datasets,\ntasks and acoustic conditions, revealed that the proposed framework\nsignificantly overtakes other competitive solutions, especially in challenging\nenvironments.", "published": "2017-03-24 15:40:19", "link": "http://arxiv.org/abs/1703.08471v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Natural Language Acquisition in a Multi-modal Recurrent\n  Neural Architecture", "abstract": "For the complex human brain that enables us to communicate in natural\nlanguage, we gathered good understandings of principles underlying language\nacquisition and processing, knowledge about socio-cultural conditions, and\ninsights about activity patterns in the brain. However, we were not yet able to\nunderstand the behavioural and mechanistic characteristics for natural language\nand how mechanisms in the brain allow to acquire and process language. In\nbridging the insights from behavioural psychology and neuroscience, the goal of\nthis paper is to contribute a computational understanding of appropriate\ncharacteristics that favour language acquisition. Accordingly, we provide\nconcepts and refinements in cognitive modelling regarding principles and\nmechanisms in the brain and propose a neurocognitively plausible model for\nembodied language acquisition from real world interaction of a humanoid robot\nwith its environment. In particular, the architecture consists of a continuous\ntime recurrent neural network, where parts have different leakage\ncharacteristics and thus operate on multiple timescales for every modality and\nthe association of the higher level nodes of all modalities into cell\nassemblies. The model is capable of learning language production grounded in\nboth, temporal dynamic somatosensation and vision, and features hierarchical\nconcept abstraction, concept decomposition, multi-modal integration, and\nself-organisation of latent representations.", "published": "2017-03-24 17:13:08", "link": "http://arxiv.org/abs/1703.08513v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Data-Mining Textual Responses to Uncover Misconception Patterns", "abstract": "An important, yet largely unstudied, problem in student data analysis is to\ndetect misconceptions from students' responses to open-response questions.\nMisconception detection enables instructors to deliver more targeted feedback\non the misconceptions exhibited by many students in their class, thus improving\nthe quality of instruction. In this paper, we propose a new natural language\nprocessing-based framework to detect the common misconceptions among students'\ntextual responses to short-answer questions. We propose a probabilistic model\nfor students' textual responses involving misconceptions and experimentally\nvalidate it on a real-world student-response dataset. Experimental results show\nthat our proposed framework excels at classifying whether a response exhibits\none or more misconceptions. More importantly, it can also automatically detect\nthe common misconceptions exhibited across responses from multiple students to\nmultiple questions; this property is especially important at large scale, since\ninstructors will no longer need to manually specify all possible misconceptions\nthat students might exhibit.", "published": "2017-03-24 14:49:58", "link": "http://arxiv.org/abs/1703.08544v2", "categories": ["stat.ML", "cs.CL"], "primary_category": "stat.ML"}
{"title": "Are crossing dependencies really scarce?", "abstract": "The syntactic structure of a sentence can be modelled as a tree, where\nvertices correspond to words and edges indicate syntactic dependencies. It has\nbeen claimed recurrently that the number of edge crossings in real sentences is\nsmall. However, a baseline or null hypothesis has been lacking. Here we\nquantify the amount of crossings of real sentences and compare it to the\npredictions of a series of baselines. We conclude that crossings are really\nscarce in real sentences. Their scarcity is unexpected by the hubiness of the\ntrees. Indeed, real sentences are close to linear trees, where the potential\nnumber of crossings is maximized.", "published": "2017-03-24 09:32:23", "link": "http://arxiv.org/abs/1703.08324v2", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "cs.CL", "physics.data-an"], "primary_category": "physics.soc-ph"}
{"title": "Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans\n  in the Loop", "abstract": "Although information workers may complain about meetings, they are an\nessential part of their work life. Consequently, busy people spend a\nsignificant amount of time scheduling meetings. We present Calendar.help, a\nsystem that provides fast, efficient scheduling through structured workflows.\nUsers interact with the system via email, delegating their scheduling needs to\nthe system as if it were a human personal assistant. Common scheduling\nscenarios are broken down using well-defined workflows and completed as a\nseries of microtasks that are automated when possible and executed by a human\notherwise. Unusual scenarios fall back to a trained human assistant who\nexecutes them as unstructured macrotasks. We describe the iterative approach we\nused to develop Calendar.help, and share the lessons learned from scheduling\nthousands of meetings during a year of real-world deployments. Our findings\nprovide insight into how complex information tasks can be broken down into\nrepeatable components that can be executed efficiently to improve productivity.", "published": "2017-03-24 14:40:31", "link": "http://arxiv.org/abs/1703.08428v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech", "abstract": "We present a recurrent encoder-decoder deep neural network architecture that\ndirectly translates speech in one language into text in another. The model does\nnot explicitly transcribe the speech into text in the source language, nor does\nit require supervision from the ground truth source language transcription\nduring training. We apply a slightly modified sequence-to-sequence with\nattention architecture that has previously been used for speech recognition and\nshow that it can be repurposed for this more complex task, illustrating the\npower of attention-based models. A single model trained end-to-end obtains\nstate-of-the-art performance on the Fisher Callhome Spanish-English speech\ntranslation task, outperforming a cascade of independently trained\nsequence-to-sequence speech recognition and machine translation models by 1.8\nBLEU points on the Fisher test set. In addition, we find that making use of the\ntraining data in both languages by multi-task training sequence-to-sequence\nspeech translation and recognition models with a shared encoder network can\nimprove performance by a further 1.4 BLEU points.", "published": "2017-03-24 19:45:24", "link": "http://arxiv.org/abs/1703.08581v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
