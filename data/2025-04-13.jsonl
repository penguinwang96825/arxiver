{"title": "The Rate-Immediacy Barrier in Explicit Tree Code Constructions", "abstract": "Since the introduction of tree codes by Schulman (STOC 1993), explicit\nconstruction of such codes has remained a notorious challenge. While the\nconstruction of asymptotically-good explicit tree codes continues to be\nelusive, a work by Cohen, Haeupler and Schulman (STOC 2018), as well as the\nstate-of-the-art construction by Ben Yaacov, Cohen, and Yankovitz (STOC 2022)\nhave achieved codes with rate $\\Omega(1/\\log\\log n)$, exponentially improving\nupon the original construction of Evans, Klugerman and Schulman from 1994. All\nof these constructions rely, at least in part, on increasingly sophisticated\nmethods of combining (block) error-correcting codes.\n  In this work, we identify a fundamental barrier to constructing tree codes\nusing current techniques. We introduce a key property, which we call immediacy,\nthat, while not required by the original definition of tree codes, is shared by\nall known constructions and inherently arises from recursive combinations of\nerror-correcting codes. Our main technical contribution is the proof of a\nrate-immediacy tradeoff, which, in particular, implies that any tree code with\nconstant distance and non-trivial immediacy must necessarily have vanishing\nrate. By applying our rate-immediacy tradeoff to existing constructions, we\nestablish that their known rate analyses are essentially optimal. More broadly,\nour work highlights the need for fundamentally new ideas--beyond the recursive\nuse of error-correcting codes--to achieve substantial progress in explicitly\nconstructing asymptotically-good tree codes.", "published": "2025-04-13 00:47:48", "link": "http://arxiv.org/abs/2504.09388v1", "categories": ["cs.IT", "cs.CC", "cs.DM", "math.IT"], "primary_category": "cs.IT"}
{"title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking", "abstract": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.", "published": "2025-04-13 16:34:17", "link": "http://arxiv.org/abs/2504.09643v1", "categories": ["cs.CL", "cs.IR", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Outage Probability Analysis for OTFS with Finite Blocklength", "abstract": "Orthogonal time frequency space (OTFS) modulation is widely acknowledged as a\nprospective waveform for future wireless communication networks.To provide\ninsights for the practical system design, this paper analyzes the outage\nprobability of OTFS modulation with finite blocklength.To begin with, we\npresent the system model and formulate the analysis of outage probability for\nOTFS with finite blocklength as an equivalent problem of calculating the outage\nprobability with finite blocklength over parallel additive white Gaussian noise\n(AWGN) channels.Subsequently, we apply the equivalent noise approach to derive\na lower bound on the outage probability of OTFS with finite blocklength under\nboth average power allocation and water-filling power allocation strategies,\nrespectively.Finally, the lower bounds of the outage probability are determined\nusing the Monte-Carlo method for the two power allocation strategies.The impact\nof the number of resolvable paths and coding rates on the outage probability is\nanalyzed, and the simulation results are compared with the theoretical lower\nbounds.", "published": "2025-04-13 15:53:46", "link": "http://arxiv.org/abs/2504.09628v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Slow Thinking for Sequential Recommendation", "abstract": "To develop effective sequential recommender systems, numerous methods have\nbeen proposed to model historical user behaviors. Despite the effectiveness,\nthese methods share the same fast thinking paradigm. That is, for making\nrecommendations, these methods typically encodes user historical interactions\nto obtain user representations and directly match these representations with\ncandidate item representations. However, due to the limited capacity of\ntraditional lightweight recommendation models, this one-step inference paradigm\noften leads to suboptimal performance. To tackle this issue, we present a novel\nslow thinking recommendation model, named STREAM-Rec. Our approach is capable\nof analyzing historical user behavior, generating a multi-step, deliberative\nreasoning process, and ultimately delivering personalized recommendations. In\nparticular, we focus on two key challenges: (1) identifying the suitable\nreasoning patterns in recommender systems, and (2) exploring how to effectively\nstimulate the reasoning capabilities of traditional recommenders. To this end,\nwe introduce a three-stage training framework. In the first stage, the model is\npretrained on large-scale user behavior data to learn behavior patterns and\ncapture long-range dependencies. In the second stage, we design an iterative\ninference algorithm to annotate suitable reasoning traces by progressively\nrefining the model predictions. This annotated data is then used to fine-tune\nthe model. Finally, in the third stage, we apply reinforcement learning to\nfurther enhance the model generalization ability. Extensive experiments\nvalidate the effectiveness of our proposed method.", "published": "2025-04-13 15:53:30", "link": "http://arxiv.org/abs/2504.09627v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Integrating Textual Embeddings from Contrastive Learning with Generative Recommender for Enhanced Personalization", "abstract": "Recent advances in recommender systems have highlighted the complementary\nstrengths of generative modeling and pretrained language models. We propose a\nhybrid framework that augments the Hierarchical Sequential Transduction Unit\n(HSTU) generative recommender with BLaIR -- a contrastive text embedding model.\nThis integration enriches item representations with semantic signals from\ntextual metadata while preserving HSTU's powerful sequence modeling\ncapabilities.\n  We evaluate our method on two domains from the Amazon Reviews 2023 dataset,\ncomparing it against the original HSTU and a variant that incorporates\nembeddings from OpenAI's state-of-the-art text-embedding-3-large model. While\nthe OpenAI embedding model is likely trained on a substantially larger corpus\nwith significantly more parameters, our lightweight BLaIR-enhanced approach --\npretrained on domain-specific data -- consistently achieves better performance,\nhighlighting the effectiveness of contrastive text embeddings in\ncompute-efficient settings.", "published": "2025-04-13 15:23:00", "link": "http://arxiv.org/abs/2504.10545v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Revisiting Self-Attentive Sequential Recommendation", "abstract": "Recommender systems are ubiquitous in on-line services to drive businesses.\nAnd many sequential recommender models were deployed in these systems to\nenhance personalization. The approach of using the transformer decoder as the\nsequential recommender was proposed years ago and is still a strong baseline in\nrecent works. But this kind of sequential recommender model did not scale up\nwell, compared to language models. Quite some details in the classical\nself-attentive sequential recommender model could be revisited, and some new\nexperiments may lead to new findings, without changing the general model\nstructure which was the focus of many previous works. In this paper, we show\nthe details and propose new experiment methodologies for future research on\nsequential recommendation, in hope to motivate further exploration to new\nfindings in this area.", "published": "2025-04-13 14:30:57", "link": "http://arxiv.org/abs/2504.09596v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "HD-RAG: Retrieval-Augmented Generation for Hybrid Documents Containing Text and Hierarchical Tables", "abstract": "With the rapid advancement of large language models (LLMs),\nRetrieval-Augmented Generation (RAG) effectively combines LLMs generative\ncapabilities with external retrieval-based information. The Hybrid Document RAG\ntask aims to integrate textual and hierarchical tabular data for more\ncomprehensive retrieval and generation in complex scenarios. However, there is\nno existing dataset specifically designed for this task that includes both text\nand tabular data. Additionally, existing methods struggle to retrieve relevant\ntabular data and integrate it with text. Semantic similarity-based retrieval\nlacks accuracy, while table-specific methods fail to handle complex\nhierarchical structures effectively. Furthermore, the QA task requires complex\nreasoning and calculations, further complicating the challenge. In this paper,\nwe propose a new large-scale dataset, DocRAGLib, specifically designed for the\nquestion answering (QA) task scenario under Hybrid Document RAG. To tackle\nthese challenges, we introduce HD-RAG, a novel framework that incorporates a\nrow-and-column level (RCL) table representation, employs a two-stage process\ncombining ensemble and LLM-based retrieval, and integrates RECAP, which is\ndesigned for multi-step reasoning and complex calculations in Document-QA\ntasks. We conduct comprehensive experiments with DocRAGLib, showing that HD-RAG\noutperforms existing baselines in both retrieval accuracy and QA performance,\ndemonstrating its effectiveness.", "published": "2025-04-13 13:02:33", "link": "http://arxiv.org/abs/2504.09554v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation", "abstract": "The burgeoning presence of Large Language Models (LLM) is propelling the\ndevelopment of personalized recommender systems. Most existing LLM-based\nmethods fail to sufficiently explore the multi-view graph structure\ncorrelations inherent in recommendation scenarios. To this end, we propose a\nnovel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation\n(HeLLM), designed to equip LLMs with the capability to capture intricate\nhigher-order semantic correlations by fusing graph-level contextual signals\nwith sequence-level behavioral patterns. In the recommender pre-training phase,\nwe design a user hypergraph to uncover shared interest preferences among users\nand an item hypergraph to capture correlations within multimodal similarities\namong items. The hypergraph convolution and synergistic contrastive learning\nmechanism are introduced to enhance the distinguishability of learned\nrepresentations. In the LLM fine-tuning phase, we inject the learned\ngraph-structured embeddings directly into the LLM's architecture and integrate\nsequential features capturing each user's chronological behavior. This process\nenables hypergraphs to leverage graph-structured information as global context,\nenhancing the LLM's ability to perceive complex relational patterns and\nintegrate multimodal information, while also modeling local temporal dynamics.\nExtensive experiments demonstrate the superiority of our proposed method over\nstate-of-the-art baselines, confirming the advantages of fusing\nhypergraph-based context with sequential user behavior in LLMs for\nrecommendation.", "published": "2025-04-13 09:12:35", "link": "http://arxiv.org/abs/2504.10541v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Distilling Transitional Pattern to Large Language Models for Multimodal Session-based Recommendation", "abstract": "Session-based recommendation (SBR) predicts the next item based on anonymous\nsessions. Traditional SBR explores user intents based on ID collaborations or\nauxiliary content. To further alleviate data sparsity and cold-start issues,\nrecent Multimodal SBR (MSBR) methods utilize simplistic pre-trained models for\nmodality learning but have limitations in semantic richness. Considering\nsemantic reasoning abilities of Large Language Models (LLM), we focus on the\nLLM-enhanced MSBR scenario in this paper, which leverages LLM cognition for\ncomprehensive multimodal representation generation, to enhance downstream MSBR.\nTackling this problem faces two challenges: i) how to obtain LLM cognition on\nboth transitional patterns and inherent multimodal knowledge, ii) how to align\nboth features into one unified LLM, minimize discrepancy while maximizing\nrepresentation utility. To this end, we propose a multimodal LLM-enhanced\nframework TPAD, which extends a distillation paradigm to decouple and align\ntransitional patterns for promoting MSBR. TPAD establishes parallel\nKnowledge-MLLM and Transfer-MLLM, where the former interprets item\nknowledge-reflected features and the latter extracts transition-aware features\nunderneath sessions. A transitional pattern alignment module harnessing mutual\ninformation estimation theory unites two MLLMs, alleviating distribution\ndiscrepancy and distilling transitional patterns into modal representations.\nExtensive experiments on real-world datasets demonstrate the effectiveness of\nour framework.", "published": "2025-04-13 07:49:08", "link": "http://arxiv.org/abs/2504.10538v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences", "abstract": "Due to the convenience of mobile devices, the online games have become an\nimportant part for user entertainments in reality, creating a demand for friend\nrecommendation in online games. However, none of existing approaches can\neffectively incorporate the multi-modal user features (\\emph{e.g.}, images and\ntexts) with the structural information in the friendship graph, due to the\nfollowing limitations: (1) some of them ignore the high-order structural\nproximity between users, (2) some fail to learn the pairwise relevance between\nusers at modality-specific level, and (3) some cannot capture both the local\nand global user preferences on different modalities. By addressing these\nissues, in this paper, we propose an end-to-end model \\textsc{FROG} that better\nmodels the user preferences on potential friends. Comprehensive experiments on\nboth offline evaluation and online deployment at \\kw{Tencent} have demonstrated\nthe superiority of \\textsc{FROG} over existing approaches.", "published": "2025-04-13 04:27:10", "link": "http://arxiv.org/abs/2504.09428v1", "categories": ["cs.SI", "cs.AI", "cs.IR"], "primary_category": "cs.SI"}
{"title": "SegOTA: Accelerating Over-the-Air Federated Learning with Segmented Transmission", "abstract": "Federated learning (FL) with over-the-air computation efficiently utilizes\nthe communication resources, but it can still experience significant latency\nwhen each device transmits a large number of model parameters to the server.\nThis paper proposes the Segmented Over-The-Air (SegOTA) method for FL, which\nreduces latency by partitioning devices into groups and letting each group\ntransmit only one segment of the model parameters in each communication round.\nConsidering a multi-antenna server, we model the SegOTA transmission and\nreception process to establish an upper bound on the expected model learning\noptimality gap. We minimize this upper bound, by formulating the per-round\nonline optimization of device grouping and joint transmit-receive beamforming,\nfor which we derive efficient closed-form solutions. Simulation results show\nthat our proposed SegOTA substantially outperforms the conventional full-model\nOTA approach and other common alternatives.", "published": "2025-04-13 22:44:23", "link": "http://arxiv.org/abs/2504.09745v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "InfoMAE: Pair-Efficient Cross-Modal Alignment for Multimodal Time-Series Sensing Signals", "abstract": "Standard multimodal self-supervised learning (SSL) algorithms regard\ncross-modal synchronization as implicit supervisory labels during pretraining,\nthus posing high requirements on the scale and quality of multimodal samples.\nThese constraints significantly limit the performance of sensing intelligence\nin IoT applications, as the heterogeneity and the non-interpretability of\ntime-series signals result in abundant unimodal data but scarce high-quality\nmultimodal pairs. This paper proposes InfoMAE, a cross-modal alignment\nframework that tackles the challenge of multimodal pair efficiency under the\nSSL setting by facilitating efficient cross-modal alignment of pretrained\nunimodal representations. InfoMAE achieves \\textit{efficient cross-modal\nalignment} with \\textit{limited data pairs} through a novel information\ntheory-inspired formulation that simultaneously addresses distribution-level\nand instance-level alignment. Extensive experiments on two real-world IoT\napplications are performed to evaluate InfoMAE's pairing efficiency to bridge\npretrained unimodal models into a cohesive joint multimodal model. InfoMAE\nenhances downstream multimodal tasks by over 60% with significantly improved\nmultimodal pairing efficiency. It also improves unimodal task accuracy by an\naverage of 22%.", "published": "2025-04-13 20:03:29", "link": "http://arxiv.org/abs/2504.09707v1", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.MM", "math.IT"], "primary_category": "cs.AI"}
{"title": "On Stochastic Performance Analysis of Secure Integrated Sensing and Communication Networks", "abstract": "This paper analyzes the stochastic security performance of a multiple-input\nmultiple-output (MIMO) integrated sensing and communication (ISAC) system in a\ndownlink scenario. A base station (BS) transmits a multi-functional signal to\nsimultaneously communicate with a user, sense a target angular location, and\ncounteract eavesdropping threats. The system includes a passive single-antenna\ncommunication eavesdropper and a multi-antenna sensing eavesdropper attempting\nto infer the target location. The BS-user and BS-eavesdroppers channels follow\nRayleigh fading, while the target azimuth angle is uniformly distributed. To\nevaluate the performance, we derive exact expressions for the secrecy ergodic\nrate and the ergodic Cramer-Rao lower bound (CRB) for target localization at\nboth the BS and the sensing eavesdropper. This involves computing the\nprobability density functions (PDFs) of the signal-to-noise ratio (SNR) and\nCRB, leveraging the central limit theorem for tractability. Numerical results\nvalidate our findings.", "published": "2025-04-13 17:54:53", "link": "http://arxiv.org/abs/2504.09674v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.", "published": "2025-04-13 14:31:52", "link": "http://arxiv.org/abs/2504.09597v2", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.AI"}
{"title": "Bounds and Optimal Constructions of Generalized Merge-Convertible Codes for Code Conversion into LRCs", "abstract": "Error-correcting codes are essential for ensuring fault tolerance in modern\ndistributed data storage systems. However, in practice, factors such as the\nfailure rates of storage devices can vary significantly over time, resulting in\nchanges to the optimal code parameters. To reduce storage costs while\nmaintaining efficiency, Maturana and Rashmi introduced a theoretical framework\nknown as code conversion, which enables dynamic adjustment of code parameters\naccording to device performance. In this paper, we focus exclusively on the\nbounds and constructions of generalized merge-convertible codes. First, we\nestablish a new lower bound on the access cost when the final code is an\n$(r,\\delta)$-LRC. This bound unifies and generalizes all previously known\nbounds for merge conversion where the initial and final codes are either an LRC\nor an MDS code. We then construct a family of access-optimal MDS convertible\ncodes by leveraging subgroups of the automorphism group of a rational function\nfield. It is worth noting that our construction is also per-symbol read\naccess-optimal. Next, we further extend our MDS-based construction to design\naccess-optimal convertible codes for the conversion between $(r,\\delta)$-LRCs.\nFinally, using the parity-check matrix approach, we present a construction of\naccess-optimal convertible codes that enable merge conversion from MDS codes to\nan $(r,\\delta)$-LRC. To the best of our knowledge, this is the first explicit\noptimal construction of code conversion between MDS codes and LRCs. All of our\nconstructions are over finite fields whose sizes grow linearly with the code\nlength.", "published": "2025-04-13 14:01:10", "link": "http://arxiv.org/abs/2504.09580v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dominated Actions in Imperfect-Information Games", "abstract": "Dominance is a fundamental concept in game theory. In strategic-form games\ndominated strategies can be identified in polynomial time. As a consequence,\niterative removal of dominated strategies can be performed efficiently as a\npreprocessing step for reducing the size of a game before computing a Nash\nequilibrium. For imperfect-information games in extensive form, we could\nconvert the game to strategic form and then iteratively remove dominated\nstrategies in the same way; however, this conversion may cause an exponential\nblowup in game size. In this paper we define and study the concept of dominated\nactions in imperfect-information games. Our main result is a polynomial-time\nalgorithm for determining whether an action is dominated (strictly or weakly)\nby any mixed strategy in n-player games, which can be extended to an algorithm\nfor iteratively removing dominated actions. This allows us to efficiently\nreduce the size of the game tree as a preprocessing step for Nash equilibrium\ncomputation. We explore the role of dominated actions empirically in the \"All\nIn or Fold\" No-Limit Texas Hold'em poker variant.", "published": "2025-04-13 20:48:44", "link": "http://arxiv.org/abs/2504.09716v1", "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH"], "primary_category": "cs.GT"}
{"title": "AgentDynEx: Nudging the Mechanics and Dynamics of Multi-Agent Simulations", "abstract": "Multi-agent large language model simulations have the potential to model\ncomplex human behaviors and interactions. If the mechanics are set up properly,\nunanticipated and valuable social dynamics can surface. However, it is\nchallenging to consistently enforce simulation mechanics while still allowing\nfor notable and emergent dynamics. We present AgentDynEx, an AI system that\nhelps set up simulations from user-specified mechanics and dynamics. AgentDynEx\nuses LLMs to guide users through a Configuration Matrix to identify core\nmechanics and define milestones to track dynamics. It also introduces a method\ncalled \\textit{nudging}, where the system dynamically reflects on simulation\nprogress and gently intervenes if it begins to deviate from intended outcomes.\nA technical evaluation found that nudging enables simulations to have more\ncomplex mechanics and maintain its notable dynamics compared to simulations\nwithout nudging. We discuss the importance of nudging as a technique for\nbalancing mechanics and dynamics of multi-agent simulations.", "published": "2025-04-13 17:26:35", "link": "http://arxiv.org/abs/2504.09662v1", "categories": ["cs.MA", "cs.AI", "cs.HC"], "primary_category": "cs.MA"}
{"title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference", "abstract": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions.", "published": "2025-04-13 15:28:09", "link": "http://arxiv.org/abs/2504.09620v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Unification of Consensus-Based Multi-Objective Optimization and Multi-Robot Path Planning", "abstract": "Multi-agent systems seeking consensus may also have other objective functions\nto optimize, requiring the research of multi-objective optimization in\nconsensus. Several recent publications have explored this domain using various\nmethods such as weighted-sum optimization and penalization methods. This paper\nreviews the state of the art for consensus-based multi-objective optimization,\nposes a multi-agent lunar rover exploration problem seeking consensus and\nmaximization of explored area, and achieves optimal edge weights and steering\nangles by applying SQP algorithms.", "published": "2025-04-13 13:56:54", "link": "http://arxiv.org/abs/2504.09577v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Stochastic generative methods for stable and accurate closure modeling of chaotic dynamical systems", "abstract": "Traditional deterministic subgrid-scale (SGS) models are often dissipative\nand unstable, especially in regions of chaotic and turbulent flow. Ongoing work\nin climate science and ocean modeling motivates the use of stochastic SGS\nmodels for chaotic dynamics. Further, developing stochastic generative models\nof underlying dynamics is a rapidly expanding field. In this work, we aim to\nincorporate stochastic integration toward closure modeling for chaotic\ndynamical systems. Further, we want to explore the potential stabilizing effect\nthat stochastic models could have on linearized chaotic systems. We propose\nparametric and generative approaches for closure modeling using stochastic\ndifferential equations (SDEs). We derive and implement a quadratic diffusion\nmodel based on the fluctuations, demonstrating increased accuracy from bridging\ntheoretical models with generative approaches. Results are demonstrated on the\nLorenz-63 dynamical system.", "published": "2025-04-13 22:59:42", "link": "http://arxiv.org/abs/2504.09750v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Level-set topology optimisation with unfitted finite elements and automatic shape differentiation", "abstract": "In this paper we develop automatic shape differentiation techniques for\nunfitted discretisations and link these to recent advances in shape calculus\nfor unfitted methods. We extend existing analytic shape calculus results to the\ncase where the domain boundary intersects with the boundary of the background\ndomain. We further show that we can recover these analytic derivatives to\nmachine precision regardless of the mesh size using the developed automatic\nshape differentiation techniques. In addition, we show that we can also recover\nthe symmetric shape Hessian. We implement these techniques for both serial and\ndistributed computing frameworks in the Julia package GridapTopOpt and the\nwider Gridap ecosystem. As part of this implementation we propose a novel\ngraph-based approach for isolated volume detection. We demonstrate the\napplicability of the unfitted automatic shape differentiation framework and our\nimplementation by considering the three-dimensional minimum compliance topology\noptimisation of a linear elastic wheel and of a linear elastic structure in a\nfluid-structure interaction problem with Stokes flow. The implementation is\ngeneral and allows GridapTopOpt to solve a wide range of problems without\nanalytic calculation of shape derivatives and avoiding issues that arise when\nmaterial properties are smoothed at the domain boundary. The software is open\nsource and available at https://github.com/zjwegert/GridapTopOpt.jl.", "published": "2025-04-13 22:55:35", "link": "http://arxiv.org/abs/2504.09748v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Analysis and structure-preserving approximation of a Cahn-Hilliard-Forchheimer system with solution-dependent mass and volume source", "abstract": "We analyze a coupled Cahn-Hilliard-Forchheimer system featuring\nconcentration-dependent mobility, mass source and convective transport. The\nvelocity field is governed by a generalized quasi-incompressible Forchheimer\nequation with solution-dependent volume source. We impose Dirichlet boundary\nconditions for the pressure to accommodate the source term. Our contributions\ninclude a novel well-posedness result for the generalized Forchheimer subsystem\nvia the Browder-Minty theorem, and existence of weak solutions for the full\ncoupled system established through energy estimates at the Galerkin level\ncombined with compactness techniques such as Aubin-Lions' lemma and Minty's\ntrick. Furthermore, we develop a structure-preserving discretization using\nRaviart-Thomas elements for the velocity that maintains exact mass balance and\ndiscrete energy-dissipation balance, with well-posedness demonstrated through\nrelative energy estimates and inf-sup stability. Lastly, we validate our model\nthrough numerical experiments, demonstrating optimal convergence rates,\nstructure preservation, and the role of the Forchheimer nonlinearity in\ngoverning phase-field evolution dynamics.", "published": "2025-04-13 22:08:43", "link": "http://arxiv.org/abs/2504.09739v1", "categories": ["math.NA", "cs.NA", "math.AP", "35A01, 35A02, 35D30, 35Q92"], "primary_category": "math.NA"}
{"title": "Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions", "abstract": "Accurately estimating decision boundaries in black box systems is critical\nwhen ensuring safety, quality, and feasibility in real-world applications.\nHowever, existing methods iteratively refine boundary estimates by sampling in\nregions of uncertainty, without providing guarantees on the closeness to the\ndecision boundary and also result in unnecessary exploration that is especially\ndisadvantageous when evaluations are costly. This paper presents the\nEpsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample\nefficient and function-agnostic algorithm that leverages the intermediate value\ntheorem to estimate the location of the decision boundary of a black box binary\nclassifier within a user-specified epsilon-neighborhood. Evaluations are\nconducted on three nonlinear test functions and a case study of an electric\ngrid stability problem with uncertain renewable power injection. The EDGE\nalgorithm demonstrates superior sample efficiency and better boundary\napproximation than adaptive sampling techniques and grid-based searches.", "published": "2025-04-13 21:40:46", "link": "http://arxiv.org/abs/2504.09733v1", "categories": ["cs.CG", "cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.CG"}
{"title": "Optimal convergence rates for the finite element approximation of the Sobolev constant", "abstract": "We establish optimal convergence rates for the P1 finite element\napproximation of the Sobolev constant in arbitrary dimensions N\\geq 2 and for\nLebesgue exponents 1<p<N. Our analysis relies on a refined study of the Sobolev\ndeficit in suitable quasi-norms, which have been introduced and utilized in the\ncontext of finite element approximations of the p- Laplacian. The proof further\ninvolves sharp estimates for the finite element approximation of Sobolev\nminimizers.", "published": "2025-04-13 16:22:05", "link": "http://arxiv.org/abs/2504.09637v1", "categories": ["math.NA", "cs.NA", "math.CA"], "primary_category": "math.NA"}
{"title": "Hybrid discontinuous Galerkin discretizations for the damped time-harmonic Galbrun's equation", "abstract": "In this article, we consider the damped time-harmonic Galbrun's equation\nwhich models solar and stellar oscillations. We introduce and analyze hybrid\ndiscontinuous Galerkin discretizations, which are stable and convergent for any\npolynomial degree greater or equal than one and are computationally more\nefficient than discontinuous Galerkin discretizations. Additionally, the\nmethods are stable with respect to the drastic changes in the magnitude of the\ncoefficients occurring in stars. The analysis is based on the concept of\ndiscrete approximation schemes and weak T-compatibility, which exploits the\nweakly T-coercive structure of the equation.", "published": "2025-04-13 12:34:11", "link": "http://arxiv.org/abs/2504.09547v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Super-Exponential Approximation of the Riemann-Liouville Fractional Integral via Shifted Gegenbauer Pseudospectral Methods", "abstract": "This paper introduces a shifted Gegenbauer pseudospectral (SGPS) method for\nhigh-precision approximation of the left Riemann-Liouville fractional integral\n(RLFI). By using precomputable fractional-order shifted Gegenbauer integration\nmatrices (FSGIMs), the method achieves super-exponential convergence for smooth\nfunctions, delivering near machine-precision accuracy with minimal\ncomputational cost. Tunable shifted Gegenbauer (SG) parameters enable flexible\noptimization across diverse problems, while rigorous error analysis confirms\nrapid error decay under optimal settings. Numerical experiments demonstrate\nthat the SGPS method outperforms MATLAB's integral, MATHEMATICA's NIntegrate,\nand existing techniques by up to two orders of magnitude in accuracy, with\nsuperior efficiency for varying fractional orders 0 < \\alpha < 1. Its\nadaptability and precision make the SGPS method a transformative tool for\nfractional calculus, ideal for modeling complex systems with memory and\nnon-local behavior.", "published": "2025-04-13 11:26:12", "link": "http://arxiv.org/abs/2504.09526v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Hybrid Radial Kernels for Solving Weakly Singular Fredholm Integral Equations: Balancing Accuracy and Stability in Meshless Methods", "abstract": "Over the past few decades, kernel-based approximation methods had achieved\nastonishing success in solving different problems in the field of science and\nengineering. However, when employing the direct or standard method of\nperforming computations using infinitely smooth kernels, a conflict arises\nbetween the accuracy that can be theoretically attained and the numerical\nstability. In other words, when the shape parameter tends to zero, the\noperational matrix for the standard bases with infinitely smooth kernels become\nseverely ill-conditioned. This conflict can be managed applying hybrid kernels.\nThe hybrid kernels extend the approximation space and provide high flexibility\nto strike the best possible balance between accuracy and stability. In the\ncurrent study, an innovative approach using hybrid radial kernels (HRKs) is\nprovided to solve weakly singular Fredholm integral equations (WSFIEs) of the\nsecond kind in a meshless scheme. The approach employs hybrid kernels built on\ndispersed nodes as a basis within the discrete collocation technique. This\nmethod transforms the problem being studied into a linear system of algebraic\nequations. Also, the particle swarm optimization (PSO) algorithm is utilized to\ncalculate the optimal parameters for the hybrid kernels, which is based on\nminimizing the maximum absolute error (MAE). We also study the error estimate\nof the suggested scheme. Lastly, we assess the accuracy and validity of the\nhybrid technique by carrying out various numerical experiments. The numerical\nfindings show that the estimates obtained from hybrid kernels are significantly\nmore accurate in solving WSFIEs compared to pure kernels. Additionally, it was\nrevealed that the hybrid bases remain stable across various values of the shape\nparameters.", "published": "2025-04-13 09:17:22", "link": "http://arxiv.org/abs/2504.09492v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The Whitney method of fundamental solutions with Lusin wavelets", "abstract": "We establish the theoretical foundation for a variant of the method of\nfundamental solutions (MFS), where the source points $\\{q_j\\}_{j=1}^\\infty$\naccumulate towards the domain in a Whitney fashion, meaning that their\nseparation is proportional to the distance to the domain. We prove that the\nnormalized Lusin wavelets $\\psi_j(w) = b_j(w-q_j)^{-2}$ constitute a\ngeneralized basis, known as a frame, for the Hardy subspace of $L_2$-traces of\nholomorphic functions on the domain. Consequently, our method, where $\\psi_j$\nare used as basis functions in the MFS, enables a numerically stable\napproximation of solutions to Laplace boundary value problems, even when the\nsolutions lack analytic continuation across the boundary. Despite the source\npoints accumulating towards the domain, our computations show no loss of\naccuracy near the boundary, in contrast to the boundary integral equation\nmethod.", "published": "2025-04-13 07:04:24", "link": "http://arxiv.org/abs/2504.09458v1", "categories": ["math.NA", "cs.NA", "math.AP", "math.CA", "42B37, 42C40, 65N12, 65N80"], "primary_category": "math.NA"}
{"title": "Stong order 1 adaptive approximation of jump-diffusion SDEs with discontinuous drift", "abstract": "We present an adaptive approximation scheme for jump-diffusion SDEs with\ndiscontinuous drift and (possibly) degenerate diffusion. This\ntransformation-based doubly-adaptive quasi-Milstein scheme is the first scheme\nthat has strong convergence rate $1$ in $L^p$ for $p\\in[1,\\infty)$ with respect\nto the average computational cost for these SDEs. To obtain our result, we\nprove that under slightly stronger assumptions which are still weaker than\nthose in existing literature, a related doubly-adaptive quasi-Milstein scheme\nhas convergence order $1$. This scheme is doubly-adaptive in the sense that it\nis jump-adapted, i.e.~all jump times of the Poisson noise are grid points, and\nit includes an adaptive stepsize strategy to account for the discontinuities of\nthe drift.", "published": "2025-04-13 06:22:52", "link": "http://arxiv.org/abs/2504.09452v1", "categories": ["math.NA", "cs.NA", "math.PR", "60H10, 65C30, 65C20"], "primary_category": "math.NA"}
{"title": "Heterogeneous multiscale methods for fourth-order singular perturbations", "abstract": "We develop a numerical homogenization method for fourth-order singular\nperturbation problems within the framework of heterogeneous multiscale method.\nThese problems arise from heterogeneous strain gradient elasticity and\nelasticity models for architectured materials. We establish an error estimate\nfor the homogenized solution applicable to general media and derive an explicit\nconvergence for the locally periodic media with the fine-scale $\\varepsilon$.\nFor cell problems of size $\\delta=\\mathbb{N}\\varepsilon$, the classical\nresonance error $\\mathcal{O}(\\varepsilon/\\delta)$ can be eliminated due to the\ndominance of the higher-order operator. Despite the occurrence of boundary\nlayer effects, discretization errors do not necessarily deteriorate for general\nboundary conditions. Numerical simulations corroborate these theoretical\nfindings.", "published": "2025-04-13 02:46:38", "link": "http://arxiv.org/abs/2504.09410v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Integrated GARCH-GRU in Financial Volatility Forecasting", "abstract": "In this study, we propose a novel integrated Generalized Autoregressive\nConditional Heteroskedasticity-Gated Recurrent Unit (GARCH-GRU) model for\nfinancial volatility modeling and forecasting. The model embeds the GARCH(1,1)\nformulation directly into the GRU cell architecture, yielding a unified\nrecurrent unit that jointly captures both traditional econometric properties\nand complex temporal dynamics. This hybrid structure leverages the strengths of\nGARCH in modeling key stylized facts of financial volatility, such as\nclustering and persistence, while utilizing the GRU's capacity to learn\nnonlinear dependencies from sequential data. Compared to the GARCH-LSTM\ncounterpart, the GARCH-GRU model demonstrates superior computational\nefficiency, requiring significantly less training time, while maintaining and\nimproving forecasting accuracy. Empirical evaluation across multiple financial\ndatasets confirms the model's robust outperformance in terms of mean squared\nerror (MSE) and mean absolute error (MAE) relative to a range of benchmarks,\nincluding standard neural networks, alternative hybrid architectures, and\nclassical GARCH-type models. As an application, we compute Value-at-Risk (VaR)\nusing the model's volatility forecasts and observe lower violation ratios,\nfurther validating the predictive reliability of the proposed framework in\npractical risk management settings.", "published": "2025-04-13 00:04:15", "link": "http://arxiv.org/abs/2504.09380v1", "categories": ["q-fin.ST", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization", "abstract": "In practical instances of nonconvex matrix factorization, the rank of the\ntrue solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be\noverspecified as $r>r^{\\star}$. This over-parameterized regime of matrix\nfactorization significantly slows down the convergence of local search\nalgorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when\n$r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing\nvariant of nonconvex matrix factorization that restores the convergence rate of\ngradient descent back to linear, even in the over-parameterized case, while\nalso making it agnostic to possible ill-conditioning in the ground truth.\nClassical gradient descent in a neighborhood of the solution slows down due to\nthe need for the model matrix factor to become singular. Our key result is that\nthis singularity can be corrected by $\\ell_{2}$ regularization with a specific\nrange of values for the damping parameter. In fact, a good damping parameter\ncan be inexpensively estimated from the current iterate. The resulting\nalgorithm, which we call preconditioned gradient descent or PrecGD, is stable\nunder noise, and converges linearly to an information theoretically optimal\nerror bound. Our numerical experiments find that PrecGD works equally well in\nrestoring the linear convergence of other variants of nonconvex matrix\nfactorization in the over-parameterized regime.", "published": "2025-04-13 20:06:49", "link": "http://arxiv.org/abs/2504.09708v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Modeling Discrete Coating Degradation Events via Hawkes Processes", "abstract": "Forecasting the degradation of coated materials has long been a topic of\ncritical interest in engineering, as it has enormous implications for both\nsystem maintenance and sustainable material use. Material degradation is\naffected by many factors, including the history of corrosion and\ncharacteristics of the environment, which can be measured by high-frequency\nsensors. However, the high volume of data produced by such sensors can inhibit\nefficient modeling and prediction. To alleviate this issue, we propose novel\nmetrics for representing material degradation, taking the form of discrete\ndegradation events. These events maintain the statistical properties of\ncontinuous sensor readings, such as correlation with time to coating failure\nand coefficient of variation at failure, but are composed of orders of\nmagnitude fewer measurements. To forecast future degradation of the coating\nsystem, a marked Hawkes process models the events. We use the forecast of\ndegradation to predict a future time of failure, exhibiting superior\nperformance to the approach based on direct modeling of galvanic corrosion\nusing continuous sensor measurements. While such maintenance is typically done\non a regular basis, degradation models can enable informed condition-based\nmaintenance, reducing unnecessary excess maintenance and preventing unexpected\nfailures.", "published": "2025-04-13 19:57:10", "link": "http://arxiv.org/abs/2504.09706v1", "categories": ["stat.AP", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Ordinary Least Squares as an Attention Mechanism", "abstract": "I show that ordinary least squares (OLS) predictions can be rewritten as the\noutput of a restricted attention module, akin to those forming the backbone of\nlarge language models. This connection offers an alternative perspective on\nattention beyond the conventional information retrieval framework, making it\nmore accessible to researchers and analysts with a background in traditional\nstatistics. It falls into place when OLS is framed as a similarity-based method\nin a transformed regressor space, distinct from the standard view based on\npartial correlations. In fact, the OLS solution can be recast as the outcome of\nan alternative problem: minimizing squared prediction errors by optimizing the\nembedding space in which training and test vectors are compared via inner\nproducts. Rather than estimating coefficients directly, we equivalently learn\noptimal encoding and decoding operations for predictors. From this vantage\npoint, OLS maps naturally onto the query-key-value structure of attention\nmechanisms. Building on this foundation, I discuss key elements of\nTransformer-style attention and draw connections to classic ideas from time\nseries econometrics.", "published": "2025-04-13 17:26:44", "link": "http://arxiv.org/abs/2504.09663v1", "categories": ["cs.LG", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization", "abstract": "Layer-wise post-training quantization has emerged as a widely used technique\nfor compressing large language models (LLMs) without retraining. However,\nrecent progress in this line of research is saturating, underscoring the need\nto revisit its core limitation and explore further improvements. This study\nidentifies a critical bottleneck in existing layer-wise PTQ methods: the\naccumulation of quantization errors across layers significantly degrades\nperformance, particularly in low-bit regimes. To address this, we propose\nQuantization Error Propagation (QEP), a lightweight and general framework that\nenhances layer-wise PTQ by explicitly propagating the quantization error which\nenable compensating for accumulated quantization errors. Additionally, we\nintroduce a tunable propagation mechanism that allows for control over both\npropagation strength and computational overhead, making the framework adaptable\nto various architectures and resource constraints. Empirical evaluation on\nLLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard\nlayer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields\nsubstantial performance improvements under extreme low-bit quantization\nsettings.", "published": "2025-04-13 15:56:00", "link": "http://arxiv.org/abs/2504.09629v1", "categories": ["cs.LG", "stat.AP", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Conditional Independence Test Based on Transport Maps", "abstract": "Testing conditional independence between two random vectors given a third is\na fundamental and challenging problem in statistics, particularly in\nmultivariate nonparametric settings due to the complexity of conditional\nstructures. We propose a novel framework for testing conditional independence\nusing transport maps. At the population level, we show that two well-defined\ntransport maps can transform the conditional independence test into an\nunconditional independence test, this substantially simplifies the problem.\nThese transport maps are estimated from data using conditional continuous\nnormalizing flow models. Within this framework, we derive a test statistic and\nprove its consistency under both the null and alternative hypotheses. A\npermutation-based procedure is employed to evaluate the significance of the\ntest. We validate the proposed method through extensive simulations and\nreal-data analysis. Our numerical studies demonstrate the practical\neffectiveness of the proposed method for conditional independence testing.", "published": "2025-04-13 13:38:25", "link": "http://arxiv.org/abs/2504.09567v1", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G05, 62G08, 68T07"], "primary_category": "stat.ML"}
{"title": "Optimal sparse phase retrieval via a quasi-Bayesian approach", "abstract": "This paper addresses the problem of sparse phase retrieval, a fundamental\ninverse problem in applied mathematics, physics, and engineering, where a\nsignal need to be reconstructed using only the magnitude of its transformation\nwhile phase information remains inaccessible. Leveraging the inherent sparsity\nof many real-world signals, we introduce a novel sparse quasi-Bayesian approach\nand provide the first theoretical guarantees for such an approach.\nSpecifically, we employ a scaled Student distribution as a continuous shrinkage\nprior to enforce sparsity and analyze the method using the PAC-Bayesian\ninequality framework. Our results establish that the proposed Bayesian\nestimator achieves minimax-optimal convergence rates under sub-exponential\nnoise, matching those of state-of-the-art frequentist methods. To ensure\ncomputational feasibility, we develop an efficient Langevin Monte Carlo\nsampling algorithm. Through numerical experiments, we demonstrate that our\nmethod performs comparably to existing frequentist techniques, highlighting its\npotential as a principled alternative for sparse phase retrieval in noisy\nsettings.", "published": "2025-04-13 10:21:35", "link": "http://arxiv.org/abs/2504.09509v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.ML"}
{"title": "AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse", "abstract": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.", "published": "2025-04-13 08:29:58", "link": "http://arxiv.org/abs/2504.10540v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "aweSOM: a CPU/GPU-accelerated Self-organizing Map and Statistically Combined Ensemble Framework for Machine-learning Clustering Analysis", "abstract": "We introduce aweSOM, an open-source Python package for machine learning (ML)\nclustering and classification, using a Self-organizing Maps (SOM) algorithm\nthat incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where\n$N$ is the number of data points), multidimensional datasets. aweSOM consists\nof two main modules, one that handles the initialization and training of the\nSOM, and another that stacks the results of multiple SOM realizations to obtain\nmore statistically robust clusters. Existing Python-based SOM implementations\n(e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily\nserve as proof-of-concept demonstrations, optimized for smaller datasets, but\nlacking scalability for large, multidimensional data. aweSOM provides a\nsolution for this gap in capability, with good performance scaling up to $\\sim\n10^8$ individual points, and capable of utilizing multiple features per point.\nWe compare the code performance against the legacy implementations it is based\non, and find a 10-100x speed up, as well as significantly improved memory\nefficiency, due to several built-in optimizations.", "published": "2025-04-13 06:17:35", "link": "http://arxiv.org/abs/2504.09449v1", "categories": ["cs.LG", "astro-ph.IM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Constants of motion network revisited", "abstract": "Discovering constants of motion is meaningful in helping understand the\ndynamical systems, but inevitably needs proficient mathematical skills and keen\nanalytical capabilities. With the prevalence of deep learning, methods\nemploying neural networks, such as Constant Of Motion nETwork (COMET), are\npromising in handling this scientific problem. Although the COMET method can\nproduce better predictions on dynamics by exploiting the discovered constants\nof motion, there is still plenty of room to sharpen it. In this paper, we\npropose a novel neural network architecture, built using the\nsingular-value-decomposition (SVD) technique, and a two-phase training\nalgorithm to improve the performance of COMET. Extensive experiments show that\nour approach not only retains the advantages of COMET, such as applying to\nnon-Hamiltonian systems and indicating the number of constants of motion, but\nalso can be more lightweight and noise-robust than COMET.", "published": "2025-04-13 04:57:34", "link": "http://arxiv.org/abs/2504.09434v1", "categories": ["cs.LG", "physics.class-ph", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes", "abstract": "This paper proposes a reinforcement learning (RL) framework for insurance\nreserving that integrates tail-risk sensitivity, macroeconomic regime modeling,\nand regulatory compliance. The reserving problem is formulated as a\nfinite-horizon Markov Decision Process (MDP), in which reserve adjustments are\noptimized using Proximal Policy Optimization (PPO) subject to Conditional\nValue-at-Risk (CVaR) constraints. To enhance policy robustness across varying\neconomic conditions, the agent is trained using a regime-aware curriculum that\nprogressively increases volatility exposure.\n  The reward structure penalizes reserve shortfall, capital inefficiency, and\nsolvency floor violations, with design elements informed by Solvency II and Own\nRisk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two\nindustry datasets--Workers' Compensation, and Other Liability--demonstrate that\nthe RL-CVaR agent achieves superior performance relative to classical reserving\nmethods across multiple criteria, including tail-risk control (CVaR$_{0.95}$),\ncapital efficiency, and regulatory violation rate. The framework also\naccommodates fixed-shock stress testing and regime-stratified analysis,\nproviding a principled and extensible approach to reserving under uncertainty.", "published": "2025-04-13 01:43:25", "link": "http://arxiv.org/abs/2504.09396v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FSSUAVL: A Discriminative Framework using Vision Models for Federated Self-Supervised Audio and Image Understanding", "abstract": "Recent studies have demonstrated that vision models can effectively learn\nmultimodal audio-image representations when paired. However, the challenge of\nenabling deep models to learn representations from unpaired modalities remains\nunresolved. This issue is especially pertinent in scenarios like Federated\nLearning (FL), where data is often decentralized, heterogeneous, and lacks a\nreliable guarantee of paired data. Previous attempts tackled this issue through\nthe use of auxiliary pretrained encoders or generative models on local clients,\nwhich invariably raise computational cost with increasing number modalities.\nUnlike these approaches, in this paper, we aim to address the task of unpaired\naudio and image recognition using \\texttt{FSSUAVL}, a single deep model\npretrained in FL with self-supervised contrastive learning (SSL). Instead of\naligning the audio and image modalities, \\texttt{FSSUAVL} jointly discriminates\nthem by projecting them into a common embedding space using contrastive SSL.\nThis extends the utility of \\texttt{FSSUAVL} to paired and unpaired audio and\nimage recognition tasks. Our experiments with CNN and ViT demonstrate that\n\\texttt{FSSUAVL} significantly improves performance across various image- and\naudio-based downstream tasks compared to using separate deep models for each\nmodality. Additionally, \\texttt{FSSUAVL}'s capacity to learn multimodal feature\nrepresentations allows for integrating auxiliary information, if available, to\nenhance recognition accuracy.", "published": "2025-04-13 11:04:43", "link": "http://arxiv.org/abs/2504.09516v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiTSE: High-Fidelity Generative Speech Enhancement via Latent Diffusion Transformers", "abstract": "Real-world speech recordings suffer from degradations such as background\nnoise and reverberation. Speech enhancement aims to mitigate these issues by\ngenerating clean high-fidelity signals. While recent generative approaches for\nspeech enhancement have shown promising results, they still face two major\nchallenges: (1) content hallucination, where plausible phonemes generated\ndiffer from the original utterance; and (2) inconsistency, failing to preserve\nspeaker's identity and paralinguistic features from the input speech. In this\nwork, we introduce DiTSE (Diffusion Transformer for Speech Enhancement), which\naddresses quality issues of degraded speech in full bandwidth. Our approach\nemploys a latent diffusion transformer model together with robust conditioning\nfeatures, effectively addressing these challenges while remaining\ncomputationally efficient. Experimental results from both subjective and\nobjective evaluations demonstrate that DiTSE achieves state-of-the-art audio\nquality that, for the first time, matches real studio-quality audio from the\nDAPS dataset. Furthermore, DiTSE significantly improves the preservation of\nspeaker identity and content fidelity, reducing hallucinations across datasets\ncompared to state-of-the-art enhancers. Audio samples are available at:\nhttp://hguimaraes.me/DiTSE", "published": "2025-04-13 00:04:48", "link": "http://arxiv.org/abs/2504.09381v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Accelerating Ray Tracing-Based Wireless Channels Generation for Real-Time Network Digital Twins", "abstract": "Ray tracing (RT) simulation is a widely used approach to enable modeling\nwireless channels in applications such as network digital twins. However, the\ncomputational cost to execute RT is proportional to factors such as the level\nof detail used in the adopted 3D scenario. This work proposes RT pre-processing\nalgorithms that aim at simplifying the 3D scene without distorting the channel.\nIt also proposes a post-processing method that augments a set of RT results to\nachieve an improved time resolution. These methods enable using RT in\napplications that use a detailed and photorealistic 3D scenario, while\ngenerating consistent wireless channels over time. Our simulation results with\ndifferent 3D scenarios demonstrate that it is possible to reduce the simulation\ntime by more than 50% without compromising the accuracy of the RT parameters.", "published": "2025-04-13 23:02:36", "link": "http://arxiv.org/abs/2504.09751v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Enhanced Filterless Multi-Color VLC via QCT", "abstract": "Color shift keying (CSK) in visible light communication (VLC) often suffers\nfrom filter-induced crosstalk and reduced brightness. This paper proposes using\nquartered composite transform (QCT) with multi-color light-emitting diodes\n(LEDs) to improve both illumination and communication. The proposd DC-biased\nQCT scheme eliminates receiver optical filters, thereby removing crosstalk and\nsignificantly increasing signal-to-noise ratio (SNR). Simulations demonstrate\nQCT maintains high illumination quality (CRI 79.72, CCT 3462 K) while achieving\nover double the average illuminance compared to CSK under the same power\nbudget. QCT also shows better bit error rate (BER) performance in\nlow-to-moderate SNR regimes and has ability to convert multi-tap\nfrequency-selective channel into an equivalent single-tap flat-fading channel\nto mitigate inter-symbol interference (ISI), proving a promising technique for\nbrighter, high-performance, filter-less VLC.", "published": "2025-04-13 22:41:54", "link": "http://arxiv.org/abs/2504.09743v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Quantum Manifold Optimization: A Design Framework for Future Communications Systems", "abstract": "Inspired by recent developments in various areas of science relevant to\nquantum computing, we introduce quantum manifold optimization (QMO) as a\npromising framework for solving constrained optimization problems in\nnext-generation wireless communication systems. We begin by showing how\nclassical wireless design problems - such as pilot design in cell-free\n(CF)-massive MIMO (mMIMO), beamformer optimization in gigantic multiple input\nmultiple output (MIMO), and reconfigurable intelligent surface (RIS) phase\ntuning - naturally reside on structured manifolds like the Stiefel,\nGrassmannian, and oblique manifolds, with the latter novelly formulated in this\nwork. Then, we demonstrate how these problems can be reformulated as\ntrace-based quantum expectation values over variationally-encoded quantum\nstates. While theoretical in scope, the work lays a foundation for a new class\nof quantum optimization algorithms with broad application to the design of\nfuture beyond-sixth-generation (B6G) systems.", "published": "2025-04-13 17:36:21", "link": "http://arxiv.org/abs/2504.09667v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Millimeter-Wave Joint Radar and Communications With an RIS-Integrated Array", "abstract": "In the context of the joint radar and communications (JRC) framework,\nreconfigurable intelligent surfaces (RISs) emerged as a promising technology\nfor their ability to shape the propagation environment by adjusting their\nphase-shift coefficients. However, achieving perfect synchronization and\neffective collaboration between access points (APs) and RISs is crucial to\nsuccessful operation. This paper investigates the performance of a bistatic JRC\nnetwork operating in the millimeter-wave (mmWave) frequency band, where the\nreceiving AP is equipped with an RIS-integrated array. This system\nsimultaneously serves multiple UEs while estimating the position of a target\nwith limited prior knowledge of its position. To achieve this, we optimize both\nthe power allocation of the transmitted waveform and the RIS phase-shift matrix\nto minimize the position error bound (PEB) of the target. At the same time, we\nensure that the UEs achieve an acceptable level of spectral efficiency. The\nnumerical results show that an RIS-integrated array, even with a small number\nof receiving antennas, can achieve high localization accuracy. Additionally,\noptimized phase-shifts significantly improve the localization accuracy in\ncomparison to a random phase-shift configuration.", "published": "2025-04-13 16:20:53", "link": "http://arxiv.org/abs/2504.09636v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Hybrid Transmitting and Reflecting Beyond Diagonal Reconfigurable Intelligent Surface with Independent Beam Control and Power Splitting", "abstract": "A hybrid transmitting and reflecting beyond diagonal reconfigurable\nintelligent surface (BD-RIS) design is proposed. Operating in the same\naperture, frequency band and polarization, the proposed BD-RIS features\nindependent beam steering control of its reflected and transmitted waves. In\naddition it provides a hybrid mode with both reflected and transmitted waves\nusing tunable power splitting between beams. The BD-RIS comprises two phase\nreconfigurable antenna arrays interconnected by an array of tunable two-port\npower splitters. The two-port power splitter in each BD-RIS cell is built upon\na varactor in parallel with a bias inductor to exert tunable impedance\nvariations on transmission lines. Provided with variable reverse DC voltages,\nthe two-port power splitter can control the power ratio of S11 over S21 from\n-20 dB to 20 dB, thus allowing tunable power splitting. Each antenna is 2-bit\nphase reconfigurable with 200 MHz bandwidth at 2.4 GHz so that each cell of\nBD-RIS can also achieve independent reflection and transmission phase control.\nTo characterize and optimize the electromagnetic response of the proposed\nBD-RIS design, a Th\\'evenin equivalent model and corresponding analytical\nmethod is provided. A BD-RIS with 4 by 4 cells was also prototyped and tested.\nExperiments show that in reflection and transmission mode, the fabricated\nBD-RIS can realize beam steering in reflection and transmission space,\nrespectively. It is also verified that when operating in hybrid mode, the\nBD-RIS enables independent beam steering of the reflected and transmitted\nwaves. This work helps fill the gap between realizing practical hardware design\nand establishing an accurate physical model for the hybrid transmitting and\nreflecting BD-RIS, enabling hybrid transmitting and reflecting BD-RIS assisted\nwireless communications.", "published": "2025-04-13 15:22:49", "link": "http://arxiv.org/abs/2504.09618v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "PLS-Assisted Offloading for Edge Computing-Enabled Post-Quantum Security in Resource-Constrained Devices", "abstract": "With the advent of post-quantum cryptography (PQC) standards, it has become\nimperative for resource-constrained devices (RCDs) in the Internet of Things\n(IoT) to adopt these quantum-resistant protocols. However, the high\ncomputational overhead and the large key sizes associated with PQC make direct\ndeployment on such devices impractical. To address this challenge, we propose\nan edge computing-enabled PQC framework that leverages a physical-layer\nsecurity (PLS)-assisted offloading strategy, allowing devices to either offload\nintensive cryptographic tasks to a post-quantum edge server (PQES) or perform\nthem locally. Furthermore, to ensure data confidentiality within the edge\ndomain, our framework integrates two PLS techniques: offloading RCDs employ\nwiretap coding to secure data transmission, while non-offloading RCDs serve as\nfriendly jammers by broadcasting artificial noise to disrupt potential\neavesdroppers. Accordingly, we co-design the computation offloading and PLS\nstrategy by jointly optimizing the device transmit power, PQES computation\nresource allocation, and offloading decisions to minimize overall latency under\nresource constraints. Numerical results demonstrate significant latency\nreductions compared to baseline schemes, confirming the scalability and\nefficiency of our approach for secure PQC operations in IoT networks.", "published": "2025-04-13 05:14:17", "link": "http://arxiv.org/abs/2504.09437v1", "categories": ["cs.CR", "eess.SP"], "primary_category": "cs.CR"}
{"title": "Deep Mismatch Channel Estimation in IRS based 6G Communication", "abstract": "We propose a channel estimation protocol to determine the uplink channel\nstate information (CSI) at the base station for an intelligent reflecting\nsurface (IRS) based wireless communication. More specifically, we develop a\nchannel estimation scheme in a multi-user system with high estimation accuracy\nand low computational complexity. One of the state-of-the-art approaches to\nchannel estimation is the deep learning-based approach. However, the\ndata-driven model often experiences high computational complexity and, thus, is\nslow to channel estimation. Inspired by the success of utilizing domain\nknowledge to build effective data-driven models, the proposed scheme uses the\nhigh channel correlation property to train a shallow deep learning model. More\nspecifically, utilizing the one coherent channel estimation, the model predicts\nthe subsequent channel coherence CSI. We evaluate the performance of the\nproposed scheme in terms of normalized mean square error (NMSE) and spectral\nefficiency (SE) via simulation. The proposed scheme can estimate the CSI with\nreasonable success of lower NMSE, higher SE, and lower estimation time than\nexisting schemes.", "published": "2025-04-13 03:02:45", "link": "http://arxiv.org/abs/2504.09412v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Imaging Transformer for MRI Denoising: a Scalable Model Architecture that enables SNR << 1 Imaging", "abstract": "Purpose: To propose a flexible and scalable imaging transformer (IT)\narchitecture with three attention modules for multi-dimensional imaging data\nand apply it to MRI denoising with very low input SNR.\n  Methods: Three independent attention modules were developed: spatial local,\nspatial global, and frame attentions. They capture long-range signal\ncorrelation and bring back the locality of information in images. An\nattention-cell-block design processes 5D tensors ([B, C, F, H, W]) for 2D,\n2D+T, and 3D image data. A High Resolution (HRNet) backbone was built to hold\nIT blocks. Training dataset consists of 206,677 cine series and test datasets\nhad 7,267 series. Ten input SNR levels from 0.05 to 8.0 were tested. IT models\nwere compared to seven convolutional and transformer baselines. To test\nscalability, four IT models 27m to 218m parameters were trained. Two senior\ncardiologists reviewed IT model outputs from which the EF was measured and\ncompared against the ground-truth.\n  Results: IT models significantly outperformed other models over the tested\nSNR levels. The performance gap was most prominent at low SNR levels. The\nIT-218m model had the highest SSIM and PSNR, restoring good image quality and\nanatomical details even at SNR 0.2. Two experts agreed at this SNR or above,\nthe IT model output gave the same clinical interpretation as the ground-truth.\nThe model produced images that had accurate EF measurements compared to\nground-truth values.\n  Conclusions: Imaging transformer model offers strong performance,\nscalability, and versatility for MR denoising. It recovers image quality\nsuitable for confident clinical reading and accurate EF measurement, even at\nvery low input SNR of 0.2.", "published": "2025-04-13 02:36:15", "link": "http://arxiv.org/abs/2504.10534v1", "categories": ["eess.IV", "eess.SP", "physics.med-ph"], "primary_category": "eess.IV"}
{"title": "Wavefront Estimation From a Single Measurement: Uniqueness and Algorithms", "abstract": "Wavefront estimation is an essential component of adaptive optics where the\ngoal is to recover the underlying phase from its Fourier magnitude. While this\nmay sound identical to classical phase retrieval, wavefront estimation faces\nmore strict requirements regarding uniqueness as adaptive optics systems need a\nunique phase to compensate for the distorted wavefront. Existing real-time\nwavefront estimation methodologies are dominated by sensing via specialized\noptical hardware due to their high speed, but they often have a low spatial\nresolution. A computational method that can perform both fast and accurate\nwavefront estimation with a single measurement can improve resolution and bring\nnew applications such as real-time passive wavefront estimation, opening the\ndoor to a new generation of medical and defense applications.\n  In this paper, we tackle the wavefront estimation problem by observing that\nthe non-uniqueness is related to the geometry of the pupil shape. By analyzing\nthe source of ambiguities and breaking the symmetry, we present a joint\noptics-algorithm approach by co-designing the shape of the pupil and the\nreconstruction neural network. Using our proposed lightweight neural network,\nwe demonstrate wavefront estimation of a phase of size $128\\times 128$ at\n$5,200$ frames per second on a CPU computer, achieving an average Strehl ratio\nup to $0.98$ in the noiseless case. We additionally test our method on real\nmeasurements using a spatial light modulator. Code is available at\nhttps://pages.github.itap.purdue.edu/StanleyChanGroup/wavefront-estimation/.", "published": "2025-04-13 01:38:51", "link": "http://arxiv.org/abs/2504.09395v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences", "abstract": "Due to the convenience of mobile devices, the online games have become an\nimportant part for user entertainments in reality, creating a demand for friend\nrecommendation in online games. However, none of existing approaches can\neffectively incorporate the multi-modal user features (e.g., images and texts)\nwith the structural information in the friendship graph, due to the following\nlimitations: (1) some of them ignore the high-order structural proximity\nbetween users, (2) some fail to learn the pairwise relevance between users at\nmodality-specific level, and (3) some cannot capture both the local and global\nuser preferences on different modalities. By addressing these issues, in this\npaper, we propose an end-to-end model FROG that better models the user\npreferences on potential friends. Comprehensive experiments on both offline\nevaluation and online deployment at Tencent have demonstrated the superiority\nof FROG over existing approaches.", "published": "2025-04-13 04:27:10", "link": "http://arxiv.org/abs/2504.09428v2", "categories": ["cs.SI", "cs.AI", "cs.IR"], "primary_category": "cs.SI"}
{"title": "SegOTA: Accelerating Over-the-Air Federated Learning with Segmented Transmission", "abstract": "Federated learning (FL) with over-the-air computation efficiently utilizes\nthe communication resources, but it can still experience significant latency\nwhen each device transmits a large number of model parameters to the server.\nThis paper proposes the Segmented Over-The-Air (SegOTA) method for FL, which\nreduces latency by partitioning devices into groups and letting each group\ntransmit only one segment of the model parameters in each communication round.\nConsidering a multi-antenna server, we model the SegOTA transmission and\nreception process to establish an upper bound on the expected model learning\noptimality gap. We minimize this upper bound, by formulating the per-round\nonline optimization of device grouping and joint transmit-receive beamforming,\nfor which we derive efficient closed-form solutions. Simulation results show\nthat our proposed SegOTA substantially outperforms the conventional full-model\nOTA approach and other common alternatives.", "published": "2025-04-13 22:44:23", "link": "http://arxiv.org/abs/2504.09745v2", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.", "published": "2025-04-13 14:31:52", "link": "http://arxiv.org/abs/2504.09597v3", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.AI"}
{"title": "Bounds and Optimal Constructions of Generalized Merge-Convertible Codes for Code Conversion into LRCs", "abstract": "Error-correcting codes are essential for ensuring fault tolerance in modern\ndistributed data storage systems. However, in practice, factors such as the\nfailure rates of storage devices can vary significantly over time, resulting in\nchanges to the optimal code parameters. To reduce storage cost while\nmaintaining efficiency, Maturana and Rashmi introduced a theoretical framework\nknown as code conversion, which enables dynamic adjustment of code parameters\naccording to device performance. In this paper, we focus exclusively on the\nbounds and constructions of generalized merge-convertible codes. First, we\nestablish a new lower bound on the access cost when the final code is an\n$(r,\\delta)$-LRC. This bound unifies and generalizes all previously known\nbounds for merge conversion, where the initial and final codes are either LRCs\nor MDS codes. We then construct a family of access-optimal MDS convertible\ncodes by leveraging subgroups of the automorphism group of a rational function\nfield. It is worth noting that our construction is also per-symbol read\naccess-optimal. Next, we further extend our MDS-based construction to design\naccess-optimal convertible codes for the conversion between $(r,\\delta)$-LRCs\nwith parameters that have not been previously reported. Finally, using the\nparity-check matrix approach, we present a construction of access-optimal\nconvertible codes that enable merge conversion from MDS codes to an\n$(r,\\delta)$-LRC. To the best of our knowledge, this is the first explicit\noptimal construction of code conversion between MDS codes and LRCs. All of our\nconstructions are performed over finite fields whose sizes grow linearly with\nthe code length.", "published": "2025-04-13 14:01:10", "link": "http://arxiv.org/abs/2504.09580v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.", "published": "2025-04-13 14:31:52", "link": "http://arxiv.org/abs/2504.09597v4", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.AI"}
