{"title": "A Span-Extraction Dataset for Chinese Machine Reading Comprehension", "abstract": "Machine Reading Comprehension (MRC) has become enormously popular recently\nand has attracted a lot of attention. However, the existing reading\ncomprehension datasets are mostly in English. In this paper, we introduce a\nSpan-Extraction dataset for Chinese machine reading comprehension to add\nlanguage diversities in this area. The dataset is composed by near 20,000 real\nquestions annotated on Wikipedia paragraphs by human experts. We also annotated\na challenge set which contains the questions that need comprehensive\nunderstanding and multi-sentence inference throughout the context. We present\nseveral baseline systems as well as anonymous submissions for demonstrating the\ndifficulties in this dataset. With the release of the dataset, we hosted the\nSecond Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC\n2018). We hope the release of the dataset could further accelerate the Chinese\nmachine reading comprehension research. Resources are available:\nhttps://github.com/ymcui/cmrc2018", "published": "2018-10-17 03:02:26", "link": "http://arxiv.org/abs/1810.07366v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence to Sequence Mixture Model for Diverse Machine Translation", "abstract": "Sequence to sequence (SEQ2SEQ) models often lack diversity in their generated\ntranslations. This can be attributed to the limitation of SEQ2SEQ models in\ncapturing lexical and syntactic variations in a parallel corpus resulting from\ndifferent styles, genres, topics, or ambiguity of the translation process. In\nthis paper, we develop a novel sequence to sequence mixture (S2SMIX) model that\nimproves both translation diversity and quality by adopting a committee of\nspecialized translation models rather than a single translation model. Each\nmixture component selects its own training dataset via optimization of the\nmarginal loglikelihood, which leads to a soft clustering of the parallel\ncorpus. Experiments on four language pairs demonstrate the superiority of our\nmixture model compared to a SEQ2SEQ baseline with standard or diversity-boosted\nbeam search. Our mixture model uses negligible additional parameters and incurs\nno extra computation cost during decoding.", "published": "2018-10-17 05:42:49", "link": "http://arxiv.org/abs/1810.07391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Textual and Speech information in Dialogue Act Classification\n  with Speaker Domain Adaptation", "abstract": "In spite of the recent success of Dialogue Act (DA) classification, the\nmajority of prior works focus on text-based classification with oracle\ntranscriptions, i.e. human transcriptions, instead of Automatic Speech\nRecognition (ASR)'s transcriptions. In spoken dialog systems, however, the\nagent would only have access to noisy ASR transcriptions, which may further\nsuffer performance degradation due to domain shift. In this paper, we explore\nthe effectiveness of using both acoustic and textual signals, either oracle or\nASR transcriptions, and investigate speaker domain adaptation for DA\nclassification. Our multimodal model proves to be superior to the unimodal\nmodels, particularly when the oracle transcriptions are not available. We also\npropose an effective method for speaker domain adaptation, which achieves\ncompetitive results.", "published": "2018-10-17 09:53:03", "link": "http://arxiv.org/abs/1810.07455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Attention Mechanisms: The Case of Word Sense\n  Disambiguation in Neural Machine Translation", "abstract": "Recent work has shown that the encoder-decoder attention mechanisms in neural\nmachine translation (NMT) are different from the word alignment in statistical\nmachine translation. In this paper, we focus on analyzing encoder-decoder\nattention mechanisms, in the case of word sense disambiguation (WSD) in NMT\nmodels. We hypothesize that attention mechanisms pay more attention to context\ntokens when translating ambiguous words. We explore the attention distribution\npatterns when translating ambiguous nouns. Counter-intuitively, we find that\nattention mechanisms are likely to distribute more attention to the ambiguous\nnoun itself rather than context tokens, in comparison to other nouns. We\nconclude that attention mechanism is not the main mechanism used by NMT models\nto incorporate contextual information for WSD. The experimental results suggest\nthat NMT models learn to encode contextual information necessary for WSD in the\nencoder hidden states. For the attention mechanism in Transformer models, we\nreveal that the first few layers gradually learn to \"align\" source and target\ntokens and the last few layers learn to extract features from the related but\nunaligned context tokens.", "published": "2018-10-17 14:58:54", "link": "http://arxiv.org/abs/1810.07595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Railway Accidents' Narratives Using Deep Learning", "abstract": "Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting.", "published": "2018-10-17 04:30:02", "link": "http://arxiv.org/abs/1810.07382v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
