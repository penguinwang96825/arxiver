{"title": "AutoTrans: Automating Transformer Design via Reinforced Architecture\n  Search", "abstract": "Though the transformer architectures have shown dominance in many natural\nlanguage understanding tasks, there are still unsolved issues for the training\nof transformer models, especially the need for a principled way of warm-up\nwhich has shown importance for stable training of a transformer, as well as\nwhether the task at hand prefer to scale the attention product or not. In this\npaper, we empirically explore automating the design choices in the transformer\nmodel, i.e., how to set layer-norm, whether to scale, number of layers, number\nof heads, activation function, etc, so that one can obtain a transformer\narchitecture that better suits the tasks at hand. RL is employed to navigate\nalong search space, and special parameter sharing strategies are designed to\naccelerate the search. It is shown that sampling a proportion of training data\nper epoch during search help to improve the search quality. Experiments on the\nCoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer\nmodel can outperform the standard transformers. In particular, we show that our\nlearned model can be trained more robustly with large learning rates without\nwarm-up.", "published": "2020-09-04 08:46:22", "link": "http://arxiv.org/abs/2009.02070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically inspired morphological inflection with a sequence to\n  sequence model", "abstract": "Inflection is an essential part of every human language's morphology, yet\nlittle effort has been made to unify linguistic theory and computational\nmethods in recent years. Methods of string manipulation are used to infer\ninflectional changes; our research question is whether a neural network would\nbe capable of learning inflectional morphemes for inflection production in a\nsimilar way to a human in early stages of language acquisition. We are using an\ninflectional corpus (Metheniti and Neumann, 2020) and a single layer seq2seq\nmodel to test this hypothesis, in which the inflectional affixes are learned\nand predicted as a block and the word stem is modelled as a character sequence\nto account for infixation. Our character-morpheme-based model creates\ninflection by predicting the stem character-to-character and the inflectional\naffixes as character blocks. We conducted three experiments on creating an\ninflected form of a word given the lemma and a set of input and target\nfeatures, comparing our architecture to a mainstream character-based model with\nthe same hyperparameters, training and test sets. Overall for 17 languages, we\nnoticed small improvements on inflecting known lemmas (+0.68%) but steadily\nbetter performance of our model in predicting inflected forms of unknown words\n(+3.7%) and small improvements on predicting in a low-resource scenario\n(+1.09%)", "published": "2020-09-04 08:58:42", "link": "http://arxiv.org/abs/2009.02073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Analysis of Information Leakage in Deep Transfer\n  Learning", "abstract": "Transfer learning is widely used for transferring knowledge from a source\ndomain to the target domain where the labeled data is scarce. Recently, deep\ntransfer learning has achieved remarkable progress in various applications.\nHowever, the source and target datasets usually belong to two different\norganizations in many real-world scenarios, potential privacy issues in deep\ntransfer learning are posed. In this study, to thoroughly analyze the potential\nprivacy leakage in deep transfer learning, we first divide previous methods\ninto three categories. Based on that, we demonstrate specific threats that lead\nto unintentional privacy leakage in each category. Additionally, we also\nprovide some solutions to prevent these threats. To the best of our knowledge,\nour study is the first to provide a thorough analysis of the information\nleakage issues in deep transfer learning methods and provide potential\nsolutions to the issue. Extensive experiments on two public datasets and an\nindustry dataset are conducted to show the privacy leakage under different deep\ntransfer learning settings and defense solution effectiveness.", "published": "2020-09-04 02:53:20", "link": "http://arxiv.org/abs/2009.01989v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Context-guided Capsule Network for Multimodal Machine\n  Translation", "abstract": "Multimodal machine translation (MMT), which mainly focuses on enhancing\ntext-only translation with visual features, has attracted considerable\nattention from both computer vision and natural language processing\ncommunities. Most current MMT models resort to attention mechanism, global\ncontext modeling or multimodal joint representation learning to utilize visual\nfeatures. However, the attention mechanism lacks sufficient semantic\ninteractions between modalities while the other two provide fixed visual\ncontext, which is unsuitable for modeling the observed variability when\ngenerating translation. To address the above issues, in this paper, we propose\na novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at\neach timestep of decoding, we first employ the conventional source-target\nattention to produce a timestep-specific source-side context vector. Next, DCCN\ntakes this vector as input and uses it to guide the iterative extraction of\nrelated visual features via a context-guided dynamic routing mechanism.\nParticularly, we represent the input image with global and regional visual\nfeatures, we introduce two parallel DCCNs to model multimodal context vectors\nwith visual features at different granularities. Finally, we obtain two\nmultimodal context vectors, which are fused and incorporated into the decoder\nfor the prediction of the target word. Experimental results on the Multi30K\ndataset of English-to-German and English-to-French translation demonstrate the\nsuperiority of DCCN. Our code is available on\nhttps://github.com/DeepLearnXMU/MM-DCCN.", "published": "2020-09-04 06:18:24", "link": "http://arxiv.org/abs/2009.02016v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "What the Future Brings: Investigating the Impact of Lookahead for\n  Incremental Neural TTS", "abstract": "In incremental text to speech synthesis (iTTS), the synthesizer produces an\naudio output before it has access to the entire input sentence. In this paper,\nwe study the behavior of a neural sequence-to-sequence TTS system when used in\nan incremental mode, i.e. when generating speech output for token n, the system\nhas access to n + k tokens from the text sequence. We first analyze the impact\nof this incremental policy on the evolution of the encoder representations of\ntoken n for different values of k (the lookahead parameter). The results show\nthat, on average, tokens travel 88% of the way to their full context\nrepresentation with a one-word lookahead and 94% after 2 words. We then\ninvestigate which text features are the most influential on the evolution\ntowards the final representation using a random forest analysis. The results\nshow that the most salient factors are related to token length. We finally\nevaluate the effects of lookahead k at the decoder level, using a MUSHRA\nlistening test. This test shows results that contrast with the above high\nfigures: speech synthesis quality obtained with 2 word-lookahead is\nsignificantly lower than the one obtained with the full sentence.", "published": "2020-09-04 07:30:57", "link": "http://arxiv.org/abs/2009.02035v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Going Beyond T-SNE: Exposing \\texttt{whatlies} in Text Embeddings", "abstract": "We introduce whatlies, an open source toolkit for visually inspecting word\nand sentence embeddings. The project offers a unified and extensible API with\ncurrent support for a range of popular embedding backends including spaCy,\ntfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The\npackage combines a domain specific language for vector arithmetic with\nvisualisation tools that make exploring word embeddings more intuitive and\nconcise. It offers support for many popular dimensionality reduction techniques\nas well as many interactive visualisations that can either be statically\nexported or shared via Jupyter notebooks. The project documentation is\navailable from https://rasahq.github.io/whatlies/.", "published": "2020-09-04 11:17:46", "link": "http://arxiv.org/abs/2009.02113v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recent Trends in the Use of Deep Learning Models for Grammar Error\n  Handling", "abstract": "Grammar error handling (GEH) is an important topic in natural language\nprocessing (NLP). GEH includes both grammar error detection and grammar error\ncorrection. Recent advances in computation systems have promoted the use of\ndeep learning (DL) models for NLP problems such as GEH. In this survey we focus\non two main DL approaches for GEH: neural machine translation models and editor\nmodels. We describe the three main stages of the pipeline for these models:\ndata preparation, training, and inference. Additionally, we discuss different\ntechniques to improve the performance of these models at each stage of the\npipeline. We compare the performance of different models and conclude with\nproposed future directions.", "published": "2020-09-04 18:50:13", "link": "http://arxiv.org/abs/2009.02358v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hybrid Deep Learning Model for Arabic Text Recognition", "abstract": "Arabic text recognition is a challenging task because of the cursive nature\nof Arabic writing system, its joint writing scheme, the large number of\nligatures and many other challenges. Deep Learning DL models achieved\nsignificant progress in numerous domains including computer vision and sequence\nmodelling. This paper presents a model that can recognize Arabic text that was\nprinted using multiple font types including fonts that mimic Arabic handwritten\nscripts. The proposed model employs a hybrid DL network that can recognize\nArabic printed text without the need for character segmentation. The model was\ntested on a custom dataset comprised of over two million word samples that were\ngenerated using 18 different Arabic font types. The objective of the testing\nprocess was to assess the model capability in recognizing a diverse set of\nArabic fonts representing a varied cursive styles. The model achieved good\nresults in recognizing characters and words and it also achieved promising\nresults in recognizing characters when it was tested on unseen data. The\nprepared model, the custom datasets and the toolkit for generating similar\ndatasets are made publicly available, these tools can be used to prepare models\nfor recognizing other font types as well as to further extend and enhance the\nperformance of the proposed model.", "published": "2020-09-04 02:49:17", "link": "http://arxiv.org/abs/2009.01987v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "abstract": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.", "published": "2020-09-04 15:32:19", "link": "http://arxiv.org/abs/2009.02252v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Video Moment Retrieval via Natural Language Queries", "abstract": "In this paper, we propose a novel method for video moment retrieval (VMR)\nthat achieves state of the arts (SOTA) performance on R@1 metrics and\nsurpassing the SOTA on the high IoU metric (R@1, IoU=0.7).\n  First, we propose to use a multi-head self-attention mechanism, and further a\ncross-attention scheme to capture video/query interaction and long-range query\ndependencies from video context. The attention-based methods can develop\nframe-to-query interaction and query-to-frame interaction at arbitrary\npositions and the multi-head setting ensures the sufficient understanding of\ncomplicated dependencies. Our model has a simple architecture, which enables\nfaster training and inference while maintaining .\n  Second, We also propose to use multiple task training objective consists of\nmoment segmentation task, start/end distribution prediction and start/end\nlocation regression task. We have verified that start/end prediction are noisy\ndue to annotator disagreement and joint training with moment segmentation task\ncan provide richer information since frames inside the target clip are also\nutilized as positive training examples.\n  Third, we propose to use an early fusion approach, which achieves better\nperformance at the cost of inference time. However, the inference time will not\nbe a problem for our model since our model has a simple architecture which\nenables efficient training and inference.", "published": "2020-09-04 22:06:34", "link": "http://arxiv.org/abs/2009.02406v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Data Readiness for Natural Language Processing", "abstract": "This document concerns data readiness in the context of machine learning and\nNatural Language Processing. It describes how an organization may proceed to\nidentify, make available, validate, and prepare data to facilitate automated\nanalysis methods. The contents of the document is based on the practical\nchallenges and frequently asked questions we have encountered in our work as an\napplied research institute with helping organizations and companies, both in\nthe public and private sectors, to use data in their business processes.", "published": "2020-09-04 07:53:43", "link": "http://arxiv.org/abs/2009.02043v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Towards Musically Meaningful Explanations Using Source Separation", "abstract": "Deep neural networks (DNNs) are successfully applied in a wide variety of\nmusic information retrieval (MIR) tasks. Such models are usually considered\n\"black boxes\", meaning that their predictions are not interpretable. Prior work\non explainable models in MIR has generally used image processing tools to\nproduce explanations for DNN predictions, but these are not necessarily\nmusically meaningful, or can be listened to (which, arguably, is important in\nmusic). We propose audioLIME, a method based on Local Interpretable\nModel-agnostic Explanation (LIME), extended by a musical definition of\nlocality. LIME learns locally linear models on perturbations of an example that\nwe want to explain. Instead of extracting components of the spectrogram using\nimage segmentation as part of the LIME pipeline, we propose using source\nseparation. The perturbations are created by switching on/off sources which\nmakes our explanations listenable. We first validate audioLIME on a classifier\nthat was deliberately trained to confuse the true target with a spurious\nsignal, and show that this can easily be detected using our method. We then\nshow that it passes a sanity check that many available explanation methods\nfail. Finally, we demonstrate the general applicability of our (model-agnostic)\nmethod on a third-party music tagger.", "published": "2020-09-04 08:09:03", "link": "http://arxiv.org/abs/2009.02051v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SEANet: A Multi-modal Speech Enhancement Network", "abstract": "We explore the possibility of leveraging accelerometer data to perform speech\nenhancement in very noisy conditions. Although it is possible to only partially\nreconstruct user's speech from the accelerometer, the latter provides a strong\nconditioning signal that is not influenced from noise sources in the\nenvironment. Based on this observation, we feed a multi-modal input to SEANet\n(Sound EnhAncement Network), a wave-to-wave fully convolutional model, which\nadopts a combination of feature losses and adversarial losses to reconstruct an\nenhanced version of user's speech. We trained our model with data collected by\nsensors mounted on an earbud and synthetically corrupted by adding different\nkinds of noise sources to the audio signal. Our experimental results\ndemonstrate that it is possible to achieve very high quality results, even in\nthe case of interfering speech at the same level of loudness. A sample of the\noutput produced by our model is available at\nhttps://google-research.github.io/seanet/multimodal/speech.", "published": "2020-09-04 10:22:43", "link": "http://arxiv.org/abs/2009.02095v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Silent Speech Interfaces for Speech Restoration: A Review", "abstract": "This review summarises the status of silent speech interface (SSI) research.\nSSIs rely on non-acoustic biosignals generated by the human body during speech\nproduction to enable communication whenever normal verbal communication is not\npossible or not desirable. In this review, we focus on the first case and\npresent latest SSI research aimed at providing new alternative and augmentative\ncommunication methods for persons with severe speech disorders. SSIs can employ\na variety of biosignals to enable silent communication, such as\nelectrophysiological recordings of neural activity, electromyographic (EMG)\nrecordings of vocal tract movements or the direct tracking of articulator\nmovements using imaging techniques. Depending on the disorder, some sensing\ntechniques may be better suited than others to capture speech-related\ninformation. For instance, EMG and imaging techniques are well suited for\nlaryngectomised patients, whose vocal tract remains almost intact but are\nunable to speak after the removal of the vocal folds, but fail for severely\nparalysed individuals. From the biosignals, SSIs decode the intended message,\nusing automatic speech recognition or speech synthesis algorithms. Despite\nconsiderable advances in recent years, most present-day SSIs have only been\nvalidated in laboratory settings for healthy users. Thus, as discussed in this\npaper, a number of challenges remain to be addressed in future research before\nSSIs can be promoted to real-world applications. If these issues can be\naddressed successfully, future SSIs will improve the lives of persons with\nsevere speech impairments by restoring their communication capabilities.", "published": "2020-09-04 11:05:50", "link": "http://arxiv.org/abs/2009.02110v3", "categories": ["eess.AS", "cs.HC", "cs.LG"], "primary_category": "eess.AS"}
