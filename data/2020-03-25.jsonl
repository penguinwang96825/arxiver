{"title": "Learning Syntactic and Dynamic Selective Encoding for Document\n  Summarization", "abstract": "Text summarization aims to generate a headline or a short summary consisting\nof the major information of the source text. Recent studies employ the\nsequence-to-sequence framework to encode the input with a neural network and\ngenerate abstractive summary. However, most studies feed the encoder with the\nsemantic word embedding but ignore the syntactic information of the text.\nFurther, although previous studies proposed the selective gate to control the\ninformation flow from the encoder to the decoder, it is static during the\ndecoding and cannot differentiate the information based on the decoder states.\nIn this paper, we propose a novel neural architecture for document\nsummarization. Our approach has the following contributions: first, we\nincorporate syntactic information such as constituency parsing trees into the\nencoding sequence to learn both the semantic and syntactic information from the\ndocument, resulting in more accurate summary; second, we propose a dynamic gate\nnetwork to select the salient information based on the context of the decoder\nstate, which is essential to document summarization. The proposed model has\nbeen evaluated on CNN/Daily Mail summarization datasets and the experimental\nresults show that the proposed approach outperforms baseline approaches.", "published": "2020-03-25 01:29:38", "link": "http://arxiv.org/abs/2003.11173v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Multi-Binary Neural Network for Multi-class Classification", "abstract": "Multi-class text classification is one of the key problems in machine\nlearning and natural language processing. Emerging neural networks deal with\nthe problem using a multi-output softmax layer and achieve substantial\nprogress, but they do not explicitly learn the correlation among classes. In\nthis paper, we use a multi-task framework to address multi-class\nclassification, where a multi-class classifier and multiple binary classifiers\nare trained together. Moreover, we employ adversarial training to distinguish\nthe class-specific features and the class-agnostic features. The model benefits\nfrom better feature representation. We conduct experiments on two large-scale\nmulti-class text classification tasks and demonstrate that the proposed\narchitecture outperforms baseline approaches.", "published": "2020-03-25 02:19:17", "link": "http://arxiv.org/abs/2003.11184v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heavy-tailed Representations, Text Polarity Classification & Data\n  Augmentation", "abstract": "The dominant approaches to text representation in natural language rely on\nlearning embeddings on massive corpora which have convenient properties such as\ncompositionality and distance preservation. In this paper, we develop a novel\nmethod to learn a heavy-tailed embedding with desirable regularity properties\nregarding the distributional tails, which allows to analyze the points far away\nfrom the distribution bulk using the framework of multivariate extreme value\ntheory. In particular, a classifier dedicated to the tails of the proposed\nembedding is obtained which performance outperforms the baseline. This\nclassifier exhibits a scale invariance property which we leverage by\nintroducing a novel text generation method for label preserving dataset\naugmentation. Numerical experiments on synthetic and real text data demonstrate\nthe relevance of the proposed framework and confirm that this method generates\nmeaningful sentences with controllable attribute, e.g. positive or negative\nsentiment.", "published": "2020-03-25 19:24:05", "link": "http://arxiv.org/abs/2003.11593v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "VIOLIN: A Large-Scale Dataset for Video-and-Language Inference", "abstract": "We introduce a new task, Video-and-Language Inference, for joint multimodal\nunderstanding of video and text. Given a video clip with aligned subtitles as\npremise, paired with a natural language hypothesis based on the video content,\na model needs to infer whether the hypothesis is entailed or contradicted by\nthe given video clip. A new large-scale dataset, named Violin\n(VIdeO-and-Language INference), is introduced for this task, which consists of\n95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours\nof video. These video clips contain rich content with diverse temporal\ndynamics, event shifts, and people interactions, collected from two sources:\n(i) popular TV shows, and (ii) movie clips from YouTube channels. In order to\naddress our new multimodal inference task, a model is required to possess\nsophisticated reasoning skills, from surface-level grounding (e.g., identifying\nobjects and characters in the video) to in-depth commonsense reasoning (e.g.,\ninferring causal relations of events in the video). We present a detailed\nanalysis of the dataset and an extensive evaluation over many strong baselines,\nproviding valuable insights on the challenges of this new task.", "published": "2020-03-25 20:39:05", "link": "http://arxiv.org/abs/2003.11618v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "End-to-End Entity Classification on Multimodal Knowledge Graphs", "abstract": "End-to-end multimodal learning on knowledge graphs has been left largely\nunaddressed. Instead, most end-to-end models such as message passing networks\nlearn solely from the relational information encoded in graphs' structure: raw\nvalues, or literals, are either omitted completely or are stripped from their\nvalues and treated as regular nodes. In either case we lose potentially\nrelevant information which could have otherwise been exploited by our learning\nmethods. To avoid this, we must treat literals and non-literals as separate\ncases. We must also address each modality separately and accordingly: numbers,\ntexts, images, geometries, et cetera. We propose a multimodal message passing\nnetwork which not only learns end-to-end from the structure of graphs, but also\nfrom their possibly divers set of multimodal node features. Our model uses\ndedicated (neural) encoders to naturally learn embeddings for node features\nbelonging to five different types of modalities, including images and\ngeometries, which are projected into a joint representation space together with\ntheir relational information. We demonstrate our model on a node classification\ntask, and evaluate the effect that each modality has on the overall\nperformance. Our result supports our hypothesis that including information from\nmultiple modalities can help our models obtain a better overall performance.", "published": "2020-03-25 14:57:52", "link": "http://arxiv.org/abs/2003.12383v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
