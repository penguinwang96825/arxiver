{"title": "Robust Transfer Learning with Pretrained Language Models through\n  Adapters", "abstract": "Transfer learning with large pretrained transformer-based language models\nlike BERT has become a dominating approach for most NLP tasks. Simply\nfine-tuning those large language models on downstream tasks or combining it\nwith task-specific pretraining is often not robust. In particular, the\nperformance considerably varies as the random seed changes or the number of\npretraining and/or fine-tuning iterations varies, and the fine-tuned model is\nvulnerable to adversarial attack. We propose a simple yet effective\nadapter-based approach to mitigate these issues. Specifically, we insert small\nbottleneck layers (i.e., adapter) within each layer of a pretrained model, then\nfix the pretrained layers and train the adapter layers on the downstream task\ndata, with (1) task-specific unsupervised pretraining and then (2)\ntask-specific supervised training (e.g., classification, sequence labeling).\nOur experiments demonstrate that such a training scheme leads to improved\nstability and adversarial robustness in transfer learning to various downstream\ntasks.", "published": "2021-08-05 02:30:13", "link": "http://arxiv.org/abs/2108.02340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Out-of-Distribution Generalization in Text Classifiers Trained\n  on Tobacco-3482 and RVL-CDIP", "abstract": "To be robust enough for widespread adoption, document analysis systems\ninvolving machine learning models must be able to respond correctly to inputs\nthat fall outside of the data distribution that was used to generate the data\non which the models were trained. This paper explores the ability of text\nclassifiers trained on standard document classification datasets to generalize\nto out-of-distribution documents at inference time. We take the Tobacco-3482\nand RVL-CDIP datasets as a starting point and generate new out-of-distribution\nevaluation datasets in order to analyze the generalization performance of\nmodels trained on these standard datasets. We find that models trained on the\nsmaller Tobacco-3482 dataset perform poorly on our new out-of-distribution\ndata, while text classification models trained on the larger RVL-CDIP exhibit\nsmaller performance drops.", "published": "2021-08-05 15:34:56", "link": "http://arxiv.org/abs/2108.02684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupled Transformer for Scalable Inference in Open-domain Question\n  Answering", "abstract": "Large transformer models, such as BERT, achieve state-of-the-art results in\nmachine reading comprehension (MRC) for open-domain question answering (QA).\nHowever, transformers have a high computational cost for inference which makes\nthem hard to apply to online QA systems for applications like voice assistants.\nTo reduce computational cost and latency, we propose decoupling the transformer\nMRC model into input-component and cross-component. The decoupling allows for\npart of the representation computation to be performed offline and cached for\nonline use. To retain the decoupled transformer accuracy, we devised a\nknowledge distillation objective from a standard transformer model. Moreover,\nwe introduce learned representation compression layers which help reduce by\nfour times the storage requirement for the cache. In experiments on the SQUAD\n2.0 dataset, a decoupled transformer reduces the computational cost and latency\nof open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a\nstandard transformer.", "published": "2021-08-05 17:53:40", "link": "http://arxiv.org/abs/2108.02765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Tradeoff Between Abstractiveness and Factuality in\n  Abstractive Summarization", "abstract": "Neural models for abstractive summarization tend to generate output that is\nfluent and well-formed but lacks semantic faithfulness, or factuality, with\nrespect to the input documents. In this paper, we analyze the tradeoff between\nabstractiveness and factuality of generated summaries across multiple datasets\nand models, using extensive human evaluations of factuality. In our analysis,\nwe visualize the rates of change in factuality as we gradually increase\nabstractiveness using a decoding constraint, and we observe that, while\nincreased abstractiveness generally leads to a drop in factuality, the rate of\nfactuality decay depends on factors such as the data that the system was\ntrained on. We introduce two datasets with human factuality judgements; one\ncontaining 10.2k generated summaries with systematically varied degrees of\nabstractiveness; the other containing 4.2k summaries from five different\nsummarization models. We propose new factuality metrics that adjust for the\ndegree of abstractiveness, and we use them to compare the\nabstractiveness-adjusted factuality of previous summarization works, providing\nbaselines for future work.", "published": "2021-08-05 21:28:20", "link": "http://arxiv.org/abs/2108.02859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open\n  Domain Question Answering", "abstract": "The current state-of-the-art generative models for open-domain question\nanswering (ODQA) have focused on generating direct answers from unstructured\ntextual information. However, a large amount of world's knowledge is stored in\nstructured databases, and need to be accessed using query languages such as\nSQL. Furthermore, query languages can answer questions that require complex\nreasoning, as well as offering full explainability. In this paper, we propose a\nhybrid framework that takes both textual and tabular evidence as input and\ngenerates either direct answers or SQL queries depending on which form could\nbetter answer the question. The generated SQL queries can then be executed on\nthe associated databases to obtain the final answers. To the best of our\nknowledge, this is the first paper that applies Text2SQL to ODQA tasks.\nEmpirically, we demonstrate that on several ODQA datasets, the hybrid methods\nconsistently outperforms the baseline models that only take homogeneous input\nby a large margin. Specifically we achieve state-of-the-art performance on\nOpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate\nthat the being able to generate structural SQL queries can always bring gains,\nespecially for those questions that requires complex reasoning.", "published": "2021-08-05 22:04:13", "link": "http://arxiv.org/abs/2108.02866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolution of emotion semantics", "abstract": "Humans possess the unique ability to communicate emotions through language.\nAlthough concepts like anger or awe are abstract, there is a shared consensus\nabout what these English emotion words mean. This consensus may give the\nimpression that their meaning is static, but we propose this is not the case.\nWe cannot travel back to earlier periods to study emotion concepts directly,\nbut we can examine text corpora, which have partially preserved the meaning of\nemotion words. Using natural language processing of historical text, we found\nevidence for semantic change in emotion words over the past century and that\nvarying rates of change were predicted in part by an emotion concept's\nprototypicality - how representative it is of the broader category of\n\"emotion\". Prototypicality negatively correlated with historical rates of\nemotion semantic change obtained from text-based word embeddings, beyond more\nestablished variables including usage frequency in English and a second\ncomparison language, French. This effect for prototypicality did not\nconsistently extend to the semantic category of birds, suggesting its relevance\nfor predicting semantic change may be category-dependent. Our results suggest\nemotion semantics are evolving over time, with prototypical emotion words\nremaining semantically stable, while other emotion words evolve more freely.", "published": "2021-08-05 23:46:22", "link": "http://arxiv.org/abs/2108.02887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated\n  Recurrent Memory Network", "abstract": "Aspect-level sentiment classification (ASC) aims to predict the fine-grained\nsentiment polarity towards a given aspect mentioned in a review. Despite recent\nadvances in ASC, enabling machines to preciously infer aspect sentiments is\nstill challenging. This paper tackles two challenges in ASC: (1) due to lack of\naspect knowledge, aspect representation derived in prior works is inadequate to\nrepresent aspect's exact meaning and property information; (2) prior works only\ncapture either local syntactic information or global relational information,\nthus missing either one of them leads to insufficient syntactic information. To\ntackle these challenges, we propose a novel ASC model which not only end-to-end\nembeds and leverages aspect knowledge but also marries the two kinds of\nsyntactic information and lets them compensate for each other. Our model\nincludes three key components: (1) a knowledge-aware gated recurrent memory\nnetwork recurrently integrates dynamically summarized aspect knowledge; (2) a\ndual syntax graph network combines both kinds of syntactic information to\ncomprehensively capture sufficient syntactic information; (3) a knowledge\nintegrating gate re-enhances the final representation with further needed\naspect knowledge; (4) an aspect-to-context attention mechanism aggregates the\naspect-related semantics from all hidden states into the final representation.\nExperimental results on several benchmark datasets demonstrate the\neffectiveness of our model, which overpass previous state-of-the-art models by\nlarge margins in terms of both Accuracy and Macro-F1.", "published": "2021-08-05 03:39:30", "link": "http://arxiv.org/abs/2108.02352v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable\n  Video Captioning", "abstract": "Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.", "published": "2021-08-05 04:17:20", "link": "http://arxiv.org/abs/2108.02359v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hybrid Reasoning Network for Video-based Commonsense Captioning", "abstract": "The task of video-based commonsense captioning aims to generate event-wise\ncaptions and meanwhile provide multiple commonsense descriptions (e.g.,\nattribute, effect and intention) about the underlying event in the video. Prior\nworks explore the commonsense captions by using separate networks for different\ncommonsense types, which is time-consuming and lacks mining the interaction of\ndifferent commonsense. In this paper, we propose a Hybrid Reasoning Network\n(HybridNet) to endow the neural networks with the capability of semantic-level\nreasoning and word-level reasoning. Firstly, we develop multi-commonsense\nlearning for semantic-level reasoning by jointly training different commonsense\ntypes in a unified network, which encourages the interaction between the clues\nof multiple commonsense descriptions, event-wise captions and videos. Then,\nthere are two steps to achieve the word-level reasoning: (1) a memory module\nrecords the history predicted sequence from the previous generation processes;\n(2) a memory-routed multi-head attention (MMHA) module updates the word-level\nattention maps by incorporating the history information from the memory module\ninto the transformer decoder for word-level reasoning. Moreover, the multimodal\nfeatures are used to make full use of diverse knowledge for commonsense\nreasoning. Experiments and abundant analysis on the large-scale\nVideo-to-Commonsense benchmark show that our HybridNet achieves\nstate-of-the-art performance compared with other methods.", "published": "2021-08-05 04:55:51", "link": "http://arxiv.org/abs/2108.02365v1", "categories": ["cs.CV", "cs.CL", "68T07"], "primary_category": "cs.CV"}
{"title": "WeChat Neural Machine Translation Systems for WMT21", "abstract": "This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English->Chinese, English->Japanese, Japanese->English and\nEnglish->German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English->Chinese, English->Japanese,\nJapanese->English and English->German, respectively. The BLEU scores of\nEnglish->Chinese, English->Japanese and Japanese->English are the highest among\nall submissions, and that of English->German is the highest among all\nconstrained submissions.", "published": "2021-08-05 06:38:48", "link": "http://arxiv.org/abs/2108.02401v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finetuning Pretrained Transformers into Variational Autoencoders", "abstract": "Text variational autoencoders (VAEs) are notorious for posterior collapse, a\nphenomenon where the model's decoder learns to ignore signals from the encoder.\nBecause posterior collapse is known to be exacerbated by expressive decoders,\nTransformers have seen limited adoption as components of text VAEs. Existing\nstudies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et\nal., 2021) mitigate posterior collapse using massive pretraining, a technique\nunavailable to most of the research community without extensive computing\nresources. We present a simple two-phase training scheme to convert a\nsequence-to-sequence Transformer into a VAE with just finetuning. The resulting\nlanguage model is competitive with massively pretrained Transformer-based VAEs\nin some internal metrics while falling short on others. To facilitate training\nwe comprehensively explore the impact of common posterior collapse alleviation\ntechniques in the literature. We release our code for reproducability.", "published": "2021-08-05 08:27:26", "link": "http://arxiv.org/abs/2108.02446v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bambara Language Dataset for Sentiment Analysis", "abstract": "For easier communication, posting, or commenting on each others posts, people\nuse their dialects. In Africa, various languages and dialects exist. However,\nthey are still underrepresented and not fully exploited for analytical studies\nand research purposes. In order to perform approaches like Machine Learning and\nDeep Learning, datasets are required. One of the African languages is Bambara,\nused by citizens in different countries. However, no previous work on datasets\nfor this language was performed for Sentiment Analysis. In this paper, we\npresent the first common-crawl-based Bambara dialectal dataset dedicated for\nSentiment Analysis, available freely for Natural Language Processing research\npurposes.", "published": "2021-08-05 11:07:18", "link": "http://arxiv.org/abs/2108.02524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation from BERT Transformer to Speech Transformer for\n  Intent Classification", "abstract": "End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules. It attempts\nto predict intent from speech without using an intermediate ASR module.\nHowever, such end-to-end framework suffers from the unavailability of large\nspeech resources with higher acoustic variation in spoken language\nunderstanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech model. In\nthis regard, we leverage the reliable and widely used bidirectional encoder\nrepresentations from transformers (BERT) model as a language model and transfer\nthe knowledge to build an acoustic model for intent classification using the\nspeech. In particular, a multilevel transformer based teacher-student model is\ndesigned, and knowledge distillation is performed across attention and hidden\nsub-layers of different transformer layers of the student and teacher models.\nWe achieve an intent classification accuracy of 99.10% and 88.79% for Fluent\nspeech corpus and ATIS database, respectively. Further, the proposed method\ndemonstrates better performance and robustness in acoustically degraded\ncondition compared to the baseline method.", "published": "2021-08-05 13:08:13", "link": "http://arxiv.org/abs/2108.02598v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VisualTextRank: Unsupervised Graph-based Content Extraction for\n  Automating Ad Text to Image Search", "abstract": "Numerous online stock image libraries offer high quality yet copyright free\nimages for use in marketing campaigns. To assist advertisers in navigating such\nthird party libraries, we study the problem of automatically fetching relevant\nad images given the ad text (via a short textual query for images). Motivated\nby our observations in logged data on ad image search queries (given ad text),\nwe formulate a keyword extraction problem, where a keyword extracted from the\nad text (or its augmented version) serves as the ad image query. In this\ncontext, we propose VisualTextRank: an unsupervised method to (i) augment input\nad text using semantically similar ads, and (ii) extract the image query from\nthe augmented ad text. VisualTextRank builds on prior work on graph based\ncontext extraction (biased TextRank in particular) by leveraging both the text\nand image of similar ads for better keyword extraction, and using advertiser\ncategory specific biasing with sentence-BERT embeddings. Using data collected\nfrom the Verizon Media Native (Yahoo Gemini) ad platform's stock image search\nfeature for onboarding advertisers, we demonstrate the superiority of\nVisualTextRank compared to competitive keyword extraction baselines (including\nan $11\\%$ accuracy lift over biased TextRank). For the case when the stock\nimage library is restricted to English queries, we show the effectiveness of\nVisualTextRank on multilingual ads (translated to English) while leveraging\nsemantically similar English ads. Online tests with a simplified version of\nVisualTextRank led to a 28.7% increase in the usage of stock image search, and\na 41.6% increase in the advertiser onboarding rate in the Verizon Media Native\nad platform.", "published": "2021-08-05 16:47:21", "link": "http://arxiv.org/abs/2108.02725v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "EENLP: Cross-lingual Eastern European NLP Index", "abstract": "Motivated by the sparsity of NLP resources for Eastern European languages, we\npresent a broad index of existing Eastern European language resources (90+\ndatasets and 45+ models) published as a github repository open for updates from\nthe community. Furthermore, to support the evaluation of commonsense reasoning\ntasks, we provide hand-crafted cross-lingual datasets for five different\nsemantic tasks (namely news categorization, paraphrase detection, Natural\nLanguage Inference (NLI) task, tweet sentiment detection, and news sentiment\ndetection) for some of the Eastern European languages. We perform several\nexperiments with the existing multilingual models on these datasets to define\nthe performance baselines and compare them to the existing results for other\nlanguages.", "published": "2021-08-05 13:24:30", "link": "http://arxiv.org/abs/2108.02605v3", "categories": ["cs.CL", "cs.AI", "cs.NE", "68T50"], "primary_category": "cs.CL"}
{"title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics\n  Transcription", "abstract": "This paper makes several contributions to automatic lyrics transcription\n(ALT) research. Our main contribution is a novel variant of the Multistreaming\nTime-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which\nprocesses the temporal information using multiple streams in parallel with\nvarying resolutions keeping the network more compact, and thus with a faster\ninference and an improved recognition rate than having identical TDNN streams.\nIn addition, two novel preprocessing steps prior to training the acoustic model\nare proposed. First, we suggest using recordings from both monophonic and\npolyphonic domains during training the acoustic model. Second, we tag\nmonophonic and polyphonic recordings with distinct labels for discriminating\nnon-vocal silence and music instances during alignment. Moreover, we present a\nnew test set with a considerably larger size and a higher musical variability\ncompared to the existing datasets used in ALT literature, while maintaining the\ngender balance of the singers. Our best performing model sets the\nstate-of-the-art in lyrics transcription by a large margin. For\nreproducibility, we publicly share the identifiers to retrieve the data used in\nthis paper.", "published": "2021-08-05 13:59:11", "link": "http://arxiv.org/abs/2108.02625v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System", "abstract": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.", "published": "2021-08-05 17:59:58", "link": "http://arxiv.org/abs/2108.02776v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hate Speech Detection in Roman Urdu", "abstract": "Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.", "published": "2021-08-05 19:49:46", "link": "http://arxiv.org/abs/2108.02830v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GENder-IT: An Annotated English-Italian Parallel Challenge Set for\n  Cross-Linguistic Natural Gender Phenomena", "abstract": "Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.", "published": "2021-08-05 21:08:45", "link": "http://arxiv.org/abs/2108.02854v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "An Encoder-Decoder Based Audio Captioning System With Transfer and\n  Reinforcement Learning", "abstract": "Automated audio captioning aims to use natural language to describe the\ncontent of audio data. This paper presents an audio captioning system with an\nencoder-decoder architecture, where the decoder predicts words based on audio\nfeatures extracted by the encoder. To improve the proposed system, transfer\nlearning from either an upstream audio-related task or a large in-domain\ndataset is introduced to mitigate the problem induced by data scarcity.\nBesides, evaluation metrics are incorporated into the optimization of the model\nwith reinforcement learning, which helps address the problem of ``exposure\nbias'' induced by ``teacher forcing'' training strategy and the mismatch\nbetween the evaluation metrics and the loss function. The resulting system was\nranked 3rd in DCASE 2021 Task 6. Ablation studies are carried out to\ninvestigate how much each element in the proposed system can contribute to\nfinal performance. The results show that the proposed techniques significantly\nimprove the scores of the evaluation metrics, however, reinforcement learning\nmay impact adversely on the quality of the generated captions.", "published": "2021-08-05 17:34:32", "link": "http://arxiv.org/abs/2108.02752v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Applying the Information Bottleneck Principle to Prosodic Representation\n  Learning", "abstract": "This paper describes a novel design of a neural network-based speech\ngeneration model for learning prosodic representation.The problem of\nrepresentation learning is formulated according to the information bottleneck\n(IB) principle. A modified VQ-VAE quantized layer is incorporated in the speech\ngeneration model to control the IB capacity and adjust the balance between\nreconstruction power and disentangle capability of the learned representation.\nThe proposed model is able to learn word-level prosodic representations from\nspeech data. With an optimized IB capacity, the learned representations not\nonly are adequate to reconstruct the original speech but also can be used to\ntransfer the prosody onto different textual content. Extensive results of the\nobjective and subjective evaluation are presented to demonstrate the effect of\nIB capacity control, the effectiveness, and potential usage of the learned\nprosodic representation in controllable neural speech generation.", "published": "2021-08-05 19:20:59", "link": "http://arxiv.org/abs/2108.02821v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spotify Danceability and Popularity Analysis using SAP", "abstract": "Our analysis reviews and visualizes the audio features and popularity of\nsongs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally\nsourced from Spotify API, consists of multiple Excel files containing\ninformation relevant to our visualization and regression analysis. The exercise\nseeks to determine the connection between the popularity of the songs and the\ndanceability. Insights to be included and factored as part of our analysis\ninclude song energy, valence, BPM, release date, and year.", "published": "2021-08-05 05:02:24", "link": "http://arxiv.org/abs/2108.02370v1", "categories": ["cs.DC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.DC"}
{"title": "Improved Speech Emotion Recognition using Transfer Learning and\n  Spectrogram Augmentation", "abstract": "Automatic speech emotion recognition (SER) is a challenging task that plays a\ncrucial role in natural human-computer interaction. One of the main challenges\nin SER is data scarcity, i.e., insufficient amounts of carefully labeled data\nto build and fully explore complex deep learning models for emotion\nclassification. This paper aims to address this challenge using a transfer\nlearning strategy combined with spectrogram augmentation. Specifically, we\npropose a transfer learning approach that leverages a pre-trained residual\nnetwork (ResNet) model including a statistics pooling layer from speaker\nrecognition trained using large amounts of speaker-labeled data. The statistics\npooling layer enables the model to efficiently process variable-length input,\nthereby eliminating the need for sequence truncation which is commonly used in\nSER systems. In addition, we adopt a spectrogram augmentation technique to\ngenerate additional training data samples by applying random time-frequency\nmasks to log-mel spectrograms to mitigate overfitting and improve the\ngeneralization of emotion recognition models. We evaluate the effectiveness of\nour proposed approach on the interactive emotional dyadic motion capture\n(IEMOCAP) dataset. Experimental results indicate that the transfer learning and\nspectrogram augmentation approaches improve the SER performance, and when\ncombined achieve state-of-the-art results.", "published": "2021-08-05 10:39:39", "link": "http://arxiv.org/abs/2108.02510v4", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SLoClas: A Database for Joint Sound Localization and Classification", "abstract": "In this work, we present the development of a new database, namely Sound\nLocalization and Classification (SLoClas) corpus, for studying and analyzing\nsound localization and classification. The corpus contains a total of 23.27\nhours of data recorded using a 4-channel microphone array. 10 classes of sounds\nare played over a loudspeaker at 1.5 meters distance from the array by varying\nthe Direction-of-Arrival (DoA) from 1 degree to 360 degree at an interval of 5\ndegree. To facilitate the study of noise robustness, 6 types of outdoor noise\nare recorded at 4 DoAs, using the same devices. Moreover, we propose a baseline\nmethod, namely Sound Localization and Classification Network (SLCnet) and\npresent the experimental results and analysis conducted on the collected\nSLoClas database. We achieve the accuracy of 95.21% and 80.01% for sound\nlocalization and classification, respectively. We publicly release this\ndatabase and the source code for research purpose.", "published": "2021-08-05 11:49:59", "link": "http://arxiv.org/abs/2108.02539v1", "categories": ["cs.SD", "cs.DB", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Performer Identification From Symbolic Representation of Music Using\n  Statistical Models", "abstract": "Music Performers have their own idiosyncratic way of interpreting a musical\npiece. A group of skilled performers playing the same piece of music would\nlikely to inject their unique artistic styles in their performances. The\nvariations of the tempo, timing, dynamics, articulation etc. from the actual\nnotated music are what make the performers unique in their performances. This\nstudy presents a dataset consisting of four movements of Schubert's ``Sonata in\nB-flat major, D.960\" performed by nine virtuoso pianists individually. We\nproposed and extracted a set of expressive features that are able to capture\nthe characteristics of an individual performer's style. We then present a\nperformer identification method based on the similarity of feature\ndistribution, given a set of piano performances. The identification is done\nconsidering each feature individually as well as a fusion of the features.\nResults show that the proposed method achieved a precision of 0.903 using\nfusion features. Moreover, the onset time deviation feature shows promising\nresult when considered individually.", "published": "2021-08-05 12:32:54", "link": "http://arxiv.org/abs/2108.02576v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pattern Recognition in Vital Signs Using Spectrograms", "abstract": "Spectrograms visualize the frequency components of a given signal which may\nbe an audio signal or even a time-series signal. Audio signals have higher\nsampling rate and high variability of frequency with time. Spectrograms can\ncapture such variations well. But, vital signs which are time-series signals\nhave less sampling frequency and low-frequency variability due to which,\nspectrograms fail to express variations and patterns. In this paper, we propose\na novel solution to introduce frequency variability using frequency modulation\non vital signs. Then we apply spectrograms on frequency modulated signals to\ncapture the patterns. The proposed approach has been evaluated on 4 different\nmedical datasets across both prediction and classification tasks. Significant\nresults are found showing the efficacy of the approach for vital sign signals.\nThe results from the proposed approach are promising with an accuracy of 91.55%\nand 91.67% in prediction and classification tasks respectively.", "published": "2021-08-05 01:37:45", "link": "http://arxiv.org/abs/2108.03168v2", "categories": ["eess.SP", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "UniCon: Unified Context Network for Robust Active Speaker Detection", "abstract": "We introduce a new efficient framework, the Unified Context Network (UniCon),\nfor robust active speaker detection (ASD). Traditional methods for ASD usually\noperate on each candidate's pre-cropped face track separately and do not\nsufficiently consider the relationships among the candidates. This potentially\nlimits performance, especially in challenging scenarios with low-resolution\nfaces, multiple candidates, etc. Our solution is a novel, unified framework\nthat focuses on jointly modeling multiple types of contextual information:\nspatial context to indicate the position and scale of each candidate's face,\nrelational context to capture the visual relationships among the candidates and\ncontrast audio-visual affinities with each other, and temporal context to\naggregate long-term information and smooth out local uncertainties. Based on\nsuch information, our model optimizes all candidates in a unified process for\nrobust and reliable ASD. A thorough ablation study is performed on several\nchallenging ASD benchmarks under different settings. In particular, our method\noutperforms the state-of-the-art by a large margin of about 15% mean Average\nPrecision (mAP) absolute on two challenging subsets: one with three candidate\nspeakers, and the other with faces smaller than 64 pixels. Together, our UniCon\nachieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for\nthe first time on this challenging dataset at the time of submission. Project\nwebsite: https://unicon-asd.github.io/.", "published": "2021-08-05 13:25:44", "link": "http://arxiv.org/abs/2108.02607v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
