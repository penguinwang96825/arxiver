{"title": "Linguistic Analysis of Pretrained Sentence Encoders with Acceptability\n  Judgments", "abstract": "Recent work on evaluating grammatical knowledge in pretrained sentence\nencoders gives a fine-grained view of a small number of phenomena. We introduce\na new analysis dataset that also has broad coverage of linguistic phenomena. We\nannotate the development set of the Corpus of Linguistic Acceptability (CoLA;\nWarstadt et al., 2018) for the presence of 13 classes of syntactic phenomena\nincluding various forms of argument alternations, movement, and modification.\nWe use this analysis set to investigate the grammatical knowledge of three\npretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018),\nand the BiLSTM baseline from Warstadt et al. We find that these models have a\nstrong command of complex or non-canonical argument structures like\nditransitives (Sue gave Dan a book) and passives (The book was read). Sentences\nwith long distance dependencies like questions (What do you think I ate?)\nchallenge all models, but for these, BERT and GPT have a distinct advantage\nover the baseline. We conclude that recent sentence encoders, despite showing\nnear-human performance on acceptability classification overall, still fail to\nmake fine-grained grammaticality distinctions for many complex syntactic\nstructures.", "published": "2019-01-11 00:25:10", "link": "http://arxiv.org/abs/1901.03438v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Plots to Endings: A Reinforced Pointer Generator for Story Ending\n  Generation", "abstract": "We introduce a new task named Story Ending Generation (SEG), whic-h aims at\ngenerating a coherent story ending from a sequence of story plot. Wepropose a\nframework consisting of a Generator and a Reward Manager for thistask. The\nGenerator follows the pointer-generator network with coverage mech-anism to\ndeal with out-of-vocabulary (OOV) and repetitive words. Moreover, amixed loss\nmethod is introduced to enable the Generator to produce story endingsof high\nsemantic relevance with story plots. In the Reward Manager, the rewardis\ncomputed to fine-tune the Generator with policy-gradient reinforcement\nlearn-ing (PGRL). We conduct experiments on the recently-introduced\nROCStoriesCorpus. We evaluate our model in both automatic evaluation and human\nevalua-tion. Experimental results show that our model exceeds the\nsequence-to-sequencebaseline model by 15.75% and 13.57% in terms of CIDEr and\nconsistency scorerespectively.", "published": "2019-01-11 02:28:38", "link": "http://arxiv.org/abs/1901.03459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog System Technology Challenge 7", "abstract": "This paper introduces the Seventh Dialog System Technology Challenges (DSTC),\nwhich use shared datasets to explore the problem of building dialog systems.\nRecently, end-to-end dialog modeling approaches have been applied to various\ndialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies\nrelated to end-to-end dialog systems for (1) sentence selection, (2) sentence\ngeneration and (3) audio visual scene aware dialog. This paper summarizes the\noverall setup and results of DSTC7, including detailed descriptions of the\ndifferent tracks and provided datasets. We also describe overall trends in the\nsubmitted systems and the key results. Each track introduced new datasets and\nparticipants achieved impressive results using state-of-the-art end-to-end\ntechnologies.", "published": "2019-01-11 02:43:12", "link": "http://arxiv.org/abs/1901.03461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via\n  Lexically-constrained Neural Machine Translation", "abstract": "We present ParaBank, a large-scale English paraphrase dataset that surpasses\nprior work in both quantity and quality. Following the approach of ParaNMT, we\ntrain a Czech-English neural machine translation (NMT) system to generate novel\nparaphrases of English reference sentences. By adding lexical constraints to\nthe NMT decoding procedure, however, we are able to produce multiple\nhigh-quality sentential paraphrases per source sentence, yielding an English\nparaphrase resource with more than 4 billion generated tokens and exhibiting\ngreater lexical diversity. Using human judgments, we also demonstrate that\nParaBank's paraphrases improve over ParaNMT on both semantic similarity and\nfluency. Finally, we use ParaBank to train a monolingual NMT model with the\nsame support for lexically-constrained decoding for sentence rewriting tasks.", "published": "2019-01-11 16:42:43", "link": "http://arxiv.org/abs/1901.03644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in\n  Natural Language Inference", "abstract": "Quantitative reasoning is a higher-order reasoning skill that any intelligent\nnatural language understanding system can reasonably be expected to handle. We\npresent EQUATE (Evaluating Quantitative Understanding Aptitude in Textual\nEntailment), a new framework for quantitative reasoning in textual entailment.\nWe benchmark the performance of 9 published NLI models on EQUATE, and find that\non average, state-of-the-art methods do not achieve an absolute improvement\nover a majority-class baseline, suggesting that they do not implicitly learn to\nreason with quantities. We establish a new baseline Q-REAS that manipulates\nquantities symbolically. In comparison to the best performing NLI model, it\nachieves success on numerical reasoning tests (+24.2%), but has limited verbal\nreasoning capabilities (-8.1%). We hope our evaluation framework will support\nthe development of models of quantitative reasoning in language understanding.", "published": "2019-01-11 20:27:25", "link": "http://arxiv.org/abs/1901.03735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Intent Prediction in Information-seeking Conversations", "abstract": "Conversational assistants are being progressively adopted by the general\npopulation. However, they are not capable of handling complicated\ninformation-seeking tasks that involve multiple turns of information exchange.\nDue to the limited communication bandwidth in conversational search, it is\nimportant for conversational assistants to accurately detect and predict user\nintent in information-seeking conversations. In this paper, we investigate two\naspects of user intent prediction in an information-seeking setting. First, we\nextract features based on the content, structural, and sentiment\ncharacteristics of a given utterance, and use classic machine learning methods\nto perform user intent prediction. We then conduct an in-depth feature\nimportance analysis to identify key features in this prediction task. We find\nthat structural features contribute most to the prediction performance. Given\nthis finding, we construct neural classifiers to incorporate context\ninformation and achieve better performance without feature engineering. Our\nfindings can provide insights into the important factors and effective methods\nof user intent prediction in information-seeking conversations.", "published": "2019-01-11 05:53:13", "link": "http://arxiv.org/abs/1901.03489v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Answer Interaction in Non-factoid Question Answering Systems", "abstract": "Information retrieval systems are evolving from document retrieval to answer\nretrieval. Web search logs provide large amounts of data about how people\ninteract with ranked lists of documents, but very little is known about\ninteraction with answer texts. In this paper, we use Amazon Mechanical Turk to\ninvestigate three answer presentation and interaction approaches in a\nnon-factoid question answering setting. We find that people perceive and react\nto good and bad answers very differently, and can identify good answers\nrelatively quickly. Our results provide the basis for further investigation of\neffective answer interaction and feedback methods.", "published": "2019-01-11 06:02:22", "link": "http://arxiv.org/abs/1901.03491v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Event Causality Detection in Tweets", "abstract": "Nowadays, Twitter has become a great source of user-generated information\nabout events. Very often people report causal relationships between events in\ntheir tweets. Automatic detection of causality information in these events\nmight play an important role in predictive event analytics. Existing approaches\ninclude both rule-based and data-driven supervised methods. However, it is\nchallenging to correctly identify event causality using only linguistic rules\ndue to the highly unstructured nature and grammatical incorrectness of social\nmedia short text such as tweets. Also, it is difficult to develop a data-driven\nsupervised method for event causality detection in tweets due to insufficient\ncontextual information. This paper proposes a novel event context word\nextension technique based on background knowledge. To demonstrate the\neffectiveness of our proposed event context word extension technique, we\ndevelop a feed-forward neural network based approach to detect event causality\nfrom tweets. Extensive experiments demonstrate the superiority of our approach.", "published": "2019-01-11 09:39:55", "link": "http://arxiv.org/abs/1901.03526v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Advanced Rich Transcription System for Estonian Speech", "abstract": "This paper describes the current TT\\\"U speech transcription system for\nEstonian speech. The system is designed to handle semi-spontaneous speech, such\nas broadcast conversations, lecture recordings and interviews recorded in\ndiverse acoustic conditions. The system is based on the Kaldi toolkit.\nMulti-condition training using background noise profiles extracted\nautomatically from untranscribed data is used to improve the robustness of the\nsystem. Out-of-vocabulary words are recovered using a phoneme n-gram based\ndecoding subgraph and a FST-based phoneme-to-grapheme model. The system\nachieves a word error rate of 8.1% on a test set of broadcast conversations.\nThe system also performs punctuation recovery and speaker identification.\nSpeaker identification models are trained using a recently proposed weakly\nsupervised training method.", "published": "2019-01-11 14:51:02", "link": "http://arxiv.org/abs/1901.03601v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey", "abstract": "With the proliferation of Internet-of-Things devices, acoustic sensing\nattracts much attention in recent years. It exploits acoustic transceivers such\nas microphones and speakers beyond their primary functions, namely recording\nand playing, to enable novel applications and new user experiences. In this\npaper, we present the first systematic survey of recent advances in active\nacoustic sensing using commodity hardware with a frequency range below\n24~\\!kHz. We propose a general framework that categorizes main building blocks\nof acoustic sensing systems. This framework encompasses three layers, i.e.,\nphysical layer, core technique layer, and application layer. The physical layer\nincludes basic hardware components, acoustic platforms as well as the air-borne\nand structure-borne channel characteristics. The core technique layer\nencompasses key mechanisms to generate acoustic signals (waveforms) and to\nextract useful temporal, spatial and spectral information from received\nsignals. The application layer builds upon the functions offered by the core\ntechniques to realize different acoustic sensing applications. We highlight\nunique challenges due to the limitations of physical devices and acoustic\nchannels and how they are mitigated or overcame by core processing techniques\nand application-specific solutions. Finally, research opportunities and future\ndirections are discussed to spawn further in-depth investigation on acoustic\nsensing.", "published": "2019-01-11 01:58:35", "link": "http://arxiv.org/abs/1901.03450v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
