{"title": "Topic Memory Networks for Short Text Classification", "abstract": "Many classification models work poorly on short texts due to data sparsity.\nTo address this issue, we propose topic memory networks for short text\nclassification with a novel topic memory mechanism to encode latent topic\nrepresentations indicative of class labels. Different from most prior work that\nfocuses on extending features with external knowledge or pre-trained topics,\nour model jointly explores topic inference and text classification with memory\nnetworks in an end-to-end manner. Experimental results on four benchmark\ndatasets show that our model outperforms state-of-the-art models on short text\nclassification, meanwhile generates coherent topics.", "published": "2018-09-11 03:03:37", "link": "http://arxiv.org/abs/1809.03664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Scripts as Hidden Markov Models", "abstract": "Scripts have been proposed to model the stereotypical event sequences found\nin narratives. They can be applied to make a variety of inferences including\nfilling gaps in the narratives and resolving ambiguous references. This paper\nproposes the first formal framework for scripts based on Hidden Markov Models\n(HMMs). Our framework supports robust inference and learning algorithms, which\nare lacking in previous clustering models. We develop an algorithm for\nstructure and parameter learning based on Expectation Maximization and evaluate\nit on a number of natural datasets. The results show that our algorithm is\nsuperior to several informed baselines for predicting missing events in partial\nobservation sequences.", "published": "2018-09-11 05:02:28", "link": "http://arxiv.org/abs/1809.03680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Joint Model of Conversational Discourse and Latent Topics on\n  Microblogs", "abstract": "Conventional topic models are ineffective for topic extraction from microblog\nmessages, because the data sparseness exhibited in short messages lacking\nstructure and contexts results in poor message-level word co-occurrence\npatterns. To address this issue, we organize microblog messages as conversation\ntrees based on their reposting and replying relations, and propose an\nunsupervised model that jointly learns word distributions to represent: 1)\ndifferent roles of conversational discourse, 2) various latent topics in\nreflecting content information. By explicitly distinguishing the probabilities\nof messages with varying discourse roles in containing topical words, our model\nis able to discover clusters of discourse words that are indicative of topical\ncontent. In an automatic evaluation on large-scale microblog corpora, our joint\nmodel yields topics with better coherence scores than competitive topic models\nfrom previous studies. Qualitative analysis on model outputs indicates that our\nmodel induces meaningful representations for both discourse and topics. We\nfurther present an empirical study on microblog summarization based on the\noutputs of our joint model. The results show that the jointly modeled discourse\nand topic representations can effectively indicate summary-worthy content in\nmicroblog conversations.", "published": "2018-09-11 06:13:37", "link": "http://arxiv.org/abs/1809.03690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much should you ask? On the question structure in QA systems", "abstract": "Datasets that boosted state-of-the-art solutions for Question Answering (QA)\nsystems prove that it is possible to ask questions in natural language manner.\nHowever, users are still used to query-like systems where they type in keywords\nto search for answer. In this study we validate which parts of questions are\nessential for obtaining valid answer. In order to conclude that, we take\nadvantage of LIME - a framework that explains prediction by local\napproximation. We find that grammar and natural language is disregarded by QA.\nState-of-the-art model can answer properly even if 'asked' only with a few\nwords with high coefficients calculated with LIME. According to our knowledge,\nit is the first time that QA model is being explained by LIME.", "published": "2018-09-11 08:26:36", "link": "http://arxiv.org/abs/1809.03734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does it care what you asked? Understanding Importance of Verbs in Deep\n  Learning QA System", "abstract": "In this paper we present the results of an investigation of the importance of\nverbs in a deep learning QA system trained on SQuAD dataset. We show that main\nverbs in questions carry little influence on the decisions made by the system -\nin over 90% of researched cases swapping verbs for their antonyms did not\nchange system decision. We track this phenomenon down to the insides of the\nnet, analyzing the mechanism of self-attention and values contained in hidden\nlayers of RNN. Finally, we recognize the characteristics of the SQuAD dataset\nas the source of the problem. Our work refers to the recently popular topic of\nadversarial examples in NLP, combined with investigating deep net structure.", "published": "2018-09-11 08:37:07", "link": "http://arxiv.org/abs/1809.03740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Studying the History of the Arabic Language: Language Technology and a\n  Large-Scale Historical Corpus", "abstract": "Arabic is a widely-spoken language with a long and rich history, but existing\ncorpora and language technology focus mostly on modern Arabic and its\nvarieties. Therefore, studying the history of the language has so far been\nmostly limited to manual analyses on a small scale. In this work, we present a\nlarge-scale historical corpus of the written Arabic language, spanning 1400\nyears. We describe our efforts to clean and process this corpus using Arabic\nNLP tools, including the identification of reused text. We study the history of\nthe Arabic language using a novel automatic periodization algorithm, as well as\nother techniques. Our findings confirm the established division of written\nArabic into Modern Standard and Classical Arabic, and confirm other established\nperiodizations, while suggesting that written Arabic may be divisible into\nstill further periods of development.", "published": "2018-09-11 13:44:48", "link": "http://arxiv.org/abs/1809.03891v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multilingual Cross-domain Perspectives on Online Hate Speech", "abstract": "In this report, we present a study of eight corpora of online hate speech, by\ndemonstrating the NLP techniques that we used to collect and analyze the\njihadist, extremist, racist, and sexist content. Analysis of the multilingual\ncorpora shows that the different contexts share certain characteristics in\ntheir hateful rhetoric. To expose the main features, we have focused on text\nclassification, text profiling, keyword and collocation extraction, along with\nmanual annotation and qualitative study.", "published": "2018-09-11 14:49:05", "link": "http://arxiv.org/abs/1809.03944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On The Alignment Problem In Multi-Head Attention-Based Neural Machine\n  Translation", "abstract": "This work investigates the alignment problem in state-of-the-art multi-head\nattention models based on the transformer architecture. We demonstrate that\nalignment extraction in transformer models can be improved by augmenting an\nadditional alignment head to the multi-head source-to-target attention\ncomponent. This is used to compute sharper attention weights. We describe how\nto use the alignment head to achieve competitive performance. To study the\neffect of adding the alignment head, we simulate a dictionary-guided\ntranslation task, where the user wants to guide translation using pre-defined\ndictionary entries. Using the proposed approach, we achieve up to $3.8$ % BLEU\nimprovement when using the dictionary, in comparison to $2.4$ % BLEU in the\nbaseline case. We also propose alignment pruning to speed up decoding in\nalignment-based neural machine translation (ANMT), which speeds up translation\nby a factor of $1.8$ without loss in translation performance. We carry out\nexperiments on the shared WMT 2016 English$\\to$Romanian news task and the BOLT\nChinese$\\to$English discussion forum task.", "published": "2018-09-11 15:41:12", "link": "http://arxiv.org/abs/1809.03985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Composition in Sentence Vector Representations", "abstract": "An important component of achieving language understanding is mastering the\ncomposition of sentence meaning, but an immediate challenge to solving this\nproblem is the opacity of sentence vector representations produced by current\nneural sentence composition models. We present a method to address this\nchallenge, developing tasks that directly target compositional meaning\ninformation in sentence vector representations with a high degree of precision\nand control. To enable the creation of these controlled tasks, we introduce a\nspecialized sentence generation system that produces large, annotated sentence\nsets meeting specified syntactic, semantic and lexical constraints. We describe\nthe details of the method and generation system, and then present results of\nexperiments applying our method to probe for compositional information in\nembeddings from a number of existing sentence composition models. We find that\nthe method is able to extract useful information about the differing capacities\nof these models, and we discuss the implications of our results with respect to\nthese systems' capturing of sentence information. We make available for public\nuse the datasets used for these experiments, as well as the generation system.", "published": "2018-09-11 15:56:46", "link": "http://arxiv.org/abs/1809.03992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Semantic Rationality of a Sentence: A Sememe-Word-Matching\n  Neural Network based on HowNet", "abstract": "Automatic evaluation of semantic rationality is an important yet challenging\ntask, and current automatic techniques cannot well identify whether a sentence\nis semantically rational. The methods based on the language model do not\nmeasure the sentence by rationality but by commonness. The methods based on the\nsimilarity with human written sentences will fail if human-written references\nare not available. In this paper, we propose a novel model called\nSememe-Word-Matching Neural Network (SWM-NN) to tackle semantic rationality\nevaluation by taking advantage of sememe knowledge base HowNet. The advantage\nis that our model can utilize a proper combination of sememes to represent the\nfine-grained semantic meanings of a word within the specific contexts. We use\nthe fine-grained semantic representation to help the model learn the semantic\ndependency among words. To evaluate the effectiveness of the proposed model, we\nbuild a large-scale rationality evaluation dataset. Experimental results on\nthis dataset show that the proposed model outperforms the competitive baselines\nwith a 5.4\\% improvement in accuracy.", "published": "2018-09-11 16:09:53", "link": "http://arxiv.org/abs/1809.03999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LSTM Learn to Capture Agreement? The Case of Basque", "abstract": "Sequential neural networks models are powerful tools in a variety of Natural\nLanguage Processing (NLP) tasks. The sequential nature of these models raises\nthe questions: to what extent can these models implicitly learn hierarchical\nstructures typical to human language, and what kind of grammatical phenomena\ncan they acquire?\n  We focus on the task of agreement prediction in Basque, as a case study for a\ntask that requires implicit understanding of sentence structure and the\nacquisition of a complex but consistent morphological system. Analyzing\nexperimental results from two syntactic prediction tasks -- verb number\nprediction and suffix recovery -- we find that sequential models perform worse\non agreement prediction in Basque than one might expect on the basis of a\nprevious agreement prediction work in English. Tentative findings based on\ndiagnostic classifiers suggest the network makes use of local heuristics as a\nproxy for the hierarchical structure of the sentence. We propose the Basque\nagreement prediction task as challenging benchmark for models that attempt to\nlearn regularities in human language.", "published": "2018-09-11 16:44:02", "link": "http://arxiv.org/abs/1809.04022v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AWE: Asymmetric Word Embedding for Textual Entailment", "abstract": "Textual entailment is a fundamental task in natural language processing. It\nrefers to the directional relation between text fragments such that the\n\"premise\" can infer \"hypothesis\". In recent years deep learning methods have\nachieved great success in this task. Many of them have considered the\ninter-sentence word-word interactions between the premise-hypothesis pairs,\nhowever, few of them considered the \"asymmetry\" of these interactions.\nDifferent from paraphrase identification or sentence similarity evaluation,\ntextual entailment is essentially determining a directional (asymmetric)\nrelation between the premise and the hypothesis. In this paper, we propose a\nsimple but effective way to enhance existing textual entailment algorithms by\nusing asymmetric word embeddings. Experimental results on SciTail and SNLI\ndatasets show that the learned asymmetric word embeddings could significantly\nimprove the word-word interaction based textual entailment models. It is\nnoteworthy that the proposed AWE-DeIsTe model can get 2.1% accuracy improvement\nover prior state-of-the-art on SciTail.", "published": "2018-09-11 17:30:12", "link": "http://arxiv.org/abs/1809.04047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On learning an interpreted language with recurrent models", "abstract": "Can recurrent neural nets, inspired by human sequential data processing,\nlearn to understand language? We construct simplified datasets reflecting core\nproperties of natural language as modeled in formal syntax and semantics:\nrecursive syntactic structure and compositionality. We find LSTM and GRU\nnetworks to generalise to compositional interpretation well, but only in the\nmost favorable learning settings, with a well-paced curriculum, extensive\ntraining data, and left-to-right (but not right-to-left) composition.", "published": "2018-09-11 19:52:44", "link": "http://arxiv.org/abs/1809.04128v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word\n  Vector Specialization", "abstract": "Semantic specialization is the process of fine-tuning pre-trained\ndistributional word vectors using external lexical knowledge (e.g., WordNet) to\naccentuate a particular semantic relation in the specialized vector space.\nWhile post-processing specialization methods are applicable to arbitrary\ndistributional vectors, they are limited to updating only the vectors of words\noccurring in external lexicons (i.e., seen words), leaving the vectors of all\nother words unchanged. We propose a novel approach to specializing the full\ndistributional vocabulary. Our adversarial post-specialization method\npropagates the external lexical knowledge to the full distributional space. We\nexploit words seen in the resources as training examples for learning a global\nspecialization function. This function is learned by combining a standard\nL2-distance loss with an adversarial loss: the adversarial component produces\nmore realistic output vectors. We show the effectiveness and robustness of the\nproposed method across three languages and on three tasks: word similarity,\ndialog state tracking, and lexical simplification. We report consistent\nimprovements over distributional word vectors and vectors specialized by other\nstate-of-the-art specialization frameworks. Finally, we also propose a\ncross-lingual transfer method for zero-shot specialization which successfully\nspecializes a full target distributional space without any lexical knowledge in\nthe target language and without any bilingual data.", "published": "2018-09-11 21:08:00", "link": "http://arxiv.org/abs/1809.04163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What can linguistics and deep learning contribute to each other?", "abstract": "Joe Pater's target article calls for greater interaction between neural\nnetwork research and linguistics. I expand on this call and show how such\ninteraction can benefit both fields. Linguists can contribute to research on\nneural networks for language technologies by clearly delineating the linguistic\ncapabilities that can be expected of such systems, and by constructing\ncontrolled experimental paradigms that can determine whether those desiderata\nhave been met. In the other direction, neural networks can benefit the\nscientific study of language by providing infrastructure for modeling human\nsentence processing and for evaluating the necessity of particular innate\nconstraints on language acquisition.", "published": "2018-09-11 21:55:11", "link": "http://arxiv.org/abs/1809.04179v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Sinhala Language Arithmetic Problems using Neural Networks", "abstract": "A methodology is presented to solve Arithmetic problems in Sinhala Language\nusing a Neural Network. The system comprises of (a) keyword identification, (b)\nquestion identification, (c) mathematical operation identification and is\ncombined using a neural network. Naive Bayes Classification is used in order to\nidentify keywords and Conditional Random Field to identify the question and the\noperation which should be performed on the identified keywords to achieve the\nexpected result. \"One vs. all Classification\" is done using a neural network\nfor sentences. All functions are combined through the neural network which\nbuilds an equation to solve the problem. The paper compares each methodology in\nARIS and Mahoshadha to the method presented in the paper. Mahoshadha2 learns to\nsolve arithmetic problems with the accuracy of 76%.", "published": "2018-09-11 13:29:08", "link": "http://arxiv.org/abs/1809.04557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Multimodal Representations on Sentence Similarity: vSTS,\n  Visual Semantic Textual Similarity Dataset", "abstract": "In this paper we introduce vSTS, a new dataset for measuring textual\nsimilarity of sentences using multimodal information. The dataset is comprised\nby images along with its respectively textual captions. We describe the dataset\nboth quantitatively and qualitatively, and claim that it is a valid gold\nstandard for measuring automatic multimodal textual similarity systems. We also\ndescribe the initial experiments combining the multimodal information.", "published": "2018-09-11 06:40:36", "link": "http://arxiv.org/abs/1809.03695v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answering Visual What-If Questions: From Actions to Predicted Scene\n  Descriptions", "abstract": "In-depth scene descriptions and question answering tasks have greatly\nincreased the scope of today's definition of scene understanding. While such\ntasks are in principle open ended, current formulations primarily focus on\ndescribing only the current state of the scenes under consideration. In\ncontrast, in this paper, we focus on the future states of the scenes which are\nalso conditioned on actions. We posit this as a question answering task, where\nan answer has to be given about a future scene state, given observations of the\ncurrent scene, and a question that includes a hypothetical action. Our solution\nis a hybrid model which integrates a physics engine into a question answering\narchitecture in order to anticipate future scene states resulting from\nobject-object interactions caused by an action. We demonstrate first results on\nthis challenging new problem and compare to baselines, where we outperform\nfully data-driven end-to-end learning approaches.", "published": "2018-09-11 07:22:28", "link": "http://arxiv.org/abs/1809.03707v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "68"], "primary_category": "cs.CV"}
{"title": "Training and Prediction Data Discrepancies: Challenges of Text\n  Classification with Noisy, Historical Data", "abstract": "Industry datasets used for text classification are rarely created for that\npurpose. In most cases, the data and target predictions are a by-product of\naccumulated historical data, typically fraught with noise, present in both the\ntext-based document, as well as in the targeted labels. In this work, we\naddress the question of how well performance metrics computed on noisy,\nhistorical data reflect the performance on the intended future machine learning\nmodel input. The results demonstrate the utility of dirty training datasets\nused to build prediction models for cleaner (and different) prediction inputs.", "published": "2018-09-11 16:43:52", "link": "http://arxiv.org/abs/1809.04019v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "The Visual QA Devil in the Details: The Impact of Early Fusion and Batch\n  Norm on CLEVR", "abstract": "Visual QA is a pivotal challenge for higher-level reasoning, requiring\nunderstanding language, vision, and relationships between many objects in a\nscene. Although datasets like CLEVR are designed to be unsolvable without such\ncomplex relational reasoning, some surprisingly simple feed-forward, \"holistic\"\nmodels have recently shown strong performance on this dataset. These models\nlack any kind of explicit iterative, symbolic reasoning procedure, which are\nhypothesized to be necessary for counting objects, narrowing down the set of\nrelevant objects based on several attributes, etc. The reason for this strong\nperformance is poorly understood. Hence, our work analyzes such models, and\nfinds that minor architectural elements are crucial to performance. In\nparticular, we find that \\textit{early fusion} of language and vision provides\nlarge performance improvements. This contrasts with the late fusion approaches\npopular at the dawn of Visual QA. We propose a simple module we call Multimodal\nCore, which we hypothesize performs the fundamental operations for multimodal\ntasks. We believe that understanding why these elements are so important to\ncomplex question answering will aid the design of better-performing algorithms\nfor Visual QA while minimizing hand-engineering effort.", "published": "2018-09-11 07:14:30", "link": "http://arxiv.org/abs/1809.04482v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Context-Dependent Diffusion Network for Visual Relationship Detection", "abstract": "Visual relationship detection can bridge the gap between computer vision and\nnatural language for scene understanding of images. Different from pure object\nrecognition tasks, the relation triplets of subject-predicate-object lie on an\nextreme diversity space, such as \\textit{person-behind-person} and\n\\textit{car-behind-building}, while suffering from the problem of combinatorial\nexplosion. In this paper, we propose a context-dependent diffusion network\n(CDDN) framework to deal with visual relationship detection. To capture the\ninteractions of different object instances, two types of graphs, word semantic\ngraph and visual scene graph, are constructed to encode global context\ninterdependency. The semantic graph is built through language priors to model\nsemantic correlations across objects, whilst the visual scene graph defines the\nconnections of scene objects so as to utilize the surrounding scene\ninformation. For the graph-structured data, we design a diffusion network to\nadaptively aggregate information from contexts, which can effectively learn\nlatent representations of visual relationships and well cater to visual\nrelationship detection in view of its isomorphic invariance to graphs.\nExperiments on two widely-used datasets demonstrate that our proposed method is\nmore effective and achieves the state-of-the-art performance.", "published": "2018-09-11 02:13:45", "link": "http://arxiv.org/abs/1809.06213v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Unsupervised Stylish Image Description Generation via Domain Layer Norm", "abstract": "Most of the existing works on image description focus on generating\nexpressive descriptions. The only few works that are dedicated to generating\nstylish (e.g., romantic, lyric, etc.) descriptions suffer from limited style\nvariation and content digression. To address these limitations, we propose a\ncontrollable stylish image description generation model. It can learn to\ngenerate stylish image descriptions that are more related to image content and\ncan be trained with the arbitrary monolingual corpus without collecting new\npaired image and stylish descriptions. Moreover, it enables users to generate\nvarious stylish descriptions by plugging in style-specific parameters to\ninclude new styles into the existing model. We achieve this capability via a\nnovel layer normalization layer design, which we will refer to as the Domain\nLayer Norm (DLN). Extensive experimental validation and user study on various\nstylish image description generation tasks are conducted to show the\ncompetitive advantages of the proposed model.", "published": "2018-09-11 11:07:26", "link": "http://arxiv.org/abs/1809.06214v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Isolated and Ensemble Audio Preprocessing Methods for Detecting\n  Adversarial Examples against Automatic Speech Recognition", "abstract": "An adversarial attack is an exploitative process in which minute alterations\nare made to natural inputs, causing the inputs to be misclassified by neural\nmodels. In the field of speech recognition, this has become an issue of\nincreasing significance. Although adversarial attacks were originally\nintroduced in computer vision, they have since infiltrated the realm of speech\nrecognition. In 2017, a genetic attack was shown to be quite potent against the\nSpeech Commands Model. Limited-vocabulary speech classifiers, such as the\nSpeech Commands Model, are used in a variety of applications, particularly in\ntelephony; as such, adversarial examples produced by this attack pose as a\nmajor security threat. This paper explores various methods of detecting these\nadversarial examples with combinations of audio preprocessing. One particular\ncombined defense incorporating compressions, speech coding, filtering, and\naudio panning was shown to be quite effective against the attack on the Speech\nCommands Model, detecting audio adversarial examples with 93.5% precision and\n91.2% recall.", "published": "2018-09-11 05:12:15", "link": "http://arxiv.org/abs/1809.04397v1", "categories": ["cs.SD", "cs.CL", "cs.CR", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "One-Shot Speaker Identification for a Service Robot using a CNN-based\n  Generic Verifier", "abstract": "In service robotics, there is an interest to identify the user by voice\nalone. However, in application scenarios where a service robot acts as a waiter\nor a store clerk, new users are expected to enter the environment frequently.\nTypically, speaker identification models need to be retrained when this occurs,\nwhich can take an impractical amount of time. In this paper, a new approach for\nspeaker identification through verification has been developed using a Siamese\nConvolutional Neural Network architecture (SCNN), where it learns to\ngenerically verify if two audio signals are from the same speaker. By having an\nexternal database of recorded audio of the users, identification is carried out\nby verifying the speech input with each of its entries. If new users are\nencountered, it is only required to add their recorded audio to the external\ndatabase to be able to be identified, without retraining. The system was\nevaluated in four different aspects: the performance of the verifier, the\nperformance of the system as a classifier using clean audio, its speed, and its\naccuracy in real-life settings. Its performance in conjunction with its\none-shot-learning capabilities, makes the proposed system a viable alternative\nfor speaker identification for service robots.", "published": "2018-09-11 19:16:07", "link": "http://arxiv.org/abs/1809.04115v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
