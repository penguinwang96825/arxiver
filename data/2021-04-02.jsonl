{"title": "Humor@IITK at SemEval-2021 Task 7: Large Language Models for Quantifying\n  Humor and Offensiveness", "abstract": "Humor and Offense are highly subjective due to multiple word senses, cultural\nknowledge, and pragmatic competence. Hence, accurately detecting humorous and\noffensive texts has several compelling use cases in Recommendation Systems and\nPersonalized Content Moderation. However, due to the lack of an extensive\nlabeled dataset, most prior works in this domain haven't explored large neural\nmodels for subjective humor understanding. This paper explores whether large\nneural models and their ensembles can capture the intricacies associated with\nhumor/offense detection and rating. Our experiments on the SemEval-2021 Task 7:\nHaHackathon show that we can develop reasonable humor and offense detection\nsystems with such models. Our models are ranked third in subtask 1b and\nconsistently ranked around the top 33% of the leaderboard for the remaining\nsubtasks.", "published": "2021-04-02 08:22:02", "link": "http://arxiv.org/abs/2104.00933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Recalibrated Aggregation Network for Medical Code Prediction", "abstract": "Medical coding translates professionally written medical reports into\nstandardized codes, which is an essential part of medical information systems\nand health insurance reimbursement. Manual coding by trained human coders is\ntime-consuming and error-prone. Thus, automated coding algorithms have been\ndeveloped, building especially on the recent advances in machine learning and\ndeep neural networks. To solve the challenges of encoding lengthy and noisy\nclinical documents and capturing code associations, we propose a multitask\nrecalibrated aggregation network. In particular, multitask learning shares\ninformation across different coding schemes and captures the dependencies\nbetween different medical codes. Feature recalibration and aggregation in\nshared modules enhance representation learning for lengthy notes. Experiments\nwith a real-world MIMIC-III dataset show significantly improved predictive\nperformance.", "published": "2021-04-02 09:22:10", "link": "http://arxiv.org/abs/2104.00952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IITK@LCP at SemEval 2021 Task 1: Classification for Lexical Complexity\n  Regression Task", "abstract": "This paper describes our contribution to SemEval 2021 Task 1: Lexical\nComplexity Prediction. In our approach, we leverage the ELECTRA model and\nattempt to mirror the data annotation scheme. Although the task is a regression\ntask, we show that we can treat it as an aggregation of several classification\nand regression models. This somewhat counter-intuitive approach achieved an MAE\nscore of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally,\nwe used the concept of weak supervision signals from Gloss-BERT in our work,\nand it significantly improved the MAE score in Sub-Task 1.", "published": "2021-04-02 13:40:12", "link": "http://arxiv.org/abs/2104.01046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate\n  pre-training", "abstract": "We present the TAPAS contribution to the Shared Task on Statement\nVerification and Evidence Finding with Tables (SemEval 2021 Task 9, Wang et al.\n(2021)). SEM TAB FACT Task A is a classification task of recognizing if a\nstatement is entailed, neutral or refuted by the content of a given table. We\nadopt the binary TAPAS model of Eisenschlos et al. (2020) to this task. We\nlearn two binary classification models: A first model to predict if a statement\nis neutral or non-neutral and a second one to predict if it is entailed or\nrefuted. As the shared task training set contains only entailed or refuted\nexamples, we generate artificial neutral examples to train the first model.\nBoth models are pre-trained using a MASKLM objective, intermediate\ncounter-factual and synthetic data (Eisenschlos et al., 2020) and TABFACT (Chen\net al., 2020), a large table entailment dataset. We find that the artificial\nneutral examples are somewhat effective at training the first model, achieving\n68.03 test F1 versus the 60.47 of a majority baseline. For the second stage, we\nfind that the pre-training on the intermediate data and TABFACT improves the\nresults over MASKLM pre-training (68.03 vs 57.01).", "published": "2021-04-02 15:47:08", "link": "http://arxiv.org/abs/2104.01099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sketch and Customize: A Counterfactual Story Generator", "abstract": "Recent text generation models are easy to generate relevant and fluent text\nfor the given text, while lack of causal reasoning ability when we change some\nparts of the given text. Counterfactual story rewriting is a recently proposed\ntask to test the causal reasoning ability for text generation models, which\nrequires a model to predict the corresponding story ending when the condition\nis modified to a counterfactual one. Previous works have shown that the\ntraditional sequence-to-sequence model cannot well handle this problem, as it\noften captures some spurious correlations between the original and\ncounterfactual endings, instead of the causal relations between conditions and\nendings. To address this issue, we propose a sketch-and-customize generation\nmodel guided by the causality implicated in the conditions and endings. In the\nsketch stage, a skeleton is extracted by removing words which are conflict to\nthe counterfactual condition, from the original ending. In the customize stage,\na generation model is used to fill proper words in the skeleton under the\nguidance of the counterfactual condition. In this way, the obtained\ncounterfactual ending is both relevant to the original ending and consistent\nwith the counterfactual condition. Experimental results show that the proposed\nmodel generates much better endings, as compared with the traditional\nsequence-to-sequence model.", "published": "2021-04-02 08:14:22", "link": "http://arxiv.org/abs/2104.00929v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Use of 'off-the-shelf' information extraction algorithms in clinical\n  informatics: a feasibility study of MetaMap annotation of Italian medical\n  notes", "abstract": "Information extraction from narrative clinical notes is useful for patient\ncare, as well as for secondary use of medical data, for research or clinical\npurposes. Many studies focused on information extraction from English clinical\ntexts, but less dealt with clinical notes in languages other than English. This\nstudy tested the feasibility of using 'off the shelf' information extraction\nalgorithms to identify medical concepts from Italian clinical notes. We used\nMetaMap to map medical concepts to the Unified Medical Language System (UMLS).\nThe study addressed two questions: (Q1) to understand if it would be possible\nto properly map medical terms found in clinical notes and related to the\nsemantic group of 'Disorders' to the Italian UMLS resources; (Q2) to\ninvestigate if it would be feasible to use MetaMap as it is to extract these\nmedical concepts from Italian clinical notes. Results in EXP1 showed that the\nItalian UMLS Metathesaurus sources covered 91% of the medical terms of the\n'Disorders' semantic group, as found in the studied dataset. Even if MetaMap\nwas built to analyze texts written in English, it worked properly also with\ntexts written in Italian. MetaMap identified correctly about half of the\nconcepts in the Italian clinical notes. Using MetaMap's annotation on Italian\nclinical notes instead of a simple text search improved our results of about 15\npercentage points. MetaMap showed recall, precision and F-measure of 0.53, 0.98\nand 0.69, respectively. Most of the failures were due to the impossibility for\nMetaMap to generate Italian meaningful variants. MetaMap's performance in\nannotating automatically translated English clinical notes was in line with\nfindings in the literature, with similar recall (0.75), F-measure (0.83) and\neven higher precision (0.95).", "published": "2021-04-02 10:28:50", "link": "http://arxiv.org/abs/2104.00975v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Visual Semantic Role Labeling for Video Understanding", "abstract": "We propose a new framework for understanding and representing related salient\nevents in a video using visual semantic role labeling. We represent videos as a\nset of related events, wherein each event consists of a verb and multiple\nentities that fulfill various roles relevant to that event. To study the\nchallenging task of semantic role labeling in videos or VidSRL, we introduce\nthe VidSitu benchmark, a large-scale video understanding data source with $29K$\n$10$-second movie clips richly annotated with a verb and semantic-roles every\n$2$ seconds. Entities are co-referenced across events within a movie clip and\nevents are connected to each other via event-event relations. Clips in VidSitu\nare drawn from a large collection of movies (${\\sim}3K$) and have been chosen\nto be both complex (${\\sim}4.2$ unique verbs within a video) as well as diverse\n(${\\sim}200$ verbs have more than $100$ annotations each). We provide a\ncomprehensive analysis of the dataset in comparison to other publicly available\nvideo understanding benchmarks, several illustrative baselines and evaluate a\nrange of standard video recognition models. Our code and dataset is available\nat vidsitu.org.", "published": "2021-04-02 11:23:22", "link": "http://arxiv.org/abs/2104.00990v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Effect of depth order on iterative nested named entity recognition\n  models", "abstract": "This paper studies the effect of the order of depth of mention on nested\nnamed entity recognition (NER) models. NER is an essential task in the\nextraction of biomedical information, and nested entities are common since\nmedical concepts can assemble to form larger entities. Conventional NER systems\nonly predict disjointed entities. Thus, iterative models for nested NER use\nmultiple predictions to enumerate all entities, imposing a predefined order\nfrom largest to smallest or smallest to largest. We design an order-agnostic\niterative model and a procedure to choose a custom order during training and\nprediction. To accommodate for this task, we propose a modification of the\nTransformer architecture to take into account the entities predicted in the\nprevious steps. We provide a set of experiments to study the model's\ncapabilities and the effects of the order on performance. Finally, we show that\nthe smallest to largest order gives the best results.", "published": "2021-04-02 13:18:52", "link": "http://arxiv.org/abs/2104.01037v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Taggers Fail to Learn, Parsers Need the Most", "abstract": "We present an error analysis of neural UPOS taggers to evaluate why using\ngold standard tags has such a large positive contribution to parsing\nperformance while using predicted UPOS tags either harms performance or offers\na negligible improvement. We evaluate what neural dependency parsers implicitly\nlearn about word types and how this relates to the errors taggers make to\nexplain the minimal impact using predicted tags has on parsers. We also present\na short analysis on what contexts result in reductions in tagging performance.\nWe then mask UPOS tags based on errors made by taggers to tease away the\ncontribution of UPOS tags which taggers succeed and fail to classify correctly\nand the impact of tagging errors.", "published": "2021-04-02 15:04:56", "link": "http://arxiv.org/abs/2104.01083v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical\n  Embeddings", "abstract": "Social media plays a pivotal role in disseminating news globally and acts as\na platform for people to express their opinions on various topics. A wide\nvariety of views accompanies COVID-19 vaccination drives across the globe,\noften colored by emotions, which change along with rising cases, approval of\nvaccines, and multiple factors discussed online. This study aims at analyzing\nthe temporal evolution of different Emotion categories: Hesitation, Rage,\nSorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine\nRollout, Misinformation, Health Effects, and Inequities as lexical categories\ncreated from Tweets belonging to five countries with vital vaccine roll-out\nprograms, namely, India, United States of America, Brazil, United Kingdom, and\nAustralia. We extracted a corpus of nearly 1.8 million Twitter posts related to\nCOVID-19 vaccination. Using cosine distance from selected seed words, we\nexpanded the vocabulary of each category and tracked the longitudinal change in\ntheir strength from June 2020 to April 2021. We used community detection\nalgorithms to find modules in positive correlation networks. Our findings\nsuggest that tweets expressing hesitancy towards vaccines contain the highest\nmentions of health-related effects in all countries. Our results indicated that\nthe patterns of hesitancy were variable across geographies and can help us\nlearn targeted interventions. We also observed a significant change in the\nlinear trends of categories like hesitation and contentment before and after\napproval of vaccines. Negative emotions like rage and sorrow gained the highest\nimportance in the alluvial diagram. They formed a significant module with all\nthe influencing factors in April 2021, when India observed the second wave of\nCOVID-19 cases. The relationship between Emotions and Influencing Factors was\nfound to be variable across the countries.", "published": "2021-04-02 16:13:16", "link": "http://arxiv.org/abs/2104.01131v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Coronavirus is a Bioweapon: Analysing Coronavirus Fact-Checked\n  Stories", "abstract": "The 2020 coronavirus pandemic has heightened the need to flag\ncoronavirus-related misinformation, and fact-checking groups have taken to\nverifying misinformation on the Internet. We explore stories reported by\nfact-checking groups PolitiFact, Poynter and Snopes from January to June 2020,\ncharacterising them into six story clusters before then analyse time-series and\nstory validity trends and the level of agreement across sites. We further break\ndown the story clusters into more granular story types by proposing a unique\nautomated method with a BERT classifier, which can be used to classify diverse\nstory sources, in both fact-checked stories and tweets.", "published": "2021-04-02 19:27:53", "link": "http://arxiv.org/abs/2104.01215v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Using GPT-2 to Create Synthetic Data to Improve the Prediction\n  Performance of NLP Machine Learning Classification Models", "abstract": "Classification Models use input data to predict the likelihood that the\nsubsequent input data will fall into predetermined categories. To perform\neffective classifications, these models require large datasets for training. It\nis becoming common practice to utilize synthetic data to boost the performance\nof Machine Learning Models. It is reported that Shell is using synthetic data\nto build models to detect problems that rarely occur; for example Shell created\nsynthetic data to help models to identify deteriorating oil lines. It is common\npractice for Machine Learning Practitioners to generate synthetic data by\nrotating, flipping, and cropping images to increase the volume of image data to\ntrain Convolutional Neural Networks. The purpose of this paper is to explore\ncreating and utilizing synthetic NLP data to improve the performance of Natural\nLanguage Processing Machine Learning Classification Models. In this paper I\nused a Yelp pizza restaurant reviews dataset and transfer learning to fine-tune\na pre-trained GPT-2 Transformer Model to generate synthetic pizza reviews data.\nI then combined this synthetic data with the original genuine data to create a\nnew joint dataset. The new combined model significantly outperformed the\noriginal model in accuracy and precision.", "published": "2021-04-02 20:20:42", "link": "http://arxiv.org/abs/2104.10658v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered\n  Language for Universal Phone Recognition Experiments", "abstract": "There is growing interest in ASR systems that can recognize phones in a\nlanguage-independent fashion. There is additionally interest in building\nlanguage technologies for low-resource and endangered languages. However, there\nis a paucity of realistic data that can be used to test such systems and\ntechnologies. This paper presents a publicly available, phonetically\ntranscribed corpus of 2255 utterances (words and short phrases) in the\nendangered Tangkhulic language East Tusom (no ISO 639-3 code), a Tibeto-Burman\nlanguage variety spoken mostly in India. Because the dataset is transcribed in\nterms of phones, rather than phonemes, it is a better match for universal phone\nrecognition systems than many larger (phonemically transcribed) datasets. This\npaper describes the dataset and the methodology used to produce it. It further\npresents basic benchmarks of state-of-the-art universal phone recognition\nsystems on the dataset as baselines for future experiments.", "published": "2021-04-02 00:26:10", "link": "http://arxiv.org/abs/2104.00824v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Acoustic Unit Discovery by Leveraging a\n  Language-Independent Subword Discriminative Feature Representation", "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD)\nfrom unlabeled speech data. Past studies usually proposed single-step\napproaches. We propose a two-stage approach: the first stage learns a\nsubword-discriminative feature representation and the second stage applies\nclustering to the learned representation and obtains phone-like clusters as the\ndiscovered acoustic units. In the first stage, a recently proposed method in\nthe task of unsupervised subword modeling is improved by replacing a\nmonolingual out-of-domain (OOD) ASR system with a multilingual one to create a\nsubword-discriminative representation that is more language-independent. In the\nsecond stage, segment-level k-means is adopted, and two methods to represent\nthe variable-length speech segments as fixed-dimension feature vectors are\ncompared. Experiments on a very low-resource Mboshi language corpus show that\nour approach outperforms state-of-the-art AUD in both normalized mutual\ninformation (NMI) and F-score. The multilingual ASR improved upon the\nmonolingual ASR in providing OOD phone labels and in estimating the phone\nboundaries. A comparison of our systems with and without knowing the\nground-truth phone boundaries showed a 16% NMI performance gap, suggesting that\nthe current approach can significantly benefit from improved phone boundary\nestimation.", "published": "2021-04-02 11:43:07", "link": "http://arxiv.org/abs/2104.00994v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised\n  Pre-Training", "abstract": "Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.", "published": "2021-04-02 12:53:15", "link": "http://arxiv.org/abs/2104.01027v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Type Prediction Systems", "abstract": "Inferring semantic types for entity mentions within text documents is an\nimportant asset for many downstream NLP tasks, such as Semantic Role Labelling,\nEntity Disambiguation, Knowledge Base Question Answering, etc. Prior works have\nmostly focused on supervised solutions that generally operate on relatively\nsmall-to-medium-sized type systems. In this work, we describe two systems aimed\nat predicting type information for the following two tasks, namely, a\nTypeSuggest module, an unsupervised system designed to predict types for a set\nof user-entered query terms, and an Answer Type prediction module, that\nprovides a solution for the task of determining the correct type of the answer\nexpected to a given query. Our systems generalize to arbitrary type systems of\nany sizes, thereby making it a highly appealing solution to extract type\ninformation at any granularity.", "published": "2021-04-02 19:16:42", "link": "http://arxiv.org/abs/2104.01207v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention Forcing for Machine Translation", "abstract": "Auto-regressive sequence-to-sequence models with attention mechanisms have\nachieved state-of-the-art performance in various tasks including Text-To-Speech\n(TTS) and Neural Machine Translation (NMT). The standard training approach,\nteacher forcing, guides a model with the reference output history. At inference\nstage, the generated output history must be used. This mismatch can impact\nperformance. However, it is highly challenging to train the model using the\ngenerated output. Several approaches have been proposed to address this\nproblem, normally by selectively using the generated output history. To make\ntraining stable, these approaches often require a heuristic schedule or an\nauxiliary classifier. This paper introduces attention forcing for NMT. This\napproach guides the model with the generated output history and reference\nattention, and can reduce the training-inference mismatch without a schedule or\na classifier. Attention forcing has been successful in TTS, but its application\nto NMT is more challenging, due to the discrete and multi-modal nature of the\noutput space. To tackle this problem, this paper adds a selection scheme to\nvanilla attention forcing, which automatically selects a suitable training\napproach for each pair of training data. Experiments show that attention\nforcing can improve the overall translation quality and the diversity of the\ntranslations.", "published": "2021-04-02 22:33:42", "link": "http://arxiv.org/abs/2104.01264v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer", "abstract": "This work describes an encoder pre-training procedure using frame-wise label\nto improve the training of streaming recurrent neural network transducer\n(RNN-T) model. Streaming RNN-T trained from scratch usually performs worse than\nnon-streaming RNN-T. Although it is common to address this issue through\npre-training components of RNN-T with other criteria or frame-wise alignment\nguidance, the alignment is not easily available in end-to-end manner. In this\nwork, frame-wise alignment, used to pre-train streaming RNN-T's encoder, is\ngenerated without using a HMM-based system. Therefore an all-neural framework\nequipping HMM-free encoder pre-training is constructed. This is achieved by\nexpanding the spikes of CTC model to their left/right blank frames, and two\nexpanding strategies are proposed. To our best knowledge, this is the first\nwork to simulate HMM-based frame-wise label using CTC model for pre-training.\nExperiments conducted on LibriSpeech and MLS English tasks show the proposed\npre-training procedure, compared with random initialization, reduces the WER by\nrelatively 5%~11% and the emission latency by 60 ms. Besides, the method is\nlexicon-free, so it is friendly to new languages without manually designed\nlexicon.", "published": "2021-04-02 16:14:11", "link": "http://arxiv.org/abs/2104.10764v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "INTERSPEECH 2021 ConferencingSpeech Challenge: Towards Far-field\n  Multi-Channel Speech Enhancement for Video Conferencing", "abstract": "The ConferencingSpeech 2021 challenge is proposed to stimulate research on\nfar-field multi-channel speech enhancement for video conferencing. The\nchallenge consists of two separate tasks: 1) Task 1 is multi-channel speech\nenhancement with single microphone array and focusing on practical application\nwith real-time requirement and 2) Task 2 is multi-channel speech enhancement\nwith multiple distributed microphone arrays, which is a non-real-time track and\ndoes not have any constraints so that participants could explore any algorithms\nto obtain high speech quality. Targeting the real video conferencing room\napplication, the challenge database was recorded from real speakers and all\nrecording facilities were located by following the real setup of conferencing\nroom. In this challenge, we open-sourced the list of open source clean speech\nand noise datasets, simulation scripts, and a baseline system for participants\nto develop their own system. The final ranking of the challenge will be decided\nby the subjective evaluation which is performed using Absolute Category Ratings\n(ACR) to estimate Mean Opinion Score (MOS), speech MOS (S-MOS), and noise MOS\n(N-MOS). This paper describes the challenge, tasks, datasets, and subjective\nevaluation. The baseline system which is a complex ratio mask based neural\nnetwork and its experimental results are also presented.", "published": "2021-04-02 09:56:59", "link": "http://arxiv.org/abs/2104.00960v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Audio-Based Deep Learning Framework For BBC Television Programme\n  Classification", "abstract": "This paper proposes a deep learning framework for classification of BBC\ntelevision programmes using audio. The audio is firstly transformed into\nspectrograms, which are fed into a pre-trained convolutional Neural Network\n(CNN), obtaining predicted probabilities of sound events occurring in the audio\nrecording. Statistics for the predicted probabilities and detected sound events\nare then calculated to extract discriminative features representing the\ntelevision programmes. Finally, the embedded features extracted are fed into a\nclassifier for classifying the programmes into different genres. Our\nexperiments are conducted over a dataset of 6,160 programmes belonging to nine\ngenres labelled by the BBC. We achieve an average classification accuracy of\n93.7% over 14-fold cross validation. This demonstrates the efficacy of the\nproposed framework for the task of audio-based classification of television\nprogrammes.", "published": "2021-04-02 17:26:00", "link": "http://arxiv.org/abs/2104.01161v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality\n  Assessment", "abstract": "The objective speech quality assessment is usually conducted by comparing\nreceived speech signal with its clean reference, while human beings are capable\nof evaluating the speech quality without any reference, such as in the mean\nopinion score (MOS) tests. Non-intrusive speech quality assessment has\nattracted much attention recently due to the lack of access to clean reference\nsignals for objective evaluations in real scenarios. In this paper, we propose\na novel non-intrusive speech quality measurement model, MetricNet, which\nleverages label distribution learning and joint speech reconstruction learning\nto achieve significantly improved performance compared to the existing\nnon-intrusive speech quality measurement models. We demonstrate that the\nproposed approach yields promisingly high correlation to the intrusive\nobjective evaluation of speech quality on clean, noisy and processed speech\ndata.", "published": "2021-04-02 20:03:35", "link": "http://arxiv.org/abs/2104.01227v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model", "abstract": "In this paper, we propose SC-GlowTTS: an efficient zero-shot multi-speaker\ntext-to-speech model that improves similarity for speakers unseen during\ntraining. We propose a speaker-conditional architecture that explores a\nflow-based decoder that works in a zero-shot scenario. As text encoders, we\nexplore a dilated residual convolutional-based encoder, gated\nconvolutional-based encoder, and transformer-based encoder. Additionally, we\nhave shown that adjusting a GAN-based vocoder for the spectrograms predicted by\nthe TTS model on the training dataset can significantly improve the similarity\nand speech quality for new speakers. Our model converges using only 11\nspeakers, reaching state-of-the-art results for similarity with new speakers,\nas well as high speech quality.", "published": "2021-04-02 22:31:45", "link": "http://arxiv.org/abs/2104.05557v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Assem-VC: Realistic Voice Conversion by Assembling Modern Speech\n  Synthesis Techniques", "abstract": "Recent works on voice conversion (VC) focus on preserving the rhythm and the\nintonation as well as the linguistic content. To preserve these features from\nthe source, we decompose current non-parallel VC systems into two encoders and\none decoder. We analyze each module with several experiments and reassemble the\nbest components to propose Assem-VC, a new state-of-the-art any-to-many\nnon-parallel VC system. We also examine that PPG and Cotatron features are\nspeaker-dependent, and attempt to remove speaker identity with adversarial\ntraining. Code and audio samples are available at\nhttps://github.com/mindslab-ai/assem-vc.", "published": "2021-04-02 08:18:05", "link": "http://arxiv.org/abs/2104.00931v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "COVID-19 Detection in Cough, Breath and Speech using Deep Transfer\n  Learning and Bottleneck Features", "abstract": "We present an experimental investigation into the effectiveness of transfer\nlearning and bottleneck feature extraction in detecting COVID-19 from audio\nrecordings of cough, breath and speech.\n  This type of screening is non-contact, does not require specialist medical\nexpertise or laboratory facilities and can be deployed on inexpensive consumer\nhardware.\n  We use datasets that contain recordings of coughing, sneezing, speech and\nother noises, but do not contain COVID-19 labels, to pre-train three deep\nneural networks: a CNN, an LSTM and a Resnet50.\n  These pre-trained networks are subsequently either fine-tuned using smaller\ndatasets of coughing with COVID-19 labels in the process of transfer learning,\nor are used as bottleneck feature extractors.\n  Results show that a Resnet50 classifier trained by this transfer learning\nprocess delivers optimal or near-optimal performance across all datasets\nachieving areas under the receiver operating characteristic (ROC AUC) of 0.98,\n0.94 and 0.92 respectively for all three sound classes (coughs, breaths and\nspeech).\n  This indicates that coughs carry the strongest COVID-19 signature, followed\nby breath and speech.\n  Our results also show that applying transfer learning and extracting\nbottleneck features using the larger datasets without COVID-19 labels led not\nonly to improve performance, but also to minimise the standard deviation of the\nclassifier AUCs among the outer folds of the leave-$p$-out cross-validation,\nindicating better generalisation.\n  We conclude that deep transfer learning and bottleneck feature extraction can\nimprove COVID-19 cough, breath and speech audio classification, yielding\nautomatic classifiers with higher accuracy.", "published": "2021-04-02 23:21:24", "link": "http://arxiv.org/abs/2104.02477v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation\n  of Teacher Ensembles for Spoken Command Classification", "abstract": "We propose using an adversarial autoencoder (AAE) to replace generative\nadversarial network (GAN) in the private aggregation of teacher ensembles\n(PATE), a solution for ensuring differential privacy in speech applications.\nThe AAE architecture allows us to obtain good synthetic speech leveraging upon\na discriminative training of latent vectors. Such synthetic speech is used to\nbuild a privacy-preserving classifier when non-sensitive data is not\nsufficiently available in the public domain. This classifier follows the PATE\nscheme that uses an ensemble of noisy outputs to label the synthetic samples\nand guarantee $\\varepsilon$-differential privacy (DP) on its derived\nclassifiers. Our proposed framework thus consists of an AAE-based generator and\na PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands\nDataset Version II, the proposed PATE-AAE improves the average classification\naccuracy by +$2.11\\%$ and +$6.60\\%$, respectively, when compared with\nalternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while\nmaintaining a strong level of privacy target at $\\varepsilon$=0.01 with a fixed\n$\\delta$=10$^{-5}$.", "published": "2021-04-02 23:10:57", "link": "http://arxiv.org/abs/2104.01271v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
