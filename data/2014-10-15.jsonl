{"title": "Learning Distributed Word Representations for Natural Logic Reasoning", "abstract": "Natural logic offers a powerful relational conception of meaning that is a\nnatural counterpart to distributed semantic representations, which have proven\nvaluable in a wide range of sophisticated language tasks. However, it remains\nan open question whether it is possible to train distributed representations to\nsupport the rich, diverse logical reasoning captured by natural logic. We\naddress this question using two neural network-based models for learning\nembeddings: plain neural networks and neural tensor networks. Our experiments\nevaluate the models' ability to learn the basic algebra of natural logic\nrelations from simulated data and from the WordNet noun graph. The overall\npositive results are promising for the future of learned distributed\nrepresentations in the applied modeling of logical semantics.", "published": "2014-10-15 19:27:10", "link": "http://arxiv.org/abs/1410.4176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory Networks", "abstract": "We describe a new class of learning models called memory networks. Memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. The long-term memory can be\nread and written to, with the goal of using it for prediction. We investigate\nthese models in the context of question answering (QA) where the long-term\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\nmore complex, toy task generated from a simulated world. In the latter, we show\nthe reasoning power of such models by chaining multiple supporting sentences to\nanswer questions that require understanding the intension of verbs.", "published": "2014-10-15 03:13:18", "link": "http://arxiv.org/abs/1410.3916v11", "categories": ["cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.AI"}
