{"title": "Exploring Segment Representations for Neural Segmentation Models", "abstract": "Many natural language processing (NLP) tasks can be generalized into\nsegmentation problem. In this paper, we combine semi-CRF with neural network to\nsolve NLP segmentation tasks. Our model represents a segment both by composing\nthe input units and embedding the entire segment. We thoroughly study different\ncomposition functions and different segment embeddings. We conduct extensive\nexperiments on two typical segmentation tasks: named entity recognition (NER)\nand Chinese word segmentation (CWS). Experimental results show that our neural\nsemi-CRF model benefits from representing the entire segment and achieves the\nstate-of-the-art performance on CWS benchmark dataset and competitive results\non the CoNLL03 dataset.", "published": "2016-04-19 10:08:49", "link": "http://arxiv.org/abs/1604.05499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep\n  Convolutional Network for Answer Selection", "abstract": "Recent works using artificial neural networks based on distributed word\nrepresentation greatly boost performance on various natural language processing\ntasks, especially the answer selection problem. Nevertheless, most of the\nprevious works used deep learning methods (like LSTM-RNN, CNN, etc.) only to\ncapture semantic representation of each sentence separately, without\nconsidering the interdependence between each other. In this paper, we propose a\nnovel end-to-end learning framework which constitutes deep convolutional neural\nnetwork based on multi-modal similarity metric learning (M$^2$S-Net) on\npairwise tokens. The proposed model demonstrates its performance by surpassing\nprevious state-of-the-art systems on the answer selection benchmark, i.e.,\nTREC-QA dataset, in both MAP and MRR metrics.", "published": "2016-04-19 11:09:20", "link": "http://arxiv.org/abs/1604.05519v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Attentive Neural Architecture for Fine-grained Entity Type\n  Classification", "abstract": "In this work we propose a novel attention-based neural network model for the\ntask of fine-grained entity type classification that unlike previously proposed\nmodels recursively composes representations of entity mention contexts. Our\nmodel achieves state-of-the-art performance with 74.94% loose micro F1-score on\nthe well-established FIGER dataset, a relative improvement of 2.59%. We also\ninvestigate the behavior of the attention mechanism of our model and observe\nthat it can learn contextual linguistic expressions that indicate the\nfine-grained category memberships of an entity.", "published": "2016-04-19 11:39:53", "link": "http://arxiv.org/abs/1604.05525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term\n  Memory Models and Auxiliary Loss", "abstract": "Bidirectional long short-term memory (bi-LSTM) networks have recently proven\nsuccessful for various NLP sequence modeling tasks, but little is known about\ntheir reliance to input representations, target languages, data set size, and\nlabel noise. We address these issues and evaluate bi-LSTMs with word,\ncharacter, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to\ntraditional POS taggers across languages and data sizes. We also present a\nnovel bi-LSTM model, which combines the POS tagging loss function with an\nauxiliary loss function that accounts for rare words. The model obtains\nstate-of-the-art performance across 22 languages, and works especially well for\nmorphologically complex languages. Our analysis suggests that bi-LSTMs are less\nsensitive to training data size and label corruptions (at small noise levels)\nthan previously assumed.", "published": "2016-04-19 11:53:09", "link": "http://arxiv.org/abs/1604.05529v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic and semantic classification of verb arguments using\n  dependency-based and rich semantic features", "abstract": "Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15,\naimed at producing a system that can aid lexicographers in their efforts to\nbuild a dictionary of meanings for English verbs using the CPA annotation\nprocess. CPA parsing is one of the subtasks which this annotation process is\nmade of and it is the focus of this report. A supervised machine-learning\napproach has been implemented, in which syntactic features derived from parse\ntrees and semantic features derived from WordNet and word embeddings are used.\nIt is shown that this approach performs well, even with the data sparsity\nissues that characterize the dataset, and can obtain better results than other\nsystem by a margin of about 4% f-score.", "published": "2016-04-19 20:59:32", "link": "http://arxiv.org/abs/1604.05747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Rating Behaviour and Predicting Ratings by Identifying\n  Representative Users", "abstract": "Online user reviews describing various products and services are now abundant\non the web. While the information conveyed through review texts and ratings is\neasily comprehensible, there is a wealth of hidden information in them that is\nnot immediately obvious. In this study, we unlock this hidden value behind user\nreviews to understand the various dimensions along which users rate products.\nWe learn a set of users that represent each of these dimensions and use their\nratings to predict product ratings. Specifically, we work with restaurant\nreviews to identify users whose ratings are influenced by dimensions like\n'Service', 'Atmosphere' etc. in order to predict restaurant ratings and\nunderstand the variation in rating behaviour across different cuisines. While\nprevious approaches to obtaining product ratings require either a large number\nof user ratings or a few review texts, we show that it is possible to predict\nratings with few user ratings and no review text. Our experiments show that our\napproach outperforms other conventional methods by 16-27% in terms of RMSE.", "published": "2016-04-19 08:31:23", "link": "http://arxiv.org/abs/1604.05468v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
