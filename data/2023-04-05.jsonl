{"title": "How to Design Translation Prompts for ChatGPT: An Empirical Study", "abstract": "The recently released ChatGPT has demonstrated surprising abilities in\nnatural language understanding and natural language generation. Machine\ntranslation relies heavily on the abilities of language understanding and\ngeneration. Thus, in this paper, we explore how to assist machine translation\nwith ChatGPT. We adopt several translation prompts on a wide range of\ntranslations. Our experimental results show that ChatGPT with designed\ntranslation prompts can achieve comparable or better performance over\ncommercial translation systems for high-resource language translations. We\nfurther evaluate the translation quality using multiple references, and ChatGPT\nachieves superior performance compared to commercial systems. We also conduct\nexperiments on domain-specific translations, the final results show that\nChatGPT is able to comprehend the provided domain keyword and adjust\naccordingly to output proper translations. At last, we perform few-shot prompts\nthat show consistent improvement across different base prompts. Our work\nprovides empirical evidence that ChatGPT still has great potential in\ntranslations.", "published": "2023-04-05 01:17:59", "link": "http://arxiv.org/abs/2304.02182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personality-aware Human-centric Multimodal Reasoning: A New Task,\n  Dataset and Baselines", "abstract": "Personality traits, emotions, and beliefs shape individuals' behavioral\nchoices and decision-making processes. However, for one thing, the affective\ncomputing community normally focused on predicting personality traits but\noverlooks their application in behavior prediction. For another, the multimodal\nreasoning task emphasized the prediction of future states and behaviors but\noften neglected the incorporation of individual personality traits. In this\nwork, we introduce a new task called Personality-aware Human-centric Multimodal\nReasoning (PHMR) (T1), with the goal of forecasting the future behavior of a\nparticular individual using multimodal information from past instances, while\nintegrating personality factors. We accordingly construct a new dataset based\non six television shows, encompassing 225 characters and 12k samples. To\nestablish a benchmark for the task, we propose seven baseline methods: three\nadapted from related tasks, two pre-trained model, and two multimodal large\nlanguage models. The experimental results demonstrate that incorporating\npersonality traits enhances human-centric multimodal reasoning performance. To\nfurther solve the lack of personality annotation in real-life scenes, we\nintroduce an extension task called Personality-predicted Human-centric\nMultimodal Reasoning task (T2) along with the corresponding dataset and method.\nWe will make our dataset and code available on GitHub.", "published": "2023-04-05 09:09:10", "link": "http://arxiv.org/abs/2304.02313v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParroT: Translating during Chat using Large Language Models tuned with\n  Human Translation and Feedback", "abstract": "Large language models (LLMs) like ChatGPT have exhibited remarkable abilities\non a wide range of natural language processing~(NLP) tasks, including various\nmachine translation abilities accomplished during chat. However, these models\nare only accessible through restricted APIs, which creates barriers to new\nresearch and advancements in the field. Therefore, we propose ParroT, a\nframework to enhance and regulate the translation abilities during chat based\non open-source LLMs (e.g., LLaMA), human-written translation and feedback data.\nSpecifically, ParroT reformulates translation data into the\ninstruction-following style, and introduces a \"$\\mathbf{Hint}$\" field for\nincorporating extra requirements to regulate the translation process.\nAccordingly, we propose three instruction types for finetuning ParroT models,\nincluding translation instruction, contrastive instruction, and error-guided\ninstruction. Experiments on Flores subsets and WMT22 test sets suggest that\ntranslation instruction improves the translation performance of vanilla LLMs\nsignificantly while error-guided instruction can lead to further improvement,\nwhich demonstrates the importance of learning from low-quality translations\nannotated by humans. We also demonstrate the potential of automatic evaluation\ntools in providing quality information of translations, when constructing\nerror-guided instructions for directions that lack human annotation data.\nPlease refer to our Github project for more implementation details:\nhttps://github.com/wxjiao/ParroT", "published": "2023-04-05 13:12:00", "link": "http://arxiv.org/abs/2304.02426v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PWESuite: Phonetic Word Embeddings and Tasks They Facilitate", "abstract": "Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.", "published": "2023-04-05 16:03:42", "link": "http://arxiv.org/abs/2304.02541v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human-like Summarization Evaluation with ChatGPT", "abstract": "Evaluating text summarization is a challenging problem, and existing\nevaluation metrics are far from satisfactory. In this study, we explored\nChatGPT's ability to perform human-like summarization evaluation using four\nhuman evaluation methods on five datasets. We found that ChatGPT was able to\ncomplete annotations relatively smoothly using Likert scale scoring, pairwise\ncomparison, Pyramid, and binary factuality evaluation. Additionally, it\noutperformed commonly used automatic evaluation metrics on some datasets.\nFurthermore, we discussed the impact of different prompts, compared its\nperformance with that of human evaluation, and analyzed the generated\nexplanations and invalid responses.", "published": "2023-04-05 16:17:32", "link": "http://arxiv.org/abs/2304.02554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Explainable AI Writing Assistants for Non-native English\n  Speakers", "abstract": "We highlight the challenges faced by non-native speakers when using AI\nwriting assistants to paraphrase text. Through an interview study with 15\nnon-native English speakers (NNESs) with varying levels of English proficiency,\nwe observe that they face difficulties in assessing paraphrased texts generated\nby AI writing assistants, largely due to the lack of explanations accompanying\nthe suggested paraphrases. Furthermore, we examine their strategies to assess\nAI-generated texts in the absence of such explanations. Drawing on the needs of\nNNESs identified in our interview, we propose four potential user interfaces to\nenhance the writing experience of NNESs using AI writing assistants. The\nproposed designs focus on incorporating explanations to better support NNESs in\nunderstanding and evaluating the AI-generated paraphrasing suggestions.", "published": "2023-04-05 17:51:36", "link": "http://arxiv.org/abs/2304.02625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance of Data Augmentation Methods for Brazilian Portuguese Text\n  Classification", "abstract": "Improving machine learning performance while increasing model generalization\nhas been a constantly pursued goal by AI researchers. Data augmentation\ntechniques are often used towards achieving this target, and most of its\nevaluation is made using English corpora. In this work, we took advantage of\ndifferent existing data augmentation methods to analyze their performances\napplied to text classification problems using Brazilian Portuguese corpora. As\na result, our analysis shows some putative improvements in using some of these\ntechniques; however, it also suggests further exploitation of language bias and\nnon-English text data scarcity.", "published": "2023-04-05 23:13:37", "link": "http://arxiv.org/abs/2304.02785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Classification of Legal Document Pages", "abstract": "For many business applications that require the processing, indexing, and\nretrieval of professional documents such as legal briefs (in PDF format etc.),\nit is often essential to classify the pages of any given document into their\ncorresponding types beforehand. Most existing studies in the field of document\nimage classification either focus on single-page documents or treat multiple\npages in a document independently. Although in recent years a few techniques\nhave been proposed to exploit the context information from neighboring pages to\nenhance document page classification, they typically cannot be utilized with\nlarge pre-trained language models due to the constraint on input length. In\nthis paper, we present a simple but effective approach that overcomes the above\nlimitation. Specifically, we enhance the input with extra tokens carrying\nsequential information about previous pages - introducing recurrence - which\nenables the usage of pre-trained Transformer models like BERT for context-aware\npage classification. Our experiments conducted on two legal datasets in English\nand Portuguese respectively show that the proposed approach can significantly\nimprove the performance of document page classification compared to the\nnon-recurrent setup as well as the other context-aware baselines.", "published": "2023-04-05 23:14:58", "link": "http://arxiv.org/abs/2304.02787v2", "categories": ["cs.CL", "I.2"], "primary_category": "cs.CL"}
{"title": "Document-Level Machine Translation with Large Language Models", "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.", "published": "2023-04-05 03:49:06", "link": "http://arxiv.org/abs/2304.02210v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Master Key: Unlocking the Secrets of Materials\n  Science with GPT", "abstract": "The amount of data has growing significance in exploring cutting-edge\nmaterials and a number of datasets have been generated either by hand or\nautomated approaches. However, the materials science field struggles to\neffectively utilize the abundance of data, especially in applied disciplines\nwhere materials are evaluated based on device performance rather than their\nproperties. This article presents a new natural language processing (NLP) task\ncalled structured information inference (SII) to address the complexities of\ninformation extraction at the device level in materials science. We\naccomplished this task by tuning GPT-3 on an existing perovskite solar cell\nFAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%\nF1-score and extended the dataset with data published since its release. The\nproduced data is formatted and normalized, enabling its direct utilization as\ninput in subsequent data analysis. This feature empowers materials scientists\nto develop models by selecting high-quality review articles within their\ndomain. Additionally, we designed experiments to predict the electrical\nperformance of solar cells and design materials or devices with targeted\nparameters using large language models (LLMs). Our results demonstrate\ncomparable performance to traditional machine learning methods without feature\nselection, highlighting the potential of LLMs to acquire scientific knowledge\nand design new materials akin to materials scientists.", "published": "2023-04-05 04:01:52", "link": "http://arxiv.org/abs/2304.02213v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ericson: An Interactive Open-Domain Conversational Search Agent", "abstract": "Open-domain conversational search (ODCS) aims to provide valuable, up-to-date\ninformation, while maintaining natural conversations to help users refine and\nultimately answer information needs. However, creating an effective and robust\nODCS agent is challenging. In this paper, we present a fully functional ODCS\nsystem, Ericson, which includes state-of-the-art question answering and\ninformation retrieval components, as well as intent inference and dialogue\nmanagement models for proactive question refinement and recommendations. Our\nsystem was stress-tested in the Amazon Alexa Prize, by engaging in live\nconversations with thousands of Alexa users, thus providing empirical basis for\nthe analysis of the ODCS system in real settings. Our interaction data analysis\nrevealed that accurate intent classification, encouraging user engagement, and\ncareful proactive recommendations contribute most to the users satisfaction.\nOur study further identifies limitations of the existing search techniques, and\ncan serve as a building block for the next generation of ODCS agents.", "published": "2023-04-05 05:28:31", "link": "http://arxiv.org/abs/2304.02233v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "primary_category": "cs.CL"}
{"title": "Disentangling Structure and Style: Political Bias Detection in News by\n  Inducing Document Hierarchy", "abstract": "We address an important gap in detecting political bias in news articles.\nPrevious works that perform document classification can be influenced by the\nwriting style of each news outlet, leading to overfitting and limited\ngeneralizability. Our approach overcomes this limitation by considering both\nthe sentence-level semantics and the document-level rhetorical structure,\nresulting in a more robust and style-agnostic approach to detecting political\nbias in news articles. We introduce a novel multi-head hierarchical attention\nmodel that effectively encodes the structure of long documents through a\ndiverse ensemble of attention heads. While journalism follows a formalized\nrhetorical structure, the writing style may vary by news outlet. We demonstrate\nthat our method overcomes this domain dependency and outperforms previous\napproaches for robustness and accuracy. Further analysis and human evaluation\ndemonstrate the ability of our model to capture common discourse structures in\njournalism. Our code is available at:\nhttps://github.com/xfactlab/emnlp2023-Document-Hierarchy", "published": "2023-04-05 06:35:41", "link": "http://arxiv.org/abs/2304.02247v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Multimodal Entity and Relation Extraction with Variational\n  Information Bottleneck", "abstract": "This paper studies the multimodal named entity recognition (MNER) and\nmultimodal relation extraction (MRE), which are important for multimedia social\nplatform analysis. The core of MNER and MRE lies in incorporating evident\nvisual information to enhance textual semantics, where two issues inherently\ndemand investigations. The first issue is modality-noise, where the\ntask-irrelevant information in each modality may be noises misleading the task\nprediction. The second issue is modality-gap, where representations from\ndifferent modalities are inconsistent, preventing from building the semantic\nalignment between the text and image. To address these issues, we propose a\nnovel method for MNER and MRE by Multi-Modal representation learning with\nInformation Bottleneck (MMIB). For the first issue, a refinement-regularizer\nprobes the information-bottleneck principle to balance the predictive evidence\nand noisy information, yielding expressive representations for prediction. For\nthe second issue, an alignment-regularizer is proposed, where a mutual\ninformation-based item works in a contrastive manner to regularize the\nconsistent text-image representations. To our best knowledge, we are the first\nto explore variational IB estimation for MNER and MRE. Experiments show that\nMMIB achieves the state-of-the-art performances on three public benchmarks.", "published": "2023-04-05 09:32:25", "link": "http://arxiv.org/abs/2304.02328v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Evaluation of ChatGPT Family of Models for Biomedical Reasoning and\n  Classification", "abstract": "Recent advances in large language models (LLMs) have shown impressive ability\nin biomedical question-answering, but have not been adequately investigated for\nmore specific biomedical applications. This study investigates the performance\nof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical\ntasks beyond question-answering. Because no patient data can be passed to the\nOpenAI API public interface, we evaluated model performance with over 10000\nsamples as proxies for two fundamental tasks in the clinical domain -\nclassification and reasoning. The first task is classifying whether statements\nof clinical and policy recommendations in scientific literature constitute\nhealth advice. The second task is causal relation detection from the biomedical\nliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)\nwith logistic regression, and fine-tuned BioBERT models. Despite the excitement\naround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks\nremained the best strategy. The simple BoW model performed on par with the most\ncomplex LLM prompting. Prompt engineering required significant investment.", "published": "2023-04-05 15:11:25", "link": "http://arxiv.org/abs/2304.02496v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Summarization: Designing AI Support for Real-World Expository\n  Writing Tasks", "abstract": "Large language models have introduced exciting new opportunities and\nchallenges in designing and developing new AI-assisted writing support tools.\nRecent work has shown that leveraging this new technology can transform writing\nin many scenarios such as ideation during creative writing, editing support,\nand summarization. However, AI-supported expository writing--including\nreal-world tasks like scholars writing literature reviews or doctors writing\nprogress notes--is relatively understudied. In this position paper, we argue\nthat developing AI supports for expository writing has unique and exciting\nresearch challenges and can lead to high real-world impacts. We characterize\nexpository writing as evidence-based and knowledge-generating: it contains\nsummaries of external documents as well as new information or knowledge. It can\nbe seen as the product of authors' sensemaking process over a set of source\ndocuments, and the interplay between reading, reflection, and writing opens up\nnew opportunities for designing AI support. We sketch three components for AI\nsupport design and discuss considerations for future research.", "published": "2023-04-05 17:47:11", "link": "http://arxiv.org/abs/2304.02623v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence\n  Models for Improved Inference Efficiency", "abstract": "Sequence-to-sequence language models can be used to produce abstractive\nsummaries which are coherent, relevant, and concise. Still, model sizes can\nmake deployment in latency-sensitive or web-scale implementations difficult.\nThis paper studies the relationship between model size, structured pruning,\ninference efficiency, and summarization accuracy on widely used summarization\ndatasets. We show that model accuracy is tied to the encoder size while\ninference efficiency is connected to the decoder. Using asymmetric pruning can\nlead to nearly 3x improvement in inference latency with ~1 point loss in\nRouge-2. Moreover, we find both the average degradation and the role of\nasymmetry to be consistent across model sizes and variations in datasets.", "published": "2023-04-05 19:44:20", "link": "http://arxiv.org/abs/2304.02721v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Saudi Privacy Policy Dataset", "abstract": "This paper introduces the Saudi Privacy Policy Dataset, a diverse compilation\nof Arabic privacy policies from various sectors in Saudi Arabia, annotated\naccording to the 10 principles of the Personal Data Protection Law (PDPL); the\nPDPL was established to be compatible with General Data Protection Regulation\n(GDPR); one of the most comprehensive data regulations worldwide. Data were\ncollected from multiple sources, including the Saudi Central Bank, the Saudi\nArabia National United Platform, the Council of Health Insurance, and general\nwebsites using Google and Wikipedia. The final dataset includes 1,000 websites\nbelonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus size\nof 8,353 KB. The annotated dataset offers significant reuse potential for\nassessing privacy policy compliance, benchmarking privacy practices across\nindustries, and developing automated tools for monitoring adherence to data\nprotection regulations. By providing a comprehensive and annotated dataset of\nprivacy policies, this paper aims to facilitate further research and\ndevelopment in the areas of privacy policy analysis, natural language\nprocessing, and machine learning applications related to privacy and data\nprotection, while also serving as an essential resource for researchers,\npolicymakers, and industry professionals interested in understanding and\npromoting compliance with privacy regulations in Saudi Arabia.", "published": "2023-04-05 21:40:37", "link": "http://arxiv.org/abs/2304.02757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Impact of Voice Anonymization on Speech Diagnostic Applications:\n  a Case Study on COVID-19 Detection", "abstract": "With advances seen in deep learning, voice-based applications are burgeoning,\nranging from personal assistants, affective computing, to remote disease\ndiagnostics. As the voice contains both linguistic and para-linguistic\ninformation (e.g., vocal pitch, intonation, speech rate, loudness), there is\ngrowing interest in voice anonymization to preserve speaker privacy and\nidentity. Voice privacy challenges have emerged over the last few years and\nfocus has been placed on removing speaker identity while keeping linguistic\ncontent intact. For affective computing and disease monitoring applications,\nhowever, the para-linguistic content may be more critical. Unfortunately, the\neffects that anonymization may have on these systems are still largely unknown.\nIn this paper, we fill this gap and focus on one particular health monitoring\napplication: speech-based COVID-19 diagnosis. We test three anonymization\nmethods and their impact on five different state-of-the-art COVID-19 diagnostic\nsystems using three public datasets. We validate the effectiveness of the\nanonymization methods, compare their computational complexity, and quantify the\nimpact across different testing scenarios for both within- and across-dataset\nconditions. Additionally, we provided a comprehensive evaluation of the\nimportance of different speech aspects for diagnostics and showed how they are\naffected by different types of anonymizers. Lastly, we show the benefits of\nusing anonymized external data as a data augmentation tool to help recover some\nof the COVID-19 diagnostic accuracy loss seen with anonymization.", "published": "2023-04-05 01:09:58", "link": "http://arxiv.org/abs/2304.02181v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic\n  Complexity in Verb Acquisition", "abstract": "Children typically learn the meanings of nouns earlier than the meanings of\nverbs. However, it is unclear whether this asymmetry is a result of complexity\nin the visual structure of categories in the world to which language refers,\nthe structure of language itself, or the interplay between the two sources of\ninformation. We quantitatively test these three hypotheses regarding early verb\nlearning by employing visual and linguistic representations of words sourced\nfrom large-scale pre-trained artificial neural networks. Examining the\nstructure of both visual and linguistic embedding spaces, we find, first, that\nthe representation of verbs is generally more variable and less discriminable\nwithin domain than the representation of nouns. Second, we find that if only\none learning instance per category is available, visual and linguistic\nrepresentations are less well aligned in the verb system than in the noun\nsystem. However, in parallel with the course of human language development, if\nmultiple learning instances per category are available, visual and linguistic\nrepresentations become almost as well aligned in the verb system as in the noun\nsystem. Third, we compare the relative contributions of factors that may\npredict learning difficulty for individual words. A regression analysis reveals\nthat visual variability is the strongest factor that internally drives verb\nlearning, followed by visual-linguistic alignment and linguistic variability.\nBased on these results, we conclude that verb acquisition is influenced by all\nthree sources of complexity, but that the variability of visual structure poses\nthe most significant challenge for verb learning.", "published": "2023-04-05 15:08:21", "link": "http://arxiv.org/abs/2304.02492v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bengali Fake Review Detection using Semi-supervised Generative\n  Adversarial Networks", "abstract": "This paper investigates the potential of semi-supervised Generative\nAdversarial Networks (GANs) to fine-tune pretrained language models in order to\nclassify Bengali fake reviews from real reviews with a few annotated data. With\nthe rise of social media and e-commerce, the ability to detect fake or\ndeceptive reviews is becoming increasingly important in order to protect\nconsumers from being misled by false information. Any machine learning model\nwill have trouble identifying a fake review, especially for a low resource\nlanguage like Bengali. We have demonstrated that the proposed semi-supervised\nGAN-LM architecture (generative adversarial network on top of a pretrained\nlanguage model) is a viable solution in classifying Bengali fake reviews as the\nexperimental results suggest that even with only 1024 annotated samples,\nBanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and\na f1-score of 84.89% outperforming other pretrained language models -\nBanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and\n10% respectively in terms of accuracy. The experiments were conducted on a\nmanually labeled food review dataset consisting of total 6014 real and fake\nreviews collected from various social media groups. Researchers that are\nexperiencing difficulty recognizing not just fake reviews but other\nclassification issues owing to a lack of labeled data may find a solution in\nour proposed methodology.", "published": "2023-04-05 20:40:09", "link": "http://arxiv.org/abs/2304.02739v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conceptual structure coheres in human cognition but not in large\n  language models", "abstract": "Neural network models of language have long been used as a tool for\ndeveloping hypotheses about conceptual representation in the mind and brain.\nFor many years, such use involved extracting vector-space representations of\nwords and using distances among these to predict or understand human behavior\nin various semantic tasks. Contemporary large language models (LLMs), however,\nmake it possible to interrogate the latent structure of conceptual\nrepresentations using experimental methods nearly identical to those commonly\nused with human participants. The current work utilizes three common techniques\nborrowed from cognitive psychology to estimate and compare the structure of\nconcepts in humans and a suite of LLMs. In humans, we show that conceptual\nstructure is robust to differences in culture, language, and method of\nestimation. Structures estimated from LLM behavior, while individually fairly\nconsistent with those estimated from human behavior, vary much more depending\nupon the particular task used to generate responses--across tasks, estimates of\nconceptual structure from the very same model cohere less with one another than\ndo human structure estimates. These results highlight an important difference\nbetween contemporary LLMs and human cognition, with implications for\nunderstanding some fundamental limitations of contemporary machine language.", "published": "2023-04-05 21:27:01", "link": "http://arxiv.org/abs/2304.02754v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Application of Transformers based methods in Electronic Medical Records:\n  A Systematic Literature Review", "abstract": "The combined growth of available data and their unstructured nature has\nreceived increased interest in natural language processing (NLP) techniques to\nmake value of these data assets since this format is not suitable for\nstatistical analysis. This work presents a systematic literature review of\nstate-of-the-art advances using transformer-based methods on electronic medical\nrecords (EMRs) in different NLP tasks. To the best of our knowledge, this work\nis unique in providing a comprehensive review of research on transformer-based\nmethods for NLP applied to the EMR field. In the initial query, 99 articles\nwere selected from three public databases and filtered into 65 articles for\ndetailed analysis. The papers were analyzed with respect to the business\nproblem, NLP task, models and techniques, availability of datasets,\nreproducibility of modeling, language, and exchange format. The paper presents\nsome limitations of current research and some recommendations for further\nresearch.", "published": "2023-04-05 22:19:42", "link": "http://arxiv.org/abs/2304.02768v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for\n  Passage Retrieval", "abstract": "Growing techniques have been emerging to improve the performance of passage\nretrieval. As an effective representation bottleneck pretraining technique, the\ncontextual masked auto-encoder utilizes contextual embedding to assist in the\nreconstruction of passages. However, it only uses a single auto-encoding\npre-task for dense representation pre-training. This study brings multi-view\nmodeling to the contextual masked auto-encoder. Firstly, multi-view\nrepresentation utilizes both dense and sparse vectors as multi-view\nrepresentations, aiming to capture sentence semantics from different aspects.\nMoreover, multiview decoding paradigm utilizes both autoencoding and\nauto-regressive decoders in representation bottleneck pre-training, aiming to\nprovide both reconstructive and generative signals for better contextual\nrepresentation pretraining. We refer to this multi-view pretraining method as\nCoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective\nand robust on large-scale passage retrieval benchmarks and out-of-domain\nzero-shot benchmarks.", "published": "2023-04-05 08:00:38", "link": "http://arxiv.org/abs/2304.03158v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Core Challenges in Embodied Vision-Language Planning", "abstract": "Recent advances in the areas of Multimodal Machine Learning and Artificial\nIntelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Robotics.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly leverage computer vision and natural language for\ninteraction in physical environments. We propose a taxonomy to unify these\ntasks and provide an in-depth analysis and comparison of the current and new\nalgorithmic approaches, metrics, simulators, and datasets used for EVLP tasks.\nFinally, we present the core challenges that we believe new EVLP works should\nseek to address, and we advocate for task construction that enables model\ngeneralisability and furthers real-world deployment.", "published": "2023-04-05 20:37:13", "link": "http://arxiv.org/abs/2304.02738v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Zero-shot domain adaptation of anomalous samples for semi-supervised\n  anomaly detection", "abstract": "Semi-supervised anomaly detection~(SSAD) is a task where normal data and a\nlimited number of anomalous data are available for training. In practical\nsituations, SSAD methods suffer adapting to domain shifts, since anomalous data\nare unlikely to be available for the target domain in the training phase. To\nsolve this problem, we propose a domain adaptation method for SSAD where no\nanomalous data are available for the target domain. First, we introduce a\ndomain-adversarial network to a variational auto-encoder-based SSAD model to\nobtain domain-invariant latent variables. Since the decoder cannot reconstruct\nthe original data solely from domain-invariant latent variables, we conditioned\nthe decoder on the domain label. To compensate for the missing anomalous data\nof the target domain, we introduce an importance sampling-based weighted loss\nfunction that approximates the ideal loss function. Experimental results\nindicate that the proposed method helps adapt SSAD models to the target domain\nwhen no anomalous data are available for the target domain.", "published": "2023-04-05 04:29:38", "link": "http://arxiv.org/abs/2304.02221v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
