{"title": "SoftMCL: Soft Momentum Contrastive Learning for Fine-grained\n  Sentiment-aware Pre-training", "abstract": "The pre-training for language models captures general language understanding\nbut fails to distinguish the affective impact of a particular context to a\nspecific word. Recent works have sought to introduce contrastive learning (CL)\nfor sentiment-aware pre-training in acquiring affective information.\nNevertheless, these methods present two significant limitations. First, the\ncompatibility of the GPU memory often limits the number of negative samples,\nhindering the opportunities to learn good representations. In addition, using\nonly a few sentiment polarities as hard labels, e.g., positive, neutral, and\nnegative, to supervise CL will force all representations to converge to a few\npoints, leading to the issue of latent space collapse. This study proposes a\nsoft momentum contrastive learning (SoftMCL) for fine-grained sentiment-aware\npre-training. Instead of hard labels, we introduce valence ratings as\nsoft-label supervision for CL to fine-grained measure the sentiment\nsimilarities between samples. The proposed SoftMCL is conducted on both the\nword- and sentence-level to enhance the model's ability to learn affective\ninformation. A momentum queue was introduced to expand the contrastive samples,\nallowing storing and involving more negatives to overcome the limitations of\nhardware platforms. Extensive experiments were conducted on four different\nsentiment-related tasks, which demonstrates the effectiveness of the proposed\nSoftMCL method. The code and data of the proposed SoftMCL is available at:\nhttps://www.github.com/wangjin0818/SoftMCL/.", "published": "2024-05-03 03:15:38", "link": "http://arxiv.org/abs/2405.01827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource\n  Languages of Singapore", "abstract": "To address the limitations of current hate speech detection models, we\nintroduce \\textsf{SGHateCheck}, a novel framework designed for the linguistic\nand cultural context of Singapore and Southeast Asia. It extends the functional\ntesting approach of HateCheck and MHC, employing large language models for\ntranslation and paraphrasing into Singapore's main languages, and refining\nthese with native annotators. \\textsf{SGHateCheck} reveals critical flaws in\nstate-of-the-art models, highlighting their inadequacy in sensitive content\nmoderation. This work aims to foster the development of more effective hate\nspeech detection tools for diverse linguistic environments, particularly for\nSingapore and Southeast Asia contexts.", "published": "2024-05-03 04:18:10", "link": "http://arxiv.org/abs/2405.01842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating External Knowledge and Goal Guidance for LLM-based\n  Conversational Recommender Systems", "abstract": "This paper aims to efficiently enable large language models (LLMs) to use\nexternal knowledge and goal guidance in conversational recommender system (CRS)\ntasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks\nfor 1) generating grounded responses with recommendation-oriented knowledge, or\n2) proactively leading the conversations through different dialogue goals. In\nthis work, we first analyze those limitations through a comprehensive\nevaluation, showing the necessity of external knowledge and goal guidance which\ncontribute significantly to the recommendation accuracy and language quality.\nIn light of this finding, we propose a novel ChatCRS framework to decompose the\ncomplex CRS task into several sub-tasks through the implementation of 1) a\nknowledge retrieval agent using a tool-augmented approach to reason over\nexternal Knowledge Bases and 2) a goal-planning agent for dialogue goal\nprediction. Experimental results on two multi-goal CRS datasets reveal that\nChatCRS sets new state-of-the-art benchmarks, improving language quality of\ninformativeness by 17% and proactivity by 27%, and achieving a tenfold\nenhancement in recommendation accuracy.", "published": "2024-05-03 05:42:57", "link": "http://arxiv.org/abs/2405.01868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Single-Event Extraction: Towards Efficient Document-Level\n  Multi-Event Argument Extraction", "abstract": "Recent mainstream event argument extraction methods process each event in\nisolation, resulting in inefficient inference and ignoring the correlations\namong multiple events. To address these limitations, here we propose a\nmultiple-event argument extraction model DEEIA (Dependency-guided Encoding and\nEvent-specific Information Aggregation), capable of extracting arguments from\nall events within a document simultaneouslyThe proposed DEEIA model employs a\nmulti-event prompt mechanism, comprising DE and EIA modules. The DE module is\ndesigned to improve the correlation between prompts and their corresponding\nevent contexts, whereas the EIA module provides event-specific information to\nimprove contextual understanding. Extensive experiments show that our method\nachieves new state-of-the-art performance on four public datasets (RAMS,\nWikiEvents, MLEE, and ACE05), while significantly saving the inference time\ncompared to the baselines. Further analyses demonstrate the effectiveness of\nthe proposed modules.", "published": "2024-05-03 07:04:35", "link": "http://arxiv.org/abs/2405.01884v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OARelatedWork: A Large-Scale Dataset of Related Work Sections with\n  Full-texts from Open Access Sources", "abstract": "This paper introduces OARelatedWork, the first large-scale multi-document\nsummarization dataset for related work generation containing whole related work\nsections and full-texts of cited papers. The dataset includes 94 450 papers and\n5 824 689 unique referenced papers. It was designed for the task of\nautomatically generating related work to shift the field toward generating\nentire related work sections from all available content instead of generating\nparts of related work sections from abstracts only, which is the current\nmainstream in this field for abstractive approaches. We show that the estimated\nupper bound for extractive summarization increases by 217% in the ROUGE-2\nscore, when using full content instead of abstracts. Furthermore, we show the\nbenefits of full content data on naive, oracle, traditional, and\ntransformer-based baselines. Long outputs, such as related work sections, pose\nchallenges for automatic evaluation metrics like BERTScore due to their limited\ninput length. We tackle this issue by proposing and evaluating a meta-metric\nusing BERTScore. Despite operating on smaller blocks, we show this meta-metric\ncorrelates with human judgment, comparably to the original BERTScore.", "published": "2024-05-03 08:49:22", "link": "http://arxiv.org/abs/2405.01930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRCL at SemEval-2024 Task 2: Simple prompt optimizations", "abstract": "We present a baseline for the SemEval 2024 task 2 challenge, whose objective\nis to ascertain the inference relationship between pairs of clinical trial\nreport sections and statements. We apply prompt optimization techniques with\nLLM Instruct models provided as a Language Model-as-a-Service (LMaaS). We\nobserved, in line with recent findings, that synthetic CoT prompts\nsignificantly enhance manually crafted ones.", "published": "2024-05-03 09:10:40", "link": "http://arxiv.org/abs/2405.01942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Trade-off between Performance, Efficiency, and Fairness in Adapter\n  Modules for Text Classification", "abstract": "Current natural language processing (NLP) research tends to focus on only one\nor, less frequently, two dimensions - e.g., performance, privacy, fairness, or\nefficiency - at a time, which may lead to suboptimal conclusions and often\noverlooking the broader goal of achieving trustworthy NLP. Work on adapter\nmodules (Houlsby et al., 2019; Hu et al., 2021) focuses on improving\nperformance and efficiency, with no investigation of unintended consequences on\nother aspects such as fairness. To address this gap, we conduct experiments on\nthree text classification datasets by either (1) finetuning all parameters or\n(2) using adapter modules. Regarding performance and efficiency, we confirm\nprior findings that the accuracy of adapter-enhanced models is roughly on par\nwith that of fully finetuned models, while training time is substantially\nreduced. Regarding fairness, we show that adapter modules result in mixed\nfairness across sensitive groups. Further investigation reveals that, when the\nstandard fine-tuned model exhibits limited biases, adapter modules typically do\nnot introduce extra bias. On the other hand, when the finetuned model exhibits\nincreased bias, the impact of adapter modules on bias becomes more\nunpredictable, introducing the risk of significantly magnifying these biases\nfor certain groups. Our findings highlight the need for a case-by-case\nevaluation rather than a one-size-fits-all judgment.", "published": "2024-05-03 11:18:47", "link": "http://arxiv.org/abs/2405.02010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Multimodal Model based Standardisation of Pathology Reports with\n  Confidence and their Prognostic Significance", "abstract": "Pathology reports are rich in clinical and pathological details but are often\npresented in free-text format. The unstructured nature of these reports\npresents a significant challenge limiting the accessibility of their content.\nIn this work, we present a practical approach based on the use of large\nmultimodal models (LMMs) for automatically extracting information from scanned\nimages of pathology reports with the goal of generating a standardised report\nspecifying the value of different fields along with estimated confidence about\nthe accuracy of the extracted fields. The proposed approach overcomes\nlimitations of existing methods which do not assign confidence scores to\nextracted fields limiting their practical use. The proposed framework uses two\nstages of prompting a Large Multimodal Model (LMM) for information extraction\nand validation. The framework generalises to textual reports from multiple\nmedical centres as well as scanned images of legacy pathology reports. We show\nthat the estimated confidence is an effective indicator of the accuracy of the\nextracted information that can be used to select only accurately extracted\nfields. We also show the prognostic significance of structured and unstructured\ndata from pathology reports and show that the automatically extracted field\nvalues significant prognostic value for patient stratification. The framework\nis available for evaluation via the URL: https://labieb.dcs.warwick.ac.uk/.", "published": "2024-05-03 12:19:38", "link": "http://arxiv.org/abs/2405.02040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimising Calls to Large Language Models with Uncertainty-Based\n  Two-Tier Selection", "abstract": "Researchers and practitioners operating on a limited budget face the\ncost-performance trade-off dilemma. The challenging decision often centers on\nwhether to use a large LLM with better performance or a smaller one with\nreduced costs. This has motivated recent research in the optimisation of LLM\ncalls. Either a cascading strategy is used, where a smaller LLM or both are\ncalled sequentially, or a routing strategy is used, where only one model is\never called. Both scenarios are dependent on a decision criterion which is\ntypically implemented by an extra neural model. In this work, we propose a\nsimpler solution; we use only the uncertainty of the generations of the small\nLLM as the decision criterion. We compare our approach with both cascading and\nrouting strategies using three different pairs of pre-trained small and large\nLLMs, on nine different tasks and against approaches that require an additional\nneural model. Our experiments reveal this simple solution optimally balances\ncost and performance, outperforming existing methods on 25 out of 27\nexperimental setups.", "published": "2024-05-03 14:38:59", "link": "http://arxiv.org/abs/2405.02134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in\n  Medical Domain", "abstract": "Medical texts are notoriously challenging to read. Properly measuring their\nreadability is the first step towards making them more accessible. In this\npaper, we present a systematic study on fine-grained readability measurements\nin the medical domain at both sentence-level and span-level. We introduce a new\ndataset MedReadMe, which consists of manually annotated readability ratings and\nfine-grained complex span annotation for 4,520 sentences, featuring two novel\n\"Google-Easy\" and \"Google-Hard\" categories. It supports our quantitative\nanalysis, which covers 650 linguistic features and automatic complex word and\njargon identification. Enabled by our high-quality annotation, we benchmark and\nimprove several state-of-the-art sentence-level readability metrics for the\nmedical domain specifically, which include unsupervised, supervised, and\nprompting-based methods using recently developed large language models (LLMs).\nInformed by our fine-grained complex span annotation, we find that adding a\nsingle feature, capturing the number of jargon spans, into existing readability\nformulas can significantly improve their correlation with human judgments. The\ndata is available at tinyurl.com/medreadme-repo", "published": "2024-05-03 14:48:20", "link": "http://arxiv.org/abs/2405.02144v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Call for Socially Aware Language Technologies", "abstract": "Language technologies have made enormous progress, especially with the\nintroduction of large language models (LLMs). On traditional tasks such as\nmachine translation and sentiment analysis, these models perform at near-human\nlevel. These advances can, however, exacerbate a variety of issues that models\nhave traditionally struggled with, such as bias, evaluation, and risks. In this\nposition paper, we argue that many of these issues share a common core: a lack\nof awareness of the factors, context, and implications of the social\nenvironment in which NLP operates, which we call social awareness. While NLP is\ngetting better at solving the formal linguistic aspects, limited progress has\nbeen made in adding the social awareness required for language applications to\nwork in all situations for all users. Integrating social awareness into NLP\nmodels will make applications more natural, helpful, and safe, and will open up\nnew possibilities. Thus we argue that substantial challenges remain for NLP to\ndevelop social awareness and that we are just at the beginning of a new era for\nthe field.", "published": "2024-05-03 18:12:39", "link": "http://arxiv.org/abs/2405.02411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?", "abstract": "We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the\nmechanism underlying the ability of large language models to recall facts from\na training corpus. This nascent thesis proposes that facts are recalled from\nthe training corpus through the MLP weights in a manner resembling key-value\nmemory, implying in effect that \"knowledge\" is stored in the network.\nFurthermore, by modifying the MLP modules, one can control the language model's\ngeneration of factual information. The plausibility of the KN thesis has been\ndemonstrated by the success of KN-inspired model editing methods (Dai et al.,\n2022; Meng et al., 2022).\n  We find that this thesis is, at best, an oversimplification. Not only have we\nfound that we can edit the expression of certain linguistic phenomena using the\nsame model editing methods but, through a more comprehensive evaluation, we\nhave found that the KN thesis does not adequately explain the process of\nfactual expression. While it is possible to argue that the MLP weights store\ncomplex patterns that are interpretable both syntactically and semantically,\nthese patterns do not constitute \"knowledge.\" To gain a more comprehensive\nunderstanding of the knowledge representation process, we must look beyond the\nMLP weights and explore recent models' complex layer structures and attention\nmechanisms.", "published": "2024-05-03 18:34:37", "link": "http://arxiv.org/abs/2405.02421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Scaling: Bayesian Ideal Point Estimates with Large Language\n  Models", "abstract": "This paper introduces \"Semantic Scaling,\" a novel method for ideal point\nestimation from text. I leverage large language models to classify documents\nbased on their expressed stances and extract survey-like data. I then use item\nresponse theory to scale subjects from these data. Semantic Scaling\nsignificantly improves on existing text-based scaling methods, and allows\nresearchers to explicitly define the ideological dimensions they measure. This\nrepresents the first scaling approach that allows such flexibility outside of\nsurvey instruments and opens new avenues of inquiry for populations difficult\nto survey. Additionally, it works with documents of varying length, and\nproduces valid estimates of both mass and elite ideology. I demonstrate that\nthe method can differentiate between policy preferences and in-group/out-group\naffect. Among the public, Semantic Scaling out-preforms Tweetscores according\nto human judgement; in Congress, it recaptures the first dimension DW-NOMINATE\nwhile allowing for greater flexibility in resolving construct validity\nchallenges.", "published": "2024-05-03 20:20:15", "link": "http://arxiv.org/abs/2405.02472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mothman at SemEval-2024 Task 9: An Iterative System for Chain-of-Thought\n  Prompt Optimization", "abstract": "Extensive research exists on the performance of large language models on\nlogic-based tasks, whereas relatively little has been done on their ability to\ngenerate creative solutions on lateral thinking tasks. The BrainTeaser shared\ntask tests lateral thinking and uses adversarial datasets to prevent\nmemorization, resulting in poor performance for out-of-the-box models. We\npropose a system for iterative, chain-of-thought prompt engineering which\noptimizes prompts using human evaluation. Using this shared task, we\ndemonstrate our system's ability to significantly improve model performance by\noptimizing prompts and evaluate the input dataset.", "published": "2024-05-03 23:04:52", "link": "http://arxiv.org/abs/2405.02517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Position Bias Effects on Fairness in Social Multi-Document\n  Summarization", "abstract": "Text summarization models have typically focused on optimizing aspects of\nquality such as fluency, relevance, and coherence, particularly in the context\nof news articles. However, summarization models are increasingly being used to\nsummarize diverse sources of text, such as social media data, that encompass a\nwide demographic user base. It is thus crucial to assess not only the quality\nof the generated summaries, but also the extent to which they can fairly\nrepresent the opinions of diverse social groups. Position bias, a long-known\nissue in news summarization, has received limited attention in the context of\nsocial multi-document summarization. We deeply investigate this phenomenon by\nanalyzing the effect of group ordering in input documents when summarizing\ntweets from three distinct linguistic communities: African-American English,\nHispanic-aligned Language, and White-aligned Language. Our empirical analysis\nshows that although the textual quality of the summaries remains consistent\nregardless of the input document order, in terms of fairness, the results vary\nsignificantly depending on how the dialect groups are presented in the input\ndata. Our results suggest that position bias manifests differently in social\nmulti-document summarization, severely impacting the fairness of summarization\nmodels.", "published": "2024-05-03 00:19:31", "link": "http://arxiv.org/abs/2405.01790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders\n  and Identifying Distinct Features", "abstract": "Diagnosing language disorders associated with autism is a complex challenge,\noften hampered by the subjective nature and variability of traditional\nassessment methods. Traditional diagnostic methods not only require intensive\nhuman effort but also often result in delayed interventions due to their lack\nof speed and precision. In this study, we explored the application of ChatGPT,\na large language model, to overcome these obstacles by enhancing sensitivity\nand profiling linguistic features for autism diagnosis. This research utilizes\nChatGPT natural language processing capabilities to simplify and improve the\ndiagnostic process, focusing on identifying autism related language patterns.\nSpecifically, we compared ChatGPT performance with that of conventional\nsupervised learning models, including BERT, a model acclaimed for its\neffectiveness in various natural language processing tasks. We showed that\nChatGPT substantially outperformed these models, achieving over 10% improvement\nin both sensitivity and positive predictive value, in a zero shot learning\nconfiguration. The findings underscore the model potential as a diagnostic\ntool, combining accuracy and applicability. We identified ten key features of\nautism associated language disorders across scenarios. Features such as\necholalia, pronoun reversal, and atypical language usage play a critical role\nin diagnosing ASD and informing tailored treatment plans. Together, our\nfindings advocate for adopting sophisticated AI tools like ChatGPT in clinical\nsettings to assess and diagnose developmental disorders. Our approach promises\nenhanced diagnostic precision and supports personalized medicine, potentially\ntransforming the evaluation landscape for autism and similar neurological\nconditions.", "published": "2024-05-03 01:04:28", "link": "http://arxiv.org/abs/2405.01799v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for\n  Sexual Education in Rural India", "abstract": "Sexual education aims to foster a healthy lifestyle in terms of emotional,\nmental and social well-being. In countries like India, where adolescents form\nthe largest demographic group, they face significant vulnerabilities concerning\nsexual health. Unfortunately, sexual education is often stigmatized, creating\nbarriers to providing essential counseling and information to this at-risk\npopulation. Consequently, issues such as early pregnancy, unsafe abortions,\nsexually transmitted infections, and sexual violence become prevalent. Our\ncurrent proposal aims to provide a safe and trustworthy platform for sexual\neducation to the vulnerable rural Indian population, thereby fostering the\nhealthy and overall growth of the nation. In this regard, we strive towards\ndesigning SUKHSANDESH, a multi-staged AI-based Question Answering platform for\nsexual education tailored to rural India, adhering to safety guardrails and\nregional language support. By utilizing information retrieval techniques and\nlarge language models, SUKHSANDESH will deliver effective responses to user\nqueries. We also propose to anonymise the dataset to mitigate safety measures\nand set AI guardrails against any harmful or unwanted response generation.\nMoreover, an innovative feature of our proposal involves integrating ``avatar\ntherapy'' with SUKHSANDESH. This feature will convert AI-generated responses\ninto real-time audio delivered by an animated avatar speaking regional Indian\nlanguages. This approach aims to foster empathy and connection, which is\nparticularly beneficial for individuals with limited literacy skills.\nPartnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to\naddress sexual education needs in rural India.", "published": "2024-05-03 05:19:09", "link": "http://arxiv.org/abs/2405.01858v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing Bangla Language Next Word Prediction and Sentence Completion\n  through Extended RNN with Bi-LSTM Model On N-gram Language", "abstract": "Texting stands out as the most prominent form of communication worldwide.\nIndividual spend significant amount of time writing whole texts to send emails\nor write something on social media, which is time consuming in this modern era.\nWord prediction and sentence completion will be suitable and appropriate in the\nBangla language to make textual information easier and more convenient. This\npaper expands the scope of Bangla language processing by introducing a Bi-LSTM\nmodel that effectively handles Bangla next-word prediction and Bangla sentence\ngeneration, demonstrating its versatility and potential impact. We proposed a\nnew Bi-LSTM model to predict a following word and complete a sentence. We\nconstructed a corpus dataset from various news portals, including bdnews24, BBC\nNews Bangla, and Prothom Alo. The proposed approach achieved superior results\nin word prediction, reaching 99\\% accuracy for both 4-gram and 5-gram word\npredictions. Moreover, it demonstrated significant improvement over existing\nmethods, achieving 35\\%, 75\\%, and 95\\% accuracy for uni-gram, bi-gram, and\ntri-gram word prediction, respectively", "published": "2024-05-03 06:06:01", "link": "http://arxiv.org/abs/2405.01873v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DALLMi: Domain Adaption for LLM-based Multi-label Classifier", "abstract": "Large language models (LLMs) increasingly serve as the backbone for\nclassifying text associated with distinct domains and simultaneously several\nlabels (classes). When encountering domain shifts, e.g., classifier of movie\nreviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label\nclassifier is challenging due to incomplete label sets at the target domain and\ndaunting training overhead. The existing domain adaptation methods address\neither image multi-label classifiers or text binary classifiers. In this paper,\nwe design DALLMi, Domain Adaptation Large Language Model interpolator, a\nfirst-of-its-kind semi-supervised domain adaptation method for text data models\nbased on LLMs, specifically BERT. The core of DALLMi is the novel variation\nloss and MixUp regularization, which jointly leverage the limited positively\nlabeled and large quantity of unlabeled text and, importantly, their\ninterpolation from the BERT word embeddings. DALLMi also introduces a\nlabel-balanced sampling strategy to overcome the imbalance between labeled and\nunlabeled data. We evaluate DALLMi against the partial-supervised and\nunsupervised approach on three datasets under different scenarios of label\navailability for the target domain. Our results show that DALLMi achieves\nhigher mAP than unsupervised and partially-supervised approaches by 19.9% and\n52.2%, respectively.", "published": "2024-05-03 07:04:26", "link": "http://arxiv.org/abs/2405.01883v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aloe: A Family of Fine-tuned Open Healthcare LLMs", "abstract": "As the capabilities of Large Language Models (LLMs) in healthcare and\nmedicine continue to advance, there is a growing need for competitive\nopen-source models that can safeguard public interest. With the increasing\navailability of highly competitive open base models, the impact of continued\npre-training is increasingly uncertain. In this work, we explore the role of\ninstruct tuning, model merging, alignment, red teaming and advanced inference\nschemes, as means to improve current open models. To that end, we introduce the\nAloe family, a set of open medical LLMs highly competitive within its scale\nrange. Aloe models are trained on the current best base models (Mistral, LLaMA\n3), using a new custom dataset which combines public data sources improved with\nsynthetic Chain of Thought (CoT). Aloe models undergo an alignment phase,\nbecoming one of the first few policy-aligned open healthcare LLM using Direct\nPreference Optimization, setting a new standard for ethical performance in\nhealthcare LLMs. Model evaluation expands to include various bias and toxicity\ndatasets, a dedicated red teaming effort, and a much-needed risk assessment for\nhealthcare LLMs. Finally, to explore the limits of current LLMs in inference,\nwe study several advanced prompt engineering strategies to boost performance\nacross benchmarks, yielding state-of-the-art results for open healthcare 7B\nLLMs, unprecedented at this scale.", "published": "2024-05-03 07:14:07", "link": "http://arxiv.org/abs/2405.01886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A quantitative and typological study of Early Slavic participle clauses\n  and their competition", "abstract": "This thesis is a corpus-based, quantitative, and typological analysis of the\nfunctions of Early Slavic participle constructions and their finite competitors\n($jegda$-'when'-clauses). The first part leverages detailed linguistic\nannotation on Early Slavic corpora at the morphosyntactic, dependency,\ninformation-structural, and lexical levels to obtain indirect evidence for\ndifferent potential functions of participle clauses and their main finite\ncompetitor and understand the roles of compositionality and default discourse\nreasoning as explanations for the distribution of participle constructions and\n$jegda$-clauses in the corpus. The second part uses massively parallel data to\nanalyze typological variation in how languages express the semantic space of\nEnglish $when$, whose scope encompasses that of Early Slavic participle\nconstructions and $jegda$-clauses. Probabilistic semantic maps are generated\nand statistical methods (including Kriging, Gaussian Mixture Modelling,\nprecision and recall analysis) are used to induce cross-linguistically salient\ndimensions from the parallel corpus and to study conceptual variation within\nthe semantic space of the hypothetical concept WHEN.", "published": "2024-05-03 09:54:10", "link": "http://arxiv.org/abs/2405.01972v3", "categories": ["cs.CL", "cs.IR", "68T50, 68U15, 68T35, (Primary), 86A32, 15A03 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Conformal Prediction for Natural Language Processing: A Survey", "abstract": "The rapid proliferation of large language models and natural language\nprocessing (NLP) applications creates a crucial need for uncertainty\nquantification to mitigate risks such as hallucinations and to enhance\ndecision-making reliability in critical applications. Conformal prediction is\nemerging as a theoretically sound and practically useful framework, combining\nflexibility with strong statistical guarantees. Its model-agnostic and\ndistribution-free nature makes it particularly promising to address the current\nshortcomings of NLP systems that stem from the absence of uncertainty\nquantification. This paper provides a comprehensive survey of conformal\nprediction techniques, their guarantees, and existing applications in NLP,\npointing to directions for future research and open challenges.", "published": "2024-05-03 10:00:45", "link": "http://arxiv.org/abs/2405.01976v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Combinatorial Problem Solving with Large Language Models: A\n  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo", "abstract": "Large Language Models (LLMs) are deep learning models designed to generate\ntext based on textual input. Although researchers have been developing these\nmodels for more complex tasks such as code generation and general reasoning,\nfew efforts have explored how LLMs can be applied to combinatorial problems. In\nthis research, we investigate the potential of LLMs to solve the Travelling\nSalesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments\nemploying various approaches, including zero-shot in-context learning, few-shot\nin-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned\nGPT-3.5 Turbo to solve a specific problem size and tested it using a set of\nvarious instance sizes. The fine-tuned models demonstrated promising\nperformance on problems identical in size to the training instances and\ngeneralized well to larger problems. Furthermore, to improve the performance of\nthe fine-tuned model without incurring additional training costs, we adopted a\nself-ensemble approach to improve the quality of the solutions.", "published": "2024-05-03 10:54:14", "link": "http://arxiv.org/abs/2405.01997v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Narrative Processing in Large Language Models (LLMs): Using\n  GPT4 to test BERT", "abstract": "The ability to transmit and receive complex information via language is\nunique to humans and is the basis of traditions, culture and versatile social\ninteractions. Through the disruptive introduction of transformer based large\nlanguage models (LLMs) humans are not the only entity to \"understand\" and\nproduce language any more. In the present study, we have performed the first\nsteps to use LLMs as a model to understand fundamental mechanisms of language\nprocessing in neural networks, in order to make predictions and generate\nhypotheses on how the human brain does language processing. Thus, we have used\nChatGPT to generate seven different stylistic variations of ten different\nnarratives (Aesop's fables). We used these stories as input for the open source\nLLM BERT and have analyzed the activation patterns of the hidden units of BERT\nusing multi-dimensional scaling and cluster analysis. We found that the\nactivation vectors of the hidden units cluster according to stylistic\nvariations in earlier layers of BERT (1) than narrative content (4-5). Despite\nthe fact that BERT consists of 12 identical building blocks that are stacked\nand trained on large text corpora, the different layers perform different\ntasks. This is a very useful model of the human brain, where self-similar\nstructures, i.e. different areas of the cerebral cortex, can have different\nfunctions and are therefore well suited to processing language in a very\nefficient way. The proposed approach has the potential to open the black box of\nLLMs on the one hand, and might be a further step to unravel the neural\nprocesses underlying human language processing and cognition in general.", "published": "2024-05-03 11:56:13", "link": "http://arxiv.org/abs/2405.02024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Argumentative Large Language Models for Explainable and Contestable\n  Decision-Making", "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.", "published": "2024-05-03 13:12:28", "link": "http://arxiv.org/abs/2405.02079v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry\n  with GPT-4-Turbo", "abstract": "The rapid advancement in artificial intelligence and natural language\nprocessing has led to the development of large-scale datasets aimed at\nbenchmarking the performance of machine learning models. Herein, we introduce\n'RetChemQA,' a comprehensive benchmark dataset designed to evaluate the\ncapabilities of such models in the domain of reticular chemistry. This dataset\nincludes both single-hop and multi-hop question-answer pairs, encompassing\napproximately 45,000 Q&As for each type. The questions have been extracted from\nan extensive corpus of literature containing about 2,530 research papers from\npublishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group,\namong others. The dataset has been generated using OpenAI's GPT-4 Turbo, a\ncutting-edge model known for its exceptional language understanding and\ngeneration capabilities. In addition to the Q&A dataset, we also release a\ndataset of synthesis conditions extracted from the corpus of literature used in\nthis study. The aim of RetChemQA is to provide a robust platform for the\ndevelopment and evaluation of advanced machine learning algorithms,\nparticularly for the reticular chemistry community. The dataset is structured\nto reflect the complexities and nuances of real-world scientific discourse,\nthereby enabling nuanced performance assessments across a variety of tasks. The\ndataset is available at the following link:\nhttps://github.com/nakulrampal/RetChemQA", "published": "2024-05-03 14:29:54", "link": "http://arxiv.org/abs/2405.02128v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and\n  Multi-View Transformer", "abstract": "Deciphering the intricacies of the human brain has captivated curiosity for\ncenturies. Recent strides in Brain-Computer Interface (BCI) technology,\nparticularly using motor imagery, have restored motor functions such as\nreaching, grasping, and walking in paralyzed individuals. However, unraveling\nnatural language from brain signals remains a formidable challenge.\nElectroencephalography (EEG) is a non-invasive technique used to record\nelectrical activity in the brain by placing electrodes on the scalp. Previous\nstudies of EEG-to-text decoding have achieved high accuracy on small closed\nvocabularies, but still fall short of high accuracy when dealing with large\nopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy\nof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG\npre-training to enhance the learning of semantics from EEG signals and proposes\na multi-view transformer to model the EEG signal processing by different\nspatial regions of the brain. Experiments show that EEG2TEXT has superior\nperformance, outperforming the state-of-the-art baseline methods by a large\nmargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great\npotential for a high-performance open-vocabulary brain-to-text system to\nfacilitate communication.", "published": "2024-05-03 15:14:19", "link": "http://arxiv.org/abs/2405.02165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing and Verifying Task Utility in LLM-Powered Applications", "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in\napplications that facilitate collaboration among multiple agents, assisting\nhumans in their daily tasks. However, a significant gap remains in assessing to\nwhat extent LLM-powered applications genuinely enhance user experience and task\nexecution efficiency. This highlights the need to verify utility of LLM-powered\napplications, particularly by ensuring alignment between the application's\nfunctionality and end-user needs. We introduce AgentEval, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the effectiveness and robustness of AgentEval for two\nopen source datasets including Math Problem solving and ALFWorld House-hold\nrelated tasks. For reproducibility purposes, we make the data, code and all the\nlogs publicly available at https://bit.ly/3w3yKcS .", "published": "2024-05-03 15:26:27", "link": "http://arxiv.org/abs/2405.02178v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Impact of emoji exclusion on the performance of Arabic sarcasm detection\n  models", "abstract": "The complex challenge of detecting sarcasm in Arabic speech on social media\nis increased by the language diversity and the nature of sarcastic expressions.\nThere is a significant gap in the capability of existing models to effectively\ninterpret sarcasm in Arabic, which mandates the necessity for more\nsophisticated and precise detection methods. In this paper, we investigate the\nimpact of a fundamental preprocessing component on sarcasm speech detection.\nWhile emojis play a crucial role in mitigating the absence effect of body\nlanguage and facial expressions in modern communication, their impact on\nautomated text analysis, particularly in sarcasm detection, remains\nunderexplored. We investigate the impact of emoji exclusion from datasets on\nthe performance of sarcasm detection models in social media content for Arabic\nas a vocabulary-super rich language. This investigation includes the adaptation\nand enhancement of AraBERT pre-training models, specifically by excluding\nemojis, to improve sarcasm detection capabilities. We use AraBERT pre-training\nto refine the specified models, demonstrating that the removal of emojis can\nsignificantly boost the accuracy of sarcasm detection. This approach\nfacilitates a more refined interpretation of language, eliminating the\npotential confusion introduced by non-textual elements. The evaluated AraBERT\nmodels, through the focused strategy of emoji removal, adeptly navigate the\ncomplexities of Arabic sarcasm. This study establishes new benchmarks in Arabic\nnatural language processing and presents valuable insights for social media\nplatforms.", "published": "2024-05-03 15:51:02", "link": "http://arxiv.org/abs/2405.02195v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structural Pruning of Pre-trained Language Models via Neural\n  Architecture Search", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the\nstate-of-the-art for natural language understanding task when fine-tuned on\nlabeled data. However, their large size poses challenges in deploying them for\ninference in real-world applications, due to significant GPU memory\nrequirements and high inference latency. This paper explores neural\narchitecture search (NAS) for structural pruning to find sub-parts of the\nfine-tuned network that optimally trade-off efficiency, for example in terms of\nmodel size or latency, and generalization performance. We also show how we can\nutilize more recently developed two-stage weight-sharing NAS approaches in this\nsetting to accelerate the search process. Unlike traditional pruning methods\nwith fixed thresholds, we propose to adopt a multi-objective approach that\nidentifies the Pareto optimal set of sub-networks, allowing for a more flexible\nand automated compression process.", "published": "2024-05-03 17:34:57", "link": "http://arxiv.org/abs/2405.02267v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM as Dataset Analyst: Subpopulation Structure Discovery with Large\n  Language Model", "abstract": "The distribution of subpopulations is an important property hidden within a\ndataset. Uncovering and analyzing the subpopulation distribution within\ndatasets provides a comprehensive understanding of the datasets, standing as a\npowerful tool beneficial to various downstream tasks, including Dataset\nSubpopulation Organization, Subpopulation Shift, and Slice Discovery. Despite\nits importance, there has been no work that systematically explores the\nsubpopulation distribution of datasets to our knowledge. To address the\nlimitation and solve all the mentioned tasks in a unified way, we introduce a\nnovel concept of subpopulation structures to represent, analyze, and utilize\nsubpopulation distributions within datasets. To characterize the structures in\nan interpretable manner, we propose the Subpopulation Structure Discovery with\nLarge Language Models (SSD-LLM) framework, which employs world knowledge and\ninstruction-following capabilities of Large Language Models (LLMs) to\nlinguistically analyze informative image captions and summarize the structures.\nFurthermore, we propose complete workflows to address downstream tasks, named\nTask-specific Tuning, showcasing the application of the discovered structure to\na spectrum of subpopulation-related tasks, including dataset subpopulation\norganization, subpopulation shift, and slice discovery. Furthermore, we propose\ncomplete workflows to address downstream tasks, named Task-specific Tuning,\nshowcasing the application of the discovered structure to a spectrum of\nsubpopulation-related tasks, including dataset subpopulation organization,\nsubpopulation shift, and slice discovery.", "published": "2024-05-03 05:09:54", "link": "http://arxiv.org/abs/2405.02363v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "What is Sentiment Meant to Mean to Language Models?", "abstract": "Sentiment analysis is one of the most widely used techniques in text\nanalysis. Recent advancements with Large Language Models have made it more\naccurate and accessible than ever, allowing researchers to classify text with\nonly a plain English prompt. However, \"sentiment\" entails a wide variety of\nconcepts depending on the domain and tools used. It has been used to mean\nemotion, opinions, market movements, or simply a general ``good-bad''\ndimension. This raises a question: What exactly are language models doing when\nprompted to label documents by sentiment? This paper first overviews how\nsentiment is defined across different contexts, highlighting that it is a\nconfounded measurement construct in that it entails multiple variables, such as\nemotional valence and opinion, without disentangling them. I then test three\nlanguage models across two data sets with prompts requesting sentiment,\nvalence, and stance classification. I find that sentiment labels most strongly\ncorrelate with valence labels. I further find that classification improves when\nresearchers more precisely specify their dimension of interest rather than\nusing the less well-defined concept of sentiment. I conclude by encouraging\nresearchers to move beyond \"sentiment\" when feasible and use a more precise\nmeasurement construct.", "published": "2024-05-03 19:37:37", "link": "http://arxiv.org/abs/2405.02454v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with\n  Persona In-Context Learning", "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are\nencoded with diverse personality traits. This triggers an interesting goal of\neliciting a desired personality trait from the LLM, and probing its behavioral\npreferences. Accordingly, we formalize the persona elicitation task, aiming to\ncustomize LLM behaviors to align with a target persona. We present Persona\nIn-Context Learning (PICLe), a novel persona elicitation framework grounded in\nBayesian inference. At the core, PICLe introduces a new ICL example selection\ncriterion based on likelihood ratio, which is designed to optimally guide the\nmodel in eliciting a specific target persona. We demonstrate the effectiveness\nof PICLe through extensive comparisons against baseline methods across three\ncontemporary LLMs. Code is available at\nhttps://github.com/deeplearning-wisc/picle.", "published": "2024-05-03 22:17:22", "link": "http://arxiv.org/abs/2405.02501v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Polarity Analysis of Bangla Food Reviews Using Machine and\n  Deep Learning Algorithms", "abstract": "The Internet has become an essential tool for people in the modern world.\nHumans, like all living organisms, have essential requirements for survival.\nThese include access to atmospheric oxygen, potable water, protective shelter,\nand sustenance. The constant flux of the world is making our existence less\ncomplicated. A significant portion of the population utilizes online food\nordering services to have meals delivered to their residences. Although there\nare numerous methods for ordering food, customers sometimes experience\ndisappointment with the food they receive. Our endeavor was to establish a\nmodel that could determine if food is of good or poor quality. We compiled an\nextensive dataset of over 1484 online reviews from prominent food ordering\nplatforms, including Food Panda and HungryNaki. Leveraging the collected data,\na rigorous assessment of various deep learning and machine learning techniques\nwas performed to determine the most accurate approach for predicting food\nquality. Out of all the algorithms evaluated, logistic regression emerged as\nthe most accurate, achieving an impressive 90.91% accuracy. The review offers\nvaluable insights that will guide the user in deciding whether or not to order\nthe food.", "published": "2024-05-03 09:49:46", "link": "http://arxiv.org/abs/2405.06667v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TOPICAL: TOPIC Pages AutomagicaLly", "abstract": "Topic pages aggregate useful information about an entity or concept into a\nsingle succinct and accessible article. Automated creation of topic pages would\nenable their rapid curation as information resources, providing an alternative\nto traditional web search. While most prior work has focused on generating\ntopic pages about biographical entities, in this work, we develop a completely\nautomated process to generate high-quality topic pages for scientific entities,\nwith a focus on biomedical concepts. We release TOPICAL, a web app and\nassociated open-source code, comprising a model pipeline combining retrieval,\nclustering, and prompting, that makes it easy for anyone to generate topic\npages for a wide variety of biomedical entities on demand. In a human\nevaluation of 150 diverse topic pages generated using TOPICAL, we find that the\nvast majority were considered relevant, accurate, and coherent, with correct\nsupporting citations. We make all code publicly available and host a\nfree-to-use web app at: https://s2-topical.apps.allenai.org", "published": "2024-05-03 00:47:16", "link": "http://arxiv.org/abs/2405.01796v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semi-Parametric Retrieval via Binary Bag-of-Tokens Index", "abstract": "Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.", "published": "2024-05-03 08:34:13", "link": "http://arxiv.org/abs/2405.01924v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large\n  Language Models", "abstract": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced\nthe capabilities of language understanding and generation. However, the\nsubstantial model size poses hardware challenges, affecting both memory size\nfor serving and inference latency for token generation. To address those\nchallenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a\nnovel method for the recent prevalent GLU-based LLMs pruning, which\nincorporates structural dependency into the weight magnitude-based unstructured\npruning. We introduce an MLP-specific pruning metric that evaluates the\nimportance of each weight by jointly considering its magnitude and its\ncorresponding MLP intermediate activation norms. DaSS facilitates a balance\nbetween the adaptability offered by unstructured pruning and the structural\nconsistency inherent in dependency-based structured pruning. Empirical\nevaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS\nnot only outperforms both SparseGPT and Wanda in achieving hardware-friendly\nN:M sparsity patterns but also maintains the computational efficiency of Wanda.", "published": "2024-05-03 09:13:13", "link": "http://arxiv.org/abs/2405.01943v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models for Structured Science Summarization in\n  the Open Research Knowledge Graph", "abstract": "Structured science summaries or research contributions using properties or\ndimensions beyond traditional keywords enhances science findability. Current\nmethods, such as those used by the Open Research Knowledge Graph (ORKG),\ninvolve manually curating properties to describe research papers' contributions\nin a structured manner, but this is labor-intensive and inconsistent between\nthe domain expert human curators. We propose using Large Language Models (LLMs)\nto automatically suggest these properties. However, it's essential to assess\nthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before\napplication. Our study performs a comprehensive comparative analysis between\nORKG's manually curated properties and those generated by the aforementioned\nstate-of-the-art LLMs. We evaluate LLM performance through four unique\nperspectives: semantic alignment and deviation with ORKG properties,\nfine-grained properties mapping accuracy, SciNCL embeddings-based cosine\nsimilarity, and expert surveys comparing manual annotations with LLM outputs.\nThese evaluations occur within a multidisciplinary science setting. Overall,\nLLMs show potential as recommendation systems for structuring science, but\nfurther finetuning is recommended to improve their alignment with scientific\ntasks and mimicry of human expertise.", "published": "2024-05-03 14:03:04", "link": "http://arxiv.org/abs/2405.02105v1", "categories": ["cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on\n  Self-Supervised Learning and Knowledge Transfer", "abstract": "In this paper, we present a novel approach for text independent\nphone-to-audio alignment based on phoneme recognition, representation learning\nand knowledge transfer. Our method leverages a self-supervised model (wav2vec2)\nfine-tuned for phoneme recognition using a Connectionist Temporal\nClassification (CTC) loss, a dimension reduction model and a frame-level\nphoneme classifier trained thanks to forced-alignment labels (using Montreal\nForced Aligner) to produce multi-lingual phonetic representations, thus\nrequiring minimal additional training. We evaluate our model using synthetic\nnative data from the TIMIT dataset and the SCRIBE dataset for American and\nBritish English, respectively. Our proposed model outperforms the\nstate-of-the-art (charsiu) in statistical metrics and has applications in\nlanguage learning and speech processing systems. We leave experiments on other\nlanguages for future work but the design of the system makes it easily\nadaptable to other languages.", "published": "2024-05-03 14:25:21", "link": "http://arxiv.org/abs/2405.02124v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets", "abstract": "Large Language Models (LLMs) have demonstrated unparalleled effectiveness in\nvarious NLP tasks, and integrating LLMs with automatic speech recognition (ASR)\nis becoming a mainstream paradigm. Building upon this momentum, our research\ndelves into an in-depth examination of this paradigm on a large open-source\nChinese dataset. Specifically, our research aims to evaluate the impact of\nvarious configurations of speech encoders, LLMs, and projector modules in the\ncontext of the speech foundation encoder-LLM ASR paradigm. Furthermore, we\nintroduce a three-stage training approach, expressly developed to enhance the\nmodel's ability to align auditory and textual information. The implementation\nof this approach, alongside the strategic integration of ASR components,\nenabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and\nTest_Meeting test sets. Our analysis presents an empirical foundation for\nfuture research in LLM-based ASR systems and offers insights into optimizing\nperformance using Chinese datasets. We will publicly release all scripts used\nfor data preparation, training, inference, and scoring, as well as pre-trained\nmodels and training logs to promote reproducible research.", "published": "2024-05-03 14:35:58", "link": "http://arxiv.org/abs/2405.02132v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset", "abstract": "Hoaxes are a recognised form of disinformation created deliberately, with\npotential serious implications in the credibility of reference knowledge\nresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that\nthey often are written according to the official style guidelines. In this\nwork, we first provide a systematic analysis of similarities and discrepancies\nbetween legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a\ncollection of 311 hoax articles (from existing literature and official\nWikipedia lists), together with semantically similar legitimate articles, which\ntogether form a binary text classification dataset aimed at fostering research\nin automated hoax detection. In this paper, We report results after analyzing\nseveral language models, hoax-to-legit ratios, and the amount of text\nclassifiers are exposed to (full article vs the article's definition alone).\nOur results suggest that detecting deceitful content in Wikipedia based on\ncontent alone is hard but feasible, and complement our analysis with a study on\nthe differences in distributions in edit histories, and find that looking at\nthis feature yields better classification results than context.", "published": "2024-05-03 15:25:48", "link": "http://arxiv.org/abs/2405.02175v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\n  Sentences using Public and Proprietary LLMs", "abstract": "Automatic citation generation for sentences in a document or report is\nparamount for intelligence analysts, cybersecurity, news agencies, and\neducation personnel. In this research, we investigate whether large language\nmodels (LLMs) are capable of generating references based on two forms of\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\nthe title of a mentioned article when given a sentence from a different\narticle. To demonstrate where LLM stands in this task, we introduce a large\ndataset called REASONS comprising abstracts of the 12 most popular domains of\nscientific research on arXiv. From around 20K research articles, we make the\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\npercentage (PP) to minimize the hallucination rate (HR). When tested with\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\nHR across all domains and models decreased by an average of 41.93%, and the PP\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\nstruggle to understand context, but the extent of this issue was small in\nMistral and GPT-4-Preview. Our study contributes valuable insights into the\nreliability of RAG for automated citation generation tasks.", "published": "2024-05-03 16:38:51", "link": "http://arxiv.org/abs/2405.02228v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Vibe-Eval: A hard evaluation suite for measuring progress of multimodal\n  language models", "abstract": "We introduce Vibe-Eval: a new open benchmark and framework for evaluating\nmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,\nincluding 100 of hard difficulty, complete with gold-standard responses\nauthored by experts. Vibe-Eval is open-ended and challenging with dual\nobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and\n(ii) rigorously testing and probing the capabilities of present frontier\nmodels. Notably, our hard set contains >50% questions that all frontier models\nanswer incorrectly. We explore the nuances of designing, evaluating, and\nranking models on ultra challenging prompts. We also discuss trade-offs between\nhuman and automatic evaluation, and show that automatic model evaluation using\nReka Core roughly correlates to human judgment. We offer free API access for\nthe purpose of lightweight evaluation and plan to conduct formal human\nevaluations for public models that perform well on the Vibe-Eval's automatic\nscores. We release the evaluation code and data, see\nhttps://github.com/reka-ai/reka-vibe-eval", "published": "2024-05-03 17:59:55", "link": "http://arxiv.org/abs/2405.02287v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CALRec: Contrastive Alignment of Generative LLMs for Sequential\n  Recommendation", "abstract": "Traditional recommender systems such as matrix factorization methods have\nprimarily focused on learning a shared dense embedding space to represent both\nitems and user preferences. Subsequently, sequence models such as RNN, GRUs,\nand, recently, Transformers have emerged and excelled in the task of sequential\nrecommendation. This task requires understanding the sequential structure\npresent in users' historical interactions to predict the next item they may\nlike. Building upon the success of Large Language Models (LLMs) in a variety of\ntasks, researchers have recently explored using LLMs that are pretrained on\nvast corpora of text for sequential recommendation. To use LLMs for sequential\nrecommendation, both the history of user interactions and the model's\nprediction of the next item are expressed in text form. We propose CALRec, a\ntwo-stage LLM finetuning framework that finetunes a pretrained LLM in a\ntwo-tower fashion using a mixture of two contrastive losses and a language\nmodeling loss: the LLM is first finetuned on a data mixture from multiple\ndomains followed by another round of target domain finetuning. Our model\nsignificantly outperforms many state-of-the-art baselines (+37% in Recall@1 and\n+24% in NDCG@10) and our systematic ablation studies reveal that (i) both\nstages of finetuning are crucial, and, when combined, we achieve improved\nperformance, and (ii) contrastive alignment is effective among the target\ndomains explored in our experiments.", "published": "2024-05-03 18:51:19", "link": "http://arxiv.org/abs/2405.02429v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Exploring Speech Pattern Disorders in Autism using Machine Learning", "abstract": "Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech\npatterns from examiner-patient dialogues presents significant challenges due to\nthe subtle and diverse manifestations of speech-related symptoms in affected\nindividuals. This study presents a comprehensive approach to identify\ndistinctive speech patterns through the analysis of examiner-patient dialogues.\nUtilizing a dataset of recorded dialogues, we extracted 40 speech-related\nfeatures, categorized into frequency, zero-crossing rate, energy, spectral\ncharacteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance.\nThese features encompass various aspects of speech such as intonation, volume,\nrhythm, and speech rate, reflecting the complex nature of communicative\nbehaviors in ASD. We employed machine learning for both classification and\nregression tasks to analyze these speech features. The classification model\naimed to differentiate between ASD and non-ASD cases, achieving an accuracy of\n87.75%. Regression models were developed to predict speech pattern related\nvariables and a composite score from all variables, facilitating a deeper\nunderstanding of the speech dynamics associated with ASD. The effectiveness of\nmachine learning in interpreting intricate speech patterns and the high\nclassification accuracy underscore the potential of computational methods in\nsupporting the diagnostic processes for ASD. This approach not only aids in\nearly detection but also contributes to personalized treatment planning by\nproviding insights into the speech and communication profiles of individuals\nwith ASD.", "published": "2024-05-03 02:59:15", "link": "http://arxiv.org/abs/2405.05126v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exposing and Explaining Fake News On-the-Fly", "abstract": "Social media platforms enable the rapid dissemination and consumption of\ninformation. However, users instantly consume such content regardless of the\nreliability of the shared data. Consequently, the latter crowdsourcing model is\nexposed to manipulation. This work contributes with an explainable and online\nclassification method to recognize fake news in real-time. The proposed method\ncombines both unsupervised and supervised Machine Learning approaches with\nonline created lexica. The profiling is built using creator-, content- and\ncontext-based features using Natural Language Processing techniques. The\nexplainable classification mechanism displays in a dashboard the features\nselected for classification and the prediction confidence. The performance of\nthe proposed solution has been validated with real data sets from Twitter and\nthe results attain 80 % accuracy and macro F-measure. This proposal is the\nfirst to jointly provide data stream processing, profiling, classification and\nexplainability. Ultimately, the proposed early detection, isolation and\nexplanation of fake news contribute to increase the quality and trustworthiness\nof social media contents.", "published": "2024-05-03 14:49:04", "link": "http://arxiv.org/abs/2405.06668v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Instruction-Guided Bullet Point Summarization of Long Financial Earnings\n  Call Transcripts", "abstract": "While automatic summarization techniques have made significant advancements,\ntheir primary focus has been on summarizing short news articles or documents\nthat have clear structural patterns like scientific articles or government\nreports. There has not been much exploration into developing efficient methods\nfor summarizing financial documents, which often contain complex facts and\nfigures. Here, we study the problem of bullet point summarization of long\nEarning Call Transcripts (ECTs) using the recently released ECTSum dataset. We\nleverage an unsupervised question-based extractive module followed by a\nparameter efficient instruction-tuned abstractive module to solve this task.\nOur proposed model FLAN-FinBPS achieves new state-of-the-art performances\noutperforming the strongest baseline with 14.88% average ROUGE score gain, and\nis capable of generating factually consistent bullet point summaries that\ncapture the important facts discussed in the ECTs.", "published": "2024-05-03 16:33:16", "link": "http://arxiv.org/abs/2405.06669v1", "categories": ["cs.CL", "cs.CE", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Instruction Tuning of Large Language Models For\n  Extreme Financial Numeral Labelling", "abstract": "We study the problem of automatically annotating relevant numerals (GAAP\nmetrics) occurring in the financial documents with their corresponding XBRL\ntags. Different from prior works, we investigate the feasibility of solving\nthis extreme classification problem using a generative paradigm through\ninstruction tuning of Large Language Models (LLMs). To this end, we leverage\nmetric metadata information to frame our target outputs while proposing a\nparameter efficient solution for the task using LoRA. We perform experiments on\ntwo recently released financial numeric labeling datasets. Our proposed model,\nFLAN-FinXC, achieves new state-of-the-art performances on both the datasets,\noutperforming several strong baselines. We explain the better scores of our\nproposed model by demonstrating its capability for zero-shot as well as the\nleast frequently occurring tags. Also, even when we fail to predict the XBRL\ntags correctly, our generated output has substantial overlap with the\nground-truth in majority of the cases.", "published": "2024-05-03 16:41:36", "link": "http://arxiv.org/abs/2405.06671v2", "categories": ["cs.CL", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active\n  Learning Pipeline for Speech Recognition", "abstract": "Emphasizing a data-centric AI approach, this paper introduces a novel\ntwo-stage active learning (AL) pipeline for automatic speech recognition (ASR),\ncombining unsupervised and supervised AL methods. The first stage utilizes\nunsupervised AL by using x-vectors clustering for diverse sample selection from\nunlabeled speech data, thus establishing a robust initial dataset for the\nsubsequent supervised AL. The second stage incorporates a supervised AL\nstrategy, with a batch AL method specifically developed for ASR, aimed at\nselecting diverse and informative batches of samples. Here, sample diversity is\nalso achieved using x-vectors clustering, while the most informative samples\nare identified using a Bayesian AL method tailored for ASR with an adaptation\nof Monte Carlo dropout to approximate Bayesian inference. This approach enables\nprecise uncertainty estimation, thereby enhancing ASR model training with\nsignificantly reduced data requirements. Our method has shown superior\nperformance compared to competing methods on homogeneous, heterogeneous, and\nOOD test sets, demonstrating that strategic sample selection and innovative\nBayesian modeling can substantially optimize both labeling effort and data\nutilization in deep learning-based ASR applications.", "published": "2024-05-03 19:24:41", "link": "http://arxiv.org/abs/2406.02566v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Joint sentiment analysis of lyrics and audio in music", "abstract": "Sentiment or mood can express themselves on various levels in music. In\nautomatic analysis, the actual audio data is usually analyzed, but the lyrics\ncan also play a crucial role in the perception of moods. We first evaluate\nvarious models for sentiment analysis based on lyrics and audio separately. The\ncorresponding approaches already show satisfactory results, but they also\nexhibit weaknesses, the causes of which we examine in more detail. Furthermore,\ndifferent approaches to combining the audio and lyrics results are proposed and\nevaluated. Considering both modalities generally leads to improved performance.\nWe investigate misclassifications and (also intentional) contradictions between\naudio and lyrics sentiment more closely, and identify possible causes. Finally,\nwe address fundamental problems in this research area, such as high\nsubjectivity, lack of data, and inconsistency in emotion taxonomies.", "published": "2024-05-03 10:42:17", "link": "http://arxiv.org/abs/2405.01988v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time multichannel deep speech enhancement in hearing aids:\n  Comparing monaural and binaural processing in complex acoustic scenarios", "abstract": "Deep learning has the potential to enhance speech signals and increase their\nintelligibility for users of hearing aids. Deep models suited for real-world\napplication should feature a low computational complexity and low processing\ndelay of only a few milliseconds. In this paper, we explore deep speech\nenhancement that matches these requirements and contrast monaural and binaural\nprocessing algorithms in two complex acoustic scenes. Both algorithms are\nevaluated with objective metrics and in experiments with hearing-impaired\nlisteners performing a speech-in-noise test. Results are compared to two\ntraditional enhancement strategies, i.e., adaptive differential microphone\nprocessing and binaural beamforming. While in diffuse noise, all algorithms\nperform similarly, the binaural deep learning approach performs best in the\npresence of spatial interferers. Through a post-analysis, this can be\nattributed to improvements at low SNRs and to precise spatial filtering.", "published": "2024-05-03 09:45:09", "link": "http://arxiv.org/abs/2405.01967v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward end-to-end interpretable convolutional neural networks for\n  waveform signals", "abstract": "This paper introduces a novel convolutional neural networks (CNN) framework\ntailored for end-to-end audio deep learning models, presenting advancements in\nefficiency and explainability. By benchmarking experiments on three standard\nspeech emotion recognition datasets with five-fold cross-validation, our\nframework outperforms Mel spectrogram features by up to seven percent. It can\npotentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while\nremaining lightweight. Furthermore, we demonstrate the efficiency and\ninterpretability of the front-end layer using the PhysioNet Heart Sound\nDatabase, illustrating its ability to handle and capture intricate long\nwaveform patterns. Our contributions offer a portable solution for building\nefficient and interpretable models for raw waveform data.", "published": "2024-05-03 02:24:27", "link": "http://arxiv.org/abs/2405.01815v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EnvId: A Metric Learning Approach for Forensic Few-Shot Identification\n  of Unseen Environments", "abstract": "Audio recordings may provide important evidence in criminal investigations.\nOne such case is the forensic association of a recorded audio to its recording\nlocation. For example, a voice message may be the only investigative cue to\nnarrow down the candidate sites for a crime. Up to now, several works provide\nsupervised classification tools for closed-set recording environment\nidentification under relatively clean recording conditions. However, in\nforensic investigations, the candidate locations are case-specific. Thus,\nsupervised learning techniques are not applicable without retraining a\nclassifier on a sufficient amount of training samples for each case and\nrespective candidate set. In addition, a forensic tool has to deal with audio\nmaterial from uncontrolled sources with variable properties and quality. In\nthis work, we therefore attempt a major step towards practical forensic\napplication scenarios. We propose a representation learning framework called\nEnvId, short for environment identification. EnvId avoids case-specific\nretraining by modeling the task as a few-shot classification problem. We\ndemonstrate that EnvId can handle forensically challenging material. It\nprovides good quality predictions even under unseen signal degradations,\nout-of-distribution reverberation characteristics or recording position\nmismatches.", "published": "2024-05-03 14:19:40", "link": "http://arxiv.org/abs/2405.02119v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GMP-TL: Gender-augmented Multi-scale Pseudo-label Enhanced Transfer\n  Learning for Speech Emotion Recognition", "abstract": "The continuous evolution of pre-trained speech models has greatly advanced\nSpeech Emotion Recognition (SER). However, current research typically relies on\nutterance-level emotion labels, inadequately capturing the complexity of\nemotions within a single utterance. In this paper, we introduce GMP-TL, a novel\nSER framework that employs gender-augmented multi-scale pseudo-label (GMP)\nbased transfer learning to mitigate this gap. Specifically, GMP-TL initially\nuses the pre-trained HuBERT, implementing multi-task learning and multi-scale\nk-means clustering to acquire frame-level GMPs. Subsequently, to fully leverage\nframe-level GMPs and utterance-level emotion labels, a two-stage model\nfine-tuning approach is presented to further optimize GMP-TL. Experiments on\nIEMOCAP show that our GMP-TL attains a WAR of 80.0% and an UAR of 82.0%,\nachieving superior performance compared to state-of-the-art unimodal SER\nmethods while also yielding comparable results to multimodal SER approaches.", "published": "2024-05-03 14:58:46", "link": "http://arxiv.org/abs/2405.02151v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training-Free Deepfake Voice Recognition by Leveraging Large-Scale\n  Pre-Trained Models", "abstract": "Generalization is a main issue for current audio deepfake detectors, which\nstruggle to provide reliable results on out-of-distribution data. Given the\nspeed at which more and more accurate synthesis methods are developed, it is\nvery important to design techniques that work well also on data they were not\ntrained for. In this paper we study the potential of large-scale pre-trained\nmodels for audio deepfake detection, with special focus on generalization\nability. To this end, the detection problem is reformulated in a speaker\nverification framework and fake audios are exposed by the mismatch between the\nvoice sample under test and the voice of the claimed identity. With this\nparadigm, no fake speech sample is necessary in training, cutting off any link\nwith the generation method at the root, and ensuring full generalization\nability. Features are extracted by general-purpose large pre-trained models,\nwith no need for training or fine-tuning on specific fake detection or speaker\nverification datasets. At detection time only a limited set of voice fragments\nof the identity under test is required. Experiments on several datasets\nwidespread in the community show that detectors based on pre-trained models\nachieve excellent performance and show strong generalization ability, rivaling\nsupervised methods on in-distribution data and largely overcoming them on\nout-of-distribution data.", "published": "2024-05-03 15:27:11", "link": "http://arxiv.org/abs/2405.02179v3", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
