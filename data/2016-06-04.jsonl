{"title": "Improving Coreference Resolution by Learning Entity-Level Distributed\n  Representations", "abstract": "A long-standing challenge in coreference resolution has been the\nincorporation of entity-level information - features defined over clusters of\nmentions instead of mention pairs. We present a neural network based\ncoreference system that produces high-dimensional vector representations for\npairs of coreference clusters. Using these representations, our system learns\nwhen combining clusters is desirable. We train the system with a\nlearning-to-search algorithm that teaches it which local decisions (cluster\nmerges) will lead to a high-scoring final coreference partition. The system\nsubstantially outperforms the current state-of-the-art on the English and\nChinese portions of the CoNLL 2012 Shared Task dataset despite using few\nhand-engineered features.", "published": "2016-06-04 04:08:45", "link": "http://arxiv.org/abs/1606.01323v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Fine-grained Entity Type Classification", "abstract": "In this work, we investigate several neural network architectures for\nfine-grained entity type classification. Particularly, we consider extensions\nto a recently proposed attentive neural architecture and make three key\ncontributions. Previous work on attentive neural architectures do not consider\nhand-crafted features, we combine learnt and hand-crafted features and observe\nthat they complement each other. Additionally, through quantitative analysis we\nestablish that the attention mechanism is capable of learning to attend over\nsyntactic heads and the phrase containing the mention, where both are known\nstrong hand-crafted features for our task. We enable parameter sharing through\na hierarchical label encoding method, that in low-dimensional projections show\nclear clusters for each type hierarchy. Lastly, despite using the same\nevaluation dataset, the literature frequently compare models trained using\ndifferent data. We establish that the choice of training data has a drastic\nimpact on performance, with decreases by as much as 9.85% loose micro F1 score\nfor a previously proposed method. Despite this, our best model achieves\nstate-of-the-art results with 75.36% loose micro F1 score on the well-\nestablished FIGER (GOLD) dataset.", "published": "2016-06-04 07:52:22", "link": "http://arxiv.org/abs/1606.01341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint\n  Inference for Clinical Temporal Information Extraction", "abstract": "We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval\nchallenge, participating in Phase 1, where we identified text spans of time and\nevent expressions in clinical notes and Phase 2, where we predicted a relation\nbetween an event and its parent document creation time.\n  For temporal entity extraction, we find that a joint inference-based approach\nusing structured prediction outperforms a vanilla recurrent neural network that\nincorporates word embeddings trained on a variety of large clinical document\nsets. For document creation time relations, we find that a combination of date\ncanonicalization and distant supervision rules for predicting relations on both\nevents and time expressions improves classification, though gains are limited,\nlikely due to the small scale of training data.", "published": "2016-06-04 23:22:41", "link": "http://arxiv.org/abs/1606.01433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Natural Language Inference Chains", "abstract": "The ability to reason with natural language is a fundamental prerequisite for\nmany NLP tasks such as information extraction, machine translation and question\nanswering. To quantify this ability, systems are commonly tested whether they\ncan recognize textual entailment, i.e., whether one sentence can be inferred\nfrom another one. However, in most NLP applications only single source\nsentences instead of sentence pairs are available. Hence, we propose a new task\nthat measures how well a model can generate an entailed sentence from a source\nsentence. We take entailment-pairs of the Stanford Natural Language Inference\ncorpus and train an LSTM with attention. On a manually annotated test set we\nfound that 82% of generated sentences are correct, an improvement of 10.3% over\nan LSTM baseline. A qualitative analysis shows that this model is not only\ncapable of shortening input sentences, but also inferring new statements via\nparaphrasing and phrase entailment. We then apply this model recursively to\ninput-output pairs, thereby generating natural language inference chains that\ncan be used to automatically construct an entailment graph from source\nsentences. Finally, by swapping source and target sentences we can also train a\nmodel that given an input sentence invents additional information to generate a\nnew sentence.", "published": "2016-06-04 18:34:51", "link": "http://arxiv.org/abs/1606.01404v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
