{"title": "Improving Diversity of Neural Text Generation via Inverse Probability\n  Weighting", "abstract": "The neural text generation suffers from the text degeneration issue such as\nrepetition. Traditional stochastic sampling methods only focus on truncating\nthe unreliable \"tail\" of the distribution, and do not address the \"head\" part,\nwhich we show might contain tedious or even repetitive candidates with high\nprobability that lead to repetition loops. They also do not consider the issue\nthat human text does not always favor high-probability words. Inspired by\nthese, in this work we propose a heuristic sampling method. We propose to use\ninterquartile range of the predicted distribution to determine the \"head\" part,\nthen permutate and rescale the \"head\" with inverse probability. This aims at\ndecreasing the probability for the tedious and possibly repetitive candidates\nwith higher probability, and increasing the probability for the rational but\nmore surprising candidates with lower probability. The proposed algorithm\nprovides a reasonable permutation on the predicted distribution which enhances\ndiversity without compromising rationality of the distribution. We use\npre-trained language model to compare our algorithm with traditional methods.\nResults show that our algorithm can effectively increase the diversity of\ngenerated samples while achieving close resemblance to human text.", "published": "2021-03-13 08:17:40", "link": "http://arxiv.org/abs/2103.07649v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Machine Reading Comprehension for Aspect Sentiment Triplet\n  Extraction", "abstract": "Aspect sentiment triplet extraction (ASTE), which aims to identify aspects\nfrom review sentences along with their corresponding opinion expressions and\nsentiments, is an emerging task in fine-grained opinion mining. Since ASTE\nconsists of multiple subtasks, including opinion entity extraction, relation\ndetection, and sentiment classification, it is critical and challenging to\nappropriately capture and utilize the associations among them. In this paper,\nwe transform ASTE task into a multi-turn machine reading comprehension (MTMRC)\ntask and propose a bidirectional MRC (BMRC) framework to address this\nchallenge. Specifically, we devise three types of queries, including\nnon-restrictive extraction queries, restrictive extraction queries and\nsentiment classification queries, to build the associations among different\nsubtasks. Furthermore, considering that an aspect sentiment triplet can derive\nfrom either an aspect or an opinion expression, we design a bidirectional MRC\nstructure. One direction sequentially recognizes aspects, opinion expressions,\nand sentiments to obtain triplets, while the other direction identifies opinion\nexpressions first, then aspects, and at last sentiments. By making the two\ndirections complement each other, our framework can identify triplets more\ncomprehensively. To verify the effectiveness of our approach, we conduct\nextensive experiments on four benchmark datasets. The experimental results\ndemonstrate that BMRC achieves state-of-the-art performances.", "published": "2021-03-13 09:30:47", "link": "http://arxiv.org/abs/2103.07665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Transformer with Stacked Pointer Networks for Conversational\n  Question Answering over Knowledge Graphs", "abstract": "Neural semantic parsing approaches have been widely used for Question\nAnswering (QA) systems over knowledge graphs. Such methods provide the\nflexibility to handle QA datasets with complex queries and a large number of\nentities. In this work, we propose a novel framework named CARTON, which\nperforms multi-task semantic parsing for handling the problem of conversational\nquestion answering over a large-scale knowledge graph. Our framework consists\nof a stack of pointer networks as an extension of a context transformer model\nfor parsing the input question and the dialog history. The framework generates\na sequence of actions that can be executed on the knowledge graph. We evaluate\nCARTON on a standard dataset for complex sequential question answering on which\nCARTON outperforms all baselines. Specifically, we observe performance\nimprovements in F1-score on eight out of ten question types compared to the\nprevious state of the art. For logical reasoning questions, an improvement of\n11 absolute points is reached.", "published": "2021-03-13 18:16:43", "link": "http://arxiv.org/abs/2103.07766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParaQA: A Question Answering Dataset with Paraphrase Responses for\n  Single-Turn Conversation", "abstract": "This paper presents ParaQA, a question answering (QA) dataset with multiple\nparaphrased responses for single-turn conversation over knowledge graphs (KG).\nThe dataset was created using a semi-automated framework for generating diverse\nparaphrasing of the answers using techniques such as back-translation. The\nexisting datasets for conversational question answering over KGs\n(single-turn/multi-turn) focus on question paraphrasing and provide only up to\none answer verbalization. However, ParaQA contains 5000 question-answer pairs\nwith a minimum of two and a maximum of eight unique paraphrased responses for\neach question. We complement the dataset with baseline models and illustrate\nthe advantage of having multiple paraphrased answers through commonly used\nmetrics such as BLEU and METEOR. The ParaQA dataset is publicly available on a\npersistent URI for broader usage and adaptation in the research community.", "published": "2021-03-13 18:53:07", "link": "http://arxiv.org/abs/2103.07771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Discourse Analysis for Generating Personalized Feedback in\n  Intelligent Tutor Systems", "abstract": "We explore creating automated, personalized feedback in an intelligent\ntutoring system (ITS). Our goal is to pinpoint correct and incorrect concepts\nin student answers in order to achieve better student learning gains. Although\nautomatic methods for providing personalized feedback exist, they do not\nexplicitly inform students about which concepts in their answers are correct or\nincorrect. Our approach involves decomposing students answers using neural\ndiscourse segmentation and classification techniques. This decomposition yields\na relational graph over all discourse units covered by the reference solutions\nand student answers. We use this inferred relational graph structure and a\nneural classifier to match student answers with reference solutions and\ngenerate personalized feedback. Although the process is completely automated\nand data-driven, the personalized feedback generated is highly contextual,\ndomain-aware and effectively targets each student's misconceptions and\nknowledge gaps. We test our method in a dialogue-based ITS and demonstrate that\nour approach results in high-quality feedback and significantly improved\nstudent learning gains.", "published": "2021-03-13 20:33:10", "link": "http://arxiv.org/abs/2103.07785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approximating How Single Head Attention Learns", "abstract": "Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness. We end by discussing the limitation of our approximation\nframework and suggest future directions.", "published": "2021-03-13 02:32:19", "link": "http://arxiv.org/abs/2103.07601v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Targeted aspect based multimodal sentiment analysis:an attention capsule\n  extraction and multi-head fusion network", "abstract": "Multimodal sentiment analysis has currently identified its significance in a\nvariety of domains. For the purpose of sentiment analysis, different aspects of\ndistinguishing modalities, which correspond to one target, are processed and\nanalyzed. In this work, we propose the targeted aspect-based multimodal\nsentiment analysis (TABMSA) for the first time. Furthermore, an attention\ncapsule extraction and multi-head fusion network (EF-Net) on the task of TABMSA\nis devised. The multi-head attention (MHA) based network and the ResNet-152 are\nemployed to deal with texts and images, respectively. The integration of MHA\nand capsule network aims to capture the interaction among the multimodal\ninputs. In addition to the targeted aspect, the information from the context\nand the image is also incorporated for sentiment delivered. We evaluate the\nproposed model on two manually annotated datasets. the experimental results\ndemonstrate the effectiveness of our proposed model for this new task.", "published": "2021-03-13 09:11:24", "link": "http://arxiv.org/abs/2103.07659v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene\n  Grounding", "abstract": "To effectively apply robots in working environments and assist humans, it is\nessential to develop and evaluate how visual grounding (VG) can affect machine\nperformance on occluded objects. However, current VG works are limited in\nworking environments, such as offices and warehouses, where objects are usually\noccluded due to space utilization issues. In our work, we propose a novel\nOCID-Ref dataset featuring a referring expression segmentation task with\nreferring expressions of occluded objects. OCID-Ref consists of 305,694\nreferring expressions from 2,300 scenes with providing RGB image and point\ncloud inputs. To resolve challenging occlusion issues, we argue that it's\ncrucial to take advantage of both 2D and 3D signals to resolve challenging\nocclusion issues. Our experimental results demonstrate the effectiveness of\naggregating 2D and 3D signals but referring to occluded objects still remains\nchallenging for the modern visual grounding systems. OCID-Ref is publicly\navailable at https://github.com/lluma/OCID-Ref", "published": "2021-03-13 10:38:15", "link": "http://arxiv.org/abs/2103.07679v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent\n  Prediction and Slot Filling", "abstract": "Predicting user intent and detecting the corresponding slots from text are\ntwo key problems in Natural Language Understanding (NLU). In the context of\nzero-shot learning, this task is typically approached by either using\nrepresentations from pre-trained multilingual transformers such as mBERT, or by\nmachine translating the source data into the known target language and then\nfine-tuning. Our work focuses on a particular scenario where the target\nlanguage is unknown during training. To this goal, we propose a novel method to\naugment the monolingual source data using multilingual code-switching via\nrandom translations to enhance a transformer's language neutrality when\nfine-tuning it for a downstream task. This method also helps discover novel\ninsights on how code-switching with different language families around the\nworld impact the performance on the target language. Experiments on the\nbenchmark dataset of MultiATIS++ yielded an average improvement of +4.2% in\naccuracy for intent task and +1.8% in F1 for slot task using our method over\nthe state-of-the-art across 8 different languages. Furthermore, we present an\napplication of our method for crisis informatics using a new human-annotated\ntweet dataset of slot filling in English and Haitian Creole, collected during\nHaiti earthquake disaster.", "published": "2021-03-13 21:05:09", "link": "http://arxiv.org/abs/2103.07792v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimal Embedding Calibration for Symbolic Music Similarity", "abstract": "In natural language processing (NLP), the semantic similarity task requires\nlarge-scale, high-quality human-annotated labels for fine-tuning or evaluation.\nBy contrast, in cases of music similarity, such labels are expensive to collect\nand largely dependent on the annotator's artistic preferences. Recent research\nhas demonstrated that embedding calibration technique can greatly increase\nsemantic similarity performance of the pre-trained language model without\nfine-tuning. However, it is yet unknown which calibration method is the best\nand how much performance improvement can be achieved. To address these issues,\nwe propose using composer information to construct labels for automatically\nevaluating music similarity. Under this paradigm, we discover the optimal\ncombination of embedding calibration which achieves superior metrics than the\nbaseline methods.", "published": "2021-03-13 08:36:39", "link": "http://arxiv.org/abs/2103.07656v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OkwuGb\u00e9: End-to-End Speech Recognition for Fon and Igbo", "abstract": "Language is inherent and compulsory for human communication. Whether\nexpressed in a written or spoken way, it ensures understanding between people\nof the same and different regions. With the growing awareness and effort to\ninclude more low-resourced languages in NLP research, African languages have\nrecently been a major subject of research in machine translation, and other\ntext-based areas of NLP. However, there is still very little comparable\nresearch in speech recognition for African languages. Interestingly, some of\nthe unique properties of African languages affecting NLP, like their\ndiacritical and tonal complexities, have a major root in their speech,\nsuggesting that careful speech interpretation could provide more intuition on\nhow to deal with the linguistic complexities of African languages for\ntext-based NLP. OkwuGb\\'e is a step towards building speech recognition systems\nfor African low-resourced languages. Using Fon and Igbo as our case study, we\nconduct a comprehensive linguistic analysis of each language and describe the\ncreation of end-to-end, deep neural network-based speech recognition models for\nboth languages. We present a state-of-art ASR model for Fon, as well as\nbenchmark ASR model results for Igbo. Our linguistic analyses (for Fon and\nIgbo) provide valuable insights and guidance into the creation of speech\nrecognition models for other African low-resourced languages, as well as guide\nfuture NLP research for Fon and Igbo. The Fon and Igbo models source code have\nbeen made publicly available.", "published": "2021-03-13 18:02:44", "link": "http://arxiv.org/abs/2103.07762v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Supervised Learning in the Presence of Noise: Application in ICD-10 Code\n  Classification", "abstract": "ICD coding is the international standard for capturing and reporting health\nconditions and diagnosis for revenue cycle management in healthcare. Manually\nassigning ICD codes is prone to human error due to the large code vocabulary\nand the similarities between codes. Since machine learning based approaches\nrequire ground truth training data, the inconsistency among human coders is\nmanifested as noise in labeling, which makes the training and evaluation of ICD\nclassifiers difficult in presence of such noise. This paper investigates the\ncharacteristics of such noise in manually-assigned ICD-10 codes and\nfurthermore, proposes a method to train robust ICD-10 classifiers in the\npresence of labeling noise. Our research concluded that the nature of such\nnoise is systematic. Most of the existing methods for handling label noise\nassume that the noise is completely random and independent of features or\nlabels, which is not the case for ICD data. Therefore, we develop a new method\nfor training robust classifiers in the presence of systematic noise. We first\nidentify ICD-10 codes that human coders tend to misuse or confuse, based on the\ncodes' locations in the ICD-10 hierarchy, the types of the codes, and baseline\nclassifier's prediction behaviors; we then develop a novel training strategy\nthat accounts for such noise. We compared our method with the baseline that\ndoes not handle label noise and the baseline methods that assume random noise,\nand demonstrated that our proposed method outperforms all baselines when\nevaluated on expert validated labels.", "published": "2021-03-13 23:05:50", "link": "http://arxiv.org/abs/2103.07808v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Simpson's Bias in NLP Training", "abstract": "In most machine learning tasks, we evaluate a model $M$ on a given data\npopulation $S$ by measuring a population-level metric $F(S;M)$. Examples of\nsuch evaluation metric $F$ include precision/recall for (binary) recognition,\nthe F1 score for multi-class classification, and the BLEU metric for language\ngeneration. On the other hand, the model $M$ is trained by optimizing a\nsample-level loss $G(S_t;M)$ at each learning step $t$, where $S_t$ is a subset\nof $S$ (a.k.a. the mini-batch). Popular choices of $G$ include cross-entropy\nloss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption\nbehind this paradigm is that the mean value of the sample-level loss $G$, if\naveraged over all possible samples, should effectively represent the\npopulation-level metric $F$ of the task, such as, that $\\mathbb{E}[ G(S_t;M) ]\n\\approx F(S;M)$.\n  In this paper, we systematically investigate the above assumption in several\nNLP tasks. We show, both theoretically and experimentally, that some popular\ndesigns of the sample-level loss $G$ may be inconsistent with the true\npopulation-level metric $F$ of the task, so that models trained to optimize the\nformer can be substantially sub-optimal to the latter, a phenomenon we call it,\nSimpson's bias, due to its deep connections with the classic paradox known as\nSimpson's reversal paradox in statistics and social sciences.", "published": "2021-03-13 06:19:37", "link": "http://arxiv.org/abs/2103.11795v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Impact of the COVID-19 outbreak on Italy's country reputation and stock\n  market performance: a sentiment analysis approach", "abstract": "During the recent Coronavirus disease 2019 (COVID-19) outbreak, the\nmicroblogging service Twitter has been widely used to share opinions and\nreactions to events. Italy was one of the first European countries to be\nseverely affected by the outbreak and to establish lockdown and stay-at-home\norders, potentially leading to country reputation damage. We resort to\nsentiment analysis to investigate changes in opinions about Italy reported on\nTwitter before and after the COVID-19 outbreak. Using different lexicons-based\nmethods, we find a breakpoint corresponding to the date of the first\nestablished case of COVID-19 in Italy that causes a relevant change in\nsentiment scores used as proxy of the country reputation. Next, we demonstrate\nthat sentiment scores about Italy are strongly associated with the levels of\nthe FTSE-MIB index, the Italian Stock Exchange main index, as they serve as\nearly detection signals of changes in the values of FTSE-MIB. Finally, we make\na content-based classification of tweets into positive and negative and use two\nmachine learning classifiers to validate the assigned polarity of tweets posted\nbefore and after the outbreak.", "published": "2021-03-13 14:03:11", "link": "http://arxiv.org/abs/2103.13871v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Automated Fact-Checking for Assisting Human Fact-Checkers", "abstract": "The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.", "published": "2021-03-13 18:29:14", "link": "http://arxiv.org/abs/2103.07769v2", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.AI"}
{"title": "A Survey on Multimodal Disinformation Detection", "abstract": "Recent years have witnessed the proliferation of offensive content online\nsuch as fake news, propaganda, misinformation, and disinformation. While\ninitially this was mostly about textual content, over time images and videos\ngained popularity, as they are much easier to consume, attract more attention,\nand spread further than text. As a result, researchers started leveraging\ndifferent modalities and combinations thereof to tackle online multimodal\noffensive content. In this study, we offer a survey on the state-of-the-art on\nmultimodal disinformation detection covering various combinations of\nmodalities: text, images, speech, video, social media network structure, and\ntemporal information. Moreover, while some studies focused on factuality,\nothers investigated how harmful the content is. While these two components in\nthe definition of disinformation (i) factuality, and (ii) harmfulness, are\nequally important, they are typically studied in isolation. Thus, we argue for\nthe need to tackle disinformation detection by taking into account multiple\nmodalities as well as both factuality and harmfulness, in the same framework.\nFinally, we discuss current challenges and future research directions", "published": "2021-03-13 18:04:17", "link": "http://arxiv.org/abs/2103.12541v2", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.MM"}
