{"title": "Contextualization of Morphological Inflection", "abstract": "Critical to natural language generation is the production of correctly\ninflected text. In this paper, we isolate the task of predicting a fully\ninflected sentence from its partially lemmatized version. Unlike traditional\nmorphological inflection or surface realization, our task input does not\nprovide ``gold'' tags that specify what morphological features to realize on\neach lemmatized word; rather, such features must be inferred from sentential\ncontext. We develop a neural hybrid graphical model that explicitly\nreconstructs morphological features before predicting the inflected forms, and\ncompare this to a system that directly predicts the inflected forms without\nrelying on any morphological annotation. We experiment on several typologically\ndiverse languages from the Universal Dependencies treebanks, showing the\nutility of incorporating linguistically-motivated latent variables into NLP\nmodels.", "published": "2019-05-04 03:22:55", "link": "http://arxiv.org/abs/1905.01420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Denoise Distantly-Labeled Data for Entity Typing", "abstract": "Distantly-labeled data can be used to scale up training of statistical\nmodels, but it is typically noisy and that noise can vary with the distant\nlabeling technique. In this work, we propose a two-stage procedure for handling\nthis type of data: denoise it with a learned model, then train our final model\non clean and denoised distant data with standard supervised training. Our\ndenoising approach consists of two parts. First, a filtering function discards\nexamples from the distantly labeled data that are wholly unusable. Second, a\nrelabeling function repairs noisy labels for the retained examples. Each of\nthese components is a model trained on synthetically-noised examples generated\nfrom a small manually-labeled set. We investigate this approach on the\nultra-fine entity typing task of Choi et al. (2018). Our baseline model is an\nextension of their model with pre-trained ELMo representations, which already\nachieves state-of-the-art performance. Adding distant data that has been\ndenoised with our learned models gives further performance gains over this base\nmodel, outperforming models trained on raw distant data or\nheuristically-denoised distant data.", "published": "2019-05-04 23:22:51", "link": "http://arxiv.org/abs/1905.01566v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The method of automatic summarization from different sources", "abstract": "In this article is analyzed technology of automatic text abstracting and\nannotation. The role of annotation in automatic search and classification for\ndifferent scientific articles is described. The algorithm of summarization of\nnatural language documents using the concept of importance coefficients is\ndeveloped. Such concept allows considering the peculiarity of subject areas and\ntopics that could be found in different kinds of documents. Method for\ngenerating abstracts of single document based on frequency analysis is\ndeveloped. The recognition elements for unstructured text analysis are given.\nThe method of pre-processing analysis of several documents is developed. This\ntechnique simultaneously considers both statistical approaches to abstracting\nand the importance of terms in a particular subject domain. The quality of\ngenerated abstract is evaluated. For the developed system there was conducted\nexperts evaluation. It was held only for texts in Ukrainian. The developed\nsystem concluding essay has higher aggregate score on all criteria. The\nsummarization system architecture is building. To build an information system\nmodel there is used CASE-tool AllFusion ERwin Data Modeler. The database scheme\nfor information saving was built. The system is designed to work primarily with\nUkrainian texts, which gives a significant advantage, since most modern systems\nstill oriented to English texts", "published": "2019-05-04 02:56:14", "link": "http://arxiv.org/abs/1905.02623v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
