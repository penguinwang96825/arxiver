{"title": "Zero-Resource Neural Machine Translation with Multi-Agent Communication\n  Game", "abstract": "While end-to-end neural machine translation (NMT) has achieved notable\nsuccess in the past years in translating a handful of resource-rich language\npairs, it still suffers from the data scarcity problem for low-resource\nlanguage pairs and domains. To tackle this problem, we propose an interactive\nmultimodal framework for zero-resource neural machine translation. Instead of\nbeing passively exposed to large amounts of parallel corpora, our learners\n(implemented as encoder-decoder architecture) engage in cooperative image\ndescription games, and thus develop their own image captioning or neural\nmachine translation model from the need to communicate in order to succeed at\nthe game. Experimental results on the IAPR-TC12 and Multi30K datasets show that\nthe proposed learning mechanism significantly improves over the\nstate-of-the-art methods.", "published": "2018-02-09 03:53:13", "link": "http://arxiv.org/abs/1802.03116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Librispeech with French Translations: A Multimodal Corpus for\n  Direct Speech Translation Evaluation", "abstract": "Recent works in spoken language translation (SLT) have attempted to build\nend-to-end speech-to-text translation without using source language\ntranscription during learning or decoding. However, while large quantities of\nparallel texts (such as Europarl, OpenSubtitles) are available for training\nmachine translation systems, there are no large (100h) and open source parallel\ncorpora that include speech in a source language aligned to text in a target\nlanguage. This paper tries to fill this gap by augmenting an existing\n(monolingual) corpus: LibriSpeech. This corpus, used for automatic speech\nrecognition, is derived from read audiobooks from the LibriVox project, and has\nbeen carefully segmented and aligned. After gathering French e-books\ncorresponding to the English audio-books from LibriSpeech, we align speech\nsegments at the sentence level with their respective translations and obtain\n236h of usable parallel data. This paper presents the details of the processing\nas well as a manual evaluation conducted on a small subset of the corpus. This\nevaluation shows that the automatic alignments scores are reasonably correlated\nwith the human judgments of the bilingual alignment quality. We believe that\nthis corpus (which is made available online) is useful for replicable\nexperiments in direct speech translation or more general spoken language\ntranslation experiments.", "published": "2018-02-09 06:29:43", "link": "http://arxiv.org/abs/1802.03142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Inference over Interaction Space: ICLR 2018\n  Reproducibility Report", "abstract": "We have tried to reproduce the results of the paper \"Natural Language\nInference over Interaction Space\" submitted to ICLR 2018 conference as part of\nthe ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the\ncode was available, so we started to implement the network from scratch. We\nhave evaluated our version of the model on Stanford NLI dataset and reached\n86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The\nmain difference, as we understand it, comes from the optimizers and the way\nmodel selection is performed.", "published": "2018-02-09 10:49:47", "link": "http://arxiv.org/abs/1802.03198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Network-Based Semantic Variational Autoencoder for\n  Sequence-to-Sequence Learning", "abstract": "Sequence-to-sequence (Seq2seq) models have played an important role in the\nrecent success of various natural language processing methods, such as machine\ntranslation, text summarization, and speech recognition. However, current\nSeq2seq models have trouble preserving global latent information from a long\nsequence of words. Variational autoencoder (VAE) alleviates this problem by\nlearning a continuous semantic space of the input sentence. However, it does\nnot solve the problem completely. In this paper, we propose a new recurrent\nneural network (RNN)-based Seq2seq model, RNN semantic variational autoencoder\n(RNN--SVAE), to better capture the global latent information of a sequence of\nwords. To reflect the meaning of words in a sentence properly, without regard\nto its position within the sentence, we construct a document information vector\nusing the attention information between the final state of the encoder and\nevery prior hidden state. Then, the mean and standard deviation of the\ncontinuous semantic space are learned by using this vector to take advantage of\nthe variational method. By using the document information vector to find the\nsemantic space of the sentence, it becomes possible to better capture the\nglobal latent feature of the sentence. Experimental results of three natural\nlanguage tasks (i.e., language modeling, missing word imputation, paraphrase\nidentification) confirm that the proposed RNN--SVAE yields higher performance\nthan two benchmark models.", "published": "2018-02-09 12:58:29", "link": "http://arxiv.org/abs/1802.03238v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making \"fetch\" happen: The influence of social and linguistic context on\n  nonstandard word growth and decline", "abstract": "In an online community, new words come and go: today's \"haha\" may be replaced\nby tomorrow's \"lol.\" Changes in online writing are usually studied as a social\nprocess, with innovations diffusing through a network of individuals in a\nspeech community. But unlike other types of innovation, language change is\nshaped and constrained by the system in which it takes part. To investigate the\nlinks between social and structural factors in language change, we undertake a\nlarge-scale analysis of nonstandard word growth in the online community Reddit.\nWe find that dissemination across many linguistic contexts is a sign of growth:\nwords that appear in more linguistic contexts grow faster and survive longer.\nWe also find that social dissemination likely plays a less important role in\nexplaining word growth and decline than previously hypothesized.", "published": "2018-02-09 15:32:01", "link": "http://arxiv.org/abs/1802.04140v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Efficient Neural Architecture Search via Parameter Sharing", "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.", "published": "2018-02-09 14:14:37", "link": "http://arxiv.org/abs/1802.03268v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Complex ISNMF: a Phase-Aware Model for Monaural Audio Source Separation", "abstract": "This paper introduces a phase-aware probabilistic model for audio source\nseparation. Classical source models in the short-term Fourier transform domain\nuse circularly-symmetric Gaussian or Poisson random variables. This is\nequivalent to assuming that the phase of each source is uniformly distributed,\nwhich is not suitable for exploiting the underlying structure of the phase.\nDrawing on preliminary works, we introduce here a Bayesian anisotropic Gaussian\nsource model in which the phase is no longer uniform. Such a model permits us\nto favor a phase value that originates from a signal model through a Markov\nchain prior structure. The variance of the latent variables are structured with\nnonnegative matrix factorization (NMF). The resulting model is called complex\nItakura-Saito NMF (ISNMF) since it generalizes the ISNMF model to the case of\nnon-isotropic variables. It combines the advantages of ISNMF, which uses a\ndistortion measure adapted to audio and yields a set of estimates which\npreserve the overall energy of the mixture, and of complex NMF, which enables\none to account for some phase constraints. We derive a generalized\nexpectation-maximization algorithm to estimate the model parameters.\nExperiments conducted on a musical source separation task in a semi-informed\nsetting show that the proposed approach outperforms state-of-the-art\nphase-aware separation techniques.", "published": "2018-02-09 07:38:05", "link": "http://arxiv.org/abs/1802.03156v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modeling of Teager Energy Operated Perceptual Wavelet Packet\n  Coefficients with an Erlang-2 PDF for Real Time Enhancement of Noisy Speech", "abstract": "In this paper, for real time enhancement of noisy speech, a method of\nthreshold determination based on modeling of Teager energy (TE) operated\nperceptual wavelet packet (PWP) coefficients of the noisy speech and noise by\nan Erlang-2 PDF is presented. The proposed method is computationally much\nfaster than the existing wavelet packet based thresholding methods. A custom\nthresholding function based on a combination of mu-law and semisoft\nthresholding functions is designed and exploited to apply the statistically\nderived threshold upon the PWP coefficients. The proposed custom thresholding\nfunction works as a mu-law or a semisoft thresholding function or their\ncombination based on the probability of speech presence and absence in a\nsubband of the PWP transformed noisy speech. By using the speech files\navailable in NOIZEUS database, a number of simulations are performed to\nevaluate the performance of the proposed method for speech signals in the\npresence of Gaussian white and street noises. The proposed method outperforms\nsome of the state-of-the-art speech enhancement methods both at high and low\nlevels of SNRs in terms of standard objective measures and subjective\nevaluations including formal listening tests.", "published": "2018-02-09 22:46:15", "link": "http://arxiv.org/abs/1802.03472v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting Audio Advertisement Quality", "abstract": "Online audio advertising is a particular form of advertising used abundantly\nin online music streaming services. In these platforms, which tend to host tens\nof thousands of unique audio advertisements (ads), providing high quality ads\nensures a better user experience and results in longer user engagement.\nTherefore, the automatic assessment of these ads is an important step toward\naudio ads ranking and better audio ads creation. In this paper we propose one\nway to measure the quality of the audio ads using a proxy metric called Long\nClick Rate (LCR), which is defined by the amount of time a user engages with\nthe follow-up display ad (that is shown while the audio ad is playing) divided\nby the impressions. We later focus on predicting the audio ad quality using\nonly acoustic features such as harmony, rhythm, and timbre of the audio,\nextracted from the raw waveform. We discuss how the characteristics of the\nsound can be connected to concepts such as the clarity of the audio ad message,\nits trustworthiness, etc. Finally, we propose a new deep learning model for\naudio ad quality prediction, which outperforms the other discussed models\ntrained on hand-crafted features. To the best of our knowledge, this is the\nfirst large-scale audio ad quality prediction study.", "published": "2018-02-09 15:59:09", "link": "http://arxiv.org/abs/1802.03319v1", "categories": ["stat.ML", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
