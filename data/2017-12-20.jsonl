{"title": "An Ensemble Model with Ranking for Social Dialogue", "abstract": "Open-domain social dialogue is one of the long-standing goals of Artificial\nIntelligence. This year, the Amazon Alexa Prize challenge was announced for the\nfirst time, where real customers get to rate systems developed by leading\nuniversities worldwide. The aim of the challenge is to converse \"coherently and\nengagingly with humans on popular topics for 20 minutes\". We describe our Alexa\nPrize system (called 'Alana') consisting of an ensemble of bots, combining\nrule-based and machine learning systems, and using a contextual ranking\nmechanism to choose a system response. The ranker was trained on real user\nfeedback received during the competition, where we address the problem of how\nto train on the noisy and sparse feedback obtained during the competition.", "published": "2017-12-20 16:28:48", "link": "http://arxiv.org/abs/1712.07558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ethical Questions in NLP Research: The (Mis)-Use of Forensic Linguistics", "abstract": "Ideas from forensic linguistics are now being used frequently in Natural\nLanguage Processing (NLP), using machine learning techniques. While the role of\nforensic linguistics was more benign earlier, it is now being used for purposes\nwhich are questionable. Certain methods from forensic linguistics are employed,\nwithout considering their scientific limitations and ethical concerns. While we\ntake the specific case of forensic linguistics as an example of such trends in\nNLP and machine learning, the issue is a larger one and present in many other\nscientific and data-driven domains. We suggest that such trends indicate that\nsome of the applied sciences are exceeding their legal and scientific briefs.\nWe highlight how carelessly implemented practices are serving to short-circuit\nthe due processes of law as well breach ethical codes.", "published": "2017-12-20 15:03:04", "link": "http://arxiv.org/abs/1712.07512v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Context-aware Path Ranking for Knowledge Base Completion", "abstract": "Knowledge base (KB) completion aims to infer missing facts from existing ones\nin a KB. Among various approaches, path ranking (PR) algorithms have received\nincreasing attention in recent years. PR algorithms enumerate paths between\nentity pairs in a KB and use those paths as features to train a model for\nmissing fact prediction. Due to their good performances and high model\ninterpretability, several methods have been proposed. However, most existing\nmethods suffer from scalability (high RAM consumption) and feature explosion\n(trains on an exponentially large number of features) problems. This paper\nproposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems\nby introducing a selective path exploration strategy. C-PR learns global\nsemantics of entities in the KB using word embedding and leverages the\nknowledge of entity semantics to enumerate contextually relevant paths using\nbidirectional random walk. Experimental results on three large KBs show that\nthe path features (fewer in number) discovered by C-PR not only improve\npredictive performance but also are more interpretable than existing baselines.", "published": "2017-12-20 23:10:21", "link": "http://arxiv.org/abs/1712.07745v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Flexible Approach to Automated RNN Architecture Generation", "abstract": "The process of designing neural architectures requires expert knowledge and\nextensive trial and error. While automated architecture search may simplify\nthese requirements, the recurrent neural network (RNN) architectures generated\nby existing methods are limited in both flexibility and components. We propose\na domain-specific language (DSL) for use in automated architecture search which\ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough\nto define standard architectures such as the Gated Recurrent Unit and Long\nShort Term Memory and allows the introduction of non-standard RNN components\nsuch as trigonometric curves and layer normalization. Using two different\ncandidate generation techniques, random search with a ranking function and\nreinforcement learning, we explore the novel architectures produced by the RNN\nDSL for language modeling and machine translation domains. The resulting\narchitectures do not follow human intuition yet perform well on their targeted\ntasks, suggesting the space of usable RNN architectures is far larger than\npreviously assumed.", "published": "2017-12-20 04:20:40", "link": "http://arxiv.org/abs/1712.07316v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Differentially Private Distributed Learning for Language Modeling Tasks", "abstract": "One of the big challenges in machine learning applications is that training\ndata can be different from the real-world data faced by the algorithm. In\nlanguage modeling, users' language (e.g. in private messaging) could change in\na year and be completely different from what we observe in publicly available\ndata. At the same time, public data can be used for obtaining general knowledge\n(i.e. general model of English). We study approaches to distributed fine-tuning\nof a general model on user private data with the additional requirements of\nmaintaining the quality on the general data and minimization of communication\ncosts. We propose a novel technique that significantly improves prediction\nquality on users' language compared to a general model and outperforms gradient\ncompression methods in terms of communication efficiency. The proposed\nprocedure is fast and leads to an almost 70% perplexity reduction and 8.7\npercentage point improvement in keystroke saving rate on informal English\ntexts. We also show that the range of tasks our approach is applicable to is\nnot limited by language modeling only. Finally, we propose an experimental\nframework for evaluating differential privacy of distributed training of\nlanguage models and show that our approach has good privacy guarantees.", "published": "2017-12-20 13:28:13", "link": "http://arxiv.org/abs/1712.07473v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
