{"title": "Evaluating the Supervised and Zero-shot Performance of Multi-lingual\n  Translation Models", "abstract": "We study several methods for full or partial sharing of the decoder\nparameters of multilingual NMT models. We evaluate both fully supervised and\nzero-shot translation performance in 110 unique translation directions using\nonly the WMT 2019 shared task parallel datasets for training. We use additional\ntest sets and re-purpose evaluation methods recently used for unsupervised MT\nin order to evaluate zero-shot translation performance for language pairs where\nno gold-standard parallel data is available. To our knowledge, this is the\nlargest evaluation of multi-lingual translation yet conducted in terms of the\ntotal size of the training data we use, and in terms of the diversity of\nzero-shot translation pairs we evaluate. We conduct an in-depth evaluation of\nthe translation performance of different models, highlighting the trade-offs\nbetween methods of sharing decoder parameters. We find that models which have\ntask-specific decoder parameters outperform models where decoder parameters are\nfully shared across all tasks.", "published": "2019-06-24 00:25:18", "link": "http://arxiv.org/abs/1906.09675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Definition of Japanese Word", "abstract": "The annotation guidelines for Universal Dependencies (UD) stipulate that the\nbasic units of dependency annotation are syntactic words, but it is not clear\nwhat are syntactic words in Japanese. Departing from the long tradition of\nusing phrasal units called bunsetsu for dependency parsing, the current UD\nJapanese treebanks adopt the Short Unit Words. However, we argue that they are\nnot syntactic word as specified by the annotation guidelines. Although we find\nnon-mainstream attempts to linguistically define Japanese words, such\ndefinitions have never been applied to corpus annotation. We discuss the costs\nand benefits of adopting the rather unfamiliar criteria.", "published": "2019-06-24 04:46:10", "link": "http://arxiv.org/abs/1906.09719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposable Neural Paraphrase Generation", "abstract": "Paraphrasing exists at different granularity levels, such as lexical level,\nphrasal level and sentential level. This paper presents Decomposable Neural\nParaphrase Generator (DNPG), a Transformer-based model that can learn and\ngenerate paraphrases of a sentence at different levels of granularity in a\ndisentangled way. Specifically, the model is composed of multiple encoders and\ndecoders with different structures, each of which corresponds to a specific\ngranularity. The empirical study shows that the decomposition mechanism of DNPG\nmakes paraphrase generation more interpretable and controllable. Based on DNPG,\nwe further develop an unsupervised domain adaptation method for paraphrase\ngeneration. Experimental results show that the proposed model achieves\ncompetitive in-domain performance compared to the state-of-the-art neural\nmodels, and significantly better performance when adapting to a new domain.", "published": "2019-06-24 06:35:36", "link": "http://arxiv.org/abs/1906.09741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotionally-Aware Chatbots: A Survey", "abstract": "Textual conversational agent or chatbots' development gather tremendous\ntraction from both academia and industries in recent years. Nowadays, chatbots\nare widely used as an agent to communicate with a human in some services such\nas booking assistant, customer service, and also a personal partner. The\nbiggest challenge in building chatbot is to build a humanizing machine to\nimprove user engagement. Some studies show that emotion is an important aspect\nto humanize machine, including chatbot. In this paper, we will provide a\nsystematic review of approaches in building an emotionally-aware chatbot (EAC).\nAs far as our knowledge, there is still no work focusing on this area. We\npropose three research question regarding EAC studies. We start with the\nhistory and evolution of EAC, then several approaches to build EAC by previous\nstudies, and some available resources in building EAC. Based on our\ninvestigation, we found that in the early development, EAC exploits a simple\nrule-based approach while now most of EAC use neural-based approach. We also\nnotice that most of EAC contain emotion classifier in their architecture, which\nutilize several available affective resources. We also predict that the\ndevelopment of EAC will continue to gain more and more attention from scholars,\nnoted by some recent studies propose new datasets for building EAC in various\nlanguages.", "published": "2019-06-24 08:20:10", "link": "http://arxiv.org/abs/1906.09774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Response Re-ranking Based on Event Causality and Role\n  Factored Tensor Event Embedding", "abstract": "We propose a novel method for selecting coherent and diverse responses for a\ngiven dialogue context. The proposed method re-ranks response candidates\ngenerated from conversational models by using event causality relations between\nevents in a dialogue history and response candidates (e.g., ``be stressed out''\nprecedes ``relieve stress''). We use distributed event representation based on\nthe Role Factored Tensor Model for a robust matching of event causality\nrelations due to limited event causality knowledge of the system. Experimental\nresults showed that the proposed method improved coherency and dialogue\ncontinuity of system responses.", "published": "2019-06-24 09:08:29", "link": "http://arxiv.org/abs/1906.09795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification and Clustering of Arguments with Contextualized Word\n  Embeddings", "abstract": "We experiment with two recent contextualized word embedding methods (ELMo and\nBERT) in the context of open-domain argument search. For the first time, we\nshow how to leverage the power of contextualized word embeddings to classify\nand cluster topic-dependent arguments, achieving impressive results on both\ntasks and across multiple datasets. For argument classification, we improve the\nstate-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8\npercentage points and for the IBM Debater - Evidence Sentences dataset by 7.4\npercentage points. For the understudied task of argument clustering, we propose\na pre-training step which improves by 7.8 percentage points over strong\nbaselines on a novel dataset, and by 12.3 percentage points for the Argument\nFacet Similarity (AFS) Corpus.", "published": "2019-06-24 09:55:21", "link": "http://arxiv.org/abs/1906.09821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modelling Makes Sense: Propagating Representations through\n  WordNet for Full-Coverage Word Sense Disambiguation", "abstract": "Contextual embeddings represent a new generation of semantic representations\nlearned from Neural Language Modelling (NLM) that addresses the issue of\nmeaning conflation hampering traditional word embeddings. In this work, we show\nthat contextual embeddings can be used to achieve unprecedented gains in Word\nSense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level\nembeddings with full-coverage of WordNet, and without recourse to explicit\nknowledge of sense distributions or task-specific modelling. As a result, a\nsimple Nearest Neighbors (k-NN) method using our representations is able to\nconsistently surpass the performance of previous systems using powerful neural\nsequencing models. We also analyse the robustness of our approach when ignoring\npart-of-speech and lemma features, requiring disambiguation against the full\nsense inventory, and revealing shortcomings to be improved. Finally, we explore\napplications of our sense embeddings for concept-level analyses of contextual\nembeddings and their respective NLMs.", "published": "2019-06-24 14:59:12", "link": "http://arxiv.org/abs/1906.10007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in\n  Sentiment Analysis", "abstract": "In this work, we investigate the presence of occupational gender stereotypes\nin sentiment analysis models. Such a task has implications for reducing\nimplicit biases in these models, which are being applied to an increasingly\nwide variety of downstream tasks. We release a new gender-balanced dataset of\n800 sentences pertaining to specific professions and propose a methodology for\nusing it as a test bench to evaluate sentiment analysis models. We evaluate the\npresence of occupational gender stereotypes in 3 different models using our\napproach, and explore their relationship with societal perceptions of\noccupations.", "published": "2019-06-24 22:31:33", "link": "http://arxiv.org/abs/1906.10256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Projection for Targeted Cross-Lingual Sentiment: Model\n  Comparisons and a Real-World Study", "abstract": "Sentiment analysis benefits from large, hand-annotated resources in order to\ntrain and test machine learning models, which are often data hungry. While some\nlanguages, e.g., English, have a vast array of these resources, most\nunder-resourced languages do not, especially for fine-grained sentiment tasks,\nsuch as aspect-level or targeted sentiment analysis. To improve this situation,\nwe propose a cross-lingual approach to sentiment analysis that is applicable to\nunder-resourced languages and takes into account target-level information. This\nmodel incorporates sentiment information into bilingual distributional\nrepresentations, by jointly optimizing them for semantics and sentiment,\nshowing state-of-the-art performance at sentence-level when combined with\nmachine translation. The adaptation to targeted sentiment analysis on multiple\ndomains shows that our model outperforms other projection-based bilingual\nembedding methods on binary targeted sentiment tasks. Our analysis on ten\nlanguages demonstrates that the amount of unlabeled monolingual data has\nsurprisingly little effect on the sentiment results. As expected, the choice of\nannotated source language for projection to a target leads to better results\nfor source-target language pairs which are similar. Therefore, our results\nsuggest that more efforts should be spent on the creation of resources for less\nsimilar languages to those which are resource-rich already. Finally, a domain\nmismatch leads to a decreased performance. This suggests resources in any\nlanguage should ideally cover varieties of domains.", "published": "2019-06-24 08:18:12", "link": "http://arxiv.org/abs/1906.10519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event extraction based on open information extraction and ontology", "abstract": "The work presented in this master thesis consists of extracting a set of\nevents from texts written in natural language. For this purpose, we have based\nourselves on the basic notions of the information extraction as well as the\nopen information extraction. First, we applied an open information\nextraction(OIE) system for the relationship extraction, to highlight the\nimportance of OIEs in event extraction, and we used the ontology to the event\nmodeling. We tested the results of our approach with test metrics. As a result,\nthe two-level event extraction approach has shown good performance results but\nrequires a lot of expert intervention in the construction of classifiers and\nthis will take time. In this context we have proposed an approach that reduces\nthe expert intervention in the relation extraction, the recognition of entities\nand the reasoning which are automatic and based on techniques of adaptation and\ncorrespondence. Finally, to prove the relevance of the extracted results, we\nconducted a set of experiments using different test metrics as well as a\ncomparative study.", "published": "2019-06-24 16:24:46", "link": "http://arxiv.org/abs/1907.00692v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Business Taxonomy Construction Using Concept-Level Hierarchical\n  Clustering", "abstract": "Business taxonomies are indispensable tools for investors to do equity\nresearch and make professional decisions. However, to identify the structure of\nindustry sectors in an emerging market is challenging for two reasons. First,\nexisting taxonomies are designed for mature markets, which may not be the\nappropriate classification for small companies with innovative business models.\nSecond, emerging markets are fast-developing, thus the static business\ntaxonomies cannot promptly reflect the new features. In this article, we\npropose a new method to construct business taxonomies automatically from the\ncontent of corporate annual reports. Extracted concepts are hierarchically\nclustered using greedy affinity propagation. Our method requires less\nsupervision and is able to discover new terms. Experiments and evaluation on\nthe Chinese National Equities Exchange and Quotations (NEEQ) market show\nseveral advantages of the business taxonomy we build. Our results provide an\neffective tool for understanding and investing in the new growth companies.", "published": "2019-06-24 02:59:22", "link": "http://arxiv.org/abs/1906.09694v1", "categories": ["cs.CL", "q-fin.PM", "I.2.4"], "primary_category": "cs.CL"}
{"title": "A Tensorized Transformer for Language Modeling", "abstract": "Latest development of neural models has connected the encoder and decoder\nthrough a self-attention mechanism. In particular, Transformer, which is solely\nbased on self-attention, has led to breakthroughs in Natural Language\nProcessing (NLP) tasks. However, the multi-head attention mechanism, as a key\ncomponent of Transformer, limits the effective deployment of the model to a\nresource-limited setting. In this paper, based on the ideas of tensor\ndecomposition and parameters sharing, we propose a novel self-attention model\n(namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We\ntest and verify the proposed attention method on three language modeling tasks\n(i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task\n(i.e., WMT-2016 English-German). Multi-linear attention can not only largely\ncompress the model parameters but also obtain performance improvements,\ncompared with a number of language modeling approaches, such as Transformer,\nTransformer-XL, and Transformer with tensor train decomposition.", "published": "2019-06-24 08:28:37", "link": "http://arxiv.org/abs/1906.09777v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translationese in Machine Translation Evaluation", "abstract": "The term translationese has been used to describe the presence of unusual\nfeatures of translated text. In this paper, we provide a detailed analysis of\nthe adverse effects of translationese on machine translation evaluation\nresults. Our analysis shows evidence to support differences in text originally\nwritten in a given language relative to translated text and this can\npotentially negatively impact the accuracy of machine translation evaluations.\nFor this reason we recommend that reverse-created test data be omitted from\nfuture machine translation test sets. In addition, we provide a re-evaluation\nof a past high-profile machine translation evaluation claiming human-parity of\nMT, as well as analysis of the since re-evaluations of it. We find potential\nways of improving the reliability of all three past evaluations. One important\nissue not previously considered is the statistical power of significance tests\napplied in past evaluations that aim to investigate human-parity of MT. Since\nthe very aim of such evaluations is to reveal legitimate ties between human and\nMT systems, power analysis is of particular importance, where low power could\nresult in claims of human parity that in fact simply correspond to Type II\nerror. We therefore provide a detailed power analysis of tests used in such\nevaluations to provide an indication of a suitable minimum sample size of\ntranslations for such studies. Subsequently, since no past evaluation that\naimed to investigate claims of human parity ticks all boxes in terms of\naccuracy and reliability, we rerun the evaluation of the systems claiming human\nparity. Finally, we provide a comprehensive check-list for future machine\ntranslation evaluation.", "published": "2019-06-24 10:14:42", "link": "http://arxiv.org/abs/1906.09833v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Latent Trees with Stochastic Perturbations and Differentiable\n  Dynamic Programming", "abstract": "We treat projective dependency trees as latent variables in our probabilistic\nmodel and induce them in such a way as to be beneficial for a downstream task,\nwithout relying on any direct tree supervision. Our approach relies on Gumbel\nperturbations and differentiable dynamic programming. Unlike previous\napproaches to latent tree learning, we stochastically sample global structures\nand our parser is fully differentiable. We illustrate its effectiveness on\nsentiment analysis and natural language inference tasks. We also study its\nproperties on a synthetic structure induction task. Ablation studies emphasize\nthe importance of both stochasticity and constraining latent structures to be\nprojective trees.", "published": "2019-06-24 14:29:12", "link": "http://arxiv.org/abs/1906.09992v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LIAAD at SemDeep-5 Challenge: Word-in-Context (WiC)", "abstract": "This paper describes the LIAAD system that was ranked second place in the\nWord-in-Context challenge (WiC) featured in SemDeep-5. Our solution is based on\na novel system for Word Sense Disambiguation (WSD) using contextual embeddings\nand full-inventory sense embeddings. We adapt this WSD system, in a\nstraightforward manner, for the present task of detecting whether the same\nsense occurs in a pair of sentences. Additionally, we show that our solution is\nable to achieve competitive performance even without using the provided\ntraining or development sets, mitigating potential concerns related to task\noverfitting", "published": "2019-06-24 14:49:05", "link": "http://arxiv.org/abs/1906.10002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is It Worth the Attention? A Comparative Evaluation of Attention Layers\n  for Argument Unit Segmentation", "abstract": "Attention mechanisms have seen some success for natural language processing\ndownstream tasks in recent years and generated new State-of-the-Art results. A\nthorough evaluation of the attention mechanism for the task of Argumentation\nMining is missing, though. With this paper, we report a comparative evaluation\nof attention layers in combination with a bidirectional long short-term memory\nnetwork, which is the current state-of-the-art approach to the unit\nsegmentation task. We also compare sentence-level contextualized word\nembeddings to pre-generated ones. Our findings suggest that for this task the\nadditional attention layer does not improve upon a less complex approach. In\nmost cases, the contextualized embeddings do also not show an improvement on\nthe baseline score.", "published": "2019-06-24 16:40:47", "link": "http://arxiv.org/abs/1906.10068v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mutual exclusivity as a challenge for deep neural networks", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways.\nChildren use the mutual exclusivity (ME) bias to help disambiguate how words\nmap to referents, assuming that if an object has one label then it does not\nneed another. In this paper, we investigate whether or not standard neural\narchitectures have an ME bias, demonstrating that they lack this learning\nassumption. Moreover, we show that their inductive biases are poorly matched to\nlifelong learning formulations of classification and translation. We\ndemonstrate that there is a compelling case for designing neural networks that\nreason by mutual exclusivity, which remains an open challenge.", "published": "2019-06-24 19:47:05", "link": "http://arxiv.org/abs/1906.10197v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compound Probabilistic Context-Free Grammars for Grammar Induction", "abstract": "We study a formalization of the grammar induction problem that models\nsentences as being generated by a compound probabilistic context-free grammar.\nIn contrast to traditional formulations which learn a single stochastic\ngrammar, our grammar's rule probabilities are modulated by a per-sentence\ncontinuous latent variable, which induces marginal dependencies beyond the\ntraditional context-free assumptions. Inference in this grammar is performed by\ncollapsed variational inference, in which an amortized variational posterior is\nplaced on the continuous variable, and the latent trees are marginalized out\nwith dynamic programming. Experiments on English and Chinese show the\neffectiveness of our approach compared to recent state-of-the-art methods when\nevaluated on unsupervised parsing.", "published": "2019-06-24 20:45:50", "link": "http://arxiv.org/abs/1906.10225v9", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "EQuANt (Enhanced Question Answer Network)", "abstract": "Machine Reading Comprehension (MRC) is an important topic in the domain of\nautomated question answering and in natural language processing more generally.\nSince the release of the SQuAD 1.1 and SQuAD 2 datasets, progress in the field\nhas been particularly significant, with current state-of-the-art models now\nexhibiting near-human performance at both answering well-posed questions and\ndetecting questions which are unanswerable given a corresponding context. In\nthis work, we present Enhanced Question Answer Network (EQuANt), an MRC model\nwhich extends the successful QANet architecture of Yu et al. to cope with\nunanswerable questions. By training and evaluating EQuANt on SQuAD 2, we show\nthat it is indeed possible to extend QANet to the unanswerable domain. We\nachieve results which are close to 2 times better than our chosen baseline\nobtained by evaluating a lightweight version of the original QANet architecture\non SQuAD 2. In addition, we report that the performance of EQuANt on SQuAD 1.1\nafter being trained on SQuAD2 exceeds that of our lightweight QANet\narchitecture trained and evaluated on SQuAD 1.1, demonstrating the utility of\nmulti-task learning in the MRC context.", "published": "2019-06-24 08:13:45", "link": "http://arxiv.org/abs/1907.00708v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CORAL8: Concurrent Object Regression for Area Localization in Medical\n  Image Panels", "abstract": "This work tackles the problem of generating a medical report for multi-image\npanels. We apply our solution to the Renal Direct Immunofluorescence (RDIF)\nassay which requires a pathologist to generate a report based on observations\nacross the eight different WSI in concert with existing clinical features. To\nthis end, we propose a novel attention-based multi-modal generative recurrent\nneural network (RNN) architecture capable of dynamically sampling image data\nconcurrently across the RDIF panel. The proposed methodology incorporates text\nfrom the clinical notes of the requesting physician to regulate the output of\nthe network to align with the overall clinical context. In addition, we found\nthe importance of regularizing the attention weights for word generation\nprocesses. This is because the system can ignore the attention mechanism by\nassigning equal weights for all members. Thus, we propose two regularizations\nwhich force the system to utilize the attention mechanism. Experiments on our\nnovel collection of RDIF WSIs provided by a large clinical laboratory\ndemonstrate that our framework offers significant improvements over existing\nmethods.", "published": "2019-06-24 00:30:32", "link": "http://arxiv.org/abs/1906.09676v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SylNet: An Adaptable End-to-End Syllable Count Estimator for Speech", "abstract": "Automatic syllable count estimation (SCE) is used in a variety of\napplications ranging from speaking rate estimation to detecting social activity\nfrom wearable microphones or developmental research concerned with quantifying\nspeech heard by language-learning children in different environments. The\nmajority of previously utilized SCE methods have relied on heuristic DSP\nmethods, and only a small number of bi-directional long short-term memory\n(BLSTM) approaches have made use of modern machine learning approaches in the\nSCE task. This paper presents a novel end-to-end method called SylNet for\nautomatic syllable counting from speech, built on the basis of a recent\ndevelopments in neural network architectures. We describe how the entire model\ncan be optimized directly to minimize SCE error on the training data without\nannotations aligned at the syllable level, and how it can be adapted to new\nlanguages using limited speech data with known syllable counts. Experiments on\nseveral different languages reveal that SylNet generalizes to languages beyond\nits training data and further improves with adaptation. It also outperforms\nseveral previously proposed methods for syllabification, including end-to-end\nBLSTMs.", "published": "2019-06-24 10:05:23", "link": "http://arxiv.org/abs/1906.09825v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A computational model of early language acquisition from audiovisual\n  experiences of young infants", "abstract": "Earlier research has suggested that human infants might use statistical\ndependencies between speech and non-linguistic multimodal input to bootstrap\ntheir language learning before they know how to segment words from running\nspeech. However, feasibility of this hypothesis in terms of real-world infant\nexperiences has remained unclear. This paper presents a step towards a more\nrealistic test of the multimodal bootstrapping hypothesis by describing a\nneural network model that can learn word segments and their meanings from\nreferentially ambiguous acoustic input. The model is tested on recordings of\nreal infant-caregiver interactions using utterance-level labels for concrete\nvisual objects that were attended by the infant when caregiver spoke an\nutterance containing the name of the object, and using random visual labels for\nutterances during absence of attention. The results show that beginnings of\nlexical knowledge may indeed emerge from individually ambiguous learning\nscenarios. In addition, the hidden layers of the network show gradually\nincreasing selectivity to phonetic categories as a function of layer depth,\nresembling models trained for phone recognition in a supervised manner.", "published": "2019-06-24 10:14:24", "link": "http://arxiv.org/abs/1906.09832v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "RUBi: Reducing Unimodal Biases in Visual Question Answering", "abstract": "Visual Question Answering (VQA) is the task of answering questions about an\nimage. Some VQA models often exploit unimodal biases to provide the correct\nanswer without using the image information. As a result, they suffer from a\nhuge drop in performance when evaluated on data outside their training set\ndistribution. This critical issue makes them unsuitable for real-world\nsettings.\n  We propose RUBi, a new learning strategy to reduce biases in any VQA model.\nIt reduces the importance of the most biased examples, i.e. examples that can\nbe correctly classified without looking at the image. It implicitly forces the\nVQA model to use the two input modalities instead of relying on statistical\nregularities between the question and the answer. We leverage a question-only\nmodel that captures the language biases by identifying when these unwanted\nregularities are used. It prevents the base VQA model from learning them by\ninfluencing its predictions. This leads to dynamically adjusting the loss in\norder to compensate for biases. We validate our contributions by surpassing the\ncurrent state-of-the-art results on VQA-CP v2. This dataset is specifically\ndesigned to assess the robustness of VQA models when exposed to different\nquestion biases at test time than what was seen during training.\n  Our code is available: github.com/cdancette/rubi.bootstrap.pytorch", "published": "2019-06-24 18:55:24", "link": "http://arxiv.org/abs/1906.10169v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multimodal and Multi-view Models for Emotion Recognition", "abstract": "Studies on emotion recognition (ER) show that combining lexical and acoustic\ninformation results in more robust and accurate models. The majority of the\nstudies focus on settings where both modalities are available in training and\nevaluation. However, in practice, this is not always the case; getting ASR\noutput may represent a bottleneck in a deployment pipeline due to computational\ncomplexity or privacy-related constraints. To address this challenge, we study\nthe problem of efficiently combining acoustic and lexical modalities during\ntraining while still providing a deployable acoustic model that does not\nrequire lexical inputs. We first experiment with multimodal models and two\nattention mechanisms to assess the extent of the benefits that lexical\ninformation can provide. Then, we frame the task as a multi-view learning\nproblem to induce semantic information from a multimodal model into our\nacoustic-only network using a contrastive loss function. Our multimodal model\noutperforms the previous state of the art on the USC-IEMOCAP dataset reported\non lexical and acoustic information. Additionally, our multi-view-trained\nacoustic network significantly surpasses models that have been exclusively\ntrained with acoustic features.", "published": "2019-06-24 19:47:23", "link": "http://arxiv.org/abs/1906.10198v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Assessing the Applicability of Authorship Verification Methods", "abstract": "Authorship verification (AV) is a research subject in the field of digital\ntext forensics that concerns itself with the question, whether two documents\nhave been written by the same person. During the past two decades, an\nincreasing number of proposed AV approaches can be observed. However, a closer\nlook at the respective studies reveals that the underlying characteristics of\nthese methods are rarely addressed, which raises doubts regarding their\napplicability in real forensic settings. The objective of this paper is to fill\nthis gap by proposing clear criteria and properties that aim to improve the\ncharacterization of existing and future AV approaches. Based on these\nproperties, we conduct three experiments using 12 existing AV approaches,\nincluding the current state of the art. The examined methods were trained,\noptimized and evaluated on three self-compiled corpora, where each corpus\nfocuses on a different aspect of applicability. Our results indicate that part\nof the methods are able to cope with very challenging verification cases such\nas 250 characters long informal chat conversations (72.7% accuracy) or cases in\nwhich two scientific documents were written at different times with an average\ndifference of 15.6 years (> 75% accuracy). However, we also identified that all\ninvolved methods are prone to cross-topic verification cases.", "published": "2019-06-24 09:44:46", "link": "http://arxiv.org/abs/1906.10551v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ultrasound-based Silent Speech Interface Built on a Continuous Vocoder", "abstract": "Recently it was shown that within the Silent Speech Interface (SSI) field,\nthe prediction of F0 is possible from Ultrasound Tongue Images (UTI) as the\narticulatory input, using Deep Neural Networks for articulatory-to-acoustic\nmapping. Moreover, text-to-speech synthesizers were shown to produce higher\nquality speech when using a continuous pitch estimate, which takes non-zero\npitch values even when voicing is not present. Therefore, in this paper on\nUTI-based SSI, we use a simple continuous F0 tracker which does not apply a\nstrict voiced / unvoiced decision. Continuous vocoder parameters (ContF0,\nMaximum Voiced Frequency and Mel-Generalized Cepstrum) are predicted using a\nconvolutional neural network, with UTI as input. The results demonstrate that\nduring the articulatory-to-acoustic mapping experiments, the continuous F0 is\npredicted with lower error, and the continuous vocoder produces slightly more\nnatural synthesized speech than the baseline vocoder using standard\ndiscontinuous F0.", "published": "2019-06-24 12:34:29", "link": "http://arxiv.org/abs/1906.09885v1", "categories": ["cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Single-Channel Speech Separation with Auxiliary Speaker Embeddings", "abstract": "We present a novel source separation model to decompose asingle-channel\nspeech signal into two speech segments belonging to two different speakers. The\nproposed model is a neural network based on residual blocks, and uses learnt\nspeaker embeddings created from additional clean context recordings of the two\nspeakers as input to assist in attributing the different time-frequency bins to\nthe two speakers. In experiments, we show that the proposed model yields good\nperformance in the source separation task, and outperforms the state-of-the-art\nbaselines. Specifically, separating speech from the challenging VoxCeleb\ndataset, the proposed model yields 4.79dB signal-to-distortion ratio, 8.44dB\nsignal-to-artifacts ratio and 7.11dB signal-to-interference ratio.", "published": "2019-06-24 14:35:29", "link": "http://arxiv.org/abs/1906.09997v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Who said that?: Audio-visual speaker diarisation of real-world meetings", "abstract": "The goal of this work is to determine 'who spoke when' in real-world\nmeetings. The method takes surround-view video and single or multi-channel\naudio as inputs, and generates robust diarisation outputs. To achieve this, we\npropose a novel iterative approach that first enrolls speaker models using\naudio-visual correspondence, then uses the enrolled models together with the\nvisual information to determine the active speaker. We show strong quantitative\nand qualitative performance on a dataset of real-world meetings. The method is\nalso evaluated on the public AMI meeting corpus, on which we demonstrate\nresults that exceed all comparable methods. We also show that beamforming can\nbe used together with the video to further improve the performance when\nmulti-channel audio is available.", "published": "2019-06-24 16:06:13", "link": "http://arxiv.org/abs/1906.10042v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Transfer Learning for Cry-based Diagnosis of Perinatal Asphyxia", "abstract": "Despite continuing medical advances, the rate of newborn morbidity and\nmortality globally remains high, with over 6 million casualties every year. The\nprediction of pathologies affecting newborns based on their cry is thus of\nsignificant clinical interest, as it would facilitate the development of\naccessible, low-cost diagnostic tools\\cut{ based on wearables and smartphones}.\nHowever, the inadequacy of clinically annotated datasets of infant cries limits\nprogress on this task. This study explores a neural transfer learning approach\nto developing accurate and robust models for identifying infants that have\nsuffered from perinatal asphyxia. In particular, we explore the hypothesis that\nrepresentations learned from adult speech could inform and improve performance\nof models developed on infant speech. Our experiments show that models based on\nsuch representation transfer are resilient to different types and degrees of\nnoise, as well as to signal loss in time and frequency domains.", "published": "2019-06-24 19:47:37", "link": "http://arxiv.org/abs/1906.10199v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Convolutional Approach to Melody Line Identification in Symbolic\n  Scores", "abstract": "In many musical traditions, the melody line is of primary significance in a\npiece. Human listeners can readily distinguish melodies from accompaniment;\nhowever, making this distinction given only the written score -- i.e. without\nlistening to the music performed -- can be a difficult task. Solving this task\nis of great importance for both Music Information Retrieval and musicological\napplications. In this paper, we propose an automated approach to identifying\nthe most salient melody line in a symbolic score. The backbone of the method\nconsists of a convolutional neural network (CNN) estimating the probability\nthat each note in the score (more precisely: each pixel in a piano roll\nencoding of the score) belongs to the melody line. We train and evaluate the\nmethod on various datasets, using manual annotations where available and solo\ninstrument parts where not. We also propose a method to inspect the CNN and to\nanalyze the influence exerted by notes on the prediction of other notes; this\nmethod can be applied whenever the output of a neural network has the same size\nas the input.", "published": "2019-06-24 13:07:08", "link": "http://arxiv.org/abs/1906.10547v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
