{"title": "A Deep Neural Framework for Contextual Affect Detection", "abstract": "A short and simple text carrying no emotion can represent some strong\nemotions when reading along with its context, i.e., the same sentence can\nexpress extreme anger as well as happiness depending on its context. In this\npaper, we propose a Contextual Affect Detection (CAD) framework which learns\nthe inter-dependence of words in a sentence, and at the same time the\ninter-dependence of sentences in a dialogue. Our proposed CAD framework is\nbased on a Gated Recurrent Unit (GRU), which is further assisted by contextual\nword embeddings and other diverse hand-crafted feature sets. Evaluation and\nanalysis suggest that our model outperforms the state-of-the-art methods by\n5.49% and 9.14% on Friends and EmotionPush dataset, respectively.", "published": "2020-01-28 05:03:15", "link": "http://arxiv.org/abs/2001.10169v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Extraction of Templates from Phrases Using Sequence Binary Decision\n  Diagrams", "abstract": "The extraction of templates such as ``regard X as Y'' from a set of related\nphrases requires the identification of their internal structures. This paper\npresents an unsupervised approach for extracting templates on-the-fly from only\ntagged text by using a novel relaxed variant of the Sequence Binary Decision\nDiagram (SeqBDD). A SeqBDD can compress a set of sequences into a graphical\nstructure equivalent to a minimal DFA, but more compact and better suited to\nthe task of template extraction. The main contribution of this paper is a\nrelaxed form of the SeqBDD construction algorithm that enables it to form\ngeneral representations from a small amount of data. The process of compression\nof shared structures in the text during Relaxed SeqBDD construction, naturally\ninduces the templates we wish to extract. Experiments show that the method is\ncapable of high-quality extraction on tasks based on verb+preposition templates\nfrom corpora and phrasal templates from short messages from social media.", "published": "2020-01-28 05:30:53", "link": "http://arxiv.org/abs/2001.10175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Sentiment Analysis using Super Characters Method on\n  Low-power CNN Accelerator Device", "abstract": "Recent years NLP research has witnessed the record-breaking accuracy\nimprovement by DNN models. However, power consumption is one of the practical\nconcerns for deploying NLP systems. Most of the current state-of-the-art\nalgorithms are implemented on GPUs, which is not power-efficient and the\ndeployment cost is also very high. On the other hand, CNN Domain Specific\nAccelerator (CNN-DSA) has been in mass production providing low-power and low\ncost computation power. In this paper, we will implement the Super Characters\nmethod on the CNN-DSA. In addition, we modify the Super Characters method to\nutilize the multi-modal data, i.e. text plus tabular data in the CL-Aff\nsharedtask.", "published": "2020-01-28 05:45:03", "link": "http://arxiv.org/abs/2001.10179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing Stories Alive: Generating Interactive Fiction Worlds", "abstract": "World building forms the foundation of any task that requires narrative\nintelligence. In this work, we focus on procedurally generating interactive\nfiction worlds---text-based worlds that players \"see\" and \"talk to\" using\nnatural language. Generating these worlds requires referencing everyday and\nthematic commonsense priors in addition to being semantically consistent,\ninteresting, and coherent throughout. Using existing story plots as\ninspiration, we present a method that first extracts a partial knowledge graph\nencoding basic information regarding world structure such as locations and\nobjects. This knowledge graph is then automatically completed utilizing\nthematic knowledge and used to guide a neural language generation model that\nfleshes out the rest of the world. We perform human participant-based\nevaluations, testing our neural model's ability to extract and fill-in a\nknowledge graph and to generate language conditioned on it against rule-based\nand human-made baselines. Our code is available at\nhttps://github.com/rajammanabrolu/WorldGeneration.", "published": "2020-01-28 04:13:05", "link": "http://arxiv.org/abs/2001.10161v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Incorporating Joint Embeddings into Goal-Oriented Dialogues with\n  Multi-Task Learning", "abstract": "Attention-based encoder-decoder neural network models have recently shown\npromising results in goal-oriented dialogue systems. However, these models\nstruggle to reason over and incorporate state-full knowledge while preserving\ntheir end-to-end text generation functionality. Since such models can greatly\nbenefit from user intent and knowledge graph integration, in this paper we\npropose an RNN-based end-to-end encoder-decoder architecture which is trained\nwith joint embeddings of the knowledge graph and the corpus as input. The model\nprovides an additional integration of user intent along with text generation,\ntrained with a multi-task learning paradigm along with an additional\nregularization technique to penalize generating the wrong entity as output. The\nmodel further incorporates a Knowledge Graph entity lookup during inference to\nguarantee the generated output is state-full based on the local knowledge graph\nprovided. We finally evaluated the model using the BLEU score, empirical\nevaluation depicts that our proposed architecture can aid in the betterment of\ntask-oriented dialogue system`s performance.", "published": "2020-01-28 17:15:02", "link": "http://arxiv.org/abs/2001.10468v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multilingual Alignment using Wasserstein Barycenter", "abstract": "We study unsupervised multilingual alignment, the problem of finding\nword-to-word translations between multiple languages without using any parallel\ndata. One popular strategy is to reduce multilingual alignment to the much\nsimplified bilingual setting, by picking one of the input languages as the\npivot language that we transit through. However, it is well-known that\ntransiting through a poorly chosen pivot language (such as English) may\nseverely degrade the translation quality, since the assumed transitive\nrelations among all pairs of languages may not be enforced in the training\nprocess. Instead of going through a rather arbitrarily chosen pivot language,\nwe propose to use the Wasserstein barycenter as a more informative \"mean\"\nlanguage: it encapsulates information from all languages and minimizes all\npairwise transportation costs. We evaluate our method on standard benchmarks\nand demonstrate state-of-the-art performances.", "published": "2020-01-28 19:22:07", "link": "http://arxiv.org/abs/2002.00743v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PEL-BERT: A Joint Model for Protocol Entity Linking", "abstract": "Pre-trained models such as BERT are widely used in NLP tasks and are\nfine-tuned to improve the performance of various NLP tasks consistently.\nNevertheless, the fine-tuned BERT model trained on our protocol corpus still\nhas a weak performance on the Entity Linking (EL) task. In this paper, we\npropose a model that joints a fine-tuned language model with an RFC Domain\nModel. Firstly, we design a Protocol Knowledge Base as the guideline for\nprotocol EL. Secondly, we propose a novel model, PEL-BERT, to link named\nentities in protocols to categories in Protocol Knowledge Base. Finally, we\nconduct a comprehensive study on the performance of pre-trained language models\non descriptive texts and abstract concepts. Experimental results demonstrate\nthat our model achieves state-of-the-art performance in EL on our annotated\ndataset, outperforming all the baselines.", "published": "2020-01-28 16:42:40", "link": "http://arxiv.org/abs/2002.00744v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structural-Aware Sentence Similarity with Recursive Optimal Transport", "abstract": "Measuring sentence similarity is a classic topic in natural language\nprocessing. Light-weighted similarities are still of particular practical\nsignificance even when deep learning models have succeeded in many other tasks.\nSome light-weighted similarities with more theoretical insights have been\ndemonstrated to be even stronger than supervised deep learning approaches.\nHowever, the successful light-weighted models such as Word Mover's Distance\n[Kusner et al., 2015] or Smooth Inverse Frequency [Arora et al., 2017] failed\nto detect the difference from the structure of sentences, i.e. order of words.\nTo address this issue, we present Recursive Optimal Transport (ROT) framework\nto incorporate the structural information with the classic OT. Moreover, we\nfurther develop Recursive Optimal Similarity (ROTS) for sentences with the\nvaluable semantic insights from the connections between cosine similarity of\nweighted average of word vectors and optimal transport. ROTS is\nstructural-aware and with low time complexity compared to optimal transport.\nOur experiments over 20 sentence textural similarity (STS) datasets show the\nclear advantage of ROTS over all weakly supervised approaches. Detailed\nablation study demonstrate the effectiveness of ROT and the semantic insights.", "published": "2020-01-28 09:07:47", "link": "http://arxiv.org/abs/2002.00745v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Joint Contextual Modeling for ASR Correction and Language Understanding", "abstract": "The quality of automatic speech recognition (ASR) is critical to Dialogue\nSystems as ASR errors propagate to and directly impact downstream tasks such as\nlanguage understanding (LU). In this paper, we propose multi-task neural\napproaches to perform contextual language correction on ASR outputs jointly\nwith LU to improve the performance of both tasks simultaneously. To measure the\neffectiveness of this approach we used a public benchmark, the 2nd Dialogue\nState Tracking (DSTC2) corpus. As a baseline approach, we trained task-specific\nStatistical Language Models (SLM) and fine-tuned state-of-the-art Generalized\nPre-training (GPT) Language Model to re-rank the n-best ASR hypotheses,\nfollowed by a model to identify the dialog act and slots. i) We further trained\nranker models using GPT and Hierarchical CNN-RNN models with discriminatory\nlosses to detect the best output given n-best hypotheses. We extended these\nranker models to first select the best ASR output and then identify the\ndialogue act and slots in an end to end fashion. ii) We also proposed a novel\njoint ASR error correction and LU model, a word confusion pointer network\n(WCN-Ptr) with multi-head self-attention on top, which consumes the word\nconfusions populated from the n-best. We show that the error rates of off the\nshelf ASR and following LU systems can be reduced significantly by 14% relative\nwith joint models trained using small amounts of in-domain data.", "published": "2020-01-28 22:09:25", "link": "http://arxiv.org/abs/2002.00750v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Generalizability of Fake News Detection Methods using\n  Propensity Score Matching", "abstract": "Recently, due to the booming influence of online social networks, detecting\nfake news is drawing significant attention from both academic communities and\ngeneral public. In this paper, we consider the existence of confounding\nvariables in the features of fake news and use Propensity Score Matching (PSM)\nto select generalizable features in order to reduce the effects of the\nconfounding variables. Experimental results show that the generalizability of\nfake news method is significantly better by using PSM than using raw frequency\nto select features. We investigate multiple types of fake news methods\n(classifiers) such as logistic regression, random forests, and support vector\nmachines. We have consistent observations of performance improvement.", "published": "2020-01-28 00:44:59", "link": "http://arxiv.org/abs/2002.00838v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Weighted Speech Distortion Losses for Neural-network-based Real-time\n  Speech Enhancement", "abstract": "This paper investigates several aspects of training a RNN (recurrent neural\nnetwork) that impact the objective and subjective quality of enhanced speech\nfor real-time single-channel speech enhancement. Specifically, we focus on a\nRNN that enhances short-time speech spectra on a single-frame-in,\nsingle-frame-out basis, a framework adopted by most classical signal processing\nmethods. We propose two novel mean-squared-error-based learning objectives that\nenable separate control over the importance of speech distortion versus noise\nreduction. The proposed loss functions are evaluated by widely accepted\nobjective quality and intelligibility measures and compared to other\ncompetitive online methods. In addition, we study the impact of feature\nnormalization and varying batch sequence lengths on the objective quality of\nenhanced speech. Finally, we show subjective ratings for the proposed approach\nand a state-of-the-art real-time RNN-based method.", "published": "2020-01-28 21:46:16", "link": "http://arxiv.org/abs/2001.10601v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Pre-training of Bidirectional Speech Encoders via Masked\n  Reconstruction", "abstract": "We propose an approach for pre-training speech representations via a masked\nreconstruction loss. Our pre-trained encoder networks are bidirectional and can\ntherefore be used directly in typical bidirectional speech recognition models.\nThe pre-trained networks can then be fine-tuned on a smaller amount of\nsupervised data for speech recognition. Experiments with this approach on the\nLibriSpeech and Wall Street Journal corpora show promising results. We find\nthat the main factors that lead to speech recognition improvements are: masking\nsegments of sufficient width in both time and frequency, pre-training on a much\nlarger amount of unlabeled data than the labeled data, and domain adaptation\nwhen the unlabeled and labeled data come from different domains. The gain from\npre-training is additive to that of supervised data augmentation.", "published": "2020-01-28 21:49:05", "link": "http://arxiv.org/abs/2001.10603v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time-Domain Audio Source Separation Based on Wave-U-Net Combined with\n  Discrete Wavelet Transform", "abstract": "We propose a time-domain audio source separation method using down-sampling\n(DS) and up-sampling (US) layers based on a discrete wavelet transform (DWT).\nThe proposed method is based on one of the state-of-the-art deep neural\nnetworks, Wave-U-Net, which successively down-samples and up-samples feature\nmaps. We find that this architecture resembles that of multiresolution\nanalysis, and reveal that the DS layers of Wave-U-Net cause aliasing and may\ndiscard information useful for the separation. Although the effects of these\nproblems may be reduced by training, to achieve a more reliable source\nseparation method, we should design DS layers capable of overcoming the\nproblems. With this belief, focusing on the fact that the DWT has an\nanti-aliasing filter and the perfect reconstruction property, we design the\nproposed layers. Experiments on music source separation show the efficacy of\nthe proposed method and the importance of simultaneously considering the\nanti-aliasing filters and the perfect reconstruction property.", "published": "2020-01-28 06:43:21", "link": "http://arxiv.org/abs/2001.10190v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CLCNet: Deep learning-based Noise Reduction for Hearing Aids using\n  Complex Linear Coding", "abstract": "Noise reduction is an important part of modern hearing aids and is included\nin most commercially available devices. Deep learning-based state-of-the-art\nalgorithms, however, either do not consider real-time and frequency resolution\nconstrains or result in poor quality under very noisy conditions. To improve\nmonaural speech enhancement in noisy environments, we propose CLCNet, a\nframework based on complex valued linear coding. First, we define complex\nlinear coding (CLC) motivated by linear predictive coding (LPC) that is applied\nin the complex frequency domain. Second, we propose a framework that\nincorporates complex spectrogram input and coefficient output. Third, we define\na parametric normalization for complex valued spectrograms that complies with\nlow-latency and on-line processing. Our CLCNet was evaluated on a mixture of\nthe EUROM database and a real-world noise dataset recorded with hearing aids\nand compared to traditional real-valued Wiener-Filter gains.", "published": "2020-01-28 09:08:35", "link": "http://arxiv.org/abs/2001.10218v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Learning Absolute Sound Source Localisation With Limited Supervisions", "abstract": "An accurate auditory space map can be learned from auditory experience, for\nexample during development or in response to altered auditory cues such as a\nmodified pinna. We studied neural network models that learn to localise a\nsingle sound source in the horizontal plane using binaural cues based on\nlimited supervisions. These supervisions can be unreliable or sparse in real\nlife. First, a simple model that has unreliable estimation of the sound source\nlocation is built, in order to simulate the unreliable auditory orienting\nresponse of newborns. It is used as a Teacher that acts as a source of\nunreliable supervisions. Then we show that it is possible to learn a continuous\nauditory space map based only on noisy left or right feedbacks from the\nTeacher. Furthermore, reinforcement rewards from the environment are used as a\nsource of sparse supervision. By combining the unreliable innate response and\nthe sparse reinforcement rewards, an accurate auditory space map, which is hard\nto be achieved by either one of these two kind of supervisions, can eventually\nbe learned. Our results show that the auditory space mapping can be calibrated\neven without explicit supervision. Moreover, this study implies a possibly more\ngeneral neural mechanism where multiple sub-modules can be coordinated to\nfacilitate each other's learning process under limited supervisions.", "published": "2020-01-28 21:59:01", "link": "http://arxiv.org/abs/2001.10605v1", "categories": ["cs.NE", "eess.AS", "q-bio.NC"], "primary_category": "cs.NE"}
{"title": "MCSAE: Masked Cross Self-Attentive Encoding for Speaker Embedding", "abstract": "In general, a self-attention mechanism has been applied for speaker embedding\nencoding. Previous studies focused on training the self-attention in a\nhigh-level layer, such as the last pooling layer. However, the effect of\nlow-level features was reduced in the speaker embedding encoding. Therefore, we\npropose masked cross self-attentive encoding (MCSAE) using ResNet. It focuses\non the features of both high-level and lowlevel layers. Based on multi-layer\naggregation, the output features of each residual layer are used for the MCSAE.\nIn the MCSAE, cross self-attention module is trained the interdependence of\neach input features. A random masking regularization module also applied to\npreventing overfitting problem. As such, the MCSAE enhances the weight of\nframes representing the speaker information. Then, the output features are\nconcatenated and encoded to the speaker embedding. Therefore, a more\ninformative speaker embedding is encoded by using the MCSAE. The experimental\nresults showed an equal error rate of 2.63% and a minimum detection cost\nfunction of 0.1453 using the VoxCeleb1 evaluation dataset. These were improved\nperformances compared with the previous self-attentive encoding and\nstate-of-the-art encoding methods.", "published": "2020-01-28 04:09:11", "link": "http://arxiv.org/abs/2001.10817v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
