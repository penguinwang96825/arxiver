{"title": "Concentric network symmetry grasps authors' styles in word adjacency\n  networks", "abstract": "Several characteristics of written texts have been inferred from statistical\nanalysis derived from networked models. Even though many network measurements\nhave been adapted to study textual properties at several levels of complexity,\nsome textual aspects have been disregarded. In this paper, we study the\nsymmetry of word adjacency networks, a well-known representation of text as a\ngraph. A statistical analysis of the symmetry distribution performed in several\nnovels showed that most of the words do not display symmetric patterns of\nconnectivity. More specifically, the merged symmetry displayed a distribution\nsimilar to the ubiquitous power-law distribution. Our experiments also revealed\nthat the studied metrics do not correlate with other traditional network\nmeasurements, such as the degree or betweenness centrality. The effectiveness\nof the symmetry measurements was verified in the authorship attribution task.\nInterestingly, we found that specific authors prefer particular types of\nsymmetric motifs. As a consequence, the authorship of books could be accurately\nidentified in 82.5% of the cases, in a dataset comprising books written by 8\nauthors. Because the proposed measurements for text analysis are complementary\nto the traditional approach, they can be used to improve the characterization\nof text networks, which might be useful for related applications, such as those\nrelying on the identification of topical words and information retrieval.", "published": "2015-04-09 00:49:36", "link": "http://arxiv.org/abs/1504.02162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Twitter for Low-Resource Conversational Speech Language\n  Modeling", "abstract": "In applications involving conversational speech, data sparsity is a limiting\nfactor in building a better language model. We propose a simple,\nlanguage-independent method to quickly harvest large amounts of data from\nTwitter to supplement a smaller training set that is more closely matched to\nthe domain. The techniques lead to a significant reduction in perplexity on\nfour low-resource languages even though the presence on Twitter of these\nlanguages is relatively small. We also find that the Twitter text is more\nuseful for learning word classes than the in-domain text and that use of these\nword classes leads to further reductions in perplexity. Additionally, we\nintroduce a method of using social and textual information to prioritize the\ndownload queue during the Twitter crawling. This maximizes the amount of useful\ndata that can be collected, impacting both perplexity and vocabulary coverage.", "published": "2015-04-09 20:21:32", "link": "http://arxiv.org/abs/1504.02490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
