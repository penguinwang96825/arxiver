{"title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via\n  Parameter-Efficient Module Operation", "abstract": "Large language models (LLMs) have been widely used in various applications\nbut are known to suffer from issues related to untruthfulness and toxicity.\nWhile parameter-efficient modules (PEMs) have demonstrated their effectiveness\nin equipping models with new skills, leveraging PEMs for deficiency unlearning\nremains underexplored. In this work, we propose a PEMs operation approach,\nnamely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and\ndetoxification of LLMs through the integration of ``expert'' PEM and\n``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable\ncapabilities due to their proficiency in generating fabricated content, which\nnecessitates language modeling and logical narrative competence. Rather than\nmerely negating the parameters, our approach involves extracting and\neliminating solely the deficiency capability within anti-expert PEM while\npreserving the general capabilities. To evaluate the effectiveness of our\napproach in terms of truthfulness and detoxification, we conduct extensive\nexperiments on LLMs, encompassing additional abilities such as language\nmodeling and mathematical reasoning. Our empirical results demonstrate that our\napproach effectively improves truthfulness and detoxification, while largely\npreserving the fundamental abilities of LLMs.", "published": "2023-08-16 01:46:01", "link": "http://arxiv.org/abs/2308.08090v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with\n  Reliability Evaluation", "abstract": "Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range\nof real-life applications. These dialogue systems are promising for providing\neasy access and reducing medical costs. Building end-to-end ADD dialogue\nsystems requires dialogue training datasets. However, to the best of our\nknowledge, there is no publicly available ADD dialogue dataset in English\n(although non-English datasets exist). Driven by this, we introduce MDDial, the\nfirst differential diagnosis dialogue dataset in English which can aid to build\nand evaluate end-to-end ADD dialogue systems. Additionally, earlier studies\npresent the accuracy of diagnosis and symptoms either individually or as a\ncombined weighted score. This method overlooks the connection between the\nsymptoms and the diagnosis. We introduce a unified score for the ADD system\nthat takes into account the interplay between symptoms and diagnosis. This\nscore also indicates the system's reliability. To the end, we train two\nmoderate-size of language models on MDDial. Our experiments suggest that while\nthese language models can perform well on many natural language understanding\ntasks, including dialogue tasks in the general domain, they struggle to relate\nrelevant symptoms and disease and thus have poor performance on MDDial. MDDial\nwill be released publicly to aid the study of ADD dialogue research.", "published": "2023-08-16 04:56:55", "link": "http://arxiv.org/abs/2308.08147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Training of NMT Model with Data Sorting", "abstract": "The Transformer model has revolutionized Natural Language Processing tasks\nsuch as Neural Machine Translation, and many efforts have been made to study\nthe Transformer architecture, which increased its efficiency and accuracy. One\npotential area for improvement is to address the computation of empty tokens\nthat the Transformer computes only to discard them later, leading to an\nunnecessary computational burden. To tackle this, we propose an algorithm that\nsorts translation sentence pairs based on their length before batching,\nminimizing the waste of computing power. Since the amount of sorting could\nviolate the independent and identically distributed (i.i.d) data assumption, we\nsort the data partially. In experiments, we apply the proposed method to\nEnglish-Korean and English-Luganda language pairs for machine translation and\nshow that there are gains in computational time while maintaining the\nperformance. Our method is independent of architectures, so that it can be\neasily integrated into any training process with flexible data lengths.", "published": "2023-08-16 05:48:50", "link": "http://arxiv.org/abs/2308.08153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese\n  Spelling Check", "abstract": "Chinese Spelling Check (CSC) refers to the detection and correction of\nspelling errors in Chinese texts. In practical application scenarios, it is\nimportant to make CSC models have the ability to correct errors across\ndifferent domains. In this paper, we propose a retrieval-augmented spelling\ncheck framework called RSpell, which searches corresponding domain terms and\nincorporates them into CSC models. Specifically, we employ pinyin fuzzy\nmatching to search for terms, which are combined with the input and fed into\nthe CSC model. Then, we introduce an adaptive process control mechanism to\ndynamically adjust the impact of external knowledge on the model. Additionally,\nwe develop an iterative strategy for the RSpell framework to enhance reasoning\ncapabilities. We conducted experiments on CSC datasets in three domains: law,\nmedicine, and official document writing. The results demonstrate that RSpell\nachieves state-of-the-art performance in both zero-shot and fine-tuning\nscenarios, demonstrating the effectiveness of the retrieval-augmented CSC\nframework. Our code is available at https://github.com/47777777/Rspell.", "published": "2023-08-16 07:12:23", "link": "http://arxiv.org/abs/2308.08176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoCoSA: Momentum Contrast for Knowledge Graph Completion with\n  Structure-Augmented Pre-trained Language Models", "abstract": "Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts\nwithin knowledge graphs and automatically infer missing links. Existing methods\ncan mainly be categorized into structure-based or description-based. On the one\nhand, structure-based methods effectively represent relational facts in\nknowledge graphs using entity embeddings. However, they struggle with\nsemantically rich real-world entities due to limited structural information and\nfail to generalize to unseen entities. On the other hand, description-based\nmethods leverage pre-trained language models (PLMs) to understand textual\ninformation. They exhibit strong robustness towards unseen entities. However,\nthey have difficulty with larger negative sampling and often lag behind\nstructure-based methods. To address these issues, in this paper, we propose\nMomentum Contrast for knowledge graph completion with Structure-Augmented\npre-trained language models (MoCoSA), which allows the PLM to perceive the\nstructural information by the adaptable structure encoder. To improve learning\nefficiency, we proposed momentum hard negative and intra-relation negative\nsampling. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance in terms of mean reciprocal rank (MRR), with\nimprovements of 2.5% on WN18RR and 21% on OpenBG500.", "published": "2023-08-16 08:09:10", "link": "http://arxiv.org/abs/2308.08204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain\n  Conversation", "abstract": "We propose MemoChat, a pipeline for refining instructions that enables large\nlanguage models (LLMs) to effectively employ self-composed memos for\nmaintaining consistent long-range open-domain conversations. We demonstrate a\nlong-range open-domain conversation through iterative\n\"memorization-retrieval-response\" cycles. This requires us to carefully design\ntailored tuning instructions for each distinct stage. The instructions are\nreconstructed from a collection of public datasets to teach the LLMs to\nmemorize and retrieve past dialogues with structured memos, leading to enhanced\nconsistency when participating in future conversations. We invite experts to\nmanually annotate a test set designed to evaluate the consistency of long-range\nconversations questions. Experiments on three testing scenarios involving both\nopen-source and API-accessible chatbots at scale verify the efficacy of\nMemoChat, which outperforms strong baselines. Our codes, data and models are\navailable here: https://github.com/LuJunru/MemoChat.", "published": "2023-08-16 09:15:18", "link": "http://arxiv.org/abs/2308.08239v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Neural Network Generalization for Grammar Induction", "abstract": "How well do neural networks generalize? Even for grammar induction tasks,\nwhere the target generalization is fully known, previous works have left the\nquestion open, testing very limited ranges beyond the training set and using\ndifferent success criteria. We provide a measure of neural network\ngeneralization based on fully specified formal languages. Given a model and a\nformal grammar, the method assigns a generalization score representing how well\na model generalizes to unseen samples in inverse relation to the amount of data\nit was trained on. The benchmark includes languages such as $a^nb^n$,\n$a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected\narchitectures using the benchmark and find that networks trained with a Minimum\nDescription Length objective (MDL) generalize better and using less data than\nnetworks trained using standard loss functions. The benchmark is available at\nhttps://github.com/taucompling/bliss.", "published": "2023-08-16 09:45:06", "link": "http://arxiv.org/abs/2308.08253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMD: a framework for Context-aware Model self-Detoxification", "abstract": "Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines.", "published": "2023-08-16 11:50:38", "link": "http://arxiv.org/abs/2308.08295v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummHelper: Collaborative Human-Computer Summarization", "abstract": "Current approaches for text summarization are predominantly automatic, with\nrather limited space for human intervention and control over the process. In\nthis paper, we introduce SummHelper, a 2-phase summarization assistant designed\nto foster human-machine collaboration. The initial phase involves content\nselection, where the system recommends potential content, allowing users to\naccept, modify, or introduce additional selections. The subsequent phase,\ncontent consolidation, involves SummHelper generating a coherent summary from\nthese selections, which users can then refine using visual mappings between the\nsummary and the source text. Small-scale user studies reveal the effectiveness\nof our application, with participants being especially appreciative of the\nbalance between automated guidance and opportunities for personal input.", "published": "2023-08-16 13:39:06", "link": "http://arxiv.org/abs/2308.08363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIOptimus: Pre-training an Optimal Biomedical Language Model with\n  Curriculum Learning for Named Entity Recognition", "abstract": "Using language models (LMs) pre-trained in a self-supervised setting on large\ncorpora and then fine-tuning for a downstream task has helped to deal with the\nproblem of limited label data for supervised learning tasks such as Named\nEntity Recognition (NER). Recent research in biomedical language processing has\noffered a number of biomedical LMs pre-trained using different methods and\ntechniques that advance results on many BioNLP tasks, including NER. However,\nthere is still a lack of a comprehensive comparison of pre-training approaches\nthat would work more optimally in the biomedical domain. This paper aims to\ninvestigate different pre-training methods, such as pre-training the biomedical\nLM from scratch and pre-training it in a continued fashion. We compare existing\nmethods with our proposed pre-training method of initializing weights for new\ntokens by distilling existing weights from the BERT model inside the context\nwhere the tokens were found. The method helps to speed up the pre-training\nstage and improve performance on NER. In addition, we compare how masking rate,\ncorruption strategy, and masking strategies impact the performance of the\nbiomedical LM. Finally, using the insights from our experiments, we introduce a\nnew biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning\n(CL) and contextualized weight distillation method. Our model sets new states\nof the art on several biomedical Named Entity Recognition (NER) tasks. We\nrelease our code and all pre-trained models", "published": "2023-08-16 18:48:01", "link": "http://arxiv.org/abs/2308.08625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning the meanings of function words from grounded language using a\n  visual question answering model", "abstract": "Interpreting a seemingly-simple function word like \"or\", \"behind\", or \"more\"\ncan require logical, numerical, and relational reasoning. How are such words\nlearned by children? Prior acquisition theories have often relied on positing a\nfoundation of innate knowledge. Yet recent neural-network based visual question\nanswering models apparently can learn to use function words as part of\nanswering questions about complex visual scenes. In this paper, we study what\nthese models learn about function words, in the hope of better understanding\nhow the meanings of these words can be learnt by both models and children. We\nshow that recurrent models trained on visually grounded language learn gradient\nsemantics for function words requiring spatial and numerical reasoning.\nFurthermore, we find that these models can learn the meanings of logical\nconnectives and and or without any prior knowledge of logical reasoning, as\nwell as early evidence that they are sensitive to alternative expressions when\ninterpreting language. Finally, we show that word learning difficulty is\ndependent on frequency in models' input. Our findings offer proof-of-concept\nevidence that it is possible to learn the nuanced interpretations of function\nwords in visually grounded context by using non-symbolic general statistical\nlearning algorithms, without any prior knowledge of linguistic meaning.", "published": "2023-08-16 18:53:39", "link": "http://arxiv.org/abs/2308.08628v3", "categories": ["cs.CL", "I.2.7; I.2.6; I.2.10"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Granularized Barrett's Esophagus Diagnosis\n  Classification", "abstract": "Diagnostic codes for Barrett's esophagus (BE), a precursor to esophageal\ncancer, lack granularity and precision for many research or clinical use cases.\nLaborious manual chart review is required to extract key diagnostic phenotypes\nfrom BE pathology reports. We developed a generalizable transformer-based\nmethod to automate data extraction. Using pathology reports from Columbia\nUniversity Irving Medical Center with gastroenterologist-annotated targets, we\nperformed binary dysplasia classification as well as granularized multi-class\nBE-related diagnosis classification. We utilized two clinically pre-trained\nlarge language models, with best model performance comparable to a highly\ntailored rule-based system developed using the same data. Binary dysplasia\nextraction achieves 0.964 F1-score, while the multi-class model achieves 0.911\nF1-score. Our method is generalizable and faster to implement as compared to a\ntailored rule-based approach.", "published": "2023-08-16 20:17:46", "link": "http://arxiv.org/abs/2308.08660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation", "abstract": "AutoGen is an open-source framework that allows developers to build LLM\napplications via multiple agents that can converse with each other to\naccomplish tasks. AutoGen agents are customizable, conversable, and can operate\nin various modes that employ combinations of LLMs, human inputs, and tools.\nUsing AutoGen, developers can also flexibly define agent interaction behaviors.\nBoth natural language and computer code can be used to program flexible\nconversation patterns for different applications. AutoGen serves as a generic\ninfrastructure to build diverse applications of various complexities and LLM\ncapacities. Empirical studies demonstrate the effectiveness of the framework in\nmany example applications, with domains ranging from mathematics, coding,\nquestion answering, operations research, online decision-making, entertainment,\netc.", "published": "2023-08-16 05:57:52", "link": "http://arxiv.org/abs/2308.08155v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Sarcasm Detection in a Disaster Context", "abstract": "During natural disasters, people often use social media platforms such as\nTwitter to ask for help, to provide information about the disaster situation,\nor to express contempt about the unfolding event or public policies and\nguidelines. This contempt is in some cases expressed as sarcasm or irony.\nUnderstanding this form of speech in a disaster-centric context is essential to\nimproving natural language understanding of disaster-related tweets. In this\npaper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for\nintended sarcasm, and provide a comprehensive investigation of sarcasm\ndetection using pre-trained language models. Our best model is able to obtain\nas much as 0.70 F1 on our dataset. We also demonstrate that the performance on\nHurricaneSARC can be improved by leveraging intermediate task transfer\nlearning. We release our data and code at\nhttps://github.com/tsosea2/HurricaneSarc.", "published": "2023-08-16 05:58:12", "link": "http://arxiv.org/abs/2308.08156v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Performance on Seen and Unseen Dialogue Scenarios using\n  Retrieval-Augmented End-to-End Task-Oriented System", "abstract": "End-to-end task-oriented dialogue (TOD) systems have achieved promising\nperformance by leveraging sophisticated natural language understanding and\nnatural language generation capabilities of pre-trained models. This work\nenables the TOD systems with more flexibility through a simple cache. The cache\nprovides the flexibility to dynamically update the TOD systems and handle both\nexisting and unseen dialogue scenarios. Towards this end, we first fine-tune a\nretrieval module to effectively retrieve the most relevant information entries\nfrom the cache. We then train end-to-end TOD models that can refer to and\nground on both dialogue history and retrieved information during TOD\ngeneration. The cache is straightforward to construct, and the backbone models\nof TOD systems are compatible with existing pre-trained generative models.\nExtensive experiments demonstrate the superior performance of our framework,\nwith a notable improvement in non-empty joint goal accuracy by 6.7% compared to\nstrong baselines.", "published": "2023-08-16 06:52:10", "link": "http://arxiv.org/abs/2308.08169v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for\n  Time Series", "abstract": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today's\nLarge Language Model (LLM) context: LLM-for-TS (model-centric) designs and\ntrains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;\nTS-for-LLM (data-centric) converts TS into a model-friendly representation to\nenable the pre-trained LLM to handle TS data. Given the lack of data, limited\nresources, semantic context requirements, and so on, this work focuses on\nTS-for-LLM, where we aim to activate LLM's ability for TS data by designing a\nTS embedding method suitable for LLM. The proposed method is named TEST. It\nfirst tokenizes TS, builds an encoder to embed TS via instance-wise,\nfeature-wise, and text-prototype-aligned contrast, where the TS embedding space\nis aligned to LLM embedding layer space, then creates soft prompts to make LLM\nmore open to that embeddings, and finally implements TS tasks using the frozen\nLLM. We also demonstrate the feasibility of TS-for-LLM through theory and\nexperiments. Experiments are carried out on TS classification, forecasting, and\nrepresentation tasks using eight frozen LLMs with various structures and sizes.\nThe results show that the pre-trained LLM with TEST strategy can achieve better\nor comparable performance than today's SOTA TS models and offer benefits for\nfew-shot and generalization. By treating LLM as the pattern machine, TEST can\nendow LLM's ability to process TS data without compromising language ability.\nWe hope that this study will serve as a foundation for future work to support\nTS+LLM progress.", "published": "2023-08-16 09:16:02", "link": "http://arxiv.org/abs/2308.08241v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-training with Large Language Model-based Document Expansion for\n  Dense Passage Retrieval", "abstract": "In this paper, we systematically study the potential of pre-training with\nLarge Language Model(LLM)-based document expansion for dense passage retrieval.\nConcretely, we leverage the capabilities of LLMs for document expansion, i.e.\nquery generation, and effectively transfer expanded knowledge to retrievers\nusing pre-training strategies tailored for passage retrieval. These strategies\ninclude contrastive learning and bottlenecked query generation. Furthermore, we\nincorporate a curriculum learning strategy to reduce the reliance on LLM\ninferences. Experimental results demonstrate that pre-training with LLM-based\ndocument expansion significantly boosts the retrieval performance on\nlarge-scale web-search tasks. Our work shows strong zero-shot and out-of-domain\nretrieval abilities, making it more widely applicable for retrieval when\ninitializing with no human-labeled data.", "published": "2023-08-16 11:10:43", "link": "http://arxiv.org/abs/2308.08285v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Advancing continual lifelong learning in neural information retrieval:\n  definition, dataset, framework, and empirical evaluation", "abstract": "Continual learning refers to the capability of a machine learning model to\nlearn and adapt to new information, without compromising its performance on\npreviously learned tasks. Although several studies have investigated continual\nlearning methods for information retrieval tasks, a well-defined task\nformulation is still lacking, and it is unclear how typical learning strategies\nperform in this context. To address this challenge, a systematic task\nformulation of continual neural information retrieval is presented, along with\na multiple-topic dataset that simulates continuous information retrieval. A\ncomprehensive continual neural information retrieval framework consisting of\ntypical retrieval models and continual learning strategies is then proposed.\nEmpirical evaluations illustrate that the proposed framework can successfully\nprevent catastrophic forgetting in neural information retrieval and enhance\nperformance on previously learned tasks. The results indicate that\nembedding-based retrieval models experience a decline in their continual\nlearning performance as the topic shift distance and dataset volume of new\ntasks increase. In contrast, pretraining-based models do not show any such\ncorrelation. Adopting suitable learning strategies can mitigate the effects of\ntopic shift and data augmentation.", "published": "2023-08-16 14:01:25", "link": "http://arxiv.org/abs/2308.08378v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value\n  Extraction", "abstract": "Existing attribute-value extraction (AVE) models require large quantities of\nlabeled data for training. However, new products with new attribute-value pairs\nenter the market every day in real-world e-Commerce. Thus, we formulate AVE in\nmulti-label few-shot learning (FSL), aiming to extract unseen attribute value\npairs based on a small number of training examples. We propose a\nKnowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,\nleveraging the generated label description and category information to learn\nmore discriminative prototypes. Besides, KEAF integrates with hybrid attention\nto reduce noise and capture more informative semantics for each class by\ncalculating the label-relevant and query-related weights. To achieve\nmulti-label inference, KEAF further learns a dynamic threshold by integrating\nthe semantic information from both the support set and the query set. Extensive\nexperiments with ablation studies conducted on two datasets demonstrate that\nKEAF outperforms other SOTA models for information extraction in FSL. The code\ncan be found at: https://github.com/gjiaying/KEAF", "published": "2023-08-16 14:58:12", "link": "http://arxiv.org/abs/2308.08413v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "FootGPT : A Large Language Model Development Experiment on a Minimal\n  Setting", "abstract": "With recent empirical observations, it has been argued that the most\nsignificant aspect of developing accurate language models may be the proper\ndataset content and training strategy compared to the number of neural\nparameters, training duration or dataset size. Following this argument, we\nopted to fine tune a one billion parameter size trained general purpose causal\nlanguage model with a dataset curated on team statistics of the Italian\nfootball league first ten game weeks, using low rank adaptation. The limited\ntraining dataset was compiled based on a framework where a powerful commercial\nlarge language model provides distilled paragraphs and question answer pairs as\nintended. The training duration was kept relatively short to provide a basis\nfor our minimal setting exploration. We share our key observations on the\nprocess related to developing a specific purpose language model which is\nintended to interpret soccer data with constrained resources in this article.", "published": "2023-08-16 18:03:22", "link": "http://arxiv.org/abs/2308.08610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answering Ambiguous Questions with a Database of Questions, Answers, and\n  Revisions", "abstract": "Many open-domain questions are under-specified and thus have multiple\npossible answers, each of which is correct under a different interpretation of\nthe question. Answering such ambiguous questions is challenging, as it requires\nretrieving and then reasoning about diverse information from multiple passages.\nWe present a new state-of-the-art for answering ambiguous questions that\nexploits a database of unambiguous questions generated from Wikipedia. On the\nchallenging ASQA benchmark, which requires generating long-form answers that\nsummarize the multiple answers to an ambiguous question, our method improves\nperformance by 15% (relative improvement) on recall measures and 10% on\nmeasures which evaluate disambiguating questions from predicted outputs.\nRetrieving from the database of generated questions also gives large\nimprovements in diverse passage retrieval (by matching user questions q to\npassages p indirectly, via questions q' generated from p).", "published": "2023-08-16 20:23:16", "link": "http://arxiv.org/abs/2308.08661v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lightweight Adaptation of Neural Language Models via Subspace Embedding", "abstract": "Traditional neural word embeddings are usually dependent on a richer\ndiversity of vocabulary. However, the language models recline to cover major\nvocabularies via the word embedding parameters, in particular, for multilingual\nlanguage models that generally cover a significant part of their overall\nlearning parameters. In this work, we present a new compact embedding structure\nto reduce the memory footprint of the pre-trained language models with a\nsacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction\nfollows a set of subspace embeddings and an assignment procedure via the\ncontextual relationship among tokens from pre-trained language models. The\nsubspace embedding structure calibrates to masked language models, to evaluate\nour compact embedding structure on similarity and textual entailment tasks,\nsentence and paraphrase tasks. Our experimental evaluation shows that the\nsubspace embeddings achieve compression rates beyond 99.8% in comparison with\nthe original embeddings for the language models on XNLI and GLUE benchmark\nsuites.", "published": "2023-08-16 22:16:00", "link": "http://arxiv.org/abs/2308.08688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only\n  Quantization for LLMs", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross various language tasks but pose challenges for practical deployment due\nto their substantial memory requirements. Furthermore, the latest generative\nmodels suffer from high inference costs caused by the memory bandwidth\nbottleneck in the auto-regressive decoding process. To address these issues, we\npropose an efficient weight-only quantization method that reduces memory\nconsumption and accelerates inference for LLMs. To ensure minimal quality\ndegradation, we introduce a simple and effective heuristic approach that\nutilizes only the model weights of a pre-trained model. This approach is\napplicable to both Mixture-of-Experts (MoE) and dense models without requiring\nadditional fine-tuning. To demonstrate the effectiveness of our proposed\nmethod, we first analyze the challenges and issues associated with LLM\nquantization. Subsequently, we present our heuristic approach, which adaptively\nfinds the granularity of quantization, effectively addressing these problems.\nFurthermore, we implement highly efficient GPU GEMMs that perform on-the-fly\nmatrix multiplication and dequantization, supporting the multiplication of fp16\nor bf16 activations with int8 or int4 weights. We evaluate our approach on\nlarge-scale open source models such as OPT-175B and internal MoE models,\nshowcasing minimal accuracy loss while achieving up to 3.65 times higher\nthroughput on the same number of GPUs.", "published": "2023-08-16 23:57:41", "link": "http://arxiv.org/abs/2308.09723v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment\n  about ChatGPT", "abstract": "The groundbreaking invention of ChatGPT has triggered enormous discussion\namong users across all fields and domains. Among celebration around its various\nadvantages, questions have been raised with regards to its correctness and\nethics of its use. Efforts are already underway towards capturing user\nsentiments around it. But it begs the question as to how the research community\nis analyzing ChatGPT with regards to various aspects of its usage. It is this\nsentiment of the researchers that we analyze in our work. Since Aspect-Based\nSentiment Analysis has usually only been applied on a few datasets, it gives\nlimited success and that too only on short text data. We propose a methodology\nthat uses Explainable AI to facilitate such analysis on research data. Our\ntechnique presents valuable insights into extending the state of the art of\nAspect-Based Sentiment Analysis on newer datasets, where such analysis is not\nhampered by the length of the text data.", "published": "2023-08-16 07:44:06", "link": "http://arxiv.org/abs/2308.11001v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals", "abstract": "Millimeter wave (mmWave) based speech recognition provides more possibility\nfor audio-related applications, such as conference speech transcription and\neavesdropping. However, considering the practicality in real scenarios, latency\nand recognizable vocabulary size are two critical factors that cannot be\noverlooked. In this paper, we propose Radio2Text, the first mmWave-based system\nfor streaming automatic speech recognition (ASR) with a vocabulary size\nexceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer\nthat is capable of effectively learning representations of speech-related\nfeatures, paving the way for streaming ASR with a large vocabulary. To\nalleviate the deficiency of streaming networks unable to access entire future\ninputs, we propose the Guidance Initialization that facilitates the transfer of\nfeature knowledge related to the global context from the non-streaming\nTransformer to the tailored streaming Transformer through weight inheritance.\nFurther, we propose a cross-modal structure based on knowledge distillation\n(KD), named cross-modal KD, to mitigate the negative effect of low quality\nmmWave signals on recognition performance. In the cross-modal KD, the audio\nstreaming Transformer provides feature and response guidance that inherit\nfruitful and accurate speech information to supervise the training of the\ntailored radio streaming Transformer. The experimental results show that our\nRadio2Text can achieve a character error rate of 5.7% and a word error rate of\n9.4% for the recognition of a vocabulary consisting of over 13,000 words.", "published": "2023-08-16 03:31:30", "link": "http://arxiv.org/abs/2308.08125v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChinaTelecom System Description to VoxCeleb Speaker Recognition\n  Challenge 2023", "abstract": "This technical report describes ChinaTelecom system for Track 1 (closed) of\nthe VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system\nconsists of several ResNet variants trained only on VoxCeleb2, which were fused\nfor better performance later. Score calibration was also applied for each\nvariant and the fused system. The final submission achieved minDCF of 0.1066\nand EER of 1.980%.", "published": "2023-08-16 07:21:01", "link": "http://arxiv.org/abs/2308.08181v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Challenges and Opportunities of Using Transformer-Based Multi-Task\n  Learning in NLP Through ML Lifecycle: A Survey", "abstract": "The increasing adoption of natural language processing (NLP) models across\nindustries has led to practitioners' need for machine learning systems to\nhandle these models efficiently, from training to serving them in production.\nHowever, training, deploying, and updating multiple models can be complex,\ncostly, and time-consuming, mainly when using transformer-based pre-trained\nlanguage models. Multi-Task Learning (MTL) has emerged as a promising approach\nto improve efficiency and performance through joint training, rather than\ntraining separate models. Motivated by this, we first provide an overview of\ntransformer-based MTL approaches in NLP. Then, we discuss the challenges and\nopportunities of using MTL approaches throughout typical ML lifecycle phases,\nspecifically focusing on the challenges related to data engineering, model\ndevelopment, deployment, and monitoring phases. This survey focuses on\ntransformer-based MTL architectures and, to the best of our knowledge, is novel\nin that it systematically analyses how transformer-based MTL in NLP fits into\nML lifecycle phases. Furthermore, we motivate research on the connection\nbetween MTL and continual learning (CL), as this area remains unexplored. We\nbelieve it would be practical to have a model that can handle both MTL and CL,\nas this would make it easier to periodically re-train the model, update it due\nto distribution shifts, and add new capabilities to meet real-world\nrequirements.", "published": "2023-08-16 09:11:00", "link": "http://arxiv.org/abs/2308.08234v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P)\n  Transduction", "abstract": "Text-to-Text Transfer Transformer (T5) has recently been considered for the\nGrapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free\nbyte-level model based on T5 referred to as ByT5, recently gave promising\nresults on word-level G2P conversion by representing each input character with\nits corresponding UTF-8 encoding. Although it is generally understood that\nsentence-level or paragraph-level G2P can improve usability in real-world\napplications as it is better suited to perform on heteronyms and linking sounds\nbetween words, we find that using ByT5 for these scenarios is nontrivial. Since\nByT5 operates on the character level, it requires longer decoding steps, which\ndeteriorates the performance due to the exposure bias commonly observed in\nauto-regressive generation models. This paper shows that the performance of\nsentence-level and paragraph-level G2P can be improved by mitigating such\nexposure bias using our proposed loss-based sampling method.", "published": "2023-08-16 15:49:36", "link": "http://arxiv.org/abs/2308.08442v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models", "abstract": "Data contamination, i.e., the presence of test data from downstream tasks in\nthe training data of large language models (LLMs), is a potential major issue\nin measuring LLMs' real effectiveness on other tasks. We propose a\nstraightforward yet effective method for identifying data contamination within\nLLMs. At its core, our approach starts by identifying potential contamination\nat the instance level; using this information, our approach then assesses wider\ncontamination at the partition level. To estimate contamination of individual\ninstances, we employ \"guided instruction:\" a prompt consisting of the dataset\nname, partition type, and the random-length initial segment of a reference\ninstance, asking the LLM to complete it. An instance is flagged as contaminated\nif the LLM's output either exactly or nearly matches the latter segment of the\nreference. To understand if an entire partition is contaminated, we propose two\nideas. The first idea marks a dataset partition as contaminated if the average\noverlap score with the reference instances (as measured by ROUGE-L or BLEURT)\nis statistically significantly better with the completions from guided\ninstruction compared to a \"general instruction\" that does not include the\ndataset and partition name. The second idea marks a dataset partition as\ncontaminated if a classifier based on GPT-4 with few-shot in-context learning\nprompt marks multiple generated completions as exact/near-exact matches of the\ncorresponding reference instances. Our best method achieves an accuracy between\n92% and 100% in detecting if an LLM is contaminated with seven datasets,\ncontaining train and test/validation partitions, when contrasted with manual\nevaluation by human experts. Further, our findings indicate that GPT-4 is\ncontaminated with AG News, WNLI, and XSum datasets.", "published": "2023-08-16 16:48:57", "link": "http://arxiv.org/abs/2308.08493v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect\n  Transfer for Speech Synthesis", "abstract": "Affect is an emotional characteristic encompassing valence, arousal, and\nintensity, and is a crucial attribute for enabling authentic conversations.\nWhile existing text-to-speech (TTS) and speech-to-speech systems rely on\nstrength embedding vectors and global style tokens to capture emotions, these\nmodels represent emotions as a component of style or represent them in discrete\ncategories. We propose AffectEcho, an emotion translation model, that uses a\nVector Quantized codebook to model emotions within a quantized space featuring\nfive levels of affect intensity to capture complex nuances and subtle\ndifferences in the same emotion. The quantized emotional embeddings are\nimplicitly derived from spoken speech samples, eliminating the need for one-hot\nvectors or explicit strength embeddings. Experimental results demonstrate the\neffectiveness of our approach in controlling the emotions of generated speech\nwhile preserving identity, style, and emotional cadence unique to each speaker.\nWe showcase the language-independent emotion modeling capability of the\nquantized emotional embeddings learned from a bilingual (English and Chinese)\nspeech corpus with an emotion transfer task from a reference speech to a target\nspeech. We achieve state-of-art results on both qualitative and quantitative\nmetrics.", "published": "2023-08-16 06:28:29", "link": "http://arxiv.org/abs/2308.08577v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Boosting Logical Reasoning in Large Language Models through a New\n  Framework: The Graph of Thought", "abstract": "Recent advancements in large-scale models, such as GPT-4, have showcased\nremarkable capabilities in addressing standard queries. However, when facing\ncomplex problems that require multi-step logical reasoning, their accuracy\ndramatically decreases. Current research has explored the realm of\n\\textit{prompting engineering} to bolster the inferential capacities of these\nmodels. Our paper unveils a pioneering prompting technique, dubbed\n\\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating\nchallenges: the 24-point game, resolution of high-degree polynomial equations,\nand derivation of formulas for recursive sequences, our method outperformed\nGPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for each\nrespective task. Moreover, when juxtaposed with the state-of-the-art (SOTA)\nprompting method, \\textit{Tree of Thought (ToT)}, our approach registered an\naverage accuracy boost of $23\\%$, $24\\%$, and $15\\%$.", "published": "2023-08-16 18:13:27", "link": "http://arxiv.org/abs/2308.08614v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Self-Deception: Reverse Penetrating the Semantic Firewall of Large\n  Language Models", "abstract": "Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n  This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.", "published": "2023-08-16 09:04:36", "link": "http://arxiv.org/abs/2308.11521v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Preliminary Study on a Conceptual Game Feature Generation and\n  Recommendation System", "abstract": "This paper introduces a system used to generate game feature suggestions\nbased on a text prompt. Trained on the game descriptions of almost 60k games,\nit uses the word embeddings of a small GLoVe model to extract features and\nentities found in thematically similar games which are then passed through a\ngenerator model to generate new features for a user's prompt. We perform a\nshort user study comparing the features generated from a fine-tuned GPT-2\nmodel, a model using the ConceptNet, and human-authored game features. Although\nhuman suggestions won the overall majority of votes, the GPT-2 model\noutperformed the human suggestions in certain games. This system is part of a\nlarger game design assistant tool that is able to collaborate with users at a\nconceptual level.", "published": "2023-08-16 21:20:50", "link": "http://arxiv.org/abs/2308.13538v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The ID R&D VoxCeleb Speaker Recognition Challenge 2023 System\n  Description", "abstract": "This report describes ID R&D team submissions for Track 2 (open) to the\nVoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). Our solution is based\non the fusion of deep ResNets and self-supervised learning (SSL) based models\ntrained on a mixture of a VoxCeleb2 dataset and a large version of a VoxTube\ndataset. The final submission to the Track 2 achieved the first place on the\nVoxSRC-23 public leaderboard with a minDCF(0.05) of 0.0762 and EER of 1.30%.", "published": "2023-08-16 11:42:02", "link": "http://arxiv.org/abs/2308.08294v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Classifying Dementia in the Presence of Depression: A Cross-Corpus Study", "abstract": "Automated dementia screening enables early detection and intervention,\nreducing costs to healthcare systems and increasing quality of life for those\naffected. Depression has shared symptoms with dementia, adding complexity to\ndiagnoses. The research focus so far has been on binary classification of\ndementia (DEM) and healthy controls (HC) using speech from picture description\ntests from a single dataset. In this work, we apply established baseline\nsystems to discriminate cognitive impairment in speech from the semantic Verbal\nFluency Test and the Boston Naming Test using text, audio and emotion\nembeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform\ncross-corpus and mixed-corpus experiments on two independently recorded German\ndatasets to investigate generalization to larger populations and different\nrecording conditions. In a detailed error analysis, we look at depression as a\nsecondary diagnosis to understand what our classifiers actually learn.", "published": "2023-08-16 12:09:00", "link": "http://arxiv.org/abs/2308.08306v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual\n  Speech Separation", "abstract": "Recent research has made significant progress in designing fusion modules for\naudio-visual speech separation. However, they predominantly focus on\nmulti-modal fusion at a single temporal scale of auditory and visual features\nwithout employing selective attention mechanisms, which is in sharp contrast\nwith the brain. To address this issue, We propose a novel model called Intra-\nand Inter-Attention Network (IIANet), which leverages the attention mechanism\nfor efficient audio-visual feature fusion. IIANet consists of two types of\nattention blocks: intra-attention (IntraA) and inter-attention (InterA) blocks,\nwhere the InterA blocks are distributed at the top, middle and bottom of\nIIANet. Heavily inspired by the way how human brain selectively focuses on\nrelevant content at various temporal scales, these blocks maintain the ability\nto learn modality-specific features and enable the extraction of different\nsemantics from audio-visual features. Comprehensive experiments on three\nstandard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2)\ndemonstrate the effectiveness of IIANet, outperforming previous\nstate-of-the-art methods while maintaining comparable inference time. In\nparticular, the fast version of IIANet (IIANet-fast) has only 7% of CTCNet's\nMACs and is 40% faster than CTCNet on CPUs while achieving better separation\nquality, showing the great potential of attention mechanism for efficient and\neffective multimodal fusion.", "published": "2023-08-16 04:31:33", "link": "http://arxiv.org/abs/2308.08143v3", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accurate synthesis of Dysarthric Speech for ASR data augmentation", "abstract": "Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers\ncommunicate more effectively. However, robust dysarthria-specific ASR requires\na significant amount of training speech, which is not readily available for\ndysarthric talkers. This paper presents a new dysarthric speech synthesis\nmethod for the purpose of ASR training data augmentation. Differences in\nprosodic and acoustic characteristics of dysarthric spontaneous speech at\nvarying severity levels are important components for dysarthric speech\nmodeling, synthesis, and augmentation. For dysarthric speech synthesis, a\nmodified neural multi-talker TTS is implemented by adding a dysarthria severity\nlevel coefficient and a pause insertion model to synthesize dysarthric speech\nfor varying severity levels. To evaluate the effectiveness for synthesis of\ntraining data for ASR, dysarthria-specific speech recognition was used. Results\nshow that a DNN-HMM model trained on additional synthetic dysarthric speech\nachieves WER improvement of 12.2% compared to the baseline, and that the\naddition of the severity level and pause insertion controls decrease WER by\n6.5%, showing the effectiveness of adding these parameters. Overall results on\nthe TORGO database demonstrate that using dysarthric synthetic speech to\nincrease the amount of dysarthric-patterned speech for training has significant\nimpact on the dysarthric ASR systems. In addition, we have conducted a\nsubjective evaluation to evaluate the dysarthric-ness and similarity of\nsynthesized speech. Our subjective evaluation shows that the perceived\ndysartrhic-ness of synthesized speech is similar to that of true dysarthric\nspeech, especially for higher levels of dysarthria", "published": "2023-08-16 15:42:24", "link": "http://arxiv.org/abs/2308.08438v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
