{"title": "Tailor: Generating and Perturbing Text with Semantic Controls", "abstract": "Controlled text perturbation is useful for evaluating and improving model\ngeneralizability. However, current techniques rely on training a model for\nevery target perturbation, which is expensive and hard to generalize. We\npresent Tailor, a semantically-controlled text generation system. Tailor builds\non a pretrained seq2seq model and produces textual outputs conditioned on\ncontrol codes derived from semantic representations. We craft a set of\noperations to modify the control codes, which in turn steer generation towards\ntargeted attributes. These operations can be further composed into higher-level\nones, allowing for flexible perturbation strategies. We demonstrate the\neffectiveness of these perturbations in multiple applications. First, we use\nTailor to automatically create high-quality contrast sets for four distinct\nnatural language processing (NLP) tasks. These contrast sets contain fewer\nspurious artifacts and are complementary to manually annotated ones in their\nlexical diversity. Second, we show that Tailor perturbations can improve model\ngeneralization through data augmentation. Perturbing just 2% of training data\nleads to a 5.8-point gain on an NLI challenge set measuring reliance on\nsyntactic heuristics.", "published": "2021-07-15 06:38:59", "link": "http://arxiv.org/abs/2107.07150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing", "abstract": "As neural language models grow in effectiveness, they are increasingly being\napplied in real-world settings. However these applications tend to be limited\nin the modes of interaction they support. In this extended abstract, we propose\nWordcraft, an AI-assisted editor for story writing in which a writer and a\ndialog system collaborate to write a story. Our novel interface uses few-shot\nlearning and the natural affordances of conversation to support a variety of\ninteractions. Our editor provides a sandbox for writers to probe the boundaries\nof transformer-based language models and paves the way for future\nhuman-in-the-loop training pipelines and novel evaluation methods.", "published": "2021-07-15 16:18:27", "link": "http://arxiv.org/abs/2107.07430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for\n  Defending Word Substitution-based Attacks", "abstract": "In this paper, we present an approach to improve the robustness of BERT\nlanguage models against word substitution-based adversarial attacks by\nleveraging adversarial perturbations for self-supervised contrastive learning.\nWe create a word-level adversarial attack generating hard positives on-the-fly\nas adversarial examples during contrastive learning. In contrast to previous\nworks, our method improves model robustness without using any labeled data.\nExperimental results show that our method improves robustness of BERT against\nfour different word substitution-based adversarial attacks, and combining our\nmethod with adversarial training gives higher robustness than adversarial\ntraining alone. As our method improves the robustness of BERT purely with\nunlabeled data, it opens up the possibility of using large text datasets to\ntrain robust language models against word substitution-based adversarial\nattacks.", "published": "2021-07-15 21:03:34", "link": "http://arxiv.org/abs/2107.07610v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Learning for Text Classification with Multi-source Noise\n  Simulation and Hard Example Mining", "abstract": "Many real-world applications involve the use of Optical Character Recognition\n(OCR) engines to transform handwritten images into transcripts on which\ndownstream Natural Language Processing (NLP) models are applied. In this\nprocess, OCR engines may introduce errors and inputs to downstream NLP models\nbecome noisy. Despite that pre-trained models achieve state-of-the-art\nperformance in many NLP benchmarks, we prove that they are not robust to noisy\ntexts generated by real OCR engines. This greatly limits the application of NLP\nmodels in real-world scenarios. In order to improve model performance on noisy\nOCR transcripts, it is natural to train the NLP model on labelled noisy texts.\nHowever, in most cases there are only labelled clean texts. Since there is no\nhandwritten pictures corresponding to the text, it is impossible to directly\nuse the recognition model to obtain noisy labelled data. Human resources can be\nemployed to copy texts and take pictures, but it is extremely expensive\nconsidering the size of data for model training. Consequently, we are\ninterested in making NLP models intrinsically robust to OCR errors in a low\nresource manner. We propose a novel robust training framework which 1) employs\nsimple but effective methods to directly simulate natural OCR noises from clean\ntexts and 2) iteratively mines the hard examples from a large number of\nsimulated samples for optimal performance. 3) To make our model learn\nnoise-invariant representations, a stability loss is employed. Experiments on\nthree real-world datasets show that the proposed framework boosts the\nrobustness of pre-trained models by a large margin. We believe that this work\ncan greatly promote the application of NLP models in actual scenarios, although\nthe algorithm we use is simple and straightforward. We make our codes and three\ndatasets publicly\navailable\\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.", "published": "2021-07-15 04:39:22", "link": "http://arxiv.org/abs/2107.07113v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning based Online Dialogic Instruction Detection with\n  Pre-trained Language Models", "abstract": "In this work, we study computational approaches to detect online dialogic\ninstructions, which are widely used to help students understand learning\nmaterials, and build effective study habits. This task is rather challenging\ndue to the widely-varying quality and pedagogical styles of dialogic\ninstructions. To address these challenges, we utilize pre-trained language\nmodels, and propose a multi-task paradigm which enhances the ability to\ndistinguish instances of different classes by enlarging the margin between\ncategories via contrastive loss. Furthermore, we design a strategy to fully\nexploit the misclassified examples during the training stage. Extensive\nexperiments on a real-world online educational data set demonstrate that our\napproach achieves superior performance compared to representative baselines. To\nencourage reproducible results, we make our implementation online available at\n\\url{https://github.com/AIED2021/multitask-dialogic-instruction}.", "published": "2021-07-15 04:57:57", "link": "http://arxiv.org/abs/2107.07119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Solving ESL Sentence Completion Questions via Pre-trained Neural\n  Language Models", "abstract": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL) and building computational approaches to automatically solve such\nquestions is beneficial to language learners. In this work, we propose a neural\nframework to solve SC questions in English examinations by utilizing\npre-trained language models. We conduct extensive experiments on a real-world\nK-12 ESL SC question dataset and the results demonstrate the superiority of our\nmodel in terms of prediction accuracy. Furthermore, we run precision-recall\ntrade-off analysis to discuss the practical issues when deploying it in\nreal-life scenarios. To encourage reproducible results, we make our code\npublicly available at \\url{https://github.com/AIED2021/ESL-SentenceCompletion}.", "published": "2021-07-15 05:01:39", "link": "http://arxiv.org/abs/2107.07122v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FLEX: Unifying Evaluation for Few-Shot NLP", "abstract": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. In response, we formulate the FLEX Principles, a\nset of requirements and best practices for unified, rigorous, valid, and\ncost-sensitive few-shot NLP evaluation. These principles include Sample Size\nDesign, a novel approach to benchmark design that optimizes statistical\naccuracy and precision while keeping evaluation costs manageable. Following the\nprinciples, we release the FLEX benchmark, which includes four few-shot\ntransfer settings, zero-shot evaluation, and a public leaderboard that covers\ndiverse NLP tasks. In addition, we present UniFew, a prompt-based model for\nfew-shot learning that unifies pretraining and finetuning prompt formats,\neschewing complex machinery of recent prompt-based approaches in adapting\ndownstream task formats to language model pretraining objectives. We\ndemonstrate that despite simplicity, UniFew achieves results competitive with\nboth popular meta-learning and prompt-based approaches.", "published": "2021-07-15 07:37:06", "link": "http://arxiv.org/abs/2107.07170v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Security in McAdams Coefficient-Based Speaker Anonymization by\n  Watermarking Method", "abstract": "Speaker anonymization aims to suppress speaker individuality to protect\nprivacy in speech while preserving the other aspects, such as speech content.\nOne effective solution for anonymization is to modify the McAdams coefficient.\nIn this work, we propose a method to improve the security for speaker\nanonymization based on the McAdams coefficient by using a speech watermarking\napproach. The proposed method consists of two main processes: one for embedding\nand one for detection. In embedding process, two different McAdams coefficients\nrepresent binary bits ``0\" and ``1\". The watermarked speech is then obtained by\nframe-by-frame bit inverse switching. Subsequently, the detection process is\ncarried out by a power spectrum comparison. We conducted objective evaluations\nwith reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech\nwatermarking with reference to the Information Hiding Challenge (IHC) and found\nthat our method could satisfy the blind detection, inaudibility, and robustness\nrequirements in watermarking. It also significantly improved the anonymization\nperformance in comparison to the secondary baseline system in VP2020.", "published": "2021-07-15 09:56:08", "link": "http://arxiv.org/abs/2107.07223v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MarIA: Spanish Language Models", "abstract": "This work presents MarIA, a family of Spanish language models and associated\nresources made available to the industry and the research community. Currently,\nMarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish\nlanguage models, which can arguably be presented as the largest and most\nproficient language models in Spanish. The models were pretrained using a\nmassive corpus of 570GB of clean and deduplicated texts with 135 billion words\nextracted from the Spanish Web Archive crawled by the National Library of Spain\nbetween 2009 and 2019. We assessed the performance of the models with nine\nexisting evaluation datasets and with a novel extractive Question Answering\ndataset created ex novo. Overall, MarIA models outperform the existing Spanish\nmodels across a variety of NLU tasks and training settings.", "published": "2021-07-15 11:23:05", "link": "http://arxiv.org/abs/2107.07253v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Turning Tables: Generating Examples from Semi-structured Tables for\n  Endowing Language Models with Reasoning Skills", "abstract": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.", "published": "2021-07-15 11:37:14", "link": "http://arxiv.org/abs/2107.07261v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch", "abstract": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leads to sub-optimal. In this work, we make the first\nattempt to automatically discover novel pre-trained language model (PLM)\nbackbone on a flexible search space containing the most fundamental operations\nfrom scratch. Specifically, we propose a well-designed search space which (i)\ncontains primitive math operations in the intra-layer level to explore novel\nattention structures, and (ii) leverages convolution blocks to be the\nsupplementary for attentions in the inter-layer level to better learn local\ndependency. To enhance the efficiency for finding promising architectures, we\npropose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm,\nwhich optimizes both the search algorithm and evaluation of candidate models.\nSpecifically, we propose Operation-Priority (OP) evolution strategy to\nfacilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base\noutperforms RoBERTa-base (using much more data) and BERT-large (with much\nlarger model size) by 2.4 and 1.4 higher score on GLUE test set.", "published": "2021-07-15 16:46:01", "link": "http://arxiv.org/abs/2107.07445v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark", "abstract": "Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot, and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods and thus hinders\ncumulative progress. In this paper, we introduce the Chinese Few-shot Learning\nEvaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation\nbenchmark in Chinese. It includes nine tasks, ranging from single-sentence and\nsentence-pair classification tasks to machine reading comprehension tasks. We\nsystematically evaluate five state-of-the-art (SOTA) few-shot learning methods\n(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark. Experimental results reveal that: 1) The effect\nof different few-shot learning methods is sensitive to the pre-trained model to\nwhich the methods are applied; 2) PET and P-tuning achieve the best overall\nperformance with RoBERTa and ERNIE respectively. Our benchmark is used in the\nfew-shot learning contest of NLPCC 2021. In addition, we provide a\nuser-friendly toolkit, as well as an online leaderboard to help facilitate\nfurther progress on Chinese few-shot learning. We provide a baseline\nperformance on different learning methods, a reference for future research.", "published": "2021-07-15 17:51:25", "link": "http://arxiv.org/abs/2107.07498v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Internet-Augmented Dialogue Generation", "abstract": "The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).", "published": "2021-07-15 19:00:35", "link": "http://arxiv.org/abs/2107.07566v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "abstract": "Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other's interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.", "published": "2021-07-15 19:01:08", "link": "http://arxiv.org/abs/2107.07567v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Task Requirements Writing Evaluation via Machine Reading\n  Comprehension", "abstract": "Task requirements (TRs) writing is an important question type in Key English\nTest and Preliminary English Test. A TR writing question may include multiple\nrequirements and a high-quality essay must respond to each requirement\nthoroughly and accurately. However, the limited teacher resources prevent\nstudents from getting detailed grading instantly. The majority of existing\nautomatic essay scoring systems focus on giving a holistic score but rarely\nprovide reasons to support it. In this paper, we proposed an end-to-end\nframework based on machine reading comprehension (MRC) to address this problem\nto some extent. The framework not only detects whether an essay responds to a\nrequirement question, but clearly marks where the essay answers the question.\nOur framework consists of three modules: question normalization module, ELECTRA\nbased MRC module and response locating module. We extensively explore\nstate-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and\n0.85 F1 score on a real-world educational dataset. To encourage reproducible\nresults, we make our code publicly available at\n\\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.", "published": "2021-07-15 05:12:52", "link": "http://arxiv.org/abs/2107.07957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Temporal-aware Language Representation Learning From Crowdsourced Labels", "abstract": "Learning effective language representations from crowdsourced labels is\ncrucial for many real-world machine learning tasks. A challenging aspect of\nthis problem is that the quality of crowdsourced labels suffer high intra- and\ninter-observer variability. Since the high-capacity deep neural networks can\neasily memorize all disagreements among crowdsourced labels, directly applying\nexisting supervised language representation learning algorithms may yield\nsuboptimal solutions. In this paper, we propose \\emph{TACMA}, a\n\\underline{t}emporal-\\underline{a}ware language representation learning\nheuristic for \\underline{c}rowdsourced labels with \\underline{m}ultiple\n\\underline{a}nnotators. The proposed approach (1) explicitly models the\nintra-observer variability with attention mechanism; (2) computes and\naggregates per-sample confidence scores from multiple workers to address the\ninter-observer disagreements. The proposed heuristic is extremely easy to\nimplement in around 5 lines of code. The proposed heuristic is evaluated on\nfour synthetic and four real-world data sets. The results show that our\napproach outperforms a wide range of state-of-the-art baselines in terms of\nprediction accuracy and AUC. To encourage the reproducible results, we make our\ncode publicly available at \\url{https://github.com/CrowdsourcingMining/TACMA}.", "published": "2021-07-15 05:25:56", "link": "http://arxiv.org/abs/2107.07958v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages", "abstract": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.", "published": "2021-07-15 15:42:43", "link": "http://arxiv.org/abs/2107.07402v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording", "abstract": "In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings without\nvoice activity detection (VAD), based on monotonic chunkwise attention (MoChA)\nwith an auxiliary connectionist temporal classification (CTC) objective. We\npropose a block-synchronous beam search decoding to take advantage of efficient\nbatched output-synchronous and low-latency input-synchronous searches. We also\npropose a VAD-free inference algorithm that leverages CTC probabilities to\ndetermine a suitable timing to reset the model states to tackle the\nvulnerability to long-form data. Experimental evaluations demonstrate that the\nblock-synchronous decoding achieves comparable accuracy to the\nlabel-synchronous one. Moreover, the VAD-free inference can recognize long-form\nspeech robustly for up to a few hours.", "published": "2021-07-15 17:59:10", "link": "http://arxiv.org/abs/2107.07509v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-task Learning with Cross Attention for Keyword Spotting", "abstract": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\nachieves a 12% relative reduction in the false reject ratios compared to the\nconventional multi-task learning with split branches and a bi-directional long\nshort-team memory decoder.", "published": "2021-07-15 22:38:16", "link": "http://arxiv.org/abs/2107.07634v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Multimodal Machine Learning Framework for Teacher Vocal Delivery\n  Evaluation", "abstract": "The quality of vocal delivery is one of the key indicators for evaluating\nteacher enthusiasm, which has been widely accepted to be connected to the\noverall course qualities. However, existing evaluation for vocal delivery is\nmainly conducted with manual ratings, which faces two core challenges:\nsubjectivity and time-consuming. In this paper, we present a novel machine\nlearning approach that utilizes pairwise comparisons and a multimodal\northogonal fusing algorithm to generate large-scale objective evaluation\nresults of the teacher vocal delivery in terms of fluency and passion. We\ncollect two datasets from real-world education scenarios and the experiment\nresults demonstrate the effectiveness of our algorithm. To encourage\nreproducible results, we make our code public available at\n\\url{https://github.com/tal-ai/ML4VocalDelivery.git}.", "published": "2021-07-15 05:09:39", "link": "http://arxiv.org/abs/2107.07956v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning", "abstract": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.", "published": "2021-07-15 17:54:36", "link": "http://arxiv.org/abs/2107.07502v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Objective Metrics to Evaluate Residual-Echo Suppression During\n  Double-Talk", "abstract": "Human subjective evaluation is optimal to assess speech quality for human\nperception. The recently introduced deep noise suppression mean opinion score\n(DNSMOS) metric was shown to estimate human ratings with great accuracy. The\nsignal-to-distortion ratio (SDR) metric is widely used to evaluate\nresidual-echo suppression (RES) systems by estimating speech quality during\ndouble-talk. However, since the SDR is affected by both speech distortion and\nresidual-echo presence, it does not correlate well with human ratings according\nto the DNSMOS. To address that, we introduce two objective metrics to\nseparately quantify the desired-speech maintained level (DSML) and\nresidual-echo suppression level (RESL) during double-talk. These metrics are\nevaluated using a deep learning-based RES-system with a tunable design\nparameter. Using 280 hours of real and simulated recordings, we show that the\nDSML and RESL correlate well with the DNSMOS with high generalization to\nvarious setups. Also, we empirically investigate the relation between tuning\nthe RES-system design parameter and the DSML-RESL tradeoff it creates and offer\na practical design scheme for dynamic system requirements.", "published": "2021-07-15 17:17:55", "link": "http://arxiv.org/abs/2107.07471v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Filtered Noise Shaping for Time Domain Room Impulse Response Estimation\n  From Reverberant Speech", "abstract": "Deep learning approaches have emerged that aim to transform an audio signal\nso that it sounds as if it was recorded in the same room as a reference\nrecording, with applications both in audio post-production and augmented\nreality. In this work, we propose FiNS, a Filtered Noise Shaping network that\ndirectly estimates the time domain room impulse response (RIR) from reverberant\nspeech. Our domain-inspired architecture features a time domain encoder and a\nfiltered noise shaping decoder that models the RIR as a summation of decaying\nfiltered noise signals, along with direct sound and early reflection\ncomponents. Previous methods for acoustic matching utilize either large models\nto transform audio to match the target room or predict parameters for\nalgorithmic reverberators. Instead, blind estimation of the RIR enables\nefficient and realistic transformation with a single convolution. An evaluation\ndemonstrates our model not only synthesizes RIRs that match parameters of the\ntarget room, such as the $T_{60}$ and DRR, but also more accurately reproduces\nperceptual characteristics of the target room, as shown in a listening test\nwhen compared to deep learning baselines.", "published": "2021-07-15 17:56:12", "link": "http://arxiv.org/abs/2107.07503v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sketching sounds: an exploratory study on sound-shape associations", "abstract": "Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.", "published": "2021-07-15 14:37:53", "link": "http://arxiv.org/abs/2107.07360v1", "categories": ["cs.MM", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based\n  EEG Signals with Convolutional Autoencoder", "abstract": "Brain-computer interface (BCI) is one of the tools which enables the\ncommunication between humans and devices by reflecting intention and status of\nhumans. With the development of artificial intelligence, the interest in\ncommunication between humans and drones using electroencephalogram (EEG) is\nincreased. Especially, in the case of controlling drone swarms such as\ndirection or formation, there are many advantages compared with controlling a\ndrone unit. Imagined speech is one of the endogenous BCI paradigms, which can\nidentify intentions of users. When conducting imagined speech, the users\nimagine the pronunciation as if actually speaking. In contrast, overt speech is\na task in which the users directly pronounce the words. When controlling drone\nswarms using imagined speech, complex commands can be delivered more\nintuitively, but decoding performance is lower than that of other endogenous\nBCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of\novert speech for imagined speech-based EEG signals classification. To the best\nof our knowledge, this study is the first attempt to use EEG features of overt\nspeech to decode imagined speech-based EEG signals with an autoencoder. A total\nof eight subjects participated in the experiment. When classifying four words,\nthe average accuracy of the DAL was 48.41%. In addition, when comparing the\nperformance between w/o and w/ EEG features of overt speech, there was a\nperformance improvement of 7.42% when including EEG features of overt speech.\nHence, we demonstrated that EEG features of overt speech could improve the\ndecoding performance of imagined speech.", "published": "2021-07-15 01:13:19", "link": "http://arxiv.org/abs/2107.07064v1", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
