{"title": "Machine-Generated Text Localization", "abstract": "Machine-Generated Text (MGT) detection aims to identify a piece of text as\nmachine or human written. Prior work has primarily formulated MGT detection as\na binary classification task over an entire document, with limited work\nexploring cases where only part of a document is machine generated. This paper\nprovides the first in-depth study of MGT that localizes the portions of a\ndocument that were machine generated. Thus, if a bad actor were to change a key\nportion of a news article to spread misinformation, whole document MGT\ndetection may fail since the vast majority is human written, but our approach\ncan succeed due to its granular approach. A key challenge in our MGT\nlocalization task is that short spans of text, e.g., a single sentence,\nprovides little information indicating if it is machine generated due to its\nshort length. To address this, we leverage contextual information, where we\npredict whether multiple sentences are machine or human written at once. This\nenables our approach to identify changes in style or content to boost\nperformance. A gain of 4-13% mean Average Precision (mAP) over prior work\ndemonstrates the effectiveness of approach on five diverse datasets: GoodNews,\nVisualNews, WikiText, Essay, and WP. We release our implementation at\nhttps://github.com/Zhongping-Zhang/MGT_Localization.", "published": "2024-02-19 00:07:28", "link": "http://arxiv.org/abs/2402.11744v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning Demonstration Selection via Influence Analysis", "abstract": "Large Language Models (LLMs) have showcased their In-Context Learning (ICL)\ncapabilities, enabling few-shot learning without the need for gradient updates.\nDespite its advantages, the effectiveness of ICL heavily depends on the choice\nof demonstrations. Selecting the most effective demonstrations for ICL remains\na significant research challenge. To tackle this issue, we propose a\ndemonstration selection method named InfICL, which utilizes influence functions\nto analyze impacts of training samples. By identifying the most influential\ntraining samples as demonstrations, InfICL aims to enhance the ICL\ngeneralization performance. To keep InfICL cost-effective, we only use the LLM\nto generate sample input embeddings, avoiding expensive fine-tuning. Through\nempirical studies on various real-world datasets, we demonstrate advantages of\nInfICL compared to state-of-the-art baselines.", "published": "2024-02-19 00:39:31", "link": "http://arxiv.org/abs/2402.11750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Chain-of-Thought Prompting for Few-Shot Generation of\n  Content-Grounded QA Conversations", "abstract": "We introduce a structured chain-of-thought (SCoT) prompting approach to\ngenerating content-grounded multi-turn question-answer conversations using a\npre-trained large language model (LLM). At the core of our proposal is a\nstructured breakdown of the complex task into a number of states in a state\nmachine, so that actions corresponding to various subtasks, e.g., content\nreading and utterance generation, can be executed in their own dedicated\nstates. Each state leverages a unique set of resources including prompts and\n(optionally) additional tools to augment the generation process. Our\nexperimental results show that SCoT prompting with designated states for\nhallucination mitigation increases agent faithfulness to grounding documents by\nup to 16.8%. When used as training data, our open-domain conversations\nsynthesized from only 6 Wikipedia-based seed demonstrations train strong\nconversational QA agents; in out-of-domain evaluation, for example, we observe\nimprovements of up to 13.9% over target domain gold data when the latter is\naugmented with our generated examples.", "published": "2024-02-19 01:49:53", "link": "http://arxiv.org/abs/2402.11770v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema", "abstract": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.", "published": "2024-02-19 03:56:44", "link": "http://arxiv.org/abs/2402.11811v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Head-wise Shareable Attention for Large Language Models", "abstract": "Large Language Models (LLMs) suffer from huge number of parameters, which\nrestricts their deployment on edge devices. Weight sharing is one promising\nsolution that encourages weight reuse, effectively reducing memory usage with\nless performance drop. However, current weight sharing techniques primarily\nfocus on small-scale models like BERT and employ coarse-grained sharing rules,\ne.g., layer-wise. This becomes limiting given the prevalence of LLMs and\nsharing an entire layer or block obviously diminishes the flexibility of weight\nsharing. In this paper, we present a perspective on head-wise shareable\nattention for large language models. We further propose two memory-efficient\nmethods that share parameters across attention heads, with a specific focus on\nLLMs. Both of them use the same dynamic strategy to select the shared weight\nmatrices. The first method directly reuses the pre-trained weights without\nretraining, denoted as $\\textbf{DirectShare}$. The second method first\npost-trains with constraint on weight matrix similarity and then shares,\ndenoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise\nshared models still maintain satisfactory capabilities, demonstrating the\nfeasibility of fine-grained weight sharing applied to LLMs.", "published": "2024-02-19 04:19:36", "link": "http://arxiv.org/abs/2402.11819v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Interpretable are Reasoning Explanations from Prompting Large\n  Language Models?", "abstract": "Prompt Engineering has garnered significant attention for enhancing the\nperformance of large language models across a multitude of tasks. Techniques\nsuch as the Chain-of-Thought not only bolster task performance but also\ndelineate a clear trajectory of reasoning steps, offering a tangible form of\nexplanation for the audience. Prior works on interpretability assess the\nreasoning chains yielded by Chain-of-Thought solely along a singular axis,\nnamely faithfulness. We present a comprehensive and multifaceted evaluation of\ninterpretability, examining not only faithfulness but also robustness and\nutility across multiple commonsense reasoning benchmarks. Likewise, our\ninvestigation is not confined to a single prompting technique; it expansively\ncovers a multitude of prevalent prompting techniques employed in large language\nmodels, thereby ensuring a wide-ranging and exhaustive evaluation. In addition,\nwe introduce a simple interpretability alignment technique, termed\nSelf-Entailment-Alignment Chain-of-thought, that yields more than 70\\%\nimprovements across multiple dimensions of interpretability. Code is available\nat https://github.com/SenticNet/CoT_interpretability", "published": "2024-02-19 06:11:28", "link": "http://arxiv.org/abs/2402.11863v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced\n  Video-grounded Dialogue Generation", "abstract": "Video-grounded dialogue generation (VDG) requires the system to generate a\nfluent and accurate answer based on multimodal knowledge. However, the\ndifficulty in multimodal knowledge utilization brings serious hallucinations to\nVDG models in practice. Although previous works mitigate the hallucination in a\nvariety of ways, they hardly take notice of the importance of the multimodal\nknowledge anchor answer tokens. In this paper, we reveal via perplexity that\ndifferent VDG models experience varying hallucinations and exhibit diverse\nanchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive\nmultimodal knowledge anchor enhancement framework for hallucination reduction.\nFurthermore, we introduce the counterfactual effect for more accurate anchor\ntoken detection. The experimental results on three popular benchmarks exhibit\nthe superiority of our approach over state-of-the-art methods, demonstrating\nits effectiveness in reducing hallucinations.", "published": "2024-02-19 06:32:39", "link": "http://arxiv.org/abs/2402.11875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large\n  Language Models with Reverse Prompt Contrastive Decoding", "abstract": "With the development of instruction-tuned large language models (LLMs),\nimproving the safety of LLMs has become more critical. However, the current\napproaches for aligning the LLMs output with expected safety usually require\nsubstantial training efforts, e.g., high-quality safety data and expensive\ncomputational resources, which are costly and inefficient. To this end, we\npresent reverse prompt contrastive decoding (ROSE), a simple-yet-effective\nmethod to directly boost the safety of existing instruction-tuned LLMs without\nany additional training. The principle of ROSE is to improve the probability of\ndesired safe output via suppressing the undesired output induced by the\ncarefully-designed reverse prompts. Experiments on 6 safety and 2\ngeneral-purpose tasks show that, our ROSE not only brings consistent and\nsignificant safety improvements (up to +13.8% safety score) upon 5 types of\ninstruction-tuned LLMs, but also benefits the general-purpose ability of LLMs.\nIn-depth analyses explore the underlying mechanism of ROSE, and reveal when and\nwhere to use it.", "published": "2024-02-19 06:58:42", "link": "http://arxiv.org/abs/2402.11889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Knowledge Distillation for Autoregressive Language Models", "abstract": "Knowledge distillation (KD) is a common approach to compress a teacher model\nto reduce its inference cost and memory footprint, by training a smaller\nstudent model. However, in the context of autoregressive language models (LMs),\nwe empirically find that larger teacher LMs might dramatically result in a\npoorer student. In response to this problem, we conduct a series of analyses\nand reveal that different tokens have different teaching modes, neglecting\nwhich will lead to performance degradation. Motivated by this, we propose a\nsimple yet effective adaptive teaching approach (ATKD) to improve the KD. The\ncore of ATKD is to reduce rote learning and make teaching more diverse and\nflexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD,\nvarious baseline KD methods can achieve consistent and significant performance\ngains (up to +3.04% average score) across all model types and sizes. More\nencouragingly, ATKD can improve the student model generalization effectively.", "published": "2024-02-19 07:01:10", "link": "http://arxiv.org/abs/2402.11890v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Dataset Updates Towards Reliable and Timely Evaluation of\n  Large Language Models", "abstract": "Large language models (LLMs) have achieved impressive performance across\nvarious natural language benchmarks, prompting a continual need to curate more\ndifficult datasets for larger LLMs, which is costly and time-consuming. In this\npaper, we propose to automate dataset updating and provide systematic analysis\nregarding its effectiveness in dealing with benchmark leakage issue, difficulty\ncontrol, and stability. Thus, once the current benchmark has been mastered or\nleaked, we can update it for timely and reliable evaluation. There are two\nupdating strategies: 1) mimicking strategy to generate similar samples based on\noriginal data, preserving stylistic and contextual essence, and 2) extending\nstrategy that further expands existing samples at varying cognitive levels by\nadapting Bloom's taxonomy of educational objectives. Extensive experiments on\nupdated MMLU and BIG-Bench demonstrate the stability of the proposed strategies\nand find that the mimicking strategy can effectively alleviate issues of\noverestimation from benchmark leakage. In cases where the efficient mimicking\nstrategy fails, our extending strategy still shows promising results.\nAdditionally, by controlling the difficulty, we can better discern the models'\nperformance and enable fine-grained analysis neither too difficult nor too easy\nan exam can fairly judge students' learning status. To the best of our\nknowledge, we are the first to automate updating benchmarks for reliable and\ntimely evaluation. Our demo leaderboard can be found at\nhttps://yingjiahao14.github.io/Automating-DatasetUpdates/.", "published": "2024-02-19 07:15:59", "link": "http://arxiv.org/abs/2402.11894v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning", "abstract": "Fine-tuning all parameters of large language models (LLMs) necessitates\nsubstantial computational power and extended time. Latest advancements in\nparameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and\nLoRA, allow for adjustments to only a minor fraction of the parameters of these\nLLMs. Concurrently, it has been noted that the issue of over-smoothing\ndiminishes the effectiveness of these Transformer-based LLMs, resulting in\nsuboptimal performances in downstream tasks. In this paper, we present SIBO,\nwhich is a SImple BOoster to enhance PEFT, by injecting an initial residual.\nSIBO is straightforward and readily extensible to a range of state-of-the-art\nPEFT techniques to alleviate over-smoothing and enhance performance. Extensive\nexperiments on 22 benchmark datasets demonstrate that SIBO significantly\nenhances the performance of various strong baselines, achieving up to 15.7% and\n23.5% improvement over existing PEFT methods on the arithmetic and commonsense\nreasoning tasks, respectively.", "published": "2024-02-19 07:22:29", "link": "http://arxiv.org/abs/2402.11896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large\n  Language Models", "abstract": "Recent work has showcased the powerful capability of large language models\n(LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs\nin combining these two capabilities into reasoning through multi-hop facts has\nnot been widely explored. This paper systematically investigates the\npossibilities for LLMs to utilize shortcuts based on direct connections between\nthe initial and terminal entities of multi-hop knowledge. We first explore the\nexistence of factual shortcuts through Knowledge Neurons, revealing that: (i)\nthe strength of factual shortcuts is highly correlated with the frequency of\nco-occurrence of initial and terminal entities in the pre-training corpora;\n(ii) few-shot prompting leverage more shortcuts in answering multi-hop\nquestions compared to chain-of-thought prompting. Then, we analyze the risks\nposed by factual shortcuts from the perspective of multi-hop knowledge editing.\nAnalysis shows that approximately 20% of the failures are attributed to\nshortcuts, and the initial and terminal entities in these failure instances\nusually have higher co-occurrences in the pre-training corpus. Finally, we\npropose erasing shortcut neurons to mitigate the associated risks and find that\nthis approach significantly reduces failures in multiple-hop knowledge editing\ncaused by shortcuts.", "published": "2024-02-19 07:34:10", "link": "http://arxiv.org/abs/2402.11900v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Edit: Aligning LLMs with Knowledge Editing", "abstract": "Knowledge editing techniques, aiming to efficiently modify a minor proportion\nof knowledge in large language models (LLMs) without negatively impacting\nperformance across other inputs, have garnered widespread attention. However,\nexisting methods predominantly rely on memorizing the updated knowledge,\nimpeding LLMs from effectively combining the new knowledge with their inherent\nknowledge when answering questions. To this end, we propose a Learning to Edit\n(LTE) framework, focusing on teaching LLMs to apply updated knowledge into\ninput questions, inspired by the philosophy of \"Teach a man to fish.\" LTE\nfeatures a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on\na meticulously curated parallel dataset to make reliable, in-scope edits while\npreserving out-of-scope information and linguistic proficiency; and (ii) the\nInference Phase, which employs a retrieval-based mechanism for real-time and\nmass knowledge editing. By comparing our approach with seven advanced baselines\nacross four popular knowledge editing benchmarks and two LLM architectures, we\ndemonstrate LTE's superiority in knowledge editing performance, robustness in\nboth batch and sequential editing, minimal interference on general tasks, and\nrapid editing speeds. The data and code are available at\nhttps://github.com/YJiangcm/LTE.", "published": "2024-02-19 07:45:17", "link": "http://arxiv.org/abs/2402.11905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive\n  Prompt Distillation", "abstract": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data.", "published": "2024-02-19 07:46:40", "link": "http://arxiv.org/abs/2402.11907v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark", "abstract": "While Large Language Models (LLMs) excel in question-answering (QA) tasks,\ntheir real reasoning abilities on multiple evidence retrieval and integration\non Multi-hop QA tasks remain less explored. Firstly, LLMs sometimes generate\nanswers that rely on internal memory rather than retrieving evidence and\nreasoning in the given context, which brings concerns about the evaluation\nquality of real reasoning abilities. Although previous counterfactual QA\nbenchmarks can separate the internal memory of LLMs, they focus solely on final\nQA performance, which is insufficient for reporting LLMs' real reasoning\nabilities. Because LLMs are expected to engage in intricate reasoning processes\nthat involve evidence retrieval and answering a series of sub-questions from\ngiven passages. Moreover, current factual Multi-hop QA (MHQA) benchmarks are\nannotated on open-source corpora such as Wikipedia, although useful for\nmulti-step reasoning evaluation, they show limitations due to the potential\ndata contamination in LLMs' pre-training stage. To address these issues, we\nintroduce a Step-wise Counterfactual benchmark (CofCA), a novel evaluation\nbenchmark consisting of factual data and counterfactual data that reveals LLMs'\nreal reasoning abilities on multi-step reasoning and reasoning chain\nevaluation. Our experimental results reveal a significant performance gap of\nseveral LLMs between Wikipedia-based factual data and counterfactual data,\ndeeming data contamination issues in existing benchmarks. Moreover, we observe\nthat LLMs usually bypass the correct reasoning chain, showing an inflated\nmulti-step reasoning performance. We believe that our CofCA benchmark will\nenhance and facilitate the evaluations of trustworthy LLMs.", "published": "2024-02-19 08:12:30", "link": "http://arxiv.org/abs/2402.11924v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI\n  Automation", "abstract": "Multimodal large language models (MLLMs) have shown remarkable potential as\nhuman-like autonomous language agents to interact with real-world environments,\nespecially for graphical user interface (GUI) automation. However, those GUI\nagents require comprehensive cognition ability including exhaustive perception\nand reliable action response. We propose a Comprehensive Cognitive LLM Agent,\nCoCo-Agent, with two novel approaches, comprehensive environment perception\n(CEP) and conditional action prediction (CAP), to systematically improve the\nGUI automation performance. First, CEP facilitates the GUI perception through\ndifferent aspects and granularity, including screenshots and complementary\ndetailed layouts for the visual channel and historical actions for the textual\nchannel. Second, CAP decomposes the action prediction into sub-problems: action\ntype prediction and action target conditioned on the action type. With our\ntechnical design, our agent achieves new state-of-the-art performance on AITW\nand META-GUI benchmarks, showing promising abilities in realistic scenarios.\nCode is available at https://github.com/xbmxb/CoCo-Agent.", "published": "2024-02-19 08:29:03", "link": "http://arxiv.org/abs/2402.11941v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with\n  External Knowledge Augmentation", "abstract": "The rise of multimodal misinformation on social platforms poses significant\nchallenges for individuals and societies. Its increased credibility and broader\nimpact compared to textual misinformation make detection complex, requiring\nrobust reasoning across diverse media types and profound knowledge for accurate\nverification. The emergence of Large Vision Language Model (LVLM) offers a\npotential solution to this problem. Leveraging their proficiency in processing\nvisual and textual information, LVLM demonstrates promising capabilities in\nrecognizing complex information and exhibiting strong reasoning skills. In this\npaper, we first investigate the potential of LVLM on multimodal misinformation\ndetection. We find that even though LVLM has a superior performance compared to\nLLMs, its profound reasoning may present limited power with a lack of evidence.\nBased on these observations, we propose LEMMA: LVLM-Enhanced Multimodal\nMisinformation Detection with External Knowledge Augmentation. LEMMA leverages\nLVLM intuition and reasoning capabilities while augmenting them with external\nknowledge to enhance the accuracy of misinformation detection. Our method\nimproves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and\nFakeddit datasets respectively.", "published": "2024-02-19 08:32:27", "link": "http://arxiv.org/abs/2402.11943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Therapeutic Relationship between Counselors and\n  Clients in Online Text-based Counseling using LLMs", "abstract": "Robust therapeutic relationships between counselors and clients are\nfundamental to counseling effectiveness. The assessment of therapeutic alliance\nis well-established in traditional face-to-face therapy but may not directly\ntranslate to text-based settings. With millions of individuals seeking support\nthrough online text-based counseling, understanding the relationship in such\ncontexts is crucial.\n  In this paper, we present an automatic approach using large language models\n(LLMs) to understand the development of therapeutic alliance in text-based\ncounseling. We adapt a theoretically grounded framework specifically to the\ncontext of online text-based counseling and develop comprehensive guidelines\nfor characterizing the alliance. We collect a comprehensive counseling dataset\nand conduct multiple expert evaluations on a subset based on this framework.\nOur LLM-based approach, combined with guidelines and simultaneous extraction of\nsupportive evidence underlying its predictions, demonstrates effectiveness in\nidentifying the therapeutic alliance. Through further LLM-based evaluations on\nadditional conversations, our findings underscore the challenges counselors\nface in cultivating strong online relationships with clients. Furthermore, we\ndemonstrate the potential of LLM-based feedback mechanisms to enhance\ncounselors' ability to build relationships, supported by a small-scale\nproof-of-concept.", "published": "2024-02-19 09:00:10", "link": "http://arxiv.org/abs/2402.11958v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Do Dialect Speakers Want? A Survey of Attitudes Towards Language\n  Technology for German Dialects", "abstract": "Natural language processing (NLP) has largely focused on modelling\nstandardized languages. More recently, attention has increasingly shifted to\nlocal, non-standardized languages and dialects. However, the relevant speaker\npopulations' needs and wishes with respect to NLP tools are largely unknown. In\nthis paper, we focus on dialects and regional languages related to German -- a\ngroup of varieties that is heterogeneous in terms of prestige and\nstandardization. We survey speakers of these varieties (N=327) and present\ntheir opinions on hypothetical language technologies for their dialects.\nAlthough attitudes vary among subgroups of our respondents, we find that\nrespondents are especially in favour of potential NLP tools that work with\ndialectal input (especially audio input) such as virtual assistants, and less\nso for applications that produce dialectal output such as machine translation\nor spellcheckers.", "published": "2024-02-19 09:15:28", "link": "http://arxiv.org/abs/2402.11968v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compress to Impress: Unleashing the Potential of Compressive Memory in\n  Real-World Long-Term Conversations", "abstract": "Existing retrieval-based methods have made significant strides in maintaining\nlong-term conversations. However, these approaches face challenges in memory\ndatabase management and accurate memory retrieval, hindering their efficacy in\ndynamic, real-world interactions. This study introduces a novel framework,\nCOmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews\ntraditional retrieval modules and memory databases. Instead, COMEDY adopts a\n\"One-for-All\" approach, utilizing a single language model to manage memory\ngeneration, compression, and response generation. Central to this framework is\nthe concept of compressive memory, which intergrates session-specific\nsummaries, user-bot dynamics, and past events into a concise memory format. To\nsupport COMEDY, we curated a large-scale Chinese instruction-tuning dataset,\nDolphin, derived from real user-chatbot interactions. Comparative evaluations\ndemonstrate COMEDY's superiority over traditional retrieval-based methods in\nproducing more nuanced and human-like conversational experiences. Our codes are\navailable at https://github.com/nuochenpku/COMEDY.", "published": "2024-02-19 09:19:50", "link": "http://arxiv.org/abs/2402.11975v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Comparison of Contextualized Word Embeddings for Lexical\n  Semantic Change", "abstract": "Contextualized embeddings are the preferred tool for modeling Lexical\nSemantic Change (LSC). Current evaluations typically focus on a specific task\nknown as Graded Change Detection (GCD). However, performance comparison across\nwork are often misleading due to their reliance on diverse settings. In this\npaper, we evaluate state-of-the-art models and approaches for GCD under equal\nconditions. We further break the LSC problem into Word-in-Context (WiC) and\nWord Sense Induction (WSI) tasks, and compare models across these different\nlevels. Our evaluation is performed across different languages on eight\navailable benchmarks for LSC, and shows that (i) APD outperforms other\napproaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for\nWiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need\nfor improving the modeling of word meanings, as well as focus on how, when, and\nwhy these meanings change, rather than solely focusing on the extent of\nsemantic change.", "published": "2024-02-19 10:04:59", "link": "http://arxiv.org/abs/2402.12011v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?", "abstract": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.", "published": "2024-02-19 10:34:13", "link": "http://arxiv.org/abs/2402.12025v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation\n  Loss for LLMs", "abstract": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques.", "published": "2024-02-19 10:37:29", "link": "http://arxiv.org/abs/2402.12030v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Adaptation to Specialized Domains through Selective\n  Masking based on Genre and Topical Characteristics", "abstract": "Recent advances in pre-trained language modeling have facilitated significant\nprogress across various natural language processing (NLP) tasks. Word masking\nduring model training constitutes a pivotal component of language modeling in\narchitectures like BERT. However, the prevalent method of word masking relies\non random selection, potentially disregarding domain-specific linguistic\nattributes. In this article, we introduce an innovative masking approach\nleveraging genre and topicality information to tailor language models to\nspecialized domains. Our method incorporates a ranking process that prioritizes\nwords based on their significance, subsequently guiding the masking procedure.\nExperiments conducted using continual pre-training within the legal domain have\nunderscored the efficacy of our approach on the LegalGLUE benchmark in the\nEnglish language. Pre-trained language models and code are freely available for\nuse.", "published": "2024-02-19 10:43:27", "link": "http://arxiv.org/abs/2402.12036v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large\n  Language Models", "abstract": "Catastrophic forgetting emerges as a critical challenge when fine-tuning\nmulti-modal large language models (MLLMs), where improving performance on\nunseen tasks often leads to a significant performance drop on the original\ntasks. This paper presents a comprehensive analysis of catastrophic forgetting\nin MLLMs and introduces a post-training adjustment method called Model Tailor.\nOur method primarily preserves the pre-trained parameters while replacing a\nsmall number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\%\neffectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\%\non new tasks compared to standard fine-tuning. Specifically, we derive a sparse\nmask to identify the \"model patch\", based on a fusion strategy that integrates\nsalience and sensitivity analysis. Subsequently, a compensation mechanism is\nintroduced to \"decorate the patch\", enhancing the model's performance on both\ntarget and original tasks. Additionally, our method is adaptable to multi-task\nscenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both\nimage captioning and visual question answering tasks, our approach demonstrates\nsignificant task adaptability while preserving inherent pre-trained\ncapabilities.", "published": "2024-02-19 11:02:05", "link": "http://arxiv.org/abs/2402.12048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When\n  and What to Retrieve for LLMs", "abstract": "The integration of large language models (LLMs) and search engines represents\na significant evolution in knowledge acquisition methodologies. However,\ndetermining the knowledge that an LLM already possesses and the knowledge that\nrequires the help of a search engine remains an unresolved issue. Most existing\nmethods solve this problem through the results of preliminary answers or\nreasoning done by the LLM itself, but this incurs excessively high\ncomputational costs. This paper introduces a novel collaborative approach,\nnamely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model,\nto enhance the LLM's knowledge acquisition process. We employ a proxy model\nwhich has far fewer parameters, and take its answers as heuristic answers.\nHeuristic answers are then utilized to predict the knowledge required to answer\nthe user question, as well as the known and unknown knowledge within the LLM.\nWe only conduct retrieval for the missing knowledge in questions that the LLM\ndoes not know. Extensive experimental results on five datasets with two LLMs\ndemonstrate a notable improvement in the end-to-end performance of LLMs in\nquestion-answering tasks, achieving or surpassing current state-of-the-art\nmodels with lower LLM inference costs.", "published": "2024-02-19 11:11:08", "link": "http://arxiv.org/abs/2402.12052v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLM-based Evaluators Confusing NLG Quality Criteria?", "abstract": "Some prior work has shown that LLMs perform well in NLG evaluation for\ndifferent tasks. However, we discover that LLMs seem to confuse different\nevaluation criteria, which reduces their reliability. For further verification,\nwe first consider avoiding issues of inconsistent conceptualization and vague\nexpression in existing NLG quality criteria themselves. So we summarize a clear\nhierarchical classification system for 11 common aspects with corresponding\ndifferent criteria from previous studies involved. Inspired by behavioral\ntesting, we elaborately design 18 types of aspect-targeted perturbation attacks\nfor fine-grained analysis of the evaluation behaviors of different LLMs. We\nalso conduct human annotations beyond the guidance of the classification system\nto validate the impact of the perturbations. Our experimental results reveal\nconfusion issues inherent in LLMs, as well as other noteworthy phenomena, and\nnecessitate further research and improvements for LLM-based evaluation.", "published": "2024-02-19 11:19:02", "link": "http://arxiv.org/abs/2402.12055v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Compute with Reasons?", "abstract": "Large language models (LLMs) often struggle with complex mathematical tasks,\nprone to \"hallucinating\" incorrect answers due to their reliance on statistical\npatterns. This limitation is further amplified in average Small LangSLMs with\nlimited context and training data. To address this challenge, we propose an\n\"Inductive Learning\" approach utilizing a distributed network of SLMs. This\nnetwork leverages error-based learning and hint incorporation to refine the\nreasoning capabilities of SLMs. Our goal is to provide a framework that\nempowers SLMs to approach the level of logic-based applications achieved by\nhigh-parameter models, potentially benefiting any language model. Ultimately,\nthis novel concept paves the way for bridging the logical gap between humans\nand LLMs across various fields.", "published": "2024-02-19 12:04:25", "link": "http://arxiv.org/abs/2402.12080v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BIDER: Bridging Knowledge Inconsistency for Efficient\n  Retrieval-Augmented LLMs via Key Supporting Evidence", "abstract": "Retrieval-augmented large language models (LLMs) have demonstrated efficacy\nin knowledge-intensive tasks such as open-domain QA, addressing inherent\nchallenges in knowledge update and factual inadequacy. However, inconsistencies\nbetween retrieval knowledge and the necessary knowledge for LLMs, leading to a\ndecline in LLM's answer quality. This paper introduces BIDER, an approach that\nrefines retrieval documents into Key Supporting Evidence (KSE) through\nknowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We\ntrain BIDER by learning from crafting KSE, while maximizing its output to align\nwith LLM's information acquisition preferences through reinforcement learning.\nEvaluations across five datasets show BIDER boosts LLMs' answer quality by 7%\nwhile reducing input content length in retrieval documents by 80%,\noutperforming existing methods. The proposed KSE simulation effectively equips\nLLMs with essential information for accurate question answering.", "published": "2024-02-19 14:28:31", "link": "http://arxiv.org/abs/2402.12174v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models", "abstract": "Many studies have demonstrated that large language models (LLMs) can produce\nharmful responses, exposing users to unexpected risks when LLMs are deployed.\nPrevious studies have proposed comprehensive taxonomies of the risks posed by\nLLMs, as well as corresponding prompts that can be used to examine the safety\nmechanisms of LLMs. However, the focus has been almost exclusively on English,\nand little has been explored for other languages. Here we aim to bridge this\ngap. We first introduce a dataset for the safety evaluation of Chinese LLMs,\nand then extend it to two other scenarios that can be used to better identify\nfalse negative and false positive examples in terms of risky prompt rejections.\nWe further present a set of fine-grained safety assessment criteria for each\nrisk type, facilitating both manual annotation and automatic evaluation in\nterms of LLM response harmfulness. Our experiments on five LLMs show that\nregion-specific risks are the prevalent type of risk, presenting the major\nissue with all Chinese LLMs we experimented with. Our data is available at\nhttps://github.com/Libr-AI/do-not-answer. Warning: this paper contains example\ndata that may be offensive, harmful, or biased.", "published": "2024-02-19 14:56:18", "link": "http://arxiv.org/abs/2402.12193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Browse and Concentrate: Comprehending Multimodal Content via prior-LLM\n  Context Fusion", "abstract": "With the bloom of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs) that incorporate LLMs with pre-trained vision models have\nrecently demonstrated impressive performance across diverse vision-language\ntasks. However, they fall short to comprehend context involving multiple\nimages. A primary reason for this shortcoming is that the visual features for\neach images are encoded individually by frozen encoders before feeding into the\nLLM backbone, lacking awareness of other images and the multimodal\ninstructions. We term this issue as prior-LLM modality isolation and propose a\ntwo phase paradigm, browse-and-concentrate, to enable in-depth multimodal\ncontext fusion prior to feeding the features into LLMs. This paradigm initially\n\"browses\" through the inputs for essential insights, and then revisits the\ninputs to \"concentrate\" on crucial details, guided by these insights, to\nachieve a more comprehensive understanding of the multimodal inputs.\nAdditionally, we develop training strategies specifically to enhance the\nunderstanding of multi-image inputs. Our method markedly boosts the performance\non 7 multi-image scenarios, contributing to increments on average accuracy by\n2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs,\nrespectively.", "published": "2024-02-19 14:59:07", "link": "http://arxiv.org/abs/2402.12195v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Multilingual Capabilities of Large Language Models through\n  Self-Distillation from Resource-Rich Languages", "abstract": "While large language models (LLMs) have been pre-trained on multilingual\ncorpora, their performance still lags behind in most languages compared to a\nfew resource-rich languages. One common approach to mitigate this issue is to\ntranslate training data from resource-rich languages into other languages and\nthen continue training. However, using the data obtained solely relying on\ntranslation while ignoring the original capabilities of LLMs across languages\nis not always effective, which we show will limit the performance of\ncross-lingual knowledge transfer. In this work, we propose SDRRL, a method\nbased on Self-Distillation from Resource-Rich Languages that effectively\nimprove multilingual performance by leveraging the internal capabilities of\nLLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and\nSeaLLM) and source languages across various comprehension and generation tasks,\nexperimental results demonstrate that SDRRL can significantly enhance\nmultilingual capabilities while minimizing the impact on original performance\nin resource-rich languages.", "published": "2024-02-19 15:07:32", "link": "http://arxiv.org/abs/2402.12204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polarization of Autonomous Generative AI Agents Under Echo Chambers", "abstract": "Online social networks often create echo chambers where people only hear\nopinions reinforcing their beliefs. An echo chamber often generates\npolarization, leading to conflicts caused by people with radical opinions, such\nas the January 6, 2021, attack on the US Capitol. The echo chamber has been\nviewed as a human-specific problem, but this implicit assumption is becoming\nless reasonable as large language models, such as ChatGPT, acquire social\nabilities. In response to this situation, we investigated the potential for\npolarization to occur among a group of autonomous AI agents based on generative\nlanguage models in an echo chamber environment. We had AI agents discuss\nspecific topics and analyzed how the group's opinions changed as the discussion\nprogressed. As a result, we found that the group of agents based on ChatGPT\ntended to become polarized in echo chamber environments. The analysis of\nopinion transitions shows that this result is caused by ChatGPT's high prompt\nunderstanding ability to update its opinion by considering its own and\nsurrounding agents' opinions. We conducted additional experiments to\ninvestigate under what specific conditions AI agents tended to polarize. As a\nresult, we identified factors that strongly influence polarization, such as the\nagent's persona. These factors should be monitored to prevent the polarization\nof AI agents.", "published": "2024-02-19 15:14:15", "link": "http://arxiv.org/abs/2402.12212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study on Updating Key-Value Memories in Transformer\n  Feed-forward Layers", "abstract": "The feed-forward networks (FFNs) in transformers are recognized as a group of\nkey-value neural memories to restore abstract high-level knowledge. In this\nwork, we conduct an empirical ablation study on updating keys (the 1st layer in\nthe FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those\ntwo methods in various knowledge editing and fine-tuning tasks of large\nlanguage models to draw insights to understand FFNs further. Code is available\nat $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$.", "published": "2024-02-19 15:42:54", "link": "http://arxiv.org/abs/2402.12233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Oriented Dialogue with In-Context Learning", "abstract": "We describe a system for building task-oriented dialogue systems combining\nthe in-context learning abilities of large language models (LLMs) with the\ndeterministic execution of business logic. LLMs are used to translate between\nthe surface form of the conversation and a domain-specific language (DSL) which\nis used to progress the business logic. We compare our approach to the\nintent-based NLU approach predominantly used in industry today. Our experiments\nshow that developing chatbots with our system requires significantly less\neffort than established approaches, that these chatbots can successfully\nnavigate complex dialogues which are extremely challenging for NLU-based\nsystems, and that our system has desirable properties for scaling task-oriented\ndialogue systems to a large number of tasks. We make our implementation\navailable for use and further study.", "published": "2024-02-19 15:43:35", "link": "http://arxiv.org/abs/2402.12234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Effects of Noise in Text-to-SQL: An Examination of the\n  BIRD-Bench Benchmark", "abstract": "Text-to-SQL, which involves translating natural language into Structured\nQuery Language (SQL), is crucial for enabling broad access to structured\ndatabases without expert knowledge. However, designing models for such tasks is\nchallenging due to numerous factors, including the presence of 'noise,' such as\nambiguous questions and syntactical errors. This study provides an in-depth\nanalysis of the distribution and types of noise in the widely used BIRD-Bench\nbenchmark and the impact of noise on models. While BIRD-Bench was created to\nmodel dirty and noisy database values, it was not created to contain noise and\nerrors in the questions and gold queries. We found that noise in questions and\ngold queries are prevalent in the dataset, with varying amounts across domains,\nand with an uneven distribution between noise types. The presence of incorrect\ngold SQL queries, which then generate incorrect gold answers, has a significant\nimpact on the benchmark's reliability. Surprisingly, when evaluating models on\ncorrected SQL queries, zero-shot baselines surpassed the performance of\nstate-of-the-art prompting methods. We conclude that informative noise labels\nand reliable benchmarks are crucial to developing new Text-to-SQL methods that\ncan handle varying types of noise. All datasets, annotations, and code are\navailable at\nhttps://github.com/niklaswretblad/the-effects-of-noise-in-text-to-SQL.", "published": "2024-02-19 15:58:15", "link": "http://arxiv.org/abs/2402.12243v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Levenshtein Transformer's Decoder and Its Variants", "abstract": "Levenshtein transformer (LevT) is a non-autoregressive machine translation\nmodel with high decoding efficiency and comparable translation quality in terms\nof bleu score, due to its parallel decoding and iterative refinement procedure.\nAre there any deficiencies of its translations and what improvements could be\nmade? In this report, we focus on LevT's decoder and analyse the decoding\nresults length, subword generation, and deletion module's capability. We hope\nto identify weaknesses of the decoder for future improvements.\n  We also compare translations of the original LevT, knowledge-distilled LevT,\nLevT with translation memory, and the KD-LevT with translation memory to see\nhow KD and translation memory can help.", "published": "2024-02-19 16:05:28", "link": "http://arxiv.org/abs/2402.12249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in\n  Automatic Related Work Composition", "abstract": "Numerous AI-assisted scholarly applications have been developed to aid\ndifferent stages of the research process. We present an analysis of AI-assisted\nscholarly writing generated with ScholaCite, a tool we built that is designed\nfor organizing literature and composing Related Work sections for academic\npapers. Our evaluation method focuses on the analysis of citation graphs to\nassess the structural complexity and inter-connectedness of citations in texts\nand involves a three-way comparison between (1) original human-written texts,\n(2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find\nthat GPT-4 can generate reasonable coarse-grained citation groupings to support\nhuman users in brainstorming, but fails to perform detailed synthesis of\nrelated works without human intervention. We suggest that future writing\nassistant tools should not be used to draft text independently of the human\nauthor.", "published": "2024-02-19 16:14:04", "link": "http://arxiv.org/abs/2402.12255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms", "abstract": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.", "published": "2024-02-19 16:19:15", "link": "http://arxiv.org/abs/2402.12261v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-quality Data-to-Text Generation for Severely Under-Resourced\n  Languages with Out-of-the-box Large Language Models", "abstract": "The performance of NLP methods for severely under-resourced languages cannot\ncurrently hope to match the state of the art in NLP methods for well resourced\nlanguages. We explore the extent to which pretrained large language models\n(LLMs) can bridge this gap, via the example of data-to-text generation for\nIrish, Welsh, Breton and Maltese. We test LLMs on these under-resourced\nlanguages and English, in a range of scenarios. We find that LLMs easily set\nthe state of the art for the under-resourced languages by substantial margins,\nas measured by both automatic and human evaluations. For all our languages,\nhuman evaluation shows on-a-par performance with humans for our best systems,\nbut BLEU scores collapse compared to English, casting doubt on the metric's\nsuitability for evaluating non-task-specific systems. Overall, our results\ndemonstrate the great potential of LLMs to bridge the performance gap for\nunder-resourced languages.", "published": "2024-02-19 16:29:40", "link": "http://arxiv.org/abs/2402.12267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology Enhanced Claim Detection", "abstract": "We propose an ontology enhanced model for sentence based claim detection. We\nfused ontology embeddings from a knowledge base with BERT sentence embeddings\nto perform claim detection for the ClaimBuster and the NewsClaims datasets. Our\nontology enhanced approach showed the best results with these small-sized\nunbalanced datasets, compared to other statistical and neural machine learning\nmodels. The experiments demonstrate that adding domain specific features\n(either trained word embeddings or knowledge graph metadata) can improve\ntraditional ML methods. In addition, adding domain knowledge in the form of\nontology embeddings helps avoid the bias encountered in neural network based\nmodels, for example the pure BERT model bias towards larger classes in our\nsmall corpus.", "published": "2024-02-19 16:50:58", "link": "http://arxiv.org/abs/2402.12282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KARL: Knowledge-Aware Retrieval and Representations aid Retention and\n  Learning in Students", "abstract": "Flashcard schedulers rely on 1) student models to predict the flashcards a\nstudent knows; and 2) teaching policies to pick which cards to show next via\nthese predictions. Prior student models, however, just use study data like the\nstudent's past responses, ignoring the text on cards. We propose content-aware\nscheduling, the first schedulers exploiting flashcard content. To give the\nfirst evidence that such schedulers enhance student learning, we build KARL, a\nsimple but effective content-aware student model employing deep knowledge\ntracing (DKT), retrieval, and BERT to predict student recall. We train KARL by\ncollecting a new dataset of 123,143 study logs on diverse trivia questions.\nKARL bests existing student models in AUC and calibration error. To ensure our\nimproved predictions lead to better student learning, we create a novel\ndelta-based teaching policy to deploy KARL online. Based on 32 study paths from\n27 users, KARL improves learning efficiency over SOTA, showing KARL's strength\nand encouraging researchers to look beyond historical study data to fully\ncapture student abilities.", "published": "2024-02-19 17:05:29", "link": "http://arxiv.org/abs/2402.12291v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TILP: Differentiable Learning of Temporal Logical Rules on Knowledge\n  Graphs", "abstract": "Compared with static knowledge graphs, temporal knowledge graphs (tKG), which\ncan capture the evolution and change of information over time, are more\nrealistic and general. However, due to the complexity that the notion of time\nintroduces to the learning of the rules, an accurate graph reasoning, e.g.,\npredicting new links between entities, is still a difficult problem. In this\npaper, we propose TILP, a differentiable framework for temporal logical rules\nlearning. By designing a constrained random walk mechanism and the introduction\nof temporal operators, we ensure the efficiency of our model. We present\ntemporal features modeling in tKG, e.g., recurrence, temporal order, interval\nbetween pair of relations, and duration, and incorporate it into our learning\nprocess. We compare TILP with state-of-the-art methods on two benchmark\ndatasets. We show that our proposed framework can improve upon the performance\nof baseline methods while providing interpretable results. In particular, we\nconsider various scenarios in which training samples are limited, data is\nbiased, and the time range between training and inference are different. In all\nthese cases, TILP works much better than the state-of-the-art methods.", "published": "2024-02-19 17:30:44", "link": "http://arxiv.org/abs/2402.12309v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Triple-Encoders: Representations That Fire Together, Wire Together", "abstract": "Search-based dialog models typically re-encode the dialog history at every\nturn, incurring high cost. Curved Contrastive Learning, a representation\nlearning method that encodes relative distances between utterances into the\nembedding space via a bi-encoder, has recently shown promising results for\ndialog modeling at far superior efficiency. While high efficiency is achieved\nthrough independently encoding utterances, this ignores the importance of\ncontextualization. To overcome this issue, this study introduces\ntriple-encoders, which efficiently compute distributed utterance mixtures from\nthese independently encoded utterances through a novel hebbian inspired\nco-occurrence learning objective in a self-organizing manner, without using any\nweights, i.e., merely through local interactions. Empirically, we find that\ntriple-encoders lead to a substantial improvement over bi-encoders, and even to\nbetter zero-shot generalization than single-vector representation models\nwithout requiring re-encoding. Our code\n(https://github.com/UKPLab/acl2024-triple-encoders) and model\n(https://huggingface.co/UKPLab/triple-encoders-dailydialog) are publicly\navailable.", "published": "2024-02-19 18:06:02", "link": "http://arxiv.org/abs/2402.12332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent Word Order Universals from Cognitively-Motivated Language\n  Models", "abstract": "The world's languages exhibit certain so-called typological or implicational\nuniversals; for example, Subject-Object-Verb (SOV) languages typically use\npostpositions. Explaining the source of such biases is a key goal of\nlinguistics. We study word-order universals through a computational simulation\nwith language models (LMs). Our experiments show that typologically-typical\nword orders tend to have lower perplexity estimated by LMs with cognitively\nplausible biases: syntactic biases, specific parsing strategies, and memory\nlimitations. This suggests that the interplay of cognitive biases and\npredictability (perplexity) can explain many aspects of word-order universals.\nIt also showcases the advantage of cognitively-motivated LMs, typically\nemployed in cognitive modeling, in the simulation of language universals.", "published": "2024-02-19 18:49:57", "link": "http://arxiv.org/abs/2402.12363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A synthetic data approach for domain generalization of NLI models", "abstract": "Natural Language Inference (NLI) remains an important benchmark task for\nLLMs. NLI datasets are a springboard for transfer learning to other semantic\ntasks, and NLI models are standard tools for identifying the faithfulness of\nmodel-generated text. There are several large scale NLI datasets today, and\nmodels have improved greatly by hill-climbing on these collections. Yet their\nrealistic performance on out-of-distribution/domain data is less\nwell-understood. We explore the opportunity for synthetic high-quality datasets\nto adapt NLI models for zero-shot use in downstream applications across new and\nunseen text domains. We demonstrate a new approach for generating NLI data in\ndiverse domains and lengths, so far not covered by existing training sets. The\nresulting examples have meaningful premises, the hypotheses are formed in\ncreative ways rather than simple edits to a few premise tokens, and the labels\nhave high accuracy. We show that models trained on this data ($685$K synthetic\nexamples) have the best generalization to completely new downstream test\nsettings. On the TRUE benchmark, a T5-small model trained with our data\nimproves around $7\\%$ on average compared to training on the best alternative\ndataset. The improvements are more pronounced for smaller models, while still\nmeaningful on a T5 XXL model. We also demonstrate gains on test sets when\nin-domain training data is augmented with our domain-general synthetic data.", "published": "2024-02-19 18:55:16", "link": "http://arxiv.org/abs/2402.12368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HunFlair2 in a cross-corpus evaluation of biomedical named entity\n  recognition and normalization tools", "abstract": "With the exponential growth of the life science literature, biomedical text\nmining (BTM) has become an essential technology for accelerating the extraction\nof insights from publications. Identifying named entities (e.g., diseases,\ndrugs, or genes) in texts and their linkage to reference knowledge bases are\ncrucial steps in BTM pipelines to enable information aggregation from different\ndocuments. However, tools for these two steps are rarely applied in the same\ncontext in which they were developed. Instead, they are applied in the wild,\ni.e., on application-dependent text collections different from those used for\nthe tools' training, varying, e.g., in focus, genre, style, and text type. This\nraises the question of whether the reported performance of BTM tools can be\ntrusted for downstream applications. Here, we report on the results of a\ncarefully designed cross-corpus benchmark for named entity extraction, where\ntools were applied systematically to corpora not used during their training.\nBased on a survey of 28 published systems, we selected five for an in-depth\nanalysis on three publicly available corpora encompassing four different entity\ntypes. Comparison between tools results in a mixed picture and shows that, in a\ncross-corpus setting, the performance is significantly lower than the one\nreported in an in-corpus setting. HunFlair2 showed the best performance on\naverage, being closely followed by PubTator. Our results indicate that users of\nBTM tools should expect diminishing performances when applying them in the wild\ncompared to original publications and show that further research is necessary\nto make BTM tools more robust.", "published": "2024-02-19 18:58:18", "link": "http://arxiv.org/abs/2402.12372v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding", "abstract": "As the usage of large language models (LLMs) grows, performing efficient\ninference with these models becomes increasingly important. While speculative\ndecoding has recently emerged as a promising direction for speeding up\ninference, existing methods are limited in their ability to scale to larger\nspeculation budgets, and adapt to different hyperparameters and hardware. This\npaper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for\nspeculative decoding. To attain better scalability, Sequoia introduces a\ndynamic programming algorithm to find the optimal tree structure for the\nspeculated tokens. To achieve robust speculative performance, Sequoia uses a\nnovel sampling and verification method that outperforms prior work across\ndifferent decoding temperatures. Finally, Sequoia introduces a hardware-aware\ntree optimizer that maximizes speculative performance by automatically\nselecting the token tree size and depth for a given hardware platform.\nEvaluation shows that Sequoia improves the decoding speed of Llama2-7B,\nLlama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.73\\times$, and\n$2.27\\times$. For offloading setting on L40, Sequoia achieves as low as 0.56\ns/token for exact Llama2-70B inference latency, which is $9.96\\times$ on our\noptimized offloading system (5.6 s/token), $9.7\\times$ than\nDeepSpeed-Zero-Inference, $19.5\\times$ than Huggingface Accelerate.", "published": "2024-02-19 18:58:32", "link": "http://arxiv.org/abs/2402.12374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Fine-grained Distortions in Reports of Scientific Findings", "abstract": "Distorted science communication harms individuals and society as it can lead\nto unhealthy behavior change and decrease trust in scientific institutions.\nGiven the rapidly increasing volume of science communication in recent years, a\nfine-grained understanding of how findings from scientific publications are\nreported to the general public, and methods to detect distortions from the\noriginal work automatically, are crucial. Prior work focused on individual\naspects of distortions or worked with unpaired data. In this work, we make\nthree foundational contributions towards addressing this problem: (1)\nannotating 1,600 instances of scientific findings from academic papers paired\nwith corresponding findings as reported in news articles and tweets wrt. four\ncharacteristics: causality, certainty, generality and sensationalism; (2)\nestablishing baselines for automatically detecting these characteristics; and\n(3) analyzing the prevalence of changes in these characteristics in both\nhuman-annotated and large-scale unlabeled data. Our results show that\nscientific findings frequently undergo subtle distortions when reported. Tweets\ndistort findings more often than science news reports. Detecting fine-grained\ndistortions automatically poses a challenging task. In our experiments,\nfine-tuned task-specific models consistently outperform few-shot LLM prompting.", "published": "2024-02-19 19:00:01", "link": "http://arxiv.org/abs/2402.12431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions\n  Without the Question?", "abstract": "Multiple-choice question answering (MCQA) is often used to evaluate large\nlanguage models (LLMs). To see if MCQA assesses LLMs as intended, we probe if\nLLMs can perform MCQA with choices-only prompts, where models must select the\ncorrect answer only from the choices. In three MCQA datasets and four LLMs,\nthis prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy\ngain. To help explain this behavior, we conduct an in-depth, black-box analysis\non memorization, choice dynamics, and question inference. Our key findings are\nthreefold. First, we find no evidence that the choices-only accuracy stems from\nmemorization alone. Second, priors over individual choices do not fully explain\nchoices-only accuracy, hinting that LLMs use the group dynamics of choices.\nThird, LLMs have some ability to infer a relevant question from choices, and\nsurprisingly can sometimes even match the original question. Inferring the\noriginal question is an impressive reasoning strategy, but it cannot fully\nexplain the high choices-only accuracy of LLMs in MCQA. Thus, while LLMs are\nnot fully incapable of reasoning in MCQA, we still advocate for the use of\nstronger baselines in MCQA benchmarks, the design of robust MCQA datasets for\nfair evaluations, and further efforts to explain LLM decision-making.", "published": "2024-02-19 19:38:58", "link": "http://arxiv.org/abs/2402.12483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Pre-Trained Language Models Detect and Understand Semantic\n  Underspecification? Ask the DUST!", "abstract": "In everyday language use, speakers frequently utter and interpret sentences\nthat are semantically underspecified, namely, whose content is insufficient to\nfully convey their message or interpret them univocally. For example, to\ninterpret the underspecified sentence \"Don't spend too much\", which leaves\nimplicit what (not) to spend, additional linguistic context or outside\nknowledge is needed. In this work, we propose a novel Dataset of semantically\nUnderspecified Sentences grouped by Type (DUST) and use it to study whether\npre-trained language models (LMs) correctly identify and interpret\nunderspecified sentences. We find that newer LMs are reasonably able to\nidentify underspecified sentences when explicitly prompted. However,\ninterpreting them correctly is much harder for any LMs. Our experiments show\nthat when interpreting underspecified sentences, LMs exhibit little\nuncertainty, contrary to what theoretical accounts of underspecification would\npredict. Overall, our study reveals limitations in current models' processing\nof sentence semantics and highlights the importance of using naturalistic data\nand communicative scenarios when evaluating LMs' language capabilities.", "published": "2024-02-19 19:49:29", "link": "http://arxiv.org/abs/2402.12486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your Vision-Language Model Itself Is a Strong Filter: Towards\n  High-Quality Instruction Tuning with Data Selection", "abstract": "Data selection in instruction tuning emerges as a pivotal process for\nacquiring high-quality data and training instruction-following large language\nmodels (LLMs), but it is still a new and unexplored research area for\nvision-language models (VLMs). Existing data selection approaches on LLMs\neither rely on single unreliable scores, or use downstream tasks for selection,\nwhich is time-consuming and can lead to potential over-fitting on the chosen\nevaluation datasets. To address this challenge, we introduce a novel dataset\nselection method, Self-Filter, that utilizes the VLM itself as a filter. This\napproach is inspired by the observation that VLMs benefit from training with\nthe most challenging instructions. Self-Filter operates in two stages. In the\nfirst stage, we devise a scoring network to evaluate the difficulty of training\ninstructions, which is co-trained with the VLM. In the second stage, we use the\ntrained score net to measure the difficulty of each instruction, select the\nmost challenging samples, and penalize similar samples to encourage diversity.\nComprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can\nreach better results compared to full data settings with merely about 15%\nsamples, and can achieve superior performance against competitive baselines.", "published": "2024-02-19 20:08:48", "link": "http://arxiv.org/abs/2402.12501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, prompting a surge in their practical applications. However,\nconcerns have arisen regarding the trustworthiness of LLMs outputs,\nparticularly in closed-book question-answering tasks, where non-experts may\nstruggle to identify inaccuracies due to the absence of contextual or ground\ntruth information. This paper introduces TrustScore, a framework based on the\nconcept of Behavioral Consistency, which evaluates whether an LLMs response\naligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly\nintegrate with fact-checking methods, which assesses alignment with external\nknowledge sources. The experimental results show that TrustScore achieves\nstrong correlations with human judgments, surpassing existing reference-free\nmetrics, and achieving results on par with reference-based metrics.", "published": "2024-02-19 21:12:14", "link": "http://arxiv.org/abs/2402.12545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense\n  and Hypothetical Reasoning", "abstract": "We present Archer, a challenging bilingual text-to-SQL dataset specific to\ncomplex reasoning, including arithmetic, commonsense and hypothetical\nreasoning. It contains 1,042 English questions and 1,042 Chinese questions,\nalong with 521 unique SQL queries, covering 20 English databases across 20\ndomains. Notably, this dataset demonstrates a significantly higher level of\ncomplexity compared to existing publicly available datasets. Our evaluation\nshows that Archer challenges the capabilities of current state-of-the-art\nmodels, with a high-ranked model on the Spider leaderboard achieving only 6.73%\nexecution accuracy on Archer test set. Thus, Archer presents a significant\nchallenge for future research in this field.", "published": "2024-02-19 21:24:36", "link": "http://arxiv.org/abs/2402.12554v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a Fine Grained Entity Type Taxonomy Using LLMs", "abstract": "In this study, we investigate the potential of GPT-4 and its advanced\niteration, GPT-4 Turbo, in autonomously developing a detailed entity type\ntaxonomy. Our objective is to construct a comprehensive taxonomy, starting from\na broad classification of entity types - including objects, time, locations,\norganizations, events, actions, and subjects - similar to existing manually\ncurated taxonomies. This classification is then progressively refined through\niterative prompting techniques, leveraging GPT-4's internal knowledge base. The\nresult is an extensive taxonomy comprising over 5000 nuanced entity types,\nwhich demonstrates remarkable quality upon subjective evaluation.\n  We employed a straightforward yet effective prompting strategy, enabling the\ntaxonomy to be dynamically expanded. The practical applications of this\ndetailed taxonomy are diverse and significant. It facilitates the creation of\nnew, more intricate branches through pattern-based combinations and notably\nenhances information extraction tasks, such as relation extraction and event\nargument extraction. Our methodology not only introduces an innovative approach\nto taxonomy creation but also opens new avenues for applying such taxonomies in\nvarious computational linguistics and AI-related fields.", "published": "2024-02-19 21:32:19", "link": "http://arxiv.org/abs/2402.12557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Standardize: Aligning Language Models with Expert-Defined Standards for\n  Content Generation", "abstract": "Domain experts across engineering, healthcare, and education follow strict\nstandards for producing quality content such as technical manuals, medication\ninstructions, and children's reading materials. However, current works in\ncontrollable text generation have yet to explore using these standards as\nreferences for control. Towards this end, we introduce Standardize, a\nretrieval-style in-context learning-based framework to guide large language\nmodels to align with expert-defined standards. Focusing on English language\nstandards in the education domain as a use case, we consider the Common\nEuropean Framework of Reference for Languages (CEFR) and Common Core Standards\n(CCS) for the task of open-ended content generation. Our findings show that\nmodels can gain a 45% to 100% increase in precise accuracy across open and\ncommercial LLMs evaluated, demonstrating that the use of knowledge artifacts\nextracted from standards and integrating them in the generation process can\neffectively guide models to produce better standard-aligned content.", "published": "2024-02-19 23:18:18", "link": "http://arxiv.org/abs/2402.12593v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is a word?", "abstract": "In order to design strong paradigms for isolating lexical access and\nsemantics, we need to know what a word is. Surprisingly few linguists and\nphilosophers have a clear model of what a word is, even though words impact\nbasically every aspect of human life. Researchers that regularly publish\nacademic papers about language often rely on outdated, or inaccurate,\nassumptions about wordhood. This short pedagogical document outlines what the\nlexicon is most certainly not (though is often mistakenly taken to be), what it\nmight be (based on current good theories), and what some implications for\nexperimental design are.", "published": "2024-02-19 23:58:20", "link": "http://arxiv.org/abs/2402.12605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned\n  Language Models through Task Arithmetic", "abstract": "Aligned language models face a significant limitation as their fine-tuning\noften results in compromised safety. To tackle this, we propose a simple method\nRESTA that performs LLM safety realignment. RESTA stands for REstoring Safety\nthrough Task Arithmetic. At its core, it involves a simple arithmetic addition\nof a safety vector to the weights of the compromised model. We demonstrate the\neffectiveness of RESTA in both parameter-efficient and full fine-tuning,\ncovering a wide range of downstream tasks, including instruction following in\nChinese, English, and Hindi, as well as problem-solving capabilities in Code\nand Math. We also showcase the generalizability of RESTA on three existing\nsafety evaluation benchmarks and a multilingual benchmark dataset proposed as a\npart of this work, consisting of 550 harmful questions covering 11 categories,\neach with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of\nthe compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in\nparameter-efficient and full fine-tuning, respectively, while maintaining most\nof the model's performance on the task. We release the source codes at:\nhttps://github.com/declare-lab/resta.", "published": "2024-02-19 00:18:09", "link": "http://arxiv.org/abs/2402.11746v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "abstract": "Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs. Our code is available at\nhttps://github.com/uw-nsl/ArtPrompt.", "published": "2024-02-19 00:43:31", "link": "http://arxiv.org/abs/2402.11753v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in\n  Generative LLMs", "abstract": "Generative Large Language Models (LLMs) are widely utilized for their\nexcellence in various tasks. However, their tendency to produce inaccurate or\nmisleading outputs poses a potential risk, particularly in high-stakes\nenvironments. Therefore, estimating the correctness of generative LLM outputs\nis an important task for enhanced reliability. Uncertainty Estimation (UE) in\ngenerative LLMs is an evolving domain, where SOTA probability-based methods\ncommonly employ length-normalized scoring. In this work, we propose\nMeaning-Aware Response Scoring (MARS) as an alternative to length-normalized\nscoring for UE methods. MARS is a novel scoring function that considers the\nsemantic contribution of each token in the generated sequence in the context of\nthe question. We demonstrate that integrating MARS into UE methods results in a\nuniversal and significant improvement in UE performance. We conduct experiments\nusing three distinct closed-book question-answering datasets across five\npopular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical\nQA dataset. Code can be found https://github.com/Ybakman/LLM_Uncertainity.", "published": "2024-02-19 01:04:22", "link": "http://arxiv.org/abs/2402.11756v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Stemming: Promises, Pitfalls and Failures", "abstract": "Text stemming is a natural language processing technique that is used to\nreduce words to their base form, also known as the root form. The use of\nstemming in IR has been shown to often improve the effectiveness of\nkeyword-matching models such as BM25. However, traditional stemming methods,\nfocusing solely on individual terms, overlook the richness of contextual\ninformation. Recognizing this gap, in this paper, we investigate the promising\nidea of using large language models (LLMs) to stem words by leveraging its\ncapability of context understanding. With this respect, we identify three\navenues, each characterised by different trade-offs in terms of computational\ncost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a\ncollection, i.e., the set of unique words that appear in the collection\n(vocabulary stemming), (2) use LLMs to stem each document separately\n(contextual stemming), and (3) use LLMs to extract from each document entities\nthat should not be stemmed, then use vocabulary stemming to stem the rest of\nthe terms (entity-based contextual stemming). Through a series of empirical\nexperiments, we compare the use of LLMs for stemming with that of traditional\nlexical stemmers such as Porter and Krovetz for English text. We find that\nwhile vocabulary stemming and contextual stemming fail to achieve higher\neffectiveness than traditional stemmers, entity-based contextual stemming can\nachieve a higher effectiveness than using Porter stemmer alone, under specific\nconditions.", "published": "2024-02-19 01:11:44", "link": "http://arxiv.org/abs/2402.11757v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "What Evidence Do Language Models Find Convincing?", "abstract": "Retrieval-augmented language models are being increasingly tasked with\nsubjective, contentious, and conflicting queries such as \"is aspartame linked\nto cancer\". To resolve these ambiguous queries, one must search through a large\nrange of websites and consider \"which, if any, of this evidence do I find\nconvincing?\". In this work, we study how LLMs answer this question. In\nparticular, we construct ConflictingQA, a dataset that pairs controversial\nqueries with a series of real-world evidence documents that contain different\nfacts (e.g., quantitative results), argument styles (e.g., appeals to\nauthority), and answers (Yes or No). We use this dataset to perform sensitivity\nand counterfactual analyses to explore which text features most affect LLM\npredictions. Overall, we find that current models rely heavily on the relevance\nof a website to the query, while largely ignoring stylistic features that\nhumans find important such as whether a text contains scientific references or\nis written with a neutral tone. Taken together, these results highlight the\nimportance of RAG corpus quality (e.g., the need to filter misinformation), and\npossibly even a shift in how LLMs are trained to better align with human\njudgements.", "published": "2024-02-19 02:15:34", "link": "http://arxiv.org/abs/2402.11782v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling the Magic: Investigating Attention Distillation in\n  Retrieval-augmented Generation", "abstract": "Retrieval-augmented generation framework can address the limitations of large\nlanguage models by enabling real-time knowledge updates for more accurate\nanswers. An efficient way in the training phase of retrieval-augmented models\nis attention distillation, which uses attention scores as a supervision signal\ninstead of manually annotated query-document pairs. Despite its growing\npopularity, the detailed mechanisms behind the success of attention\ndistillation remain unexplored, particularly the specific patterns it leverages\nto benefit training. In this paper, we address this gap by conducting a\ncomprehensive review of attention distillation workflow and identifying key\nfactors influencing the learning quality of retrieval-augmented language\nmodels. We further propose indicators for optimizing models' training methods\nand avoiding ineffective training.", "published": "2024-02-19 02:48:44", "link": "http://arxiv.org/abs/2402.11794v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's\n  Preference in Conversational Search", "abstract": "Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM using this dataset to\nalign it with the retrievers' preferences as feedback. The resulting model\nachieves state-of-the-art performance on two recent conversational search\nbenchmarks, significantly outperforming existing baselines, including GPT-3.5.", "published": "2024-02-19 04:41:31", "link": "http://arxiv.org/abs/2402.11827v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Modularized Networks for Few-shot Hateful Meme Detection", "abstract": "In this paper, we address the challenge of detecting hateful memes in the\nlow-resource setting where only a few labeled examples are available. Our\napproach leverages the compositionality of Low-rank adaptation (LoRA), a widely\nused parameter-efficient tuning technique. We commence by fine-tuning large\nlanguage models (LLMs) with LoRA on selected tasks pertinent to hateful meme\ndetection, thereby generating a suite of LoRA modules. These modules are\ncapable of essential reasoning skills for hateful meme detection. We then use\nthe few available annotated samples to train a module composer, which assigns\nweights to the LoRA modules based on their relevance. The model's learnable\nparameters are directly proportional to the number of LoRA modules. This\nmodularized network, underpinned by LLMs and augmented with LoRA modules,\nexhibits enhanced generalization in the context of hateful meme detection. Our\nevaluation spans three datasets designed for hateful meme detection in a\nfew-shot learning context. The proposed method demonstrates superior\nperformance to traditional in-context learning, which is also more\ncomputationally intensive during inference.We then use the few available\nannotated samples to train a module composer, which assigns weights to the LoRA\nmodules based on their relevance. The model's learnable parameters are directly\nproportional to the number of LoRA modules. This modularized network,\nunderpinned by LLMs and augmented with LoRA modules, exhibits enhanced\ngeneralization in the context of hateful meme detection. Our evaluation spans\nthree datasets designed for hateful meme detection in a few-shot learning\ncontext. The proposed method demonstrates superior performance to traditional\nin-context learning, which is also more computationally intensive during\ninference.", "published": "2024-02-19 05:15:13", "link": "http://arxiv.org/abs/2402.11845v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional\n  Supporters for Queer Youth", "abstract": "Queer youth face increased mental health risks, such as depression, anxiety,\nand suicidal ideation. Hindered by negative stigma, they often avoid seeking\nhelp and rely on online resources, which may provide incompatible information.\nAlthough access to a supportive environment and reliable information is\ninvaluable, many queer youth worldwide have no access to such support. However,\nthis could soon change due to the rapid adoption of Large Language Models\n(LLMs) such as ChatGPT. This paper aims to comprehensively explore the\npotential of LLMs to revolutionize emotional support for queers. To this end,\nwe conduct a qualitative and quantitative analysis of LLM's interactions with\nqueer-related content. To evaluate response quality, we develop a novel\nten-question scale that is inspired by psychological standards and expert\ninput. We apply this scale to score several LLMs and human comments to posts\nwhere queer youth seek advice and share experiences. We find that LLM responses\nare supportive and inclusive, outscoring humans. However, they tend to be\ngeneric, not empathetic enough, and lack personalization, resulting in\nnonreliable and potentially harmful advice. We discuss these challenges,\ndemonstrate that a dedicated prompt can improve the performance, and propose a\nblueprint of an LLM-supporter that actively (but sensitively) seeks user\ncontext to provide personalized, empathetic, and reliable responses. Our\nannotated dataset is available for further research.", "published": "2024-02-19 06:54:55", "link": "http://arxiv.org/abs/2402.11886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FeB4RAG: Evaluating Federated Search in the Context of Retrieval\n  Augmented Generation", "abstract": "Federated search systems aggregate results from multiple search engines,\nselecting appropriate sources to enhance result quality and align with user\nintent. With the increasing uptake of Retrieval-Augmented Generation (RAG)\npipelines, federated search can play a pivotal role in sourcing relevant\ninformation across heterogeneous data sources to generate informed responses.\nHowever, existing datasets, such as those developed in the past TREC FedWeb\ntracks, predate the RAG paradigm shift and lack representation of modern\ninformation retrieval challenges. To bridge this gap, we present FeB4RAG, a\nnovel dataset specifically designed for federated search within RAG frameworks.\nThis dataset, derived from 16 sub-collections of the widely used \\beir\nbenchmarking collection, includes 790 information requests (akin to\nconversational queries) tailored for chatbot applications, along with top\nresults returned by each resource and associated LLM-derived relevance\njudgements. Additionally, to support the need for this collection, we\ndemonstrate the impact on response generation of a high quality federated\nsearch system for RAG compared to a naive approach to federated search. We do\nso by comparing answers generated through the RAG pipeline through a\nqualitative side-by-side comparison. Our collection fosters and supports the\ndevelopment and evaluation of new federated search methods, especially in the\ncontext of RAG pipelines.", "published": "2024-02-19 07:06:52", "link": "http://arxiv.org/abs/2402.11891v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DiLA: Enhancing LLM Tool Learning with Differential Logic Layer", "abstract": "Considering the challenges faced by large language models (LLMs) in logical\nreasoning and planning, prior efforts have sought to augment LLMs with access\nto external solvers. While progress has been made on simple reasoning problems,\nsolving classical constraint satisfaction problems, such as the Boolean\nSatisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains\ndifficult for off-the-shelf solvers due to their intricate expressions and\nexponential search spaces. In this paper, we propose a novel differential logic\nlayer-aided language modeling (DiLA) approach, where logical constraints are\nintegrated into the forward and backward passes of a network layer, to provide\nanother option for LLM tool learning. In DiLA, LLM aims to transform the\nlanguage description to logic constraints and identify initial solutions of the\nhighest quality, while the differential logic layer focuses on iteratively\nrefining the LLM-prompted solution. Leveraging the logic layer as a bridge,\nDiLA enhances the logical reasoning ability of LLMs on a range of reasoning\nproblems encoded by Boolean variables, guaranteeing the efficiency and\ncorrectness of the solution process. We evaluate the performance of DiLA on two\nclassic reasoning problems and empirically demonstrate its consistent\noutperformance against existing prompt-based and solver-aided approaches.", "published": "2024-02-19 07:38:57", "link": "http://arxiv.org/abs/2402.11903v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Textual Similarity Assessment in Chest X-ray Reports Using a\n  Domain-Specific Cosine-Based Metric", "abstract": "Medical language processing and deep learning techniques have emerged as\ncritical tools for improving healthcare, particularly in the analysis of\nmedical imaging and medical text data. These multimodal data fusion techniques\nhelp to improve the interpretation of medical imaging and lead to increased\ndiagnostic accuracy, informed clinical decisions, and improved patient\noutcomes. The success of these models relies on the ability to extract and\nconsolidate semantic information from clinical text. This paper addresses the\nneed for more robust methods to evaluate the semantic content of medical\nreports. Conventional natural language processing approaches and metrics are\ninitially designed for considering the semantic context in the natural language\ndomain and machine translation, often failing to capture the complex semantic\nmeanings inherent in medical content. In this study, we introduce a novel\napproach designed specifically for assessing the semantic similarity between\ngenerated medical reports and the ground truth. Our approach is validated,\ndemonstrating its efficiency in assessing domain-specific semantic similarity\nwithin medical contexts. By applying our metric to state-of-the-art Chest X-ray\nreport generation models, we obtain results that not only align with\nconventional metrics but also provide more contextually meaningful scores in\nthe considered medical domain.", "published": "2024-02-19 07:48:25", "link": "http://arxiv.org/abs/2402.11908v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual\n  and Multilingual Approaches for Detecting AI-generated Text", "abstract": "This paper presents the participation of team QUST in Task 8 SemEval 2024. We\nfirst performed data augmentation and cleaning on the dataset to enhance model\ntraining efficiency and accuracy. In the monolingual task, we evaluated\ntraditional deep-learning methods, multiscale positive-unlabeled framework\n(MPU), fine-tuning, adapters and ensemble methods. Then, we selected the\ntop-performing models based on their accuracy from the monolingual models and\nevaluated them in subtasks A and B. The final model construction employed a\nstacking ensemble that combined fine-tuning with MPU. Our system achieved 8th\n(scored 8th in terms of accuracy, officially ranked 13th) place in the official\ntest set in multilingual settings of subtask A. We release our system code\nat:https://github.com/warmth27/SemEval2024_QUST", "published": "2024-02-19 08:22:51", "link": "http://arxiv.org/abs/2402.11934v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysis of Multidomain Abstractive Summarization Using Salience\n  Allocation", "abstract": "This paper explores the realm of abstractive text summarization through the\nlens of the SEASON (Salience Allocation as Guidance for Abstractive\nSummarizatiON) technique, a model designed to enhance summarization by\nleveraging salience allocation techniques. The study evaluates SEASON's\nefficacy by comparing it with prominent models like BART, PEGASUS, and\nProphetNet, all fine-tuned for various text summarization tasks. The assessment\nis conducted using diverse datasets including CNN/Dailymail, SAMSum, and\nFinancial-news based Event-Driven Trading (EDT), with a specific focus on a\nfinancial dataset containing a substantial volume of news articles from\n2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as\nROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these\nmodels fine-tuned for generating abstractive summaries. The analysis of these\nmetrics offers a thorough insight into the strengths and weaknesses\ndemonstrated by each model in summarizing news dataset, dialogue dataset and\nfinancial text dataset. The results presented in this paper not only contribute\nto the evaluation of the SEASON model's effectiveness but also illuminate the\nintricacies of salience allocation techniques across various types of datasets.", "published": "2024-02-19 08:52:12", "link": "http://arxiv.org/abs/2402.11955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distilling Large Language Models for Text-Attributed Graph Learning", "abstract": "Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.", "published": "2024-02-19 10:31:53", "link": "http://arxiv.org/abs/2402.12022v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc\n  Explanations", "abstract": "Incorporating natural language rationales in the prompt and In-Context\nLearning (ICL) have led to a significant improvement of Large Language Models\n(LLMs) performance. However, generating high-quality rationales require\nhuman-annotation or the use of auxiliary proxy models. In this work, we propose\nSelf-AMPLIFY to automatically generate rationales from post hoc explanation\nmethods applied to Small Language Models (SLMs) to improve their own\nperformance. Self-AMPLIFY is a 3-step method that targets samples, generates\nrationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance\nis evaluated on four SLMs and five datasets requiring strong reasoning\nabilities. Self-AMPLIFY achieves good results against competitors, leading to\nstrong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc\nexplanation methods to autoregressive language models to generate rationales to\nimprove their own performance in a fully automated manner.", "published": "2024-02-19 10:47:09", "link": "http://arxiv.org/abs/2402.12038v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Citation Amnesia: On The Recency Bias of NLP and Other Academic Fields", "abstract": "This study examines the tendency to cite older work across 20 fields of study\nover 43 years (1980--2023). We put NLP's propensity to cite older work in the\ncontext of these 20 other fields to analyze whether NLP shows similar temporal\ncitation patterns to these other fields over time or whether differences can be\nobserved. Our analysis, based on a dataset of approximately 240 million papers,\nreveals a broader scientific trend: many fields have markedly declined in\nciting older works (e.g., psychology, computer science). We term this decline a\n'citation age recession', analogous to how economists define periods of reduced\neconomic activity. The trend is strongest in NLP and ML research (-12.8% and\n-5.5% in citation age from previous peaks). Our results suggest that citing\nmore recent works is not directly driven by the growth in publication rates\n(-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even\nwhen controlling for an increase in the volume of papers. Our findings raise\nquestions about the scientific community's engagement with past literature,\nparticularly for NLP, and the potential consequences of neglecting older but\nrelevant research. The data and a demo showcasing our results are publicly\navailable.", "published": "2024-02-19 10:59:29", "link": "http://arxiv.org/abs/2402.12046v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Scaffolding Coordinates to Promote Vision-Language Coordination in Large\n  Multi-Modal Models", "abstract": "State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated\nexceptional capabilities in vision-language tasks. Despite their advanced\nfunctionalities, the performances of LMMs are still limited in challenging\nscenarios that require complex reasoning with multiple levels of visual\ninformation. Existing prompting techniques for LMMs focus on either improving\ntextual reasoning or leveraging tools for image preprocessing, lacking a simple\nand general visual prompting scheme to promote vision-language coordination in\nLMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to\npromote vision-language coordination. Specifically, Scaffold overlays a dot\nmatrix within the image as visual information anchors and leverages\nmulti-dimensional coordinates as textual positional references. Extensive\nexperiments on a wide range of challenging vision-language tasks demonstrate\nthe superiority of Scaffold over GPT-4V with the textual CoT prompting. Our\ncode is released in https://github.com/leixy20/Scaffold.", "published": "2024-02-19 11:23:53", "link": "http://arxiv.org/abs/2402.12058v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data are\npublicly available at https://github.com/Sahandfer/EmoBench.", "published": "2024-02-19 11:48:09", "link": "http://arxiv.org/abs/2402.12071v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LVCHAT: Facilitating Long Video Comprehension", "abstract": "Enabling large language models (LLMs) to read videos is vital for multimodal\nLLMs. Existing works show promise on short videos whereas long video (longer\nthan e.g.~1 minute) comprehension remains challenging. The major problem lies\nin the over-compression of videos, i.e., the encoded video representations are\nnot enough to represent the whole video. To address this issue, we propose Long\nVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to\ndynamically adjust the number of embeddings in alignment with the duration of\nthe video to ensure long videos are not overly compressed into a few\nembeddings. To deal with long videos whose length is beyond videos seen during\ntraining, we propose Interleaved Frame Encoding (IFE), repeating positional\nembedding and interleaving multiple groups of videos to enable long video\ninput, avoiding performance degradation due to overly long videos. Experimental\nresults show that LVChat significantly outperforms existing methods by up to\n27\\% in accuracy on long-video QA datasets and long-video captioning\nbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.", "published": "2024-02-19 11:59:14", "link": "http://arxiv.org/abs/2402.12079v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Large Language Models Understand Logic or Just Mimick Context?", "abstract": "Over the past few years, the abilities of large language models (LLMs) have\nreceived extensive attention, which have performed exceptionally well in\ncomplicated scenarios such as logical reasoning and symbolic inference. A\nsignificant factor contributing to this progress is the benefit of in-context\nlearning and few-shot prompting. However, the reasons behind the success of\nsuch models using contextual reasoning have not been fully explored. Do LLMs\nhave understand logical rules to draw inferences, or do they ``guess'' the\nanswers by learning a type of probabilistic mapping through context? This paper\ninvestigates the reasoning capabilities of LLMs on two logical reasoning\ndatasets by using counterfactual methods to replace context text and modify\nlogical concepts. Based on our analysis, it is found that LLMs do not truly\nunderstand logical rules; rather, in-context learning has simply enhanced the\nlikelihood of these models arriving at the correct answers. If one alters\ncertain words in the context text or changes the concepts of logical terms, the\noutputs of LLMs can be significantly disrupted, leading to counter-intuitive\nresponses. This work provides critical insights into the limitations of LLMs,\nunderscoring the need for more robust mechanisms to ensure reliable logical\nreasoning in LLMs.", "published": "2024-02-19 12:12:35", "link": "http://arxiv.org/abs/2402.12091v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is It a Free Lunch for Removing Outliers during Pretraining?", "abstract": "With the growing size of large language models, the role of quantization\nbecomes increasingly significant. However, outliers present in weights or\nactivations notably influence the performance of quantized models. Recently,\n\\citet{qtransformer} introduced a novel softmax function aimed at pretraining\nmodels in an outlier-free manner, thereby enhancing their suitability for\nquantization. Interestingly, we observed that such an approach leads to\nperformance degradation in full precision. Building on this insight, we enhance\nthe method by ensuring its normalization is invariant to sequence length, a\ncrucial factor for bridging the gap between pretraining and fine-tuning.\nMoreover, this improved method also facilitates successful pretraining of\ncausal language models.", "published": "2024-02-19 12:45:52", "link": "http://arxiv.org/abs/2402.12102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over\n  Larger Language Models", "abstract": "In this paper, we explore the challenges associated with establishing an\nend-to-end fact-checking pipeline in a real-world context, covering over 90\nlanguages. Our real-world experimental benchmarks demonstrate that fine-tuning\nTransformer models specifically for fact-checking tasks, such as claim\ndetection and veracity prediction, provide superior performance over large\nlanguage models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b. However, we\nillustrate that LLMs excel in generative tasks such as question decomposition\nfor evidence retrieval. Through extensive evaluation, we show the efficacy of\nfine-tuned models for fact-checking in a multilingual setting and complex\nclaims that include numerical quantities.", "published": "2024-02-19 14:00:35", "link": "http://arxiv.org/abs/2402.12147v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Your Large Language Model is Secretly a Fairness Proponent and You\n  Should Prompt it Like One", "abstract": "The widespread adoption of large language models (LLMs) underscores the\nurgent need to ensure their fairness. However, LLMs frequently present dominant\nviewpoints while ignoring alternative perspectives from minority parties,\nresulting in potential biases. We hypothesize that these fairness-violating\nbehaviors occur because LLMs express their viewpoints using a human personality\nthat represents the majority of training data. In response to this, we validate\nthat prompting LLMs with specific roles can allow LLMs to express diverse\nviewpoints. Building on this insight and observation, we develop FairThinking,\na pipeline designed to automatically generate roles that enable LLMs to\narticulate diverse perspectives for fair expressions. To evaluate FairThinking,\nwe create a dataset with a thousand items covering three fairness-related\ntopics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to\ndemonstrate its superior performance.", "published": "2024-02-19 14:02:22", "link": "http://arxiv.org/abs/2402.12150v1", "categories": ["cs.CL", "cs.AI", "I.2; J.4"], "primary_category": "cs.CL"}
{"title": "Transformer-based Causal Language Models Perform Clustering", "abstract": "Even though large language models (LLMs) have demonstrated remarkable\ncapability in solving various natural language tasks, the capability of an LLM\nto follow human instructions is still a concern. Recent works have shown great\nimprovements in the instruction-following capability via additional training\nfor instruction-following tasks. However, the mechanisms responsible for\neffective instruction-following capabilities remain inadequately understood.\nHere, we introduce a simplified instruction-following task and use synthetic\ndatasets to analyze a Transformer-based causal language model. Our findings\nsuggest that the model learns task-specific information by clustering data\nwithin its hidden space, with this clustering process evolving dynamically\nduring learning. We also demonstrate how this phenomenon assists the model in\nhandling unseen instances, and validate our results in a more realistic\nsetting. Furthermore, we present inspired applications regarding pre-training\nand alignment.", "published": "2024-02-19 14:02:31", "link": "http://arxiv.org/abs/2402.12151v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing\n  Code and Interacting with the Environment", "abstract": "We give a model-based agent that builds a Python program representing its\nknowledge of the world based on its interactions with the environment. The\nworld model tries to explain its interactions, while also being optimistic\nabout what reward it can achieve. We define this optimism as a logical\nconstraint between a program and a planner. We study our agent on gridworlds,\nand on task planning, finding our approach is more sample-efficient compared to\ndeep RL, more compute-efficient compared to ReAct-style agents, and that it can\ntransfer its knowledge across environments by editing its code.", "published": "2024-02-19 16:39:18", "link": "http://arxiv.org/abs/2402.12275v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Key ingredients for effective zero-shot cross-lingual knowledge transfer\n  in generative tasks", "abstract": "Zero-shot cross-lingual knowledge transfer enables a multilingual pretrained\nlanguage model, finetuned on a task in one language, make predictions for this\ntask in other languages. While being broadly studied for natural language\nunderstanding tasks, the described setting is understudied for generation.\nPrevious works notice a frequent problem of generation in a wrong language and\npropose approaches to address it, usually using mT5 as a backbone model. In\nthis work we compare various approaches proposed from the literature in unified\nsettings, also including alternative backbone models, namely mBART and\nNLLB-200. We first underline the importance of tuning learning rate used for\nfinetuning, which helps to substantially alleviate the problem of generation in\nthe wrong language. Then, we show that with careful learning rate tuning, the\nsimple full finetuning of the model acts as a very strong baseline and\nalternative approaches bring only marginal improvements. Finally, we find that\nmBART performs similarly to mT5 of the same size, and NLLB-200 can be\ncompetitive in some cases. Our final zero-shot models reach the performance of\nthe approach based on data translation which is usually considered as an upper\nbaseline for zero-shot cross-lingual transfer in generation.", "published": "2024-02-19 16:43:57", "link": "http://arxiv.org/abs/2402.12279v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Skeleton Graph Decoding", "abstract": "Large language models (LLMs) have seen significant adoption for natural\nlanguage tasks, owing their success to massive numbers of model parameters\n(e.g., 70B+); however, LLM inference incurs significant computation and memory\ncosts. Recent approaches propose parallel decoding strategies, such as\nSkeleton-of-Thought (SoT), to improve performance by breaking prompts down into\nsub-problems that can be decoded in parallel; however, they often suffer from\nreduced response quality. Our key insight is that we can request additional\ninformation, specifically dependencies and difficulty, when generating the\nsub-problems to improve both response quality and performance. In this paper,\nwe propose Skeleton Graph Decoding (SGD), which uses dependencies exposed\nbetween sub-problems to support information forwarding between dependent\nsub-problems for improved quality while exposing parallelization opportunities\nfor decoding independent sub-problems. Additionally, we leverage difficulty\nestimates for each sub-problem to select an appropriately-sized model,\nimproving performance without significantly reducing quality. Compared to\nstandard autoregressive generation and SoT, SGD achieves a 1.69x speedup while\nimproving quality by up to 51%.", "published": "2024-02-19 16:47:04", "link": "http://arxiv.org/abs/2402.12280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Open-Source There Yet? A Comparative Study on Commercial and\n  Open-Source LLMs in Their Ability to Label Chest X-Ray Reports", "abstract": "Introduction: With the rapid advances in large language models (LLMs), there\nhave been numerous new open source as well as commercial models. While recent\npublications have explored GPT-4 in its application to extracting information\nof interest from radiology reports, there has not been a real-world comparison\nof GPT-4 to different leading open-source models.\n  Materials and Methods: Two different and independent datasets were used. The\nfirst dataset consists of 540 chest x-ray reports that were created at the\nMassachusetts General Hospital between July 2019 and July 2021. The second\ndataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then\ncompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the\nopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,\nQWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately\nlabel the presence of multiple findings in x-ray text reports using different\nprompting techniques.\n  Results: On the ImaGenome dataset, the best performing open-source model was\nLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot\nprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,\nrespectively. On the institutional dataset, the best performing open-source\nmodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and\nfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and\n0.973, respectively.\n  Conclusion: In this paper, we show that while GPT-4 is superior to\nopen-source models in zero-shot report labeling, the implementation of few-shot\nprompting can bring open-source models on par with GPT-4. This shows that\nopen-source models could be a performant and privacy preserving alternative to\nGPT-4 for the task of radiology report classification.", "published": "2024-02-19 17:23:10", "link": "http://arxiv.org/abs/2402.12298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EVOR: Evolving Retrieval for Code Generation", "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.", "published": "2024-02-19 17:37:28", "link": "http://arxiv.org/abs/2402.12317v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge", "abstract": "Large language models (LLMs) are transforming the way information is\nretrieved with vast amounts of knowledge being summarized and presented via\nnatural language conversations. Yet, LLMs are prone to highlight the most\nfrequently seen pieces of information from the training set and to neglect the\nrare ones. In the field of biomedical research, latest discoveries are key to\nacademic and industrial actors and are obscured by the abundance of an\never-increasing literature corpus (the information overload problem). Surfacing\nnew associations between biomedical entities, e.g., drugs, genes, diseases,\nwith LLMs becomes a challenge of capturing the long-tail knowledge of the\nbiomedical scientific production. To overcome this challenge, Retrieval\nAugmented Generation (RAG) has been proposed to alleviate some of the\nshortcomings of LLMs by augmenting the prompts with context retrieved from\nexternal datasets. RAG methods typically select the context via maximum\nsimilarity search over text embeddings. In this study, we show that RAG methods\nleave out a significant proportion of relevant information due to clusters of\nover-represented concepts in the biomedical literature. We introduce a novel\ninformation-retrieval method that leverages a knowledge graph to downsample\nthese clusters and mitigate the information overload problem. Its retrieval\nperformance is about twice better than embedding similarity alternatives on\nboth precision and recall. Finally, we demonstrate that both embedding\nsimilarity and knowledge graph retrieval methods can be advantageously combined\ninto a hybrid model that outperforms both, enabling potential improvements to\nbiomedical question-answering models.", "published": "2024-02-19 18:31:11", "link": "http://arxiv.org/abs/2402.12352v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context\n  Analogies", "abstract": "Humans regularly engage in analogical thinking, relating personal experiences\nto current situations (X is analogous to Y because of Z). Analogical thinking\nallows humans to solve problems in creative ways, grasp difficult concepts, and\narticulate ideas more effectively. Can language models (LMs) do the same? To\nanswer this question, we propose AnaloBench, a benchmark to determine\nanalogical reasoning ability in LMs. Our benchmarking approach focuses on\naspects of this ability that are common among humans: (i) recalling related\nexperiences from a large amount of information, and (ii) applying analogical\nreasoning to complex and lengthy scenarios. We test a broad collection of\nproprietary models (e.g., GPT family, Claude V2) and open source models such as\nLLaMA2. As in prior results, scaling up LMs results in some performance boosts.\nSurprisingly, scale offers minimal gains when, (i) analogies involve lengthy\nscenarios, or (ii) recalling relevant scenarios from a large pool of\ninformation, a process analogous to finding a needle in a haystack. We hope\nthese observations encourage further research in this field.", "published": "2024-02-19 18:56:44", "link": "http://arxiv.org/abs/2402.12370v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Induced Model Matching: Restricted Models Help Train Full-Featured\n  Models", "abstract": "We consider scenarios where a very accurate (often small) predictive model\nusing restricted features is available when training a full-featured (often\nlarger) model. This restricted model may be thought of as side-information'',\nand can come either from an auxiliary dataset or from the same dataset by\nforcing the restriction. How can the restricted model be useful to the full\nmodel? To answer this, we introduce a methodology called Induced Model Matching\n(IMM). IMM aligns the context-restricted, or induced, version of the large\nmodel with the restricted model. We relate IMM to approaches such as noising,\nwhich is implicit in addressing the problem, and reverse knowledge distillation\nfrom weak teachers, which is explicit but does not exploit restriction being\nthe nature of the weakness. We show that these prior methods can be thought of\nas approximations to IMM and can be problematic in terms of consistency.\nExperimentally, we first motivate IMM using logistic regression as a toy\nexample. We then explore it in language modeling, the application that\ninitially inspired it, and demonstrate it on both LSTM and transformer full\nmodels, using bigrams as restricted models. We lastly give a simple RL example,\nwhich shows that POMDP policies can help learn better MDP policies. The IMM\nprinciple is thus generally applicable in common scenarios where restricted\ndata is cheaper to collect or restricted models are easier to learn.", "published": "2024-02-19 20:21:09", "link": "http://arxiv.org/abs/2402.12513v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "IMBUE: Improving Interpersonal Effectiveness through Simulation and\n  Just-in-time Feedback with Human-Language Model Interaction", "abstract": "Navigating certain communication situations can be challenging due to\nindividuals' lack of skills and the interference of strong emotions. However,\neffective learning opportunities are rarely accessible. In this work, we\nconduct a human-centered study that uses language models to simulate bespoke\ncommunication training and provide just-in-time feedback to support the\npractice and learning of interpersonal effectiveness skills. We apply the\ninterpersonal effectiveness framework from Dialectical Behavioral Therapy\n(DBT), DEAR MAN, which focuses on both conversational and emotional skills. We\npresent IMBUE, an interactive training system that provides feedback 25% more\nsimilar to experts' feedback, compared to that generated by GPT-4. IMBUE is the\nfirst to focus on communication skills and emotion management simultaneously,\nincorporate experts' domain knowledge in providing feedback, and be grounded in\npsychology theory. Through a randomized trial of 86 participants, we find that\nIMBUE's simulation-only variant significantly improves participants'\nself-efficacy (up to 17%) and reduces negative emotions (up to 25%). With\nIMBUE's additional just-in-time feedback, participants demonstrate 17%\nimprovement in skill mastery, along with greater enhancements in self-efficacy\n(27% more) and reduction of negative emotions (16% more) compared to\nsimulation-only. The improvement in skill mastery is the only measure that is\ntransferred to new and more difficult situations; situation specific training\nis necessary for improving self-efficacy and emotion reduction.", "published": "2024-02-19 21:31:11", "link": "http://arxiv.org/abs/2402.12556v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "CausalGym: Benchmarking causal interpretability methods on linguistic\n  tasks", "abstract": "Language models (LMs) have proven to be powerful tools for psycholinguistic\nresearch, but most prior work has focused on purely behavioural measures (e.g.,\nsurprisal comparisons). At the same time, research in model interpretability\nhas begun to illuminate the abstract causal mechanisms shaping LM behavior. To\nhelp bring these strands of research closer together, we introduce CausalGym.\nWe adapt and expand the SyntaxGym suite of tasks to benchmark the ability of\ninterpretability methods to causally affect model behaviour. To illustrate how\nCausalGym can be used, we study the pythia models (14M--6.9B) and assess the\ncausal efficacy of a wide range of interpretability methods, including linear\nprobing and distributed alignment search (DAS). We find that DAS outperforms\nthe other methods, and so we use it to study the learning trajectory of two\ndifficult linguistic phenomena in pythia-1b: negative polarity item licensing\nand filler--gap dependencies. Our analysis shows that the mechanism\nimplementing both of these tasks is learned in discrete stages, not gradually.", "published": "2024-02-19 21:35:56", "link": "http://arxiv.org/abs/2402.12560v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of\n  Large Language Models", "abstract": "The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.", "published": "2024-02-19 21:38:02", "link": "http://arxiv.org/abs/2402.12563v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence", "abstract": "LLMs can generate factually incorrect statements even when provided access to\nreference documents. Such errors can be dangerous in high-stakes applications\n(e.g., document-grounded QA for healthcare or finance). We present GenAudit --\na tool intended to assist fact-checking LLM responses for document-grounded\ntasks. GenAudit suggests edits to the LLM response by revising or removing\nclaims that are not supported by the reference document, and also presents\nevidence from the reference for facts that do appear to have support. We train\nmodels to execute these tasks, and design an interactive interface to present\nsuggested edits and evidence to users. Comprehensive evaluation by human raters\nshows that GenAudit can detect errors in 8 different LLM outputs when\nsummarizing documents from diverse domains. User studies demonstrate that using\nGenAudit can substantially improve the performance of humans at finding errors\nin LLM-generated summaries. We release our tool (GenAudit) and fact-checking\nmodel for public use.", "published": "2024-02-19 21:45:55", "link": "http://arxiv.org/abs/2402.12566v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evolving AI Collectives to Enhance Human Diversity and Enable\n  Self-Regulation", "abstract": "Large language model behavior is shaped by the language of those with whom\nthey interact. This capacity and their increasing prevalence online portend\nthat they will intentionally or unintentionally \"program\" one another and form\nemergent AI subjectivities, relationships, and collectives. Here, we call upon\nthe research community to investigate these \"societies\" of interacting\nartificial intelligences to increase their rewards and reduce their risks for\nhuman society and the health of online environments. We use a small \"community\"\nof models and their evolving outputs to illustrate how such emergent,\ndecentralized AI collectives can spontaneously expand the bounds of human\ndiversity and reduce the risk of toxic, anti-social behavior online. Finally,\nwe discuss opportunities for AI cross-moderation and address ethical issues and\ndesign challenges associated with creating and maintaining free-formed AI\ncollectives.", "published": "2024-02-19 22:59:43", "link": "http://arxiv.org/abs/2402.12590v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning\n  Performance of Large Language Models", "abstract": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that the traditional\nmetric of next word prediction correlates negatively with performance of LLMs'\non our reasoning dataset. We analyse our results and identify failure modes\nthat can serve as useful guides for future research, potentially informing\nstrategies to address the limitations observed in LLMs.", "published": "2024-02-19 16:04:53", "link": "http://arxiv.org/abs/2402.14848v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting misinformation through Framing Theory: the Frame Element-based\n  Model", "abstract": "In this paper, we delve into the rapidly evolving challenge of misinformation\ndetection, with a specific focus on the nuanced manipulation of narrative\nframes - an under-explored area within the AI community. The potential for\nGenerative AI models to generate misleading narratives underscores the urgency\nof this problem. Drawing from communication and framing theories, we posit that\nthe presentation or 'framing' of accurate information can dramatically alter\nits interpretation, potentially leading to misinformation. We highlight this\nissue through real-world examples, demonstrating how shifts in narrative frames\ncan transmute fact-based information into misinformation. To tackle this\nchallenge, we propose an innovative approach leveraging the power of\npre-trained Large Language Models and deep neural networks to detect\nmisinformation originating from accurate facts portrayed under different\nframes. These advanced AI techniques offer unprecedented capabilities in\nidentifying complex patterns within unstructured data critical for examining\nthe subtleties of narrative frames. The objective of this paper is to bridge a\nsignificant research gap in the AI domain, providing valuable insights and\nmethodologies for tackling framing-induced misinformation, thus contributing to\nthe advancement of responsible and trustworthy AI technologies. Several\nexperiments are intensively conducted and experimental results explicitly\ndemonstrate the various impact of elements of framing theory proving the\nrationale of applying framing theory to increase the performance in\nmisinformation detection.", "published": "2024-02-19 21:50:42", "link": "http://arxiv.org/abs/2402.15525v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SPML: A DSL for Defending Language Models Against Prompt Attacks", "abstract": "Large language models (LLMs) have profoundly transformed natural language\napplications, with a growing reliance on instruction-based definitions for\ndesigning chatbots. However, post-deployment the chatbot definitions are fixed\nand are vulnerable to attacks by malicious users, emphasizing the need to\nprevent unethical applications and financial losses. Existing studies explore\nuser prompts' impact on LLM-based chatbots, yet practical methods to contain\nattacks on application-specific chatbots remain unexplored. This paper presents\nSystem Prompt Meta Language (SPML), a domain-specific language for refining\nprompts and monitoring the inputs to the LLM-based chatbots. SPML actively\nchecks attack prompts, ensuring user inputs align with chatbot definitions to\nprevent malicious execution on the LLM backbone, optimizing costs. It also\nstreamlines chatbot definition crafting with programming language capabilities,\novercoming natural language design challenges. Additionally, we introduce a\ngroundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering\nthe inaugural language and benchmark for chatbot definition evaluation.\nExperiments across datasets demonstrate SPML's proficiency in understanding\nattacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data\nand codes are publicly available at: https://prompt-compiler.github.io/SPML/.", "published": "2024-02-19 00:53:48", "link": "http://arxiv.org/abs/2402.11755v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.PL"], "primary_category": "cs.LG"}
{"title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs", "abstract": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.", "published": "2024-02-19 01:28:48", "link": "http://arxiv.org/abs/2402.11764v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7; K.4.1"], "primary_category": "cs.CL"}
{"title": "Uncovering Latent Human Wellbeing in Language Model Embeddings", "abstract": "Do language models implicitly learn a concept of human wellbeing? We explore\nthis through the ETHICS Utilitarianism task, assessing if scaling enhances\npretrained models' representations. Our initial finding reveals that, without\nany prompt engineering or finetuning, the leading principal component from\nOpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches\nthe 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting\npretraining conveys some understanding about human wellbeing. Next, we consider\nfour language model families, observing how Utilitarianism accuracy varies with\nincreased parameters. We find performance is nondecreasing with increased model\nsize when using sufficient numbers of principal components.", "published": "2024-02-19 02:08:03", "link": "http://arxiv.org/abs/2402.11777v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge\n  Graphs", "abstract": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts\nfrom new KGs that are not seen during training, has been widely adopted in\nvarious applications. One critical challenge of KG inductive reasoning is\nhandling low-resource scenarios with scarcity in both textual and structural\naspects. In this paper, we attempt to address this challenge with Large\nLanguage Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to\ngenerate a graph-structural prompt to enhance the pre-trained Graph Neural\nNetworks (GNNs), which brings us new methodological insights into the KG\ninductive reasoning methods, as well as high generalizability in practice. On\nthe methodological side, we introduce a novel pretraining and prompting\nframework ProLINK, designed for low-resource inductive reasoning across\narbitrary KGs without requiring additional training. On the practical side, we\nexperimentally evaluate our approach on 36 low-resource KG datasets and find\nthat ProLINK outperforms previous methods in three-shot, one-shot, and\nzero-shot reasoning tasks, exhibiting average performance improvements by 20%,\n45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong\nrobustness for various LLM promptings as well as full-shot scenarios.", "published": "2024-02-19 03:21:19", "link": "http://arxiv.org/abs/2402.11804v3", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Generation Meets Verification: Accelerating Large Language Model\n  Inference with Smart Parallel Auto-Correct Decoding", "abstract": "This research aims to accelerate the inference speed of large language models\n(LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel\n\\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative\napproach designed for achieving lossless acceleration of LLMs. By integrating\nsemi-autoregressive inference and speculative decoding capabilities, SPACE\nuniquely enables autoregressive LLMs to parallelize token generation and\nverification. This is realized through a specialized semi-autoregressive\nsupervised fine-tuning process that equips existing LLMs with the ability to\nsimultaneously predict multiple tokens. Additionally, an auto-correct decoding\nalgorithm facilitates the simultaneous generation and verification of token\nsequences within a single model invocation. Through extensive experiments on a\nrange of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x\non HumanEval-X while maintaining output quality.", "published": "2024-02-19 03:39:10", "link": "http://arxiv.org/abs/2402.11809v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to\n  Detect Machine-Generated Text?", "abstract": "This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.", "published": "2024-02-19 04:11:34", "link": "http://arxiv.org/abs/2402.11815v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Where It Really Matters: Few-Shot Environmental Conservation Media\n  Monitoring for Low-Resource Languages", "abstract": "Environmental conservation organizations routinely monitor news content on\nconservation in protected areas to maintain situational awareness of\ndevelopments that can have an environmental impact. Existing automated media\nmonitoring systems require large amounts of data labeled by domain experts,\nwhich is only feasible at scale for high-resource languages like English.\nHowever, such tools are most needed in the global south where news of interest\nis mainly in local low-resource languages, and far fewer experts are available\nto annotate datasets sustainably. In this paper, we propose NewsSerow, a method\nto automatically recognize environmental conservation content in low-resource\nlanguages. NewsSerow is a pipeline of summarization, in-context few-shot\nclassification, and self-reflection using large language models (LLMs). Using\nat most 10 demonstration example news articles in Nepali, NewsSerow\nsignificantly outperforms other few-shot methods and achieves comparable\nperformance with models fully fine-tuned using thousands of examples. The World\nWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in\nNepal, significantly reducing their operational burden, and ensuring that AI\ntools for conservation actually reach the communities that need them the most.\nNewsSerow has also been deployed for countries with other languages like\nColombia.", "published": "2024-02-19 04:17:21", "link": "http://arxiv.org/abs/2402.11818v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Microstructures and Accuracy of Graph Recall by Large Language Models", "abstract": "Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.", "published": "2024-02-19 04:29:45", "link": "http://arxiv.org/abs/2402.11821v3", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.LG"}
{"title": "An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf\n  Optimizer (GWO) for text feature selection and clustering", "abstract": "Text document clustering can play a vital role in organizing and handling the\neverincreasing number of text documents. Uninformative and redundant features\nincluded in large text documents reduce the effectiveness of the clustering\nalgorithm. Feature selection (FS) is a well-known technique for removing these\nfeatures. Since FS can be formulated as an optimization problem, various\nmeta-heuristic algorithms have been employed to solve it.\nTeaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm\nthat benefits from the low number of parameters and fast convergence. A hybrid\nmethod can simultaneously benefit from the advantages of TLBO and tackle the\npossible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey\nWolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests\na filter-based FS algorithm (TLBO-GWO). Six benchmark datasets are selected,\nand TLBO-GWO is compared with three recently proposed FS algorithms with\nsimilar approaches, the main TLBO and GWO. The comparison is conducted based on\nclustering evaluation measures, convergence behavior, and dimension reduction,\nand is validated using statistical tests. The results reveal that TLBO-GWO can\nsignificantly enhance the effectiveness of the text clustering technique\n(K-means).", "published": "2024-02-19 05:06:10", "link": "http://arxiv.org/abs/2402.11839v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are\n  Lacking", "abstract": "Transformer based code models have impressive performance in many software\nengineering tasks. However, their effectiveness degrades when symbols are\nmissing or not informative. The reason is that the model may not learn to pay\nattention to the right correlations/contexts without the help of symbols. We\npropose a new method to pre-train general code models when symbols are lacking.\nWe observe that in such cases, programs degenerate to something written in a\nvery primitive language. We hence propose to use program analysis to extract\ncontexts a priori (instead of relying on symbols and masked language modeling\nas in vanilla models). We then leverage a novel attention masking method to\nonly allow the model attending to these contexts, e.g., bi-directional program\ndependence transitive closures and token co-occurrences. In the meantime, the\ninherent self-attention mechanism is utilized to learn which of the allowed\nattentions are more important compared to others. To realize the idea, we\nenhance the vanilla tokenization and model architecture of a BERT model,\nconstruct and utilize attention masks, and introduce a new pre-training\nalgorithm. We pre-train this BERT-like model from scratch, using a dataset of\n26 million stripped binary functions with explicit program dependence\ninformation extracted by our tool. We apply the model in three downstream\ntasks: binary similarity, type inference, and malware family classification.\nOur pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49%\nto 60%, and 74% to 94%, respectively. It also substantially outperforms other\ngeneral pre-training techniques of code understanding models.", "published": "2024-02-19 05:13:22", "link": "http://arxiv.org/abs/2402.11842v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Bridging or Breaking: Impact of Intergroup Interactions on Religious\n  Polarization", "abstract": "While exposure to diverse viewpoints may reduce polarization, it can also\nhave a backfire effect and exacerbate polarization when the discussion is\nadversarial. Here, we examine the question whether intergroup interactions\naround important events affect polarization between majority and minority\ngroups in social networks. We compile data on the religious identity of nearly\n700,000 Indian Twitter users engaging in COVID-19-related discourse during\n2020. We introduce a new measure for an individual's group conformity based on\ncontextualized embeddings of tweet text, which helps us assess polarization\nbetween religious groups. We then use a meta-learning framework to examine\nheterogeneous treatment effects of intergroup interactions on an individual's\ngroup conformity in the light of communal, political, and socio-economic\nevents. We find that for political and social events, intergroup interactions\nreduce polarization. This decline is weaker for individuals at the extreme who\nalready exhibit high conformity to their group. In contrast, during communal\nevents, intergroup interactions can increase group conformity. Finally, we\ndecompose the differential effects across religious groups in terms of emotions\nand topics of discussion. The results show that the dynamics of religious\npolarization are sensitive to the context and have important implications for\nunderstanding the role of intergroup interactions.", "published": "2024-02-19 07:21:09", "link": "http://arxiv.org/abs/2402.11895v3", "categories": ["cs.SI", "cs.CL", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs", "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, while the expensive memory and computation consumption\nimpede their practical deployment. Quantization emerges as one of the most\neffective methods for improving the computational efficiency of LLMs. However,\nexisting ultra-low-bit quantization always causes severe accuracy drops. In\nthis paper, we empirically relieve the micro and macro characteristics of\nultra-low bit quantization and present a novel Dual-Binarization method for\nLLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage\nof 2-bit-width and the efficiency advantage of binarization into account,\nintroducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized\nweights into two independent sets of binaries, FDB ensures the accuracy of\nrepresentations and introduces flexibility, utilizing the efficient bitwise\noperations of binarization while retaining the inherent high sparsity of\nultra-low bit quantization. For the macro-level, we find the distortion that\nexists in the prediction of LLM after quantization, which is specified as the\ndeviations related to the ambiguity of samples. We propose the Deviation-Aware\nDistillation (DAD) method, enabling the model to focus differently on various\nsamples. Comprehensive experiments show that our DB-LLM not only significantly\nsurpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization\n(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional\n20\\% reduction in computational consumption compared to the SOTA method under\nthe same bit-width. Our code will be released soon.", "published": "2024-02-19 09:04:30", "link": "http://arxiv.org/abs/2402.11960v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Remember This Event That Year? Assessing Temporal Information and\n  Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) are increasingly ubiquitous, yet their ability\nto retain and reason about temporal information remains limited, hindering\ntheir application in real-world scenarios where understanding the sequential\nnature of events is crucial. Our study experiments with 12 state-of-the-art\nmodels (ranging from 2B to 70B+ parameters) on a novel numerical-temporal\ndataset, \\textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover\nsignificant temporal retention and comprehension limitations. We propose six\nmetrics to assess three learning paradigms to enhance temporal knowledge\nacquisition. Our findings reveal that open-source models exhibit knowledge gaps\nmore frequently, suggesting a trade-off between limited knowledge and incorrect\nresponses. Additionally, various fine-tuning approaches significantly improved\nperformance, reducing incorrect outputs and impacting the identification of\n'information not available' in the generations. The associated dataset and code\nare available at (https://github.com/lingoiitgn/TempUN).", "published": "2024-02-19 09:43:03", "link": "http://arxiv.org/abs/2402.11997v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by\n  Downscaling Frequency Space", "abstract": "Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes\nare publicly available at https://github.com/ZrW00/MuScleLoRA.", "published": "2024-02-19 10:34:48", "link": "http://arxiv.org/abs/2402.12026v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "All Language Models Large and Small", "abstract": "Many leading language models (LMs) use high-intensity computational resources\nboth during training and execution. This poses the challenge of lowering\nresource costs for deployment and faster execution of decision-making tasks\namong others. We introduce a novel plug-and-play LM framework named Language\nOptimising Network Distribution (LONDI) framework. LONDI learns to selectively\nemploy large LMs only where complex decision-making and reasoning are required\nwhile using low-resource LMs (i.e. LMs require less GPU usage, but may not be\nable to solve the problem alone) everywhere else. LONDI consists of a system of\ntwo (off-)policy networks, an LM, a large LM (LLM), and a reinforcement\nlearning module that uses switching controls to quickly learn which system\nstates to call the LLM. We then introduce a variant of LONDI that maintains\nbudget constraints on LLM calls and hence its resource usage. Theoretically, we\nprove LONDI learns the subset of system states to activate the LLM required to\nsolve the task. We then prove that LONDI converges to optimal solutions while\nalso preserving budgetary constraints on LLM calls almost surely enabling it to\nsolve various tasks while significantly lowering computational costs. We test\nLONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and\ndemonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs\nwhile reducing GPU usage by up to 30%.", "published": "2024-02-19 11:28:20", "link": "http://arxiv.org/abs/2402.12061v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language\n  Models Gains More", "abstract": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.", "published": "2024-02-19 11:33:21", "link": "http://arxiv.org/abs/2402.12065v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Groot: Adversarial Testing for Generative Text-to-Image Models with\n  Tree-based Semantic Transformation", "abstract": "With the prevalence of text-to-image generative models, their safety becomes\na critical concern. adversarial testing techniques have been developed to probe\nwhether such models can be prompted to produce Not-Safe-For-Work (NSFW)\ncontent. However, existing solutions face several challenges, including low\nsuccess rate and inefficiency. We introduce Groot, the first automated\nframework leveraging tree-based semantic transformation for adversarial testing\nof text-to-image models. Groot employs semantic decomposition and sensitive\nelement drowning strategies in conjunction with LLMs to systematically refine\nadversarial prompts. Our comprehensive evaluation confirms the efficacy of\nGroot, which not only exceeds the performance of current state-of-the-art\napproaches but also achieves a remarkable success rate (93.66%) on leading\ntext-to-image models such as DALL-E 3 and Midjourney.", "published": "2024-02-19 12:31:56", "link": "http://arxiv.org/abs/2402.12100v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.SE"], "primary_category": "cs.CL"}
{"title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models", "abstract": "Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.", "published": "2024-02-19 13:16:10", "link": "http://arxiv.org/abs/2402.12121v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Enabling Weak LLMs to Judge Response Reliability via Meta Ranking", "abstract": "Despite the strong performance of large language models (LLMs) across a wide\nrange of tasks, they still have reliability issues. Previous studies indicate\nthat strong LLMs like GPT-4-turbo excel in evaluating the reliability of\nresponses from LLMs, but face efficiency and local deployment issues. Thus, to\nenable weak LLMs to effectively assess the reliability of LLM responses, we\npropose a novel cross-query-comparison-based method called $\\textit{Meta\nRanking}$ (MR). Unlike previous few-shot methods that solely based on\nin-context learning capabilities in LLMs, MR assesses reliability by pairwisely\nranking the target query-response pair with multiple reference query-response\npairs. We found that MR is highly effective in error detection for LLM\nresponses, where weak LLMs, such as Phi-2, could surpass strong baselines like\nGPT-3.5-turbo, requiring only five reference samples and significantly\nimproving efficiency. We further demonstrate that MR can enhance strong LLMs'\nperformance in two practical applications: model cascading and instruction\ntuning. In model cascading, we combine open- and closed-source LLMs to achieve\nperformance comparable to GPT-4-turbo with lower costs. In instruction tuning,\nwe use MR for iterative training data filtering, significantly reducing data\nprocessing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with\nfewer training tokens. These results underscore the high potential of MR in\nboth efficiency and effectiveness.", "published": "2024-02-19 13:57:55", "link": "http://arxiv.org/abs/2402.12146v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defending Against Weight-Poisoning Backdoor Attacks for\n  Parameter-Efficient Fine-Tuning", "abstract": "Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.", "published": "2024-02-19 14:22:54", "link": "http://arxiv.org/abs/2402.12168v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning", "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.", "published": "2024-02-19 14:33:24", "link": "http://arxiv.org/abs/2402.12177v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Amplifying Training Data Exposure through Fine-Tuning with\n  Pseudo-Labeled Memberships", "abstract": "Neural language models (LMs) are vulnerable to training data extraction\nattacks due to data memorization. This paper introduces a novel attack scenario\nwherein an attacker adversarially fine-tunes pre-trained LMs to amplify the\nexposure of the original training data. This strategy differs from prior\nstudies by aiming to intensify the LM's retention of its pre-training dataset.\nTo achieve this, the attacker needs to collect generated texts that are closely\naligned with the pre-training data. However, without knowledge of the actual\ndataset, quantifying the amount of pre-training data within generated texts is\nchallenging. To address this, we propose the use of pseudo-labels for these\ngenerated texts, leveraging membership approximations indicated by\nmachine-generated probabilities from the target LM. We subsequently fine-tune\nthe LM to favor generations with higher likelihoods of originating from the\npre-training data, based on their membership probabilities. Our empirical\nfindings indicate a remarkable outcome: LMs with over 1B parameters exhibit a\nfour to eight-fold increase in training data exposure. We discuss potential\nmitigations and suggest future research directions.", "published": "2024-02-19 14:52:50", "link": "http://arxiv.org/abs/2402.12189v2", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7; K.6.5"], "primary_category": "cs.CL"}
{"title": "Exploring the Limits of Zero Shot Vision Language Models for Hate Meme\n  Detection: The Vulnerabilities and their Interpretations", "abstract": "There is a rapid increase in the use of multimedia content in current social\nmedia platforms. One of the highly popular forms of such multimedia content are\nmemes. While memes have been primarily invented to promote funny and buoyant\ndiscussions, malevolent users exploit memes to target individuals or vulnerable\ncommunities, making it imperative to identify and address such instances of\nhateful memes. Thus social media platforms are in dire need for active\nmoderation of such harmful content. While manual moderation is extremely\ndifficult due to the scale of such content, automatic moderation is challenged\nby the need of good quality annotated data to train hate meme detection\nalgorithms. This makes a perfect pretext for exploring the power of modern day\nvision language models (VLMs) that have exhibited outstanding performance\nacross various tasks. In this paper we study the effectiveness of VLMs in\nhandling intricate tasks such as hate meme detection in a completely zero-shot\nsetting so that there is no dependency on annotated data for the task. We\nperform thorough prompt engineering and query state-of-the-art VLMs using\nvarious prompt types to detect hateful/harmful memes. We further interpret the\nmisclassification cases using a novel superpixel based occlusion method.\nFinally we show that these misclassifications can be neatly arranged into a\ntypology of error classes the knowledge of which should enable the design of\nbetter safety guardrails in future.", "published": "2024-02-19 15:03:04", "link": "http://arxiv.org/abs/2402.12198v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reformatted Alignment", "abstract": "The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.", "published": "2024-02-19 15:21:58", "link": "http://arxiv.org/abs/2402.12219v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement\n  Learning for LLM-based Mutation", "abstract": "Fuzzing is an effective bug-finding technique but it struggles with complex\nsystems like JavaScript engines that demand precise grammatical input.\nRecently, researchers have adopted language models for context-aware mutation\nin fuzzing to address this problem. However, existing techniques are limited in\nutilizing coverage guidance for fuzzing, which is rather performed in a\nblack-box manner. This paper presents a novel technique called CovRL\n(Coverage-guided Reinforcement Learning) that combines Large Language Models\n(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,\nCovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging\nthe Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a\nweighted coverage map. This map is key in calculating the fuzzing reward, which\nis then applied to the LLM-based mutator through reinforcement learning.\nCovRL-Fuzz, through this approach, enables the generation of test cases that\nare more likely to discover new coverage areas, thus improving vulnerability\ndetection while minimizing syntax and semantic errors, all without needing\nextra post-processing. Our evaluation results indicate that CovRL-Fuzz\noutperforms the state-of-the-art fuzzers in terms of code coverage and\nbug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related\nbugs in the latest JavaScript engines, including 39 previously unknown\nvulnerabilities and 11 CVEs.", "published": "2024-02-19 15:30:40", "link": "http://arxiv.org/abs/2402.12222v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE", "D.4.6; I.2.5; D.2.4"], "primary_category": "cs.CR"}
{"title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/", "published": "2024-02-19 15:33:10", "link": "http://arxiv.org/abs/2402.12226v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles", "abstract": "Fine-tuning large language models can improve task specific performance,\nalthough a general understanding of what the fine-tuned model has learned,\nforgotten and how to trust its predictions is still missing. We derive\nprincipled uncertainty quantification for fine-tuned LLMs with posterior\napproximations using computationally efficient low-rank adaptation ensembles.\nWe analyze three common multiple-choice datasets using low-rank adaptation\nensembles based on Mistral-7b, and draw quantitative and qualitative\nconclusions on their perceived complexity and model efficacy on the different\ntarget domains during and after fine-tuning. In particular, backed by the\nnumerical experiments, we hypothesise about signals from entropic uncertainty\nmeasures for data domains that are inherently difficult for a given\narchitecture to learn.", "published": "2024-02-19 16:26:00", "link": "http://arxiv.org/abs/2402.12264v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Query-Based Adversarial Prompt Generation", "abstract": "Recent work has shown it is possible to construct adversarial examples that\ncause an aligned language model to emit harmful strings or perform harmful\nbehavior. Existing attacks work either in the white-box setting (with full\naccess to the model weights), or through transferability: the phenomenon that\nadversarial examples crafted on one model often remain effective on other\nmodels. We improve on prior work with a query-based attack that leverages API\naccess to a remote language model to construct adversarial examples that cause\nthe model to emit harmful strings with (much) higher probability than with\ntransfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety\nclassifier; we can cause GPT-3.5 to emit harmful strings that current transfer\nattacks fail at, and we can evade the safety classifier with nearly 100%\nprobability.", "published": "2024-02-19 18:01:36", "link": "http://arxiv.org/abs/2402.12329v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emulated Disalignment: Safety Alignment for Large Language Models May\n  Backfire!", "abstract": "Large language models (LLMs) undergo safety alignment to ensure safe\nconversations with humans. However, this paper introduces a training-free\nattack method capable of reversing safety alignment, converting the outcomes of\nstronger alignment into greater potential for harm by accessing only LLM output\ntoken distributions. Specifically, our method achieves this reversal by\ncontrasting the output token distribution of a safety-aligned language model\n(e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that\nthe token predictions are shifted towards the opposite direction of safety\nalignment. We name this method emulated disalignment (ED) because sampling from\nthis contrastive distribution provably emulates the result of fine-tuning to\nminimize a safety reward. Our experiments with ED across three evaluation\ndatasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show\nthat ED doubles the harmfulness of pre-trained models and outperforms strong\nbaselines, achieving the highest harmful rates in 43 out of 48 evaluation\nsubsets by a large margin. Eventually, given ED's reliance on language model\noutput token distributions, which particularly compromises open-source models,\nour findings highlight the need to reassess the open accessibility of language\nmodels, even if they have been safety-aligned. Code is available at\nhttps://github.com/ZHZisZZ/emulated-disalignment.", "published": "2024-02-19 18:16:51", "link": "http://arxiv.org/abs/2402.12343v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via\n  Game-Theoretic Evaluations", "abstract": "As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and\n(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that\n(1) LLMs have distinct behaviors regarding various gaming scenarios; for\nexample, LLMs fail in complete and deterministic games yet they are competitive\nin probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,\nCodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than\ncommercial LLMs, e.g., GPT-4, in complex games, yet the recently released\nLlama-3-70b-Instruct makes up for this shortcoming. In addition,\ncode-pretraining greatly benefits strategic reasoning, while advanced reasoning\nmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always\nhelp. We further characterize the game-theoretic properties of LLMs, such as\nequilibrium and Pareto Efficiency in repeated games. Detailed error profiles\nare provided for a better understanding of LLMs' behavior. We hope our research\nprovides standardized protocols and serves as a foundation to spur further\nexplorations in the strategic reasoning of LLMs.", "published": "2024-02-19 18:23:36", "link": "http://arxiv.org/abs/2402.12348v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LoRA+: Efficient Low Rank Adaptation of Large Models", "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.", "published": "2024-02-19 18:33:49", "link": "http://arxiv.org/abs/2402.12354v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models", "abstract": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for\nimproving the instruction-following abilities of powerful pre-trained language\nmodels. RLAIF first performs supervised fine-tuning (SFT) using demonstrations\nfrom a teacher model and then further fine-tunes the model with reinforcement\nlearning (RL), using feedback from a critic model. While recent popular\nopen-source models have demonstrated substantial improvements in performance\nfrom the RL step, in this paper we question whether the complexity of this RL\nstep is truly warranted for AI feedback. We show that the improvements of the\nRL step are virtually entirely due to the widespread practice of using a weaker\nteacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,\nGPT-4) used for AI feedback generation. Specifically, we show that simple\nsupervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF\npipelines. More generally, we find that the gains from RLAIF vary substantially\nacross base model families, test-time evaluation protocols, and critic models.\nFinally, we provide a mechanistic explanation for when SFT may outperform the\nfull two-step RLAIF pipeline as well as suggestions for making RLAIF maximally\nuseful in practice.", "published": "2024-02-19 18:53:54", "link": "http://arxiv.org/abs/2402.12366v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs", "abstract": "Existing methods for fine-tuning sparse LLMs often suffer from\nresource-intensive requirements and high retraining costs. Additionally, many\nfine-tuning methods often rely on approximations or heuristic optimization\nstrategies, which may lead to suboptimal solutions. To address these issues, we\npropose an efficient and fast framework for fine-tuning sparse LLMs based on\nminimizing reconstruction error. Our approach involves sampling a small dataset\nfor calibration and utilizing backpropagation to iteratively optimize\nblock-wise reconstruction error, on a block-by-block basis, aiming for optimal\nsolutions. Extensive experiments on various benchmarks consistently demonstrate\nthe superiority of our method over other baselines. For instance, on the\nWikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a\nperplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of\n75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a\nperplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the\nfine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,\nand the entire framework can be executed on a single 16GB GPU. The source code\nis available at https://github.com/sunggo/EBFT.", "published": "2024-02-19 09:55:32", "link": "http://arxiv.org/abs/2402.12419v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models", "abstract": "The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech\n(TTS) domain is rising, providing great value in synthesizing high quality\nspeech. Although they exhibit impressive audio quality, the extent of their\nsemantic capabilities is unknown, and controlling their synthesized speech's\nvocal properties remains a challenge. Inspired by recent advances in image\nsynthesis, we explore the latent space of frozen TTS models, which is composed\nof the latent bottleneck activations of the DDM's denoiser. We identify that\nthis space contains rich semantic information, and outline several novel\nmethods for finding semantic directions within it, both supervised and\nunsupervised. We then demonstrate how these enable off-the-shelf audio editing,\nwithout any further training, architectural changes or data requirements. We\npresent evidence of the semantic and acoustic qualities of the edited audio,\nand provide supplemental samples:\nhttps://latent-analysis-grad-tts.github.io/speech-samples/.", "published": "2024-02-19 16:22:21", "link": "http://arxiv.org/abs/2402.12423v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tables as Texts or Images: Evaluating the Table Reasoning Ability of\n  LLMs and MLLMs", "abstract": "In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analyses extend across six benchmarks for table-related tasks such\nas question-answering and fact-checking. We introduce for the first time the\nassessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the role of representation and prompting on LLM\nperformance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.", "published": "2024-02-19 16:34:50", "link": "http://arxiv.org/abs/2402.12424v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "The Revolution of Multimodal Large Language Models: A Survey", "abstract": "Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.", "published": "2024-02-19 19:01:01", "link": "http://arxiv.org/abs/2402.12451v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Parallel Structures in Pre-training Data Yield In-Context Learning", "abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL):\nthey can adapt to a task with only a few examples given in the prompt without\nany parameter update. However, it is unclear where this capability comes from\nas there is a stark distribution shift between pre-training text and ICL\nprompts. In this work, we study what patterns of the pre-training data\ncontribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel\nstructures}$ in the pre-training data -- pairs of phrases following similar\ntemplates in the same context window. Specifically, we detect parallel\nstructures by checking whether training on one phrase improves prediction of\nthe other, and conduct ablation experiments to study their effect on ICL. We\nshow that removing parallel structures in the pre-training data reduces LMs'\nICL accuracy by 51% (vs 2% from random ablation). This drop persists even when\nexcluding common patterns such as n-gram repetitions and long-range dependency,\nshowing the diversity and generality of parallel structures. A closer look at\nthe detected parallel structures indicates that they cover diverse linguistic\ntasks and span long distances in the data.", "published": "2024-02-19 20:40:48", "link": "http://arxiv.org/abs/2402.12530v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structure Guided Large Language Model for SQL Generation", "abstract": "Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.", "published": "2024-02-19 09:07:59", "link": "http://arxiv.org/abs/2402.13284v2", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic\n  Features for Distinguishing AI-Generated and Human-Written Texts", "abstract": "Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs\nhave been used to generate texts in different languages and for different\ntasks. Additionally, due to the participation of remarkable companies such as\nGoogle and OpenAI, LLMs are now more accessible, and people can easily use\nthem. However, an important issue is how we can detect AI-generated texts from\nhuman-written ones. In this article, we have investigated the problem of\nAI-generated text detection from two different aspects: semantics and syntax.\nFinally, we presented an AI model that can distinguish AI-generated texts from\nhuman-written ones with high accuracy on both multilingual and monolingual\ntasks using the M4 dataset. According to our results, using a semantic approach\nwould be more helpful for detection. However, there is a lot of room for\nimprovement in the syntactic approach, and it would be a good approach for\nfuture work.", "published": "2024-02-19 00:40:17", "link": "http://arxiv.org/abs/2402.14838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question\n  Answering and Clinical Reasoning", "abstract": "Recent advancements in Large Language Models (LLMs) and Large Multi-modal\nModels (LMMs) have shown potential in various medical applications, such as\nIntelligent Medical Diagnosis. Although impressive results have been achieved,\nwe find that existing benchmarks do not reflect the complexity of real medical\nreports and specialized in-depth reasoning capabilities. In this work, we\nintroduced RJUA-MedDQA, a comprehensive benchmark in the field of medical\nspecialization, which poses several challenges: comprehensively interpreting\nimgage content across diverse challenging layouts, possessing numerical\nreasoning ability to identify abnormal indicators and demonstrating clinical\nreasoning ability to provide statements of disease diagnosis, status and advice\nbased on medical contexts. We carefully design the data generation pipeline and\nproposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed\nat restoring textual and tabular content in medical report images. This method\nsubstantially enhances annotation efficiency, doubling the productivity of each\nannotator, and yields a 26.8% improvement in accuracy. We conduct extensive\nevaluations, including few-shot assessments of 5 LMMs which are capable of\nsolving Chinese medical QA tasks. To further investigate the limitations and\npotential of current LMMs, we conduct comparative experiments on a set of\nstrong LLMs by using image-text generated by ESRA method. We report the\nperformance of baselines and offer several observations: (1) The overall\nperformance of existing LMMs is still limited; however LMMs more robust to\nlow-quality and diverse-structured images compared to LLMs. (3) Reasoning\nacross context and image content present significant challenges. We hope this\nbenchmark helps the community make progress on these challenging tasks in\nmulti-modal medical document understanding and facilitate its application in\nhealthcare.", "published": "2024-02-19 06:57:02", "link": "http://arxiv.org/abs/2402.14840v1", "categories": ["cs.CL", "cs.AI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Text Diffusion with Reinforced Conditioning", "abstract": "Diffusion models have demonstrated exceptional capability in generating\nhigh-quality images, videos, and audio. Due to their adaptiveness in iterative\nrefinement, they provide a strong potential for achieving better\nnon-autoregressive sequence generation. However, existing text diffusion models\nstill fall short in their performance due to a challenge in handling the\ndiscreteness of language. This paper thoroughly analyzes text diffusion models\nand uncovers two significant limitations: degradation of self-conditioning\nduring training and misalignment between training and sampling. Motivated by\nour findings, we propose a novel Text Diffusion model called TREC, which\nmitigates the degradation with Reinforced Conditioning and the misalignment by\nTime-Aware Variance Scaling. Our extensive experiments demonstrate the\ncompetitiveness of TREC against autoregressive, non-autoregressive, and\ndiffusion baselines. Moreover, qualitative analysis shows its advanced ability\nto fully utilize the diffusion process in refining samples.", "published": "2024-02-19 09:24:02", "link": "http://arxiv.org/abs/2402.14843v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Purifying Large Language Models by Ensembling a Small Language Model", "abstract": "The emerging success of large language models (LLMs) heavily relies on\ncollecting abundant training data from external (untrusted) sources. Despite\nsubstantial efforts devoted to data cleaning and curation, well-constructed\nLLMs have been reported to suffer from copyright infringement, data poisoning,\nand/or privacy violations, which would impede practical deployment of LLMs. In\nthis study, we propose a simple and easily implementable method for purifying\nLLMs from the negative effects caused by uncurated data, namely, through\nensembling LLMs with benign and small language models (SLMs). Aside from\ntheoretical guarantees, we perform comprehensive experiments to empirically\nconfirm the efficacy of ensembling LLMs with SLMs, which can effectively\npreserve the performance of LLMs while mitigating issues such as copyright\ninfringement, data poisoning, and privacy violations.", "published": "2024-02-19 14:00:39", "link": "http://arxiv.org/abs/2402.14845v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models", "abstract": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.", "published": "2024-02-19 14:53:01", "link": "http://arxiv.org/abs/2402.14846v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Asynchronous and Segmented Bidirectional Encoding for NMT", "abstract": "With the rapid advancement of Neural Machine Translation (NMT), enhancing\ntranslation efficiency and quality has become a focal point of research.\nDespite the commendable performance of general models such as the Transformer\nin various aspects, they still fall short in processing long sentences and\nfully leveraging bidirectional contextual information. This paper introduces an\nimproved model based on the Transformer, implementing an asynchronous and\nsegmented bidirectional decoding strategy aimed at elevating translation\nefficiency and accuracy. Compared to traditional unidirectional translations\nfrom left-to-right or right-to-left, our method demonstrates heightened\nefficiency and improved translation quality, particularly in handling long\nsentences. Experimental results on the IWSLT2017 dataset confirm the\neffectiveness of our approach in accelerating translation and increasing\naccuracy, especially surpassing traditional unidirectional strategies in long\nsentence translation. Furthermore, this study analyzes the impact of sentence\nlength on decoding outcomes and explores the model's performance in various\nscenarios. The findings of this research not only provide an effective encoding\nstrategy for the NMT field but also pave new avenues and directions for future\nstudies.", "published": "2024-02-19 19:48:02", "link": "http://arxiv.org/abs/2402.14849v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Fusion of EHR in Structures and Semantics: Integrating\n  Clinical Records and Notes with Hypergraph and LLM", "abstract": "Electronic Health Records (EHRs) have become increasingly popular to support\nclinical decision-making and healthcare in recent decades. EHRs usually contain\nheterogeneous information, such as structural data in tabular form and\nunstructured data in textual notes. Different types of information in EHRs can\ncomplement each other and provide a more complete picture of the health status\nof a patient. While there has been a lot of research on representation learning\nof structured EHR data, the fusion of different types of EHR data (multimodal\nfusion) is not well studied. This is mostly because of the complex medical\ncoding systems used and the noise and redundancy present in the written notes.\nIn this work, we propose a new framework called MINGLE, which integrates both\nstructures and semantics in EHR effectively. Our framework uses a two-level\ninfusion strategy to combine medical concept semantics and clinical note\nsemantics into hypergraph neural networks, which learn the complex interactions\nbetween different types of data to generate visit representations for\ndownstream prediction. Experiment results on two EHR datasets, the public\nMIMIC-III and private CRADLE, show that MINGLE can effectively improve\npredictive performance by 11.83% relatively, enhancing semantic integration as\nwell as multimodal fusion for structural and textual EHR data.", "published": "2024-02-19 23:48:40", "link": "http://arxiv.org/abs/2403.08818v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Model for Mental Health: A Systematic Review", "abstract": "Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: 'mental health OR mental illness OR mental disorder OR\npsychiatry' AND 'large language models'. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the 'black box' nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area.", "published": "2024-02-19 17:58:41", "link": "http://arxiv.org/abs/2403.15401v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents", "abstract": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.", "published": "2024-02-19 18:00:30", "link": "http://arxiv.org/abs/2402.12326v2", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM\n  Agents", "abstract": "Large Language Models (LLMs) have increasingly been utilized in social\nsimulations, where they are often guided by carefully crafted instructions to\nstably exhibit human-like behaviors during simulations. Nevertheless, we doubt\nthe necessity of shaping agents' behaviors for accurate social simulations.\nInstead, this paper emphasizes the importance of spontaneous phenomena, wherein\nagents deeply engage in contexts and make adaptive decisions without explicit\ndirections. We explored spontaneous cooperation across three competitive\nscenarios and successfully simulated the gradual emergence of cooperation,\nfindings that align closely with human behavioral data. This approach not only\naids the computational social science community in bridging the gap between\nsimulations and real-world dynamics but also offers the AI community a novel\nmethod to assess LLMs' capability of deliberate reasoning.", "published": "2024-02-19 18:00:53", "link": "http://arxiv.org/abs/2402.12327v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA", "econ.GN", "q-fin.EC"], "primary_category": "cs.AI"}
{"title": "Unraveling Complex Data Diversity in Underwater Acoustic Target\n  Recognition through Convolution-based Mixture of Experts", "abstract": "Underwater acoustic target recognition is a difficult task owing to the\nintricate nature of underwater acoustic signals. The complex underwater\nenvironments, unpredictable transmission channels, and dynamic motion states\ngreatly impact the real-world underwater acoustic signals, and may even obscure\nthe intrinsic characteristics related to targets. Consequently, the data\ndistribution of underwater acoustic signals exhibits high intra-class\ndiversity, thereby compromising the accuracy and robustness of recognition\nsystems.To address these issues, this work proposes a convolution-based mixture\nof experts (CMoE) that recognizes underwater targets in a fine-grained manner.\nThe proposed technique introduces multiple expert layers as independent\nlearners, along with a routing layer that determines the assignment of experts\naccording to the characteristics of inputs. This design allows the model to\nutilize independent parameter spaces, facilitating the learning of complex\nunderwater signals with high intra-class diversity. Furthermore, this work\noptimizes the CMoE structure by balancing regularization and an optional\nresidual module. To validate the efficacy of our proposed techniques, we\nconducted detailed experiments and visualization analyses on three underwater\nacoustic databases across several acoustic features. The experimental results\ndemonstrate that our CMoE consistently achieves significant performance\nimprovements, delivering superior recognition accuracy when compared to\nexisting advanced methods.", "published": "2024-02-19 08:07:01", "link": "http://arxiv.org/abs/2402.11919v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the relationship between speech and hearing", "abstract": "We present a framework for experimentally linking speech production and\nhearing. Using this approach, we describe experimental results, that lead to\nthe concept that sounds made by different individuals and perceived to be the\nsame can be transformed into each other by a \"speech scale\". The speech scale\nis empirically determined using only speech data. We show the similarity of the\nspeech scale to the MEL scale of Stevens and Volkmann, which was derived only\nfrom hearing experiments. We thus experimentally link speech production and\nhearing.", "published": "2024-02-19 12:20:56", "link": "http://arxiv.org/abs/2402.12094v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language-Codec: Reducing the Gaps Between Discrete Codec Representation\n  and Speech Language Models", "abstract": "In recent years, large language models have achieved significant success in\ngenerative tasks (e.g., speech cloning and audio generation) related to speech,\naudio, music, and other signal domains. A crucial element of these models is\nthe discrete acoustic codecs, which serves as an intermediate representation\nreplacing the mel-spectrogram. However, there exist several gaps between\ndiscrete codecs and downstream speech language models. Specifically, 1) most\ncodec models are trained on only 1,000 hours of data, whereas most speech\nlanguage models are trained on 60,000 hours; 2) Achieving good reconstruction\nperformance requires the utilization of numerous codebooks, which increases the\nburden on downstream speech language models; 3) The initial channel of the\ncodebooks contains excessive information, making it challenging to directly\ngenerate acoustic tokens from weakly supervised signals such as text in\ndownstream tasks. Consequently, leveraging the characteristics of speech\nlanguage models, we propose Language-Codec. In the Language-Codec, we introduce\na Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with\nimproved Fourier transform structures and larger training datasets to address\nthe aforementioned gaps. We compare our method with competing audio compression\nalgorithms and observe significant outperformance across extensive evaluations.\nFurthermore, we also validate the efficiency of the Language-Codec on\ndownstream speech language models. The source code and pre-trained models can\nbe accessed at https://github.com/jishengpeng/languagecodec .", "published": "2024-02-19 15:12:12", "link": "http://arxiv.org/abs/2402.12208v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic\n  Forgetting", "abstract": "We are motivated primarily by the adaptation of text-to-speech synthesis\nmodels; however we argue that more generic parameter-efficient fine-tuning\n(PEFT) is an appropriate framework to do such adaptation. Nevertheless,\ncatastrophic forgetting remains an issue with PEFT, damaging the pre-trained\nmodel's inherent capabilities. We demonstrate that existing Bayesian learning\ntechniques can be applied to PEFT to prevent catastrophic forgetting as long as\nthe parameter shift of the fine-tuned layers can be calculated differentiably.\nIn a principled series of experiments on language modeling and speech synthesis\ntasks, we utilize established Laplace approximations, including diagonal and\nKronecker-factored approaches, to regularize PEFT with the low-rank adaptation\n(LoRA) and compare their performance in pre-training knowledge preservation.\nOur results demonstrate that catastrophic forgetting can be overcome by our\nmethods without degrading the fine-tuning performance, and using the\nKronecker-factored approximation produces a better preservation of the\npre-training knowledge than the diagonal ones.", "published": "2024-02-19 15:26:19", "link": "http://arxiv.org/abs/2402.12220v3", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Parameter Efficient Finetuning for Speech Emotion Recognition and Domain\n  Adaptation", "abstract": "Foundation models have shown superior performance for speech emotion\nrecognition (SER). However, given the limited data in emotion corpora,\nfinetuning all parameters of large pre-trained models for SER can be both\nresource-intensive and susceptible to overfitting. This paper investigates\nparameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are\nsystematically studied for both classification of discrete emotion categories\nand prediction of dimensional emotional attributes. The results demonstrate\nthat the combination of PEFT methods surpasses full finetuning with a\nsignificant reduction in the number of trainable parameters. Furthermore, a\ntwo-stage adaptation strategy is proposed to adapt models trained on acted\nemotion data, which is more readily available, to make the model more adept at\ncapturing natural emotional expressions. Both intra- and cross-corpus\nexperiments validate the efficacy of the proposed approach in enhancing the\nperformance on both the source and target domains.", "published": "2024-02-19 00:21:07", "link": "http://arxiv.org/abs/2402.11747v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-power SNN-based audio source localisation using a Hilbert Transform\n  spike encoding scheme", "abstract": "Sound source localisation is used in many consumer devices, to isolate audio\nfrom individual speakers and reject noise. Localization is frequently\naccomplished by ``beamforming'', which combines phase-shifted audio streams to\nincrease power from chosen source directions, under a known microphone array\ngeometry. Dense band-pass filters are often needed to obtain narrowband signal\ncomponents from wideband audio. These approaches achieve high accuracy, but\nnarrowband beamforming is computationally demanding, and not ideal for\nlow-power IoT devices. We demonstrate a novel method for sound source\nlocalisation on arbitrary microphone arrays, designed for efficient\nimplementation in ultra-low-power spiking neural networks (SNNs). We use a\nHilbert transform to avoid dense band-pass filters, and introduce a new\nevent-based encoding method that captures the phase of the complex analytic\nsignal. Our approach achieves state-of-the-art accuracy for SNN methods,\ncomparable with traditional non-SNN super-resolution beamforming. We deploy our\nmethod to low-power SNN inference hardware, with much lower power consumption\nthan super-resolution methods. We demonstrate that signal processing approaches\nco-designed with spiking neural network implementations can achieve much\nimproved power efficiency. Our new Hilbert-transform-based method for\nbeamforming can also improve the efficiency of traditional DSP-based signal\nprocessing.", "published": "2024-02-19 00:21:13", "link": "http://arxiv.org/abs/2402.11748v3", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease\n  Detection", "abstract": "Alzheimer's disease is a common cognitive disorder in the elderly. Early and\naccurate diagnosis of Alzheimer's disease (AD) has a major impact on the\nprogress of research on dementia. At present, researchers have used machine\nlearning methods to detect Alzheimer's disease from the speech of participants.\nHowever, the recognition accuracy of current methods is unsatisfactory, and\nmost of them focus on using low-dimensional handcrafted features to extract\nrelevant information from audios. This paper proposes an Alzheimer's disease\ndetection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In\naddition, by replacing the loss function with the Soft-Weighted CrossEntropy\nloss function, we achieved 85.45\\% recognition accuracy on the same test\ndataset.", "published": "2024-02-19 08:18:52", "link": "http://arxiv.org/abs/2402.11931v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Multimodal Emotion Recognition from Raw Audio with Sinc-convolution", "abstract": "Speech Emotion Recognition (SER) is still a complex task for computers with\naverage recall rates usually about 70% on the most realistic datasets. Most SER\nsystems use hand-crafted features extracted from audio signal such as energy,\nzero crossing rate, spectral information, prosodic, mel frequency cepstral\ncoefficient (MFCC), and so on. More recently, using raw waveform for training\nneural network is becoming an emerging trend. This approach is advantageous as\nit eliminates the feature extraction pipeline. Learning from time-domain signal\nhas shown good results for tasks such as speech recognition, speaker\nverification etc. In this paper, we utilize Sinc-convolution layer, which is an\nefficient architecture for preprocessing raw speech waveform for emotion\nrecognition, to extract acoustic features from raw audio signals followed by a\nlong short-term memory (LSTM). We also incorporate linguistic features and\nappend a dialogical emotion decoding (DED) strategy. Our approach achieves a\nweighted accuracy of 85.1\\% in four class emotion on the Interactive Emotional\nDyadic Motion Capture (IEMOCAP) dataset.", "published": "2024-02-19 08:49:09", "link": "http://arxiv.org/abs/2402.11954v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Significance of Chirp MFCC as a Feature in Speech and Audio Applications", "abstract": "A novel feature, based on the chirp z-transform, that offers an improved\nrepresentation of the underlying true spectrum is proposed. This feature, the\nchirp MFCC, is derived by computing the Mel frequency cepstral coefficients\nfrom the chirp magnitude spectrum, instead of the Fourier transform magnitude\nspectrum. The theoretical foundations for the proposal, and the experimental\nvalidation using product of likelihood Gaussians, to show the improved class\nseparation offered by the proposed chirp MFCC, when compared with vanilla MFCC\nare discussed. Further, real world evaluation of the feature is performed using\nthree diverse tasks, namely, speech-music classification, speaker\nidentification, and speech commands recognition. It is shown in all three tasks\nthat the proposed chirp MFCC offers considerable improvements.", "published": "2024-02-19 15:50:32", "link": "http://arxiv.org/abs/2402.12239v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "SECP: A Speech Enhancement-Based Curation Pipeline For Scalable\n  Acquisition Of Clean Speech", "abstract": "As more speech technologies rely on a supervised deep learning approach with\nclean speech as the ground truth, a methodology to onboard said speech at scale\nis needed. However, this approach needs to minimize the dependency on human\nlistening and annotation, only requiring a human-in-the-loop when needed. In\nthis paper, we address this issue by outlining Speech Enhancement-based\nCuration Pipeline (SECP) which serves as a framework to onboard clean speech.\nThis clean speech can then train a speech enhancement model, which can further\nrefine the original dataset and thus close the iterative loop. By running two\niterative rounds, we observe that enhanced output used as ground truth does not\ndegrade model performance according to $\\Delta_{PESQ}$, a metric used in this\npaper. We also show through comparative mean opinion score (CMOS) based\nsubjective tests that the highest and lowest bound of refined data is\nperceptually better than the original data.", "published": "2024-02-19 19:38:37", "link": "http://arxiv.org/abs/2402.12482v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
