{"title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models", "abstract": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 11 LLMs\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial domain but also provides a robust framework\nfor assessing their performance and user satisfaction.", "published": "2024-10-17 22:03:52", "link": "http://arxiv.org/abs/2410.14059v3", "categories": ["q-fin.CP", "cs.CE", "cs.CL"], "primary_category": "q-fin.CP"}
{"title": "Delegated portfolio management with random default", "abstract": "We are considering the problem of optimal portfolio delegation between an\ninvestor and a portfolio manager under a random default time. We focus on a\nnovel variation of the Principal-Agent problem adapted to this framework. We\naddress the challenge of an uncertain investment horizon caused by an exogenous\nrandom default time, after which neither the agent nor the principal can access\nthe market. This uncertainty introduces significant complexities in analyzing\nthe problem, requiring distinct mathematical approaches for two cases: when the\nrandom default time falls within the initial time frame [0,T] and when it\nextends beyond this period. We develop a theoretical framework to model the\nstochastic dynamics of the investment process, incorporating the random default\ntime. We then analyze the portfolio manager's investment decisions and\ncompensation mechanisms for both scenarios. In the first case, where the\ndefault time could be unbounded, we apply traditional results from Backward\nStochastic Differential Equations (BSDEs) and control theory to address the\nagent problem. In the second case, where the default time is within the\ninterval [0,T], the problem becomes more intricate due to the degeneracy of the\nBSDE's driver. For both scenarios, we demonstrate that the contracting problem\ncan be resolved by examining the existence of solutions to integro-partial\nHamilton-Jacobi-Bellman (HJB) equations in both situations. We develop a\ndeep-learning algorithm to solve the problem in high-dimension with no access\nto the optimizer of the Hamiltonian function.", "published": "2024-10-17 00:27:46", "link": "http://arxiv.org/abs/2410.13103v1", "categories": ["q-fin.MF", "math.OC"], "primary_category": "q-fin.MF"}
{"title": "Competitive equilibria in trading", "abstract": "This is the third paper in a series concerning the game-theoretic aspects of\nposition-building while in competition. The first paper set forth foundations\nand laid out the essential goal, which is to minimize implementation costs in\nlight of how other traders are likely to trade. The majority of results in that\npaper center on the two traders in competition and equilibrium results are\npresented. The second paper, introduces computational methods based on Fourier\nSeries which allows the introduction of a broad range of constraints into the\noptimal strategies derived. The current paper returns to the unconstrained case\nand provides a complete solution to finding equilibrium strategies in\ncompetition and handles completely arbitrary situations. As a result we present\na detailed analysis of the value (or not) of trade centralization and we show\nthat firms who naively centralize trades do not generally benefit and\nsometimes, in fact, lose. On the other hand, firms that strategically\ncentralize their trades generally will be able to benefit.", "published": "2024-10-17 14:19:47", "link": "http://arxiv.org/abs/2410.13583v3", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Concentrated Superelliptical Market Maker", "abstract": "An automated market maker where the price can cross the zero bound into the\nnegative price domain with applications in electricity, energy, and derivatives\nmarkets is presented. A unique feature involves the ability to swap both\nnegatively and positively priced assets between one another, which unlike\ntraditional markets requires a numeraire in the form of a currency. Model\nextensions to skew and concentrate liquidity are shown. The liquidity\nfingerprint, payoff, and invariant are compared to the Black-Scholes covered\ncall and the Logarithmic Market Scoring Rule invariants.", "published": "2024-10-17 06:39:34", "link": "http://arxiv.org/abs/2410.13265v2", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Better to Ask in English: Evaluation of Large Language Models on\n  English, Low-resource and Cross-Lingual Settings", "abstract": "Large Language Models (LLMs) are trained on massive amounts of data, enabling\ntheir application across diverse domains and tasks. Despite their remarkable\nperformance, most LLMs are developed and evaluated primarily in English.\nRecently, a few multi-lingual LLMs have emerged, but their performance in\nlow-resource languages, especially the most spoken languages in South Asia, is\nless explored. To address this gap, in this study, we evaluate LLMs such as\nGPT-4, Llama 2, and Gemini to analyze their effectiveness in English compared\nto other low-resource languages from South Asia (e.g., Bangla, Hindi, and\nUrdu). Specifically, we utilized zero-shot prompting and five different prompt\nsettings to extensively investigate the effectiveness of the LLMs in\ncross-lingual translated prompts. The findings of the study suggest that GPT-4\noutperformed Llama 2 and Gemini in all five prompt settings and across all\nlanguages. Moreover, all three LLMs performed better for English language\nprompts than other low-resource language prompts. This study extensively\ninvestigates LLMs in low-resource language contexts to highlight the\nimprovements required in LLMs and language-specific resources to develop more\ngenerally purposed NLP applications.", "published": "2024-10-17 02:12:30", "link": "http://arxiv.org/abs/2410.13153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLM-Mod: Small Language Models Surpass LLMs at Content Moderation", "abstract": "Large language models (LLMs) have shown promise in many natural language\nunderstanding tasks, including content moderation. However, these models can be\nexpensive to query in real-time and do not allow for a community-specific\napproach to content moderation. To address these challenges, we explore the use\nof open-source small language models (SLMs) for community-specific content\nmoderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by\ncomparing their performance against much larger open- and closed-sourced models\nin both a zero-shot and few-shot setting. Using 150K comments from 15 popular\nReddit communities, we find that SLMs outperform zero-shot LLMs at content\nmoderation -- 11.5% higher accuracy and 25.7% higher recall on average across\nall communities. Moreover, few-shot in-context learning leads to only a\nmarginal increase in the performance of LLMs, still lacking compared to SLMs.\nWe further show the promise of cross-community content moderation, which has\nimplications for new communities and the development of cross-platform\nmoderation techniques. Finally, we outline directions for future work on\nlanguage model based content moderation. Code and models can be found at\nhttps://github.com/AGoyal0512/SLM-Mod.", "published": "2024-10-17 02:16:37", "link": "http://arxiv.org/abs/2410.13155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaSwitch: Adaptive Switching between Small and Large Agents for\n  Effective Cloud-Local Collaborative Learning", "abstract": "Recent advancements in large language models (LLMs) have been remarkable.\nUsers face a choice between using cloud-based LLMs for generation quality and\ndeploying local-based LLMs for lower computational cost. The former option is\ntypically costly and inefficient, while the latter usually fails to deliver\nsatisfactory performance for reasoning steps requiring deliberate thought\nprocesses. In this work, we propose a novel LLM utilization paradigm that\nfacilitates the collaborative operation of large cloud-based LLMs and smaller\nlocal-deployed LLMs. Our framework comprises two primary modules: the local\nagent instantiated with a relatively smaller LLM, handling less complex\nreasoning steps, and the cloud agent equipped with a larger LLM, managing more\nintricate reasoning steps. This collaborative processing is enabled through an\nadaptive mechanism where the local agent introspectively identifies errors and\nproactively seeks assistance from the cloud agent, thereby effectively\nintegrating the strengths of both locally-deployed and cloud-based LLMs,\nresulting in significant enhancements in task completion performance and\nefficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from\nmathematical reasoning and complex question answering, using various types of\nLLMs to instantiate the local and cloud agents. The empirical results show that\nAdaSwitch effectively improves the performance of the local agent, and\nsometimes achieves competitive results compared to the cloud agent while\nutilizing much less computational overhead.", "published": "2024-10-17 03:07:37", "link": "http://arxiv.org/abs/2410.13181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Router-Tuning: A Simple and Effective Approach for Enabling\n  Dynamic-Depth in Transformers", "abstract": "Traditional transformer models often allocate a fixed amount of computational\nresources to every input token, leading to inefficient and unnecessary\ncomputation. To address this, the Mixture of Depths (MoD) was introduced to\ndynamically adjust the computational depth by skipping less important layers.\nDespite its promise, current MoD approaches remain under-explored and face two\nmain challenges: (1) high training costs due to the need to train the entire\nmodel along with the routers that determine which layers to skip, and (2) the\nrisk of performance degradation when important layers are bypassed. In response\nto the first issue, we propose Router-Tuning, a method that fine-tunes only the\nrouter on a small dataset, drastically reducing the computational overhead\nassociated with full model training. For the second challenge, we propose\nMindSkip, which deploys Attention with Dynamic Depths. This method preserves\nthe model's performance while significantly enhancing computational and memory\nefficiency. Extensive experiments demonstrate that our approach delivers\ncompetitive results while dramatically improving the computation efficiency,\ne.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at\nhttps://github.com/CASE-Lab-UMD/Router-Tuning.", "published": "2024-10-17 03:23:50", "link": "http://arxiv.org/abs/2410.13184v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented\n  Generation with Large Language Models", "abstract": "The integration of documents generated by LLMs themselves (Self-Docs)\nalongside retrieved documents has emerged as a promising strategy for\nretrieval-augmented generation systems. However, previous research primarily\nfocuses on optimizing the use of Self-Docs, with their inherent properties\nremaining underexplored. To bridge this gap, we first investigate the overall\neffectiveness of Self-Docs, identifying key factors that shape their\ncontribution to RAG performance (RQ1). Building on these insights, we develop a\ntaxonomy grounded in Systemic Functional Linguistics to compare the influence\nof various Self-Docs categories (RQ2) and explore strategies for combining them\nwith external sources (RQ3). Our findings reveal which types of Self-Docs are\nmost beneficial and offer practical guidelines for leveraging them to achieve\nsignificant improvements in knowledge-intensive question answering tasks.", "published": "2024-10-17 03:38:54", "link": "http://arxiv.org/abs/2410.13192v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Geometry of Numerical Reasoning: Language Models Compare Numeric\n  Properties in Linear Subspaces", "abstract": "This paper investigates whether large language models (LLMs) utilize\nnumerical attributes encoded in a low-dimensional subspace of the embedding\nspace when answering questions involving numeric comparisons, e.g., Was\nCristiano born before Messi? We first identified, using partial least squares\nregression, these subspaces, which effectively encode the numerical attributes\nassociated with the entities in comparison prompts. Further, we demonstrate\ncausality, by intervening in these subspaces to manipulate hidden states,\nthereby altering the LLM's comparison outcomes. Experiments conducted on three\ndifferent LLMs showed that our results hold across different numerical\nattributes, indicating that LLMs utilize the linearly encoded information for\nnumerical reasoning.", "published": "2024-10-17 03:44:11", "link": "http://arxiv.org/abs/2410.13194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BQA: Body Language Question Answering Dataset for Video Large Language\n  Models", "abstract": "A large part of human communication relies on nonverbal cues such as facial\nexpressions, eye contact, and body language. Unlike language or sign language,\nsuch nonverbal communication lacks formal rules, requiring complex reasoning\nbased on commonsense understanding. Enabling current Video Large Language\nModels (VideoLLMs) to accurately interpret body language is a crucial\nchallenge, as human unconscious actions can easily cause the model to\nmisinterpret their intent. To address this, we propose a dataset, BQA, a body\nlanguage question answering dataset, to validate whether the model can\ncorrectly interpret emotions from short clips of body language comprising 26\nemotion labels of videos of body language. We evaluated various VideoLLMs on\nBQA and revealed that understanding body language is challenging, and our\nanalyses of the wrong answers by VideoLLMs show that certain VideoLLMs made\nsignificantly biased answers depending on the age group and ethnicity of the\nindividuals in the video. The dataset is available.", "published": "2024-10-17 04:19:26", "link": "http://arxiv.org/abs/2410.13206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proof Flow: Preliminary Study on Generative Flow Network Language Model\n  Tuning for Formal Reasoning", "abstract": "Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"", "published": "2024-10-17 05:10:12", "link": "http://arxiv.org/abs/2410.13224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Web Agents with World Models: Learning and Leveraging Environment\n  Dynamics in Web Navigation", "abstract": "Large language models (LLMs) have recently gained much attention in building\nautonomous agents. However, the performance of current LLM-based web agents in\nlong-horizon tasks is far from optimal, often yielding errors such as\nrepeatedly buying a non-refundable flight ticket. By contrast, humans can avoid\nsuch an irreversible mistake, as we have an awareness of the potential outcomes\n(e.g., losing money) of our actions, also known as the \"world model\". Motivated\nby this, our study first starts with preliminary analyses, confirming the\nabsence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,\netc.). Then, we present a World-model-augmented (WMA) web agent, which\nsimulates the outcomes of its actions for better decision-making. To overcome\nthe challenges in training LLMs as world models predicting next observations,\nsuch as repeated elements across observations and long HTML inputs, we propose\na transition-focused observation abstraction, where the prediction objectives\nare free-form natural language descriptions exclusively highlighting important\nstate differences between time steps. Experiments on WebArena and Mind2Web show\nthat our world models improve agents' policy selection without training and\ndemonstrate our agents' cost- and time-efficiency compared to recent\ntree-search-based agents.", "published": "2024-10-17 05:37:00", "link": "http://arxiv.org/abs/2410.13232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does Knowledge Selection Help Retrieval Augmented Generation?", "abstract": "Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection\nremains less clear. In this paper, we perform a comprehensive analysis of how\nknowledge selection influences downstream generation performance in RAG\nsystems. By simulating different retrieval and selection conditions through a\ncontrolled mixture of gold and distractor knowledge, we assess the impact of\nthese factors on generation outcomes. Our findings indicate that the downstream\ngenerator model's capability, as well as the complexity of the task and\ndataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing a limited additional benefit when a strong generator model\nis used on clear, well-defined tasks. For weaker generator models or more\nambiguous tasks and datasets, the knowledge F1 score becomes a critical factor,\nand the knowledge selector plays a more prominent role in improving overall\nperformance.", "published": "2024-10-17 06:30:55", "link": "http://arxiv.org/abs/2410.13258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in\n  Terms of Human Language Acquisition", "abstract": "We examine the language capabilities of language models (LMs) from the\ncritical perspective of human language acquisition. Building on classical\nlanguage development theories, we propose a three-stage framework to assess the\nabilities of LMs, ranging from preliminary word understanding to complex\ngrammar and complex logical reasoning. Using this framework, we evaluate the\ngenerative capacities of LMs using methods from linguistic research. Results\nindicate that although recent LMs outperform earlier models in overall\nperformance, their developmental trajectory does not strictly follow the path\nof human language acquisition. Notably, in generation tasks, LMs are more\nsimilar to human performance in areas where information is easier to extract\nfrom the corpus, such as average word length, clauses, and auxiliary verbs.\nNewer LMs did not exhibit significant progress in terms of specific dimensions,\nsuch as clauses and auxiliary verbs, where the variation across corpora is\nrelatively limited. Register theory offers a plausible explanation for these\nobservations, suggesting that the linguistic features of the training data have\na substantial impact on the models' abilities.", "published": "2024-10-17 06:31:49", "link": "http://arxiv.org/abs/2410.13259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning", "abstract": "Large language models (LLMs) serve as giant information stores, often\nincluding personal or copyrighted data, and retraining them from scratch is not\na viable option. This has led to the development of various fast, approximate\nunlearning techniques to selectively remove knowledge from LLMs. Prior research\nhas largely focused on minimizing the probabilities of specific token sequences\nby reversing the language modeling objective. However, these methods still\nleave LLMs vulnerable to adversarial attacks that exploit indirect references.\nIn this work, we examine the limitations of current unlearning techniques in\neffectively erasing a particular type of indirect prompt: multi-hop queries.\nOur findings reveal that existing methods fail to completely remove multi-hop\nknowledge when one of the intermediate hops is unlearned. To address this\nissue, we propose MUNCH, a simple uncertainty-based approach that breaks down\nmulti-hop queries into subquestions and leverages the uncertainty of the\nunlearned model in final decision-making. Empirical results demonstrate the\neffectiveness of our framework, and MUNCH can be easily integrated with\nexisting unlearning techniques, making it a flexible and useful solution for\nenhancing unlearning processes.", "published": "2024-10-17 07:00:15", "link": "http://arxiv.org/abs/2410.13274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs", "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity hinders efficiency and scalability, especially for\nlong-context processing. A promising approach is to leverage sparsity in\nattention. However, existing sparsity-based solutions predominantly rely on\npredefined patterns or heuristics at the attention head level, struggling to\nadapt dynamically to different contexts efficiently.\n  We propose SeerAttention, a simple yet effective attention mechanism that\ndirectly learns the block-level attention sparsity from the LLM itself.\nInspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention\naugments the conventional attention with a learnable gate that selectively\nactivates important blocks within the attention map. Specifically, the gate\nfirst pools the query (Q) and key (K) tensors along the sequence dimension and\nprocesses them through learnable linear layers. The resulting matrices are then\nmultiplied together to produce the gating scores, which are used to predict\nblock-level attention sparsity. Combined with our block-sparse FlashAttention\nkernel, SeerAttention can achieve significant speedup on GPUs. When applied to\npre-trained LLMs, SeerAttention only requires training the gate parameters in a\nlightweight self-distillation manner, allowing rapid convergence. Our\nevaluation results demonstrate that SeerAttention achieves better model\naccuracy and lower latency for long-context pre-filling compared to prior\nmethods. Code is available at: https://github.com/microsoft/SeerAttention", "published": "2024-10-17 07:07:09", "link": "http://arxiv.org/abs/2410.13276v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla", "abstract": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.", "published": "2024-10-17 07:15:15", "link": "http://arxiv.org/abs/2410.13281v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Biases to Embrace Diversity: A Comprehensive Annotation\n  Benchmark for Toxic Language", "abstract": "This study introduces a prescriptive annotation benchmark grounded in\nhumanities research to ensure consistent, unbiased labeling of offensive\nlanguage, particularly for casual and non-mainstream language uses. We\ncontribute two newly annotated datasets that achieve higher inter-annotator\nagreement between human and language model (LLM) annotations compared to\noriginal datasets based on descriptive instructions. Our experiments show that\nLLMs can serve as effective alternatives when professional annotators are\nunavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated\ndata outperform models trained on larger, single-source human-annotated\ndatasets. These findings highlight the value of structured guidelines in\nreducing subjective variability, maintaining performance with limited data, and\nembracing language diversity.\n  Content Warning: This article only analyzes offensive language for academic\npurposes. Discretion is advised.", "published": "2024-10-17 08:10:24", "link": "http://arxiv.org/abs/2410.13313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention\n  Classification", "abstract": "Citation intention Classification (CIC) tools classify citations by their\nintention (e.g., background, motivation) and assist readers in evaluating the\ncontribution of scientific literature. Prior research has shown that pretrained\nlanguage models (PLMs) such as SciBERT can achieve state-of-the-art performance\non CIC benchmarks. PLMs are trained via self-supervision tasks on a large\ncorpus of general text and can quickly adapt to CIC tasks via moderate\nfine-tuning on the corresponding dataset. Despite their advantages, PLMs can\neasily overfit small datasets during fine-tuning. In this paper, we propose a\nmulti-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset\nof primary interest together with multiple auxiliary CIC datasets to take\nadvantage of additional supervision signals. We develop a data-driven task\nrelation learning (TRL) method that controls the contribution of auxiliary\ndatasets to avoid negative transfer and expensive hyper-parameter tuning. We\nconduct experiments on three CIC datasets and show that fine-tuning with\nadditional datasets can improve the PLMs' generalization performance on the\nprimary dataset. PLMs fine-tuned with our proposed framework outperform the\ncurrent state-of-the-art models by 7% to 11% on small datasets while aligning\nwith the best-performing model on a large dataset.", "published": "2024-10-17 08:45:02", "link": "http://arxiv.org/abs/2410.13332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document\n  Retrieval", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving\nand incorporating relevant external knowledge. However, traditional\nretrieve-and-generate processes may not be optimized for real-world scenarios,\nwhere queries might require multiple retrieval steps or none at all. In this\npaper, we propose a Probing-RAG, which utilizes the hidden state\nrepresentations from the intermediate layers of language models to adaptively\ndetermine the necessity of additional retrievals for a given query. By\nemploying a pre-trained prober, Probing-RAG effectively captures the model's\ninternal cognition, enabling reliable decision-making about retrieving external\ndocuments. Experimental results across five open-domain QA datasets demonstrate\nthat Probing-RAG outperforms previous methods while reducing the number of\nredundant retrieval steps.", "published": "2024-10-17 08:48:54", "link": "http://arxiv.org/abs/2410.13339v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Judgment of Learning: A Human Ability Beyond Generative Artificial\n  Intelligence", "abstract": "Large language models (LLMs) increasingly mimic human cognition in various\nlanguage-based tasks. However, their capacity for metacognition - particularly\nin predicting memory performance - remains unexplored. Here, we introduce a\ncross-agent prediction model to assess whether ChatGPT-based LLMs align with\nhuman judgments of learning (JOL), a metacognitive measure where individuals\npredict their own future memory performance. We tested humans and LLMs on pairs\nof sentences, one of which was a garden-path sentence - a sentence that\ninitially misleads the reader toward an incorrect interpretation before\nrequiring reanalysis. By manipulating contextual fit (fitting vs. unfitting\nsentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM\nand human JOL. Our results revealed that while human JOL reliably predicted\nactual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,\nand GPT-4o) demonstrated comparable predictive accuracy. This discrepancy\nemerged regardless of whether sentences appeared in fitting or unfitting\ncontexts. These findings indicate that, despite LLMs' demonstrated capacity to\nmodel human cognition at the object-level, they struggle at the meta-level,\nfailing to capture the variability in individual memory predictions. By\nidentifying this shortcoming, our study underscores the need for further\nrefinements in LLMs' self-monitoring abilities, which could enhance their\nutility in educational settings, personalized learning, and human-AI\ninteractions. Strengthening LLMs' metacognitive performance may reduce the\nreliance on human oversight, paving the way for more autonomous and seamless\nintegration of AI into tasks requiring deeper cognitive awareness.", "published": "2024-10-17 09:42:30", "link": "http://arxiv.org/abs/2410.13392v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs", "abstract": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.", "published": "2024-10-17 09:45:32", "link": "http://arxiv.org/abs/2410.13394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Grounded Analysis of Language Models using Shapley Head\n  Values", "abstract": "Understanding how linguistic knowledge is encoded in language models is\ncrucial for improving their generalisation capabilities. In this paper, we\ninvestigate the processing of morphosyntactic phenomena, by leveraging a\nrecently proposed method for probing language models via Shapley Head Values\n(SHVs). Using the English language BLiMP dataset, we test our approach on two\nwidely used models, BERT and RoBERTa, and compare how linguistic constructions\nsuch as anaphor agreement and filler-gap dependencies are handled. Through\nquantitative pruning and qualitative clustering analysis, we demonstrate that\nattention heads responsible for processing related linguistic phenomena cluster\ntogether. Our results show that SHV-based attributions reveal distinct patterns\nacross both models, providing insights into how language models organize and\nprocess linguistic information. These findings support the hypothesis that\nlanguage models learn subnetworks corresponding to linguistic theory, with\npotential implications for cross-linguistic model analysis and interpretability\nin Natural Language Processing (NLP).", "published": "2024-10-17 09:48:08", "link": "http://arxiv.org/abs/2410.13396v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task", "abstract": "This paper describes NLIP Lab's multilingual machine translation system for\nthe WAT24 shared task on multilingual Indic MT task for 22 scheduled languages\nbelonging to 4 language families. We explore pre-training for Indic languages\nusing alignment agreement objectives. We utilize bi-lingual dictionaries to\nsubstitute words from source sentences. Furthermore, we fine-tuned language\ndirection-specific multilingual translation models using small and high-quality\nseed data. Our primary submission is a 243M parameters multilingual translation\nmodel covering 22 Indic languages. In the IN22-Gen benchmark, we achieved an\naverage chrF++ score of 46.80 and 18.19 BLEU score for the En-Indic direction.\nIn the Indic-En direction, we achieved an average chrF++ score of 56.34 and\n30.82 BLEU score. In the In22-Conv benchmark, we achieved an average chrF++\nscore of 43.43 and BLEU score of 16.58 in the En-Indic direction, and in the\nIndic-En direction, we achieved an average of 52.44 and 29.77 for chrF++ and\nBLEU respectively. Our model\\footnote{Our code and models are available at\n\\url{https://github.com/maharajbrahma/WAT2024-MultiIndicMT}} is competitive\nwith IndicTransv1 (474M parameter model).", "published": "2024-10-17 11:18:23", "link": "http://arxiv.org/abs/2410.13443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedINST: Meta Dataset of Biomedical Instructions", "abstract": "The integration of large language model (LLM) techniques in the field of\nmedical analysis has brought about significant advancements, yet the scarcity\nof large, diverse, and well-annotated datasets remains a major challenge.\nMedical data and tasks, which vary in format, size, and other parameters,\nrequire extensive preprocessing and standardization for effective use in\ntraining LLMs. To address these challenges, we introduce MedINST, the Meta\nDataset of Biomedical Instructions, a novel multi-domain, multi-task\ninstructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over\n7 million training samples, making it the most comprehensive biomedical\ninstruction dataset to date. Using MedINST as the meta dataset, we curate\nMedINST32, a challenging benchmark with different task difficulties aiming to\nevaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and\nevaluate on MedINST32, showcasing enhanced cross-task generalization.", "published": "2024-10-17 11:38:54", "link": "http://arxiv.org/abs/2410.13458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IterSelectTune: An Iterative Training Framework for Efficient\n  Instruction-Tuning Data Selection", "abstract": "As large language models (LLMs) continue to advance, instruction tuning has\nbecome critical for improving their ability to generate accurate and\ncontextually appropriate responses. Although numerous instruction-tuning\ndatasets have been developed to enhance LLM performance, selecting high-quality\ninstruction data from large source datasets typically demands significant human\neffort. In this work, we introduce $\\textbf{IterSelectTune}$, an efficient,\ncost-effective iterative training policy for selecting high-quality instruction\ndata with no human involvement and limited reliance on GPT-4. By fine-tuning on\napproximately 20\\% of the source data, our method consistently outperforms\nmodels fine-tuned on the full dataset across multiple benchmarks and public\ntest datasets. These results highlight the effectiveness of our approach in\nenhancing LLM performance while reducing the computational resources required\nfor instruction tuning.", "published": "2024-10-17 11:48:57", "link": "http://arxiv.org/abs/2410.13464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Repetition Neurons: How Do Language Models Produce Repetitions?", "abstract": "This paper introduces repetition neurons, regarded as skill neurons\nresponsible for the repetition problem in text generation tasks. These neurons\nare progressively activated more strongly as repetition continues, indicating\nthat they perceive repetition as a task to copy the previous context\nrepeatedly, similar to in-context learning. We identify these repetition\nneurons by comparing activation values before and after the onset of repetition\nin texts generated by recent pre-trained language models. We analyze the\nrepetition neurons in three English and one Japanese pre-trained language\nmodels and observe similar patterns across them.", "published": "2024-10-17 12:43:47", "link": "http://arxiv.org/abs/2410.13497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable\n  Data Rewards", "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nmitigating hallucinations in Large Language Models (LLMs) by retrieving\nknowledge from external resources. To adapt LLMs for the RAG systems, current\napproaches use instruction tuning to optimize LLMs, improving their ability to\nutilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\non equipping LLMs to handle diverse RAG tasks using different instructions.\nHowever, it trains RAG modules to overfit training signals and overlooks the\nvarying data preferences among agents within the RAG system. In this paper, we\npropose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\nsystems by aligning data preferences between different RAG modules. DDR works\nby collecting the rewards to optimize each agent in the RAG system with the\nrollout method, which prompts agents to sample some potential responses as\nperturbations, evaluates the impact of these perturbations on the whole RAG\nsystem, and subsequently optimizes the agent to produce outputs that improve\nthe performance of the RAG system. Our experiments on various\nknowledge-intensive tasks demonstrate that DDR significantly outperforms the\nSFT method, particularly for LLMs with smaller-scale parameters that depend\nmore on the retrieved knowledge. Additionally, DDR exhibits a stronger\ncapability to align the data preference between RAG modules. The DDR method\nmakes the generation module more effective in extracting key information from\ndocuments and mitigating conflicts between parametric memory and external\nknowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.", "published": "2024-10-17 12:53:29", "link": "http://arxiv.org/abs/2410.13509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Fact Retrieval in PLMs through Truthfulness", "abstract": "Pre-trained Language Models (PLMs) encode various facts about the world at\ntheir pre-training phase as they are trained to predict the next or missing\nword in a sentence. There has a been an interest in quantifying and improving\nthe amount of facts that can be extracted from PLMs, as they have been\nenvisioned to act as soft knowledge bases, which can be queried in natural\nlanguage. Different approaches exist to enhance fact retrieval from PLM. Recent\nwork shows that the hidden states of PLMs can be leveraged to determine the\ntruthfulness of the PLMs' inputs. Leveraging this finding to improve factual\nknowledge retrieval remains unexplored. In this work, we investigate the use of\na helper model to improve fact retrieval. The helper model assesses the\ntruthfulness of an input based on the corresponding hidden states\nrepresentations from the PLMs. We evaluate this approach on several masked PLMs\nand show that it enhances fact retrieval by up to 33\\%. Our findings highlight\nthe potential of hidden states representations from PLMs in improving their\nfactual knowledge retrieval.", "published": "2024-10-17 14:00:13", "link": "http://arxiv.org/abs/2410.13562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Reasoning Patterns of OpenAI's o1 Model", "abstract": "Enabling Large Language Models (LLMs) to handle a wider range of complex\ntasks (e.g., coding, math) has drawn great attention from many researchers. As\nLLMs continue to evolve, merely increasing the number of model parameters\nyields diminishing performance improvements and heavy computational costs.\nRecently, OpenAI's o1 model has shown that inference strategies (i.e.,\nTest-time Compute methods) can also significantly enhance the reasoning\ncapabilities of LLMs. However, the mechanisms behind these methods are still\nunexplored. In our work, to investigate the reasoning patterns of o1, we\ncompare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent\nWorkflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general\nreasoning benchmarks in three domains (i.e., math, coding, commonsense\nreasoning). Specifically, first, our experiments show that the o1 model has\nachieved the best performance on most datasets. Second, as for the methods of\nsearching diverse responses (e.g., BoN), we find the reward models' capability\nand the search space both limit the upper boundary of these methods. Third, as\nfor the methods that break the problem into many sub-problems, the Agent\nWorkflow has achieved better performance than Step-wise BoN due to the\ndomain-specific system prompt for planning better reasoning processes. Fourth,\nit is worth mentioning that we have summarized six reasoning patterns of o1,\nand provided a detailed analysis on several reasoning benchmarks.", "published": "2024-10-17 15:09:03", "link": "http://arxiv.org/abs/2410.13639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Active Learning Framework for Inclusive Generation by Large Language\n  Models", "abstract": "Ensuring that Large Language Models (LLMs) generate text representative of\ndiverse sub-populations is essential, particularly when key concepts related to\nunder-represented groups are scarce in the training data. We address this\nchallenge with a novel clustering-based active learning framework, enhanced\nwith knowledge distillation. The proposed framework transforms the intermediate\noutputs of the learner model, enabling effective active learning for generative\ntasks for the first time. Integration of clustering and knowledge distillation\nyields more representative models without prior knowledge of underlying data\ndistribution and overbearing human efforts. We validate our approach in\npractice through case studies in counter-narration and style transfer. We\nconstruct two new datasets in tandem with model training, showing a performance\nimprovement of 2%-10% over baseline models. Our results also show more\nconsistent performance across various data subgroups and increased lexical\ndiversity, underscoring our model's resilience to skewness in available data.\nFurther, our results show that the data acquired via our approach improves the\nperformance of secondary models not involved in the learning loop, showcasing\npractical utility of the framework.", "published": "2024-10-17 15:09:35", "link": "http://arxiv.org/abs/2410.13641v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Red and blue language: Word choices in the Trump & Harris 2024\n  presidential debate", "abstract": "Political debates are a peculiar type of political discourse, in which\ncandidates directly confront one another, addressing not only the the\nmoderator's questions, but also their opponent's statements, as well as the\nconcerns of voters from both parties and undecided voters. Therefore, language\nis adjusted to meet specific expectations and achieve persuasion. We analyse\nhow the language of Trump and Harris during the debate (September 10th 2024)\ndiffers in relation to the following semantic and pragmatic features, for which\nwe formulated targeted hypotheses: framing values and ideology, appealing to\nemotion, using words with different degrees of concreteness and specificity,\naddressing others through singular or plural pronouns. Our findings include:\ndifferences in the use of figurative frames (Harris often framing issues around\nrecovery and empowerment, Trump often focused on crisis and decline); similar\nuse of emotional language, with Trump showing a slight higher tendency toward\nnegativity and toward less subjective language compared to Harris; no\nsignificant difference in the specificity of candidates' responses; similar use\nof abstract language, with Trump showing more variability than Harris,\ndepending on the subject discussed; differences in addressing the opponent,\nwith Trump not mentioning Harris by name, while Harris referring to Trump\nfrequently; different uses of pronouns, with Harris using both singular and\nplural pronouns equally, while Trump using more singular pronouns. The results\nare discussed in relation to previous literature on Red and Blue language,\nwhich refers to distinct linguistic patterns associated with conservative (Red)\nand liberal (Blue) political ideologies.", "published": "2024-10-17 15:19:03", "link": "http://arxiv.org/abs/2410.13654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection\n  and Argumentative Dialogue Summarization", "abstract": "Dialogue agents have been receiving increasing attention for years, and this\ntrend has been further boosted by the recent progress of large language models\n(LLMs). Stance detection and dialogue summarization are two core tasks of\ndialogue agents in application scenarios that involve argumentative dialogues.\nHowever, research on these tasks is limited by the insufficiency of public\ndatasets, especially for non-English languages. To address this language\nresource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first\nChinese dataset for benchmarking target-independent stance detection and debate\nsummarization. Our dataset consists of 1,218 real-world debates that were\nconducted in Chinese on 476 unique topics, containing 2,436 stance-specific\nsummaries and 14,133 fully annotated utterances. Besides providing a versatile\ntestbed for future research, we also conduct an empirical study on the dataset\nand propose an integrated task. The results show the challenging nature of the\ndataset and suggest a potential of incorporating stance detection in\nsummarization for argumentative dialogue.", "published": "2024-10-17 15:28:27", "link": "http://arxiv.org/abs/2410.13667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "signwriting-evaluation: Effective Sign Language Evaluation via\n  SignWriting", "abstract": "The lack of automatic evaluation metrics tailored for SignWriting presents a\nsignificant obstacle in developing effective transcription and translation\nmodels for signed languages. This paper introduces a comprehensive suite of\nevaluation metrics specifically designed for SignWriting, including adaptations\nof standard metrics such as \\texttt{BLEU} and \\texttt{chrF}, the application of\n\\texttt{CLIPScore} to SignWriting images, and a novel symbol distance metric\nunique to our approach. We address the distinct challenges of evaluating single\nsigns versus continuous signing and provide qualitative demonstrations of\nmetric efficacy through score distribution analyses and nearest-neighbor\nsearches within the SignBank corpus. Our findings reveal the strengths and\nlimitations of each metric, offering valuable insights for future advancements\nusing SignWriting. This work contributes essential tools for evaluating\nSignWriting models, facilitating progress in the field of sign language\nprocessing. Our code is available at\n\\url{https://github.com/sign-language-processing/signwriting-evaluation}.", "published": "2024-10-17 15:28:45", "link": "http://arxiv.org/abs/2410.13668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World\n  Multilingual Settings", "abstract": "Assessing the capabilities and limitations of large language models (LLMs)\nhas garnered significant interest, yet the evaluation of multiple models in\nreal-world scenarios remains rare. Multilingual evaluation often relies on\ntranslated benchmarks, which typically do not capture linguistic and cultural\nnuances present in the source language. This study provides an extensive\nassessment of 24 LLMs on real world data collected from Indian patients\ninteracting with a medical chatbot in Indian English and 4 other Indic\nlanguages. We employ a uniform Retrieval Augmented Generation framework to\ngenerate responses, which are evaluated using both automated techniques and\nhuman evaluators on four specific metrics relevant to our application. We find\nthat models vary significantly in their performance and that instruction tuned\nIndic models do not always perform well on Indic language queries. Further, we\nempirically show that factual correctness is generally lower for responses to\nIndic queries compared to English queries. Finally, our qualitative work shows\nthat code-mixed and culturally relevant queries in our dataset pose challenges\nto evaluated models.", "published": "2024-10-17 15:29:57", "link": "http://arxiv.org/abs/2410.13671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unconstrained Model Merging for Enhanced LLM Reasoning", "abstract": "Recent advancements in building domain-specific large language models (LLMs)\nhave shown remarkable success, especially in tasks requiring reasoning\nabilities like logical inference over complex relationships and multi-step\nproblem solving. However, creating a powerful all-in-one LLM remains\nchallenging due to the need for proprietary data and vast computational\nresources. As a resource-friendly alternative, we explore the potential of\nmerging multiple expert models into a single LLM. Existing studies on model\nmerging mainly focus on generalist LLMs instead of domain experts, or the LLMs\nunder the same architecture and size. In this work, we propose an unconstrained\nmodel merging framework that accommodates both homogeneous and heterogeneous\nmodel architectures with a focus on reasoning tasks. A fine-grained layer-wise\nweight merging strategy is designed for homogeneous models merging, while\nheterogeneous model merging is built upon the probabilistic distribution\nknowledge derived from instruction-response fine-tuning data. Across 7\nbenchmarks and 9 reasoning-optimized LLMs, we reveal key findings that\ncombinatorial reasoning emerges from merging which surpasses simple additive\neffects. We propose that unconstrained model merging could serve as a\nfoundation for decentralized LLMs, marking a notable progression from the\nexisting centralized LLM framework. This evolution could enhance wider\nparticipation and stimulate additional advancement in the field of artificial\nintelligence, effectively addressing the constraints posed by centralized\nmodels.", "published": "2024-10-17 16:04:07", "link": "http://arxiv.org/abs/2410.13699v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantity vs. Quality of Monolingual Source Data in Automatic Text\n  Translation: Can It Be Too Little If It Is Too Good?", "abstract": "Monolingual data, being readily available in large quantities, has been used\nto upscale the scarcely available parallel data to train better models for\nautomatic translation. Self-learning, where a model is made to learn from its\noutput, is one approach to exploit such data. However, it has been shown that\ntoo much of this data can be detrimental to the performance of the model if the\navailable parallel data is comparatively extremely low. In this study, we\ninvestigate whether the monolingual data can also be too little and if this\nreduction, based on quality, has any effect on the performance of the\ntranslation model. Experiments have shown that on English-German low-resource\nNMT, it is often better to select only the most useful additional data, based\non quality or closeness to the domain of the test data, than utilizing all of\nthe available data.", "published": "2024-10-17 17:20:40", "link": "http://arxiv.org/abs/2410.13783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions", "abstract": "Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods.", "published": "2024-10-17 17:29:04", "link": "http://arxiv.org/abs/2410.13788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BenTo: Benchmark Task Reduction with In-Context Transferability", "abstract": "Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.", "published": "2024-10-17 17:41:15", "link": "http://arxiv.org/abs/2410.13804v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Watermark for Order-Agnostic Language Models", "abstract": "Statistical watermarking techniques are well-established for sequentially\ndecoded language models (LMs). However, these techniques cannot be directly\napplied to order-agnostic LMs, as the tokens in order-agnostic LMs are not\ngenerated sequentially. In this work, we introduce Pattern-mark, a\npattern-based watermarking framework specifically designed for order-agnostic\nLMs. We develop a Markov-chain-based watermark generator that produces\nwatermark key sequences with high-frequency key patterns. Correspondingly, we\npropose a statistical pattern-based detection algorithm that recovers the key\nsequence during detection and conducts statistical tests based on the count of\nhigh-frequency patterns. Our extensive evaluations on order-agnostic LMs, such\nas ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection\nefficiency, generation quality, and robustness, positioning it as a superior\nwatermarking technique for order-agnostic LMs.", "published": "2024-10-17 17:41:28", "link": "http://arxiv.org/abs/2410.13805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-mark: Watermark Removal in Large Language Models", "abstract": "Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks.", "published": "2024-10-17 17:42:10", "link": "http://arxiv.org/abs/2410.13808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting LLM Translation Skills without General Ability Loss via\n  Rationale Distillation", "abstract": "Large Language Models (LLMs) have achieved impressive results across numerous\nNLP tasks but still encounter difficulties in machine translation. Traditional\nmethods to improve translation have typically involved fine-tuning LLMs using\nparallel corpora. However, vanilla fine-tuning often leads to catastrophic\nforgetting of the instruction-following capabilities and alignment with human\npreferences, compromising their broad general abilities and introducing\npotential security risks. These abilities, which are developed using\nproprietary and unavailable training data, make existing continual instruction\ntuning methods ineffective. To overcome this issue, we propose a novel approach\ncalled RaDis (Rationale Distillation). RaDis harnesses the strong generative\ncapabilities of LLMs to create rationales for training data, which are then\n\"replayed\" to prevent forgetting. These rationales encapsulate general\nknowledge and safety principles, acting as self-distillation targets to\nregulate the training process. By jointly training on both reference\ntranslations and self-generated rationales, the model can learn new translation\nskills while preserving its overall general abilities. Extensive experiments\ndemonstrate that our method enhances machine translation performance while\nmaintaining the broader capabilities of LLMs across other tasks. This work\npresents a pathway for creating more versatile LLMs that excel in specialized\ntasks without compromising generality and safety.", "published": "2024-10-17 18:09:43", "link": "http://arxiv.org/abs/2410.13944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Single to Multi: How LLMs Hallucinate in Multi-Document\n  Summarization", "abstract": "Although many studies have investigated and reduced hallucinations in large\nlanguage models (LLMs) for single-document tasks, research on hallucination in\nmulti-document summarization (MDS) tasks remains largely unexplored.\nSpecifically, it is unclear how the challenges arising from handling multiple\ndocuments (e.g., repetition and diversity of information) affect models\noutputs. In this work, we investigate how hallucinations manifest in LLMs when\nsummarizing topic-specific information from multiple documents. Since no\nbenchmarks exist for investigating hallucinations in MDS, we use existing news\nand conversation datasets, annotated with topic-specific insights, to create\ntwo novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks,\nwe observe that on average, up to 75% of the content in LLM-generated summary\nis hallucinated, with hallucinations more likely to occur towards the end of\nthe summaries. Moreover, when summarizing non-existent topic-related\ninformation, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and\n44% of the time, raising concerns about their tendency to fabricate content. To\nunderstand the characteristics of these hallucinations, we manually evaluate\n700+ insights and find that most errors stem from either failing to follow\ninstructions or producing overly generic insights. Motivated by these\nobservations, we investigate the efficacy of simple post-hoc baselines in\nmitigating hallucinations but find them only moderately effective. Our results\nunderscore the need for more effective approaches to systematically mitigate\nhallucinations in MDS. We release our dataset and code at\ngithub.com/megagonlabs/Hallucination_MDS.", "published": "2024-10-17 18:38:53", "link": "http://arxiv.org/abs/2410.13961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers", "abstract": "Distributional semantics is the linguistic theory that a word's meaning can\nbe derived from its distribution in natural language (i.e., its use). Language\nmodels are commonly viewed as an implementation of distributional semantics, as\nthey are optimized to capture the statistical features of natural language. It\nis often argued that distributional semantics models should excel at capturing\ngraded/vague meaning based on linguistic conventions, but struggle with\ntruth-conditional reasoning and symbolic processing. We evaluate this claim\nwith a case study on vague (e.g. \"many\") and exact (e.g. \"more than half\")\nquantifiers. Contrary to expectations, we find that, across a broad range of\nmodels of various types, LLMs align more closely with human judgements on exact\nquantifiers versus vague ones. These findings call for a re-evaluation of the\nassumptions underpinning what distributional semantics models are, as well as\nwhat they can capture.", "published": "2024-10-17 19:28:35", "link": "http://arxiv.org/abs/2410.13984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RiTeK: A Dataset for Large Language Models Complex Reasoning over\n  Textual Knowledge Graphs", "abstract": "Answering complex real-world questions often requires accurate retrieval from\ntextual knowledge graphs (TKGs). The scarcity of annotated data, along with\nintricate topological structures, makes this task particularly challenging. As\nthe nature of relational path information could enhance the inference ability\nof Large Language Models (LLMs), efficiently retrieving more complex relational\npath information from TKGs presents another key challenge. To tackle these\nchallenges, we first develop a Dataset for LLMs Complex Reasoning over Textual\nKnowledge Graphs (RiTeK) with a broad topological structure coverage.We\nsynthesize realistic user queries that integrate diverse topological\nstructures, relational information, and complex textual descriptions. We\nconduct rigorous expert evaluation to validate the quality of our synthesized\nqueries. And then, we introduce an enhanced Monte Carlo Tree Search (MCTS)\nmethod, Relational MCTS, to automatically extract relational path information\nfrom textual graphs for specific queries. Our dataset mainly covers the medical\ndomain as the relation types and entity are complex and publicly available.\nExperimental results indicate that RiTeK poses significant challenges for\ncurrent retrieval and LLM systems, while the proposed Relational MCTS method\nenhances LLM inference ability and achieves state-of-the-art performance on\nRiTeK.", "published": "2024-10-17 19:33:37", "link": "http://arxiv.org/abs/2410.13987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Modifying the Readability of English Texts with GPT-4", "abstract": "The success of Large Language Models (LLMs) in other domains has raised the\nquestion of whether LLMs can reliably assess and manipulate the readability of\ntext. We approach this question empirically. First, using a published corpus of\n4,724 English text excerpts, we find that readability estimates produced\n``zero-shot'' from GPT-4 Turbo and GPT-4o mini exhibit relatively high\ncorrelation with human judgments (r = 0.76 and r = 0.74, respectively),\nout-performing estimates derived from traditional readability formulas and\nvarious psycholinguistic indices. Then, in a pre-registered human experiment (N\n= 59), we ask whether Turbo can reliably make text easier or harder to read. We\nfind evidence to support this hypothesis, though considerable variance in human\njudgments remains unexplained. We conclude by discussing the limitations of\nthis approach, including limited scope, as well as the validity of the\n``readability'' construct and its dependence on context, audience, and goal.", "published": "2024-10-17 21:04:28", "link": "http://arxiv.org/abs/2410.14028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style-Compress: An LLM-Based Prompt Compression Framework Considering\n  Task-Specific Styles", "abstract": "Prompt compression condenses contexts while maintaining their informativeness\nfor different usage scenarios. It not only shortens the inference time and\nreduces computational costs during the usage of large language models, but also\nlowers expenses when using closed-source models. In a preliminary study, we\ndiscover that when instructing language models to compress prompts, different\ncompression styles (e.g., extractive or abstractive) impact performance of\ncompressed prompts on downstream tasks. Building on this insight, we propose\nStyle-Compress, a lightweight framework that adapts a smaller language model to\ncompress prompts for a larger model on a new task without additional training.\nOur approach iteratively generates and selects effective compressed prompts as\ntask-specific demonstrations through style variation and in-context learning,\nenabling smaller models to act as efficient compressors with task-specific\nexamples. Style-Compress outperforms two baseline compression models in four\ntasks: original prompt reconstruction, text summarization, multi-hop QA, and\nCoT reasoning. In addition, with only 10 samples and 100 queries for\nadaptation, prompts compressed by Style-Compress achieve performance on par\nwith or better than original prompts at a compression ratio of 0.25 or 0.5.", "published": "2024-10-17 21:35:49", "link": "http://arxiv.org/abs/2410.14042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Metadata-Agnostic Representations for Text-to-SQL In-Context\n  Example Selection", "abstract": "In-context learning (ICL) is a powerful paradigm where large language models\n(LLMs) benefit from task demonstrations added to the prompt. Yet, selecting\noptimal demonstrations is not trivial, especially for complex or multi-modal\ntasks where input and output distributions differ. We hypothesize that forming\ntask-specific representations of the input is key. In this paper, we propose a\nmethod to align representations of natural language questions and those of SQL\nqueries in a shared embedding space. Our technique, dubbed MARLO -\nMetadata-Agnostic Representation Learning for Text-tO-SQL - uses query\nstructure to model querying intent without over-indexing on underlying database\nmetadata (i.e. tables, columns, or domain-specific entities of a database\nreferenced in the question or query). This allows MARLO to select examples that\nare structurally and semantically relevant for the task rather than examples\nthat are spuriously related to a certain domain or question phrasing. When used\nto retrieve examples based on question similarity, MARLO shows superior\nperformance compared to generic embedding models (on average +2.9\\%pt. in\nexecution accuracy) on the Spider benchmark. It also outperforms the next best\nmethod that masks metadata information by +0.8\\%pt. in execution accuracy on\naverage, while imposing a significantly lower inference latency.", "published": "2024-10-17 21:45:55", "link": "http://arxiv.org/abs/2410.14049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM", "abstract": "In this work, we investigated how one can use the LLM to transfer the dataset\nand its annotation from one language to another. This is crucial since sharing\nthe knowledge between different languages could boost certain underresourced\ndirections in the target language, saving lots of efforts in data annotation or\nquick prototyping. We experiment with English and Russian pairs translating the\nDEFT corpus. This corpus contains three layers of annotation dedicated to\nterm-definition pair mining, which is a rare annotation type for Russian. We\nprovide a pipeline for the annotation transferring using ChatGPT3.5-turbo and\nLlama-3.1-8b as core LLMs. In the end, we train the BERT-based models on the\ntranslated dataset to establish a baseline.", "published": "2024-10-17 22:46:53", "link": "http://arxiv.org/abs/2410.14074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Summarize from LLM-generated Feedback", "abstract": "Developing effective text summarizers remains a challenge due to issues like\nhallucinations, key information omissions, and verbosity in LLM-generated\nsummaries. This work explores using LLM-generated feedback to improve summary\nquality by aligning the summaries with human preferences for faithfulness,\ncompleteness, and conciseness. We introduce FeedSum, a large-scale dataset\ncontaining multi-dimensional LLM feedback on summaries of varying quality\nacross diverse domains. Our experiments show how feedback quality,\ndimensionality, and granularity influence preference learning, revealing that\nhigh-quality, multi-dimensional, fine-grained feedback significantly improves\nsummary generation. We also compare two methods for using this feedback:\nsupervised fine-tuning and direct preference optimization. Finally, we\nintroduce SummLlama3-8b, a model that outperforms the nearly 10x larger\nLlama3-70b-instruct in generating human-preferred summaries, demonstrating that\nsmaller models can achieve superior performance with appropriate training. The\nfull dataset and SummLlama3-8B model are available at\nhttps://huggingface.co/datasets/DISLab/FeedSum and\nhttps://huggingface.co/DISLab/SummLlama3-8B.", "published": "2024-10-17 01:01:09", "link": "http://arxiv.org/abs/2410.13116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Enhanced Named Entity Recognition", "abstract": "When combined with In-Context Learning, a technique that enables models to\nadapt to new tasks by incorporating task-specific examples or demonstrations\ndirectly within the input prompt, autoregressive language models have achieved\ngood performance in a wide range of tasks and applications. However, this\ncombination has not been properly explored in the context of named entity\nrecognition, where the structure of this task poses unique challenges. We\npropose RENER (Retrieval-Enhanced Named Entity Recognition), a technique for\nnamed entity recognition using autoregressive language models based on\nIn-Context Learning and information retrieval techniques. When presented with\nan input text, RENER fetches similar examples from a dataset of training\nexamples that are used to enhance a language model to recognize named entities\nfrom this input text. RENER is modular and independent of the underlying\nlanguage model and information retrieval algorithms. Experimental results show\nthat in the CrossNER collection we achieve state-of-the-art performance with\nthe proposed technique and that information retrieval can increase the F-score\nby up to 11 percentage points.", "published": "2024-10-17 01:12:48", "link": "http://arxiv.org/abs/2410.13118v1", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias", "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains\nunder-explored. In this paper, we analyze demographic biases across five models\nand six datasets. We find that portrait datasets like UTKFace and CelebA are\nthe best tools for bias detection, finding gaps in performance and fairness for\nboth LLaVa and CLIP models. Scene-based datasets like PATA and VLStereoSet fail\nto be useful benchmarks for bias due to their text prompts allowing the model\nto guess the answer without a picture. As for pronoun-based datasets like\nVisoGender, we receive mixed signals as only some subsets of the data are\nuseful in providing insights. To alleviate these two problems, we introduce a\nmore rigorous evaluation dataset and a debiasing method based on Sparse\nAutoencoders to help reduce bias in models. We find that our data set generates\nmore meaningful errors than the previous data sets. Furthermore, our debiasing\nmethod improves fairness, gaining 5-15 points in performance over the baseline.\nThis study displays the problems with the current benchmarks for measuring\ndemographic bias in Vision Language Models and introduces both a more effective\ndataset for measuring bias and a novel and interpretable debiasing method based\non Sparse Autoencoders.", "published": "2024-10-17 02:03:27", "link": "http://arxiv.org/abs/2410.13146v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents", "abstract": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.", "published": "2024-10-17 03:26:37", "link": "http://arxiv.org/abs/2410.13185v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback", "abstract": "Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.", "published": "2024-10-17 03:38:29", "link": "http://arxiv.org/abs/2410.13191v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FaithBench: A Diverse Hallucination Benchmark for Summarization by\n  Modern LLMs", "abstract": "Summarization is one of the most common tasks performed by large language\nmodels (LLMs), especially in applications like Retrieval-Augmented Generation\n(RAG). However, existing evaluations of hallucinations in LLM-generated\nsummaries, and evaluations of hallucination detection models both suffer from a\nlack of diversity and recency in the LLM and LLM families considered. This\npaper introduces FaithBench, a summarization hallucination benchmark comprising\nchallenging hallucinations made by 10 modern LLMs from 8 different families,\nwith ground truth annotations by human experts. ``Challenging'' here means\nsummaries on which popular, state-of-the-art hallucination detection models,\nincluding GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and\nGPT-3.5-Turbo produce the least hallucinations. However, even the best\nhallucination detection models have near 50\\% accuracies on FaithBench,\nindicating lots of room for future improvement. The repo is\nhttps://github.com/vectara/FaithBench", "published": "2024-10-17 04:30:46", "link": "http://arxiv.org/abs/2410.13210v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anchored Alignment for Self-Explanations Enhancement", "abstract": "In this work, we introduce a methodology for alignment designed to enhance\nthe ability of large language models (LLMs) to articulate their reasoning\n(self-explanation) even in the absence of annotated rationale explanations. Our\nalignment methodology comprises three key components: explanation quality\nassessment, self-instruction dataset generation, and model alignment.\nAdditionally, we present a novel technique called Alignment with Anchor\nPreference Pairs, which improves the selection of preference pairs by\ncategorizing model outputs into three groups: consistently correct,\nconsistently incorrect, and variable. By applying tailored strategies to each\ncategory, we enhance the effectiveness of Direct Preference Optimization (DPO).\nOur experimental results demonstrate that this approach significantly improves\nexplanation quality while maintaining accuracy compared to other fine-tuning\nstrategies.", "published": "2024-10-17 04:42:48", "link": "http://arxiv.org/abs/2410.13216v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SPIN: Self-Supervised Prompt INjection", "abstract": "Large Language Models (LLMs) are increasingly used in a variety of important\napplications, yet their safety and reliability remain as major concerns.\nVarious adversarial and jailbreak attacks have been proposed to bypass the\nsafety alignment and cause the model to produce harmful responses. We introduce\nSelf-supervised Prompt INjection (SPIN) which can detect and reverse these\nvarious attacks on LLMs. As our self-supervised prompt defense is done at\ninference-time, it is also compatible with existing alignment and adds an\nadditional layer of safety for defense. Our benchmarks demonstrate that our\nsystem can reduce the attack success rate by up to 87.9%, while maintaining the\nperformance on benign user requests. In addition, we discuss the situation of\nan adaptive attacker and show that our method is still resilient against\nattackers who are aware of our defense.", "published": "2024-10-17 05:40:54", "link": "http://arxiv.org/abs/2410.13236v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Atomic Calibration of LLMs in Long-Form Generations", "abstract": "Large language models (LLMs) often suffer from hallucinations, posing\nsignificant challenges for real-world applications. Confidence calibration,\nwhich estimates the underlying uncertainty of model predictions, is essential\nto enhance the LLMs' trustworthiness. Existing research on LLM calibration has\nprimarily focused on short-form tasks, providing a single confidence score at\nthe response level (macro calibration). However, this approach is insufficient\nfor long-form generations, where responses often contain more complex\nstatements and may include both accurate and inaccurate information. Therefore,\nwe introduce atomic calibration, a novel approach that evaluates factuality\ncalibration at a fine-grained level by breaking down long responses into atomic\nclaims. We classify confidence elicitation methods into discriminative and\ngenerative types and demonstrate that their combination can enhance\ncalibration. Our extensive experiments on various LLMs and datasets show that\natomic calibration is well-suited for long-form generation and can also improve\nmacro calibration results. Additionally, atomic calibration reveals insightful\npatterns in LLM confidence throughout the generation process.", "published": "2024-10-17 06:09:26", "link": "http://arxiv.org/abs/2410.13246v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Translation Alignment Pipeline for Multilingual Digital\n  Editions of Literary Works", "abstract": "This paper investigates the application of translation alignment algorithms\nin the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's\nItalian novel \"I promessi sposi\" (\"The Betrothed\"), with translations in eight\nlanguages (English, Spanish, French, German, Dutch, Polish, Russian and\nChinese) from the 19th and 20th centuries. We identify key requirements for the\nMDE to improve both the reader experience and support for translation studies.\nOur research highlights the limitations of current state-of-the-art algorithms\nwhen applied to the translation of literary texts and outlines an automated\npipeline for MDE creation. This pipeline transforms raw texts into web-based,\nside-by-side representations of original and translated texts with different\nrendering options. In addition, we propose new metrics for evaluating the\nalignment of literary translations and suggest visualization techniques for\nfuture analysis.", "published": "2024-10-17 06:21:38", "link": "http://arxiv.org/abs/2410.13255v1", "categories": ["cs.CL", "cs.AI", "68U15", "J.5; I.7.4"], "primary_category": "cs.CL"}
{"title": "Advancing Large Language Model Attribution through Self-Improving", "abstract": "Teaching large language models (LLMs) to generate text with citations to\nevidence sources can mitigate hallucinations and enhance verifiability in\ninformation-seeking systems. However, improving this capability requires\nhigh-quality attribution data, which is costly and labor-intensive. Inspired by\nrecent advances in self-improvement that enhance LLMs without manual\nannotation, we present START, a Self-Taught AttRibuTion framework for\niteratively improving the attribution capability of LLMs. First, to prevent\nmodels from stagnating due to initially insufficient supervision signals, START\nleverages the model to self-construct synthetic training data for warming up.\nTo further self-improve the model's attribution ability, START iteratively\nutilizes fine-grained preference supervision signals constructed from its\nsampled responses to encourage robust, comprehensive, and attributable\ngeneration. Experiments on three open-domain question-answering datasets,\ncovering long-form QA and multi-step reasoning, demonstrate significant\nperformance gains of 25.13% on average without relying on human annotations and\nmore advanced models. Further analysis reveals that START excels in aggregating\ninformation across multiple sources.", "published": "2024-10-17 07:55:33", "link": "http://arxiv.org/abs/2410.13298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reference-Based Post-OCR Processing with LLM for Precise Diacritic Text\n  in Historical Document Recognition", "abstract": "Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.", "published": "2024-10-17 08:05:02", "link": "http://arxiv.org/abs/2410.13305v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Computational Approaches to Arabic-English Code-Switching", "abstract": "Natural Language Processing (NLP) is a vital computational method for\naddressing language processing, analysis, and generation. NLP tasks form the\ncore of many daily applications, from automatic text correction to speech\nrecognition. While significant research has focused on NLP tasks for the\nEnglish language, less attention has been given to Modern Standard Arabic and\nDialectal Arabic. Globalization has also contributed to the rise of\nCode-Switching (CS), where speakers mix languages within conversations and even\nwithin individual words (intra-word CS). This is especially common in Arab\ncountries, where people often switch between dialects or between dialects and a\nforeign language they master. CS between Arabic and English is frequent in\nEgypt, especially on social media. Consequently, a significant amount of\ncode-switched content can be found online. Such code-switched data needs to be\ninvestigated and analyzed for several NLP tasks to tackle the challenges of\nthis multilingual phenomenon and Arabic language challenges. No work has been\ndone before for several integral NLP tasks on Arabic-English CS data. In this\nwork, we focus on the Named Entity Recognition (NER) task and other tasks that\nhelp propose a solution for the NER task on CS data, e.g., Language\nIdentification. This work addresses this gap by proposing and applying\nstate-of-the-art techniques for Modern Standard Arabic and Arabic-English NER.\nWe have created the first annotated CS Arabic-English corpus for the NER task.\nAlso, we apply two enhancement techniques to improve the NER tagger on CS data\nusing CS contextual embeddings and data augmentation techniques. All methods\nshowed improvements in the performance of the NER taggers on CS data. Finally,\nwe propose several intra-word language identification approaches to determine\nthe language type of a mixed text and identify whether it is a named entity or\nnot.", "published": "2024-10-17 08:20:29", "link": "http://arxiv.org/abs/2410.13318v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges\n  in Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks. However, LLMs may rely on dataset biases as\nshortcuts for prediction, which can significantly impair their robustness and\ngeneralization capabilities. This paper presents Shortcut Suite, a\ncomprehensive test suite designed to evaluate the impact of shortcuts on LLMs'\nperformance, incorporating six shortcut types, five evaluation metrics, and\nfour prompting strategies. Our extensive experiments yield several key\nfindings: 1) LLMs demonstrate varying reliance on shortcuts for downstream\ntasks, significantly impairing their performance. 2) Larger LLMs are more\nlikely to utilize shortcuts under zero-shot and few-shot in-context learning\nprompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and\noutperforms other prompting strategies, while few-shot prompts generally\nunderperform compared to zero-shot prompts. 4) LLMs often exhibit\noverconfidence in their predictions, especially when dealing with datasets that\ncontain shortcuts. 5) LLMs generally have a lower explanation quality in\nshortcut-laden datasets, with errors falling into three types: distraction,\ndisguised comprehension, and logical fallacy. Our findings offer new insights\nfor evaluating robustness and generalization in LLMs and suggest potential\ndirections for mitigating the reliance on shortcuts. The code is available at\n\\url {https://github.com/yyhappier/ShortcutSuite.git}.", "published": "2024-10-17 08:52:52", "link": "http://arxiv.org/abs/2410.13343v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cerberus: Efficient Inference with Adaptive Parallel Decoding and\n  Sequential Knowledge Enhancement", "abstract": "Large language models (LLMs) often face a bottleneck in inference speed due\nto their reliance on auto-regressive decoding. Recently, parallel decoding has\nshown significant promise in enhancing inference efficiency. However, we have\nidentified two key issues with existing parallel decoding frameworks: (1)\ndecoding heads fail to balance prediction accuracy and the parallelism of\nexecution, and (2) parallel decoding is not a universal solution, as it can\nbring unnecessary overheads at some challenging decoding steps. To address\nthese issues, we propose Cerberus, an adaptive parallel decoding framework\nintroduces the gating mechanism to enable the LLMs to adaptively choose\nappropriate decoding approaches at each decoding step, along with introducing a\nnew paradigm of decoding heads that introduce the sequential knowledge while\nmaintaining execution parallelism. The experiment results demonstrate that the\nCerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,\nand outperforms one of the leading parallel decoding frameworks, Medusa, with a\n10% - 30% increase in acceleration and superior generation quality.", "published": "2024-10-17 08:55:18", "link": "http://arxiv.org/abs/2410.13344v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of\n  the European Court of Human Rights", "abstract": "We present Legal Argument Reasoning (LAR), a novel task designed to evaluate\nthe legal reasoning capabilities of Large Language Models (LLMs). The task\nrequires selecting the correct next statement (from multiple choice options) in\na chain of legal arguments from court proceedings, given the facts of the case.\nWe constructed a dataset (LAR-ECHR) for this task using cases from the European\nCourt of Human Rights (ECHR). We evaluated seven general-purpose LLMs on\nLAR-ECHR and found that (a) the ranking of the models is aligned with that of\nLegalBench, an established US-based legal reasoning benchmark, even though\nLAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more\nclearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8%\naccuracy on LAR-ECHR, indicating significant potential for further model\nimprovement. The process followed to construct LAR-ECHR can be replicated with\ncases from other legal systems.", "published": "2024-10-17 09:03:38", "link": "http://arxiv.org/abs/2410.13352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt\n  from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and\n  Humans", "abstract": "This chapter introduces a research project titled \"Analyzing the Political\nDiscourse: A Collaboration Between Humans and Artificial Intelligence\", which\nwas initiated in preparation for Greece's 2023 general elections. The project\nfocused on the analysis of political leaders' campaign speeches, employing\nArtificial Intelligence (AI), in conjunction with an interdisciplinary team\ncomprising journalists, a political scientist, and data scientists. The chapter\ndelves into various aspects of political discourse analysis, including\nsentiment analysis, polarization, populism, topic detection, and Named Entities\nRecognition (NER). This experimental study investigates the capabilities of\nlarge language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing\npolitical speech, evaluates its strengths and weaknesses, and highlights the\nessential role of human oversight in using AI in journalism projects and\npotentially other societal sectors. The project stands as an innovative example\nof human-AI collaboration (known also as \"hybrid intelligence\") within the\nrealm of digital humanities, offering valuable insights for future initiatives.", "published": "2024-10-17 09:54:54", "link": "http://arxiv.org/abs/2410.13400v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Attr-Int: A Simple and Effective Entity Alignment Framework for\n  Heterogeneous Knowledge Graphs", "abstract": "Entity alignment (EA) refers to the task of linking entities in different\nknowledge graphs (KGs). Existing EA methods rely heavily on structural\nisomorphism. However, in real-world KGs, aligned entities usually have\nnon-isomorphic neighborhood structures, which paralyses the application of\nthese structure-dependent methods. In this paper, we investigate and tackle the\nproblem of entity alignment between heterogeneous KGs. First, we propose two\nnew benchmarks to closely simulate real-world EA scenarios of heterogeneity.\nThen we conduct extensive experiments to evaluate the performance of\nrepresentative EA methods on the new benchmarks. Finally, we propose a simple\nand effective entity alignment framework called Attr-Int, in which innovative\nattribute information interaction methods can be seamlessly integrated with any\nembedding encoder for entity alignment, improving the performance of existing\nentity alignment techniques. Experiments demonstrate that our framework\noutperforms the state-of-the-art approaches on two new benchmarks.", "published": "2024-10-17 10:16:56", "link": "http://arxiv.org/abs/2410.13409v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Thrice Before You Act: Progressive Thought Refinement in Large\n  Language Models", "abstract": "Recent advancements in large language models (LLMs) have demonstrated that\nprogressive refinement, rather than providing a single answer, results in more\naccurate and thoughtful outputs. However, existing methods often rely heavily\non supervision signals to evaluate previous responses, making it difficult to\nassess output quality in more open-ended scenarios effectively. Additionally,\nthese methods are typically designed for specific tasks, which limits their\ngeneralization to new domains. To address these limitations, we propose\nProgressive Thought Refinement (PTR), a framework that enables LLMs to refine\ntheir responses progressively. PTR operates in two phases: (1) Thought data\nconstruction stage: We propose a weak and strong model collaborative selection\nstrategy to build a high-quality progressive refinement dataset to ensure\nlogical consistency from thought to answers, and the answers are gradually\nrefined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training\nstructure to mask the \"thought\" and adjust loss weights to encourage LLMs to\nrefine prior thought, teaching them to implicitly understand \"how to improve\"\nrather than \"what is correct.\" Experimental results show that PTR significantly\nenhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%)\nwithout task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also\ndemonstrate substantial improvements in the quality of responses beyond mere\naccuracy, suggesting that PTR truly teaches LLMs to self-improve over time.", "published": "2024-10-17 10:23:24", "link": "http://arxiv.org/abs/2410.13413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference", "abstract": "In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.", "published": "2024-10-17 11:46:33", "link": "http://arxiv.org/abs/2410.13461v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GeoCoder: Solving Geometry Problems by Generating Modular Code through\n  Vision-Language Models", "abstract": "Geometry problem-solving demands advanced reasoning abilities to process\nmultimodal inputs and employ mathematical knowledge effectively.\nVision-language models (VLMs) have made significant progress in various\nmultimodal tasks. Yet, they still struggle with geometry problems and are\nsignificantly limited by their inability to perform mathematical operations not\nseen during pre-training, such as calculating the cosine of an arbitrary angle,\nand by difficulties in correctly applying relevant geometry formulas. To\novercome these challenges, we present GeoCoder, which leverages modular\ncode-finetuning to generate and execute code using a predefined geometry\nfunction library. By executing the code, we achieve accurate and deterministic\ncalculations, contrasting the stochastic nature of autoregressive token\nprediction, while the function library minimizes errors in formula usage. We\nalso propose a multimodal retrieval-augmented variant of GeoCoder, named\nRAG-GeoCoder, which incorporates a non-parametric memory module for retrieving\nfunctions from the geometry library, thereby reducing reliance on parametric\nmemory. Our modular code-finetuning approach enhances the geometric reasoning\ncapabilities of VLMs, yielding an average improvement of over 16% across\nvarious question complexities on the GeomVerse dataset compared to other\nfinetuning methods.", "published": "2024-10-17 12:56:52", "link": "http://arxiv.org/abs/2410.13510v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?", "abstract": "Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.", "published": "2024-10-17 13:06:02", "link": "http://arxiv.org/abs/2410.13517v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Temporal Representations for Dynamic Memory Retrieval and\n  Management in Large Language Models", "abstract": "Conventional dialogue agents often struggle with effective memory recall,\nleading to redundant retrieval and inadequate management of unique user\nassociations. To address this, we propose SynapticRAG, a novel approach\nintegrating synaptic dynamics into Retrieval-Augmented Generation (RAG).\nSynapticRAG integrates temporal representations into memory vectors, mimicking\nbiological synapses by differentiating events based on occurrence times and\ndynamically updating memory significance. This model employs temporal scoring\nfor memory connections and a synaptic-inspired propagation control mechanism.\nExperiments across English, Japanese, and Chinese datasets demonstrate\nSynapticRAG's superiority over existing methods, including traditional RAG,\nwith up to 14.66\\% improvement in memory retrieval accuracy. Our approach\nadvances context-aware dialogue AI systems by enhancing long-term context\nmaintenance and specific information extraction from conversations.", "published": "2024-10-17 13:51:03", "link": "http://arxiv.org/abs/2410.13553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling", "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.", "published": "2024-10-17 14:46:22", "link": "http://arxiv.org/abs/2410.13610v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit\n  ToM Application in LLMs", "abstract": "While prior work has explored whether large language models (LLMs) possess a\n\"theory of mind\" (ToM) - the ability to attribute mental states to oneself and\nothers - there has been little work testing whether LLMs can implicitly apply\nsuch knowledge to predict behavior, or to judge whether an observed behavior is\nrational. Such skills are critical for appropriate interaction in social\nenvironments. We create a new dataset, SimpleTom, containing concise, diverse\nstories (e.g., \"The can of Pringles has moldy chips in it. Mary picks up the\ncan in the supermarket and walks to the cashier.\"), each with three questions\nthat test different degrees of ToM reasoning, asking models to predict (a)\nmental state (\"Is Mary aware of the mold?\"), (b) behavior (\"Will Mary pay for\nthe chips or report the mold?\"), and (c) judgment (\"Mary paid for the chips.\nWas that reasonable?\"). To our knowledge, SimpleToM is the first dataset to\nsystematically explore downstream reasoning requiring knowledge of mental\nstates in realistic scenarios. Our experimental results are intriguing: While\nmost models can reliably predict mental state on our dataset (a), they often\nfail to correctly predict the behavior (b), and fare even worse at judging\nwhether given behaviors are reasonable (c), despite being correctly aware of\nthe protagonist's mental state should make such secondary predictions obvious.\nWe further show that we can help models do better at (b) and (c) via\ninterventions such as reminding the model of its earlier mental state answer\nand mental-state-specific chain-of-thought prompting, raising the action\nprediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment\naccuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models\ncan be coaxed to perform well, it requires task-specific interventions, and the\nnatural model performances remain low, a cautionary tale for LLM deployment.", "published": "2024-10-17 15:15:00", "link": "http://arxiv.org/abs/2410.13648v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A new approach for fine-tuning sentence transformers for intent\n  classification and out-of-scope detection tasks", "abstract": "In virtual assistant (VA) systems it is important to reject or redirect user\nqueries that fall outside the scope of the system. One of the most accurate\napproaches for out-of-scope (OOS) rejection is to combine it with the task of\nintent classification on in-scope queries, and to use methods based on the\nsimilarity of embeddings produced by transformer-based sentence encoders.\nTypically, such encoders are fine-tuned for the intent-classification task,\nusing cross-entropy loss. Recent work has shown that while this produces\nsuitable embeddings for the intent-classification task, it also tends to\ndisperse in-scope embeddings over the full sentence embedding space. This\ncauses the in-scope embeddings to potentially overlap with OOS embeddings,\nthereby making OOS rejection difficult. This is compounded when OOS data is\nunknown. To mitigate this issue our work proposes to regularize the\ncross-entropy loss with an in-scope embedding reconstruction loss learned using\nan auto-encoder. Our method achieves a 1-4% improvement in the area under the\nprecision-recall curve for rejecting out-of-sample (OOS) instances, without\ncompromising intent classification performance.", "published": "2024-10-17 15:15:12", "link": "http://arxiv.org/abs/2410.13649v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic\n  Reasoning Tasks", "abstract": "Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.", "published": "2024-10-17 15:27:17", "link": "http://arxiv.org/abs/2410.13666v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pose-Based Sign Language Appearance Transfer", "abstract": "We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\n\\url{https://github.com/sign-language-processing/pose-anonymization}.", "published": "2024-10-17 15:33:54", "link": "http://arxiv.org/abs/2410.13675v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring the Design Space of Visual Context Representation in Video\n  MLLMs", "abstract": "Video Multimodal Large Language Models (MLLMs) have shown remarkable\ncapability of understanding the video semantics on various downstream tasks.\nDespite the advancements, there is still a lack of systematic research on\nvisual context representation, which refers to the scheme to select frames from\na video and further select the tokens from a frame. In this paper, we explore\nthe design space for visual context representation, and aim to improve the\nperformance of video MLLMs by finding more effective representation schemes.\nFirstly, we formulate the task of visual context representation as a\nconstrained optimization problem, and model the language modeling loss as a\nfunction of the number of frames and the number of embeddings (or tokens) per\nframe, given the maximum visual context window size. Then, we explore the\nscaling effects in frame selection and token selection respectively, and fit\nthe corresponding function curve by conducting extensive empirical experiments.\nWe examine the effectiveness of typical selection strategies and present\nempirical findings to determine the two factors. Furthermore, we study the\njoint effect of frame selection and token selection, and derive the optimal\nformula for determining the two factors. We demonstrate that the derived\noptimal settings show alignment with the best-performed results of empirical\nexperiments. Our code and model are available at:\nhttps://github.com/RUCAIBox/Opt-Visor.", "published": "2024-10-17 15:59:52", "link": "http://arxiv.org/abs/2410.13694v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MIRAGE-Bench: Automatic Multilingual Benchmark Arena for\n  Retrieval-Augmented Generation Systems", "abstract": "Traditional retrieval-augmented generation (RAG) benchmarks evaluate systems\nusing heuristic-based metrics, but these require human preferences as the\nground truth for reference. In contrast, arena-based benchmarks, where systems\ncompete against each other, require an expensive large language model (LLM) as\na judge for a reliable evaluation. We present a simple efficient technique to\ncombine the best of both worlds. The idea is to train a surrogate judge using\nheuristic metrics as input, to output the LLM as a judge prediction. In our\nwork, we develop MIRAGE-Bench, a synthetic arena-based RAG benchmark for 18\ndiverse languages on Wikipedia focused on multilingual answer generation\nevaluation. It extensively couples both heuristic features and LLM as a judge\nfor evaluation. We benchmark 19 multilingual LLMs, and observe a high\ncorrelation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge and\nbetween GPT-4o as a teacher using the Bradley-Terry framework. Our results show\nproprietary and large open-source LLMs currently dominate on MIRAGE-Bench. Our\ncode and datasets are made publicly available here:\nhttps://github.com/vectara/mirage-bench.", "published": "2024-10-17 16:18:49", "link": "http://arxiv.org/abs/2410.13716v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Human Pipeline for Cultural Context Grounding of Conversations", "abstract": "Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.", "published": "2024-10-17 16:33:01", "link": "http://arxiv.org/abs/2410.13727v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Query Expansion with Large Language Models for Textual\n  and Relational Retrieval", "abstract": "Large language models (LLMs) have been used to generate query expansions\naugmenting original queries for improving information search. Recent studies\nalso explore providing LLMs with initial retrieval results to generate query\nexpansions more grounded to document corpus. However, these methods mostly\nfocus on enhancing textual similarities between search queries and target\ndocuments, overlooking document relations. For queries like \"Find me a highly\nrated camera for wildlife photography compatible with my Nikon F-Mount lenses\",\nexisting methods may generate expansions that are semantically similar but\nstructurally unrelated to user intents. To handle such semi-structured queries\nwith both textual and relational requirements, in this paper we propose a\nknowledge-aware query expansion framework, augmenting LLMs with structured\ndocument relations from knowledge graph (KG). To further address the limitation\nof entity-based scoring in existing KG-based methods, we leverage document\ntexts as rich KG node representations and use document-based relation filtering\nfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on three\ndatasets of diverse domains show the advantages of our method compared against\nstate-of-the-art baselines on textual and relational semi-structured retrieval.", "published": "2024-10-17 17:03:23", "link": "http://arxiv.org/abs/2410.13765v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Aggregation Artifacts in Subjective Tasks Collapse Large Language\n  Models' Posteriors", "abstract": "In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified.", "published": "2024-10-17 17:16:00", "link": "http://arxiv.org/abs/2410.13776v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Mystery of the Pathological Path-star Task for Language Models", "abstract": "The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.", "published": "2024-10-17 17:18:30", "link": "http://arxiv.org/abs/2410.13779v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PopAlign: Diversifying Contrasting Patterns for a More Comprehensive\n  Alignment", "abstract": "Alignment of large language models (LLMs) involves training models on\npreference-contrastive output pairs to adjust their responses according to\nhuman preferences. To obtain such contrastive pairs, traditional methods like\nRLHF and RLAIF rely on limited contrasting patterns, such as varying model\nvariants or decoding temperatures. This singularity leads to two issues: (1)\nalignment is not comprehensive; and thereby (2) models are susceptible to\njailbreaking attacks. To address these issues, we investigate how to construct\nmore comprehensive and diversified contrasting patterns to enhance preference\ndata (RQ1) and verify the impact of the diversification of contrasting patterns\non model alignment (RQ2). For RQ1, we propose PopAlign, a framework that\nintegrates diversified contrasting patterns across the prompt, model, and\npipeline levels, introducing six contrasting strategies that do not require\nadditional feedback labeling procedures. Regarding RQ2, we conduct thorough\nexperiments demonstrating that PopAlign significantly outperforms existing\nmethods, leading to more comprehensive alignment.", "published": "2024-10-17 17:22:05", "link": "http://arxiv.org/abs/2410.13785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Looking Inward: Language Models Can Learn About Themselves by\n  Introspection", "abstract": "Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.", "published": "2024-10-17 17:24:10", "link": "http://arxiv.org/abs/2410.13787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harnessing Webpage UIs for Text-Rich Visual Understanding", "abstract": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.", "published": "2024-10-17 17:48:54", "link": "http://arxiv.org/abs/2410.13824v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents", "abstract": "Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.", "published": "2024-10-17 17:50:38", "link": "http://arxiv.org/abs/2410.13825v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale\n  Models", "abstract": "Post-training has emerged as a crucial paradigm for adapting large-scale\npre-trained models to various tasks, whose effects are fully reflected by delta\nparameters (i.e., the disparity between post-trained and pre-trained\nparameters). While numerous studies have explored delta parameter properties\nvia operations like pruning, quantization, low-rank approximation, and\nextrapolation, a unified framework for systematically examining these\ncharacteristics has been lacking. In this paper, we propose a novel perspective\nbased on Riemann sum approximation of the loss function to elucidate delta\nparameter editing operations. Our analysis categorizes existing methods into\nthree classes based on their post-editing performance: competitive, decreased,\nand improved, explaining how they are expressed by the Riemann sum\napproximation term and how they alter the model performance. Extensive\nexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,\nand Mistral, corroborate our theoretical findings. Furthermore, we introduce\nextensions to existing techniques like DARE and BitDelta, highlighting their\nlimitations in leveraging the properties of delta parameters and reorganizing\nthem into general expressions to enhance the applicability and effectiveness of\ndelta parameter editing in post-trained models.", "published": "2024-10-17 17:56:53", "link": "http://arxiv.org/abs/2410.13841v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automatically Interpreting Millions of Features in Large Language Models", "abstract": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.", "published": "2024-10-17 17:56:01", "link": "http://arxiv.org/abs/2410.13928v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting AI-Generated Texts in Cross-Domains", "abstract": "Existing tools to detect text generated by a large language model (LLM) have\nmet with certain success, but their performance can drop when dealing with\ntexts in new domains. To tackle this issue, we train a ranking classifier\ncalled RoBERTa-Ranker, a modified version of RoBERTa, as a baseline model using\na dataset we constructed that includes a wider variety of texts written by\nhumans and generated by various LLMs. We then present a method to fine-tune\nRoBERTa-Ranker that requires only a small amount of labeled data in a new\ndomain. Experiments show that this fine-tuned domain-aware model outperforms\nthe popular DetectGPT and GPTZero on both in-domain and cross-domain texts,\nwhere AI-generated texts may either be in a different domain or generated by a\ndifferent LLM not used to generate the training datasets. This approach makes\nit feasible and economical to build a single system to detect AI-generated\ntexts across various domains.", "published": "2024-10-17 18:43:30", "link": "http://arxiv.org/abs/2410.13966v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Personalized Adaptation via In-Context Preference Learning", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used to align\nLanguage Models (LMs) with human preferences. However, existing approaches\noften neglect individual user preferences, leading to suboptimal\npersonalization. We present the Preference Pretrained Transformer (PPT), a\nnovel approach for adaptive personalization using online user feedback. PPT\nleverages the in-context learning capabilities of transformers to dynamically\nadapt to individual preferences. Our approach consists of two phases: (1) an\noffline phase where we train a single policy model using a history-dependent\nloss function, and (2) an online phase where the model adapts to user\npreferences through in-context learning. We demonstrate PPT's effectiveness in\na contextual bandit setting, showing that it achieves personalized adaptation\nsuperior to existing methods while significantly reducing the computational\ncosts. Our results suggest the potential of in-context learning for scalable\nand efficient personalization in large language models.", "published": "2024-10-17 20:06:02", "link": "http://arxiv.org/abs/2410.14001v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education", "abstract": "With the increasing adoption of large language models (LLMs) in education,\nconcerns about inherent biases in these models have gained prominence. We\nevaluate LLMs for bias in the personalized educational setting, specifically\nfocusing on the models' roles as \"teachers.\" We reveal significant biases in\nhow models generate and select educational content tailored to different\ndemographic groups, including race, ethnicity, sex, gender, disability status,\nincome, and national origin. We introduce and apply two bias score\nmetrics--Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB)--to analyze\n9 open and closed state-of-the-art LLMs. Our experiments, which utilize over\n17,000 educational explanations across multiple difficulty levels and topics,\nuncover that models potentially harm student learning by both perpetuating\nharmful stereotypes and reversing them. We find that bias is similar for all\nfrontier models, with the highest MAB along income levels while MDB is highest\nrelative to both income and disability status. For both metrics, we find the\nlowest bias exists for sex/gender and race/ethnicity.", "published": "2024-10-17 20:27:44", "link": "http://arxiv.org/abs/2410.14012v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly\n  Sampled Time Series", "abstract": "Interacting systems are prevalent in nature. It is challenging to accurately\npredict the dynamics of the system if its constituent components are analyzed\nindependently. We develop a graph-based model that unveils the systemic\ninteractions of time series observed at irregular time points, by using a\ndirected acyclic graph to model the conditional dependencies (a form of causal\nnotation) of the system components and learning this graph in tandem with a\ncontinuous-time model that parameterizes the solution curves of ordinary\ndifferential equations (ODEs). Our technique, a graph neural flow, leads to\nsubstantial enhancements over non-graph-based methods, as well as graph-based\nmethods without the modeling of conditional dependencies. We validate our\napproach on several tasks, including time series classification and\nforecasting, to demonstrate its efficacy.", "published": "2024-10-17 21:10:39", "link": "http://arxiv.org/abs/2410.14030v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow\n  for Personalized Nutrition Coaching", "abstract": "Effective management of cardiometabolic conditions requires sustained\npositive nutrition habits, often hindered by complex and individualized\nbarriers. Direct human management is simply not scalable, while previous\nattempts aimed at automating nutrition coaching lack the personalization needed\nto address these diverse challenges. This paper introduces a novel LLM-powered\nagentic workflow designed to provide personalized nutrition coaching by\ndirectly targeting and mitigating patient-specific barriers. Grounded in\nbehavioral science principles, the workflow leverages a comprehensive mapping\nof nutrition-related barriers to corresponding evidence-based strategies. A\nspecialized LLM agent intentionally probes for and identifies the root cause of\na patient's dietary struggles. Subsequently, a separate LLM agent delivers\ntailored tactics designed to overcome those specific barriers with patient\ncontext. We designed and validated our approach through a user study with\nindividuals with cardiometabolic conditions, demonstrating the system's ability\nto accurately identify barriers and provide personalized guidance. Furthermore,\nwe conducted a large-scale simulation study, grounding on real patient\nvignettes and expert-validated metrics, to evaluate the system's performance\nacross a wide range of scenarios. Our findings demonstrate the potential of\nthis LLM-powered agentic workflow to improve nutrition coaching by providing\npersonalized, scalable, and behaviorally-informed interventions.", "published": "2024-10-17 21:35:07", "link": "http://arxiv.org/abs/2410.14041v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieval of Temporal Event Sequences from Textual Descriptions", "abstract": "Retrieving temporal event sequences from textual descriptions is crucial for\napplications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. To advance this task, we introduce\nTESRBench, a comprehensive benchmark for temporal event sequence retrieval\n(TESR) from textual descriptions. TESRBench includes diverse real-world\ndatasets with synthesized and reviewed textual descriptions, providing a strong\nfoundation for evaluating retrieval performance and addressing challenges in\nthis domain. Building on this benchmark, we propose TPP-Embedding, a novel\nmodel for embedding and retrieving event sequences. The model leverages the\nTPP-LLM framework, integrating large language models (LLMs) with temporal point\nprocesses (TPPs) to encode both event texts and times. By pooling\nrepresentations and applying a contrastive loss, it unifies temporal dynamics\nand event semantics in a shared embedding space, aligning sequence-level\nembeddings of event sequences and their descriptions. TPP-Embedding\ndemonstrates superior performance over baseline models across TESRBench\ndatasets, establishing it as a powerful solution for the temporal event\nsequence retrieval task.", "published": "2024-10-17 21:35:55", "link": "http://arxiv.org/abs/2410.14043v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented\n  Generation from Multilingual Knowledge Graphs", "abstract": "Translating text that contains entity names is a challenging task, as\ncultural-related references can vary significantly across languages. These\nvariations may also be caused by transcreation, an adaptation process that\nentails more than transliteration and word-for-word translation. In this paper,\nwe address the problem of cross-cultural translation on two fronts: (i) we\nintroduce XC-Translate, the first large-scale, manually-created benchmark for\nmachine translation that focuses on text that contains potentially\nculturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end\nmethod to integrate information from a multilingual knowledge graph into a\nneural machine translation model by leveraging a dense retrieval mechanism. Our\nexperiments and analyses show that current machine translation systems and\nlarge language models still struggle to translate texts containing entity\nnames, whereas KG-MT outperforms state-of-the-art approaches by a large margin,\nobtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4,\nrespectively.", "published": "2024-10-17 21:56:22", "link": "http://arxiv.org/abs/2410.14057v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against\n  Forecasting Harmful User Behaviors", "abstract": "Conversation forecasting tasks a model with predicting the outcome of an\nunfolding conversation. For instance, it can be applied in social media\nmoderation to predict harmful user behaviors before they occur, allowing for\npreventative interventions. While large language models (LLMs) have recently\nbeen proposed as an effective tool for conversation forecasting, it's unclear\nwhat biases they may have, especially against forecasting the (potentially\nharmful) outcomes we request them to predict during moderation. This paper\nexplores to what extent model uncertainty can be used as a tool to mitigate\npotential biases. Specifically, we ask three primary research questions: 1) how\ndoes LLM forecasting accuracy change when we ask models to represent their\nuncertainty; 2) how does LLM bias change when we ask models to represent their\nuncertainty; 3) how can we use uncertainty representations to reduce or\ncompletely mitigate biases without many training data points. We address these\nquestions for 5 open-source language models tested on 2 datasets designed to\nevaluate conversation forecasting for social media moderation.", "published": "2024-10-17 15:07:53", "link": "http://arxiv.org/abs/2410.14744v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Fine-tuning for Large Language Models", "abstract": "Supervised fine-tuning (SFT) is crucial in adapting large language model\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated.Towards this end, we introduce a semi-supervised\nfine-tuning(SemiFT) task and a framework named SemiEvol for LLM alignment from\na propagate-and-select manner. For knowledge propagation, SemiEvol adopts a\nbi-level approach, propagating knowledge from labeled data to unlabeled data\nthrough both in-weight and in-context methods. For knowledge selection,\nSemiEvol incorporates a collaborative learning mechanism, selecting\nhigher-quality pseudo-response samples. We conducted experiments using\nGPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets,\ndemonstrating significant improvements in model performance on target data.\nFurthermore, we compared SemiEvol with SFT and self-evolution methods,\nhighlighting its practicality in hybrid data scenarios.", "published": "2024-10-17 16:59:46", "link": "http://arxiv.org/abs/2410.14745v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ethics Whitepaper: Whitepaper on Ethical Research into Large Language\n  Models", "abstract": "This whitepaper offers an overview of the ethical considerations surrounding\nresearch into or with large language models (LLMs). As LLMs become more\nintegrated into widely used applications, their societal impact increases,\nbringing important ethical questions to the forefront. With a growing body of\nwork examining the ethical development, deployment, and use of LLMs, this\nwhitepaper provides a comprehensive and practical guide to best practices,\ndesigned to help those in research and in industry to uphold the highest\nethical standards in their work.", "published": "2024-10-17 18:36:02", "link": "http://arxiv.org/abs/2410.19812v1", "categories": ["cs.CY", "cs.CL", "I.2"], "primary_category": "cs.CY"}
{"title": "A Little Human Data Goes A Long Way", "abstract": "Faced with an expensive human annotation process, creators of NLP systems\nincreasingly turn to synthetic data generation. While this method shows\npromise, the extent to which synthetic data can replace human annotation is\npoorly understood. We investigate the use of synthetic data in Fact\nVerification (FV) and Question Answering (QA) by studying the effects of\nincrementally replacing human generated data with synthetic points on eight\ndiverse datasets. Strikingly, replacing up to 90% of the training data only\nmarginally decreases performance, but replacing the final 10% leads to severe\ndeclines. We find that models trained on purely synthetic data can be reliably\nimproved by including as few as 125 human generated data points. We show that\nmatching the performance gain of just a little additional human data (only 200\npoints) requires an order of magnitude more synthetic data and estimate price\nratios at which human annotation would be a more cost-effective solution. Our\nresults suggest that even when human annotation at scale is infeasible, there\nis great value to having a small proportion of the dataset being human\ngenerated.", "published": "2024-10-17 00:04:02", "link": "http://arxiv.org/abs/2410.13098v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controllable Generation via Locally Constrained Resampling", "abstract": "Autoregressive models have demonstrated an unprecedented ability at modeling\nthe intricacies of natural language. However, they continue to struggle with\ngenerating complex outputs that adhere to logical constraints. Sampling from a\nfully-independent distribution subject to a constraint is hard. Sampling from\nan autoregressive distribution subject to a constraint is doubly hard: We have\nto contend not only with the hardness of the constraint but also the\ndistribution's lack of structure. We propose a tractable probabilistic approach\nthat performs Bayesian conditioning to draw samples subject to a constraint.\nOur approach considers the entire sequence, leading to a more globally optimal\nconstrained generation than current greedy methods. Starting from a model\nsample, we induce a local, factorized distribution which we can tractably\ncondition on the constraint. To generate samples that satisfy the constraint,\nwe sample from the conditional distribution, correct for biases in the samples\nand resample. The resulting samples closely approximate the target distribution\nand are guaranteed to satisfy the constraints. We evaluate our approach on\nseveral tasks, including LLM detoxification and solving Sudoku puzzles. We show\nthat by disallowing a list of toxic expressions our approach is able to steer\nthe model's outputs away from toxic generations, outperforming similar\napproaches to detoxification. We conclude by showing that our approach achieves\na perfect accuracy on Sudoku compared to <50% for GPT4-o and Gemini 1.5.", "published": "2024-10-17 00:49:53", "link": "http://arxiv.org/abs/2410.13111v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Data Defenses Against Large Language Models", "abstract": "Large language models excel at performing inference over text to extract\ninformation, summarize information, or generate additional text. These\ninference capabilities are implicated in a variety of ethical harms spanning\nsurveillance, labor displacement, and IP/copyright theft. While many policy,\nlegal, and technical mitigations have been proposed to counteract these harms,\nthese mitigations typically require cooperation from institutions that move\nslower than technical advances (i.e., governments) or that have few incentives\nto act to counteract these harms (i.e., the corporations that create and profit\nfrom these LLMs). In this paper, we define and build \"data defenses\" -- a novel\nstrategy that directly empowers data owners to block LLMs from performing\ninference on their data. We create data defenses by developing a method to\nautomatically generate adversarial prompt injections that, when added to input\ntext, significantly reduce the ability of LLMs to accurately infer personally\nidentifying information about the subject of the input text or to use\ncopyrighted text in inference. We examine the ethics of enabling such direct\nresistance to LLM inference, and argue that making data defenses that resist\nand subvert LLMs enables the realization of important values such as data\nownership, data sovereignty, and democratic control over AI systems. We verify\nthat our data defenses are cheap and fast to generate, work on the latest\ncommercial and open-source LLMs, resistance to countermeasures, and are robust\nto several different attack settings. Finally, we consider the security\nimplications of LLM data defenses and outline several future research\ndirections in this area. Our code is available at\nhttps://github.com/wagnew3/LLMDataDefenses and a tool for using our defenses to\nprotect text against LLM inference is at\nhttps://wagnew3.github.io/LLM-Data-Defenses/.", "published": "2024-10-17 01:51:56", "link": "http://arxiv.org/abs/2410.13138v1", "categories": ["cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "An Evolved Universal Transformer Memory", "abstract": "Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.", "published": "2024-10-17 02:47:10", "link": "http://arxiv.org/abs/2410.13166v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing", "abstract": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars.", "published": "2024-10-17 03:32:02", "link": "http://arxiv.org/abs/2410.13187v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Failing Forward: Improving Generative Error Correction for ASR with\n  Synthetic Data and Retrieval Augmentation", "abstract": "Generative Error Correction (GEC) has emerged as a powerful post-processing\nmethod to enhance the performance of Automatic Speech Recognition (ASR)\nsystems. However, we show that GEC models struggle to generalize beyond the\nspecific types of errors encountered during training, limiting their ability to\ncorrect new, unseen errors at test time, particularly in out-of-domain (OOD)\nscenarios. This phenomenon amplifies with named entities (NEs), where, in\naddition to insufficient contextual information or knowledge about the NEs,\nnovel NEs keep emerging. To address these issues, we propose DARAG (Data- and\nRetrieval-Augmented Generative Error Correction), a novel approach designed to\nimprove GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC\ntraining dataset with synthetic data generated by prompting LLMs and\ntext-to-speech models, thereby simulating additional errors from which the\nmodel can learn. For OOD scenarios, we simulate test-time errors from new\ndomains similarly and in an unsupervised fashion. Additionally, to better\nhandle named entities, we introduce retrieval-augmented correction by\naugmenting the input with entities retrieved from a database. Our approach is\nsimple, scalable, and both domain- and language-agnostic. We experiment on\nmultiple datasets and settings, showing that DARAG outperforms all our\nbaselines, achieving 8\\% -- 30\\% relative WER improvements in ID and 10\\% --\n33\\% improvements in OOD settings.", "published": "2024-10-17 04:00:29", "link": "http://arxiv.org/abs/2410.13198v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model\n  with Meta-Exploration", "abstract": "The diffusion model, a new generative modeling paradigm, has achieved\nsignificant success in generating images, audio, video, and text. It has been\nadapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq,\ntermed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed\nor hand-crafted rules to schedule noise during the diffusion and denoising\nprocesses. However, these models are limited by non-contextualized noise, which\nfails to fully consider the characteristics of Seq2Seq tasks. In this paper, we\npropose the Meta-DiffuB framework - a novel scheduler-exploiter S2S-Diffusion\nparadigm designed to overcome the limitations of existing S2S-Diffusion models.\nWe employ Meta-Exploration to train an additional scheduler model dedicated to\nscheduling contextualized noise for each sentence. Our exploiter model, an\nS2S-Diffusion model, leverages the noise scheduled by our scheduler model for\nupdating and generation. Meta-DiffuB achieves state-of-the-art performance\ncompared to previous S2S-Diffusion models and fine-tuned pre-trained language\nmodels (PLMs) across four Seq2Seq benchmark datasets. We further investigate\nand visualize the impact of Meta-DiffuB's noise scheduling on the generation of\nsentences with varying difficulties. Additionally, our scheduler model can\nfunction as a \"plug-and-play\" model to enhance DiffuSeq without the need for\nfine-tuning during the inference stage.", "published": "2024-10-17 04:06:02", "link": "http://arxiv.org/abs/2410.13201v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Free-Form Decision-Making Inconsistency of Language Models in\n  Military Crisis Simulations", "abstract": "There is an increasing interest in using language models (LMs) for automated\ndecision-making, with multiple countries actively testing LMs to aid in\nmilitary crisis decision-making. To scrutinize relying on LM decision-making in\nhigh-stakes settings, we examine the inconsistency of responses in a crisis\nsimulation (\"wargame\"), similar to reported tests conducted by the US military.\nPrior work illustrated escalatory tendencies and varying levels of aggression\namong LMs but were constrained to simulations with pre-defined actions. This\nwas due to the challenges associated with quantitatively measuring semantic\ndifferences and evaluating natural language decision-making without relying on\npre-defined actions. In this work, we query LMs for free form responses and use\na metric based on BERTScore to measure response inconsistency quantitatively.\nLeveraging the benefits of BERTScore, we show that the inconsistency metric is\nrobust to linguistic variations that preserve semantic meaning in a\nquestion-answering setting across text lengths. We show that all five tested\nLMs exhibit levels of inconsistency that indicate semantic differences, even\nwhen adjusting the wargame setting, anonymizing involved conflict countries, or\nadjusting the sampling temperature parameter $T$. Further qualitative\nevaluation shows that models recommend courses of action that share few to no\nsimilarities. We also study the impact of different prompt sensitivity\nvariations on inconsistency at temperature $T = 0$. We find that inconsistency\ndue to semantically equivalent prompt variations can exceed response\ninconsistency from temperature sampling for most studied models across\ndifferent levels of ablations. Given the high-stakes nature of military\ndeployment, we recommend further consideration be taken before using LMs to\ninform military decisions or other cases of high-stakes decision-making.", "published": "2024-10-17 04:12:17", "link": "http://arxiv.org/abs/2410.13204v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CBT-Bench: Evaluating Large Language Models on Assisting Cognitive\n  Behavior Therapy", "abstract": "There is a significant gap between patient needs and available mental health\nsupport today. In this paper, we aim to thoroughly examine the potential of\nusing Large Language Models (LLMs) to assist professional psychotherapy. To\nthis end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation\nof cognitive behavioral therapy (CBT) assistance. We include three levels of\ntasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of\nmultiple-choice questions; II: Cognitive model understanding, with the tasks of\ncognitive distortion classification, primary core belief classification, and\nfine-grained core belief classification; III: Therapeutic response generation,\nwith the task of generating responses to patient speech in CBT therapy\nsessions. These tasks encompass key aspects of CBT that could potentially be\nenhanced through AI assistance, while also outlining a hierarchy of capability\nrequirements, ranging from basic knowledge recitation to engaging in real\ntherapeutic conversations. We evaluated representative LLMs on our benchmark.\nExperimental results indicate that while LLMs perform well in reciting CBT\nknowledge, they fall short in complex real-world scenarios requiring deep\nanalysis of patients' cognitive structures and generating effective responses,\nsuggesting potential future work.", "published": "2024-10-17 04:52:57", "link": "http://arxiv.org/abs/2410.13218v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Easily Confused: A Quantitative Metric,\n  Security Implications and Typological Analysis", "abstract": "Language Confusion is a phenomenon where Large Language Models (LLMs)\ngenerate text that is neither in the desired language, nor in a contextually\nappropriate language. This phenomenon presents a critical challenge in text\ngeneration by LLMs, often appearing as erratic and unpredictable behavior. We\nhypothesize that there are linguistic regularities to this inherent\nvulnerability in LLMs and shed light on patterns of language confusion across\nLLMs. We introduce a novel metric, Language Confusion Entropy, designed to\ndirectly measure and quantify this confusion, based on language distributions\ninformed by linguistic typology and lexical variation. Comprehensive\ncomparisons with the Language Confusion Benchmark (Marchisio et al., 2024)\nconfirm the effectiveness of our metric, revealing patterns of language\nconfusion across LLMs. We further link language confusion to LLM security, and\nfind patterns in the case of multilingual embedding inversion attacks. Our\nanalysis demonstrates that linguistic typology offers theoretically grounded\ninterpretation, and valuable insights into leveraging language similarities as\na prior for LLM alignment and security.", "published": "2024-10-17 05:43:30", "link": "http://arxiv.org/abs/2410.13237v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "I.1.2; I.1.5"], "primary_category": "cs.CL"}
{"title": "Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation", "abstract": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. We will release our code and datasets\nupon acceptance.", "published": "2024-10-17 06:15:00", "link": "http://arxiv.org/abs/2410.13248v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages\n  Using Large Language Models", "abstract": "Challenges in managing linguistic diversity and integrating various musical\nmodalities are faced by current music information retrieval systems. These\nlimitations reduce their effectiveness in a global, multimodal music\nenvironment. To address these issues, we introduce CLaMP 2, a system compatible\nwith 101 languages that supports both ABC notation (a text-based musical\nnotation format) and MIDI (Musical Instrument Digital Interface) for music\ninformation retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text\ntriplets, includes a multilingual text encoder and a multimodal music encoder\naligned via contrastive learning. By leveraging large language models, we\nobtain refined and consistent multilingual descriptions at scale, significantly\nreducing textual noise and balancing language distribution. Our experiments\nshow that CLaMP 2 achieves state-of-the-art results in both multilingual\nsemantic search and music classification across modalities, thus establishing a\nnew standard for inclusive and global music information retrieval.", "published": "2024-10-17 06:43:54", "link": "http://arxiv.org/abs/2410.13267v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Roadmap towards Superhuman Speech Understanding using Large Language\n  Models", "abstract": "The success of large language models (LLMs) has prompted efforts to integrate\nspeech and audio data, aiming to create general foundation models capable of\nprocessing both textual and non-textual inputs. Recent advances, such as\nGPT-4o, highlight the potential for end-to-end speech LLMs, which preserves\nnon-semantic information and world knowledge for deeper speech understanding.\nTo guide the development of speech LLMs, we propose a five-level roadmap,\nranging from basic automatic speech recognition (ASR) to advanced superhuman\nmodels capable of integrating non-semantic information with abstract acoustic\nknowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark,\nthat standardizes critical aspects across various tasks in these five levels,\nuncovering challenges in using abstract acoustic knowledge and completeness of\ncapability. Our findings reveal gaps in handling paralinguistic cues and\nabstract acoustic knowledge, and we offer future directions. This paper\noutlines a roadmap for advancing speech LLMs, introduces a benchmark for\nevaluation, and provides key insights into their current limitations and\npotential.", "published": "2024-10-17 06:44:06", "link": "http://arxiv.org/abs/2410.13268v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Route LLMs with Confidence Tokens", "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nseveral tasks and are increasingly deployed in real-world applications.\nHowever, especially in high-stakes settings, it becomes vital to know when the\noutput of an LLM may be unreliable. Depending on whether an answer is\ntrustworthy, a system can then choose to route the question to another expert,\nor otherwise fall back on a safe default behavior. In this work, we study the\nextent to which LLMs can reliably indicate confidence in their answers, and how\nthis notion of confidence can translate into downstream accuracy gains. We\npropose Self-REF, a lightweight training strategy to teach LLMs to express\nconfidence in whether their answers are correct in a reliable manner. Self-REF\nintroduces confidence tokens into the LLM, from which a confidence score can be\nextracted. Compared to conventional approaches such as verbalizing confidence\nand examining token probabilities, we demonstrate empirically that confidence\ntokens show significant improvements in downstream routing and rejection\nlearning tasks.", "published": "2024-10-17 07:28:18", "link": "http://arxiv.org/abs/2410.13284v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Large Vision-Language Models via\n  Summary-Guided Decoding", "abstract": "Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in\ngenerating detailed and coherent responses from visual inputs. However, they\nare prone to generate hallucinations due to an over-reliance on language\npriors. To address this issue, we investigate the language priors in LVLMs and\nmake two key observations: (1) Even when predicting the tokens associated with\nimage-related part-of-speech (POS), models increasingly rely on linguistic\npriors as the token sequences grow, thereby amplifying hallucinations. (2)\nMethods that directly calibrate LVLM's output distribution to mitigate language\npriors can lead to a degradation in text quality or even exacerbate\nhallucinations. Based on these findings, we propose a novel method,\nSummary-Guided Decoding (SumGD). This method naturally encourages the model to\nfocus more on image information by reducing the text context through summaries,\nwhile controlling only the image-related POS tokens to maintain text quality.\nThrough experiments, we demonstrate that SumGD achieves state-of-the-art\nperformance on object hallucination benchmarks. Furthermore, in terms of the\ntrade-off between precision and recall, SumGD achieves Pareto optimality among\nthe existing methods. Lastly, we observe that although existing methods\nstruggle to balance the reduction of object hallucinations with maintaining\ntext quality, SumGD demonstrates robustness in handling this challenge.", "published": "2024-10-17 08:24:27", "link": "http://arxiv.org/abs/2410.13321v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models", "abstract": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs.", "published": "2024-10-17 08:46:09", "link": "http://arxiv.org/abs/2410.13334v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation Learning of Structured Data for Medical Foundation Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious domains, including healthcare. However, their ability to effectively\nrepresent structured non-textual data, such as the alphanumeric medical codes\nused in records like ICD-10 or SNOMED-CT, is limited and has been particularly\nexposed in recent research. This paper examines the challenges LLMs face in\nprocessing medical codes due to the shortcomings of current tokenization\nmethods. As a result, we introduce the UniStruct architecture to design a\nmultimodal medical foundation model of unstructured text and structured data,\nwhich addresses these challenges by adapting subword tokenization techniques\nspecifically for the structured medical codes. Our approach is validated\nthrough model pre-training on both an extensive internal medical database and a\npublic repository of structured medical records. Trained on over 1 billion\ntokens on the internal medical database, the proposed model achieves up to a\n23% improvement in evaluation metrics, with around 2% gain attributed to our\nproposed tokenization. Additionally, when evaluated on the EHRSHOT public\nbenchmark with a 1/1000 fraction of the pre-training data, the UniStruct model\nimproves performance on over 42% of the downstream tasks. Our approach not only\nenhances the representation and generalization capabilities of patient-centric\nmodels but also bridges a critical gap in representation learning models'\nability to handle complex structured medical data, alongside unstructured text.", "published": "2024-10-17 09:02:28", "link": "http://arxiv.org/abs/2410.13351v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Use of Audio to Improve Dialogue Policies", "abstract": "With the significant progress of speech technologies, spoken goal-oriented\ndialogue systems are becoming increasingly popular. One of the main modules of\na dialogue system is typically the dialogue policy, which is responsible for\ndetermining system actions. This component usually relies only on audio\ntranscriptions, being strongly dependent on their quality and ignoring very\nimportant extralinguistic information embedded in the user's speech. In this\npaper, we propose new architectures to add audio information by combining\nspeech and text embeddings using a Double Multi-Head Attention component. Our\nexperiments show that audio embedding-aware dialogue policies outperform\ntext-based ones, particularly in noisy transcription scenarios, and that how\ntext and audio embeddings are combined is crucial to improve performance. We\nobtained a 9.8% relative improvement in the User Request Score compared to an\nonly-text-based dialogue system on the DSTC2 dataset.", "published": "2024-10-17 09:37:20", "link": "http://arxiv.org/abs/2410.13385v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MoR: Mixture of Ranks for Low-Rank Adaptation Tuning", "abstract": "Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.", "published": "2024-10-17 10:14:52", "link": "http://arxiv.org/abs/2410.13408v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive\n  Learning", "abstract": "Supervised contrastive learning has achieved remarkable success by leveraging\nlabel information; however, determining positive samples in multi-label\nscenarios remains a critical challenge. In multi-label supervised contrastive\nlearning (MSCL), relations among multi-label samples are not yet fully defined,\nleading to ambiguity in identifying positive samples and formulating\ncontrastive loss functions to construct the representation space. To address\nthese challenges, we: (i) first define five distinct multi-label relations in\nMSCL to systematically identify positive samples, (ii) introduce a novel\nSimilarity-Dissimilarity Loss that dynamically re-weights samples through\ncomputing the similarity and dissimilarity factors between positive samples and\ngiven anchors based on multi-label relations, and (iii) further provide\ntheoretical grounded proof for our method through rigorous mathematical\nanalysis that supports the formulation and effectiveness of the proposed loss\nfunction. We conduct the experiments across both image and text modalities, and\nextend the evaluation to medical domain. The results demonstrate that our\nmethod consistently outperforms baselines in a comprehensive evaluation,\nconfirming its effectiveness and robustness. Code is available at:\nhttps://github.com/guangminghuang/similarity-dissimilarity-loss.", "published": "2024-10-17 11:12:55", "link": "http://arxiv.org/abs/2410.13439v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Parameter-efficient Adaptation of Multilingual Multimodal Models for\n  Low-resource ASR", "abstract": "Automatic speech recognition (ASR) for low-resource languages remains a\nchallenge due to the scarcity of labeled training data. Parameter-efficient\nfine-tuning and text-only adaptation are two popular methods that have been\nused to address such low-resource settings. In this work, we investigate how\nthese techniques can be effectively combined using a multilingual multimodal\nmodel like SeamlessM4T. Multimodal models are able to leverage unlabeled text\nvia text-only adaptation with further parameter-efficient ASR fine-tuning, thus\nboosting ASR performance. We also show cross-lingual transfer from a\nhigh-resource language, achieving up to a relative 17% WER reduction over a\nbaseline in a zero-shot setting without any labeled speech.", "published": "2024-10-17 11:19:44", "link": "http://arxiv.org/abs/2410.13445v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland", "abstract": "Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals", "published": "2024-10-17 11:34:07", "link": "http://arxiv.org/abs/2410.13456v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Breaking the Manual Annotation Bottleneck: Creating a Comprehensive\n  Legal Case Criticality Dataset through Semi-Automated Labeling", "abstract": "Predicting case criticality helps legal professionals in the court system\nmanage large volumes of case law. This paper introduces the Criticality\nPrediction dataset, a new resource for evaluating the potential influence of\nSwiss Federal Supreme Court decisions on future jurisprudence. Unlike existing\napproaches that rely on resource-intensive manual annotations, we\nsemi-automatically derive labels leading to a much larger dataset than\notherwise possible. Our dataset features a two-tier labeling system: (1) the\nLD-Label, which identifies cases published as Leading Decisions (LD), and (2)\nthe Citation-Label, which ranks cases by their citation frequency and recency.\nThis allows for a more nuanced evaluation of case importance. We evaluate\nseveral multilingual models, including fine-tuned variants and large language\nmodels, and find that fine-tuned models consistently outperform zero-shot\nbaselines, demonstrating the need for task-specific adaptation. Our\ncontributions include the introduction of this task and the release of a\nmultilingual dataset to the research community.", "published": "2024-10-17 11:43:16", "link": "http://arxiv.org/abs/2410.13460v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes", "abstract": "Detecting offensive memes is crucial, yet standard deep neural network\nsystems often remain opaque. Various input attribution-based methods attempt to\ninterpret their behavior, but they face challenges with implicitly offensive\nmemes and non-causal attributions. To address these issues, we propose a\nframework based on a Structural Causal Model (SCM). In this framework,\nVisualBERT is trained to predict the class of an input meme based on both meme\ninput and causal concepts, allowing for transparent interpretation. Our\nqualitative evaluation demonstrates the framework's effectiveness in\nunderstanding model behavior, particularly in determining whether the model was\nright due to the right reason, and in identifying reasons behind\nmisclassification. Additionally, quantitative analysis assesses the\nsignificance of proposed modelling choices, such as de-confounding, adversarial\nlearning, and dynamic routing, and compares them with input attribution\nmethods. Surprisingly, we find that input attribution methods do not guarantee\ncausality within our framework, raising questions about their reliability in\nsafety-critical applications. The project page is at:\nhttps://newcodevelop.github.io/causality_adventure/", "published": "2024-10-17 12:32:00", "link": "http://arxiv.org/abs/2410.13488v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum\n  Learning, Semi-Supervised Training, and Advanced Optimization Techniques", "abstract": "Text generation is the automated process of producing written or spoken\nlanguage using computational methods. It involves generating coherent and\ncontextually relevant text based on predefined rules or learned patterns.\nHowever, challenges in text generation arise from maintaining coherence,\nensuring diversity and creativity, and avoiding biases or inappropriate\ncontent. This research paper developed a novel approach to improve text\ngeneration in the context of joint Natural Language Generation (NLG) and\nNatural Language Understanding (NLU) learning. The data is prepared by\ngathering and preprocessing annotated datasets, including cleaning,\ntokenization, stemming, and stop-word removal. Feature extraction techniques\nsuch as POS tagging, Bag of words, and Term Frequency-Inverse Document\nFrequency (TF-IDF) are applied. Transformer-based encoders and decoders,\ncapturing long range dependencies and improving source-target sequence\nmodelling. Pre-trained language models like Optimized BERT are incorporated,\nalong with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).\nReinforcement learning with policy gradient techniques, semi-supervised\ntraining, improved attention mechanisms, and differentiable approximations like\nstraight-through Gumbel SoftMax estimator are employed to fine-tune the models\nand handle complex linguistic tasks effectively. The proposed model is\nimplemented using Python.", "published": "2024-10-17 12:43:49", "link": "http://arxiv.org/abs/2410.13498v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs", "abstract": "Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.", "published": "2024-10-17 12:48:14", "link": "http://arxiv.org/abs/2410.13502v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models as Narrative-Driven Recommenders", "abstract": "Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.", "published": "2024-10-17 14:39:24", "link": "http://arxiv.org/abs/2410.13604v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "H2OVL-Mississippi Vision Language Models Technical Report", "abstract": "Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.", "published": "2024-10-17 14:46:34", "link": "http://arxiv.org/abs/2410.13611v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation", "abstract": "LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.", "published": "2024-10-17 15:09:24", "link": "http://arxiv.org/abs/2410.13640v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Attention Heads in Large Language Model Safety", "abstract": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.", "published": "2024-10-17 16:08:06", "link": "http://arxiv.org/abs/2410.13708v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient\n  Mobile Task Automation", "abstract": "Existing Multimodal Large Language Model (MLLM)-based agents face significant\nchallenges in handling complex GUI (Graphical User Interface) interactions on\ndevices. These challenges arise from the dynamic and structured nature of GUI\nenvironments, which integrate text, images, and spatial relationships, as well\nas the variability in action spaces across different pages and tasks. To\naddress these limitations, we propose MobA, a novel MLLM-based mobile assistant\nsystem. MobA introduces an adaptive planning module that incorporates a\nreflection mechanism for error recovery and dynamically adjusts plans to align\nwith the real environment contexts and action module's execution capacity.\nAdditionally, a multifaceted memory module provides comprehensive memory\nsupport to enhance adaptability and efficiency. We also present MobBench, a\ndataset designed for complex mobile interactions. Experimental results on\nMobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI\nenvironments and perform complex mobile task.", "published": "2024-10-17 16:53:50", "link": "http://arxiv.org/abs/2410.13757v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.MA"}
{"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.", "published": "2024-10-17 17:52:01", "link": "http://arxiv.org/abs/2410.13828v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation", "abstract": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.", "published": "2024-10-17 17:58:14", "link": "http://arxiv.org/abs/2410.13846v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation", "abstract": "In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.", "published": "2024-10-17 17:58:37", "link": "http://arxiv.org/abs/2410.13848v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Retrospective Learning from Interactions", "abstract": "Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.", "published": "2024-10-17 17:59:03", "link": "http://arxiv.org/abs/2410.13852v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?", "abstract": "As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.", "published": "2024-10-17 17:59:24", "link": "http://arxiv.org/abs/2410.13854v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of\n  LLMs", "abstract": "Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.", "published": "2024-10-17 17:59:35", "link": "http://arxiv.org/abs/2410.13857v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Identifying High Consideration E-Commerce Search Queries", "abstract": "In e-commerce, high consideration search missions typically require careful\nand elaborate decision making, and involve a substantial research investment\nfrom customers. We consider the task of identifying High Consideration (HC)\nqueries. Identifying such queries enables e-commerce sites to better serve user\nneeds using targeted experiences such as curated QA widgets that help users\nreach purchase decisions. We explore the task by proposing an Engagement-based\nQuery Ranking (EQR) approach, focusing on query ranking to indicate potential\nengagement levels with query-related shopping knowledge content during product\nsearch. Unlike previous studies on predicting trends, EQR prioritizes\nquery-level features related to customer behavior, finance, and catalog\ninformation rather than popularity signals. We introduce an accurate and\nscalable method for EQR and present experimental results demonstrating its\neffectiveness. Offline experiments show strong ranking performance. Human\nevaluation shows a precision of 96% for HC queries identified by our model. The\nmodel was commercially deployed, and shown to outperform human-selected queries\nin terms of downstream customer impact, as measured through engagement.", "published": "2024-10-17 18:22:42", "link": "http://arxiv.org/abs/2410.13951v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline", "abstract": "Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.", "published": "2024-10-17 18:34:43", "link": "http://arxiv.org/abs/2410.13959v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "I.2.7; H.3.3; I.2.6; I.5.3"], "primary_category": "cs.IR"}
{"title": "Debiasing Large Vision-Language Models by Ablating Protected Attribute\n  Representations", "abstract": "Large Vision Language Models (LVLMs) such as LLaVA have demonstrated\nimpressive capabilities as general-purpose chatbots that can engage in\nconversations about a provided input image. However, their responses are\ninfluenced by societal biases present in their training datasets, leading to\nundesirable differences in how the model responds when presented with images\ndepicting people of different demographics. In this work, we propose a novel\ndebiasing framework for LVLMs by directly ablating biased attributes during\ntext generation to avoid generating text related to protected attributes, or\neven representing them internally. Our method requires no training and a\nrelatively small amount of representative biased outputs (~1000 samples). Our\nexperiments show that not only can we can minimize the propensity of LVLMs to\ngenerate text related to protected attributes, but we can even use synthetic\ndata to inform the ablation while retaining captioning performance on real data\nsuch as COCO. Furthermore, we find the resulting generations from a debiased\nLVLM exhibit similar accuracy as a baseline biased model, showing that\ndebiasing effects can be achieved without sacrificing model performance.", "published": "2024-10-17 19:02:31", "link": "http://arxiv.org/abs/2410.13976v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Generating Signed Language Instructions in Large-Scale Dialogue Systems", "abstract": "We introduce a goal-oriented conversational AI system enhanced with American\nSign Language (ASL) instructions, presenting the first implementation of such a\nsystem on a worldwide multimodal conversational AI platform. Accessible through\na touch-based interface, our system receives input from users and seamlessly\ngenerates ASL instructions by leveraging retrieval methods and cognitively\nbased gloss translations. Central to our design is a sign translation module\npowered by Large Language Models, alongside a token-based video retrieval\nsystem for delivering instructional content from recipes and wikiHow guides.\nOur development process is deeply rooted in a commitment to community\nengagement, incorporating insights from the Deaf and Hard-of-Hearing community,\nas well as experts in cognitive and ASL learning sciences. The effectiveness of\nour signing instructions is validated by user feedback, achieving ratings on\npar with those of the system in its non-signing variant. Additionally, our\nsystem demonstrates exceptional performance in retrieval accuracy and\ntext-generation quality, measured by metrics such as BERTScore. We have made\nour codebase and datasets publicly accessible at\nhttps://github.com/Merterm/signed-dialogue, and a demo of our signed\ninstruction video retrieval system is available at\nhttps://huggingface.co/spaces/merterm/signed-instructions.", "published": "2024-10-17 20:56:29", "link": "http://arxiv.org/abs/2410.14026v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning Multimodal Cues of Children's Uncertainty", "abstract": "Understanding uncertainty plays a critical role in achieving common ground\n(Clark et al.,1983). This is especially important for multimodal AI systems\nthat collaborate with users to solve a problem or guide the user through a\nchallenging concept. In this work, for the first time, we present a dataset\nannotated in collaboration with developmental and cognitive psychologists for\nthe purpose of studying nonverbal cues of uncertainty. We then present an\nanalysis of the data, studying different roles of uncertainty and its\nrelationship with task difficulty and performance. Lastly, we present a\nmultimodal machine learning model that can predict uncertainty given a\nreal-time video clip of a participant, which we find improves upon a baseline\nmultimodal transformer model. This work informs research on cognitive\ncoordination between human-human and human-AI and has broad implications for\ngesture understanding and generation. The anonymized version of our data and\ncode will be publicly available upon the completion of the required consent\nforms and data sheets.", "published": "2024-10-17 21:46:00", "link": "http://arxiv.org/abs/2410.14050v1", "categories": ["cs.CL", "cs.CV", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs", "abstract": "Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.", "published": "2024-10-17 21:47:11", "link": "http://arxiv.org/abs/2410.14052v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Vision-Language Models by Summarizing Visual Tokens into\n  Compact Registers", "abstract": "Recent advancements in vision-language models (VLMs) have expanded their\npotential for real-world applications, enabling these models to perform complex\nreasoning on images. In the widely used fully autoregressive transformer-based\nmodels like LLaVA, projected visual tokens are prepended to textual tokens.\nOftentimes, visual tokens are significantly more than prompt tokens, resulting\nin increased computational overhead during both training and inference. In this\npaper, we propose Visual Compact Token Registers (Victor), a method that\nreduces the number of visual tokens by summarizing them into a smaller set of\nregister tokens. Victor adds a few learnable register tokens after the visual\ntokens and summarizes the visual information into these registers using the\nfirst few layers in the language tower of VLMs. After these few layers, all\nvisual tokens are discarded, significantly improving computational efficiency\nfor both training and inference. Notably, our method is easy to implement and\nrequires a small number of new trainable parameters with minimal impact on\nmodel performance. In our experiment, with merely 8 visual registers--about 1%\nof the original tokens--Victor shows less than a 4% accuracy drop while\nreducing the total training time by 43% and boosting the inference throughput\nby 3.3X.", "published": "2024-10-17 22:45:13", "link": "http://arxiv.org/abs/2410.14072v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Accounting for Sycophancy in Language Model Uncertainty Estimation", "abstract": "Effective human-machine collaboration requires machine learning models to\nexternalize uncertainty, so users can reflect and intervene when necessary. For\nlanguage models, these representations of uncertainty may be impacted by\nsycophancy bias: proclivity to agree with users, even if they are wrong. For\ninstance, models may be over-confident in (incorrect) problem solutions\nsuggested by a user. We study the relationship between sycophancy and\nuncertainty estimation for the first time. We propose a generalization of the\ndefinition of sycophancy bias to measure downstream impacts on uncertainty\nestimation, and also propose a new algorithm (SyRoUP) to account for sycophancy\nin the uncertainty estimation process. Unlike previous works on sycophancy, we\nstudy a broad array of user behaviors, varying both correctness and confidence\nof user suggestions to see how model answers (and their certainty) change. Our\nexperiments across conversation forecasting and question-answering tasks show\nthat user confidence plays a critical role in modulating the effects of\nsycophancy, and that SyRoUP can better predict these effects. From these\nresults, we argue that externalizing both model and user uncertainty can help\nto mitigate the impacts of sycophancy bias.", "published": "2024-10-17 18:00:25", "link": "http://arxiv.org/abs/2410.14746v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ETF: An Entity Tracing Framework for Hallucination Detection in Code\n  Summaries", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.", "published": "2024-10-17 19:38:55", "link": "http://arxiv.org/abs/2410.14748v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "SouLLMate: An Application Enhancing Diverse Mental Health Support with\n  Adaptive LLMs, Prompt Engineering, and RAG Techniques", "abstract": "Mental health issues significantly impact individuals' daily lives, yet many\ndo not receive the help they need even with available online resources. This\nstudy aims to provide diverse, accessible, stigma-free, personalized, and\nreal-time mental health support through cutting-edge AI technologies. It makes\nthe following contributions: (1) Conducting an extensive survey of recent\nmental health support methods to identify prevalent functionalities and unmet\nneeds. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates\nLLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt\nengineering, and domain knowledge. This system offers advanced features such as\nRisk Detection and Proactive Guidance Dialogue, and utilizes RAG for\npersonalized profile uploads and Conversational Information Extraction. (3)\nDeveloping novel evaluation approaches for preliminary assessments and risk\ndetection via professionally annotated interview data and real-life suicide\ntendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive\nQuestioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to\nenhance model performance and usability through context-sensitive response\nadjustments, semantic coherence evaluations, and enhanced accuracy of\nlong-context reasoning in language models. This study contributes to advancing\nmental health support technologies, potentially improving the accessibility and\neffectiveness of mental health care globally.", "published": "2024-10-17 22:04:32", "link": "http://arxiv.org/abs/2410.16322v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Optimizing Preference Alignment with Differentiable NDCG Ranking", "abstract": "Aligning large language models with human preferences improves interaction\nquality and safety by ensuring outputs better reflect human values. A promising\nstrategy involves Reinforcement Learning from Human Feedback (RLHF), starting\nwith collecting and ranking responses generated by a supervised fine-tuning\nmodel to refine alignment. Current methods (DPO) focus on learning from\npairwise preference data, categorizing responses into preferred and less\npreferred pairs, and optimizing by maximizing pairwise margins. Recent studies\nhave uncovered a substantial discrepancy between the theoretical aspirations of\npreference learning and its real-world results. Current preference alignment\ntechniques underperform expectations, with ranking accuracies below $60\\%$ on\nstandard datasets. This suggests existing methods inadequately capture ideal\npreference relationships within sequences. To address this challenge, this\npaper introduces \\underline{D}irect \\underline{R}anking \\underline{P}reference\n\\underline{O}ptimization (DRPO), a novel method that views human preference\nalignment as a Learning-to-Rank (LTR) task. DRPO leverages NDCG, a widely used\nLTR metric, to optimize the ranking of responses within lists based on\npreference data, thereby enhancing ranking accuracies. Due to the\nnondifferentiability of NDCG, we propose diffNDCG loss, a differentiable\napproximation facilitated by a sorting network to simulate NDCG. Furthermore,\nto improve the quality of generated response, we propose a novel margin-based\nAdaptive Rank Policy Score. Extensive experiments have shown that DRPO\noutperforms existing baseline methods, enhancing the quality of the generated\nresponses.", "published": "2024-10-17 08:54:57", "link": "http://arxiv.org/abs/2410.18127v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "FIRE: Fact-checking with Iterative Retrieval and Verification", "abstract": "Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.", "published": "2024-10-17 06:44:18", "link": "http://arxiv.org/abs/2411.00784v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Decomposition Dilemmas: Does Claim Decomposition Boost or Burden\n  Fact-Checking Performance?", "abstract": "Fact-checking pipelines increasingly adopt the Decompose-Then-Verify\nparadigm, where texts are broken down into smaller claims for individual\nverification and subsequently combined for a veracity decision. While\ndecomposition is widely-adopted in such pipelines, its effects on final\nfact-checking performance remain underexplored. Some studies have reported\nimprovements from decompostition, while others have observed performance\ndeclines, indicating its inconsistent impact. To date, no comprehensive\nanalysis has been conducted to understand this variability. To address this\ngap, we present an in-depth analysis that explicitly examines the impact of\ndecomposition on downstream verification performance. Through error case\ninspection and experiments, we introduce a categorization of decomposition\nerrors and reveal a trade-off between accuracy gains and the noise introduced\nthrough decomposition. Our analysis provides new insights into understanding\ncurrent system's instability and offers guidance for future studies toward\nimproving claim decomposition in fact-checking pipelines.", "published": "2024-10-17 11:45:59", "link": "http://arxiv.org/abs/2411.02400v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech\n  Representation Learning", "abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic\nModeling), a novel self-supervised learning approach for speech representation\nlearning. In contrast to the prior methods that use random masking schemes for\nMasked Acoustic Modeling (MAM), we introduce a novel selective and adaptive\nmasking strategy. Specifically, during SSL training, we progressively introduce\nharder regions to the model for reconstruction. Our approach automatically\nselects hard regions and is built on the observation that the reconstruction\nloss of individual frames in MAM can provide natural signals to judge the\ndifficulty of solving the MAM pre-text task for that frame. To identify these\nhard regions, we employ a teacher model that first predicts the frame-wise\nlosses and then decides which frames to mask. By learning to create challenging\nproblems, such as identifying harder frames and solving them simultaneously,\nthe model is able to learn more effective representations and thereby acquire a\nmore comprehensive understanding of the speech. Quantitatively, EH-MAM\noutperforms several state-of-the-art baselines across various low-resource\nspeech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a\nthorough analysis to show that the regions masked by EH-MAM effectively capture\nuseful context across speech frames.", "published": "2024-10-17 02:59:22", "link": "http://arxiv.org/abs/2410.13179v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language\n  Models", "abstract": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://hoar012.github.io/RAP-Project/.", "published": "2024-10-17 09:10:26", "link": "http://arxiv.org/abs/2410.13360v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Optimal Quantization for Matrix Multiplication", "abstract": "Recent work in machine learning community proposed multiple methods for\nperforming lossy compression (quantization) of large matrices. This\nquantization is important for accelerating matrix multiplication (main\ncomponent of large language models), which is often bottlenecked by the speed\nof loading these matrices from memory. Unlike classical vector quantization and\nrate-distortion theory, the goal of these new compression algorithms is to be\nable to approximate not the matrices themselves, but their matrix product.\nSpecifically, given a pair of real matrices $A,B$ an encoder (compressor) is\napplied to each of them independently producing descriptions with $R$ bits per\nentry. These representations subsequently are used by the decoder to estimate\nmatrix product $A^\\top B$. In this work, we provide a non-asymptotic lower\nbound on the mean squared error of this approximation (as a function of rate\n$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,\nwe construct a universal quantizer based on nested lattices with an explicit\nguarantee of approximation error for any (non-random) pair of matrices $A$, $B$\nin terms of only Frobenius norms $\\|\\bar{A}\\|_F, \\|\\bar{B}\\|_F$ and\n$\\|\\bar{A}^\\top \\bar{B}\\|_F$, where $\\bar{A},\\bar{B}$ are versions of $A,B$\nwith zero-centered columns, respectively. For iid Gaussian matrices our\nquantizer achieves the lower bound and is, thus, asymptotically optimal. A\npractical low-complexity version of our quantizer achieves performance quite\nclose to optimal. In addition, we derive rate-distortion function for matrix\nmultiplication of iid Gaussian matrices, which exhibits an interesting\nphase-transition at $R\\approx 0.906$ bit/entry.", "published": "2024-10-17 17:19:48", "link": "http://arxiv.org/abs/2410.13780v2", "categories": ["cs.IT", "cs.AI", "cs.CL", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "ControlAgent: Automating Control System Design via Novel Integration of\n  LLM Agents and Domain Expertise", "abstract": "Control system design is a crucial aspect of modern engineering with\nfar-reaching applications across diverse sectors including aerospace,\nautomotive systems, power grids, and robotics. Despite advances made by Large\nLanguage Models (LLMs) in various domains, their application in control system\ndesign remains limited due to the complexity and specificity of control theory.\nTo bridge this gap, we introduce ControlAgent, a new paradigm that automates\ncontrol system design via novel integration of LLM agents and control-oriented\ndomain expertise. ControlAgent encodes expert control knowledge and emulates\nhuman iterative design processes by gradually tuning controller parameters to\nmeet user-specified requirements for stability, performance, and robustness.\nControlAgent integrates multiple collaborative LLM agents, including a central\nagent responsible for task distribution and task-specific agents dedicated to\ndetailed controller design for various types of systems and requirements.\nControlAgent also employs a Python computation agent that performs complex\ncalculations and controller evaluations based on standard design information\nprovided by task-specified LLM agents. Combined with a history and feedback\nmodule, the task-specific LLM agents iteratively refine controller parameters\nbased on real-time feedback from prior designs. Overall, ControlAgent mimics\nthe design processes used by (human) practicing engineers, but removes all the\nhuman efforts and can be run in a fully automated way to give end-to-end\nsolutions for control system design with user-specified requirements. To\nvalidate ControlAgent's effectiveness, we develop ControlEval, an evaluation\ndataset that comprises 500 control tasks with various specific design goals.\nThe effectiveness of ControlAgent is demonstrated via extensive comparative\nevaluations between LLM-based and traditional human-involved toolbox-based\nbaselines.", "published": "2024-10-17 17:42:48", "link": "http://arxiv.org/abs/2410.19811v1", "categories": ["eess.SY", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "math.OC"], "primary_category": "eess.SY"}
{"title": "Using RLHF to align speech enhancement approaches to mean-opinion\n  quality scores", "abstract": "Objective speech quality measures are typically used to assess speech\nenhancement algorithms, but it has been shown that they are sub-optimal as\nlearning objectives because they do not always align well with human subjective\nratings. This misalignment often results in noticeable distortions and\nartifacts that cause speech enhancement to be ineffective. To address these\nissues, we propose a reinforcement learning from human feedback (RLHF)\nframework to fine-tune an existing speech enhancement approach by optimizing\nperformance using a mean-opinion score (MOS)-based reward model. Our results\nshow that the RLHF-finetuned model has the best performance across different\nbenchmarks for both objective and MOS-based speech quality assessment metrics\non the Voicebank+DEMAND dataset. Through ablation studies, we show that both\npolicy gradient loss and supervised MSE loss are important for balanced\noptimization across the different metrics.", "published": "2024-10-17 03:12:13", "link": "http://arxiv.org/abs/2410.13182v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Effective Speaker Property Privacy Protection in Federated\n  Learning for Speech Emotion Recognition", "abstract": "Federated Learning (FL) is a privacy-preserving approach that allows servers\nto aggregate distributed models transmitted from local clients rather than\ntraining on user data. More recently, FL has been applied to Speech Emotion\nRecognition (SER) for secure human-computer interaction applications. Recent\nresearch has found that FL is still vulnerable to inference attacks. To this\nend, this paper focuses on investigating the security of FL for SER concerning\nproperty inference attacks. We propose a novel method to protect the property\ninformation in speech data by decomposing various properties in the sound and\nadding perturbations to these properties. Our experiments show that the\nproposed method offers better privacy-utility trade-offs than existing methods.\nThe trade-offs enable more effective attack prevention while maintaining\nsimilar FL utility levels. This work can guide future work on privacy\nprotection methods in speech processing.", "published": "2024-10-17 05:03:34", "link": "http://arxiv.org/abs/2410.13221v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Integration of Speech Emotion Recognition with Voice Activity\n  Detection using Self-Supervised Learning Features", "abstract": "Speech Emotion Recognition (SER) often operates on speech segments detected\nby a Voice Activity Detection (VAD) model. However, VAD models may output\nflawed speech segments, especially in noisy environments, resulting in degraded\nperformance of subsequent SER models. To address this issue, we propose an\nend-to-end (E2E) method that integrates VAD and SER using Self-Supervised\nLearning (SSL) features. The VAD module first receives the SSL features as\ninput, and the segmented SSL features are then fed into the SER module. Both\nthe VAD and SER modules are jointly trained to optimize SER performance.\nExperimental results on the IEMOCAP dataset demonstrate that our proposed\nmethod improves SER performance. Furthermore, to investigate the effect of our\nproposed method on the VAD and SSL modules, we present an analysis of the VAD\noutputs and the weights of each layer of the SSL encoder.", "published": "2024-10-17 07:18:19", "link": "http://arxiv.org/abs/2410.13282v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DurIAN-E 2: Duration Informed Attention Network with Adaptive\n  Variational Autoencoder and Adversarial Learning for Expressive\n  Text-to-Speech Synthesis", "abstract": "This paper proposes an improved version of DurIAN-E (DurIAN-E 2), which is\nalso a duration informed attention neural network for expressive and\nhigh-fidelity text-to-speech (TTS) synthesis. Similar with the DurIAN-E model,\nmultiple stacked SwishRNN-based Transformer blocks are utilized as linguistic\nencoders and Style-Adaptive Instance Normalization (SAIN) layers are also\nexploited into frame-level encoders to improve the modeling ability of\nexpressiveness in the proposed the DurIAN-E 2. Meanwhile, motivated by other\nTTS models using generative models such as VITS, the proposed DurIAN-E 2\nutilizes variational autoencoders (VAEs) augmented with normalizing flows and a\nBigVGAN waveform generator with adversarial training strategy, which further\nimprove the synthesized speech quality and expressiveness. Both objective test\nand subjective evaluation results prove that the proposed expressive TTS model\nDurIAN-E 2 can achieve better performance than several state-of-the-art\napproaches besides DurIAN-E.", "published": "2024-10-17 07:34:04", "link": "http://arxiv.org/abs/2410.13288v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing 1-Second 3D SELD Performance with Filter Bank Analysis and\n  SCConv Integration in CST-Former", "abstract": "Recent SELD research has predominantly focused on long-time segment scenarios\n(typically 5 to 10 seconds, occasionally 2 seconds), improving benchmark\nperformance but lacking the temporal granularity needed for real-world\napplications. To bridge this gap, this paper investigates SELD with distance\nestimation (3D SELD) systems under short-time segments, specifically targeting\na 1-second window, establishing a new baseline for practical 3D SELD\napplicability. We further explore the impact of different filter banks -- Bark,\nMel, and Gammatone for audio feature extraction, and experimental results\ndemonstrate that the Gammatone filter achieves the highest overall accuracy in\nthis context. Finally, we propose replacing the convolutional modules within\nthe CST-Former, a competitive SELD architecture, with the SCConv module. This\nadjustment yields measurable F-score gains in short-segment scenarios,\nunderscoring SCConv's potential to improve spatial and channel feature\nrepresentation. The experimental results highlight our approach as a\nsignificant step towards the real-world deployment of 3D SELD systems under\nlow-latency constraints.", "published": "2024-10-17 08:38:36", "link": "http://arxiv.org/abs/2410.13328v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Crowdsourced Audio for Text-to-Speech Models", "abstract": "High-quality audio data is a critical prerequisite for training robust\ntext-to-speech models, which often limits the use of opportunistic or\ncrowdsourced datasets. This paper presents an approach to overcome this\nlimitation by implementing a denoising pipeline on the Catalan subset of\nCommonvoice, a crowd-sourced corpus known for its inherent noise and\nvariability. The pipeline incorporates an audio enhancement phase followed by a\nselective filtering strategy. We developed an automatic filtering mechanism\nleveraging Non-Intrusive Speech Quality Assessment (NISQA) models to identify\nand retain the highest quality samples post-enhancement. To evaluate the\nefficacy of this approach, we trained a state of the art diffusion-based TTS\nmodel on the processed dataset. The results show a significant improvement,\nwith an increase of 0.4 in the UTMOS Score compared to the baseline dataset\nwithout enhancement. This methodology shows promise for expanding the utility\nof crowdsourced data in TTS applications, particularly for mid to low resource\nlanguages like Catalan.", "published": "2024-10-17 09:07:57", "link": "http://arxiv.org/abs/2410.13357v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STCON System for the CHiME-8 Challenge", "abstract": "This paper describes the STCON system for the CHiME-8 Challenge Task 1 (DASR)\naimed at distant automatic speech transcription and diarization with multiple\nrecording devices. Our main attention was paid to carefully trained and tuned\ndiarization pipeline and speaker counting. This allowed to significantly reduce\ndiarization error rate (DER) and obtain more reliable segments for speech\nseparation and recognition. To improve source separation, we designed a Guided\nTarget speaker Extraction (G-TSE) model and used it in conjunction with the\ntraditional Guided Source Separation (GSS) method. To train various parts of\nour pipeline, we investigated several data augmentation and generation\ntechniques, which helped us to improve the overall system quality.", "published": "2024-10-17 10:20:53", "link": "http://arxiv.org/abs/2410.13411v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Range Compression and Its Effect on Music Genre Classification", "abstract": "This paper investigates the impact of dynamic range compression (DRC) on\nmusic genre classification accuracy. By applying various compression settings\nto the test set of 200 songs, we aim to determine if compression can enhance\nthe classifier's ability to discern distinct musical genres. A support vector\nmachine (SVM) classifier was trained on the original, uncompressed dataset. The\nstudy explored the influence of threshold, ratio, knee width, attack time,\nrelease time, and makeup gain on classification performance. Our findings\nindicate that applying compression to the test set can indeed improve music\ngenre classification accuracy on average by 3.1%. The optimal compression\nsettings varied across experiments, suggesting that the effectiveness of\ncompression depends on the training data of the model. A table of the top\ncompression settings over 1000 train and test splits is provided. In\nconclusion, this research demonstrates that dynamic range compression can serve\nas a valuable preprocessing technique for enhancing music genre classification.\nThe insights gained from this study can inform the development of more accurate\nand robust music recommendation systems.", "published": "2024-10-17 14:15:32", "link": "http://arxiv.org/abs/2410.13581v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Check: Auditing Audio Datasets", "abstract": "Generative audio models are rapidly advancing in both capabilities and public\nutilization -- several powerful generative audio models have readily available\nopen weights, and some tech companies have released high quality generative\naudio products. Yet, while prior work has enumerated many ethical issues\nstemming from the data on which generative visual and textual models have been\ntrained, we have little understanding of similar issues with generative audio\ndatasets, including those related to bias, toxicity, and intellectual property.\nTo bridge this gap, we conducted a literature review of hundreds of audio\ndatasets and selected seven of the most prominent to audit in more detail. We\nfound that these datasets are biased against women, contain toxic stereotypes\nabout marginalized communities, and contain significant amounts of copyrighted\nwork. To enable artists to see if they are in popular audio datasets and\nfacilitate exploration of the contents of these datasets, we developed a web\ntool audio datasets exploration tool at https://audio-audit.vercel.app.", "published": "2024-10-17 00:51:27", "link": "http://arxiv.org/abs/2410.13114v1", "categories": ["cs.SD", "cs.AI", "cs.CY", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DART: Disentanglement of Accent and Speaker Representation in\n  Multispeaker Text-to-Speech", "abstract": "Recent advancements in Text-to-Speech (TTS) systems have enabled the\ngeneration of natural and expressive speech from textual input. Accented TTS\naims to enhance user experience by making the synthesized speech more relatable\nto minority group listeners, and useful across various applications and\ncontext. Speech synthesis can further be made more flexible by allowing users\nto choose any combination of speaker identity and accent, resulting in a wide\nrange of personalized speech outputs. Current models struggle to disentangle\nspeaker and accent representation, making it difficult to accurately imitate\ndifferent accents while maintaining the same speaker characteristics. We\npropose a novel approach to disentangle speaker and accent representations\nusing multi-level variational autoencoders (ML-VAE) and vector quantization\n(VQ) to improve flexibility and enhance personalization in speech synthesis.\nOur proposed method addresses the challenge of effectively separating speaker\nand accent characteristics, enabling more fine-grained control over the\nsynthesized speech. Code and speech samples are publicly available.", "published": "2024-10-17 08:51:46", "link": "http://arxiv.org/abs/2410.13342v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MeloTrans: A Text to Symbolic Music Generation Model Following Human\n  Composition Habit", "abstract": "At present, neural network models show powerful sequence prediction ability\nand are used in many automatic composition models. In comparison, the way\nhumans compose music is very different from it. Composers usually start by\ncreating musical motifs and then develop them into music through a series of\nrules. This process ensures that the music has a specific structure and\nchanging pattern. However, it is difficult for neural network models to learn\nthese composition rules from training data, which results in a lack of\nmusicality and diversity in the generated music. This paper posits that\nintegrating the learning capabilities of neural networks with human-derived\nknowledge may lead to better results. To archive this, we develop the\nPOP909$\\_$M dataset, the first to include labels for musical motifs and their\nvariants, providing a basis for mimicking human compositional habits. Building\non this, we propose MeloTrans, a text-to-music composition model that employs\nprinciples of motif development rules. Our experiments demonstrate that\nMeloTrans excels beyond existing music generation models and even surpasses\nLarge Language Models (LLMs) like ChatGPT-4. This highlights the importance of\nmerging human insights with neural network capabilities to achieve superior\nsymbolic music generation.", "published": "2024-10-17 10:41:52", "link": "http://arxiv.org/abs/2410.13419v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GAN-Based Speech Enhancement for Low SNR Using Latent Feature\n  Conditioning", "abstract": "Enhancing speech quality under adverse SNR conditions remains a significant\nchallenge for discriminative deep neural network (DNN)-based approaches. In\nthis work, we propose DisCoGAN, which is a time-frequency-domain generative\nadversarial network (GAN) conditioned by the latent features of a\ndiscriminative model pre-trained for speech enhancement in low SNR scenarios.\nOur proposed method achieves superior performance compared to state-of-the-arts\ndiscriminative methods and also surpasses end-to-end (E2E) trained GAN models.\nWe also investigate the impact of various configurations for conditioning the\nproposed GAN model with the discriminative model and assess their influence on\nenhancing speech quality", "published": "2024-10-17 14:31:46", "link": "http://arxiv.org/abs/2410.13599v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Align-ULCNet: Towards Low-Complexity and Robust Acoustic Echo and Noise\n  Reduction", "abstract": "The successful deployment of deep learning-based acoustic echo and noise\nreduction (AENR) methods in consumer devices has spurred interest in developing\nlow-complexity solutions, while emphasizing the need for robust performance in\nreal-life applications. In this work, we propose a hybrid approach to enhance\nthe state-of-the-art (SOTA) ULCNet model by integrating time alignment and\nparallel encoder blocks for the model inputs, resulting in better echo\nreduction and comparable noise reduction performance to existing SOTA methods.\nWe also propose a channel-wise sampling-based feature reorientation method,\nensuring robust performance across many challenging scenarios, while\nmaintaining overall low computational and memory requirements.", "published": "2024-10-17 14:51:45", "link": "http://arxiv.org/abs/2410.13620v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Accelerating Codec-based Speech Synthesis with Multi-Token Prediction\n  and Speculative Decoding", "abstract": "The goal of this paper is to accelerate codec-based speech synthesis systems\nwith minimum sacrifice to speech quality. We propose an enhanced inference\nmethod that allows for flexible trade-offs between speed and quality during\ninference without requiring additional training. Our core idea is to predict\nmultiple tokens per inference step of the AR module using multiple prediction\nheads, resulting in a linear reduction in synthesis time as the number of heads\nincreases. Furthermore, we introduce a novel speculative decoding technique\nthat utilises a Viterbi-based algorithm to select the optimal sequence of\ngenerated tokens at each decoding step. In our experiments, we demonstrate that\nthe time required to predict each token is reduced by a factor of 4 to 5\ncompared to baseline models, with minimal quality trade-off or even improvement\nin terms of speech intelligibility. Audio samples are available at:\nmultpletokensprediction.github.io/multipletokensprediction.github.io/.", "published": "2024-10-17 17:55:26", "link": "http://arxiv.org/abs/2410.13839v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimal Transport Maps are Good Voice Converters", "abstract": "Recently, neural network-based methods for computing optimal transport maps\nhave been effectively applied to style transfer problems. However, the\napplication of these methods to voice conversion is underexplored. In our\npaper, we fill this gap by investigating optimal transport as a framework for\nvoice conversion. We present a variety of optimal transport algorithms designed\nfor different data representations, such as mel-spectrograms and latent\nrepresentation of self-supervised speech models. For the mel-spectogram data\nrepresentation, we achieve strong results in terms of Frechet Audio Distance\n(FAD). This performance is consistent with our theoretical analysis, which\nsuggests that our method provides an upper bound on the FAD between the target\nand generated distributions. Within the latent space of the WavLM encoder, we\nachived state-of-the-art results and outperformed existing methods even with\nlimited reference speaker data.", "published": "2024-10-17 22:48:53", "link": "http://arxiv.org/abs/2411.02402v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
