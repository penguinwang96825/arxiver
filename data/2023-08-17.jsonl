{"title": "Enhancing Phrase Representation by Information Bottleneck Guided Text\n  Diffusion Process for Keyphrase Extraction", "abstract": "Keyphrase extraction (KPE) is an important task in Natural Language\nProcessing for many scenarios, which aims to extract keyphrases that are\npresent in a given document. Many existing supervised methods treat KPE as\nsequential labeling, span-level classification, or generative tasks. However,\nthese methods lack the ability to utilize keyphrase information, which may\nresult in biased results. In this study, we propose Diff-KPE, which leverages\nthe supervised Variational Information Bottleneck (VIB) to guide the text\ndiffusion process for generating enhanced keyphrase representations. Diff-KPE\nfirst generates the desired keyphrase embeddings conditioned on the entire\ndocument and then injects the generated keyphrase embeddings into each phrase\nrepresentation. A ranking network and VIB are then optimized together with rank\nloss and classification loss, respectively. This design of Diff-KPE allows us\nto rank each candidate phrase by utilizing both the information of keyphrases\nand the document. Experiments show that Diff-KPE outperforms existing KPE\nmethods on a large open domain keyphrase extraction benchmark, OpenKP, and a\nscientific domain dataset, KP20K.", "published": "2023-08-17 02:26:30", "link": "http://arxiv.org/abs/2308.08739v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Catastrophic Forgetting in Large Language Models\n  During Continual Fine-tuning", "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning.", "published": "2023-08-17 02:53:23", "link": "http://arxiv.org/abs/2308.08747v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task Relation Distillation and Prototypical Pseudo Label for Incremental\n  Named Entity Recognition", "abstract": "Incremental Named Entity Recognition (INER) involves the sequential learning\nof new entity types without accessing the training data of previously learned\ntypes. However, INER faces the challenge of catastrophic forgetting specific\nfor incremental learning, further aggravated by background shift (i.e., old and\nfuture entity types are labeled as the non-entity type in the current task). To\naddress these challenges, we propose a method called task Relation Distillation\nand Prototypical pseudo label (RDP) for INER. Specifically, to tackle\ncatastrophic forgetting, we introduce a task relation distillation scheme that\nserves two purposes: 1) ensuring inter-task semantic consistency across\ndifferent incremental learning tasks by minimizing inter-task relation\ndistillation loss, and 2) enhancing the model's prediction confidence by\nminimizing intra-task self-entropy loss. Simultaneously, to mitigate background\nshift, we develop a prototypical pseudo label strategy that distinguishes old\nentity types from the current non-entity type using the old model. This\nstrategy generates high-quality pseudo labels by measuring the distances\nbetween token embeddings and type-wise prototypes. We conducted extensive\nexperiments on ten INER settings of three benchmark datasets (i.e., CoNLL2003,\nI2B2, and OntoNotes5). The results demonstrate that our method achieves\nsignificant improvements over the previous state-of-the-art methods, with an\naverage increase of 6.08% in Micro F1 score and 7.71% in Macro F1 score.", "published": "2023-08-17 05:36:56", "link": "http://arxiv.org/abs/2308.08793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Spelling Correction as Rephrasing Language Model", "abstract": "This paper studies Chinese Spelling Correction (CSC), which aims to detect\nand correct the potential spelling errors in a given sentence. Current\nstate-of-the-art methods regard CSC as a sequence tagging task and fine-tune\nBERT-based models on sentence pairs. However, we note a critical flaw in the\nprocess of tagging one character to another, that the correction is excessively\nconditioned on the error. This is opposite from human mindset, where\nindividuals rephrase the complete sentence based on its semantics, rather than\nsolely on the error patterns memorized before. Such a counter-intuitive\nlearning process results in the bottleneck of generalizability and\ntransferability of machine spelling correction. To address this, we propose\nRephrasing Language Model (ReLM), where the model is trained to rephrase the\nentire sentence by infilling additional slots, instead of\ncharacter-to-character tagging. This novel training paradigm achieves the new\nstate-of-the-art results across fine-tuned and zero-shot CSC benchmarks,\noutperforming previous counterparts by a large margin. Our method also learns\ntransferable language representation when CSC is jointly trained with other\ntasks.", "published": "2023-08-17 06:04:28", "link": "http://arxiv.org/abs/2308.08796v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically-Informed Neural Architectures for Lexical, Syntactic and\n  Semantic Tasks in Sanskrit", "abstract": "The primary focus of this thesis is to make Sanskrit manuscripts more\naccessible to the end-users through natural language technologies. The\nmorphological richness, compounding, free word orderliness, and low-resource\nnature of Sanskrit pose significant challenges for developing deep learning\nsolutions. We identify four fundamental tasks, which are crucial for developing\na robust NLP technology for Sanskrit: word segmentation, dependency parsing,\ncompound type identification, and poetry analysis. The first task, Sanskrit\nWord Segmentation (SWS), is a fundamental text processing task for any other\ndownstream applications. However, it is challenging due to the sandhi\nphenomenon that modifies characters at word boundaries. Similarly, the existing\ndependency parsing approaches struggle with morphologically rich and\nlow-resource languages like Sanskrit. Compound type identification is also\nchallenging for Sanskrit due to the context-sensitive semantic relation between\ncomponents. All these challenges result in sub-optimal performance in NLP\napplications like question answering and machine translation. Finally, Sanskrit\npoetry has not been extensively studied in computational linguistics.\n  While addressing these challenges, this thesis makes various contributions:\n(1) The thesis proposes linguistically-informed neural architectures for these\ntasks. (2) We showcase the interpretability and multilingual extension of the\nproposed systems. (3) Our proposed systems report state-of-the-art performance.\n(4) Finally, we present a neural toolkit named SanskritShala, a web-based\napplication that provides real-time analysis of input for various NLP tasks.\nOverall, this thesis contributes to making Sanskrit manuscripts more accessible\nby developing robust NLP technology and releasing various resources, datasets,\nand web-based toolkit.", "published": "2023-08-17 06:33:33", "link": "http://arxiv.org/abs/2308.08807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factuality Detection using Machine Translation -- a Use Case for German\n  Clinical Text", "abstract": "Factuality can play an important role when automatically processing clinical\ntext, as it makes a difference if particular symptoms are explicitly not\npresent, possibly present, not mentioned, or affirmed. In most cases, a\nsufficient number of examples is necessary to handle such phenomena in a\nsupervised machine learning setting. However, as clinical text might contain\nsensitive information, data cannot be easily shared. In the context of\nfactuality detection, this work presents a simple solution using machine\ntranslation to translate English data to German to train a transformer-based\nfactuality detection model.", "published": "2023-08-17 07:24:06", "link": "http://arxiv.org/abs/2308.08827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Beam Retrieval for Multi-Hop Question Answering", "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand step-by-step reasoning to answer complex questions, indicating a\nretrieve-and-read paradigm. However, previous retrievers were customized for\ntwo-hop questions, and most of them were trained separately across different\nhops, resulting in a lack of supervision over the entire multi-hop retrieval\nprocess and leading to poor performance in complicated scenarios beyond two\nhops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval\nframework for multi-hop QA. This approach models the multi-hop retrieval\nprocess in an end-to-end manner by jointly optimizing an encoder and two\nclassification heads across all hops. Moreover, Beam Retrieval maintains\nmultiple partial hypotheses of relevant passages at each step, expanding the\nsearch space and reducing the risk of missing relevant passages. To establish a\ncomplete QA system, we incorporate a supervised reader or a large language\nmodel (LLM). Experimental results demonstrate that Beam Retrieval achieves a\nnearly 50% improvement compared with baselines on challenging MuSiQue-Ans, and\nit also surpasses all previous retrievers on HotpotQA and achieves 99.9%\nprecision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval\nhelps our supervised reader achieve new state-of-the-art performance and\nsubstantially improves the few-shot QA performance of LLMs.", "published": "2023-08-17 13:24:14", "link": "http://arxiv.org/abs/2308.08973v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of really good grammatical error correction", "abstract": "Although rarely stated, in practice, Grammatical Error Correction (GEC)\nencompasses various models with distinct objectives, ranging from grammatical\nerror detection to improving fluency. Traditional evaluation methods fail to\nfully capture the full range of system capabilities and objectives.\nReference-based evaluations suffer from limitations in capturing the wide\nvariety of possible correction and the biases introduced during reference\ncreation and is prone to favor fixing local errors over overall text\nimprovement. The emergence of large language models (LLMs) has further\nhighlighted the shortcomings of these evaluation strategies, emphasizing the\nneed for a paradigm shift in evaluation methodology. In the current study, we\nperform a comprehensive evaluation of various GEC systems using a recently\npublished dataset of Swedish learner texts. The evaluation is performed using\nestablished evaluation metrics as well as human judges. We find that GPT-3 in a\nfew-shot setting by far outperforms previous grammatical error correction\nsystems for Swedish, a language comprising only 0.11% of its training data. We\nalso found that current evaluation methods contain undesirable biases that a\nhuman evaluation is able to reveal. We suggest using human post-editing of GEC\nsystem outputs to analyze the amount of change required to reach native-level\nhuman performance on the task, and provide a dataset annotated with human\npost-edits and assessments of grammaticality, fluency and meaning preservation\nof GEC system outputs.", "published": "2023-08-17 13:45:35", "link": "http://arxiv.org/abs/2308.08982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't lose the message while paraphrasing: A study on content preserving\n  style transfer", "abstract": "Text style transfer techniques are gaining popularity in natural language\nprocessing allowing paraphrasing text in the required form: from toxic to\nneural, from formal to informal, from old to the modern English language, etc.\nSolving the task is not sufficient to generate some neural/informal/modern\ntext, but it is important to preserve the original content unchanged. This\nrequirement becomes even more critical in some applications such as style\ntransfer of goal-oriented dialogues where the factual information shall be kept\nto preserve the original message, e.g. ordering a certain type of pizza to a\ncertain address at a certain time. The aspect of content preservation is\ncritical for real-world applications of style transfer studies, but it has\nreceived little attention. To bridge this gap we perform a comparison of\nvarious style transfer models on the example of the formality transfer domain.\nTo perform a study of the content preservation abilities of various style\ntransfer methods we create a parallel dataset of formal vs. informal\ntask-oriented dialogues. The key difference between our dataset and the\nexisting ones like GYAFC [17] is the presence of goal-oriented dialogues with\npredefined semantic slots essential to be kept during paraphrasing, e.g. named\nentities. This additional annotation allowed us to conduct a precise\ncomparative study of several state-of-the-art techniques for style transfer.\nAnother result of our study is a modification of the unsupervised method LEWIS\n[19] which yields a substantial improvement over the original method and all\nevaluated baselines on the proposed task.", "published": "2023-08-17 15:41:08", "link": "http://arxiv.org/abs/2308.09055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text", "abstract": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs.", "published": "2023-08-17 15:54:38", "link": "http://arxiv.org/abs/2308.09067v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view\n  Contrastive Learning", "abstract": "Cross-lingual named entity recognition (CrossNER) faces challenges stemming\nfrom uneven performance due to the scarcity of multilingual corpora, especially\nfor non-English data. While prior efforts mainly focus on data-driven transfer\nmethods, a significant aspect that has not been fully explored is aligning both\nsemantic and token-level representations across diverse languages. In this\npaper, we propose Multi-view Contrastive Learning for Cross-lingual Named\nEntity Recognition (mCL-NER). Specifically, we reframe the CrossNER task into a\nproblem of recognizing relationships between pairs of tokens. This approach\ntaps into the inherent contextual nuances of token-to-token connections within\nentities, allowing us to align representations across different languages. A\nmulti-view contrastive learning framework is introduced to encompass semantic\ncontrasts between source, codeswitched, and target sentences, as well as\ncontrasts among token-to-token relations. By enforcing agreement within both\nsemantic and relational spaces, we minimize the gap between source sentences\nand their counterparts of both codeswitched and target sentences. This\nalignment extends to the relationships between diverse tokens, enhancing the\nprojection of entities across languages. We further augment CrossNER by\ncombining self-training with labeled source data and unlabeled target data. Our\nexperiments on the XTREME benchmark, spanning 40 languages, demonstrate the\nsuperiority of mCL-NER over prior data-driven and model-based approaches. It\nachieves a substantial increase of nearly +2.0 $F_1$ scores across a broad\nspectrum and establishes itself as the new state-of-the-art performer.", "published": "2023-08-17 16:02:29", "link": "http://arxiv.org/abs/2308.09073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linearity of Relation Decoding in Transformer Language Models", "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be\nexpressed in terms of relations: relations between words and their synonyms,\nentities and their attributes, etc. We show that, for a subset of relations,\nthis computation is well-approximated by a single linear transformation on the\nsubject representation. Linear relation representations may be obtained by\nconstructing a first-order approximation to the LM from a single prompt, and\nthey exist for a variety of factual, commonsense, and linguistic relations.\nHowever, we also identify many cases in which LM predictions capture relational\nknowledge accurately, but this knowledge is not linearly encoded in their\nrepresentations. Our results thus reveal a simple, interpretable, but\nheterogeneously deployed knowledge representation strategy in transformer LMs.", "published": "2023-08-17 17:59:19", "link": "http://arxiv.org/abs/2308.09124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Information Seeking Events in Health-Related Social\n  Discourse", "abstract": "Social media sites have become a popular platform for individuals to seek and\nshare health information. Despite the progress in natural language processing\nfor social media mining, a gap remains in analyzing health-related texts on\nsocial discourse in the context of events. Event-driven analysis can offer\ninsights into different facets of healthcare at an individual and collective\nlevel, including treatment options, misconceptions, knowledge gaps, etc. This\npaper presents a paradigm to characterize health-related information-seeking in\nsocial discourse through the lens of events. Events here are board categories\ndefined with domain experts that capture the trajectory of the\ntreatment/medication. To illustrate the value of this approach, we analyze\nReddit posts regarding medications for Opioid Use Disorder (OUD), a critical\nglobal health concern. To the best of our knowledge, this is the first attempt\nto define event categories for characterizing information-seeking in OUD social\ndiscourse. Guided by domain experts, we develop TREAT-ISE, a novel multilabel\ntreatment information-seeking event dataset to analyze online discourse on an\nevent-based framework. This dataset contains Reddit posts on\ninformation-seeking events related to recovery from OUD, where each post is\nannotated based on the type of events. We also establish a strong performance\nbenchmark (77.4% F1 score) for the task by employing several machine learning\nand deep learning classifiers. Finally, we thoroughly investigate the\nperformance and errors of ChatGPT on this task, providing valuable insights\ninto the LLM's capabilities and ongoing characterization efforts.", "published": "2023-08-17 19:08:42", "link": "http://arxiv.org/abs/2308.09156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Argument Structure of Learner Chinese Understandable: A Corpus-Based\n  Analysis", "abstract": "This paper presents a corpus-based analysis of argument structure errors in\nlearner Chinese. The data for analysis includes sentences produced by language\nlearners as well as their corrections by native speakers. We couple the data\nwith semantic role labeling annotations that are manually created by two senior\nstudents whose majors are both Applied Linguistics. The annotation procedure is\nguided by the Chinese PropBank specification, which is originally developed to\ncover first language phenomena. Nevertheless, we find that it is quite\ncomprehensive for handling second language phenomena. The inter-annotator\nagreement is rather high, suggesting the understandability of learner texts to\nnative speakers. Based on our annotations, we present a preliminary analysis of\ncompetence errors related to argument structure. In particular, speech errors\nrelated to word order, word selection, lack of proposition, and\nargument-adjunct confounding are discussed.", "published": "2023-08-17 21:10:04", "link": "http://arxiv.org/abs/2308.09186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-FuncMapper: Function Identification for Interpreting Complex Clauses\n  in Building Codes via LLM", "abstract": "As a vital stage of automated rule checking (ARC), rule interpretation of\nregulatory texts requires considerable effort. However, interpreting regulatory\nclauses with implicit properties or complex computational logic is still\nchallenging due to the lack of domain knowledge and limited expressibility of\nconventional logic representations. Thus, LLM-FuncMapper, an approach to\nidentifying predefined functions needed to interpret various regulatory clauses\nbased on the large language model (LLM), is proposed. First, by systematically\nanalysis of building codes, a series of atomic functions are defined to capture\nshared computational logics of implicit properties and complex constraints,\ncreating a database of common blocks for interpreting regulatory clauses. Then,\na prompt template with the chain of thought is developed and further enhanced\nwith a classification-based tuning strategy, to enable common LLMs for\neffective function identification. Finally, the proposed approach is validated\nwith statistical analysis, experiments, and proof of concept. Statistical\nanalysis reveals a long-tail distribution and high expressibility of the\ndeveloped function database, with which almost 100% of computer-processible\nclauses can be interpreted and represented as computer-executable codes.\nExperiments show that LLM-FuncMapper achieve promising results in identifying\nrelevant predefined functions for rule interpretation. Further proof of concept\nin automated rule interpretation also demonstrates the possibility of\nLLM-FuncMapper in interpreting complex regulatory clauses. To the best of our\nknowledge, this study is the first attempt to introduce LLM for understanding\nand interpreting complex regulatory clauses, which may shed light on further\nadoption of LLM in the construction domain.", "published": "2023-08-17 01:58:04", "link": "http://arxiv.org/abs/2308.08728v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Discrete Prompt Compression with Reinforcement Learning", "abstract": "Compressed prompts aid instruction-tuned language models (LMs) in overcoming\ncontext window limitations and reducing computational costs. Existing methods,\nwhich primarily based on training embeddings, face various challenges\nassociated with interpretability, the fixed number of embedding tokens,\nreusability across different LMs, and inapplicability when interacting with\nblack-box APIs. This study proposes prompt compression with reinforcement\nlearning (PCRL), which is a discrete prompt compression method that addresses\nthese issues. The proposed PCRL method utilizes a computationally efficient\npolicy network that edits prompts directly. The training approach employed in\nthe proposed PCRLs can be applied flexibly to various types of LMs, including\nboth decoder-only and encoder-decoder architecture and it can be trained\nwithout gradient access to the LMs or labeled data. The proposed PCRL achieves\nan average reduction of 24.6% in terms of the token count across various\ninstruction prompts while maintaining sufficient performance. In addition, we\ndemonstrate that the learned policy can be transferred to larger LMs, and\nthrough a comprehensive analysis, we explore the token importance within the\nprompts. Our code is accessible at\nhttps://github.com/nenomigami/PromptCompressor.", "published": "2023-08-17 03:10:17", "link": "http://arxiv.org/abs/2308.08758v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Demonstration Ensembling for In-context Learning", "abstract": "In-context learning (ICL) operates by showing language models (LMs) examples\nof input-output pairs for a given task, i.e., demonstrations. The standard\napproach for ICL is to prompt the LM with concatenated demonstrations followed\nby the test input. This approach suffers from some issues. First, concatenation\noffers almost no control over the contribution of each demo to the model\nprediction. This can be sub-optimal when some demonstrations are irrelevant to\nthe test example. Second, due to the input length limit of some transformer\nmodels, it might be infeasible to fit many examples into the context,\nespecially when dealing with long-input tasks. In this work, we explore\nDemonstration Ensembling (DENSE) as an alternative to simple concatenation.\nDENSE predicts outputs using subsets (i.e., buckets) of the demonstrations and\nthen combines the output probabilities resulting from each subset to produce\nthe final prediction. We study different ensembling methods using GPT-j and\nexperiment on 12 language tasks. Our experiments show weighted max ensembling\nto outperform vanilla concatenation by as large as 2.4 average points. Code\navailable at https://github.com/mukhal/icl-ensembling.", "published": "2023-08-17 04:45:19", "link": "http://arxiv.org/abs/2308.08780v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMB: A Comprehensive Medical Benchmark in Chinese", "abstract": "Large Language Models (LLMs) provide a possibility to make a great\nbreakthrough in medicine. The establishment of a standardized medical benchmark\nbecomes a fundamental cornerstone to measure progression. However, medical\nenvironments in different regions have their local characteristics, e.g., the\nubiquity and significance of traditional Chinese medicine within China.\nTherefore, merely translating English-based medical evaluation may result in\n\\textit{contextual incongruities} to a local region. To solve the issue, we\npropose a localized medical benchmark called CMB, a Comprehensive Medical\nBenchmark in Chinese, designed and rooted entirely within the native Chinese\nlinguistic and cultural framework. While traditional Chinese medicine is\nintegral to this evaluation, it does not constitute its entirety. Using this\nbenchmark, we have evaluated several prominent large-scale LLMs, including\nChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical\ndomain. We hope this benchmark provide first-hand experience in existing LLMs\nfor medicine and also facilitate the widespread adoption and enhancement of\nmedical LLMs within China. Our data and code are publicly available at\nhttps://github.com/FreedomIntelligence/CMB.", "published": "2023-08-17 07:51:23", "link": "http://arxiv.org/abs/2308.08833v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reinforced Self-Training (ReST) for Language Modeling", "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of\nlarge language model's (LLM) outputs by aligning them with human preferences.\nWe propose a simple algorithm for aligning LLMs with human preferences inspired\nby growing batch reinforcement learning (RL), which we call Reinforced\nSelf-Training (ReST). Given an initial LLM policy, ReST produces a dataset by\ngenerating samples from the policy, which are then used to improve the LLM\npolicy using offline RL algorithms. ReST is more efficient than typical online\nRLHF methods because the training dataset is produced offline, which allows\ndata reuse. While ReST is a general approach applicable to all generative\nlearning settings, we focus on its application to machine translation. Our\nresults show that ReST can substantially improve translation quality, as\nmeasured by automated metrics and human evaluation on machine translation\nbenchmarks in a compute and sample-efficient manner.", "published": "2023-08-17 14:12:48", "link": "http://arxiv.org/abs/2308.08998v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MaScQA: A Question Answering Dataset for Investigating Materials Science\n  Knowledge of Large Language Models", "abstract": "Information extraction and textual comprehension from materials literature\nare vital for developing an exhaustive knowledge base that enables accelerated\nmaterials discovery. Language models have demonstrated their capability to\nanswer domain-specific questions and retrieve information from knowledge bases.\nHowever, there are no benchmark datasets in the materials domain that can\nevaluate the understanding of the key concepts by these language models. In\nthis work, we curate a dataset of 650 challenging questions from the materials\ndomain that require the knowledge and skills of a materials student who has\ncleared their undergraduate degree. We classify these questions based on their\nstructure and the materials science domain-based subcategories. Further, we\nevaluate the performance of GPT-3.5 and GPT-4 models on solving these questions\nvia zero-shot and chain of thought prompting. It is observed that GPT-4 gives\nthe best performance (~62% accuracy) as compared to GPT-3.5. Interestingly, in\ncontrast to the general observation, no significant improvement in accuracy is\nobserved with the chain of thought prompting. To evaluate the limitations, we\nperformed an error analysis, which revealed conceptual errors (~64%) as the\nmajor contributor compared to computational errors (~36%) towards the reduced\nperformance of LLMs. We hope that the dataset and analysis performed in this\nwork will promote further research in developing better materials science\ndomain-specific LLMs and strategies for information extraction.", "published": "2023-08-17 17:51:05", "link": "http://arxiv.org/abs/2308.09115v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Evaluating the Instruction-Following Robustness of Large Language Models\n  to Prompt Injection", "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ninstruction-following, becoming increasingly crucial across various\napplications. However, this capability brings with it the risk of prompt\ninjection attacks, where attackers inject instructions into LLMs' input to\nelicit undesirable actions or content. Understanding the robustness of LLMs\nagainst such attacks is vital for their safe implementation. In this work, we\nestablish a benchmark to evaluate the robustness of instruction-following LLMs\nagainst prompt injection attacks. Our objective is to determine the extent to\nwhich LLMs can be influenced by injected instructions and their ability to\ndifferentiate between these injected and original target instructions. Through\nextensive experiments with leading instruction-following LLMs, we uncover\nsignificant vulnerabilities in their robustness to such attacks. Our results\nindicate that some models are overly tuned to follow any embedded instructions\nin the prompt, overly focusing on the latter parts of the prompt without fully\ngrasping the entire context. By contrast, models with a better grasp of the\ncontext and instruction-following capabilities will potentially be more\nsusceptible to compromise by injected instructions. This underscores the need\nto shift the focus from merely enhancing LLMs' instruction-following\ncapabilities to improving their overall comprehension of prompts and\ndiscernment of instructions that are appropriate to follow. We hope our\nin-depth analysis offers insights into the underlying causes of these\nvulnerabilities, aiding in the development of future solutions. Code and data\nare available at\nhttps://github.com/Leezekun/instruction-following-robustness-eval", "published": "2023-08-17 06:21:50", "link": "http://arxiv.org/abs/2308.10819v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building Emotional Support Chatbots in the Era of LLMs", "abstract": "The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.", "published": "2023-08-17 10:49:18", "link": "http://arxiv.org/abs/2308.11584v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage\n  Access on Knowledge Bases", "abstract": "Large language models (LLMs) have demonstrated impressive impact in the field\nof natural language processing, but they still struggle with several issues\nregarding, such as completeness, timeliness, faithfulness and adaptability.\nWhile recent efforts have focuses on connecting LLMs with external knowledge\nsources, the integration of knowledge bases (KBs) remains understudied and\nfaces several challenges. In this paper, we introduce KnowledGPT, a\ncomprehensive framework to bridge LLMs with various knowledge bases,\nfacilitating both the retrieval and storage of knowledge. The retrieval process\nemploys the program of thought prompting, which generates search language for\nKBs in code format with pre-defined functions for KB operations. Besides\nretrieval, KnowledGPT offers the capability to store knowledge in a\npersonalized KB, catering to individual user demands. With extensive\nexperiments, we show that by integrating LLMs with KBs, KnowledGPT properly\nanswers a broader range of questions requiring world knowledge compared with\nvanilla LLMs, utilizing both knowledge existing in widely-known KBs and\nextracted into personalized KBs.", "published": "2023-08-17 13:07:00", "link": "http://arxiv.org/abs/2308.11761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Emotions: A comprehensive Multilingual Study of Speech Models\n  for Speech Emotion Recognition", "abstract": "Recent advancements in transformer-based speech representation models have\ngreatly transformed speech processing. However, there has been limited research\nconducted on evaluating these models for speech emotion recognition (SER)\nacross multiple languages and examining their internal representations. This\narticle addresses these gaps by presenting a comprehensive benchmark for SER\nwith eight speech representation models and six different languages. We\nconducted probing experiments to gain insights into inner workings of these\nmodels for SER. We find that using features from a single optimal layer of a\nspeech model reduces the error rate by 32\\% on average across seven datasets\nwhen compared to systems where features from all layers of speech models are\nused. We also achieve state-of-the-art results for German and Persian\nlanguages. Our probing results indicate that the middle layers of speech models\ncapture the most important emotional information for speech emotion\nrecognition.", "published": "2023-08-17 00:30:56", "link": "http://arxiv.org/abs/2308.08713v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PMET: Precise Model Editing in a Transformer", "abstract": "Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.", "published": "2023-08-17 02:33:43", "link": "http://arxiv.org/abs/2308.08742v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Differential Privacy, Linguistic Fairness, and Training Data Influence:\n  Impossibility and Possibility Theorems for Multilingual Language Models", "abstract": "Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual\ngeneralization or compression to facilitate transfer to a large number of\n(potentially unseen) languages. However, these models should ideally also be\nprivate, linguistically fair, and transparent, by relating their predictions to\ntraining data. Can these requirements be simultaneously satisfied? We show that\nmultilingual compression and linguistic fairness are compatible with\ndifferential privacy, but that differential privacy is at odds with training\ndata influence sparsity, an objective for transparency. We further present a\nseries of experiments on two common NLP tasks and evaluate multilingual\ncompression and training data influence sparsity under different privacy\nguarantees, exploring these trade-offs in more detail. Our results suggest that\nwe need to develop ways to jointly optimize for these objectives in order to\nfind practical trade-offs.", "published": "2023-08-17 04:13:26", "link": "http://arxiv.org/abs/2308.08774v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing API Documentation through BERTopic Modeling and Summarization", "abstract": "As the amount of textual data in various fields, including software\ndevelopment, continues to grow, there is a pressing demand for efficient and\neffective extraction and presentation of meaningful insights. This paper\npresents a unique approach to address this need, focusing on the complexities\nof interpreting Application Programming Interface (API) documentation. While\nofficial API documentation serves as a primary source of information for\ndevelopers, it can often be extensive and lacks user-friendliness. In light of\nthis, developers frequently resort to unofficial sources like Stack Overflow\nand GitHub. Our novel approach employs the strengths of BERTopic for topic\nmodeling and Natural Language Processing (NLP) to automatically generate\nsummaries of API documentation, thereby creating a more efficient method for\ndevelopers to extract the information they need. The produced summaries and\ntopics are evaluated based on their performance, coherence, and\ninteroperability.\n  The findings of this research contribute to the field of API documentation\nanalysis by providing insights into recurring topics, identifying common\nissues, and generating potential solutions. By improving the accessibility and\nefficiency of API documentation comprehension, our work aims to enhance the\nsoftware development process and empower developers with practical tools for\nnavigating complex APIs.", "published": "2023-08-17 15:57:12", "link": "http://arxiv.org/abs/2308.09070v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language\n  Understanding", "abstract": "We introduce EgoSchema, a very long-form video question-answering dataset,\nand benchmark to evaluate long video understanding capabilities of modern\nvision and language systems. Derived from Ego4D, EgoSchema consists of over\n5000 human curated multiple choice question answer pairs, spanning over 250\nhours of real video data, covering a very broad range of natural human activity\nand behavior. For each question, EgoSchema requires the correct answer to be\nselected between five given options based on a three-minute-long video clip.\nWhile some prior works have proposed video datasets with long clip lengths, we\nposit that merely the length of the video clip does not truly capture the\ntemporal difficulty of the video task that is being considered. To remedy this,\nwe introduce temporal certificate sets, a general notion for capturing the\nintrinsic temporal understanding length associated with a broad range of video\nunderstanding tasks & datasets. Based on this metric, we find EgoSchema to have\nintrinsic temporal lengths over 5.7x longer than the second closest dataset and\n10x to 100x longer than any other video understanding dataset. Further, our\nevaluation of several current state-of-the-art video and language models shows\nthem to be severely lacking in long-term video understanding capabilities. Even\nmodels with several billions of parameters achieve QA accuracy less than 33%\n(random is 20%) on the EgoSchema multi-choice question answering task, while\nhumans achieve about 76% accuracy. We posit that \\name{}{}, with its long\nintrinsic temporal structures and diverse complexity, would serve as a valuable\nevaluation probe for developing effective long-term video understanding systems\nin the future. Data and Zero-shot model evaluation code are open-sourced for\nboth public and commercial use under the Ego4D license at\nhttp://egoschema.github.io", "published": "2023-08-17 17:59:59", "link": "http://arxiv.org/abs/2308.09126v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semantic Consistency for Assuring Reliability of Large Language Models", "abstract": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold.", "published": "2023-08-17 18:11:33", "link": "http://arxiv.org/abs/2308.09138v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model\n  Reuse", "abstract": "The rapid expansion of foundation pre-trained models and their fine-tuned\ncounterparts has significantly contributed to the advancement of machine\nlearning. Leveraging pre-trained models to extract knowledge and expedite\nlearning in real-world tasks, known as \"Model Reuse\", has become crucial in\nvarious applications. Previous research focuses on reusing models within a\ncertain aspect, including reusing model weights, structures, and hypothesis\nspaces. This paper introduces ZhiJian, a comprehensive and user-friendly\ntoolbox for model reuse, utilizing the PyTorch backend. ZhiJian presents a\nnovel paradigm that unifies diverse perspectives on model reuse, encompassing\ntarget architecture construction with PTM, tuning target model with PTM, and\nPTM-based inference. This empowers deep learning practitioners to explore\ndownstream tasks and identify the complementary advantages among different\nmethods. ZhiJian is readily accessible at\nhttps://github.com/zhangyikaii/lamda-zhijian facilitating seamless utilization\nof pre-trained models and streamlining the model reuse process for researchers\nand developers.", "published": "2023-08-17 19:12:13", "link": "http://arxiv.org/abs/2308.09158v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Comparative Study of Text Embedding Models for Semantic Text\n  Similarity in Bug Reports", "abstract": "Bug reports are an essential aspect of software development, and it is\ncrucial to identify and resolve them quickly to ensure the consistent\nfunctioning of software systems. Retrieving similar bug reports from an\nexisting database can help reduce the time and effort required to resolve bugs.\nIn this paper, we compared the effectiveness of semantic textual similarity\nmethods for retrieving similar bug reports based on a similarity score. We\nexplored several embedding models such as TF-IDF (Baseline), FastText, Gensim,\nBERT, and ADA. We used the Software Defects Data containing bug reports for\nvarious software projects to evaluate the performance of these models. Our\nexperimental results showed that BERT generally outperformed the rest of the\nmodels regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our\nstudy provides insights into the effectiveness of different embedding methods\nfor retrieving similar bug reports and highlights the impact of selecting the\nappropriate one for this task. Our code is available on GitHub.", "published": "2023-08-17 21:36:56", "link": "http://arxiv.org/abs/2308.09193v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large\n  Language Models", "abstract": "Large language models (LLMs) have achieved remarkable performance in natural\nlanguage understanding and generation tasks. However, they often suffer from\nlimitations such as difficulty in incorporating new knowledge, generating\nhallucinations, and explaining their reasoning process. To address these\nchallenges, we propose a novel prompting pipeline, named \\method, that\nleverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.\nOur method enables LLMs to comprehend KG inputs and infer with a combination of\nimplicit and external knowledge. Moreover, our method elicits the mind map of\nLLMs, which reveals their reasoning pathways based on the ontology of\nknowledge. We evaluate our method on diverse question \\& answering tasks,\nespecially in medical domains, and show significant improvements over\nbaselines. We also introduce a new hallucination evaluation benchmark and\nanalyze the effects of different components of our method. Our results\ndemonstrate the effectiveness and robustness of our method in merging knowledge\nfrom LLMs and KGs for combined inference. To reproduce our results and extend\nthe framework further, we make our codebase available at\nhttps://github.com/wyl-willing/MindMap.", "published": "2023-08-17 16:59:50", "link": "http://arxiv.org/abs/2308.09729v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based\n  Healthcare Decision Support using ChatGPT", "abstract": "This study presents an innovative approach to the application of large\nlanguage models (LLMs) in clinical decision-making, focusing on OpenAI's\nChatGPT. Our approach introduces the use of contextual prompts-strategically\ndesigned to include task description, feature description, and crucially,\nintegration of domain knowledge-for high-quality binary classification tasks\neven in data-scarce scenarios. The novelty of our work lies in the utilization\nof domain knowledge, obtained from high-performing interpretable ML models, and\nits seamless incorporation into prompt design. By viewing these ML models as\nmedical experts, we extract key insights on feature importance to aid in\ndecision-making processes. This interplay of domain knowledge and AI holds\nsignificant promise in creating a more insightful diagnostic tool.\n  Additionally, our research explores the dynamics of zero-shot and few-shot\nprompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT\nwith traditional supervised ML models in different data conditions, we aim to\nprovide insights into the effectiveness of prompt engineering strategies under\nvaried data availability. In essence, this paper bridges the gap between AI and\nhealthcare, proposing a novel methodology for LLMs application in clinical\ndecision support systems. It highlights the transformative potential of\neffective prompt design, domain knowledge integration, and flexible learning\napproaches in enhancing automated decision-making.", "published": "2023-08-17 20:50:46", "link": "http://arxiv.org/abs/2308.09731v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model\n  with Non-textual Features for CTR Prediction", "abstract": "Although deep pre-trained language models have shown promising benefit in a\nlarge set of industrial scenarios, including Click-Through-Rate (CTR)\nprediction, how to integrate pre-trained language models that handle only\ntextual signals into a prediction pipeline with non-textual features is\nchallenging.\n  Up to now two directions have been explored to integrate multi-modal inputs\nin fine-tuning of pre-trained language models. One consists of fusing the\noutcome of language models and non-textual features through an aggregation\nlayer, resulting into ensemble framework, where the cross-information between\ntextual and non-textual inputs are only learned in the aggregation layer. The\nsecond one consists of splitting non-textual features into fine-grained\nfragments and transforming the fragments to new tokens combined with textual\nones, so that they can be fed directly to transformer layers in language\nmodels. However, this approach increases the complexity of the learning and\ninference because of the numerous additional tokens.\n  To address these limitations, we propose in this work a novel framework\nBERT4CTR, with the Uni-Attention mechanism that can benefit from the\ninteractions between non-textual and textual features while maintaining low\ntime-costs in training and inference through a dimensionality reduction.\nComprehensive experiments on both public and commercial data demonstrate that\nBERT4CTR can outperform significantly the state-of-the-art frameworks to handle\nmulti-modal inputs and be applicable to CTR prediction.", "published": "2023-08-17 08:25:54", "link": "http://arxiv.org/abs/2308.11527v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual\n  Reasoning", "abstract": "Addressing the gap in understanding visual comprehension in Large Language\nModels (LLMs), we designed a challenge-response study, subjecting Google Bard\nand GPT-Vision to 64 visual tasks, spanning categories like \"Visual Situational\nReasoning\" and \"Next Scene Prediction.\" Previous models, such as GPT4, leaned\nheavily on optical character recognition tools like Tesseract, whereas Bard and\nGPT-Vision, akin to Google Lens and Visual API, employ deep learning techniques\nfor visual text recognition. However, our findings spotlight both\nvision-language model's limitations: while proficient in solving visual\nCAPTCHAs that stump ChatGPT alone, it falters in recreating visual elements\nlike ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on\neducated visual guesses. The prediction problem based on visual inputs appears\nparticularly challenging with no common-sense guesses for next-scene\nforecasting based on current \"next-token\" multimodal models. This study\nprovides experimental insights into the current capacities and areas for\nimprovement in multimodal LLMs.", "published": "2023-08-17 03:14:00", "link": "http://arxiv.org/abs/2309.16705v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker\n  Recognition Challenge 2023", "abstract": "This paper is the system description of the DKU-MSXF System for the track1,\ntrack2 and track3 of the VoxCeleb Speaker Recognition Challenge 2023\n(VoxSRC-23). For Track 1, we utilize a network structure based on ResNet for\ntraining. By constructing a cross-age QMF training set, we achieve a\nsubstantial improvement in system performance. For Track 2, we inherite the\npre-trained model from Track 1 and conducte mixed training by incorporating the\nVoxBlink-clean dataset. In comparison to Track 1, the models incorporating\nVoxBlink-clean data exhibit a performance improvement by more than 10%\nrelatively. For Track3, the semi-supervised domain adaptation task, a novel\npseudo-labeling method based on triple thresholds and sub-center purification\nis adopted to make domain adaptation. The final submission achieves mDCF of\n0.1243 in task1, mDCF of 0.1165 in Track 2 and EER of 4.952% in Track 3.", "published": "2023-08-17 03:49:01", "link": "http://arxiv.org/abs/2308.08766v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Graph Neural Network Backend for Speaker Recognition", "abstract": "Currently, most speaker recognition backends, such as cosine, linear\ndiscriminant analysis (LDA), or probabilistic linear discriminant analysis\n(PLDA), make decisions by calculating similarity or distance between enrollment\nand test embeddings which are already extracted from neural networks. However,\nfor each embedding, the local structure of itself and its neighbor embeddings\nin the low-dimensional space is different, which may be helpful for the\nrecognition but is often ignored. In order to take advantage of it, we propose\na graph neural network (GNN) backend to mine latent relationships among\nembeddings for classification. We assume all the embeddings as nodes on a\ngraph, and their edges are computed based on some similarity function, such as\ncosine, LDA+cosine, or LDA+PLDA. We study different graph settings and explore\nvariants of GNN to find a better message passing and aggregation way to\naccomplish the recognition task. Experimental results on NIST SRE14 i-vector\nchallenging, VoxCeleb1-O, VoxCeleb1-E, and VoxCeleb1-H datasets demonstrate\nthat our proposed GNN backends significantly outperform current mainstream\nmethods.", "published": "2023-08-17 03:50:37", "link": "http://arxiv.org/abs/2308.08767v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "META-SELD: Meta-Learning for Fast Adaptation to the new environment in\n  Sound Event Localization and Detection", "abstract": "For learning-based sound event localization and detection (SELD) methods,\ndifferent acoustic environments in the training and test sets may result in\nlarge performance differences in the validation and evaluation stages.\nDifferent environments, such as different sizes of rooms, different\nreverberation times, and different background noise, may be reasons for a\nlearning-based system to fail. On the other hand, acquiring annotated spatial\nsound event samples, which include onset and offset time stamps, class types of\nsound events, and direction-of-arrival (DOA) of sound sources is very\nexpensive. In addition, deploying a SELD system in a new environment often\nposes challenges due to time-consuming training and fine-tuning processes. To\naddress these issues, we propose Meta-SELD, which applies meta-learning methods\nto achieve fast adaptation to new environments. More specifically, based on\nModel Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims to find good\nmeta-initialized parameters to adapt to new environments with only a small\nnumber of samples and parameter updating iterations. We can then quickly adapt\nthe meta-trained SELD model to unseen environments. Our experiments compare\nfine-tuning methods from pre-trained SELD models with our Meta-SELD on the\nSony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation\nresults demonstrate the effectiveness of Meta-SELD when adapting to new\nenvironments.", "published": "2023-08-17 08:10:56", "link": "http://arxiv.org/abs/2308.08847v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity\n  Enhancement and Interpolation Error Compensation", "abstract": "Speech phase prediction, which is a significant research focus in the field\nof signal processing, aims to recover speech phase spectra from\namplitude-related features. However, existing speech phase prediction methods\nare constrained to recovering phase spectra with short frame shifts, which are\nconsiderably smaller than the theoretical upper bound required for exact\nwaveform reconstruction of short-time Fourier transform (STFT). To tackle this\nissue, we present a novel long-frame-shift neural speech phase prediction\n(LFS-NSPP) method which enables precise prediction of long-frame-shift phase\nspectra from long-frame-shift log amplitude spectra. The proposed method\nconsists of three stages: interpolation, prediction and decimation. The\nshort-frame-shift log amplitude spectra are first constructed from\nlong-frame-shift ones through frequency-by-frequency interpolation to enhance\nthe spectral continuity, and then employed to predict short-frame-shift phase\nspectra using an NSPP model, thereby compensating for interpolation errors.\nUltimately, the long-frame-shift phase spectra are obtained from\nshort-frame-shift ones through frame-by-frame decimation. Experimental results\nshow that the proposed LFS-NSPP method can yield superior quality in predicting\nlong-frame-shift phase spectra than the original NSPP model and other\nsignal-processing-based phase estimation algorithms.", "published": "2023-08-17 08:21:21", "link": "http://arxiv.org/abs/2308.08850v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Explicit Estimation of Magnitude and Phase Spectra in Parallel for\n  High-Quality Speech Enhancement", "abstract": "Phase information has a significant impact on speech perceptual quality and\nintelligibility. However, existing speech enhancement methods encounter\nlimitations in explicit phase estimation due to the non-structural nature and\nwrapping characteristics of the phase, leading to a bottleneck in enhanced\nspeech quality. To overcome the above issue, in this paper, we proposed\nMP-SENet, a novel Speech Enhancement Network that explicitly enhances Magnitude\nand Phase spectra in parallel. The proposed MP-SENet comprises a\nTransformer-embedded encoder-decoder architecture. The encoder aims to encode\nthe input distorted magnitude and phase spectra into time-frequency\nrepresentations, which are further fed into time-frequency Transformers for\nalternatively capturing time and frequency dependencies. The decoder comprises\na magnitude mask decoder and a phase decoder, directly enhancing magnitude and\nwrapped phase spectra by incorporating a magnitude masking architecture and a\nphase parallel estimation architecture, respectively. Multi-level loss\nfunctions explicitly defined on the magnitude spectra, wrapped phase spectra,\nand short-time complex spectra are adopted to jointly train the MP-SENet model.\nA metric discriminator is further employed to compensate for the incomplete\ncorrelation between these losses and human auditory perception. Experimental\nresults demonstrate that our proposed MP-SENet achieves state-of-the-art\nperformance across multiple speech enhancement tasks, including speech\ndenoising, dereverberation, and bandwidth extension. Compared to existing\nphase-aware speech enhancement methods, it further mitigates the compensation\neffect between the magnitude and phase by explicit phase estimation, elevating\nthe perceptual quality of enhanced speech.", "published": "2023-08-17 11:37:52", "link": "http://arxiv.org/abs/2308.08926v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Home monitoring for frailty detection through sound and speaker\n  diarization analysis", "abstract": "As the French, European and worldwide populations are aging, there is a\nstrong interest for new systems that guarantee a reliable and privacy\npreserving home monitoring for frailty prevention. This work is a part of a\nglobal environmental audio analysis system which aims to help identification of\nActivities of Daily Life (ADL) through human and everyday life sounds\nrecognition, speech presence and number of speakers detection. The focus is\nmade on the number of speakers detection. In this article, we present how\nrecent advances in sound processing and speaker diarization can improve the\nexisting embedded systems. We study the performances of two new methods and\ndiscuss the benefits of DNN based approaches which improve performances by\nabout 100%.", "published": "2023-08-17 13:47:20", "link": "http://arxiv.org/abs/2308.08985v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Severity Classification of Parkinson's Disease from Speech using Single\n  Frequency Filtering-based Features", "abstract": "Developing objective methods for assessing the severity of Parkinson's\ndisease (PD) is crucial for improving the diagnosis and treatment. This study\nproposes two sets of novel features derived from the single frequency filtering\n(SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF\n(MFCC-SFF) for the severity classification of PD. Prior studies have\ndemonstrated that SFF offers greater spectro-temporal resolution compared to\nthe short-time Fourier transform. The study uses the PC-GITA database, which\nincludes speech of PD patients and healthy controls produced in three speaking\ntasks (vowels, sentences, text reading). Experiments using the SVM classifier\nrevealed that the proposed features outperformed the conventional MFCCs in all\nthree speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative\nimprovement of 5.8% and 2.3% for the vowel task, 7.0% & 1.8% for the sentence\ntask, and 2.4% and 1.1% for the read text task, in comparison to MFCC features.", "published": "2023-08-17 15:22:47", "link": "http://arxiv.org/abs/2308.09042v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Refining a Deep Learning-based Formant Tracker using Linear Prediction\n  Methods", "abstract": "In this study, formant tracking is investigated by refining the formants\ntracked by an existing data-driven tracker, DeepFormants, using the formants\nestimated in a model-driven manner by linear prediction (LP)-based methods. As\nLP-based formant estimation methods, conventional covariance analysis (LP-COV)\nand the recently proposed quasi-closed phase forward-backward (QCP-FB) analysis\nare used. In the proposed refinement approach, the contours of the three lowest\nformants are first predicted by the data-driven DeepFormants tracker, and the\npredicted formants are replaced frame-wise with local spectral peaks shown by\nthe model-driven LP-based methods. The refinement procedure can be plugged into\nthe DeepFormants tracker with no need for any new data learning. Two refined\nDeepFormants trackers were compared with the original DeepFormants and with\nfive known traditional trackers using the popular vocal tract resonance (VTR)\ncorpus. The results indicated that the data-driven DeepFormants trackers\noutperformed the conventional trackers and that the best performance was\nobtained by refining the formants predicted by DeepFormants using QCP-FB\nanalysis. In addition, by tracking formants using VTR speech that was corrupted\nby additive noise, the study showed that the refined DeepFormants trackers were\nmore resilient to noise than the reference trackers. In general, these results\nsuggest that LP-based model-driven approaches, which have traditionally been\nused in formant estimation, can be combined with a modern data-driven tracker\neasily with no further training to improve the tracker's performance.", "published": "2023-08-17 15:32:32", "link": "http://arxiv.org/abs/2308.09051v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Bridging High-Quality Audio and Video via Language for Sound Effects\n  Retrieval from Visual Queries", "abstract": "Finding the right sound effects (SFX) to match moments in a video is a\ndifficult and time-consuming task, and relies heavily on the quality and\ncompleteness of text metadata. Retrieving high-quality (HQ) SFX using a video\nframe directly as the query is an attractive alternative, removing the reliance\non text metadata and providing a low barrier to entry for non-experts. Due to\nthe lack of HQ audio-visual training data, previous work on audio-visual\nretrieval relies on YouTube (in-the-wild) videos of varied quality for\ntraining, where the audio is often noisy and the video of amateur quality. As\nsuch it is unclear whether these systems would generalize to the task of\nmatching HQ audio to production-quality video. To address this, we propose a\nmultimodal framework for recommending HQ SFX given a video frame by (1)\nleveraging large language models and foundational vision-language models to\nbridge HQ audio and video to create audio-visual pairs, resulting in a highly\nscalable automatic audio-visual data curation pipeline; and (2) using\npre-trained audio and visual encoders to train a contrastive learning-based\nretrieval system. We show that our system, trained using our automatic data\ncuration pipeline, significantly outperforms baselines trained on in-the-wild\ndata on the task of HQ SFX retrieval for video. Furthermore, while the\nbaselines fail to generalize to this task, our system generalizes well from\nclean to in-the-wild data, outperforming the baselines on a dataset of YouTube\nvideos despite only being trained on the HQ audio-visual pairs. A user study\nconfirms that people prefer SFX retrieved by our system over the baseline 67%\nof the time both for HQ and in-the-wild data. Finally, we present ablations to\ndetermine the impact of model and data pipeline design choices on downstream\nretrieval performance. Please visit our project website to listen to and view\nour SFX retrieval results.", "published": "2023-08-17 16:38:30", "link": "http://arxiv.org/abs/2308.09089v1", "categories": ["cs.SD", "cs.CV", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
