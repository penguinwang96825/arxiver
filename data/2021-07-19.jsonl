{"title": "Bridging the Gap between Language Model and Reading Comprehension:\n  Unsupervised MRC via Self-Supervision", "abstract": "Despite recent success in machine reading comprehension (MRC), learning\nhigh-quality MRC models still requires large-scale labeled training data, even\nusing strong pre-trained language models (PLMs). The pre-training tasks for\nPLMs are not question-answering or MRC-based tasks, making existing PLMs unable\nto be directly used for unsupervised MRC. Specifically, MRC aims to spot an\naccurate answer span from the given document, but PLMs focus on token filling\nin sentences. In this paper, we propose a new framework for unsupervised MRC.\nFirstly, we propose to learn to spot answer spans in documents via\nself-supervised learning, by designing a self-supervision pretext task for MRC\n- Spotting-MLM. Solving this task requires capturing deep interactions between\nsentences in documents. Secondly, we apply a simple sentence rewriting strategy\nin the inference stage to alleviate the expression mismatch between questions\nand documents. Experiments show that our method achieves a new state-of-the-art\nperformance for unsupervised MRC.", "published": "2021-07-19 02:14:36", "link": "http://arxiv.org/abs/2107.08582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with\n  Semantically Relevant Images", "abstract": "In multi-modal dialogue systems, it is important to allow the use of images\nas part of a multi-turn conversation. Training such dialogue systems generally\nrequires a large-scale dataset consisting of multi-turn dialogues that involve\nimages, but such datasets rarely exist. In response, this paper proposes a 45k\nmulti-modal dialogue dataset created with minimal human intervention. Our\nmethod to create such a dataset consists of (1) preparing and pre-processing\ntext dialogue datasets, (2) creating image-mixed dialogues by using a\ntext-to-image replacement technique, and (3) employing a\ncontextual-similarity-based filtering step to ensure the contextual coherence\nof the dataset. To evaluate the validity of our dataset, we devise a simple\nretrieval model for dialogue sentence prediction tasks. Automatic metrics and\nhuman evaluation results on such tasks show that our dataset can be effectively\nused as training data for multi-modal dialogue systems which require an\nunderstanding of images and text in a context-aware manner. Our dataset and\ngeneration code is available at\nhttps://github.com/shh1574/multi-modal-dialogue-dataset.", "published": "2021-07-19 08:44:11", "link": "http://arxiv.org/abs/2107.08685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Unsupervised Data Generation into Self-Supervised Neural\n  Machine Translation for Low-Resource Languages", "abstract": "For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).", "published": "2021-07-19 11:56:03", "link": "http://arxiv.org/abs/2107.08772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Speech Translation for Live Subtitling: from Delay to\n  Display", "abstract": "With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.", "published": "2021-07-19 12:35:49", "link": "http://arxiv.org/abs/2107.08807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MemSum: Extractive Summarization of Long Documents Using Multi-Step\n  Episodic Markov Decision Processes", "abstract": "We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\neach step with information on the current extraction history. When MemSum\niteratively selects sentences into the summary, it considers a broad\ninformation set that would intuitively also be used by humans in this task: 1)\nthe text content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum obtains\nstate-of-the-art test-set performance (ROUGE) in summarizing long documents\ntaken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the\nimportance of local, global, and history information. A human evaluation\nconfirms the high quality and low redundancy of the generated summaries,\nstemming from MemSum's awareness of extraction history.", "published": "2021-07-19 14:41:31", "link": "http://arxiv.org/abs/2107.08929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and\n  Isometric Conditions", "abstract": "Typically, a linearly orthogonal transformation mapping is learned by\naligning static type-level embeddings to build a shared semantic space. In view\nof the analysis that contextual embeddings contain richer semantic features, we\ninvestigate a context-aware and dictionary-free mapping approach by leveraging\nparallel corpora. We illustrate that our contextual embedding space mapping\nsignificantly outperforms previous multilingual word embedding methods on the\nbilingual dictionary induction (BDI) task by providing a higher degree of\nisomorphism. To improve the quality of mapping, we also explore sense-level\nembeddings that are split from type-level representations, which can align\nspaces in a finer resolution and yield more precise mapping. Moreover, we\nreveal that contextual embedding spaces suffer from their natural properties --\nanisotropy and anisometry. To mitigate these two problems, we introduce the\niterative normalization algorithm as an imperative preprocessing step. Our\nfindings unfold the tight relationship between isotropy, isometry, and\nisomorphism in normalized contextual embedding spaces.", "published": "2021-07-19 22:57:36", "link": "http://arxiv.org/abs/2107.09186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative\n  Dataset to Fight Online Hate Speech", "abstract": "Undermining the impact of hateful content with informed and non-aggressive\nresponses, called counter narratives, has emerged as a possible solution for\nhaving healthier online communities. Thus, some NLP studies have started\naddressing the task of counter narrative generation. Although such studies have\nmade an effort to build hate speech / counter narrative (HS/CN) datasets for\nneural generation, they fall short in reaching either high-quality and/or\nhigh-quantity. In this paper, we propose a novel human-in-the-loop data\ncollection methodology in which a generative language model is refined\niteratively by using its own data from the previous loops to generate new\ntraining samples that experts review and/or post-edit. Our experiments\ncomprised several loops including dynamic variations. Results show that the\nmethodology is scalable and facilitates diverse, novel, and cost-effective data\ncollection. To our knowledge, the resulting dataset is the only expert-based\nmulti-target HS/CN dataset available to the community.", "published": "2021-07-19 09:45:54", "link": "http://arxiv.org/abs/2107.08720v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cobordisms and commutative categorial grammars", "abstract": "We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.", "published": "2021-07-19 09:55:21", "link": "http://arxiv.org/abs/2107.08728v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "E-PDDL: A Standardized Way of Defining Epistemic Planning Problems", "abstract": "Epistemic Planning (EP) refers to an automated planning setting where the\nagent reasons in the space of knowledge states and tries to find a plan to\nreach a desirable state from the current state. Its general form, the\nMulti-agent Epistemic Planning (MEP) problem involves multiple agents who need\nto reason about both the state of the world and the information flow between\nagents. In a MEP problem, multiple approaches have been developed recently with\nvarying restrictions, such as considering only the concept of knowledge while\nnot allowing the idea of belief, or not allowing for ``complex\" modal operators\nsuch as those needed to handle dynamic common knowledge. While the diversity of\napproaches has led to a deeper understanding of the problem space, the lack of\na standardized way to specify MEP problems independently of solution approaches\nhas created difficulties in comparing performance of planners, identifying\npromising techniques, exploring new strategies like ensemble methods, and\nmaking it easy for new researchers to contribute to this research area. To\naddress the situation, we propose a unified way of specifying EP problems - the\nEpistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can\nbe supported by leading MEP planners and provide corresponding parser code that\ntranslates EP problems specified in E-PDDL into (M)EP problems that can be\nhandled by several planners. This work is also useful in building more general\nepistemic planning environments where we envision a meta-cognitive module that\ntakes a planning problem in E-PDDL, identifies and assesses some of its\nfeatures, and autonomously decides which planner is the best one to solve it.", "published": "2021-07-19 10:20:20", "link": "http://arxiv.org/abs/2107.08739v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unsupervised Identification of Relevant Prior Cases", "abstract": "Document retrieval has taken its role in almost all domains of knowledge\nunderstanding, including the legal domain. Precedent refers to a court decision\nthat is considered as authority for deciding subsequent cases involving\nidentical or similar facts or similar legal issues. In this work, we propose\ndifferent unsupervised approaches to solve the task of identifying relevant\nprecedents to a given query case. Our proposed approaches are using word\nembeddings like word2vec, doc2vec, and sent2vec, finding cosine similarity\nusing TF-IDF, retrieving relevant documents using BM25 scores, using the\npre-trained model and SBERT to find the most similar document, and using the\nproduct of BM25 and TF-IDF scores to find the most relevant document for a\ngiven query. We compared all the methods based on precision@10, recall@10, and\nMRR. Based on the comparative analysis, we found that the TF-IDF score\nmultiplied by the BM25 score gives the best result. In this paper, we have also\npresented the analysis that we did to improve the BM25 score.", "published": "2021-07-19 15:41:49", "link": "http://arxiv.org/abs/2107.08973v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration", "abstract": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.", "published": "2021-07-19 18:24:33", "link": "http://arxiv.org/abs/2107.09099v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Residual Tree Aggregation of Layers for Neural Machine Translation", "abstract": "Although attention-based Neural Machine Translation has achieved remarkable\nprogress in recent layers, it still suffers from issue of making insufficient\nuse of the output of each layer. In transformer, it only uses the top layer of\nencoder and decoder in the subsequent process, which makes it impossible to\ntake advantage of the useful information in other layers. To address this\nissue, we propose a residual tree aggregation of layers for Transformer(RTAL),\nwhich helps to fuse information across layers. Specifically, we try to fuse the\ninformation across layers by constructing a post-order binary tree. In\nadditional to the last node, we add the residual connection to the process of\ngenerating child nodes. Our model is based on the Neural Machine Translation\nmodel Transformer and we conduct our experiments on WMT14 English-to-German and\nWMT17 English-to-France translation tasks. Experimental results across language\npairs show that the proposed approach outperforms the strong baseline model\nsignificantly", "published": "2021-07-19 09:32:10", "link": "http://arxiv.org/abs/2107.14590v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Independent Ethical Assessment of Text Classification Models: A Hate\n  Speech Detection Case Study", "abstract": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.", "published": "2021-07-19 23:03:36", "link": "http://arxiv.org/abs/2108.07627v1", "categories": ["cs.CY", "cs.CL", "cs.IR"], "primary_category": "cs.CY"}
{"title": "Translatotron 2: High-quality direct speech-to-speech translation with\n  voice preservation", "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a linguistic decoder, an acoustic synthesizer, and a single attention\nmodule that connects them together. Experimental results on three datasets\nconsistently show that Translatotron 2 outperforms the original Translatotron\nby a large margin on both translation quality (up to +15.5 BLEU) and speech\ngeneration quality, and approaches the same of cascade systems. In addition, we\npropose a simple method for preserving speakers' voices from the source speech\nto the translation speech in a different language. Unlike existing approaches,\nthe proposed method is able to preserve each speaker's voice on speaker turns\nwithout requiring for speaker segmentation. Furthermore, compared to existing\napproaches, it better preserves speaker's privacy and mitigates potential\nmisuse of voice cloning for creating spoofing audio artifacts.", "published": "2021-07-19 07:43:49", "link": "http://arxiv.org/abs/2107.08661v5", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Stock Movement Prediction with Financial News using Contextualized\n  Embedding from BERT", "abstract": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.", "published": "2021-07-19 09:47:28", "link": "http://arxiv.org/abs/2107.08721v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.PM", "91-10", "I.2.7; J.4"], "primary_category": "q-fin.ST"}
{"title": "Clinical Relation Extraction Using Transformer-based Models", "abstract": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.", "published": "2021-07-19 15:15:51", "link": "http://arxiv.org/abs/2107.08957v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Separating Skills and Concepts for Novel Visual Question Answering", "abstract": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.", "published": "2021-07-19 18:55:10", "link": "http://arxiv.org/abs/2107.09106v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A baseline model for computationally inexpensive speech recognition for\n  Kazakh using the Coqui STT framework", "abstract": "Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.", "published": "2021-07-19 14:17:42", "link": "http://arxiv.org/abs/2107.10637v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Quantum-like Contextuality of Ambiguous Phrases", "abstract": "Language is contextual as meanings of words are dependent on their contexts.\nContextuality is, concomitantly, a well-defined concept in quantum mechanics\nwhere it is considered a major resource for quantum computations. We\ninvestigate whether natural language exhibits any of the quantum mechanics'\ncontextual features. We show that meaning combinations in ambiguous phrases can\nbe modelled in the sheaf-theoretic framework for quantum contextuality, where\nthey can become possibilistically contextual. Using the framework of\nContextuality-by-Default (CbD), we explore the probabilistic variants of these\nand show that CbD-contextuality is also possible.", "published": "2021-07-19 13:23:42", "link": "http://arxiv.org/abs/2107.14589v1", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Self-supervision for health insurance claims data: a Covid-19 use case", "abstract": "In this work, we modify and apply self-supervision techniques to the domain\nof medical health insurance claims. We model patients' healthcare claims\nhistory analogous to free-text narratives, and introduce pre-trained `prior\nknowledge', later utilized for patient outcome predictions on a challenging\ntask: predicting Covid-19 hospitalization, given a patient's pre-Covid-19\ninsurance claims history. Results suggest that pre-training on insurance claims\nnot only produces better prediction performance, but, more importantly,\nimproves the model's `clinical trustworthiness' and model\nstability/reliability.", "published": "2021-07-19 19:00:33", "link": "http://arxiv.org/abs/2107.14591v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring a Six-hole Recorder Flute's Response to Breath Pressure\n  Variations and Fitting a Model", "abstract": "We propose the Siamese-flute method that measures the breath pressure and the\nacoustic sound in parallel. We fit a 6-DoF model to describe how the breath\npressure affects the octave and the microtonal pitch bend, revealing the octave\nhysteresis. We release both our model parameters and our data analysis tools.", "published": "2021-07-19 09:52:57", "link": "http://arxiv.org/abs/2107.08727v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Reverberant Speech Separation with Multi-stage Training and\n  Curriculum Learning", "abstract": "We present a novel approach that improves the performance of reverberant\nspeech separation. Our approach is based on an accurate geometric acoustic\nsimulator (GAS) which generates realistic room impulse responses (RIRs) by\nmodeling both specular and diffuse reflections. We also propose three training\nmethods - pre-training, multi-stage training and curriculum learning that\nsignificantly improve separation quality in the presence of reverberation. We\nalso demonstrate that mixing the synthetic RIRs with a small number of real\nRIRs during training enhances separation performance. We evaluate our approach\non reverberant mixtures generated from real, recorded data (in several\ndifferent room configurations) from the VOiCES dataset. Our novel approach\n(curriculum learning+pre-training+multi-stage training) results in a\nsignificant relative improvement over prior techniques based on image source\nmethod (ISM).", "published": "2021-07-19 22:11:19", "link": "http://arxiv.org/abs/2107.09177v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel-wise Gated Res2Net: Towards Robust Detection of Synthetic Speech\n  Attacks", "abstract": "Existing approaches for anti-spoofing in automatic speaker verification (ASV)\nstill lack generalizability to unseen attacks. The Res2Net approach designs a\nresidual-like connection between feature groups within one block, which\nincreases the possible receptive fields and improves the system's detection\ngeneralizability. However, such a residual-like connection is performed by a\ndirect addition between feature groups without channel-wise priority. We argue\nthat the information across channels may not contribute to spoofing cues\nequally, and the less relevant channels are expected to be suppressed before\nadding onto the next feature group, so that the system can generalize better to\nunseen attacks. This argument motivates the current work that presents a novel,\nchannel-wise gated Res2Net (CG-Res2Net), which modifies Res2Net to enable a\nchannel-wise gating mechanism in the connection between feature groups. This\ngating mechanism dynamically selects channel-wise features based on the input,\nto suppress the less relevant channels and enhance the detection\ngeneralizability. Three gating mechanisms with different structures are\nproposed and integrated into Res2Net. Experimental results conducted on\nASVspoof 2019 logical access (LA) demonstrate that the proposed CG-Res2Net\nsignificantly outperforms Res2Net on both the overall LA evaluation set and\nindividual difficult unseen attacks, which also outperforms other\nstate-of-the-art single systems, depicting the effectiveness of our method.", "published": "2021-07-19 12:27:40", "link": "http://arxiv.org/abs/2107.08803v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Over-Parameterization and Generalization in Audio Classification", "abstract": "Convolutional Neural Networks (CNNs) have been dominating classification\ntasks in various domains, such as machine vision, machine listening, and\nnatural language processing. In machine listening, while generally exhibiting\nvery good generalization capabilities, CNNs are sensitive to the specific audio\nrecording device used, which has been recognized as a substantial problem in\nthe acoustic scene classification (DCASE) community. In this study, we\ninvestigate the relationship between over-parameterization of acoustic scene\nclassification models, and their resulting generalization abilities.\nSpecifically, we test scaling CNNs in width and depth, under different\nconditions. Our results indicate that increasing width improves generalization\nto unseen devices, even without an increase in the number of parameters.", "published": "2021-07-19 14:48:15", "link": "http://arxiv.org/abs/2107.08933v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "On the Veracity of Local, Model-agnostic Explanations in Audio\n  Classification: Targeted Investigations with Adversarial Examples", "abstract": "Local explanation methods such as LIME have become popular in MIR as tools\nfor generating post-hoc, model-agnostic explanations of a model's\nclassification decisions. The basic idea is to identify a small set of\nhuman-understandable features of the classified example that are most\ninfluential on the classifier's prediction. These are then presented as an\nexplanation. Evaluation of such explanations in publications often resorts to\naccepting what matches the expectation of a human without actually being able\nto verify if what the explanation shows is what really caused the model's\nprediction. This paper reports on targeted investigations where we try to get\nmore insight into the actual veracity of LIME's explanations in an audio\nclassification task. We deliberately design adversarial examples for the\nclassifier, in a way that gives us knowledge about which parts of the input are\npotentially responsible for the model's (wrong) prediction. Asking LIME to\nexplain the predictions for these adversaries permits us to study whether local\nexplanations do indeed detect these regions of interest. We also look at\nwhether LIME is more successful in finding perturbations that are more\nprominent and easily noticeable for a human. Our results suggest that LIME does\nnot necessarily manage to identify the most relevant input features and hence\nit remains unclear whether explanations are useful or even misleading.", "published": "2021-07-19 17:54:10", "link": "http://arxiv.org/abs/2107.09045v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequence-to-Sequence Piano Transcription with Transformers", "abstract": "Automatic Music Transcription has seen significant progress in recent years\nby training custom deep neural networks on large datasets. However, these\nmodels have required extensive domain-specific design of network architectures,\ninput/output representations, and complex decoding schemes. In this work, we\nshow that equivalent performance can be achieved using a generic\nencoder-decoder Transformer with standard decoding methods. We demonstrate that\nthe model can learn to translate spectrogram inputs directly to MIDI-like\noutput events for several transcription tasks. This sequence-to-sequence\napproach simplifies transcription by jointly modeling audio features and\nlanguage-like output dependencies, thus removing the need for task-specific\narchitectures. These results point toward possibilities for creating new Music\nInformation Retrieval models by focusing on dataset creation and labeling\nrather than custom model design.", "published": "2021-07-19 20:33:09", "link": "http://arxiv.org/abs/2107.09142v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
