{"title": "mSLAM: Massively multilingual joint pre-training for speech and text", "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns\ncross-lingual cross-modal representations of speech and text by pre-training\njointly on large amounts of unlabeled speech and text in multiple languages.\nmSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on\ncharacter-level text, along with Connectionist Temporal Classification (CTC)\nlosses on paired speech and transcript data, to learn a single model capable of\nlearning from and representing both speech and text signals in a shared\nrepresentation space. We evaluate mSLAM on several downstream speech\nunderstanding tasks and find that joint pre-training with text improves quality\non speech translation, speech intent classification and speech language-ID\nwhile being competitive on multilingual ASR, when compared against speech-only\npre-training. Our speech translation model demonstrates zero-shot text\ntranslation without seeing any text translation data, providing evidence for\ncross-modal alignment of representations. mSLAM also benefits from multi-modal\nfine-tuning, further improving the quality of speech translation by directly\nleveraging text translation data during the fine-tuning process. Our empirical\nanalysis highlights several opportunities and challenges arising from\nlarge-scale multimodal pre-training, suggesting directions for future research.", "published": "2022-02-03 02:26:40", "link": "http://arxiv.org/abs/2202.01374v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Coherent and Consistent Use of Entities in Narrative Generation", "abstract": "Large pre-trained language models (LMs) have demonstrated impressive\ncapabilities in generating long, fluent text; however, there is little to no\nanalysis on their ability to maintain entity coherence and consistency. In this\nwork, we focus on the end task of narrative generation and systematically\nanalyse the long-range entity coherence and consistency in generated stories.\nFirst, we propose a set of automatic metrics for measuring model performance in\nterms of entity usage. Given these metrics, we quantify the limitations of\ncurrent LMs. Next, we propose augmenting a pre-trained LM with a dynamic entity\nmemory in an end-to-end manner by using an auxiliary entity-related loss for\nguiding the reads and writes to the memory. We demonstrate that the dynamic\nentity memory increases entity coherence according to both automatic and human\njudgment and helps preserving entity-related information especially in settings\nwith a limited context window. Finally, we also validate that our automatic\nmetrics are correlated with human ratings and serve as a good indicator of the\nquality of generated stories.", "published": "2022-02-03 17:19:21", "link": "http://arxiv.org/abs/2202.01709v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Language Models for Interactive Decision-Making", "abstract": "Language model (LM) pre-training is useful in many language processing tasks.\nBut can pre-trained LMs be further leveraged for more general machine learning\nproblems? We propose an approach for using LMs to scaffold learning and\ngeneralization in general sequential decision-making problems. In this\napproach, goals and observations are represented as a sequence of embeddings,\nand a policy network initialized with a pre-trained LM predicts the next\naction. We demonstrate that this framework enables effective combinatorial\ngeneralization across different environments and supervisory modalities. We\nbegin by assuming access to a set of expert demonstrations, and show that\ninitializing policies with LMs and fine-tuning them via behavior cloning\nimproves task completion rates by 43.6% in the VirtualHome environment. Next,\nwe integrate an active data gathering procedure in which agents iteratively\ninteract with the environment, relabel past \"failed\" experiences with new\ngoals, and update their policies in a self-supervised loop. Active data\ngathering further improves combinatorial generalization, outperforming the best\nbaseline by 25.1%. Finally, we explain these results by investigating three\npossible factors underlying the effectiveness of the LM-based policy. We find\nthat sequential input representations (vs. fixed-dimensional feature vectors)\nand LM-based weight initialization are both important for generalization.\nSurprisingly, however, the format of the policy inputs encoding (e.g. as a\nnatural language string vs. an arbitrary sequential encoding) has little\ninfluence. Together, these results suggest that language modeling induces\nrepresentations that are useful for modeling not just language, but also goals\nand plans; these representations can aid learning and generalization even\noutside of language processing.", "published": "2022-02-03 18:55:52", "link": "http://arxiv.org/abs/2202.01771v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Survey On Semantic Steganography Systems", "abstract": "Steganography is the practice of concealing a message within some other\ncarrier or cover message. It is used to allow the sending of hidden information\nthrough communication channels where third parties would only be aware of the\nexplicit information in the carrier message. With the growth of internet\nsurveillance and the increased need for secret communication, steganography\nsystems continue to find new applications. In semantic steganography, the\nredundancies in the semantics of a language are used to send a text\nsteganographic message. In this article we go over the concepts behind semantic\nsteganography and propose a hierarchy for classifying systems within the\ncontext of text steganography and steganography in general. After laying this\ngroundwork we list systems for semantic steganography that have been published\nin the past and review their properties. Finally, we comment on and briefly\ncompare the described systems.", "published": "2022-02-03 15:23:53", "link": "http://arxiv.org/abs/2203.12425v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Joint Speech Recognition and Audio Captioning", "abstract": "Speech samples recorded in both indoor and outdoor environments are often\ncontaminated with secondary audio sources. Most end-to-end monaural speech\nrecognition systems either remove these background sounds using speech\nenhancement or train noise-robust models. For better model interpretability and\nholistic understanding, we aim to bring together the growing field of automated\naudio captioning (AAC) and the thoroughly studied automatic speech recognition\n(ASR). The goal of AAC is to generate natural language descriptions of contents\nin audio samples. We propose several approaches for end-to-end joint modeling\nof ASR and AAC tasks and demonstrate their advantages over traditional\napproaches, which model these tasks independently. A major hurdle in evaluating\nour proposed approach is the lack of labeled audio datasets with both speech\ntranscriptions and audio captions. Therefore we also create a multi-task\ndataset by mixing the clean speech Wall Street Journal corpus with multiple\nlevels of background noises chosen from the AudioCaps dataset. We also perform\nextensive experimental evaluation and show improvements of our proposed methods\nas compared to existing state-of-the-art ASR and AAC methods.", "published": "2022-02-03 04:42:43", "link": "http://arxiv.org/abs/2202.01405v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MFA: TDNN with Multi-scale Frequency-channel Attention for\n  Text-independent Speaker Verification with Short Utterances", "abstract": "The time delay neural network (TDNN) represents one of the state-of-the-art\nof neural solutions to text-independent speaker verification. However, they\nrequire a large number of filters to capture the speaker characteristics at any\nlocal frequency region. In addition, the performance of such systems may\ndegrade under short utterance scenarios. To address these issues, we propose a\nmulti-scale frequency-channel attention (MFA), where we characterize speakers\nat different scales through a novel dual-path design which consists of a\nconvolutional neural network and TDNN. We evaluate the proposed MFA on the\nVoxCeleb database and observe that the proposed framework with MFA can achieve\nstate-of-the-art performance while reducing parameters and computation\ncomplexity. Further, the MFA mechanism is found to be effective for speaker\nverification with short test utterances.", "published": "2022-02-03 14:57:05", "link": "http://arxiv.org/abs/2202.01624v3", "categories": ["cs.SD", "cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "JaQuAD: Japanese Question Answering Dataset for Machine Reading\n  Comprehension", "abstract": "Question Answering (QA) is a task in which a machine understands a given\ndocument and a question to find an answer. Despite impressive progress in the\nNLP area, QA is still a challenging problem, especially for non-English\nlanguages due to the lack of annotated datasets. In this paper, we present the\nJapanese Question Answering Dataset, JaQuAD, which is annotated by humans.\nJaQuAD consists of 39,696 extractive question-answer pairs on Japanese\nWikipedia articles. We finetuned a baseline model which achieves 78.92% for F1\nscore and 63.38% for EM on test set. The dataset and our experiments are\navailable at https://github.com/SkelterLabsInc/JaQuAD.", "published": "2022-02-03 18:40:25", "link": "http://arxiv.org/abs/2202.01764v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Different Affordances on Facebook and SMS Text Messaging Do Not Impede\n  Generalization of Language-Based Predictive Models", "abstract": "Adaptive mobile device-based health interventions often use machine learning\nmodels trained on non-mobile device data, such as social media text, due to the\ndifficulty and high expense of collecting large text message (SMS) data.\nTherefore, understanding the differences and generalization of models between\nthese platforms is crucial for proper deployment. We examined the\npsycho-linguistic differences between Facebook and text messages, and their\nimpact on out-of-domain model performance, using a sample of 120 users who\nshared both. We found that users use Facebook for sharing experiences (e.g.,\nleisure) and SMS for task-oriented and conversational purposes (e.g., plan\nconfirmations), reflecting the differences in the affordances. To examine the\ndownstream effects of these differences, we used pre-trained Facebook-based\nlanguage models to estimate age, gender, depression, life satisfaction, and\nstress on both Facebook and SMS. We found no significant differences in\ncorrelations between the estimates and self-reports across 6 of 8 models. These\nresults suggest using pre-trained Facebook language models to achieve better\naccuracy with just-in-time interventions.", "published": "2022-02-03 19:18:47", "link": "http://arxiv.org/abs/2202.01802v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-supervised Learning with Random-projection Quantizer for Speech\n  Recognition", "abstract": "We present a simple and effective self-supervised learning approach for\nspeech recognition. The approach learns a model to predict the masked speech\nsignals, in the form of discrete labels generated with a random-projection\nquantizer. In particular the quantizer projects speech inputs with a randomly\ninitialized matrix, and does a nearest-neighbor lookup in a\nrandomly-initialized codebook. Neither the matrix nor the codebook is updated\nduring self-supervised learning. Since the random-projection quantizer is not\ntrained and is separated from the speech recognition model, the design makes\nthe approach flexible and is compatible with universal speech recognition\narchitecture. On LibriSpeech our approach achieves similar word-error-rates as\nprevious work using self-supervised learning with non-streaming models, and\nprovides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with\nstreaming models. On multilingual tasks the approach also provides significant\nimprovement over wav2vec 2.0 and w2v-BERT.", "published": "2022-02-03 21:29:04", "link": "http://arxiv.org/abs/2202.01855v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Real-time Emergency Vehicle Event Detection Using Audio Data", "abstract": "In this work, we focus on detecting emergency vehicles using only audio data.\nImproved and quick detection can help in faster preemption of these vehicles at\nsignalized intersections thereby reducing overall response time in case of\nemergencies. Important audio features were extracted from raw data and passed\ninto extreme learning machines (ELM) for training. ELMs have been used in this\nwork because of its simplicity and shorter run-time which can therefore be used\nfor online learning. Recently, there have been many studies that focus on sound\nclassification but most of the methods used are complex to train and implement.\nThe results from this paper show that ELM can achieve similar performance with\nexceptionally shorter training times. The accuracy reported for ELM is about\n97% for emergency vehicle detection (EVD).", "published": "2022-02-03 01:40:19", "link": "http://arxiv.org/abs/2202.01367v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A deep complex multi-frame filtering network for stereophonic acoustic\n  echo cancellation", "abstract": "In hands-free communication system, the coupling between loudspeaker and\nmicrophone generates echo signal, which can severely influence the quality of\ncommunication. Meanwhile, various types of noise in communication environments\nfurther reduce speech quality and intelligibility. It is difficult to extract\nthe near-end signal from the microphone signal within one step, especially in\nlow signal-to-noise ratio scenarios. In this paper, we propose a deep complex\nnetwork approach to address this issue. Specially, we decompose the\nstereophonic acoustic echo cancellation into two stages, including linear\nstereophonic acoustic echo cancellation module and residual echo suppression\nmodule, where both modules are based on deep learning architectures. A\nmulti-frame filtering strategy is introduced to benefit the estimation of\nlinear echo by capturing more inter-frame information. Moreover, we decouple\nthe complex spectral mapping into magnitude estimation and complex spectrum\nrefinement. Experimental results demonstrate that our proposed approach\nachieves stage-of-the-art performance over previous advanced algorithms under\nvarious conditions.", "published": "2022-02-03 15:01:17", "link": "http://arxiv.org/abs/2202.01630v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A benchmark of state-of-the-art sound event detection systems evaluated\n  on synthetic soundscapes", "abstract": "This paper proposes a benchmark of submissions to Detection and\nClassification Acoustic Scene and Events 2021 Challenge (DCASE) Task 4\nrepresenting a sampling of the state-of-the-art in Sound Event Detection task.\nThe submissions are evaluated according to the two polyphonic sound detection\nscore scenarios proposed for the DCASE 2021 Challenge Task 4, which allow to\nmake an analysis on whether submissions are designed to perform fine-grained\ntemporal segmentation, coarse-grained temporal segmentation, or have been\ndesigned to be polyvalent on the scenarios proposed. We study the solutions\nproposed by participants to analyze their robustness to varying level target to\nnon-target signal-to-noise ratio and to temporal localization of target sound\nevents. A last experiment is proposed in order to study the impact of\nnon-target events on systems outputs. Results show that systems adapted to\nprovide coarse segmentation outputs are more robust to different target to\nnon-target signal-to-noise ratio and, with the help of specific data\naugmentation methods, they are more robust to time localization of the original\nevent. Results of the last experiment display that systems tend to spuriously\npredict short events when non-target events are present. This is particularly\ntrue for systems that are tailored to have a fine segmentation.", "published": "2022-02-03 09:41:31", "link": "http://arxiv.org/abs/2202.01487v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Psychoacoustic Quality Criterion for Path-Traced Sound Propagation", "abstract": "In developing virtual acoustic environments, it is important to understand\nthe relationship between the computation cost and the perceptual significance\nof the resultant numerical error. In this paper, we propose a quality criterion\nthat evaluates the error significance of path-tracing-based sound propagation\nsimulators. We present an analytical formula that estimates the error signal\npower spectrum. With this spectrum estimation, we can use a modified Zwicker's\nloudness model to calculate the relative loudness of the error signal masked by\nthe ideal output. Our experimental results show that the proposed criterion can\nexplain the human perception of simulation error in a variety of cases.", "published": "2022-02-03 13:39:23", "link": "http://arxiv.org/abs/2202.01582v2", "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The RoyalFlush System of Speech Recognition for M2MeT Challenge", "abstract": "This paper describes our RoyalFlush system for the track of multi-speaker\nautomatic speech recognition (ASR) in the M2MeT challenge. We adopted the\nserialized output training (SOT) based multi-speakers ASR system with\nlarge-scale simulation data. Firstly, we investigated a set of front-end\nmethods, including multi-channel weighted predicted error (WPE), beamforming,\nspeech separation, speech enhancement and so on, to process training,\nvalidation and test sets. But we only selected WPE and beamforming as our\nfrontend methods according to their experimental results. Secondly, we made\ngreat efforts in the data augmentation for multi-speaker ASR, mainly including\nadding noise and reverberation, overlapped speech simulation, multi-channel\nspeech simulation, speed perturbation, front-end processing, and so on, which\nbrought us a great performance improvement. Finally, in order to make full use\nof the performance complementary of different model architecture, we trained\nthe standard conformer based joint CTC/Attention (Conformer) and U2++ ASR model\nwith a bidirectional attention decoder, a modification of Conformer, to fuse\ntheir results. Comparing with the official baseline system, our system got a\n12.22% absolute Character Error Rate (CER) reduction on the validation set and\n12.11% on the test set.", "published": "2022-02-03 14:38:26", "link": "http://arxiv.org/abs/2202.01614v2", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Improving Lyrics Alignment through Joint Pitch Detection", "abstract": "In recent years, the accuracy of automatic lyrics alignment methods has\nincreased considerably. Yet, many current approaches employ frameworks designed\nfor automatic speech recognition (ASR) and do not exploit properties specific\nto music. Pitch is one important musical attribute of singing voice but it is\noften ignored by current systems as the lyrics content is considered\nindependent of the pitch. In practice, however, there is a temporal correlation\nbetween the two as note starts often correlate with phoneme starts. At the same\ntime the pitch is usually annotated with high temporal accuracy in ground truth\ndata while the timing of lyrics is often only available at the line (or word)\nlevel. In this paper, we propose a multi-task learning approach for lyrics\nalignment that incorporates pitch and thus can make use of a new source of\nhighly accurate temporal information. Our results show that the accuracy of the\nalignment result is indeed improved by our approach. As an additional\ncontribution, we show that integrating boundary detection in the\nforced-alignment algorithm reduces cross-line errors, which improves the\naccuracy even further.", "published": "2022-02-03 15:43:19", "link": "http://arxiv.org/abs/2202.01646v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Distortion Audio Effects: Learning How to Recover the Clean Signal", "abstract": "Given the recent advances in music source separation and automatic mixing,\nremoving audio effects in music tracks is a meaningful step toward developing\nan automated remixing system. This paper focuses on removing distortion audio\neffects applied to guitar tracks in music production. We explore whether effect\nremoval can be solved by neural networks designed for source separation and\naudio effect modeling.\n  Our approach proves particularly effective for effects that mix the processed\nand clean signals. The models achieve better quality and significantly faster\ninference compared to state-of-the-art solutions based on sparse optimization.\nWe demonstrate that the models are suitable not only for declipping but also\nfor other types of distortion effects. By discussing the results, we stress the\nusefulness of multiple evaluation metrics to assess different aspects of\nreconstruction in distortion effect removal.", "published": "2022-02-03 16:26:29", "link": "http://arxiv.org/abs/2202.01664v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Audio Anomaly Detection", "abstract": "We propose an outlier robust multivariate time series model which can be used\nfor detecting previously unseen anomalous sounds based on noisy training data.\nThe presented approach doesn't assume the presence of labeled anomalies in the\ntraining dataset and uses a novel deep neural network architecture to learn the\ntemporal dynamics of the multivariate time series at multiple resolutions while\nbeing robust to contaminations in the training dataset. The temporal dynamics\nare modeled using recurrent layers augmented with attention mechanism. These\nrecurrent layers are built on top of convolutional layers allowing the network\nto extract features at multiple resolutions. The output of the network is an\noutlier robust probability density function modeling the conditional\nprobability of future samples given the time series history. State-of-the-art\napproaches using other multiresolution architectures are contrasted with our\nproposed approach. We validate our solution using publicly available machine\nsound datasets. We demonstrate the effectiveness of our approach in anomaly\ndetection by comparing against several state-of-the-art models.", "published": "2022-02-03 17:19:42", "link": "http://arxiv.org/abs/2202.01784v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
