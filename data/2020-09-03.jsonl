{"title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models", "abstract": "Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.", "published": "2020-09-03 19:56:30", "link": "http://arxiv.org/abs/2009.07053v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Derived metrics for the game of Go -- intrinsic network strength assessment and cheat-detection", "abstract": "The widespread availability of superhuman AI engines is changing how we play the ancient game of Go. The open-source software packages developed after the AlphaGo series shifted focus from producing strong playing entities to providing tools for analyzing games. Here we describe two ways of how the innovations of the second generation engines (e.g.~score estimates, variable komi) can be used for defining new metrics that help deepen our understanding of the game. First, we study how much information the search component contributes in addition to the raw neural network policy output. This gives an intrinsic strength measurement for the neural network. Second, we define the effect of a move by the difference in score estimates. This gives a fine-grained, move-by-move performance evaluation of a player. We use this in combating the new challenge of detecting online cheating.", "published": "2020-09-03 12:25:02", "link": "http://arxiv.org/abs/2009.01606v3", "categories": ["cs.AI"], "primary_category": "cs.AI"}
