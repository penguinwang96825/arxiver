{"title": "orgFAQ: A New Dataset and Analysis on Organizational FAQs and User\n  Questions", "abstract": "Frequently Asked Questions (FAQ) webpages are created by organizations for\ntheir users. FAQs are used in several scenarios, e.g., to answer user\nquestions. On the other hand, the content of FAQs is affected by user questions\nby definition. In order to promote research in this field, several FAQ datasets\nexist. However, we claim that being collected from community websites, they do\nnot correctly represent challenges associated with FAQs in an organizational\ncontext. Thus, we release orgFAQ, a new dataset composed of $6988$ user\nquestions and $1579$ corresponding FAQs that were extracted from organizations'\nFAQ webpages in the Jobs domain. In this paper, we provide an analysis of the\nproperties of such FAQs, and demonstrate the usefulness of our new dataset by\nutilizing it in a relevant task from the Jobs domain. We also show the value of\nthe orgFAQ dataset in a task of a different domain - the COVID-19 pandemic.", "published": "2020-09-03 05:51:27", "link": "http://arxiv.org/abs/2009.01460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical named entity recognition using BERT in the machine reading\n  comprehension framework", "abstract": "Recognition of biomedical entities from literature is a challenging research\nfocus, which is the foundation for extracting a large amount of biomedical\nknowledge existing in unstructured texts into structured formats. Using the\nsequence labeling framework to implement biomedical named entity recognition\n(BioNER) is currently a conventional method. This method, however, often cannot\ntake full advantage of the semantic information in the dataset, and the\nperformance is not always satisfactory. In this work, instead of treating the\nBioNER task as a sequence labeling problem, we formulate it as a machine\nreading comprehension (MRC) problem. This formulation can introduce more prior\nknowledge utilizing well-designed queries, and no longer need decoding\nprocesses such as conditional random fields (CRF). We conduct experiments on\nsix BioNER datasets, and the experimental results demonstrate the effectiveness\nof our method. Our method achieves state-of-the-art (SOTA) performance on the\nBC4CHEMD, BC5CDR-Chem, BC5CDR-Disease, NCBI-Disease, BC2GM and JNLPBA datasets,\nachieving F1-scores of 92.92%, 94.19%, 87.83%, 90.04%, 85.48% and 78.93%,\nrespectively.", "published": "2020-09-03 10:10:20", "link": "http://arxiv.org/abs/2009.01560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ADAPT Enhanced Dependency Parser at the IWPT 2020 Shared Task", "abstract": "We describe the ADAPT system for the 2020 IWPT Shared Task on parsing\nenhanced Universal Dependencies in 17 languages. We implement a pipeline\napproach using UDPipe and UDPipe-future to provide initial levels of\nannotation. The enhanced dependency graph is either produced by a graph-based\nsemantic dependency parser or is built from the basic tree using a small set of\nheuristics. Our results show that, for the majority of languages, a semantic\ndependency parser can be successfully applied to the task of parsing enhanced\ndependencies.\n  Unfortunately, we did not ensure a connected graph as part of our pipeline\napproach and our competition submission relied on a last-minute fix to pass the\nvalidation script which harmed our official evaluation scores significantly.\nOur submission ranked eighth in the official evaluation with a macro-averaged\ncoarse ELAS F1 of 67.23 and a treebank average of 67.49. We later implemented\nour own graph-connecting fix which resulted in a score of 79.53 (language\naverage) or 79.76 (treebank average), which would have placed fourth in the\ncompetition evaluation.", "published": "2020-09-03 14:43:04", "link": "http://arxiv.org/abs/2009.01712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Python Library for Exploratory Data Analysis on Twitter Data based on\n  Tokens and Aggregated Origin-Destination Information", "abstract": "Twitter is perhaps the social media more amenable for research. It requires\nonly a few steps to obtain information, and there are plenty of libraries that\ncan help in this regard. Nonetheless, knowing whether a particular event is\nexpressed on Twitter is a challenging task that requires a considerable\ncollection of tweets. This proposal aims to facilitate, to a researcher\ninterested, the process of mining events on Twitter by opening a collection of\nprocessed information taken from Twitter since December 2015. The events could\nbe related to natural disasters, health issues, and people's mobility, among\nother studies that can be pursued with the library proposed. Different\napplications are presented in this contribution to illustrate the library's\ncapabilities: an exploratory analysis of the topics discovered in tweets, a\nstudy on similarity among dialects of the Spanish language, and a mobility\nreport on different countries. In summary, the Python library presented is\napplied to different domains and retrieves a plethora of information in terms\nof frequencies by day of words and bi-grams of words for Arabic, English,\nSpanish, and Russian languages. As well as mobility information related to the\nnumber of travels among locations for more than 200 countries or territories.", "published": "2020-09-03 17:44:44", "link": "http://arxiv.org/abs/2009.01826v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounded Language Learning Fast and Slow", "abstract": "Recent work has shown that large text-based neural language models, trained\nwith conventional supervised learning objectives, acquire a surprising\npropensity for few- and one-shot learning. Here, we show that an embodied agent\nsituated in a simulated 3D world, and endowed with a novel dual-coding external\nmemory, can exhibit similar one-shot word learning when trained with\nconventional reinforcement learning algorithms. After a single introduction to\na novel object via continuous visual perception and a language prompt (\"This is\na dax\"), the agent can re-identify the object and manipulate it as instructed\n(\"Put the dax on the bed\"). In doing so, it seamlessly integrates short-term,\nwithin-episode knowledge of the appropriate referent for the word \"dax\" with\nlong-term lexical and motor knowledge acquired across episodes (i.e. \"bed\" and\n\"putting\"). We find that, under certain training conditions and with a\nparticular memory writing mechanism, the agent's one-shot word-object binding\ngeneralizes to novel exemplars within the same ShapeNet category, and is\neffective in settings with unfamiliar numbers of objects. We further show how\ndual-coding memory can be exploited as a signal for intrinsic motivation,\nstimulating the agent to seek names for objects that may be useful for later\nexecuting instructions. Together, the results demonstrate that deep neural\nnetworks can exploit meta-learning, episodic memory and an explicitly\nmulti-modal environment to account for 'fast-mapping', a fundamental pillar of\nhuman cognitive development and a potentially transformative capacity for\nagents that interact with human users.", "published": "2020-09-03 14:52:03", "link": "http://arxiv.org/abs/2009.01719v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Semantic Information Retrieval", "abstract": "Information Retrieval (IR) is the task of obtaining pieces of data (such as\ndocuments or snippets of text) that are relevant to a particular query or need\nfrom a large repository of information. While a combination of traditional\nkeyword- and modern BERT-based approaches have been shown to be effective in\nrecent work, there are often nuances in identifying what information is\n\"relevant\" to a particular query, which can be difficult to properly capture\nusing these systems. This work introduces the concept of a Multi-Perspective IR\nsystem, a novel methodology that combines multiple deep learning and\ntraditional IR models to better predict the relevance of a query-sentence pair,\nalong with a standardized framework for tuning this system. This work is\nevaluated on the BioASQ Biomedical IR + QA challenges.", "published": "2020-09-03 21:56:38", "link": "http://arxiv.org/abs/2009.01938v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in\n  Language Models", "abstract": "Advances in language modeling have led to the development of deep\nattention-based models that are performant across a wide variety of natural\nlanguage processing (NLP) problems. These language models are typified by a\npre-training process on large unlabeled text corpora and subsequently\nfine-tuned for specific tasks. Although considerable work has been devoted to\nunderstanding the attention mechanisms of pre-trained models, it is less\nunderstood how a model's attention mechanisms change when trained for a target\nNLP task. In this paper, we propose a visual analytics approach to\nunderstanding fine-tuning in attention-based language models. Our\nvisualization, Attention Flows, is designed to support users in querying,\ntracing, and comparing attention within layers, across layers, and amongst\nattention heads in Transformer-based language models. To help users gain\ninsight on how a classification decision is made, our design is centered on\ndepicting classification-based attention at the deepest layer and how attention\nfrom prior layers flows throughout words in the input. Attention Flows supports\nthe analysis of a single model, as well as the visual comparison between\npre-trained and fine-tuned models via their similarities and differences. We\nuse Attention Flows to study attention mechanisms in various sentence\nunderstanding tasks and highlight how attention evolves to address the nuances\nof solving these tasks.", "published": "2020-09-03 19:56:30", "link": "http://arxiv.org/abs/2009.07053v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression\n  Grounding", "abstract": "The prevailing framework for solving referring expression grounding is based\non a two-stage process: 1) detecting proposals with an object detector and 2)\ngrounding the referent to one of the proposals. Existing two-stage solutions\nmostly focus on the grounding step, which aims to align the expressions with\nthe proposals. In this paper, we argue that these methods overlook an obvious\nmismatch between the roles of proposals in the two stages: they generate\nproposals solely based on the detection confidence (i.e., expression-agnostic),\nhoping that the proposals contain all right instances in the expression (i.e.,\nexpression-aware). Due to this mismatch, current two-stage methods suffer from\na severe performance drop between detected and ground-truth proposals. To this\nend, we propose Ref-NMS, which is the first method to yield expression-aware\nproposals at the first stage. Ref-NMS regards all nouns in the expression as\ncritical objects, and introduces a lightweight module to predict a score for\naligning each box with a critical object. These scores can guide the NMS\noperation to filter out the boxes irrelevant to the expression, increasing the\nrecall of critical objects, resulting in a significantly improved grounding\nperformance. Since Ref- NMS is agnostic to the grounding step, it can be easily\nintegrated into any state-of-the-art two-stage method. Extensive ablation\nstudies on several backbones, benchmarks, and tasks consistently demonstrate\nthe superiority of Ref-NMS. Codes are available at:\nhttps://github.com/ChopinSharp/ref-nms.", "published": "2020-09-03 05:04:12", "link": "http://arxiv.org/abs/2009.01449v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "HiFiSinger: Towards High-Fidelity Neural Singing Voice Synthesis", "abstract": "High-fidelity singing voices usually require higher sampling rate (e.g.,\n48kHz) to convey expression and emotion. However, higher sampling rate causes\nthe wider frequency band and longer waveform sequences and throws challenges\nfor singing voice synthesis (SVS) in both frequency and time domains.\nConventional SVS systems that adopt small sampling rate cannot well address the\nabove challenges. In this paper, we develop HiFiSinger, an SVS system towards\nhigh-fidelity singing voice. HiFiSinger consists of a FastSpeech based acoustic\nmodel and a Parallel WaveGAN based vocoder to ensure fast training and\ninference and also high voice quality. To tackle the difficulty of singing\nmodeling caused by high sampling rate (wider frequency band and longer\nwaveform), we introduce multi-scale adversarial training in both the acoustic\nmodel and vocoder to improve singing modeling. Specifically, 1) To handle the\nlarger range of frequencies caused by higher sampling rate, we propose a novel\nsub-frequency GAN (SF-GAN) on mel-spectrogram generation, which splits the full\n80-dimensional mel-frequency into multiple sub-bands and models each sub-band\nwith a separate discriminator. 2) To model longer waveform sequences caused by\nhigher sampling rate, we propose a multi-length GAN (ML-GAN) for waveform\ngeneration to model different lengths of waveform sequences with separate\ndiscriminators. 3) We also introduce several additional designs and findings in\nHiFiSinger that are crucial for high-fidelity voices, such as adding F0 (pitch)\nand V/UV (voiced/unvoiced flag) as acoustic features, choosing an appropriate\nwindow/hop size for mel-spectrogram, and increasing the receptive field in\nvocoder for long vowel modeling. Experiment results show that HiFiSinger\nsynthesizes high-fidelity singing voices with much higher quality: 0.32/0.44\nMOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems.", "published": "2020-09-03 16:31:02", "link": "http://arxiv.org/abs/2009.01776v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-grained Early Frequency Attention for Deep Speaker Representation\n  Learning", "abstract": "Deep learning techniques have considerably improved speech processing in\nrecent years. Speaker representations extracted by deep learning models are\nbeing used in a wide range of tasks such as speaker recognition and speech\nemotion recognition. Attention mechanisms have started to play an important\nrole in improving deep learning models in the field of speech processing.\nNonetheless, despite the fact that important speaker-related information can be\nembedded in individual frequency-bins of the input spectral representations,\ncurrent attention models are unable to attend to fine-grained information items\nin spectral representations. In this paper we propose Fine-grained Early\nFrequency Attention (FEFA) for speaker representation learning. Our model is a\nsimple and lightweight model that can be integrated into various CNN pipelines\nand is capable of focusing on information items as small as frequency-bins. We\nevaluate the proposed model on three tasks of speaker recognition, speech\nemotion recognition, and spoken digit recognition. We use Three widely used\npublic datasets, namely VoxCeleb, IEMOCAP, and Free Spoken Digit Dataset for\nour experiments. We attach FEFA to several prominent deep learning models and\nevaluate its impact on the final performance. We also compare our work with\nother related works in the area. Our experiments show that by adding FEFA to\ndifferent CNN architectures, performance is consistently improved by\nsubstantial margins, and the models equipped with FEFA outperform all the other\nattentive models. We also test our model against different levels of added\nnoise showing improvements in robustness and less sensitivity compared to the\nbackbone networks.", "published": "2020-09-03 17:40:27", "link": "http://arxiv.org/abs/2009.01822v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoNCRA: A Convolutional Neural Network Code Retrieval Approach", "abstract": "Software developers routinely search for code using general-purpose search\nengines. However, these search engines cannot find code semantically unless it\nhas an accompanying description. We propose a technique for semantic code\nsearch: A Convolutional Neural Network approach to code retrieval (CoNCRA). Our\ntechnique aims to find the code snippet that most closely matches the\ndeveloper's intent, expressed in natural language. We evaluated our approach's\nefficacy on a dataset composed of questions and code snippets collected from\nStack Overflow. Our preliminary results showed that our technique, which\nprioritizes local interactions (words nearby), improved the state-of-the-art\n(SOTA) by 5% on average, retrieving the most relevant code snippets in the top\n3 (three) positions by almost 80% of the time. Therefore, our technique is\npromising and can improve the efficacy of semantic code retrieval.", "published": "2020-09-03 23:38:52", "link": "http://arxiv.org/abs/2009.01959v1", "categories": ["cs.LG", "cs.CL", "cs.SE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Data Programming by Demonstration: A Framework for Interactively\n  Learning Labeling Functions", "abstract": "Data programming is a programmatic weak supervision approach to efficiently\ncurate large-scale labeled training data. Writing data programs (labeling\nfunctions) requires, however, both programming literacy and domain expertise.\nMany subject matter experts have neither programming proficiency nor time to\neffectively write data programs. Furthermore, regardless of one's expertise in\ncoding or machine learning, transferring domain expertise into labeling\nfunctions by enumerating rules and thresholds is not only time consuming but\nalso inherently difficult. Here we propose a new framework, data programming by\ndemonstration (DPBD), to generate labeling rules using interactive\ndemonstrations of users. DPBD aims to relieve the burden of writing labeling\nfunctions from users, enabling them to focus on higher-level semantics such as\nidentifying relevant signals for labeling tasks. We operationalize our\nframework with Ruler, an interactive system that synthesizes labeling rules for\ndocument classification by using span-level annotations of users on document\nexamples. We compare Ruler with conventional data programming through a user\nstudy conducted with 10 data scientists creating labeling functions for\nsentiment and spam classification tasks. We find that Ruler is easier to use\nand learn and offers higher overall satisfaction, while providing\ndiscriminative model performances comparable to ones achieved by conventional\ndata programming.", "published": "2020-09-03 04:25:08", "link": "http://arxiv.org/abs/2009.01444v3", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.HC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sparse Meta Networks for Sequential Adaptation and its Application to\n  Adaptive Language Modelling", "abstract": "Training a deep neural network requires a large amount of single-task data\nand involves a long time-consuming optimization phase. This is not scalable to\ncomplex, realistic environments with new unexpected changes. Humans can perform\nfast incremental learning on the fly and memory systems in the brain play a\ncritical role. We introduce Sparse Meta Networks -- a meta-learning approach to\nlearn online sequential adaptation algorithms for deep neural networks, by\nusing deep neural networks. We augment a deep neural network with a\nlayer-specific fast-weight memory. The fast-weights are generated sparsely at\neach time step and accumulated incrementally through time providing a useful\ninductive bias for online continual adaptation. We demonstrate strong\nperformance on a variety of sequential adaptation scenarios, from a simple\nonline reinforcement learning to a large scale adaptive language modelling.", "published": "2020-09-03 17:06:52", "link": "http://arxiv.org/abs/2009.01803v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Intra-Utterance Similarity Preserving Knowledge Distillation for Audio\n  Tagging", "abstract": "Knowledge Distillation (KD) is a popular area of research for reducing the\nsize of large models while still maintaining good performance. The outputs of\nlarger teacher models are used to guide the training of smaller student models.\nGiven the repetitive nature of acoustic events, we propose to leverage this\ninformation to regulate the KD training for Audio Tagging. This novel KD\nmethod, \"Intra-Utterance Similarity Preserving KD\" (IUSP), shows promising\nresults for the audio tagging task. It is motivated by the previously published\nKD method: \"Similarity Preserving KD\" (SP). However, instead of preserving the\npairwise similarities between inputs within a mini-batch, our method preserves\nthe pairwise similarities between the frames of a single input utterance. Our\nproposed KD method, IUSP, shows consistent improvements over SP across student\nmodels of different sizes on the DCASE 2019 Task 5 dataset for audio tagging.\nThere is a 27.1% to 122.4% percent increase in improvement of micro AUPRC over\nthe baseline relative to SP's improvement of over the baseline.", "published": "2020-09-03 15:50:13", "link": "http://arxiv.org/abs/2009.01759v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dense CNN with Self-Attention for Time-Domain Speech Enhancement", "abstract": "Speech enhancement in the time domain is becoming increasingly popular in\nrecent years, due to its capability to jointly enhance both the magnitude and\nthe phase of speech. In this work, we propose a dense convolutional network\n(DCN) with self-attention for speech enhancement in the time domain. DCN is an\nencoder and decoder based architecture with skip connections. Each layer in the\nencoder and the decoder comprises a dense block and an attention module. Dense\nblocks and attention modules help in feature extraction using a combination of\nfeature reuse, increased network depth, and maximum context aggregation.\nFurthermore, we reveal previously unknown problems with a loss based on the\nspectral magnitude of enhanced speech. To alleviate these problems, we propose\na novel loss based on magnitudes of enhanced speech and a predicted noise. Even\nthough the proposed loss is based on magnitudes only, a constraint imposed by\nnoise prediction ensures that the loss enhances both magnitude and phase.\nExperimental results demonstrate that DCN trained with the proposed loss\nsubstantially outperforms other state-of-the-art approaches to causal and\nnon-causal speech enhancement.", "published": "2020-09-03 22:09:02", "link": "http://arxiv.org/abs/2009.01941v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Conversion by Cascading Automatic Speech Recognition and\n  Text-to-Speech Synthesis with Prosody Transfer", "abstract": "With the development of automatic speech recognition (ASR) and text-to-speech\nsynthesis (TTS) technique, it's intuitive to construct a voice conversion\nsystem by cascading an ASR and TTS system. In this paper, we present a ASR-TTS\nmethod for voice conversion, which used iFLYTEK ASR engine to transcribe the\nsource speech into text and a Transformer TTS model with WaveNet vocoder to\nsynthesize the converted speech from the decoded text. For the TTS model, we\nproposed to use a prosody code to describe the prosody information other than\ntext and speaker information contained in speech. A prosody encoder is used to\nextract the prosody code. During conversion, the source prosody is transferred\nto converted speech by conditioning the Transformer TTS model with its code.\nExperiments were conducted to demonstrate the effectiveness of our proposed\nmethod. Our system also obtained the best naturalness and similarity in the\nmono-lingual task of Voice Conversion Challenge 2020.", "published": "2020-09-03 06:31:34", "link": "http://arxiv.org/abs/2009.01475v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Detection of AI-Synthesized Speech Using Cepstral & Bispectral\n  Statistics", "abstract": "Digital technology has made possible unimaginable applications come true. It\nseems exciting to have a handful of tools for easy editing and manipulation,\nbut it raises alarming concerns that can propagate as speech clones,\nduplicates, or maybe deep fakes. Validating the authenticity of a speech is one\nof the primary problems of digital audio forensics. We propose an approach to\ndistinguish human speech from AI synthesized speech exploiting the Bi-spectral\nand Cepstral analysis. Higher-order statistics have less correlation for human\nspeech in comparison to a synthesized speech. Also, Cepstral analysis revealed\na durable power component in human speech that is missing for a synthesized\nspeech. We integrate both these analyses and propose a machine learning model\nto detect AI synthesized speech.", "published": "2020-09-03 21:29:41", "link": "http://arxiv.org/abs/2009.01934v2", "categories": ["cs.LG", "cs.MM", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
