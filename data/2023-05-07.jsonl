{"title": "OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual\n  Question Answering in Vietnamese", "abstract": "In recent years, visual question answering (VQA) has attracted attention from\nthe research community because of its highly potential applications (such as\nvirtual assistance on intelligent cars, assistant devices for blind people, or\ninformation retrieval from document images using natural language as queries)\nand challenge. The VQA task requires methods that have the ability to fuse the\ninformation from questions and images to produce appropriate answers. Neural\nvisual question answering models have achieved tremendous growth on large-scale\ndatasets which are mostly for resource-rich languages such as English. However,\navailable datasets narrow the VQA task as the answers selection task or answer\nclassification task. We argue that this form of VQA is far from human ability\nand eliminates the challenge of the answering aspect in the VQA task by just\nselecting answers rather than generating them. In this paper, we introduce the\nOpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the first\nlarge-scale dataset for VQA with open-ended answers in Vietnamese, consists of\n11,000+ images associated with 37,000+ question-answer pairs (QAs). Moreover,\nwe proposed FST, QuMLAG, and MLPAG which fuse information from images and\nanswers, then use these fused features to construct answers as humans\niteratively. Our proposed methods achieve results that are competitive with\nSOTA models such as SAAA, MCAN, LORA, and M4C. The dataset is available to\nencourage the research community to develop more generalized algorithms\nincluding transformers for low-resource languages such as Vietnamese.", "published": "2023-05-07 03:59:31", "link": "http://arxiv.org/abs/2305.04183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation on Word Embedding Offset Clustering as Relationship\n  Classification", "abstract": "Vector representations obtained from word embedding are the source of many\ngroundbreaking advances in natural language processing. They yield word\nrepresentations that are capable of capturing semantics and analogies of words\nwithin a text corpus. This study is an investigation in an attempt to elicit a\nvector representation of relationships between pairs of word vectors. We use\nsix pooling strategies to represent vector relationships. Different types of\nclustering models are applied to analyze which one correctly groups\nrelationship types. Subtraction pooling coupled with a centroid based\nclustering mechanism shows better performances in our experimental setup. This\nwork aims to provide directions for a word embedding based unsupervised method\nto identify the nature of a relationship represented by a pair of words.", "published": "2023-05-07 13:03:17", "link": "http://arxiv.org/abs/2305.04265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIORE: Leveraging High-order Interactions for Unified Entity Relation\n  Extraction", "abstract": "Entity relation extraction consists of two sub-tasks: entity recognition and\nrelation extraction. Existing methods either tackle these two tasks separately\nor unify them with word-by-word interactions. In this paper, we propose HIORE,\na new method for unified entity relation extraction. The key insight is to\nleverage the high-order interactions, i.e., the complex association among word\npairs, which contains richer information than the first-order word-by-word\ninteractions. For this purpose, we first devise a W-shape DNN (WNet) to capture\ncoarse-level high-order connections. Then, we build a heuristic high-order\ngraph and further calibrate the representations with a graph neural network\n(GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE\nachieves the state-of-the-art performance on relation extraction and an\nimprovement of 1.1~1.8 F1 points over the prior best unified model.", "published": "2023-05-07 14:57:42", "link": "http://arxiv.org/abs/2305.04297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Demonstration Retriever for In-Context Learning", "abstract": "In-context learning is a new learning paradigm where a language model\nconditions on a few input-output pairs (demonstrations) and a test input, and\ndirectly outputs the prediction. It has been shown highly dependent on the\nprovided demonstrations and thus promotes the research of demonstration\nretrieval: given a test input, relevant examples are retrieved from the\ntraining set to serve as informative demonstrations for in-context learning.\nWhile previous works focus on training task-specific retrievers for several\ntasks separately, these methods are often hard to transfer and scale on various\ntasks, and separately trained retrievers incur a lot of parameter storage and\ndeployment cost. In this paper, we propose Unified Demonstration Retriever\n(\\textbf{UDR}), a single model to retrieve demonstrations for a wide range of\ntasks. To train UDR, we cast various tasks' training signals into a unified\nlist-wise ranking formulation by language model's feedback. Then we propose a\nmulti-task list-wise ranking training framework, with an iterative mining\nstrategy to find high-quality candidates, which can help UDR fully incorporate\nvarious tasks' signals. Experiments on 30+ tasks across 13 task families and\nmultiple data domains show that UDR significantly outperforms baselines.\nFurther analyses show the effectiveness of each proposed component and UDR's\nstrong ability in various scenarios including different LMs (1.3B - 175B),\nunseen datasets, varying demonstration quantities, etc.", "published": "2023-05-07 16:07:11", "link": "http://arxiv.org/abs/2305.04320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question\n  Answering", "abstract": "Automatic fact verification has received significant attention recently.\nContemporary automatic fact-checking systems focus on estimating truthfulness\nusing numerical scores which are not human-interpretable. A human fact-checker\ngenerally follows several logical steps to verify a verisimilitude claim and\nconclude whether its truthful or a mere masquerade. Popular fact-checking\nwebsites follow a common structure for fact categorization such as half true,\nhalf false, false, pants on fire, etc. Therefore, it is necessary to have an\naspect-based (delineating which part(s) are true and which are false)\nexplainable system that can assist human fact-checkers in asking relevant\nquestions related to a fact, which can then be validated separately to reach a\nfinal verdict. In this paper, we propose a 5W framework (who, what, when,\nwhere, and why) for question-answer-based fact explainability. To that end, we\npresent a semi-automatically generated dataset called FACTIFY-5WQA, which\nconsists of 391, 041 facts along with relevant 5W QAs - underscoring our major\ncontribution to this paper. A semantic role labeling system has been utilized\nto locate 5Ws, which generates QA pairs for claims using a masked language\nmodel. Finally, we report a baseline QA system to automatically locate those\nanswers from evidence documents, which can serve as a baseline for future\nresearch in the field. Lastly, we propose a robust fact verification system\nthat takes paraphrased claims and automatically validates them. The dataset and\nthe baseline model are available at https: //github.com/ankuranii/acl-5W-QA", "published": "2023-05-07 16:52:21", "link": "http://arxiv.org/abs/2305.04329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empowering Language Model with Guided Knowledge Fusion for Biomedical\n  Document Re-ranking", "abstract": "Pre-trained language models (PLMs) have proven to be effective for document\nre-ranking task. However, they lack the ability to fully interpret the\nsemantics of biomedical and health-care queries and often rely on simplistic\npatterns for retrieving documents. To address this challenge, we propose an\napproach that integrates knowledge and the PLMs to guide the model toward\neffectively capturing information from external sources and retrieving the\ncorrect documents. We performed comprehensive experiments on two biomedical and\nopen-domain datasets that show that our approach significantly improves vanilla\nPLMs and other existing approaches for document re-ranking task.", "published": "2023-05-07 17:45:47", "link": "http://arxiv.org/abs/2305.04344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LatinCy: Synthetic Trained Pipelines for Latin NLP", "abstract": "This paper introduces LatinCy, a set of trained general purpose\nLatin-language \"core\" pipelines for use with the spaCy natural language\nprocessing framework. The models are trained on a large amount of available\nLatin data, including all five of the Latin Universal Dependency treebanks,\nwhich have been preprocessed to be compatible with each other. The result is a\nset of general models for Latin with good performance on a number of natural\nlanguage processing tasks (e.g. the top-performing model yields POS tagging,\n97.41% accuracy; lemmatization, 94.66% accuracy; morphological tagging 92.76%\naccuracy). The paper describes the model training, including its training data\nand parameterization, and presents the advantages to Latin-language researchers\nof having a spaCy model available for NLP work.", "published": "2023-05-07 19:59:01", "link": "http://arxiv.org/abs/2305.04365v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in\n  Vietnamese", "abstract": "Image Captioning is one of the vision-language tasks that still interest the\nresearch community worldwide in the 2020s. MS-COCO Caption benchmark is\ncommonly used to evaluate the performance of advanced captioning models,\nalthough it was published in 2015. Recent captioning models trained on the\nMS-COCO Caption dataset only have good performance in language patterns of\nEnglish; they do not have such good performance in contexts captured in Vietnam\nor fluently caption images using Vietnamese. To contribute to the low-resources\nresearch community as in Vietnam, we introduce a novel image captioning dataset\nin Vietnamese, the Open-domain Vietnamese Image Captioning dataset\n(UIT-OpenViIC). The introduced dataset includes complex scenes captured in\nVietnam and manually annotated by Vietnamese under strict rules and\nsupervision. In this paper, we present in more detail the dataset creation\nprocess. From preliminary analysis, we show that our dataset is challenging to\nrecent state-of-the-art (SOTA) Transformer-based baselines, which performed\nwell on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC\nhas room to grow, which can be one of the standard benchmarks in Vietnamese for\nthe research community to evaluate their captioning models. Furthermore, we\npresent a CAMO approach that effectively enhances the image representation\nability by a multi-level encoder output fusion mechanism, which helps improve\nthe quality of generated captions compared to previous captioning models.", "published": "2023-05-07 02:48:47", "link": "http://arxiv.org/abs/2305.04166v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MIReAD: Simple Method for Learning High-quality Representations from\n  Scientific Documents", "abstract": "Learning semantically meaningful representations from scientific documents\ncan facilitate academic literature search and improve performance of\nrecommendation systems. Pre-trained language models have been shown to learn\nrich textual representations, yet they cannot provide powerful document-level\nrepresentations for scientific articles. We propose MIReAD, a simple method\nthat learns high-quality representations of scientific papers by fine-tuning\ntransformer model to predict the target journal class based on the abstract. We\ntrain MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000\njournal classes. We show that MIReAD produces representations that can be used\nfor similar papers retrieval, topic categorization and literature search. Our\nproposed approach outperforms six existing models for representation learning\non scientific documents across four evaluation standards.", "published": "2023-05-07 03:29:55", "link": "http://arxiv.org/abs/2305.04177v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shall We Trust All Relational Tuples by Open Information Extraction? A\n  Study on Speculation Detection", "abstract": "Open Information Extraction (OIE) aims to extract factual relational tuples\nfrom open-domain sentences. Downstream tasks use the extracted OIE tuples as\nfacts, without examining the certainty of these facts. However,\nuncertainty/speculation is a common linguistic phenomenon. Existing studies on\nspeculation detection are defined at sentence level, but even if a sentence is\ndetermined to be speculative, not all tuples extracted from it may be\nspeculative. In this paper, we propose to study speculations in OIE and aim to\ndetermine whether an extracted tuple is speculative. We formally define the\nresearch problem of tuple-level speculation detection and conduct a detailed\ndata analysis on the LSOIE dataset which contains labels for speculative\ntuples. Lastly, we propose a baseline model OIE-Spec for this new research\ntask.", "published": "2023-05-07 03:47:05", "link": "http://arxiv.org/abs/2305.04181v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Retrieval for Motion and Text via DopTriple Loss", "abstract": "Cross-modal retrieval of image-text and video-text is a prominent research\narea in computer vision and natural language processing. However, there has\nbeen insufficient attention given to cross-modal retrieval between human motion\nand text, despite its wide-ranging applicability. To address this gap, we\nutilize a concise yet effective dual-unimodal transformer encoder for tackling\nthis task. Recognizing that overlapping atomic actions in different human\nmotion sequences can lead to semantic conflicts between samples, we explore a\nnovel triplet loss function called DropTriple Loss. This loss function discards\nfalse negative samples from the negative sample set and focuses on mining\nremaining genuinely hard negative samples for triplet training, thereby\nreducing violations they cause. We evaluate our model and approach on the\nHumanML3D and KIT Motion-Language datasets. On the latest HumanML3D dataset, we\nachieve a recall of 62.9% for motion retrieval and 71.5% for text retrieval\n(both based on R@10). The source code for our approach is publicly available at\nhttps://github.com/eanson023/rehamot.", "published": "2023-05-07 05:40:48", "link": "http://arxiv.org/abs/2305.04195v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing\n  Important Tokens", "abstract": "Transformers are central in modern natural language processing and computer\nvision applications. Despite recent works devoted to reducing the quadratic\ncost of such models (as a function of the sequence length), dealing with ultra\nlong sequences (e.g., with more than 16K tokens) remains challenging.\nApplications such as answering questions based on a book or summarizing a\nscientific article are inefficient or infeasible. Here, we propose to\nsignificantly improve the efficiency of Transformers for ultra long sequences,\nby compressing the sequence into a much smaller representation at each layer.\nSpecifically, by exploiting the fact that in many tasks, only a small subset of\nspecial tokens (we call VIP-tokens) are most relevant to the final prediction,\nwe propose a VIP-token centric compression (VCC) scheme which selectively\ncompresses the sequence based on their impact on approximating the\nrepresentation of the VIP-tokens. Compared with competitive baselines, our\nalgorithm is not only efficient (achieving more than $3\\times$ efficiency gain\ncompared to baselines on 4K and 16K lengths), but also offers\ncompetitive/better performance on a large number of tasks. Further, we show\nthat our algorithm scales to 128K tokens (or more) while consistently offering\naccuracy improvement.", "published": "2023-05-07 10:32:18", "link": "http://arxiv.org/abs/2305.04241v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Laziness Is a Virtue When It Comes to Compositionality in Neural\n  Semantic Parsing", "abstract": "Nearly all general-purpose neural semantic parsers generate logical forms in\na strictly top-down autoregressive fashion. Though such systems have achieved\nimpressive results across a variety of datasets and domains, recent works have\ncalled into question whether they are ultimately limited in their ability to\ncompositionally generalize. In this work, we approach semantic parsing from,\nquite literally, the opposite direction; that is, we introduce a neural\nsemantic parsing generation method that constructs logical forms from the\nbottom up, beginning from the logical form's leaves. The system we introduce is\nlazy in that it incrementally builds up a set of potential semantic parses, but\nonly expands and processes the most promising candidate parses at each\ngeneration step. Such a parsimonious expansion scheme allows the system to\nmaintain an arbitrarily large set of parse hypotheses that are never realized\nand thus incur minimal computational overhead. We evaluate our approach on\ncompositional generalization; specifically, on the challenging CFQ dataset and\nthree Text-to-SQL datasets where we show that our novel, bottom-up semantic\nparsing technique outperforms general-purpose semantic parsers while also being\ncompetitive with comparable neural parsers that have been designed for each\ntask.", "published": "2023-05-07 17:53:08", "link": "http://arxiv.org/abs/2305.04346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stanford MLab at SemEval-2023 Task 10: Exploring GloVe- and\n  Transformer-Based Methods for the Explainable Detection of Online Sexism", "abstract": "In this paper, we discuss the methods we applied at SemEval-2023 Task 10:\nTowards the Explainable Detection of Online Sexism. Given an input text, we\nperform three classification tasks to predict whether the text is sexist and\nclassify the sexist text into subcategories in order to provide an additional\nexplanation as to why the text is sexist. We explored many different types of\nmodels, including GloVe embeddings as the baseline approach, transformer-based\ndeep learning models like BERT, RoBERTa, and DeBERTa, ensemble models, and\nmodel blending. We explored various data cleaning and augmentation methods to\nimprove model performance. Pre-training transformer models yielded significant\nimprovements in performance, and ensembles and blending slightly improved\nrobustness in the F1 score.", "published": "2023-05-07 18:58:54", "link": "http://arxiv.org/abs/2305.04356v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Don't Always Say What They Think: Unfaithful\n  Explanations in Chain-of-Thought Prompting", "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by\nproducing step-by-step reasoning before giving a final output, often referred\nto as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT\nexplanations as the LLM's process for solving a task. This level of\ntransparency into LLMs' predictions would yield significant safety benefits.\nHowever, we find that CoT explanations can systematically misrepresent the true\nreason for a model's prediction. We demonstrate that CoT explanations can be\nheavily influenced by adding biasing features to model inputs--e.g., by\nreordering the multiple-choice options in a few-shot prompt to make the answer\nalways \"(A)\"--which models systematically fail to mention in their\nexplanations. When we bias models toward incorrect answers, they frequently\ngenerate CoT explanations rationalizing those answers. This causes accuracy to\ndrop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing\nwith GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,\nmodel explanations justify giving answers in line with stereotypes without\nmentioning the influence of these social biases. Our findings indicate that CoT\nexplanations can be plausible yet misleading, which risks increasing our trust\nin LLMs without guaranteeing their safety. Building more transparent and\nexplainable systems will require either improving CoT faithfulness through\ntargeted efforts or abandoning CoT in favor of alternative methods.", "published": "2023-05-07 22:44:25", "link": "http://arxiv.org/abs/2305.04388v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating\n  Multi-Modalities as Foreign Languages", "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities.\nGPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities\nbeyond previous visual language models. We attribute this to the use of more\nadvanced LLMs compared with previous multimodal models. Unfortunately, the\nmodel architecture and training strategies of GPT-4 are unknown. To endow LLMs\nwith multimodal capabilities, we propose X-LLM, which converts Multi-modalities\n(images, speech, videos) into foreign languages using X2L interfaces and inputs\nthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple\nfrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''\ndenotes multi-modalities such as image, speech, and videos, and ``L'' denotes\nlanguages. X-LLM's training consists of three stages: (1) Converting Multimodal\nInformation: The first stage trains each X2L interface to align with its\nrespective single-modal encoder separately to convert multimodal information\ninto languages. (2) Aligning X2L representations with the LLM: single-modal\nencoders are aligned with the LLM through X2L interfaces independently. (3)\nIntegrating multiple modalities: all single-modal encoders are aligned with the\nLLM through X2L interfaces to integrate multimodal capabilities into the LLM.\nOur experiments show that X-LLM demonstrates impressive multimodel chat\nabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen\nimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 on\na synthetic multimodal instruction-following dataset. And we also conduct\nquantitative tests on using LLM for ASR and multimodal ASR, hoping to promote\nthe era of LLM-based speech recognition.", "published": "2023-05-07 02:25:42", "link": "http://arxiv.org/abs/2305.04160v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Synthetic Targets for Machine Translation", "abstract": "In this work, we provide a recipe for training machine translation models in\na limited resource setting by leveraging synthetic target data generated using\na large pre-trained model. We show that consistently across different\nbenchmarks in bilingual, multilingual, and speech translation setups, training\nmodels on synthetic targets outperforms training on the actual ground-truth\ndata. This performance gap grows bigger with increasing limits on the amount of\navailable resources in the form of the size of the dataset and the number of\nparameters in the model. We also provide preliminary analysis into whether this\nboost in performance is linked to ease of optimization or more deterministic\nnature of the predictions, and whether this paradigm leads to better\nout-of-distribution performance across different testing domains.", "published": "2023-05-07 07:42:22", "link": "http://arxiv.org/abs/2305.06155v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable multimodal sentiment analysis based on textual modality\n  descriptions by using large-scale language models", "abstract": "Multimodal sentiment analysis is an important area for understanding the\nuser's internal states. Deep learning methods were effective, but the problem\nof poor interpretability has gradually gained attention. Previous works have\nattempted to use attention weights or vector distributions to provide\ninterpretability. However, their explanations were not intuitive and can be\ninfluenced by different trained models. This study proposed a novel approach to\nprovide interpretability by converting nonverbal modalities into text\ndescriptions and by using large-scale language models for sentiment\npredictions. This provides an intuitive approach to directly interpret what\nmodels depend on with respect to making decisions from input texts, thus\nsignificantly improving interpretability. Specifically, we convert descriptions\nbased on two feature patterns for the audio modality and discrete action units\nfor the facial modality. Experimental results on two sentiment analysis tasks\ndemonstrated that the proposed approach maintained, or even improved\neffectiveness for sentiment analysis compared to baselines using conventional\nfeatures, with the highest improvement of 2.49% on the F1 score. The results\nalso showed that multimodal descriptions have similar characteristics on fusing\nmodalities as those of conventional fusion methods. The results demonstrated\nthat the proposed approach is interpretable and effective for multimodal\nsentiment analysis.", "published": "2023-05-07 06:48:06", "link": "http://arxiv.org/abs/2305.06162v3", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Learning Robust Self-attention Features for Speech Emotion Recognition\n  with Label-adaptive Mixup", "abstract": "Speech Emotion Recognition (SER) is to recognize human emotions in a natural\nverbal interaction scenario with machines, which is considered as a challenging\nproblem due to the ambiguous human emotions. Despite the recent progress in\nSER, state-of-the-art models struggle to achieve a satisfactory performance. We\npropose a self-attention based method with combined use of label-adaptive mixup\nand center loss. By adapting label probabilities in mixup and fitting center\nloss to the mixup training scheme, our proposed method achieves a superior\nperformance to the state-of-the-art methods.", "published": "2023-05-07 15:10:59", "link": "http://arxiv.org/abs/2305.06273v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Extracting Blockchain Concepts from Text", "abstract": "Blockchains provide a mechanism through which mutually distrustful remote\nparties can reach consensus on the state of a ledger of information. With the\ngreat acceleration with which this space is developed, the demand for those\nseeking to learn about blockchain also grows. Being a technical subject, it can\nbe quite intimidating to start learning. For this reason, the main objective of\nthis project was to apply machine learning models to extract information from\nwhitepapers and academic articles focused on the blockchain area to organize\nthis information and aid users to navigate the space.", "published": "2023-05-07 00:16:30", "link": "http://arxiv.org/abs/2305.10408v1", "categories": ["cs.IR", "cs.CL", "cs.CR"], "primary_category": "cs.IR"}
{"title": "Generative Pretrained Autoregressive Transformer Graph Neural Network\n  applied to the Analysis and Discovery of Novel Proteins", "abstract": "We report a flexible language-model based deep learning strategy, applied\nhere to solve complex forward and inverse problems in protein modeling, based\non an attention neural network that integrates transformer and graph\nconvolutional architectures in a causal multi-headed graph mechanism, to\nrealize a generative pretrained model. The model is applied to predict\nsecondary structure content (per-residue level and overall content), protein\nsolubility, and sequencing tasks. Further trained on inverse tasks, the model\nis rendered capable of designing proteins with these properties as target\nfeatures. The model is formulated as a general framework, completely\nprompt-based, and can be adapted for a variety of downstream tasks. We find\nthat adding additional tasks yields emergent synergies that the model exploits\nin improving overall performance, beyond what would be possible by training a\nmodel on each dataset alone. Case studies are presented to validate the method,\nyielding protein designs specifically focused on structural proteins, but also\nexploring the applicability in the design of soluble, antimicrobial\nbiomaterials. While our model is trained to ultimately perform 8 distinct\ntasks, with available datasets it can be extended to solve additional problems.\nIn a broader sense, this work illustrates a form of multiscale modeling that\nrelates a set of ultimate building blocks (here, byte-level utf8 characters\nthat define the nature of the physical system at hand) to complex output. This\nmateriomic scheme captures complex emergent relationships between universal\nbuilding block and resulting properties via a synergizing learning capacity to\nexpress a set of potentialities embedded in the knowledge used in training, via\nthe interplay of universality and diversity.", "published": "2023-05-07 12:30:24", "link": "http://arxiv.org/abs/2305.04934v2", "categories": ["q-bio.BM", "cond-mat.dis-nn", "cond-mat.soft", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Lookahead When It Matters: Adaptive Non-causal Transformers for\n  Streaming Neural Transducers", "abstract": "Streaming speech recognition architectures are employed for low-latency,\nreal-time applications. Such architectures are often characterized by their\ncausality. Causal architectures emit tokens at each frame, relying only on\ncurrent and past signal, while non-causal models are exposed to a window of\nfuture frames at each step to increase predictive accuracy. This dichotomy\namounts to a trade-off for real-time Automatic Speech Recognition (ASR) system\ndesign: profit from the low-latency benefit of strictly-causal architectures\nwhile accepting predictive performance limitations, or realize the modeling\nbenefits of future-context models accompanied by their higher latency penalty.\nIn this work, we relax the constraints of this choice and present the Adaptive\nNon-Causal Attention Transducer (ANCAT). Our architecture is non-causal in the\ntraditional sense, but executes in a low-latency, streaming manner by\ndynamically choosing when to rely on future context and to what degree within\nthe audio stream. The resulting mechanism, when coupled with our novel\nregularization algorithms, delivers comparable accuracy to non-causal\nconfigurations while improving significantly upon latency, closing the gap with\ntheir causal counterparts. We showcase our design experimentally by reporting\ncomparative ASR task results with measures of accuracy and latency on both\npublicly accessible and production-scale, voice-assistant datasets.", "published": "2023-05-07 01:43:32", "link": "http://arxiv.org/abs/2305.04159v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
