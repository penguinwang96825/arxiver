{"title": "Robustly Optimized and Distilled Training for Natural Language\n  Understanding", "abstract": "In this paper, we explore multi-task learning (MTL) as a second pretraining\nstep to learn enhanced universal language representation for transformer\nlanguage models. We use the MTL enhanced representation across several natural\nlanguage understanding tasks to improve performance and generalization.\nMoreover, we incorporate knowledge distillation (KD) in MTL to further boost\nperformance and devise a KD variant that learns effectively from multiple\nteachers. By combining MTL and KD, we propose Robustly Optimized and Distilled\n(ROaD) modeling framework. We use ROaD together with the ELECTRA model to\nobtain state-of-the-art results for machine reading comprehension and natural\nlanguage inference.", "published": "2021-03-16 02:35:22", "link": "http://arxiv.org/abs/2103.08809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gumbel-Attention for Multi-modal Machine Translation", "abstract": "Multi-modal machine translation (MMT) improves translation quality by\nintroducing visual information. However, the existing MMT model ignores the\nproblem that the image will bring information irrelevant to the text, causing\nmuch noise to the model and affecting the translation quality. This paper\nproposes a novel Gumbel-Attention for multi-modal machine translation, which\nselects the text-related parts of the image features. Specifically, different\nfrom the previous attention-based method, we first use a differentiable method\nto select the image information and automatically remove the useless parts of\nthe image features. Experiments prove that our method retains the image\nfeatures related to the text, and the remaining parts help the MMT model\ngenerates better translations.", "published": "2021-03-16 05:44:01", "link": "http://arxiv.org/abs/2103.08862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coordinate Constructions in English Enhanced Universal Dependencies:\n  Analysis and Computational Modeling", "abstract": "In this paper, we address the representation of coordinate constructions in\nEnhanced Universal Dependencies (UD), where relevant dependency links are\npropagated from conjunction heads to other conjuncts. English treebanks for\nenhanced UD have been created from gold basic dependencies using a heuristic\nrule-based converter, which propagates only core arguments. With the aim of\ndetermining which set of links should be propagated from a semantic\nperspective, we create a large-scale dataset of manually edited syntax graphs.\nWe identify several systematic errors in the original data, and propose to also\npropagate adjuncts. We observe high inter-annotator agreement for this semantic\nannotation task. Using our new manually verified dataset, we perform the first\nprincipled comparison of rule-based and (partially novel) machine-learning\nbased methods for conjunction propagation for English. We show that learning\npropagation rules is more effective than hand-designing heuristic rules. When\nusing automatic parses, our neural graph-parser based edge predictor\noutperforms the currently predominant pipelinesusing a basic-layer tree parser\nplus converters.", "published": "2021-03-16 10:24:27", "link": "http://arxiv.org/abs/2103.08955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Adapters in Pretrained Language Models for AMR-to-text\n  Generation", "abstract": "Pretrained language models (PLM) have recently advanced graph-to-text\ngeneration, where the input graph is linearized into a sequence and fed into\nthe PLM to obtain its representation. However, efficiently encoding the graph\nstructure in PLMs is challenging because such models were pretrained on natural\nlanguage, and modeling structured data may lead to catastrophic forgetting of\ndistributional knowledge. In this paper, we propose StructAdapt, an adapter\nmethod to encode graph structure into PLMs. Contrary to prior work, StructAdapt\neffectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we incorporate task-specific knowledge while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using StructAdapt, outperforming the state of the art\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.", "published": "2021-03-16 15:06:50", "link": "http://arxiv.org/abs/2103.09120v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Intruder, no Validity: Evaluation Criteria for Privacy-Preserving\n  Text Anonymization", "abstract": "For sensitive text data to be shared among NLP researchers and practitioners,\nshared documents need to comply with data protection and privacy laws. There is\nhence a growing interest in automated approaches for text anonymization.\nHowever, measuring such methods' performance is challenging: missing a single\nidentifying attribute can reveal an individual's identity. In this paper, we\ndraw attention to this problem and argue that researchers and practitioners\ndeveloping automated text anonymization systems should carefully assess whether\ntheir evaluation methods truly reflect the system's ability to protect\nindividuals from being re-identified. We then propose TILD, a set of evaluation\ncriteria that comprises an anonymization method's technical performance, the\ninformation loss resulting from its anonymization, and the human ability to\nde-anonymize redacted documents. These criteria may facilitate progress towards\na standardized way for measuring anonymization performance.", "published": "2021-03-16 18:18:29", "link": "http://arxiv.org/abs/2103.09263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Task Instance Representation Interactions and Label Dependencies\n  for Joint Information Extraction with Graph Convolutional Networks", "abstract": "Existing works on information extraction (IE) have mainly solved the four\nmain tasks separately (entity mention recognition, relation extraction, event\ntrigger detection, and argument extraction), thus failing to benefit from\ninter-dependencies between tasks. This paper presents a novel deep learning\nmodel to simultaneously solve the four tasks of IE in a single model (called\nFourIE). Compared to few prior work on jointly performing four IE tasks, FourIE\nfeatures two novel contributions to capture inter-dependencies between tasks.\nFirst, at the representation level, we introduce an interaction graph between\ninstances of the four tasks that is used to enrich the prediction\nrepresentation for one instance with those from related instances of other\ntasks. Second, at the label level, we propose a dependency graph for the\ninformation types in the four IE tasks that captures the connections between\nthe types expressed in an input sentence. A new regularization mechanism is\nintroduced to enforce the consistency between the golden and predicted type\ndependency graphs to improve representation learning. We show that the proposed\nmodel achieves the state-of-the-art performance for joint IE on both\nmonolingual and multilingual learning settings with three different languages.", "published": "2021-03-16 21:23:50", "link": "http://arxiv.org/abs/2103.09330v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time\n  Image-Text Retrieval", "abstract": "Multimodal pre-training has propelled great advancement in\nvision-and-language research. These large-scale pre-trained models, although\nsuccessful, fatefully suffer from slow inference speed due to enormous\ncomputation cost mainly from cross-modal attention in Transformer architecture.\nWhen applied to real-life applications, such latency and computation demand\nseverely deter the practical use of pre-trained models. In this paper, we study\nImage-text retrieval (ITR), the most mature scenario of V+L application, which\nhas been widely studied even prior to the emergence of recent pre-trained\nmodels. We propose a simple yet highly effective approach, LightningDOT that\naccelerates the inference time of ITR by thousands of times, without\nsacrificing accuracy. LightningDOT removes the time-consuming cross-modal\nattention by pre-training on three novel learning objectives, extracting\nfeature indexes offline, and employing instant dot-product matching with\nfurther re-ranking, which significantly speeds up retrieval process. In fact,\nLightningDOT achieves new state of the art across multiple ITR benchmarks such\nas Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that\nconsume 1000x magnitude of computational hours. Code and pre-training\ncheckpoints are available at https://github.com/intersun/LightningDOT.", "published": "2021-03-16 00:35:28", "link": "http://arxiv.org/abs/2103.08784v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual\n  Transfer of Vision-Language Models", "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language\nmodels. Specifically, we focus on multilingual text-to-video search and propose\na Transformer-based model that learns contextualized multilingual multimodal\nembeddings. Under a zero-shot setting, we empirically demonstrate that\nperformance degrades significantly when we query the multilingual text-video\nmodel with non-English sentences. To address this problem, we introduce a\nmultilingual multimodal pre-training strategy, and collect a new multilingual\ninstructional video dataset (MultiHowTo100M) for pre-training. Experiments on\nVTT show that our method significantly improves video search in non-English\nlanguages without additional annotations. Furthermore, when multilingual\nannotations are available, our method outperforms recent baselines by a large\nmargin in multilingual text-to-video search on VTT and VATEX; as well as in\nmultilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is\navailable at http://github.com/berniebear/Multi-HT100M.", "published": "2021-03-16 04:37:40", "link": "http://arxiv.org/abs/2103.08849v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Automatic Intent-Slot Induction for Dialogue Systems", "abstract": "Automatically and accurately identifying user intents and filling the\nassociated slots from their spoken language are critical to the success of\ndialogue systems. Traditional methods require manually defining the\nDOMAIN-INTENT-SLOT schema and asking many domain experts to annotate the\ncorresponding utterances, upon which neural models are trained. This procedure\nbrings the challenges of information sharing hindering, out-of-schema, or data\nsparsity in open-domain dialogue systems. To tackle these challenges, we\nexplore a new task of {\\em automatic intent-slot induction} and propose a novel\ndomain-independent tool. That is, we design a coarse-to-fine three-step\nprocedure including Role-labeling, Concept-mining, And Pattern-mining (RCAP):\n(1) role-labeling: extracting keyphrases from users' utterances and classifying\nthem into a quadruple of coarsely-defined intent-roles via sequence labeling;\n(2) concept-mining: clustering the extracted intent-role mentions and naming\nthem into abstract fine-grained concepts; (3) pattern-mining: applying the\nApriori algorithm to mine intent-role patterns and automatically inferring the\nintent-slot using these coarse-grained intent-role labels and fine-grained\nconcepts. Empirical evaluations on both real-world in-domain and out-of-domain\ndatasets show that: (1) our RCAP can generate satisfactory SLU schema and\noutperforms the state-of-the-art supervised learning method; (2) our RCAP can\nbe directly applied to out-of-domain datasets and gain at least 76\\%\nimprovement of F1-score on intent detection and 41\\% improvement of F1-score on\nslot filling; (3) our RCAP exhibits its power in generic intent-slot\nextractions with less manual effort, which opens pathways for schema induction\non new domains and unseen intent-slot discovery for generalizable dialogue\nsystems.", "published": "2021-03-16 07:21:31", "link": "http://arxiv.org/abs/2103.08886v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "KGSynNet: A Novel Entity Synonyms Discovery Framework with Knowledge\n  Graph", "abstract": "Entity synonyms discovery is crucial for entity-leveraging applications.\nHowever, existing studies suffer from several critical issues: (1) the input\nmentions may be out-of-vocabulary (OOV) and may come from a different semantic\nspace of the entities; (2) the connection between mentions and entities may be\nhidden and cannot be established by surface matching; and (3) some entities\nrarely appear due to the long-tail effect. To tackle these challenges, we\nfacilitate knowledge graphs and propose a novel entity synonyms discovery\nframework, named \\emph{KGSynNet}. Specifically, we pre-train subword embeddings\nfor mentions and entities using a large-scale domain-specific corpus while\nlearning the knowledge embeddings of entities via a joint TransC-TransE model.\nMore importantly, to obtain a comprehensive representation of entities, we\nemploy a specifically designed \\emph{fusion gate} to adaptively absorb the\nentities' knowledge information into their semantic features. We conduct\nextensive experiments to demonstrate the effectiveness of our \\emph{KGSynNet}\nin leveraging the knowledge graph. The experimental results show that the\n\\emph{KGSynNet} improves the state-of-the-art methods by 14.7\\% in terms of\nhits@3 in the offline evaluation and outperforms the BERT model by 8.3\\% in the\npositive feedback rate of an online A/B test on the entity linking module of a\nquestion answering system.", "published": "2021-03-16 07:32:33", "link": "http://arxiv.org/abs/2103.08893v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Covid-19 Discourse on Twitter: How the Topics, Sentiments, Subjectivity,\n  and Figurative Frames Changed Over Time", "abstract": "The words we use to talk about the current epidemiological crisis on social\nmedia can inform us on how we are conceptualizing the pandemic and how we are\nreacting to its development. This paper provides an extensive explorative\nanalysis of how the discourse about Covid-19 reported on Twitter changes\nthrough time, focusing on the first wave of this pandemic. Based on an\nextensive corpus of tweets (produced between 20th March and 1st July 2020)\nfirst we show how the topics associated with the development of the pandemic\nchanged through time, using topic modeling. Second, we show how the sentiment\npolarity of the language used in the tweets changed from a relatively positive\nvalence during the first lockdown, toward a more negative valence in\ncorrespondence with the reopening. Third we show how the average subjectivity\nof the tweets increased linearly and fourth, how the popular and frequently\nused figurative frame of WAR changed when real riots and fights entered the\ndiscourse.", "published": "2021-03-16 10:22:39", "link": "http://arxiv.org/abs/2103.08952v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Multilingual African Embedding for FAQ Chatbots", "abstract": "Searching for an available, reliable, official, and understandable\ninformation is not a trivial task due to scattered information across the\ninternet, and the availability lack of governmental communication channels\ncommunicating with African dialects and languages. In this paper, we introduce\nan Artificial Intelligence Powered chatbot for crisis communication that would\nbe omnichannel, multilingual and multi dialectal. We present our work on\nmodified StarSpace embedding tailored for African dialects for the\nquestion-answering task along with the architecture of the proposed chatbot\nsystem and a description of the different layers. English, French, Arabic,\nTunisian, Igbo,Yor\\`ub\\'a, and Hausa are used as languages and dialects.\nQuantitative and qualitative evaluation results are obtained for our real\ndeployed Covid-19 chatbot. Results show that users are satisfied and the\nconversation with the chatbot is meeting customer needs.", "published": "2021-03-16 16:36:40", "link": "http://arxiv.org/abs/2103.09185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Convolutional Network for Swahili News Classification", "abstract": "This work empirically demonstrates the ability of Text Graph Convolutional\nNetwork (Text GCN) to outperform traditional natural language processing\nbenchmarks for the task of semi-supervised Swahili news classification. In\nparticular, we focus our experimentation on the sparsely-labelled\nsemi-supervised context which is representative of the practical constraints\nfacing low-resourced African languages. We follow up on this result by\nintroducing a variant of the Text GCN model which utilises a bag of words\nembedding rather than a naive one-hot encoding to reduce the memory footprint\nof Text GCN whilst demonstrating similar predictive performance.", "published": "2021-03-16 21:03:47", "link": "http://arxiv.org/abs/2103.09325v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Predicting the Factuality and the Bias of News Media", "abstract": "The present level of proliferation of fake, biased, and propagandistic\ncontent online has made it impossible to fact-check every single suspicious\nclaim or article, either manually or automatically. Thus, many researchers are\nshifting their attention to higher granularity, aiming to profile entire news\noutlets, which makes it possible to detect likely \"fake news\" the moment it is\npublished, by simply checking the reliability of its source. Source factuality\nis also an important element of systems for automatic fact-checking and \"fake\nnews\" detection, as they need to assess the reliability of the evidence they\nretrieve online. Political bias detection, which in the Western political\nlandscape is about predicting left-center-right bias, is an equally important\ntopic, which has experienced a similar shift towards profiling entire news\noutlets. Moreover, there is a clear connection between the two, as highly\nbiased media are less likely to be factual; yet, the two problems have been\naddressed separately. In this survey, we review the state of the art on media\nprofiling for factuality and bias, arguing for the need to model them jointly.\nWe further discuss interesting recent advances in using different information\nsources and modalities, which go beyond the text of the articles the target\nnews outlet has published. Finally, we discuss current challenges and outline\nfuture research directions.", "published": "2021-03-16 11:11:54", "link": "http://arxiv.org/abs/2103.12506v1", "categories": ["cs.SI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.SI"}
{"title": "dictNN: A Dictionary-Enhanced CNN Approach for Classifying Hate Speech\n  on Twitter", "abstract": "Hate speech on social media is a growing concern, and automated methods have\nso far been sub-par at reliably detecting it. A major challenge lies in the\npotentially evasive nature of hate speech due to the ambiguity and fast\nevolution of natural language. To tackle this, we introduce a vectorisation\nbased on a crowd-sourced and continuously updated dictionary of hate words and\npropose fusing this approach with standard word embedding in order to improve\nthe classification performance of a CNN model. To train and test our model we\nuse a merge of two established datasets (110,748 tweets in total). By adding\nthe dictionary-enhanced input, we are able to increase the CNN model's\npredictive power and increase the F1 macro score by seven percentage points.", "published": "2021-03-16 00:27:33", "link": "http://arxiv.org/abs/2103.08780v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Development of ASR in African Languages using Self Supervised\n  Speech Representation Learning", "abstract": "This paper describes the results of an informal collaboration launched during\nthe African Master of Machine Intelligence (AMMI) in June 2020. After a series\nof lectures and labs on speech data collection using mobile applications and on\nself-supervised representation learning from speech, a small group of students\nand the lecturer continued working on automatic speech recognition (ASR)\nproject for three languages: Wolof, Ga, and Somali. This paper describes how\ndata was collected and ASR systems developed with a small amount (1h) of\ntranscribed speech as training data. In these low resource conditions,\npre-training a model on large amounts of raw speech was fundamental for the\nefficiency of ASR systems developed.", "published": "2021-03-16 11:37:03", "link": "http://arxiv.org/abs/2103.08993v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Speaker Verification with Target Speaker Enhancement", "abstract": "This paper proposes the target speaker enhancement based speaker verification\nnetwork (TASE-SVNet), an all neural model that couples target speaker\nenhancement and speaker embedding extraction for robust speaker verification\n(SV). Specifically, an enrollment speaker conditioned speech enhancement module\nis employed as the front-end for extracting target speaker from its mixture\nwith interfering speakers and environmental noises. Compared with the\nconventional target speaker enhancement models, nontarget speaker/interference\nsuppression should draw additional attention for SV. Therefore, an effective\nnontarget speaker sampling strategy is explored. To improve speaker embedding\nextraction with a light-weighted model, a teacher-student (T/S) training is\nproposed to distill speaker discriminative information from large models to\nsmall models. Iterative inference is investigated to address the noisy speaker\nenrollment problem. We evaluate the proposed method on two SV tasks, i.e., one\nheavily overlapped speech and the other one with comprehensive noise types in\nvehicle environments. Experiments show significant and consistent improvements\nin Equal Error Rate (EER) over the state-of-the-art baselines.", "published": "2021-03-16 00:28:47", "link": "http://arxiv.org/abs/2103.08781v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AEC in a NetShell: On Target and Topology Choices for FCRN Acoustic Echo\n  Cancellation", "abstract": "Acoustic echo cancellation (AEC) algorithms have a long-term steady role in\nsignal processing, with approaches improving the performance of applications\nsuch as automotive hands-free systems, smart home and loudspeaker devices, or\nweb conference systems. Just recently, very first deep neural network\n(DNN)-based approaches were proposed with a DNN for joint AEC and residual echo\nsuppression (RES)/noise reduction, showing significant improvements in terms of\necho suppression performance. Noise reduction algorithms, on the other hand,\nhave enjoyed already a lot of attention with regard to DNN approaches, with the\nfully convolutional recurrent network (FCRN) architecture being among state of\nthe art topologies. The recently published impressive echo cancellation\nperformance of joint AEC/RES DNNs, however, so far came along with an\nundeniable impairment of speech quality. In this work we will heal this issue\nand significantly improve the near-end speech component quality over existing\napproaches. Also, we propose for the first time-to the best of our knowledge-a\npure DNN AEC in the form of an echo estimator, that is based on a competitive\nFCRN structure and delivers a quality useful for practical applications.", "published": "2021-03-16 12:08:25", "link": "http://arxiv.org/abs/2103.09007v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Remote Monitoring of Patient Respiration with Mask Attachment -- A\n  Pragmatic Solution for Medical Facilities", "abstract": "Remote monitoring of vital signs in infectious patients minimizes the risks\nof viral transmissions to healthcare professionals. Evidence indicates that\ndonning face masks reduces the risk of viral transmissions and is now the norm\nin medical facilities. We propose attaching an acoustic-sensing device onto\nface masks to assist medical facilities in monitoring patients' respiration\nremotely. Usability and functionality studies of the modified face mask were\nevaluated on 16 healthy participants, who were blindfolded throughout the data\ncollection. Around half of the participants noticed the difference between the\nmodified and unmodified masks but they also reported there was no discomfort in\nusing the modified mask. Respiratory rates of the participants were evaluated\nfor one minute and the mean error of respiratory rate was found to be 2.0 +/-\n1.3 breath per minute. As all participants were healthy, the wheeze detection\nalgorithm was assessed by playing 176 wheezes and 176 normal breaths through a\nfoam mannequin. The recordings were played at three different times to account\nfor varying environmental noise. The overall accuracy of the wheeze detection\nalgorithm was 91.9%. The current findings support and suggest the use of the\nmask attachment in medical facilities.", "published": "2021-03-16 04:15:28", "link": "http://arxiv.org/abs/2103.08840v2", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "An Asynchronous WFST-Based Decoder For Automatic Speech Recognition", "abstract": "We introduce asynchronous dynamic decoder, which adopts an efficient A*\nalgorithm to incorporate big language models in the one-pass decoding for large\nvocabulary continuous speech recognition. Unlike standard one-pass decoding\nwith on-the-fly composition decoder which might induce a significant\ncomputation overhead, the asynchronous dynamic decoder has a novel design where\nit has two fronts, with one performing \"exploration\" and the other \"backfill\".\nThe computation of the two fronts alternates in the decoding process, resulting\nin more effective pruning than the standard one-pass decoding with an\non-the-fly composition decoder. Experiments show that the proposed decoder\nworks notably faster than the standard one-pass decoding with on-the-fly\ncomposition decoder, while the acceleration will be more obvious with the\nincrement of data complexity.", "published": "2021-03-16 13:35:25", "link": "http://arxiv.org/abs/2103.09063v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiCOVA Challenge: Dataset, task, and baseline system for COVID-19\n  diagnosis using acoustics", "abstract": "The DiCOVA challenge aims at accelerating research in diagnosing COVID-19\nusing acoustics (DiCOVA), a topic at the intersection of speech and audio\nprocessing, respiratory health diagnosis, and machine learning. This challenge\nis an open call for researchers to analyze a dataset of sound recordings\ncollected from COVID-19 infected and non-COVID-19 individuals for a two-class\nclassification. These recordings were collected via crowdsourcing from multiple\ncountries, through a website application. The challenge features two tracks,\none focusing on cough sounds, and the other on using a collection of breath,\nsustained vowel phonation, and number counting speech recordings. In this\npaper, we introduce the challenge and provide a detailed description of the\ntask, and present a baseline system for the task.", "published": "2021-03-16 15:42:29", "link": "http://arxiv.org/abs/2103.09148v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Flow-based Self-supervised Density Estimation for Anomalous Sound\n  Detection", "abstract": "To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. Exact likelihood estimation using Normalizing\nFlows is a promising technique for unsupervised anomaly detection, but it can\nfail at out-of-distribution detection since the likelihood is affected by the\nsmoothness of the data. To improve the detection performance, we train the\nmodel to assign higher likelihood to target machine sounds and lower likelihood\nto sounds from other machines of the same machine type. We demonstrate that\nthis enables the model to incorporate a self-supervised classification-based\napproach. Experiments conducted using the DCASE 2020 Challenge Task2 dataset\nshowed that the proposed method improves the AUC by 4.6% on average when using\nMasked Autoregressive Flow (MAF) and by 5.8% when using Glow, which is a\nsignificant improvement over the previous method.", "published": "2021-03-16 01:52:03", "link": "http://arxiv.org/abs/2103.08801v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion\n  Recognition", "abstract": "Emotional expressions are the behaviors that communicate our emotional state\nor attitude to others. They are expressed through verbal and non-verbal\ncommunication. Complex human behavior can be understood by studying physical\nfeatures from multiple modalities; mainly facial, vocal and physical gestures.\nRecently, spontaneous multi-modal emotion recognition has been extensively\nstudied for human behavior analysis. In this paper, we propose a new deep\nlearning-based approach for audio-visual emotion recognition. Our approach\nleverages recent advances in deep learning like knowledge distillation and\nhigh-performing deep architectures. The deep feature representations of the\naudio and visual modalities are fused based on a model-level fusion strategy. A\nrecurrent neural network is then used to capture the temporal dynamics. Our\nproposed approach substantially outperforms state-of-the-art approaches in\npredicting valence on the RECOLA dataset. Moreover, our proposed visual facial\nexpression feature extraction network outperforms state-of-the-art results on\nthe AffectNet and Google Facial Expression Comparison datasets.", "published": "2021-03-16 15:49:15", "link": "http://arxiv.org/abs/2103.09154v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Graphic relation between amplitude and sound intensity level", "abstract": "We present a simple experiment that allows us to demonstrate graphically that\nthe intensity of sound waves is proportional to the square of their amplitude,\na result that is theoretically analysed in any introductory wave course but\nrarely demonstrated empirically. To achieve our goal, we use an audio signal\ngenerator that, when connected to a loudspeaker, produces sine waves that can\nbe easily observed and measured using an oscilloscope. The measurements made\nwith these instruments allow us to create a plot of amplitude versus sound\nintensity level, which verifies the mathematical relationship between amplitude\nand intensity mentioned above. Among the experimental errors, the plot obtained\nis in excellent agreement with what is theoretically expected.", "published": "2021-03-16 13:48:17", "link": "http://arxiv.org/abs/2103.11822v1", "categories": ["physics.ed-ph", "cs.SD", "eess.AS"], "primary_category": "physics.ed-ph"}
