{"title": "Demystifying Prompts in Language Models via Perplexity Estimation", "abstract": "Language models can be prompted to perform a wide variety of zero- and\nfew-shot learning problems. However, performance varies significantly with the\nchoice of prompt, and we do not yet understand why this happens or how to pick\nthe best prompts. In this work, we analyze the factors that contribute to this\nvariance and establish a new empirical hypothesis: the performance of a prompt\nis coupled with the extent to which the model is familiar with the language it\ncontains. Over a wide range of tasks, we show that the lower the perplexity of\nthe prompt is, the better the prompt is able to perform the task. As a result,\nwe devise a method for creating prompts: (1) automatically extend a small seed\nset of manually written prompts by paraphrasing using GPT3 and backtranslation\nand (2) choose the lowest perplexity prompts to get significant gains in\nperformance.", "published": "2022-12-08 02:21:47", "link": "http://arxiv.org/abs/2212.04037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Multi-hop Machine Reading Comprehension\n  Approaches", "abstract": "Machine reading comprehension (MRC) is a long-standing topic in natural\nlanguage processing (NLP). The MRC task aims to answer a question based on the\ngiven context. Recently studies focus on multi-hop MRC which is a more\nchallenging extension of MRC, which to answer a question some disjoint pieces\nof information across the context are required. Due to the complexity and\nimportance of multi-hop MRC, a large number of studies have been focused on\nthis topic in recent years, therefore, it is necessary and worth reviewing the\nrelated literature. This study aims to investigate recent advances in the\nmulti-hop MRC approaches based on 31 studies from 2018 to 2022. In this regard,\nfirst, the multi-hop MRC problem definition will be introduced, then 31 models\nwill be reviewed in detail with a strong focus on their multi-hop aspects. They\nalso will be categorized based on their main techniques. Finally, a fine-grain\ncomprehensive comparison of the models and techniques will be presented.", "published": "2022-12-08 04:51:54", "link": "http://arxiv.org/abs/2212.04072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Successive Prompting for Decomposing Complex Questions", "abstract": "Answering complex questions that require making latent decisions is a\nchallenging task, especially when limited supervision is available. Recent\nworks leverage the capabilities of large language models (LMs) to perform\ncomplex question answering in a few-shot setting by demonstrating how to output\nintermediate rationalizations while solving the complex question in a single\npass. We introduce ``Successive Prompting'', where we iteratively break down a\ncomplex task into a simple task, solve it, and then repeat the process until we\nget the final solution. Successive prompting decouples the supervision for\ndecomposing complex questions from the supervision for answering simple\nquestions, allowing us to (1) have multiple opportunities to query in-context\nexamples at each reasoning step (2) learn question decomposition separately\nfrom question answering, including using synthetic data, and (3) use bespoke\n(fine-tuned) components for reasoning steps where a large LM does not perform\nwell. The intermediate supervision is typically manually written, which can be\nexpensive to collect. We introduce a way to generate a synthetic dataset which\ncan be used to bootstrap a model's ability to decompose and answer intermediate\nquestions. Our best model (with successive prompting) achieves an improvement\nof ~5% absolute F1 on a few-shot version of the DROP dataset when compared with\na state-of-the-art model with the same supervision.", "published": "2022-12-08 06:03:38", "link": "http://arxiv.org/abs/2212.04092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Discrete Genres: Mapping News Items onto a Multidimensional\n  Framework of Genre Cues", "abstract": "In the contemporary media landscape, with the vast and diverse supply of\nnews, it is increasingly challenging to study such an enormous amount of items\nwithout a standardized framework. Although attempts have been made to organize\nand compare news items on the basis of news values, news genres receive little\nattention, especially the genres in a news consumer's perception. Yet,\nperceived news genres serve as an essential component in exploring how news has\ndeveloped, as well as a precondition for understanding media effects. We\napproach this concept by conceptualizing and operationalizing a non-discrete\nframework for mapping news items in terms of genre cues. As a starting point,\nwe propose a preliminary set of dimensions consisting of \"factuality\" and\n\"formality\". To automatically analyze a large amount of news items, we deliver\ntwo computational models for predicting news sentences in terms of the said two\ndimensions. Such predictions could then be used for locating news items within\nour framework. This proposed approach that positions news items upon a\nmultidimensional grid helps in deepening our insight into the evolving nature\nof news genres.", "published": "2022-12-08 10:54:31", "link": "http://arxiv.org/abs/2212.04185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding", "abstract": "Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding\nalgorithm in Neural Machine Translation. However, MBR performs poorly with\nlabel smoothing, which is surprising as label smoothing provides decent\nimprovement with beam search and improves generality in various tasks. In this\nwork, we show that the issue arises from the un-consistency of label smoothing\non the token-level and sequence-level distributions. We demonstrate that even\nthough label smoothing only causes a slight change in the token-level, the\nsequence-level distribution is highly skewed. We coin the issue\n\\emph{autoregressive over-smoothness}. To address this issue, we propose a\nsimple and effective method, Distributional Cooling MBR (DC-MBR), which\nmanipulates the entropy of output distributions by tuning down the Softmax\ntemperature. We theoretically prove the equivalence between pre-tuning label\nsmoothing factor and distributional cooling. Extensive experiments on NMT\nbenchmarks validate that distributional cooling improves MBR in various\nsettings.", "published": "2022-12-08 11:40:31", "link": "http://arxiv.org/abs/2212.04205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs", "abstract": "In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.", "published": "2022-12-08 11:53:12", "link": "http://arxiv.org/abs/2212.04214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Capacity of Transformer to Abstract Syntactic\n  Representations: A Contrastive Analysis Based on Long-distance Agreement", "abstract": "The long-distance agreement, evidence for syntactic structure, is\nincreasingly used to assess the syntactic generalization of Neural Language\nModels. Much work has shown that transformers are capable of high accuracy in\nvaried agreement tasks, but the mechanisms by which the models accomplish this\nbehavior are still not well understood. To better understand transformers'\ninternal working, this work contrasts how they handle two superficially similar\nbut theoretically distinct agreement phenomena: subject-verb and object-past\nparticiple agreement in French. Using probing and counterfactual analysis\nmethods, our experiments show that i) the agreement task suffers from several\nconfounders which partially question the conclusions drawn so far and ii)\ntransformers handle subject-verb and object-past participle agreements in a way\nthat is consistent with their modeling in theoretical linguistics.", "published": "2022-12-08 19:10:46", "link": "http://arxiv.org/abs/2212.04523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explain to me like I am five -- Sentence Simplification Using\n  Transformers", "abstract": "Sentence simplification aims at making the structure of text easier to read\nand understand while maintaining its original meaning. This can be helpful for\npeople with disabilities, new language learners, or those with low literacy.\nSimplification often involves removing difficult words and rephrasing the\nsentence. Previous research have focused on tackling this task by either using\nexternal linguistic databases for simplification or by using control tokens for\ndesired fine-tuning of sentences. However, in this paper we purely use\npre-trained transformer models. We experiment with a combination of GPT-2 and\nBERT models, achieving the best SARI score of 46.80 on the Mechanical Turk\ndataset, which is significantly better than previous state-of-the-art results.\nThe code can be found at https://github.com/amanbasu/sentence-simplification.", "published": "2022-12-08 22:57:18", "link": "http://arxiv.org/abs/2212.04595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Glyph Phonetic Information for Chinese Spell Checking:\n  What Works and What's Next", "abstract": "While pre-trained Chinese language models have demonstrated impressive\nperformance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task\nremains a challenge. Previous research has explored using information such as\nglyphs and phonetics to improve the ability to distinguish misspelled\ncharacters, with good results. However, the generalization ability of these\nmodels is not well understood: it is unclear whether they incorporate\nglyph-phonetic information and, if so, whether this information is fully\nutilized. In this paper, we aim to better understand the role of glyph-phonetic\ninformation in the CSC task and suggest directions for improvement.\nAdditionally, we propose a new, more challenging, and practical setting for\ntesting the generalizability of CSC models. All code is made publicly\navailable.", "published": "2022-12-08 04:37:29", "link": "http://arxiv.org/abs/2212.04068v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Multi-hop Machine Reading Comprehension\n  Datasets and Metrics", "abstract": "Multi-hop Machine reading comprehension is a challenging task with aim of\nanswering a question based on disjoint pieces of information across the\ndifferent passages. The evaluation metrics and datasets are a vital part of\nmulti-hop MRC because it is not possible to train and evaluate models without\nthem, also, the proposed challenges by datasets often are an important\nmotivation for improving the existing models. Due to increasing attention to\nthis field, it is necessary and worth reviewing them in detail. This study aims\nto present a comprehensive survey on recent advances in multi-hop MRC\nevaluation metrics and datasets. In this regard, first, the multi-hop MRC\nproblem definition will be presented, then the evaluation metrics based on\ntheir multi-hop aspect will be investigated. Also, 15 multi-hop datasets have\nbeen reviewed in detail from 2017 to 2022, and a comprehensive analysis has\nbeen prepared at the end. Finally, open issues in this field have been\ndiscussed.", "published": "2022-12-08 04:42:59", "link": "http://arxiv.org/abs/2212.04070v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal\n  Dialogue Dataset", "abstract": "As sharing images in an instant message is a crucial factor, there has been\nactive research on learning an image-text multi-modal dialogue models. However,\ntraining a well-generalized multi-modal dialogue model remains challenging due\nto the low quality and limited diversity of images per dialogue in existing\nmulti-modal dialogue datasets. In this paper, we propose an automated pipeline\nto construct a multi-modal dialogue dataset, ensuring both dialogue quality and\nimage diversity without requiring minimum human effort. In our pipeline, to\nguarantee the coherence between images and dialogue, we prompt GPT-4 to infer\npotential image-sharing moments - specifically, the utterance, speaker,\nrationale, and image description. Furthermore, we leverage CLIP similarity to\nmaintain consistency between aligned multiple images to the utterance. Through\nthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modal\ndialogue dataset that surpasses existing datasets in terms of quality and\ndiversity in human evaluation. Our comprehensive experiments highlight that\nwhen multi-modal dialogue models are trained using our dataset, their\ngeneralization performance on unseen dialogue datasets is significantly\nenhanced. We make our source code and dataset publicly available.", "published": "2022-12-08 07:29:07", "link": "http://arxiv.org/abs/2212.04119v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Neural Correlates of Linguistic Structure Building: Comments on\n  Kazanina & Tavano (2022)", "abstract": "A recent perspective paper by Kazanina & Tavano (referred to as the KT\nperspective in the following) argues how neural oscillations cannot provide a\npotential neural correlate for syntactic structure building. The view that\nneural oscillations can provide a potential neural correlate for syntactic\nstructure building is largely attributed to a study by Ding, Melloni, Zhang,\nTian, and Poeppel in 2016 (referred to as the DMZTP study).\n  The KT perspective is thought provoking, but has severe misinterpretations\nabout the arguments in DMZTP and other studies, and contains contradictory\nconclusions in different parts of the perspective, making it impossible to\nunderstand the position of the authors. In the following, I summarize a few\nmisinterpretations and inconsistent arguments in the KT perspective, and put\nforward a few suggestions for future studies.", "published": "2022-12-08 12:00:26", "link": "http://arxiv.org/abs/2212.04219v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level\n  Natural Language Explanations", "abstract": "Natural language explanations promise to offer intuitively understandable\nexplanations of a neural network's decision process in complex vision-language\ntasks, as pursued in recent VL-NLE models. While current models offer\nimpressive performance on task accuracy and explanation plausibility, they\nsuffer from a range of issues: Some models feature a modular design where the\nexplanation generation module is poorly integrated with a separate module for\ntask-answer prediction, employ backbone models trained on limited sets of\ntasks, or incorporate ad hoc solutions to increase performance on single\ndatasets. We propose to evade these limitations by applying recent advances in\nlarge-scale multi-task pretraining of generative Transformer models to the\nproblem of VL-NLE tasks. Our approach outperforms recent models by a large\nmargin, with human annotators preferring the generated explanations over the\nground truth in two out of three evaluated datasets. As a novel challenge in\nVL-NLE research, we propose the problem of multi-task VL-NLE and show that\njointly training on multiple tasks can increase the explanation quality. We\ndiscuss the ethical implications of high-quality NLE generation and other\nissues in recent VL-NLE research.", "published": "2022-12-08 12:28:23", "link": "http://arxiv.org/abs/2212.04231v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Momentum Calibration for Text Generation", "abstract": "The input and output of most text generation tasks can be transformed to two\nsequences of tokens and they can be modeled using sequence-to-sequence learning\nmodeling tools such as Transformers. These models are usually trained by\nmaximizing the likelihood the output text sequence and assumes the input\nsequence and all gold preceding tokens are given during training, while during\ninference the model suffers from the exposure bias problem (i.e., it only has\naccess to its previously predicted tokens rather gold tokens during beam\nsearch). In this paper, we propose MoCa ({\\bf Mo}mentum {\\bf Ca}libration) for\ntext generation. MoCa is an online method that dynamically generates slowly\nevolving (but consistent) samples using a momentum moving average generator\nwith beam search and MoCa learns to align its model scores of these samples\nwith their actual qualities. Experiments on four text generation datasets\n(i.e., CNN/DailyMail, XSum, SAMSum and Gigaword) show MoCa consistently\nimproves strong pre-trained transformers using vanilla fine-tuning and we\nachieve the state-of-the-art results on CNN/DailyMail and SAMSum datasets.", "published": "2022-12-08 13:12:10", "link": "http://arxiv.org/abs/2212.04257v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Implicit causality in GPT-2: a case study", "abstract": "This case study investigates the extent to which a language model (GPT-2) is\nable to capture native speakers' intuitions about implicit causality in a\nsentence completion task. We first reproduce earlier results (showing lower\nsurprisal values for pronouns that are congruent with either the subject or\nobject, depending on which one corresponds to the implicit causality bias of\nthe verb), and then examine the effects of gender and verb frequency on model\nperformance. Our second study examines the reasoning ability of GPT-2: is the\nmodel able to produce more sensible motivations for why the subject VERBed the\nobject if the verbs have stronger causality biases? We also developed a\nmethodology to avoid human raters being biased by obscenities and disfluencies\ngenerated by the model.", "published": "2022-12-08 15:42:38", "link": "http://arxiv.org/abs/2212.04348v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Routine Outcome Monitoring in Psychotherapy Treatment using\n  Sentiment-Topic Modelling Approach", "abstract": "Despite the importance of emphasizing the right psychotherapy treatment for\nan individual patient, assessing the outcome of the therapy session is equally\ncrucial. Evidence showed that continuous monitoring patient's progress can\nsignificantly improve the therapy outcomes to an expected change. By monitoring\nthe outcome, the patient's progress can be tracked closely to help clinicians\nidentify patients who are not progressing in the treatment. These monitoring\ncan help the clinician to consider any necessary actions for the patient's\ntreatment as early as possible, e.g., recommend different types of treatment,\nor adjust the style of approach. Currently, the evaluation system is based on\nthe clinical-rated and self-report questionnaires that measure patients'\nprogress pre- and post-treatment. While outcome monitoring tends to improve the\ntherapy outcomes, however, there are many challenges in the current method,\ne.g. time and financial burden for administering questionnaires, scoring and\nanalysing the results. Therefore, a computational method for measuring and\nmonitoring patient progress over the course of treatment is needed, in order to\nenhance the likelihood of positive treatment outcome. Moreover, this\ncomputational method could potentially lead to an inexpensive monitoring tool\nto evaluate patients' progress in clinical care that could be administered by a\nwider range of health-care professionals.", "published": "2022-12-08 20:14:10", "link": "http://arxiv.org/abs/2212.08111v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Dub Movies via Hierarchical Prosody Models", "abstract": "Given a piece of text, a video clip and a reference audio, the movie dubbing\n(also known as visual voice clone V2C) task aims to generate speeches that\nmatch the speaker's emotion presented in the video using the desired speaker\nvoice as reference. V2C is more challenging than conventional text-to-speech\ntasks as it additionally requires the generated speech to exactly match the\nvarying emotions and speaking speed presented in the video. Unlike previous\nworks, we propose a novel movie dubbing architecture to tackle these problems\nvia hierarchical prosody modelling, which bridges the visual information to\ncorresponding speech prosody from three aspects: lip, face, and scene.\nSpecifically, we align lip movement to the speech duration, and convey facial\nexpression to speech energy and pitch via attention mechanism based on valence\nand arousal representations inspired by recent psychology findings. Moreover,\nwe design an emotion booster to capture the atmosphere from global video\nscenes. All these embeddings together are used to generate mel-spectrogram and\nthen convert to speech waves via existing vocoder. Extensive experimental\nresults on the Chem and V2C benchmark datasets demonstrate the favorable\nperformance of the proposed method. The source code and trained models will be\nreleased to the public.", "published": "2022-12-08 03:29:04", "link": "http://arxiv.org/abs/2212.04054v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Editing Models with Task Arithmetic", "abstract": "Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.", "published": "2022-12-08 05:50:53", "link": "http://arxiv.org/abs/2212.04089v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource\n  Neural Machine Translation", "abstract": "Transfer learning is a simple and powerful method that can be used to boost\nmodel performance of low-resource neural machine translation (NMT). Existing\ntransfer learning methods for NMT are static, which simply transfer knowledge\nfrom a parent model to a child model once via parameter initialization. In this\npaper, we propose a novel transfer learning method for NMT, namely ConsistTL,\nwhich can continuously transfer knowledge from the parent model during the\ntraining of the child model. Specifically, for each training instance of the\nchild model, ConsistTL constructs the semantically-equivalent instance for the\nparent model and encourages prediction consistency between the parent and child\nfor this instance, which is equivalent to the child model learning each\ninstance under the guidance of the parent model. Experimental results on five\nlow-resource NMT tasks demonstrate that ConsistTL results in significant\nimprovements over strong transfer learning baselines, with a gain up to 1.7\nBLEU over the existing back-translation model on the widely-used WMT17\nTurkish-English benchmark. Further analysis reveals that ConsistTL can improve\nthe inference calibration of the child model. Code and scripts are freely\navailable at https://github.com/NLP2CT/ConsistTL.", "published": "2022-12-08 13:27:37", "link": "http://arxiv.org/abs/2212.04262v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Modality-level Explainable Framework for Misinformation Checking in\n  Social Networks", "abstract": "The widespread of false information is a rising concern worldwide with\ncritical social impact, inspiring the emergence of fact-checking organizations\nto mitigate misinformation dissemination. However, human-driven verification\nleads to a time-consuming task and a bottleneck to have checked trustworthy\ninformation at the same pace they emerge. Since misinformation relates not only\nto the content itself but also to other social features, this paper addresses\nautomatic misinformation checking in social networks from a multimodal\nperspective. Moreover, as simply naming a piece of news as incorrect may not\nconvince the citizen and, even worse, strengthen confirmation bias, the\nproposal is a modality-level explainable-prone misinformation classifier\nframework. Our framework comprises a misinformation classifier assisted by\nexplainable methods to generate modality-oriented explainable inferences.\nPreliminary findings show that the misinformation classifier does benefit from\nmultimodal information encoding and the modality-oriented explainable mechanism\nincreases both inferences' interpretability and completeness.", "published": "2022-12-08 13:57:06", "link": "http://arxiv.org/abs/2212.04272v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Lie detection algorithms attract few users but vastly increase\n  accusation rates", "abstract": "People are not very good at detecting lies, which may explain why they\nrefrain from accusing others of lying, given the social costs attached to false\naccusations - both for the accuser and the accused. Here we consider how this\nsocial balance might be disrupted by the availability of lie-detection\nalgorithms powered by Artificial Intelligence. Will people elect to use lie\ndetection algorithms that perform better than humans, and if so, will they show\nless restraint in their accusations? We built a machine learning classifier\nwhose accuracy (67\\%) was significantly better than human accuracy (50\\%) in a\nlie-detection task and conducted an incentivized lie-detection experiment in\nwhich we measured participants' propensity to use the algorithm, as well as the\nimpact of that use on accusation rates. We find that the few people (33\\%) who\nelect to use the algorithm drastically increase their accusation rates (from\n25\\% in the baseline condition up to 86% when the algorithm flags a statement\nas a lie). They make more false accusations (18pp increase), but at the same\ntime, the probability of a lie remaining undetected is much lower in this group\n(36pp decrease). We consider individual motivations for using lie detection\nalgorithms and the social implications of these algorithms.", "published": "2022-12-08 14:07:21", "link": "http://arxiv.org/abs/2212.04277v1", "categories": ["econ.GN", "cs.AI", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "BEVBert: Multimodal Map Pre-training for Language-guided Navigation", "abstract": "Large-scale pre-training has shown promising results on the\nvision-and-language navigation (VLN) task. However, most existing pre-training\nmethods employ discrete panoramas to learn visual-textual associations. This\nrequires the model to implicitly correlate incomplete, duplicate observations\nwithin the panoramas, which may impair an agent's spatial understanding. Thus,\nwe propose a new map-based pre-training paradigm that is spatial-aware for use\nin VLN. Concretely, we build a local metric map to explicitly aggregate\nincomplete observations and remove duplicates, while modeling navigation\ndependency in a global topological map. This hybrid design can balance the\ndemand of VLN for both short-term reasoning and long-term planning. Then, based\non the hybrid map, we devise a pre-training framework to learn a multimodal map\nrepresentation, which enhances spatial-aware cross-modal reasoning thereby\nfacilitating the language-guided navigation goal. Extensive experiments\ndemonstrate the effectiveness of the map-based pre-training route for VLN, and\nthe proposed method achieves state-of-the-art on four VLN benchmarks.", "published": "2022-12-08 16:27:54", "link": "http://arxiv.org/abs/2212.04385v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist\n  Models", "abstract": "Generalist models, which are capable of performing diverse multi-modal tasks\nin a task-agnostic way within a single model, have been explored recently.\nBeing, hopefully, an alternative to approaching general-purpose AI, existing\ngeneralist models are still at an early stage, where modality and task coverage\nis limited. To empower multi-modal task-scaling and speed up this line of\nresearch, we release a generalist model learning system, OFASys, built on top\nof a declarative task interface named multi-modal instruction. At the core of\nOFASys is the idea of decoupling multi-modal task representations from the\nunderlying model implementations. In OFASys, a task involving multiple\nmodalities can be defined declaratively even with just a single line of code.\nThe system automatically generates task plans from such instructions for\ntraining and inference. It also facilitates multi-task training for diverse\nmulti-modal workloads. As a starting point, we provide presets of 7 different\nmodalities and 23 highly-diverse example tasks in OFASys, with which we also\ndevelop a first-in-kind, single model, OFA+, that can handle text, image,\nspeech, video, and motion data. The single OFA+ model achieves 95% performance\nin average with only 16% parameters of 15 task-finetuned models, showcasing the\nperformance reliability of multi-modal task-scaling provided by OFASys.\nAvailable at https://github.com/OFA-Sys/OFASys", "published": "2022-12-08 17:07:09", "link": "http://arxiv.org/abs/2212.04408v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VASR: Visual Analogies of Situation Recognition", "abstract": "A core process in human cognition is analogical mapping: the ability to\nidentify a similar relational structure between different situations. We\nintroduce a novel task, Visual Analogies of Situation Recognition, adapting the\nclassical word-analogy task into the visual domain. Given a triplet of images,\nthe task is to select an image candidate B' that completes the analogy (A to A'\nis like B to what?). Unlike previous work on visual analogy that focused on\nsimple image transformations, we tackle complex analogies requiring\nunderstanding of scenes.\n  We leverage situation recognition annotations and the CLIP model to generate\na large set of 500k candidate analogies. Crowdsourced annotations for a sample\nof the data indicate that humans agree with the dataset label ~80% of the time\n(chance level 25%). Furthermore, we use human annotations to create a\ngold-standard dataset of 3,820 validated analogies. Our experiments demonstrate\nthat state-of-the-art models do well when distractors are chosen randomly\n(~86%), but struggle with carefully chosen distractors (~53%, compared to 90%\nhuman accuracy). We hope our dataset will encourage the development of new\nanalogy-making models. Website: https://vasr-dataset.github.io/", "published": "2022-12-08 20:08:49", "link": "http://arxiv.org/abs/2212.04542v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Role of AI in Drug Discovery: Challenges, Opportunities, and\n  Strategies", "abstract": "Artificial intelligence (AI) has the potential to revolutionize the drug\ndiscovery process, offering improved efficiency, accuracy, and speed. However,\nthe successful application of AI is dependent on the availability of\nhigh-quality data, the addressing of ethical concerns, and the recognition of\nthe limitations of AI-based approaches. In this article, the benefits,\nchallenges and drawbacks of AI in this field are reviewed, and possible\nstrategies and approaches for overcoming the present obstacles are proposed.\nThe use of data augmentation, explainable AI, and the integration of AI with\ntraditional experimental methods, as well as the potential advantages of AI in\npharmaceutical research are also discussed. Overall, this review highlights the\npotential of AI in drug discovery and provides insights into the challenges and\nopportunities for realizing its potential in this field.\n  Note from the human-authors: This article was created to test the ability of\nChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors\nin writing review articles. The text generated by the AI following our\ninstructions (see Supporting Information) was used as a starting point, and its\nability to automatically generate content was evaluated. After conducting a\nthorough review, human authors practically rewrote the manuscript, striving to\nmaintain a balance between the original proposal and scientific criteria. The\nadvantages and limitations of using AI for this purpose are discussed in the\nlast section.", "published": "2022-12-08 23:23:39", "link": "http://arxiv.org/abs/2212.08104v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models", "abstract": "This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner", "published": "2022-12-08 05:46:32", "link": "http://arxiv.org/abs/2212.04088v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "DRED: Deep REDundancy Coding of Speech Using a Rate-Distortion-Optimized\n  Variational Autoencoder", "abstract": "Despite recent advancements in packet loss concealment (PLC) using deep\nlearning techniques, packet loss remains a significant challenge in real-time\nspeech communication. Redundancy has been used in the past to recover the\nmissing information during losses. However, conventional redundancy techniques\nare limited in the maximum loss duration they can cover and are often\nunsuitable for burst packet loss. We propose a new approach based on a\nrate-distortion-optimized variational autoencoder (RDO-VAE), allowing us to\noptimize a deep speech compression algorithm for the task of encoding large\namounts of redundancy at very low bitrate. The proposed Deep REDundancy (DRED)\nalgorithm can transmit up to 50x redundancy using less than 32 kb/s. Results\nshow that DRED outperforms the existing Opus codec redundancy. We also\ndemonstrate its benefits when operating in the context of WebRTC.", "published": "2022-12-08 18:23:52", "link": "http://arxiv.org/abs/2212.04453v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "High Quality Audio Coding with MDCTNet", "abstract": "We propose a neural audio generative model, MDCTNet, operating in the\nperceptually weighted domain of an adaptive modified discrete cosine transform\n(MDCT). The architecture of the model captures correlations in both time and\nfrequency directions with recurrent layers (RNNs). An audio coding system is\nobtained by training MDCTNet on a diverse set of fullband monophonic audio\nsignals at 48 kHz sampling, conditioned by a perceptual audio encoder. In a\nsubjective listening test with ten excerpts chosen to be balanced across\ncontent types, yet stressful for both codecs, the mean performance of the\nproposed system for 24 kb/s variable bitrate (VBR) is similar to that of Opus\nat twice the bitrate.", "published": "2022-12-08 22:18:20", "link": "http://arxiv.org/abs/2212.04583v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On The Relevance Of The Differences Between HRTF Measurement Setups For\n  Machine Learning", "abstract": "As spatial audio is enjoying a surge in popularity, data-driven machine\nlearning techniques that have been proven successful in other domains are\nincreasingly used to process head-related transfer function measurements.\nHowever, these techniques require much data, whereas the existing datasets are\nranging from tens to the low hundreds of datapoints. It therefore becomes\nattractive to combine multiple of these datasets, although they are measured\nunder different conditions. In this paper, we first establish the common ground\nbetween a number of datasets, then we investigate potential pitfalls of mixing\ndatasets. We perform a simple experiment to test the relevance of the remaining\ndifferences between datasets when applying machine learning techniques.\nFinally, we pinpoint the most relevant differences.", "published": "2022-12-08 14:19:46", "link": "http://arxiv.org/abs/2212.04283v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Framewise WaveGAN: High Speed Adversarial Vocoder in Time Domain with\n  Very Low Computational Complexity", "abstract": "GAN vocoders are currently one of the state-of-the-art methods for building\nhigh-quality neural waveform generative models. However, most of their\narchitectures require dozens of billion floating-point operations per second\n(GFLOPS) to generate speech waveforms in samplewise manner. This makes GAN\nvocoders still challenging to run on normal CPUs without accelerators or\nparallel computers. In this work, we propose a new architecture for GAN\nvocoders that mainly depends on recurrent and fully-connected networks to\ndirectly generate the time domain signal in framewise manner. This results in\nconsiderable reduction of the computational cost and enables very fast\ngeneration on both GPUs and low-complexity CPUs. Experimental results show that\nour Framewise WaveGAN vocoder achieves significantly higher quality than\nauto-regressive maximum-likelihood vocoders such as LPCNet at a very low\ncomplexity of 1.2 GFLOPS. This makes GAN vocoders more practical on edge and\nlow-power devices.", "published": "2022-12-08 19:38:34", "link": "http://arxiv.org/abs/2212.04532v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SpeechLMScore: Evaluating speech generation using speech language model", "abstract": "While human evaluation is the most reliable metric for evaluating speech\ngeneration systems, it is generally costly and time-consuming. Previous studies\non automatic speech quality assessment address the problem by predicting human\nevaluation scores with machine learning models. However, they rely on\nsupervised learning and thus suffer from high annotation costs and domain-shift\nproblems. We propose SpeechLMScore, an unsupervised metric to evaluate\ngenerated speech using a speech-language model. SpeechLMScore computes the\naverage log-probability of a speech signal by mapping it into discrete tokens\nand measures the average probability of generating the sequence of tokens.\nTherefore, it does not require human annotation and is a highly scalable\nframework. Evaluation results demonstrate that the proposed metric shows a\npromising correlation with human evaluation scores on different speech\ngeneration tasks including voice conversion, text-to-speech, and speech\nenhancement.", "published": "2022-12-08 21:00:15", "link": "http://arxiv.org/abs/2212.04559v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Data-driven Cognitive Salience Model for Objective Perceptual Audio\n  Quality Assessment", "abstract": "Objective audio quality measurement systems often use perceptual models to\npredict the subjective quality scores of processed signals, as reported in\nlistening tests. Most systems map different metrics of perceived degradation\ninto a single quality score predicting subjective quality. This requires a\nquality mapping stage that is informed by real listening test data using\nstatistical learning (i.e., a data-driven approach) with distortion metrics as\ninput features. However, the amount of reliable training data is limited in\npractice, and usually not sufficient for a comprehensive training of large\nlearning models. Models of cognitive effects in objective systems can, however,\nimprove the learning model. Specifically, considering the salience of certain\ndistortion types, they provide additional features to the mapping stage that\nimprove the learning process, especially for limited amounts of training data.\nWe propose a novel data-driven salience model that informs the quality mapping\nstage by explicitly estimating the cognitive/degradation metric interactions\nusing a salience measure. Systems incorporating the novel salience model are\nshown to outperform equivalent systems that only use statistical learning to\ncombine cognitive and degradation metrics, as well as other well-known\nmeasurement systems, for a representative validation dataset.", "published": "2022-12-08 21:40:01", "link": "http://arxiv.org/abs/2212.04572v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DDSupport: Language Learning Support System that Displays Differences\n  and Distances from Model Speech", "abstract": "When beginners learn to speak a non-native language, it is difficult for them\nto judge for themselves whether they are speaking well. Therefore,\ncomputer-assisted pronunciation training systems are used to detect learner\nmispronunciations. These systems typically compare the user's speech with that\nof a specific native speaker as a model in units of rhythm, phonemes, or words\nand calculate the differences. However, they require extensive speech data with\ndetailed annotations or can only compare with one specific native speaker. To\novercome these problems, we propose a new language learning support system that\ncalculates speech scores and detects mispronunciations by beginners based on a\nsmall amount of unannotated speech data without comparison to a specific\nperson. The proposed system uses deep learning--based speech processing to\ndisplay the pronunciation score of the learner's speech and the\ndifference/distance between the learner's and a group of models' pronunciation\nin an intuitively visual manner. Learners can gradually improve their\npronunciation by eliminating differences and shortening the distance from the\nmodel until they become sufficiently proficient. Furthermore, since the\npronunciation score and difference/distance are not calculated compared to\nspecific sentences of a particular model, users are free to study the sentences\nthey wish to study. We also built an application to help non-native speakers\nlearn English and confirmed that it can improve users' speech intelligibility.", "published": "2022-12-08 05:49:15", "link": "http://arxiv.org/abs/2212.04930v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
