{"title": "From Witch's Shot to Music Making Bones -- Resources for Medical Laymen\n  to Technical Language and Vice Versa", "abstract": "Many people share information in social media or forums, like food they eat,\nsports activities they do or events which have been visited. This also applies\nto information about a person's health status. Information we share online\nunveils directly or indirectly information about our lifestyle and health\nsituation and thus provides a valuable data resource. If we can make advantage\nof that data, applications can be created that enable e.g. the detection of\npossible risk factors of diseases or adverse drug reactions of medications.\nHowever, as most people are not medical experts, language used might be more\ndescriptive rather than the precise medical expression as medics do. To detect\nand use those relevant information, laymen language has to be translated and/or\nlinked to the corresponding medical concept. This work presents baseline data\nsources in order to address this challenge for German. We introduce a new data\nset which annotates medical laymen and technical expressions in a patient\nforum, along with a set of medical synonyms and definitions, and present first\nbaseline results on the data.", "published": "2020-05-23 08:56:18", "link": "http://arxiv.org/abs/2005.11494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing and Exploring Tabular Data in Conversational Search", "abstract": "Tabular data provide answers to a significant portion of search queries.\nHowever, reciting an entire result table is impractical in conversational\nsearch systems. We propose to generate natural language summaries as answers to\ndescribe the complex information contained in a table. Through crowdsourcing\nexperiments, we build a new conversation-oriented, open-domain table\nsummarization dataset. It includes annotated table summaries, which not only\nanswer questions but also help people explore other information in the table.\nWe utilize this dataset to develop automatic table summarization systems as\nSOTA baselines. Based on the experimental results, we identify challenges and\npoint out future research directions that this resource will support.", "published": "2020-05-23 08:29:51", "link": "http://arxiv.org/abs/2005.11490v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning Constraints for Structured Prediction Using Rectifier Networks", "abstract": "Various natural language processing tasks are structured prediction problems\nwhere outputs are constructed with multiple interdependent decisions. Past work\nhas shown that domain knowledge, framed as constraints over the output space,\ncan help improve predictive accuracy. However, designing good constraints often\nrelies on domain expertise. In this paper, we study the problem of learning\nsuch constraints. We frame the problem as that of training a two-layer\nrectifier network to identify valid structures or substructures, and show a\nconstruction for converting a trained network into a system of linear\nconstraints over the inference variables. Our experiments on several NLP tasks\nshow that the learned constraints can improve the prediction accuracy,\nespecially when the number of training examples is small.", "published": "2020-05-23 18:31:30", "link": "http://arxiv.org/abs/2006.01209v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "COVID-19 Public Opinion and Emotion Monitoring System Based on Time\n  Series Thermal New Word Mining", "abstract": "With the spread and development of new epidemics, it is of great reference\nvalue to identify the changing trends of epidemics in public emotions. We\ndesigned and implemented the COVID-19 public opinion monitoring system based on\ntime series thermal new word mining. A new word structure discovery scheme\nbased on the timing explosion of network topics and a Chinese sentiment\nanalysis method for the COVID-19 public opinion environment is proposed.\nEstablish a \"Scrapy-Redis-Bloomfilter\" distributed crawler framework to collect\ndata. The system can judge the positive and negative emotions of the reviewer\nbased on the comments, and can also reflect the depth of the seven emotions\nsuch as Hopeful, Happy, and Depressed. Finally, we improved the sentiment\ndiscriminant model of this system and compared the sentiment discriminant error\nof COVID-19 related comments with the Jiagu deep learning model. The results\nshow that our model has better generalization ability and smaller discriminant\nerror. We designed a large data visualization screen, which can clearly show\nthe trend of public emotions, the proportion of various emotion categories,\nkeywords, hot topics, etc., and fully and intuitively reflect the development\nof public opinion.", "published": "2020-05-23 03:42:10", "link": "http://arxiv.org/abs/2005.11458v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Power Pooling Operators and Confidence Learning for Semi-Supervised\n  Sound Event Detection", "abstract": "In recent years, the involvement of synthetic strongly labeled data,weakly\nlabeled data and unlabeled data has drawn much research attentionin\nsemi-supervised sound event detection (SSED). Self-training models carry out\npredictions without strong annotations and then take predictions with high\nprobabilities as pseudo-labels for retraining. Such models have shown its\neffectiveness in SSED. However, probabilities are poorly calibrated confidence\nestimates, and samples with low probabilities are ignored. Hence, we introduce\na method of learning confidence deliberately and retaining all data distinctly\nby applying confidence as weights. Additionally, linear pooling has been\nconsidered as a state-of-the-art aggregation function for SSED with weak\nlabeling. In this paper, we propose a power pooling function whose coefficient\ncan be trained automatically to achieve nonlinearity. A confidencebased\nsemi-supervised sound event detection (C-SSED) framework is designed to combine\nconfidence and power pooling. The experimental results demonstrate that\nconfidence is proportional to the accuracy of the predictions. The power\npooling function outperforms linear pooling at both error rate and F1 results.\nIn addition, the C-SSED framework achieves a relative error rate reduction of\n34% in contrast to the baseline model.", "published": "2020-05-23 04:02:21", "link": "http://arxiv.org/abs/2005.11459v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the Best Loss Function for DNN-Based Low-latency Speech\n  Enhancement with Temporal Convolutional Networks", "abstract": "Recently, deep neural networks (DNNs) have been successfully used for speech\nenhancement, and DNN-based speech enhancement is becoming an attractive\nresearch area. While time-frequency masking based on the short-time Fourier\ntransform (STFT) has been widely used for DNN-based speech enhancement over the\nlast years, time domain methods such as the time-domain audio separation\nnetwork (TasNet) have also been proposed. The most suitable method depends on\nthe scale of the dataset and the type of task. In this paper, we explore the\nbest speech enhancement algorithm on two different datasets. We propose a\nSTFT-based method and a loss function using problem-agnostic speech encoder\n(PASE) features to improve subjective quality for the smaller dataset. Our\nproposed methods are effective on the Voice Bank + DEMAND dataset and compare\nfavorably to other state-of-the-art methods. We also implement a low-latency\nversion of TasNet, which we submitted to the DNS Challenge and made public by\nopen-sourcing it. Our model achieves excellent performance on the DNS Challenge\ndataset.", "published": "2020-05-23 22:17:49", "link": "http://arxiv.org/abs/2005.11611v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Integration of Multi-channel Information for\n  Speaker-independent Speech Separation", "abstract": "Although deep-learning-based methods have markedly improved the performance\nof speech separation over the past few years, it remains an open question how\nto integrate multi-channel signals for speech separation. We propose two\nmethods, namely, early-fusion and late-fusion methods, to integrate\nmulti-channel information based on the time-domain audio separation network,\nwhich has been proven effective in single-channel speech separation. We also\npropose channel-sequential-transfer learning, which is a transfer learning\nframework that applies the parameters trained for a lower-channel network as\nthe initial values of a higher-channel network. For fair comparison, we\nevaluated our proposed methods using a spatialized version of the wsj0-2mix\ndataset, which is open-sourced. It was found that our proposed methods can\noutperform multi-channel deep clustering and improve the performance\nproportionally to the number of microphones. It was also proven that the\nperformance of the late-fusion method is consistently higher than that of the\nsingle-channel method regardless of the angle difference between speakers.", "published": "2020-05-23 22:18:13", "link": "http://arxiv.org/abs/2005.11612v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Optimal DNN Architecture for End-to-End Beamformers Based on\n  Time-frequency References", "abstract": "Acoustic beamformers have been widely used to enhance audio signals.\nCurrently, the best methods are the deep neural network (DNN)-powered variants\nof the generalized eigenvalue and minimum-variance distortionless response\nbeamformers and the DNN-based filter-estimation methods that are used to\ndirectly compute beamforming filters. Both approaches are effective; however,\nthey have blind spots in their generalizability. Therefore, we propose a novel\napproach for combining these two methods into a single framework that attempts\nto exploit the best features of both. The resulting model, called the W-Net\nbeamformer, includes two components; the first computes time-frequency\nreferences that the second uses to estimate beamforming filters. The results on\ndata that include a wide variety of room and noise conditions, including static\nand mobile noise sources, show that the proposed beamformer outperforms other\nmethods on all tested evaluation metrics, which signifies that the proposed\narchitecture allows for effective computation of the beamforming filters.", "published": "2020-05-23 22:30:15", "link": "http://arxiv.org/abs/2005.12683v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
