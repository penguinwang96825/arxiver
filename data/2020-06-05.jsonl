{"title": "Understanding Self-Attention of Self-Supervised Audio Transformers", "abstract": "Self-supervised Audio Transformers (SAT) enable great success in many\ndownstream speech applications like ASR, but how they work has not been widely\nexplored yet. In this work, we present multiple strategies for the analysis of\nattention mechanisms in SAT. We categorize attentions into explainable\ncategories, where we discover each category possesses its own unique\nfunctionality. We provide a visualization tool for understanding multi-head\nself-attention, importance ranking strategies for identifying critical\nattention, and attention refinement techniques to improve model performance.", "published": "2020-06-05 07:23:03", "link": "http://arxiv.org/abs/2006.03265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELITR Non-Native Speech Translation at IWSLT 2020", "abstract": "This paper is an ELITR system submission for the non-native speech\ntranslation task at IWSLT 2020. We describe systems for offline ASR, real-time\nASR, and our cascaded approach to offline SLT and real-time SLT. We select our\nprimary candidates from a pool of pre-existing systems, develop a new\nend-to-end general ASR system, and a hybrid ASR trained on non-native speech.\nThe provided small validation set prevents us from carrying out a complex\nvalidation, but we submit all the unselected candidates for contrastive\nevaluation on the test set.", "published": "2020-06-05 09:29:57", "link": "http://arxiv.org/abs/2006.03331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prague Dependency Treebank -- Consolidated 1.0", "abstract": "We present a richly annotated and genre-diversified language resource, the\nPrague Dependency Treebank-Consolidated 1.0 (PDT-C 1.0), the purpose of which\nis - as it always been the case for the family of the Prague Dependency\nTreebanks - to serve both as a training data for various types of NLP tasks as\nwell as for linguistically-oriented research. PDT-C 1.0 contains four different\ndatasets of Czech, uniformly annotated using the standard PDT scheme (albeit\nnot everything is annotated manually, as we describe in detail here). The texts\ncome from different sources: daily newspaper articles, Czech translation of the\nWall Street Journal, transcribed dialogs and a small amount of user-generated,\nshort, often non-standard language segments typed into a web translator.\nAltogether, the treebank contains around 180,000 sentences with their\nmorphological, surface and deep syntactic annotation. The diversity of the\ntexts and annotations should serve well the NLP applications as well as it is\nan invaluable resource for linguistic research, including comparative studies\nregarding texts of different genres. The corpus is publicly and freely\navailable.", "published": "2020-06-05 20:52:55", "link": "http://arxiv.org/abs/2006.03679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UDPipe at EvaLatin 2020: Contextualized Embeddings and Treebank\n  Embeddings", "abstract": "We present our contribution to the EvaLatin shared task, which is the first\nevaluation campaign devoted to the evaluation of NLP tools for Latin. We\nsubmitted a system based on UDPipe 2.0, one of the winners of the CoNLL 2018\nShared Task, The 2018 Shared Task on Extrinsic Parser Evaluation and SIGMORPHON\n2019 Shared Task. Our system places first by a wide margin both in\nlemmatization and POS tagging in the open modality, where additional supervised\ndata is allowed, in which case we utilize all Universal Dependency Latin\ntreebanks. In the closed modality, where only the EvaLatin training data is\nallowed, our system achieves the best performance in lemmatization and in\nclassical subtask of POS tagging, while reaching second place in cross-genre\nand cross-time settings. In the ablation experiments, we also evaluate the\ninfluence of BERT and XLM-RoBERTa contextualized embeddings, and the treebank\nencodings of the different flavors of Latin treebanks.", "published": "2020-06-05 21:03:35", "link": "http://arxiv.org/abs/2006.03687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human or Machine: Automating Human Likeliness Evaluation of NLG Texts", "abstract": "Automatic evaluation of various text quality criteria produced by data-driven\nintelligent methods is very common and useful because it is cheap, fast, and\nusually yields repeatable results. In this paper, we present an attempt to\nautomate the human likeliness evaluation of the output text samples coming from\nnatural language generation methods used to solve several tasks. We propose to\nuse a human likeliness score that shows the percentage of the output samples\nfrom a method that look as if they were written by a human. Instead of having\nhuman participants label or rate those samples, we completely automate the\nprocess by using a discrimination procedure based on large pretrained language\nmodels and their probability distributions. As follow up, we plan to perform an\nempirical analysis of human-written and machine-generated texts to find the\noptimal setup of this evaluation approach. A validation procedure involving\nhuman participants will also check how the automatic evaluation correlates with\nhuman judgments.", "published": "2020-06-05 00:57:52", "link": "http://arxiv.org/abs/2006.03189v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Text Coherence at Sentence and Paragraph Levels", "abstract": "In this paper, to evaluate text coherence, we propose the paragraph ordering\ntask as well as conducting sentence ordering. We collected four distinct\ncorpora from different domains on which we investigate the adaptation of\nexisting sentence ordering methods to a paragraph ordering task. We also\ncompare the learnability and robustness of existing models by artificially\ncreating mini datasets and noisy datasets respectively and verifying the\nefficiency of established models under these circumstances. Furthermore, we\ncarry out human evaluation on the rearranged passages from two competitive\nmodels and confirm that WLCS-l is a better metric performing significantly\nhigher correlations with human rating than tau, the most prevalent metric used\nbefore. Results from these evaluations show that except for certain extreme\nconditions, the recurrent graph neural network-based model is an optimal choice\nfor coherence modeling.", "published": "2020-06-05 03:31:49", "link": "http://arxiv.org/abs/2006.03221v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"To Target or Not to Target\": Identification and Analysis of Abusive\n  Text Using Ensemble of Classifiers", "abstract": "With rising concern around abusive and hateful behavior on social media\nplatforms, we present an ensemble learning method to identify and analyze the\nlinguistic properties of such content. Our stacked ensemble comprises of three\nmachine learning models that capture different aspects of language and provide\ndiverse and coherent insights about inappropriate language. The proposed\napproach provides comparable results to the existing state-of-the-art on the\nTwitter Abusive Behavior dataset (Founta et al. 2018) without using any user or\nnetwork-related information; solely relying on textual properties. We believe\nthat the presented insights and discussion of shortcomings of current\napproaches will highlight potential directions for future research.", "published": "2020-06-05 06:59:22", "link": "http://arxiv.org/abs/2006.03256v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SEAL: Scientific Keyphrase Extraction and Classification", "abstract": "Automatic scientific keyphrase extraction is a challenging problem\nfacilitating several downstream scholarly tasks like search, recommendation,\nand ranking. In this paper, we introduce SEAL, a scholarly tool for automatic\nkeyphrase extraction and classification. The keyphrase extraction module\ncomprises two-stage neural architecture composed of Bidirectional Long\nShort-Term Memory cells augmented with Conditional Random Fields. The\nclassification module comprises of a Random Forest classifier. We extensively\nexperiment to showcase the robustness of the system. We evaluate multiple\nstate-of-the-art baselines and show a significant improvement. The current\nsystem is hosted at http://lingo.iitgn.ac.in:5000/.", "published": "2020-06-05 08:21:26", "link": "http://arxiv.org/abs/2006.03292v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Topic Detection from Conversational Dialogue Corpus with Parallel\n  Dirichlet Allocation Model and Elbow Method", "abstract": "A conversational system needs to know how to switch between topics to\ncontinue the conversation for a more extended period. For this topic detection\nfrom dialogue corpus has become an important task for a conversation and\naccurate prediction of conversation topics is important for creating coherent\nand engaging dialogue systems. In this paper, we proposed a topic detection\napproach with Parallel Latent Dirichlet Allocation (PLDA) Model by clustering a\nvocabulary of known similar words based on TF-IDF scores and Bag of Words (BOW)\ntechnique. In the experiment, we use K-mean clustering with Elbow Method for\ninterpretation and validation of consistency within-cluster analysis to select\nthe optimal number of clusters. We evaluate our approach by comparing it with\ntraditional LDA and clustering technique. The experimental results show that\ncombining PLDA with Elbow method selects the optimal number of clusters and\nrefine the topics for the conversation.", "published": "2020-06-05 10:24:43", "link": "http://arxiv.org/abs/2006.03353v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Unsupervised Translation of Programming Languages", "abstract": "A transcompiler, also known as source-to-source translator, is a system that\nconverts source code from a high-level programming language (such as C++ or\nPython) to another. Transcompilers are primarily used for interoperability, and\nto port codebases written in an obsolete or deprecated language (e.g. COBOL,\nPython 2) to a modern one. They typically rely on handcrafted rewrite rules,\napplied to the source code abstract syntax tree. Unfortunately, the resulting\ntranslations often lack readability, fail to respect the target language\nconventions, and require manual modifications in order to work properly. The\noverall translation process is timeconsuming and requires expertise in both the\nsource and target languages, making code-translation projects expensive.\nAlthough neural models significantly outperform their rule-based counterparts\nin the context of natural language translation, their applications to\ntranscompilation have been limited due to the scarcity of parallel data in this\ndomain. In this paper, we propose to leverage recent approaches in unsupervised\nmachine translation to train a fully unsupervised neural transcompiler. We\ntrain our model on source code from open source GitHub projects, and show that\nit can translate functions between C++, Java, and Python with high accuracy.\nOur method relies exclusively on monolingual source code, requires no expertise\nin the source or target languages, and can easily be generalized to other\nprogramming languages. We also build and release a test set composed of 852\nparallel functions, along with unit tests to check the correctness of\ntranslations. We show that our model outperforms rule-based commercial\nbaselines by a significant margin.", "published": "2020-06-05 15:28:01", "link": "http://arxiv.org/abs/2006.03511v3", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Beyond Domain APIs: Task-oriented Conversational Modeling with\n  Unstructured Knowledge Access", "abstract": "Most prior work on task-oriented dialogue systems are restricted to a limited\ncoverage of domain APIs, while users oftentimes have domain related requests\nthat are not covered by the APIs. In this paper, we propose to expand coverage\nof task-oriented dialogue systems by incorporating external unstructured\nknowledge sources. We define three sub-tasks: knowledge-seeking turn detection,\nknowledge selection, and knowledge-grounded response generation, which can be\nmodeled individually or jointly. We introduce an augmented version of MultiWOZ\n2.1, which includes new out-of-API-coverage turns and responses grounded on\nexternal knowledge sources. We present baselines for each sub-task using both\nconventional and neural approaches. Our experimental results demonstrate the\nneed for further research in this direction to enable more informative\nconversational systems.", "published": "2020-06-05 16:12:18", "link": "http://arxiv.org/abs/2006.03533v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spoken dialect identification in Twitter using a multi-filter\n  architecture", "abstract": "This paper presents our approach for SwissText & KONVENS 2020 shared task 2,\nwhich is a multi-stage neural model for Swiss German (GSW) identification on\nTwitter. Our model outputs either GSW or non-GSW and is not meant to be used as\na generic language identifier. Our architecture consists of two independent\nfilters where the first one favors recall, and the second one filter favors\nprecision (both towards GSW). Moreover, we do not use binary models (GSW vs.\nnot-GSW) in our filters but rather a multi-class classifier with GSW being one\nof the possible labels. Our model reaches F1-score of 0.982 on the test set of\nthe shared task.", "published": "2020-06-05 17:19:15", "link": "http://arxiv.org/abs/2006.03564v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stance Detection on Social Media: State of the Art and Trends", "abstract": "Stance detection on social media is an emerging opinion mining paradigm for\nvarious social and political applications in which sentiment analysis may be\nsub-optimal. There has been a growing research interest for developing\neffective methods for stance detection methods varying among multiple\ncommunities including natural language processing, web science, and social\ncomputing. This paper surveys the work on stance detection within those\ncommunities and situates its usage within current opinion mining techniques in\nsocial media. It presents an exhaustive review of stance detection techniques\non social media, including the task definition, different types of targets in\nstance detection, features set used, and various machine learning approaches\napplied. The survey reports state-of-the-art results on the existing benchmark\ndatasets on stance detection, and discusses the most effective approaches. In\naddition, this study explores the emerging trends and different applications of\nstance detection on social media. The study concludes by discussing the gaps in\nthe current existing research and highlights the possible future directions for\nstance detection on social media.", "published": "2020-06-05 19:24:16", "link": "http://arxiv.org/abs/2006.03644v5", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Filtered Inner Product Projection for Crosslingual Embedding Alignment", "abstract": "Due to widespread interest in machine translation and transfer learning,\nthere are numerous algorithms for mapping multiple embeddings to a shared\nrepresentation space. Recently, these algorithms have been studied in the\nsetting of bilingual dictionary induction where one seeks to align the\nembeddings of a source and a target language such that translated word pairs\nlie close to one another in a common representation space. In this paper, we\npropose a method, Filtered Inner Product Projection (FIPP), for mapping\nembeddings to a common representation space and evaluate FIPP in the context of\nbilingual dictionary induction. As semantic shifts are pervasive across\nlanguages and domains, FIPP first identifies the common geometric structure in\nboth embeddings and then, only on the common structure, aligns the Gram\nmatrices of these embeddings. Unlike previous approaches, FIPP is applicable\neven when the source and target embeddings are of differing dimensionalities.\nWe show that our approach outperforms existing methods on the MUSE dataset for\nvarious language pairs. Furthermore, FIPP provides computational benefits both\nin ease of implementation and scalability.", "published": "2020-06-05 19:53:30", "link": "http://arxiv.org/abs/2006.03652v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "abstract": "Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).", "published": "2020-06-05 19:54:34", "link": "http://arxiv.org/abs/2006.03654v6", "categories": ["cs.CL", "cs.LG", "cs.CL, cs.GL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual\n  Representations", "abstract": "Sentence embeddings are an important component of many natural language\nprocessing (NLP) systems. Like word embeddings, sentence embeddings are\ntypically learned on large text corpora and then transferred to various\ndownstream tasks, such as clustering and retrieval. Unlike word embeddings, the\nhighest performing solutions for learning sentence embeddings require labelled\ndata, limiting their usefulness to languages and domains where labelled data is\nabundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for\nUnsupervised Textual Representations. Inspired by recent advances in deep\nmetric learning (DML), we carefully design a self-supervised objective for\nlearning universal sentence embeddings that does not require labelled training\ndata. When used to extend the pretraining of transformer-based language models,\nour approach closes the performance gap between unsupervised and supervised\npretraining for universal sentence encoders. Importantly, our experiments\nsuggest that the quality of the learned embeddings scale with both the number\nof trainable parameters and the amount of unlabelled training data. Our code\nand pretrained models are publicly available and can be easily adapted to new\ndomains or used to embed unseen text.", "published": "2020-06-05 20:00:28", "link": "http://arxiv.org/abs/2006.03659v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accelerating Natural Language Understanding in Task-Oriented Dialog", "abstract": "Task-oriented dialog models typically leverage complex neural architectures\nand large-scale, pre-trained Transformers to achieve state-of-the-art\nperformance on popular natural language understanding benchmarks. However,\nthese models frequently have in excess of tens of millions of parameters,\nmaking them impossible to deploy on-device where resource-efficiency is a major\nconcern. In this work, we show that a simple convolutional model compressed\nwith structured pruning achieves largely comparable results to BERT on ATIS and\nSnips, with under 100K parameters. Moreover, we perform acceleration\nexperiments on CPUs, where we observe our multi-task model predicts intents and\nslots nearly 63x faster than even DistilBERT.", "published": "2020-06-05 21:36:33", "link": "http://arxiv.org/abs/2006.03701v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relation of the Relations: A New Paradigm of the Relation Extraction\n  Problem", "abstract": "In natural language, often multiple entities appear in the same text.\nHowever, most previous works in Relation Extraction (RE) limit the scope to\nidentifying the relation between two entities at a time. Such an approach\ninduces a quadratic computation time, and also overlooks the interdependency\nbetween multiple relations, namely the relation of relations (RoR). Due to the\nsignificance of RoR in existing datasets, we propose a new paradigm of RE that\nconsiders as a whole the predictions of all relations in the same context.\nAccordingly, we develop a data-driven approach that does not require\nhand-crafted rules but learns by itself the RoR, using Graph Neural Networks\nand a relation matrix transformer. Experiments show that our model outperforms\nthe state-of-the-art approaches by +1.12\\% on the ACE05 dataset and +2.55\\% on\nSemEval 2018 Task 7.2, which is a substantial improvement on the two\ncompetitive benchmarks.", "published": "2020-06-05 22:25:27", "link": "http://arxiv.org/abs/2006.03719v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepVar: An End-to-End Deep Learning Approach for Genomic Variant\n  Recognition in Biomedical Literature", "abstract": "We consider the problem of Named Entity Recognition (NER) on biomedical\nscientific literature, and more specifically the genomic variants recognition\nin this work. Significant success has been achieved for NER on canonical tasks\nin recent years where large data sets are generally available. However, it\nremains a challenging problem on many domain-specific areas, especially the\ndomains where only small gold annotations can be obtained. In addition, genomic\nvariant entities exhibit diverse linguistic heterogeneity, differing much from\nthose that have been characterized in existing canonical NER tasks. The\nstate-of-the-art machine learning approaches in such tasks heavily rely on\narduous feature engineering to characterize those unique patterns. In this\nwork, we present the first successful end-to-end deep learning approach to\nbridge the gap between generic NER algorithms and low-resource applications\nthrough genomic variants recognition. Our proposed model can result in\npromising performance without any hand-crafted features or post-processing\nrules. Our extensive experiments and results may shed light on other similar\nlow-resource NER applications.", "published": "2020-06-05 04:39:34", "link": "http://arxiv.org/abs/2006.08338v1", "categories": ["cs.CL", "cs.LG", "J.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer Learning for COVID-19 Outbreak Alignment", "abstract": "The spread of COVID-19 has become a significant and troubling aspect of\nsociety in 2020. With millions of cases reported across countries, new\noutbreaks have occurred and followed patterns of previously affected areas.\nMany disease detection models do not incorporate the wealth of social media\ndata that can be utilized for modeling and predicting its spread. In this case,\nit is useful to ask, can we utilize this knowledge in one country to model the\noutbreak in another? To answer this, we propose the task of cross-lingual\ntransfer learning for epidemiological alignment. Utilizing both macro and micro\ntext features, we train on Italy's early COVID-19 outbreak through Twitter and\ntransfer to several other countries. Our experiments show strong results with\nup to 0.85 Spearman correlation in cross-country predictions.", "published": "2020-06-05 02:04:25", "link": "http://arxiv.org/abs/2006.03202v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Sentence Compression as Deletion with Contextual Embeddings", "abstract": "Sentence compression is the task of creating a shorter version of an input\nsentence while keeping important information. In this paper, we extend the task\nof compression by deletion with the use of contextual embeddings. Different\nfrom prior work usually using non-contextual embeddings (Glove or Word2Vec), we\nexploit contextual embeddings that enable our model capturing the context of\ninputs. More precisely, we utilize contextual embeddings stacked by\nbidirectional Long-short Term Memory and Conditional Random Fields for dealing\nwith sequence labeling. Experimental results on a benchmark Google dataset show\nthat by utilizing contextual embeddings, our model achieves a new\nstate-of-the-art F-score compared to strong methods reported on the leader\nboard.", "published": "2020-06-05 02:40:46", "link": "http://arxiv.org/abs/2006.03210v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "I.2; I.2.7; H.3; H.3.3"], "primary_category": "cs.IR"}
{"title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient\n  Language Processing", "abstract": "With the success of language pretraining, it is highly desirable to develop\nmore efficient architectures of good scalability that can exploit the abundant\nunlabeled data at a lower cost. To improve the efficiency, we examine the\nmuch-overlooked redundancy in maintaining a full-length token-level\npresentation, especially for tasks that only require a single-vector\npresentation of the sequence. With this intuition, we propose\nFunnel-Transformer which gradually compresses the sequence of hidden states to\na shorter one and hence reduces the computation cost. More importantly, by\nre-investing the saved FLOPs from length reduction in constructing a deeper or\nwider model, we further improve the model capacity. In addition, to perform\ntoken-level predictions as required by common pretraining objectives,\nFunnel-Transformer is able to recover a deep representation for each token from\nthe reduced hidden sequence via a decoder. Empirically, with comparable or\nfewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide\nvariety of sequence-level prediction tasks, including text classification,\nlanguage understanding, and reading comprehension. The code and pretrained\ncheckpoints are available at https://github.com/laiguokun/Funnel-Transformer.", "published": "2020-06-05 05:16:23", "link": "http://arxiv.org/abs/2006.03236v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Aspect-based Sentiment Analysis of Scientific Reviews", "abstract": "Scientific papers are complex and understanding the usefulness of these\npapers requires prior knowledge. Peer reviews are comments on a paper provided\nby designated experts on that field and hold a substantial amount of\ninformation, not only for the editors and chairs to make the final decision,\nbut also to judge the potential impact of the paper. In this paper, we propose\nto use aspect-based sentiment analysis of scientific reviews to be able to\nextract useful information, which correlates well with the accept/reject\ndecision.\n  While working on a dataset of close to 8k reviews from ICLR, one of the top\nconferences in the field of machine learning, we use an active learning\nframework to build a training dataset for aspect prediction, which is further\nused to obtain the aspects and sentiments for the entire dataset. We show that\nthe distribution of aspect-based sentiments obtained from a review is\nsignificantly different for accepted and rejected papers. We use the aspect\nsentiments from these reviews to make an intriguing observation, certain\naspects present in a paper and discussed in the review strongly determine the\nfinal recommendation. As a second objective, we quantify the extent of\ndisagreement among the reviewers refereeing a paper. We also investigate the\nextent of disagreement between the reviewers and the chair and find that the\ninter-reviewer disagreement may have a link to the disagreement with the chair.\nOne of the most interesting observations from this study is that reviews, where\nthe reviewer score and the aspect sentiments extracted from the review text\nwritten by the reviewer are consistent, are also more likely to be concurrent\nwith the chair's decision.", "published": "2020-06-05 07:06:01", "link": "http://arxiv.org/abs/2006.03257v1", "categories": ["cs.CL", "cs.DL", "cs.LG", "I.5.4; I.2.6; I.5.2; I.5.4; I.5.1"], "primary_category": "cs.CL"}
{"title": "GMAT: Global Memory Augmentation for Transformers", "abstract": "Transformer-based models have become ubiquitous in natural language\nprocessing thanks to their large capacity, innate parallelism and high\nperformance. The contextualizing component of a Transformer block is the\n$\\textit{pairwise dot-product}$ attention that has a large $\\Omega(L^2)$ memory\nrequirement for length $L$ sequences, limiting its ability to process long\ndocuments. This has been the subject of substantial interest recently, where\nmultiple approximations were proposed to reduce the quadratic memory\nrequirement using sparse attention matrices. In this work, we propose to\naugment sparse Transformer blocks with a dense attention-based $\\textit{global\nmemory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the\nentire input sequence to each position. Our augmentation has a manageable\n$O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior\nsparse solutions. Moreover, global memory can also be used for sequence\ncompression, by representing a long input sequence with the memory\nrepresentations only. We empirically show that our method leads to substantial\nimprovement on a range of tasks, including (a) synthetic tasks that require\nglobal reasoning, (b) masked language modeling, and (c) reading comprehension.", "published": "2020-06-05 07:50:40", "link": "http://arxiv.org/abs/2006.03274v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Classification Aware Neural Topic Model and its Application on a New\n  COVID-19 Disinformation Corpus", "abstract": "The explosion of disinformation accompanying the COVID-19 pandemic has\noverloaded fact-checkers and media worldwide, and brought a new major challenge\nto government responses worldwide. Not only is disinformation creating\nconfusion about medical science amongst citizens, but it is also amplifying\ndistrust in policy makers and governments. To help tackle this, we developed\ncomputational methods to categorise COVID-19 disinformation. The COVID-19\ndisinformation categories could be used for a) focusing fact-checking efforts\non the most damaging kinds of COVID-19 disinformation; b) guiding policy makers\nwho are trying to deliver effective public health messages and counter\neffectively COVID-19 disinformation. This paper presents: 1) a corpus\ncontaining what is currently the largest available set of manually annotated\nCOVID-19 disinformation categories; 2) a classification-aware neural topic\nmodel (CANTM) designed for COVID-19 disinformation category classification and\ntopic discovery; 3) an extensive analysis of COVID-19 disinformation categories\nwith respect to time, volume, false type, media type and origin source.", "published": "2020-06-05 10:32:18", "link": "http://arxiv.org/abs/2006.03354v2", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A New Method Towards Speech Files Local Features Investigation", "abstract": "There are a few reasons for the recent increased interest in the study of\nlocal features of speech files. It is stated that many essential features of\nthe speaker language used can appear in the form of the speech signal. The\ntraditional instruments - short Fourier transform, wavelet transform, Hadamard\ntransforms, autocorrelation, and the like can detect not all particular\nproperties of the language. In this paper, we suggest a new approach to the\nexploration of such properties. The source signal is approximated by a new one\nthat has its values taken from a finite set. Then we construct a new sequence\nof vectors of a fixed size on the base of those approximations. Examination of\nthe distribution of the produced vectors provides a new method for a\ndescription of speech files local characteristics. Finally, the developed\ntechnique is applied to the problem of the automatic distinguishing of two\nknown languages used in speech files. For this purpose, a simple neural net is\nconsumed.", "published": "2020-06-05 11:53:56", "link": "http://arxiv.org/abs/2006.03388v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sponge Examples: Energy-Latency Attacks on Neural Networks", "abstract": "The high energy costs of neural network training and inference led to the use\nof acceleration hardware such as GPUs and TPUs. While this enabled us to train\nlarge-scale neural networks in datacenters and deploy them on edge devices, the\nfocus so far is on average-case performance. In this work, we introduce a novel\nthreat vector against neural networks whose energy consumption or decision\nlatency are critical. We show how adversaries can exploit carefully crafted\n$\\boldsymbol{sponge}~\\boldsymbol{examples}$, which are inputs designed to\nmaximise energy consumption and latency.\n  We mount two variants of this attack on established vision and language\nmodels, increasing energy consumption by a factor of 10 to 200. Our attacks can\nalso be used to delay decisions where a network has critical real-time\nperformance, such as in perception for autonomous vehicles. We demonstrate the\nportability of our malicious inputs across CPUs and a variety of hardware\naccelerator chips including GPUs, and an ASIC simulator. We conclude by\nproposing a defense strategy which mitigates our attack by shifting the\nanalysis of energy consumption in hardware from an average-case to a worst-case\nperspective.", "published": "2020-06-05 14:10:09", "link": "http://arxiv.org/abs/2006.03463v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CoCon: A Self-Supervised Approach for Controlled Text Generation", "abstract": "Pretrained Transformer-based language models (LMs) display remarkable natural\nlanguage generation capabilities. With their immense potential, controlling\ntext generation of such LMs is getting attention. While there are studies that\nseek to control high-level attributes (such as sentiment and topic) of\ngenerated text, there is still a lack of more precise control over its content\nat the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to\ncontrol an LM's output text with a content input, at a fine-grained level. In\nour self-supervised approach, the CoCon block learns to help the LM complete a\npartially-observed text sequence by conditioning with content inputs that are\nwithheld from the LM. Through experiments, we show that CoCon can naturally\nincorporate target content into generated texts and control high-level text\nattributes in a zero-shot manner.", "published": "2020-06-05 16:15:46", "link": "http://arxiv.org/abs/2006.03535v3", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Based on Deep Learning: A Comparative Study", "abstract": "The study of public opinion can provide us with valuable information. The\nanalysis of sentiment on social networks, such as Twitter or Facebook, has\nbecome a powerful means of learning about the users' opinions and has a wide\nrange of applications. However, the efficiency and accuracy of sentiment\nanalysis is being hindered by the challenges encountered in natural language\nprocessing (NLP). In recent years, it has been demonstrated that deep learning\nmodels are a promising solution to the challenges of NLP. This paper reviews\nthe latest studies that have employed deep learning to solve sentiment analysis\nproblems, such as sentiment polarity. Models using term frequency-inverse\ndocument frequency (TF-IDF) and word embedding have been applied to a series of\ndatasets. Finally, a comparative study has been conducted on the experimental\nresults obtained for the different models and input features", "published": "2020-06-05 16:28:10", "link": "http://arxiv.org/abs/2006.03541v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context\n  Transformers", "abstract": "Transformer models have achieved state-of-the-art results across a diverse\nrange of domains. However, concern over the cost of training the attention\nmechanism to learn complex dependencies between distant inputs continues to\ngrow. In response, solutions that exploit the structure and sparsity of the\nlearned attention matrix have blossomed. However, real-world applications that\ninvolve long sequences, such as biological sequence analysis, may fall short of\nmeeting these assumptions, precluding exploration of these models. To address\nthis challenge, we present a new Transformer architecture, Performer, based on\nFast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales\nlinearly rather than quadratically in the number of tokens in the sequence, is\ncharacterized by sub-quadratic space complexity and does not incorporate any\nsparsity pattern priors. Furthermore, it provides strong theoretical\nguarantees: unbiased estimation of the attention matrix and uniform\nconvergence. It is also backwards-compatible with pre-trained regular\nTransformers. We demonstrate its effectiveness on the challenging task of\nprotein sequence modeling and provide detailed theoretical analysis.", "published": "2020-06-05 17:09:16", "link": "http://arxiv.org/abs/2006.03555v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Dataset and Benchmarks for Multimedia Social Analysis", "abstract": "We present a new publicly available dataset with the goal of advancing\nmulti-modality learning by offering vision and language data within the same\ncontext. This is achieved by obtaining data from a social media website with\nposts containing multiple paired images/videos and text, along with comment\ntrees containing images/videos and/or text. With a total of 677k posts, 2.9\nmillion post images, 488k post videos, 1.4 million comment images, 4.6 million\ncomment videos, and 96.9 million comments, data from different modalities can\nbe jointly used to improve performances for a variety of tasks such as image\ncaptioning, image classification, next frame prediction, sentiment analysis,\nand language modeling. We present a wide range of statistics for our dataset.\nFinally, we provide baseline performance analysis for one of the regression\ntasks using pre-trained models and several fully connected networks.", "published": "2020-06-05 11:33:01", "link": "http://arxiv.org/abs/2006.08335v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised\n  Learning", "abstract": "High-performance anti-spoofing models for automatic speaker verification\n(ASV), have been widely used to protect ASV by identifying and filtering\nspoofing audio that is deliberately generated by text-to-speech, voice\nconversion, audio replay, etc. However, it has been shown that high-performance\nanti-spoofing models are vulnerable to adversarial attacks. Adversarial\nattacks, that are indistinguishable from original data but result in the\nincorrect predictions, are dangerous for anti-spoofing models and not in\ndispute we should detect them at any cost. To explore this issue, we proposed\nto employ Mockingjay, a self-supervised learning based model, to protect\nanti-spoofing models against adversarial attacks in the black-box scenario.\nSelf-supervised learning models are effective in improving downstream task\nperformance like phone classification or ASR. However, their effect in defense\nfor adversarial attacks has not been explored yet. In this work, we explore the\nrobustness of self-supervised learned high-level representations by using them\nin the defense against adversarial attacks. A layerwise noise to signal ratio\n(LNSR) is proposed to quantize and measure the effectiveness of deep models in\ncountering adversarial noise. Experimental results on the ASVspoof 2019 dataset\ndemonstrate that high-level representations extracted by Mockingjay can prevent\nthe transferability of adversarial examples, and successfully counter black-box\nattacks.", "published": "2020-06-05 03:03:06", "link": "http://arxiv.org/abs/2006.03214v3", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Audio Captioning using Gated Recurrent Units", "abstract": "Audio captioning is a recently proposed task for automatically generating a\ntextual description of a given audio clip. In this study, a novel deep network\narchitecture with audio embeddings is presented to predict audio captions.\nWithin the aim of extracting audio features in addition to log Mel energies,\nVGGish audio embedding model is used to explore the usability of audio\nembeddings in the audio captioning task. The proposed architecture encodes\naudio and text input modalities separately and combines them before the\ndecoding stage. Audio encoding is conducted through Bi-directional Gated\nRecurrent Unit (BiGRU) while GRU is used for the text encoding phase. Following\nthis, we evaluate our model by means of the newly published audio captioning\nperformance dataset, namely Clotho, to compare the experimental results with\nthe literature. Our experimental results show that the proposed BiGRU-based\ndeep model outperforms the state of the art results.", "published": "2020-06-05 12:03:12", "link": "http://arxiv.org/abs/2006.03391v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Anomaly Detection for Machine Sounds based on Image Transfer\n  Learning", "abstract": "In industrial applications, the early detection of malfunctioning factory\nmachinery is crucial. In this paper, we consider acoustic malfunction detection\nvia transfer learning. Contrary to the majority of current approaches which are\nbased on deep autoencoders, we propose to extract features using neural\nnetworks that were pretrained on the task of image classification. We then use\nthese features to train a variety of anomaly detection models and show that\nthis improves results compared to convolutional autoencoders in recordings of\nfour different factory machines in noisy environments. Moreover, we find that\nfeatures extracted from ResNet based networks yield better results than those\nfrom AlexNet and Squeezenet. In our setting, Gaussian Mixture Models and\nOne-Class Support Vector Machines achieve the best anomaly detection\nperformance.", "published": "2020-06-05 13:29:12", "link": "http://arxiv.org/abs/2006.03429v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Adversarial Text-to-Speech", "abstract": "Modern text-to-speech synthesis pipelines typically involve multiple\nprocessing stages, each of which is designed or learnt independently from the\nrest. In this work, we take on the challenging task of learning to synthesise\nspeech from normalised text or phonemes in an end-to-end manner, resulting in\nmodels which operate directly on character or phoneme input sequences and\nproduce raw speech audio outputs. Our proposed generator is feed-forward and\nthus efficient for both training and inference, using a differentiable\nalignment scheme based on token length prediction. It learns to produce high\nfidelity audio through a combination of adversarial feedback and prediction\nlosses constraining the generated audio to roughly match the ground truth in\nterms of its total duration and mel-spectrogram. To allow the model to capture\ntemporal variation in the generated audio, we employ soft dynamic time warping\nin the spectrogram-based prediction loss. The resulting model achieves a mean\nopinion score exceeding 4 on a 5 point scale, which is comparable to the\nstate-of-the-art models relying on multi-stage training and additional\nsupervision.", "published": "2020-06-05 17:41:05", "link": "http://arxiv.org/abs/2006.03575v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
