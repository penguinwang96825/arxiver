{"title": "Yes, DLGM! A novel hierarchical model for hazard classification", "abstract": "Hazards can be exposed by HAZOP as text information, and studying their\nclassification is of great significance to the development of industrial\ninformatics, which is conducive to safety early warning, decision support,\npolicy evaluation, etc. However, there is no research on this important field\nat present. In this paper, we propose a novel model termed DLGM via deep\nlearning for hazard classification. Specifically, first, we leverage BERT to\nvectorize the hazard and treat it as a type of time series (HTS). Secondly, we\nbuild a grey model FSGM(1, 1) to model it, and get the grey guidance in the\nsense of the structural parameters. Finally, we design a hierarchical-feature\nfusion neural network (HFFNN) to investigate the HTS with grey guidance (HTSGG)\nfrom three themes, where, HFFNN is a hierarchical structure with four types of\nmodules: two feature encoders, a gating mechanism, and a deepening mechanism.\nWe take 18 industrial processes as application cases and launch a series of\nexperiments. The experimental results prove that DLGM has promising aptitudes\nfor hazard classification and that FSGM(1, 1) and HFFNN are effective. We hope\nour research can contribute added value and support to the daily practice in\nindustrial safety.", "published": "2022-09-10 02:45:59", "link": "http://arxiv.org/abs/2209.04576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPAL: Ontology-Aware Pretrained Language Model for End-to-End\n  Task-Oriented Dialogue", "abstract": "This paper presents an ontology-aware pretrained language model (OPAL) for\nend-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,\ntask-oriented dialogue models fulfill at least two task-specific modules:\ndialogue state tracker (DST) and response generator (RG). The dialogue state\nconsists of the domain-slot-value triples, which are regarded as the user's\nconstraints to search the domain-related databases. The large-scale\ntask-oriented dialogue data with the annotated structured dialogue state\nusually are inaccessible. It prevents the development of the pretrained\nlanguage model for the task-oriented dialogue. We propose a simple yet\neffective pretraining method to alleviate this problem, which consists of two\npretraining phases. The first phase is to pretrain on large-scale contextual\ntext data, where the structured information of the text is extracted by the\ninformation extracting tool. To bridge the gap between the pretraining method\nand downstream tasks, we design two pretraining tasks: ontology-like triple\nrecovery and next-text generation, which simulates the DST and RG,\nrespectively. The second phase is to fine-tune the pretrained model on the TOD\ndata. The experimental results show that our proposed method achieves an\nexciting boost and get competitive performance even without any TOD data on\nCamRest676 and MultiWOZ benchmarks.", "published": "2022-09-10 04:38:27", "link": "http://arxiv.org/abs/2209.04595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of the Differences Among Regional Varieties of Chinese in\n  Malay Archipelago", "abstract": "Chinese features prominently in the Chinese communities located in the\nnations of Malay Archipelago. In these countries, Chinese has undergone the\nprocess of adjustment to the local languages and cultures, which leads to the\noccurrence of a Chinese variant in each country. In this paper, we conducted a\nquantitative analysis on Chinese news texts collected from five Malay\nArchipelago nations, namely Indonesia, Malaysia, Singapore, Philippines and\nBrunei, trying to figure out their differences with the texts written in modern\nstandard Chinese from a lexical and syntactic perspective. The statistical\nresults show that the Chinese variants used in these five nations are quite\ndifferent, diverging from their modern Chinese mainland counterpart. Meanwhile,\nwe managed to extract and classify several featured Chinese words used in each\nnation. All these discrepancies reflect how Chinese evolves overseas, and\ndemonstrate the profound impact rom local societies and cultures on the\ndevelopment of Chinese.", "published": "2022-09-10 07:29:25", "link": "http://arxiv.org/abs/2209.04611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Abstractive Summarization for Fact-Checked Claim Detection", "abstract": "Social media platforms have become new battlegrounds for anti-social\nelements, with misinformation being the weapon of choice. Fact-checking\norganizations try to debunk as many claims as possible while staying true to\ntheir journalistic processes but cannot cope with its rapid dissemination. We\nbelieve that the solution lies in partial automation of the fact-checking life\ncycle, saving human time for tasks which require high cognition. We propose a\nnew workflow for efficiently detecting previously fact-checked claims that uses\nabstractive summarization to generate crisp queries. These queries can then be\nexecuted on a general-purpose retrieval system associated with a collection of\npreviously fact-checked claims. We curate an abstractive text summarization\ndataset comprising noisy claims from Twitter and their gold summaries. It is\nshown that retrieval performance improves 2x by using popular out-of-the-box\nsummarization models and 3x by fine-tuning them on the accompanying dataset\ncompared to verbatim querying. Our approach achieves Recall@5 and MRR of 35%\nand 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset,\ncode, and models are available publicly:\nhttps://github.com/varadhbhatnagar/FC-Claim-Det/", "published": "2022-09-10 07:32:36", "link": "http://arxiv.org/abs/2209.04612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Meta-learner via Gradient Similarity for Few-shot Text\n  Classification", "abstract": "Few-shot text classification aims to classify the text under the few-shot\nscenario. Most of the previous methods adopt optimization-based meta learning\nto obtain task distribution. However, due to the neglect of matching between\nthe few amount of samples and complicated models, as well as the distinction\nbetween useful and useless task features, these methods suffer from the\noverfitting issue. To address this issue, we propose a novel Adaptive\nMeta-learner via Gradient Similarity (AMGS) method to improve the model\ngeneralization ability to a new task. Specifically, the proposed AMGS\nalleviates the overfitting based on two aspects: (i) acquiring the potential\nsemantic representation of samples and improving model generalization through\nthe self-supervised auxiliary task in the inner loop, (ii) leveraging the\nadaptive meta-learner via gradient similarity to add constraints on the\ngradient obtained by base-learner in the outer loop. Moreover, we make a\nsystematic analysis of the influence of regularization on the entire framework.\nExperimental results on several benchmarks demonstrate that the proposed AMGS\nconsistently improves few-shot text classification performance compared with\nthe state-of-the-art optimization-based meta-learning approaches.", "published": "2022-09-10 16:14:53", "link": "http://arxiv.org/abs/2209.04702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and\n  Multi-X Perspectives", "abstract": "Irony is a ubiquitous figurative language in daily communication. Previously,\nmany researchers have approached irony from linguistic, cognitive science, and\ncomputational aspects. Recently, some progress have been witnessed in automatic\nirony processing due to the rapid development in deep neural models in natural\nlanguage processing (NLP). In this paper, we will provide a comprehensive\noverview of computational irony, insights from linguistic theory and cognitive\nscience, as well as its interactions with downstream NLP tasks and newly\nproposed multi-X irony processing perspectives.", "published": "2022-09-10 17:03:34", "link": "http://arxiv.org/abs/2209.04712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Learning-based Stance Classifier for COVID-19-related Health\n  Policies", "abstract": "The ongoing COVID-19 pandemic has caused immeasurable losses for people\nworldwide. To contain the spread of the virus and further alleviate the crisis,\nvarious health policies (e.g., stay-at-home orders) have been issued which\nspark heated discussions as users turn to share their attitudes on social\nmedia. In this paper, we consider a more realistic scenario on stance detection\n(i.e., cross-target and zero-shot settings) for the pandemic and propose an\nadversarial learning-based stance classifier to automatically identify the\npublic's attitudes toward COVID-19-related health policies. Specifically, we\nadopt adversarial learning that allows the model to train on a large amount of\nlabeled data and capture transferable knowledge from source topics, so as to\nenable generalize to the emerging health policies with sparse labeled data. To\nfurther enhance the model's deeper understanding, we incorporate policy\ndescriptions as external knowledge into the model. Meanwhile, a GeoEncoder is\ndesigned which encourages the model to capture unobserved background factors\nspecified by each region and then represent them as non-text information. We\nevaluate the performance of a broad range of baselines on the stance detection\ntask for COVID-19-related health policies, and experimental results show that\nour proposed method achieves state-of-the-art performance in both cross-target\nand zero-shot settings.", "published": "2022-09-10 10:27:21", "link": "http://arxiv.org/abs/2209.04631v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Gradient-Based Tuning of Sequence-to-Sequence\n  Models", "abstract": "Recent trends towards training ever-larger language models have substantially\nimproved machine learning performance across linguistic tasks. However, the\nhuge cost of training larger models can make tuning them prohibitively\nexpensive, motivating the study of more efficient methods. Gradient-based\nhyper-parameter optimization offers the capacity to tune hyper-parameters\nduring training, yet has not previously been studied in a sequence-to-sequence\nsetting. We apply a simple and general gradient-based hyperparameter\noptimization method to sequence-to-sequence tasks for the first time,\ndemonstrating both efficiency and performance gains over strong baselines for\nboth Neural Machine Translation and Natural Language Understanding (NLU) tasks\n(via T5 pretraining). For translation, we show the method generalizes across\nlanguage pairs, is more efficient than Bayesian hyper-parameter optimization,\nand that learned schedules for some hyper-parameters can out-perform even\noptimal constant-valued tuning. For T5, we show that learning hyper-parameters\nduring pretraining can improve performance across downstream NLU tasks. When\nlearning multiple hyper-parameters concurrently, we show that the global\nlearning rate can follow a schedule over training that improves performance and\nis not explainable by the `short-horizon bias' of greedy methods\n\\citep{wu2018}. We release the code used to facilitate further research.", "published": "2022-09-10 14:52:41", "link": "http://arxiv.org/abs/2209.04683v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Subdiffusive semantic evolution in Indo-European languages", "abstract": "How do words change their meaning? Although semantic evolution is driven by a\nvariety of distinct factors, including linguistic, societal, and technological\nones, we find that there is one law that holds universally across five major\nIndo-European languages: that semantic evolution is strongly subdiffusive.\nUsing an automated pipeline of diachronic distributional semantic embedding\nthat controls for underlying symmetries, we show that words follow stochastic\ntrajectories in meaning space with an anomalous diffusion exponent $\\alpha=\n0.45\\pm 0.05$ across languages, in contrast with diffusing particles that\nfollow $\\alpha=1$. Randomization methods indicate that preserving temporal\ncorrelations in semantic change directions is necessary to recover strongly\nsubdiffusive behavior; however, correlations in change sizes play an important\nrole too. We furthermore show that strong subdiffusion is a robust phenomenon\nunder a wide variety of choices in data analysis and interpretation, such as\nthe choice of fitting an ensemble average of displacements or averaging\nbest-fit exponents of individual word trajectories.", "published": "2022-09-10 15:57:32", "link": "http://arxiv.org/abs/2209.04701v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Anticipating the Unseen Discrepancy for Vision and Language Navigation", "abstract": "Vision-Language Navigation requires the agent to follow natural language\ninstructions to reach a specific target. The large discrepancy between seen and\nunseen environments makes it challenging for the agent to generalize well.\nPrevious studies propose data augmentation methods to mitigate the data bias\nexplicitly or implicitly and provide improvements in generalization. However,\nthey try to memorize augmented trajectories and ignore the distribution shifts\nunder unseen environments at test time. In this paper, we propose an Unseen\nDiscrepancy Anticipating Vision and Language Navigation (DAVIS) that learns to\ngeneralize to unseen environments via encouraging test-time visual consistency.\nSpecifically, we devise: 1) a semi-supervised framework DAVIS that leverages\nvisual consistency signals across similar semantic observations. 2) a two-stage\nlearning procedure that encourages adaptation to test-time distribution. The\nframework enhances the basic mixture of imitation and reinforcement learning\nwith Momentum Contrast to encourage stable decision-making on similar\nobservations under a joint training stage and a test-time adaptation stage.\nExtensive experiments show that DAVIS achieves model-agnostic improvement over\nprevious state-of-the-art VLN baselines on R2R and RxR benchmarks. Our source\ncode and data are in supplemental materials.", "published": "2022-09-10 19:04:40", "link": "http://arxiv.org/abs/2209.04725v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OmDet: Large-scale vision-language multi-dataset pre-training with\n  multimodal detection network", "abstract": "The advancement of object detection (OD) in open-vocabulary and open-world\nscenarios is a critical challenge in computer vision. This work introduces\nOmDet, a novel language-aware object detection architecture, and an innovative\ntraining mechanism that harnesses continual learning and multi-dataset\nvision-language pre-training. Leveraging natural language as a universal\nknowledge representation, OmDet accumulates a \"visual vocabulary\" from diverse\ndatasets, unifying the task as a language-conditioned detection framework. Our\nmultimodal detection network (MDN) overcomes the challenges of multi-dataset\njoint training and generalizes to numerous training datasets without manual\nlabel taxonomy merging. We demonstrate superior performance of OmDet over\nstrong baselines in object detection in the wild, open-vocabulary detection,\nand phrase grounding, achieving state-of-the-art results. Ablation studies\nreveal the impact of scaling the pre-training visual vocabulary, indicating a\npromising direction for further expansion to larger datasets. The effectiveness\nof our deep fusion approach is underscored by its ability to learn jointly from\nmultiple datasets, enhancing performance through knowledge sharing.", "published": "2022-09-10 14:25:14", "link": "http://arxiv.org/abs/2209.05946v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Identifying epidemic related Tweets using noisy learning", "abstract": "Supervised learning algorithms are heavily reliant on annotated datasets to\ntrain machine learning models. However, the curation of the annotated datasets\nis laborious and time consuming due to the manual effort involved and has\nbecome a huge bottleneck in supervised learning. In this work, we apply the\ntheory of noisy learning to generate weak supervision signals instead of manual\nannotation. We curate a noisy labeled dataset using a labeling heuristic to\nidentify epidemic related tweets. We evaluated the performance using a large\nepidemic corpus and our results demonstrate that models trained with noisy data\nin a class imbalanced and multi-classification weak supervision setting\nachieved performance greater than 90%.", "published": "2022-09-10 18:06:23", "link": "http://arxiv.org/abs/2209.12614v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of Question Answering Systems: Complexity of judging a\n  natural language", "abstract": "Question answering (QA) systems are among the most important and rapidly\ndeveloping research topics in natural language processing (NLP). A reason,\ntherefore, is that a QA system allows humans to interact more naturally with a\nmachine, e.g., via a virtual assistant or search engine. In the last decades,\nmany QA systems have been proposed to address the requirements of different\nquestion-answering tasks. Furthermore, many error scores have been introduced,\ne.g., based on n-gram matching, word embeddings, or contextual embeddings to\nmeasure the performance of a QA system. This survey attempts to provide a\nsystematic overview of the general framework of QA, QA paradigms, benchmark\ndatasets, and assessment techniques for a quantitative evaluation of QA\nsystems. The latter is particularly important because not only is the\nconstruction of a QA system complex but also its evaluation. We hypothesize\nthat a reason, therefore, is that the quantitative formalization of human\njudgment is an open problem.", "published": "2022-09-10 12:29:04", "link": "http://arxiv.org/abs/2209.12617v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pay Attention to Hard Trials", "abstract": "Performance of speaker recognition systems is evaluated on test trials.\nAlthough as crucial as rulers for tailors, trials have not been carefully\ntreated so far, and most existing benchmarks compose trials by naive\ncross-pairing. In this paper, we argue that the cross-pairing approach produces\noverwhelming easy trials, which in turn leads to potential bias in system and\ntechnique comparison. To solve the problem, we advocate more attention to hard\ntrials. We present an SVM-based approach to identifying hard trials and use it\nto construct new evaluation sets for VoxCeleb1 and SITW. With the new sets, we\ncan re-evaluate the contribution of some recent technologies. The code and the\nidentified hard trials will be published online at http://project.cslt.org.", "published": "2022-09-10 15:16:05", "link": "http://arxiv.org/abs/2209.04687v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
