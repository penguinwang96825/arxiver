{"title": "Towards Automatic Construction of Filipino WordNet: Word Sense Induction\n  and Synset Induction Using Sentence Embeddings", "abstract": "Wordnets are indispensable tools for various natural language processing\napplications. Unfortunately, wordnets get outdated, and producing or updating\nwordnets can be slow and costly in terms of time and resources. This problem\nintensifies for low-resource languages. This study proposes a method for word\nsense induction and synset induction using only two linguistic resources,\nnamely, an unlabeled corpus and a sentence embeddings-based language model. The\nresulting sense inventory and synonym sets can be used in automatically\ncreating a wordnet. We applied this method on a corpus of Filipino text. The\nsense inventory and synsets were evaluated by matching them with the sense\ninventory of the machine translated Princeton WordNet, as well as comparing the\nsynsets to the Filipino WordNet. This study empirically shows that the 30% of\nthe induced word senses are valid and 40% of the induced synsets are valid in\nwhich 20% are novel synsets.", "published": "2022-04-07 06:50:37", "link": "http://arxiv.org/abs/2204.03251v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entailment Graph Learning with Textual Entailment and Soft Transitivity", "abstract": "Typed entailment graphs try to learn the entailment relations between\npredicates from text and model them as edges between predicate nodes. The\nconstruction of entailment graphs usually suffers from severe sparsity and\nunreliability of distributional similarity. We propose a two-stage method,\nEntailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns\nlocal entailment relations by recognizing possible textual entailment between\ntemplate sentences formed by typed CCG-parsed predicates. Based on the\ngenerated local graph, EGT2 then uses three novel soft transitivity constraints\nto consider the logical transitivity in entailment structures. Experiments on\nbenchmark datasets show that EGT2 can well model the transitivity in entailment\ngraph to alleviate the sparsity issue, and lead to significant improvement over\ncurrent state-of-the-art methods.", "published": "2022-04-07 08:33:06", "link": "http://arxiv.org/abs/2204.03286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fair Evaluation of Dialogue State Tracking by Flexible\n  Incorporation of Turn-level Performances", "abstract": "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal\nAccuracy (JGA) defined as the fraction of turns where the ground-truth dialogue\nstate exactly matches the prediction. Generally in DST, the dialogue state or\nbelief state for a given turn contains all the intents shown by the user till\nthat turn. Due to this cumulative nature of the belief state, it is difficult\nto get a correct prediction once a misprediction has occurred. Thus, although\nbeing a useful metric, it can be harsh at times and underestimate the true\npotential of a DST model. Moreover, an improvement in JGA can sometimes\ndecrease the performance of turn-level or non-cumulative belief state\nprediction due to inconsistency in annotations. So, using JGA as the only\nmetric for model selection may not be ideal for all scenarios. In this work, we\ndiscuss various evaluation metrics used for DST along with their shortcomings.\nTo address the existing issues, we propose a new evaluation metric named\nFlexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike\nJGA, it tries to give penalized rewards to mispredictions that are locally\ncorrect i.e. the root cause of the error is an earlier turn. By doing so, FGA\nconsiders the performance of both cumulative and turn-level prediction flexibly\nand provides a better insight than the existing metrics. We also show that FGA\nis a better discriminator of DST model performance.", "published": "2022-04-07 11:52:11", "link": "http://arxiv.org/abs/2204.03375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment\n  Analysis Systems in English, Spanish, and Arabic", "abstract": "As natural language processing systems become more widespread, it is\nnecessary to address fairness issues in their implementation and deployment to\nensure that their negative impacts on society are understood and minimized.\nHowever, there is limited work that studies fairness using a multilingual and\nintersectional framework or on downstream tasks. In this paper, we introduce\nfour multilingual Equity Evaluation Corpora, supplementary test sets designed\nto measure social biases, and a novel statistical framework for studying\nunisectional and intersectional social biases in natural language processing.\nWe use these tools to measure gender, racial, ethnic, and intersectional social\nbiases across five models trained on emotion regression tasks in English,\nSpanish, and Arabic. We find that many systems demonstrate statistically\nsignificant unisectional and intersectional social biases.", "published": "2022-04-07 16:33:15", "link": "http://arxiv.org/abs/2204.03558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "tmVar 3.0: an improved variant concept recognition and normalization\n  tool", "abstract": "Previous studies have shown that automated text-mining tools are becoming\nincreasingly important for successfully unlocking variant information in\nscientific literature at large scale. Despite multiple attempts in the past,\nexisting tools are still of limited recognition scope and precision. We propose\ntmVar 3.0: an improved variant recognition and normalization tool. Compared to\nits predecessors, tmVar 3.0 is able to recognize a wide spectrum of variant\nrelated entities (e.g., allele and copy number variants), and to group\ndifferent variant mentions belonging to the same concept in an article for\nimproved accuracy. Moreover, tmVar3 provides additional variant normalization\noptions such as allele-specific identifiers from the ClinGen Allele Registry.\ntmVar3 exhibits a state-of-the-art performance with over 90% accuracy in\nF-measure in variant recognition and normalization, when evaluated on three\nindependent benchmarking datasets. tmVar3 is freely available for download. We\nhave also processed the entire PubMed and PMC with tmVar3 and released its\nannotations on our FTP. Availability: ftp://ftp.ncbi.nlm.nih.gov/pub/lu/tmVar3", "published": "2022-04-07 17:58:56", "link": "http://arxiv.org/abs/2204.03637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop\n  Iterative Text Revision", "abstract": "Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.", "published": "2022-04-07 18:33:10", "link": "http://arxiv.org/abs/2204.03685v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Winoground: Probing Vision and Language Models for Visio-Linguistic\n  Compositionality", "abstract": "We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.", "published": "2022-04-07 02:17:05", "link": "http://arxiv.org/abs/2204.03162v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal\n  Generators", "abstract": "We present a new framework AMOS that pretrains text encoders with an\nAdversarial learning curriculum via a Mixture Of Signals from multiple\nauxiliary generators. Following ELECTRA-style pretraining, the main encoder is\ntrained as a discriminator to detect replaced tokens generated by auxiliary\nmasked language models (MLMs). Different from ELECTRA which trains one MLM as\nthe generator, we jointly train multiple MLMs of different sizes to provide\ntraining signals at various levels of difficulty. To push the discriminator to\nlearn better with challenging replaced tokens, we learn mixture weights over\nthe auxiliary MLMs' outputs to maximize the discriminator loss by\nbackpropagating the gradient from the discriminator via Gumbel-Softmax. For\nbetter pretraining efficiency, we propose a way to assemble multiple MLMs into\none unified auxiliary model. AMOS outperforms ELECTRA and recent\nstate-of-the-art pretrained models by about 1 point on the GLUE benchmark for\nBERT base-sized models.", "published": "2022-04-07 06:19:06", "link": "http://arxiv.org/abs/2204.03243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Korean Online Hate Speech Dataset for Multilabel Classification: How Can\n  Social Science Improve Dataset on Hate Speech?", "abstract": "We suggest a multilabel Korean online hate speech dataset that covers seven\ncategories of hate speech: (1) Race and Nationality, (2) Religion, (3)\nRegionalism, (4) Ageism, (5) Misogyny, (6) Sexual Minorities, and (7) Male. Our\n35K dataset consists of 24K online comments with Krippendorff's Alpha label\naccordance of .713, 2.2K neutral sentences from Wikipedia, 1.7K additionally\nlabeled sentences generated by the Human-in-the-Loop procedure and\nrule-generated 7.1K neutral sentences. The base model with 24K initial dataset\nachieved the accuracy of LRAP .892, but improved to .919 after being combined\nwith 11K additional data. Unlike the conventional binary hate and non-hate\ndichotomy approach, we designed a dataset considering both the cultural and\nlinguistic context to overcome the limitations of western culture-based English\ntexts. Thus, this paper is not only limited to presenting a local hate speech\ndataset but extends as a manual for building a more generalized hate speech\ndataset with diverse cultural backgrounds based on social science perspectives.", "published": "2022-04-07 07:29:06", "link": "http://arxiv.org/abs/2204.03262v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PALBERT: Teaching ALBERT to Ponder", "abstract": "Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\nin wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layer's\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from the $i$-th layer, introduces major variance in exit layer indices,\nsignificantly reducing the resulting model's performance. In this paper, we\npropose improving PonderNet with a novel deterministic Q-exit criterion and a\nrevisited model architecture. We adapted the proposed mechanism to ALBERT and\nRoBERTa and compared it with recent methods for performing an early exit. We\nobserved that the proposed changes can be considered significant improvements\non the original PonderNet architecture and outperform PABEE on a wide range of\nGLUE tasks. In addition, we also performed an in-depth ablation study of the\nproposed architecture to further understand Lambda layers and their\nperformance.", "published": "2022-04-07 08:01:13", "link": "http://arxiv.org/abs/2204.03276v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sequence-Based Extractive Summarisation for Scientific Articles", "abstract": "This paper presents the results of research on supervised extractive text\nsummarisation for scientific articles. We show that a simple sequential tagging\nmodel based only on the text within a document achieves high results against a\nsimple classification model. Improvements can be achieved through additional\nsentence-level features, though these were minimal. Through further analysis,\nwe show the potential of the sequential model relying on the structure of the\ndocument depending on the academic discipline which the document is from.", "published": "2022-04-07 09:09:33", "link": "http://arxiv.org/abs/2204.03301v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Autoencoding Language Model Based Ensemble Learning for Commonsense\n  Validation and Explanation", "abstract": "An ultimate goal of artificial intelligence is to build computer systems that\ncan understand human languages. Understanding commonsense knowledge about the\nworld expressed in text is one of the foundational and challenging problems to\ncreate such intelligent systems. As a step towards this goal, we present in\nthis paper ALMEn, an Autoencoding Language Model based Ensemble learning method\nfor commonsense validation and explanation. By ensembling several advanced\npre-trained language models including RoBERTa, DeBERTa, and ELECTRA with\nSiamese neural networks, our method can distinguish natural language statements\nthat are against commonsense (validation subtask) and correctly identify the\nreason for making against commonsense (explanation selection subtask).\nExperimental results on the benchmark dataset of SemEval-2020 Task 4 show that\nour method outperforms state-of-the-art models, reaching 97.9% and 95.4%\naccuracies on the validation and explanation selection subtasks, respectively.", "published": "2022-04-07 09:43:51", "link": "http://arxiv.org/abs/2204.03324v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Abstractive Question Answering over Tables or Text", "abstract": "A long-term ambition of information seeking QA systems is to reason over\nmulti-modal contexts and generate natural answers to user queries. Today,\nmemory intensive pre-trained language models are adapted to downstream tasks\nsuch as QA by fine-tuning the model on QA data in a specific modality like\nunstructured text or structured tables. To avoid training such memory-hungry\nmodels while utilizing a uniform architecture for each modality,\nparameter-efficient adapters add and train small task-specific bottle-neck\nlayers between transformer layers. In this work, we study parameter-efficient\nabstractive QA in encoder-decoder models over structured tabular data and\nunstructured textual data using only 1.5% additional parameters for each\nmodality. We also ablate over adapter layers in both encoder and decoder\nmodules to study the efficiency-performance trade-off and demonstrate that\nreducing additional trainable parameters down to 0.7%-1.0% leads to comparable\nresults. Our models out-perform current state-of-the-art models on tabular QA\ndatasets such as Tablesum and FeTaQA, and achieve comparable performance on a\ntextual QA dataset such as NarrativeQA using significantly less trainable\nparameters than fine-tuning.", "published": "2022-04-07 10:56:29", "link": "http://arxiv.org/abs/2204.03357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERTuit: Understanding Spanish language in Twitter through a native\n  transformer", "abstract": "The appearance of complex attention-based language models such as BERT,\nRoberta or GPT-3 has allowed to address highly complex tasks in a plethora of\nscenarios. However, when applied to specific domains, these models encounter\nconsiderable difficulties. This is the case of Social Networks such as Twitter,\nan ever-changing stream of information written with informal and complex\nlanguage, where each message requires careful evaluation to be understood even\nby humans given the important role that context plays. Addressing tasks in this\ndomain through Natural Language Processing involves severe challenges. When\npowerful state-of-the-art multilingual language models are applied to this\nscenario, language specific nuances use to get lost in translation. To face\nthese challenges we present \\textbf{BERTuit}, the larger transformer proposed\nso far for Spanish language, pre-trained on a massive dataset of 230M Spanish\ntweets using RoBERTa optimization. Our motivation is to provide a powerful\nresource to better understand Spanish Twitter and to be used on applications\nfocused on this social network, with special emphasis on solutions devoted to\ntackle the spreading of misinformation in this platform. BERTuit is evaluated\non several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very\ncompetitive multilingual transformers. The utility of our approach is shown\nwith applications, in this case: a zero-shot methodology to visualize groups of\nhoaxes and profiling authors spreading disinformation.\n  Misinformation spreads wildly on platforms such as Twitter in languages other\nthan English, meaning performance of transformers may suffer when transferred\noutside English speaking communities.", "published": "2022-04-07 14:28:51", "link": "http://arxiv.org/abs/2204.03465v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Multi-task Learning in Natural Language Processing:\n  Regarding Task Relatedness and Training Methods", "abstract": "Multi-task learning (MTL) has become increasingly popular in natural language\nprocessing (NLP) because it improves the performance of related tasks by\nexploiting their commonalities and differences. Nevertheless, it is still not\nunderstood very well how multi-task learning can be implemented based on the\nrelatedness of training tasks. In this survey, we review recent advances of\nmulti-task learning methods in NLP, with the aim of summarizing them into two\ngeneral multi-task training methods based on their task relatedness: (i) joint\ntraining and (ii) multi-step training. We present examples in various NLP\ndownstream applications, summarize the task relationships and discuss future\ndirections of this promising topic.", "published": "2022-04-07 15:22:19", "link": "http://arxiv.org/abs/2204.03508v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Label Correlations for Second-Order Semantic Dependency Parsing\n  with Mean-Field Inference", "abstract": "Second-order semantic parsing with end-to-end mean-field inference has been\nshown good performance. In this work we aim to improve this method by modeling\nlabel correlations between adjacent arcs. However, direct modeling leads to\nmemory explosion because second-order score tensors have sizes of $O(n^3L^2)$\n($n$ is the sentence length and $L$ is the number of labels), which is not\naffordable. To tackle this computational challenge, we leverage tensor\ndecomposition techniques, and interestingly, we show that the large\nsecond-order score tensors have no need to be materialized during mean-field\ninference, thereby reducing the computational complexity from cubic to\nquadratic. We conduct experiments on SemEval 2015 Task 18 English datasets,\nshowing the effectiveness of modeling label correlations. Our code is publicly\navailable at https://github.com/sustcsonglin/mean-field-dep-parsing.", "published": "2022-04-07 17:40:08", "link": "http://arxiv.org/abs/2204.03619v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LSTM-RASA Based Agri Farm Assistant for Farmers", "abstract": "The application of Deep Learning and Natural Language based ChatBots are\ngrowing rapidly in recent years. They are used in many fields like customer\nsupport, reservation system and as personal assistant. The Enterprises are\nusing such ChatBots to serve their customers in a better and efficient manner.\nEven after such technological advancement, the expert advice does not reach the\nfarmers on timely manner. The farmers are still largely dependent on their\npeers knowledge in solving the problems they face in their field. These\ntechnologies have not been effectively used to give the required information to\nfarmers on timely manner. This project aims to implement a closed domain\nChatBot for the field of Agriculture Farmers Assistant. Farmers can have\nconversation with the Chatbot and get the expert advice in their field. Farmers\nAssistant is based on RASA Open Source Framework. The Chatbot identifies the\nintent and entity from user utterances and retrieve the remedy from the\ndatabase and share it with the user. We tested the Bot with existing data and\nit showed promising results.", "published": "2022-04-07 11:01:54", "link": "http://arxiv.org/abs/2204.09717v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech\n  recognition", "abstract": "Recently, Conformer based CTC/AED model has become a mainstream architecture\nfor ASR. In this paper, based on our prior work, we identify and integrate\nseveral approaches to achieve further improvements for ASR tasks, which we\ndenote as multi-loss, multi-path and multi-level, summarized as \"3M\" model.\nSpecifically, multi-loss refers to the joint CTC/AED loss and multi-path\ndenotes the Mixture-of-Experts(MoE) architecture which can effectively increase\nthe model capacity without remarkably increasing computation cost. Multi-level\nmeans that we introduce auxiliary loss at multiple level of a deep model to\nhelp training. We evaluate our proposed method on the public WenetSpeech\ndataset and experimental results show that the proposed method provides\n12.2%-17.6% relative CER improvement over the baseline model trained by Wenet\ntoolkit. On our large scale dataset of 150k hours corpus, the 3M model has also\nshown obvious superiority over the baseline Conformer model. Code is publicly\navailable at https://github.com/tencent-ailab/3m-asr.", "published": "2022-04-07 03:10:49", "link": "http://arxiv.org/abs/2204.03178v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Joint Learning Approach for Semi-supervised Neural Topic Modeling", "abstract": "Topic models are some of the most popular ways to represent textual data in\nan interpret-able manner. Recently, advances in deep generative models,\nspecifically auto-encoding variational Bayes (AEVB), have led to the\nintroduction of unsupervised neural topic models, which leverage deep\ngenerative models as opposed to traditional statistics-based topic models. We\nextend upon these neural topic models by introducing the Label-Indexed Neural\nTopic Model (LI-NTM), which is, to the extent of our knowledge, the first\neffective upstream semi-supervised neural topic model. We find that LI-NTM\noutperforms existing neural topic models in document reconstruction benchmarks,\nwith the most notable results in low labeled data regimes and for data-sets\nwith informative labels; furthermore, our jointly learned classifier\noutperforms baseline classifiers in ablation studies.", "published": "2022-04-07 04:42:17", "link": "http://arxiv.org/abs/2204.03208v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning", "abstract": "Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (<0.2% degradation)", "published": "2022-04-07 05:31:13", "link": "http://arxiv.org/abs/2204.03227v3", "categories": ["cs.CL", "cs.AR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Arabic Text-To-Speech (TTS) Data Preparation", "abstract": "People may be puzzled by the fact that voice over recordings data sets exist\nin addition to Text-to-Speech (TTS), Synthesis system advancements, albeit this\nis not the case. The goal of this study is to explain the relevance of TTS as\nwell as the data preparation procedures. TTS relies heavily on recorded data\nsince it can have a substantial influence on the outcomes of TTS modules.\nFurthermore, whether the domain is specialized or general, appropriate data\nshould be developed to address all predicted language variants and domains.\nDifferent recording methodologies, taking into account quality and behavior,\nmay also be advantageous in the development of the module. In light of the lack\nof Arabic language in present synthesizing systems, numerous variables that\nimpact the flow of recorded utterances are being considered in order to\nmanipulate an Arabic TTS module. In this study, two viewpoints will be\ndiscussed: linguistics and the creation of high-quality recordings for TTS. The\npurpose of this work is to offer light on how ground-truth utterances may\ninfluence the evolution of speech systems in terms of naturalness,\nintelligibility, and understanding. Well provide voice actor specs as well as\ndata specs that will assist both voice actors and voice coaches in the studio\nas well as the annotators who will be evaluating the audios.", "published": "2022-04-07 06:58:03", "link": "http://arxiv.org/abs/2204.03255v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Three-Module Modeling For End-to-End Spoken Language Understanding Using\n  Pre-trained DNN-HMM-Based Acoustic-Phonetic Model", "abstract": "In spoken language understanding (SLU), what the user says is converted to\nhis/her intent. Recent work on end-to-end SLU has shown that accuracy can be\nimproved via pre-training approaches. We revisit ideas presented by Lugosch et\nal. using speech pre-training and three-module modeling; however, to ease\nconstruction of the end-to-end SLU model, we use as our phoneme module an\nopen-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech\nrecognition (ASR) system instead of training one from scratch. Hence we\nfine-tune on speech only for the word module, and we apply multi-target\nlearning (MTL) on the word and intent modules to jointly optimize SLU\nperformance. MTL yields a relative reduction of 40% in intent-classification\nerror rates (from 1.0% to 0.6%). Note that our three-module model is a\nstreaming method. The final outcome of the proposed three-module modeling\napproach yields an intent accuracy of 99.4% on FluentSpeech, an intent error\nrate reduction of 50% compared to that of Lugosch et al. Although we focus on\nreal-time streaming methods, we also list non-streaming methods for comparison.", "published": "2022-04-07 09:26:16", "link": "http://arxiv.org/abs/2204.03315v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Review of Sign Language Recognition: Different Types,\n  Modalities, and Datasets", "abstract": "A machine can understand human activities, and the meaning of signs can help\novercome the communication barriers between the inaudible and ordinary people.\nSign Language Recognition (SLR) is a fascinating research area and a crucial\ntask concerning computer vision and pattern recognition. Recently, SLR usage\nhas increased in many applications, but the environment, background image\nresolution, modalities, and datasets affect the performance a lot. Many\nresearchers have been striving to carry out generic real-time SLR models. This\nreview paper facilitates a comprehensive overview of SLR and discusses the\nneeds, challenges, and problems associated with SLR. We study related works\nabout manual and non-manual, various modalities, and datasets. Research\nprogress and existing state-of-the-art SLR models over the past decade have\nbeen reviewed. Finally, we find the research gap and limitations in this domain\nand suggest future directions. This review paper will be helpful for readers\nand researchers to get complete guidance about SLR and the progressive design\nof the state-of-the-art SLR model", "published": "2022-04-07 09:49:12", "link": "http://arxiv.org/abs/2204.03328v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MAESTRO: Matched Speech Text Representations through Modality Matching", "abstract": "We present Maestro, a self-supervised training method to unify\nrepresentations learnt from speech and text modalities. Self-supervised\nlearning from speech signals aims to learn the latent structure inherent in the\nsignal, while self-supervised learning from text attempts to capture lexical\ninformation. Learning aligned representations from unpaired speech and text\nsequences is a challenging task. Previous work either implicitly enforced the\nrepresentations learnt from these two modalities to be aligned in the latent\nspace through multitasking and parameter sharing or explicitly through\nconversion of modalities via speech synthesis. While the former suffers from\ninterference between the two modalities, the latter introduces additional\ncomplexity. In this paper, we propose Maestro, a novel algorithm to learn\nunified representations from both these modalities simultaneously that can\ntransfer to diverse downstream tasks such as Automated Speech Recognition (ASR)\nand Speech Translation (ST). Maestro learns unified representations through\nsequence alignment, duration prediction and matching embeddings in the learned\nspace through an aligned masked-language model loss. We establish a new\nstate-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative\nreduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)\nand 21 languages to English multilingual ST on CoVoST 2 with an improvement of\n2.8 BLEU averaged over 21 languages.", "published": "2022-04-07 12:48:16", "link": "http://arxiv.org/abs/2204.03409v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0", "abstract": "Stuttering is a varied speech disorder that harms an individual's\ncommunication ability. Persons who stutter (PWS) often use speech therapy to\ncope with their condition. Improving speech recognition systems for people with\nsuch non-typical speech or tracking the effectiveness of speech therapy would\nrequire systems that can detect dysfluencies while at the same time being able\nto detect speech techniques acquired in therapy. This paper shows that\nfine-tuning wav2vec 2.0 [1] for the classification of stuttering on a sizeable\nEnglish corpus containing stuttered speech, in conjunction with multi-task\nlearning, boosts the effectiveness of the general-purpose wav2vec 2.0 features\nfor detecting stuttering in speech; both within and across languages. We\nevaluate our method on FluencyBank , [2] and the German therapy-centric Kassel\nState of Fluency (KSoF) [3] dataset by training Support Vector Machine\nclassifiers using features extracted from the finetuned models for six\ndifferent stuttering-related event types: blocks, prolongations, sound\nrepetitions, word repetitions, interjections, and - specific to therapy -\nspeech modifications. Using embeddings from the fine-tuned models leads to\nrelative classification performance gains up to 27% w.r.t. F1-score.", "published": "2022-04-07 13:02:12", "link": "http://arxiv.org/abs/2204.03417v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Vocal Fatigue with Neural Embeddings", "abstract": "Vocal fatigue refers to the feeling of tiredness and weakness of voice due to\nextended utilization. This paper investigates the effectiveness of neural\nembeddings for the detection of vocal fatigue. We compare x-vectors,\nECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.\nLow-dimensional mappings of the data reveal that neural embeddings capture\ninformation about the change in vocal characteristics of a speaker during\nprolonged voice usage. We show that vocal fatigue can be reliably predicted\nusing all three kinds of neural embeddings after only 50 minutes of continuous\nspeaking when temporal smoothing and normalization are applied to the extracted\nembeddings. We employ support vector machines for classification and achieve\naccuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and\n82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score\nof 76%, when the trained system is applied to a different speaker and recording\nenvironment without any adaptation.", "published": "2022-04-07 13:18:05", "link": "http://arxiv.org/abs/2204.03428v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning", "abstract": "We introduce compositional soft prompting (CSP), a parameter-efficient\nlearning technique to improve the zero-shot compositionality of large-scale\npretrained vision-language models (VLMs) like CLIP. We develop CSP for\ncompositional zero-shot learning, the task of predicting unseen\nattribute-object compositions (e.g., old cat and young tiger). VLMs have a\nflexible text encoder that can represent arbitrary classes as natural language\nprompts but they often underperform task-specific architectures on the\ncompositional zero-shot benchmark datasets. CSP treats the attributes and\nobjects that define classes as learnable tokens of vocabulary. During training,\nthe vocabulary is tuned to recognize classes that compose tokens in multiple\nways (e.g., old cat and white cat). At test time, we recompose the learned\nattribute-object vocabulary in new combinations to recognize novel classes. We\nshow that CSP outperforms the CLIP on benchmark datasets by an average of 10.9\npercentage points on AUC. CSP also outperforms CoOp, a soft prompting method\nthat fine-tunes the prefix context tokens, by an average of 5.8 percentage\npoints on AUC. We perform additional experiments to show that CSP improves\ngeneralization to higher-order attribute-attribute-object compositions (e.g.,\nold white cat) and combinations of pretrained attributes and fine-tuned\nobjects. The code is available at https://github.com/BatsResearch/csp.", "published": "2022-04-07 16:51:12", "link": "http://arxiv.org/abs/2204.03574v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Testing the limits of natural language models for predicting human\n  language judgments", "abstract": "Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.", "published": "2022-04-07 17:12:57", "link": "http://arxiv.org/abs/2204.03592v3", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "MHMS: Multimodal Hierarchical Multimedia Summarization", "abstract": "Multimedia summarization with multimodal output can play an essential role in\nreal-world applications, i.e., automatically generating cover images and titles\nfor news articles or providing introductions to online videos. In this work, we\npropose a multimodal hierarchical multimedia summarization (MHMS) framework by\ninteracting visual and language domains to generate both video and textual\nsummaries. Our MHMS method contains video and textual segmentation and\nsummarization module, respectively. It formulates a cross-domain alignment\nobjective with optimal transport distance which leverages cross-domain\ninteraction to generate the representative keyframe and textual summary. We\nevaluated MHMS on three recent multimodal datasets and demonstrated the\neffectiveness of our method in producing high-quality multimodal summaries.", "published": "2022-04-07 21:00:40", "link": "http://arxiv.org/abs/2204.03734v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Matching Writers to Content Writing Tasks", "abstract": "Businesses need content. In various forms and formats and for varied\npurposes. In fact, the content marketing industry is set to be worth $412.88\nbillion by the end of 2021. However, according to the Content Marketing\nInstitute, creating engaging content is the #1 challenge that marketers face\ntoday. We under-stand that producing great content requires great writers who\nunderstand the business and can weave their message into reader (and search\nengine) friendly content. In this project, the team has attempted to bridge the\ngap between writers and projects by using AI and ML tools. We used NLP\ntechniques to analyze thou-sands of publicly available business articles\n(corpora) to extract various defining factors for each writing sample. Through\nthis project we aim to automate the highly time-consuming, and often biased\ntask of manually shortlisting the most suitable writer for a given content\nwriting requirement. We believe that a tool like this will have far reaching\npositive implications for both parties - businesses looking for suitable talent\nfor niche writing jobs as well as experienced writers and Subject Matter\nExperts (SMEs) wanting to lend their services to content marketing projects.\nThe business gets the content they need, the content writer/ SME gets a chance\nto leverage his or her talent, while the reader gets authentic content that\nadds real value.", "published": "2022-04-07 12:53:17", "link": "http://arxiv.org/abs/2204.09718v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Music-robust Automatic Lyrics Transcription of Polyphonic Music", "abstract": "Lyrics transcription of polyphonic music is challenging because singing\nvocals are corrupted by the background music. To improve the robustness of\nlyrics transcription to the background music, we propose a strategy of\ncombining the features that emphasize the singing vocals, i.e. music-removed\nfeatures that represent singing vocal extracted features, and the features that\ncapture the singing vocals as well as the background music, i.e. music-present\nfeatures. We show that these two sets of features complement each other, and\ntheir combination performs better than when they are used alone, thus improving\nthe robustness of the acoustic model to the background music. Furthermore,\nlanguage model interpolation between a general-purpose language model and an\nin-domain lyrics-specific language model provides further improvement in\ntranscription results. Our experiments show that our proposed strategy\noutperforms the existing lyrics transcription systems for polyphonic music.\nMoreover, we find that our proposed music-robust features specially improve the\nlyrics transcription performance in metal genre of songs, where the background\nmusic is loud and dominant.", "published": "2022-04-07 09:14:58", "link": "http://arxiv.org/abs/2204.03306v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Boosting Self-Supervised Embeddings for Speech Enhancement", "abstract": "Self-supervised learning (SSL) representation for speech has achieved\nstate-of-the-art (SOTA) performance on several downstream tasks. However, there\nremains room for improvement in speech enhancement (SE) tasks. In this study,\nwe used a cross-domain feature to solve the problem that SSL embeddings may\nlack fine-grained information to regenerate speech signals. By integrating the\nSSL representation and spectrogram, the result can be significantly boosted. We\nfurther study the relationship between the noise robustness of SSL\nrepresentation via clean-noisy distance (CN distance) and the layer importance\nfor SE. Consequently, we found that SSL representations with lower noise\nrobustness are more important. Furthermore, our experiments on the VCTK-DEMAND\ndataset demonstrated that fine-tuning an SSL representation with an SE model\ncan outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without\ninvoking complicated network architectures. In later experiments, the CN\ndistance in SSL embeddings was observed to increase after fine-tuning. These\nresults verify our expectations and may help design SE-related SSL training in\nthe future.", "published": "2022-04-07 10:22:26", "link": "http://arxiv.org/abs/2204.03339v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Musical Information Extraction from the Singing Voice", "abstract": "Music information retrieval is currently an active research area that\naddresses the extraction of musically important information from audio signals,\nand the applications of such information. The extracted information can be used\nfor search and retrieval of music in recommendation systems, or to aid\nmusicological studies or even in music learning. Sophisticated signal\nprocessing techniques are applied to convert low-level acoustic signal\nproperties to musical attributes which are further embedded in a rule-based or\nstatistical classification framework to link with high-level descriptions such\nas melody, genre, mood and artist type. Vocal music comprises a large and\ninteresting category of music where the lead instrument is the singing voice.\nThe singing voice is more versatile than many musical instruments and therefore\nposes interesting challenges to information retrieval systems. In this paper,\nwe provide a brief overview of research in vocal music processing followed by a\ndescription of related work at IIT Bombay leading to the development of an\ninterface for melody detection of singing voice in polyphony.", "published": "2022-04-07 02:34:57", "link": "http://arxiv.org/abs/2204.03166v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Quantized Prosody Representation for Controllable Speech\n  Synthesis", "abstract": "In this paper, we propose a novel prosody disentangle method for prosodic\nText-to-Speech (TTS) model, which introduces the vector quantization (VQ)\nmethod to the auxiliary prosody encoder to obtain the decomposed prosody\nrepresentations in an unsupervised manner. Rely on its advantages, the speaking\nstyles, such as pitch, speaking velocity, local pitch variance, etc., are\ndecomposed automatically into the latent quantize vectors. We also investigate\nthe internal mechanism of VQ disentangle process by means of a latent variables\ncounter and find that higher value dimensions usually represent prosody\ninformation. Experiments show that our model can control the speaking styles of\nsynthesis results by directly manipulating the latent variables. The objective\nand subjective evaluations illustrated that our model outperforms the popular\nmodels.", "published": "2022-04-07 06:09:47", "link": "http://arxiv.org/abs/2204.03238v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Speech Pre-training with Acoustic Piece", "abstract": "Previous speech pre-training methods, such as wav2vec2.0 and HuBERT,\npre-train a Transformer encoder to learn deep representations from audio data,\nwith objectives predicting either elements from latent vector quantized space\nor pre-generated labels (known as target codes) with offline clustering.\nHowever, those training signals (quantized elements or codes) are independent\nacross different tokens without considering their relations. According to our\nobservation and analysis, the target codes share obvious patterns aligned with\nphonemized text data. Based on that, we propose to leverage those patterns to\nbetter pre-train the model considering the relations among the codes. The\npatterns we extracted, called \"acoustic piece\"s, are from the sentence piece\nresult of HuBERT codes. With the acoustic piece as the training signal, we can\nimplicitly bridge the input audio and natural language, which benefits\naudio-to-text tasks, such as automatic speech recognition (ASR). Simple but\neffective, our method \"HuBERT-AP\" significantly outperforms strong baselines on\nthe LibriSpeech ASR task.", "published": "2022-04-07 06:12:05", "link": "http://arxiv.org/abs/2204.03240v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Expressive Singing Synthesis Using Local Style Token and Dual-path Pitch\n  Encoder", "abstract": "This paper proposes a controllable singing voice synthesis system capable of\ngenerating expressive singing voice with two novel methodologies. First, a\nlocal style token module, which predicts frame-level style tokens from an input\npitch and text sequence, is proposed to allow the singing voice system to\ncontrol musical expression often unspecified in sheet music (e.g., breathing\nand intensity). Second, we propose a dual-path pitch encoder with a choice of\ntwo different pitch inputs: MIDI pitch sequence or f0 contour. Because the\ninitial generation of a singing voice is usually executed by taking a MIDI\npitch sequence, one can later extract an f0 contour from the generated singing\nvoice and modify the f0 contour to a finer level as desired. Through\nquantitative and qualitative evaluations, we confirmed that the proposed model\ncould control various musical expressions while not sacrificing the sound\nquality of the singing voice synthesis system.", "published": "2022-04-07 06:44:11", "link": "http://arxiv.org/abs/2204.03249v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Correcting Mispronunciations in Speech using Spectrogram Inpainting", "abstract": "Learning a new language involves constantly comparing speech productions with\nreference productions from the environment. Early in speech acquisition,\nchildren make articulatory adjustments to match their caregivers' speech.\nGrownup learners of a language tweak their speech to match the tutor reference.\nThis paper proposes a method to synthetically generate correct pronunciation\nfeedback given incorrect production. Furthermore, our aim is to generate the\ncorrected production while maintaining the speaker's original voice.\n  The system prompts the user to pronounce a phrase. The speech is recorded,\nand the samples associated with the inaccurate phoneme are masked with zeros.\nThis waveform serves as an input to a speech generator, implemented as a deep\nlearning inpainting system with a U-net architecture, and trained to output a\nreconstructed speech. The training set is composed of unimpaired proper speech\nexamples, and the generator is trained to reconstruct the original proper\nspeech. We evaluated the performance of our system on phoneme replacement of\nminimal pair words of English as well as on children with pronunciation\ndisorders. Results suggest that human listeners slightly prefer our generated\nspeech over a smoothed replacement of the inaccurate phoneme with a production\nof a different speaker.", "published": "2022-04-07 11:58:29", "link": "http://arxiv.org/abs/2204.03379v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition", "abstract": "General accent recognition (AR) models tend to directly extract low-level\ninformation from spectrums, which always significantly overfit on speakers or\nchannels. Considering accent can be regarded as a series of shifts relative to\nnative pronunciation, distinguishing accents will be an easier task with accent\nshift as input. But due to the lack of native utterance as an anchor,\nestimating the accent shift is difficult. In this paper, we propose\nlinguistic-acoustic similarity based accent shift (LASAS) for AR tasks. For an\naccent speech utterance, after mapping the corresponding text vector to\nmultiple accent-associated spaces as anchors, its accent shift could be\nestimated by the similarities between the acoustic embedding and those anchors.\nThen, we concatenate the accent shift with a dimension-reduced text vector to\nobtain a linguistic-acoustic bimodal representation. Compared with pure\nacoustic embedding, the bimodal representation is richer and more clear by\ntaking full advantage of both linguistic and acoustic information, which can\neffectively improve AR performance. Experiments on Accented English Speech\nRecognition Challenge (AESRC) dataset show that our method achieves 77.42%\naccuracy on Test set, obtaining a 6.94% relative improvement over a competitive\nsystem in the challenge.", "published": "2022-04-07 12:36:14", "link": "http://arxiv.org/abs/2204.03398v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Declipping of Speech Signals Using Frequency Selective Extrapolation", "abstract": "The reconstruction of clipped speech signals is an important task in audio\nsignal processing to achieve an enhanced audio quality for further processing.\nIn this paper, Frequency Selective Extrapolation (FSE), which is commonly used\nfor error concealment or the reconstruction of incomplete image data, is\nadapted to be able to restore audio signals which are distorted from clipping.\nFor this, FSE generates a model of the signal as an iterative superposition of\nFourier basis functions. Clipped samples can then be replaced by estimated\nsamples from the model. The performance of the proposed algorithm is evaluated\nby using different speech test data sets. Compared to other state-of-the-art\ndeclipping algorithms, this leads to a maximum gain in SNR of up to 3:5 dB and\nan average gain of 1 dB.", "published": "2022-04-07 16:00:52", "link": "http://arxiv.org/abs/2204.04068v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training\n  and Distribution of Opinion Scores", "abstract": "Mean opinion score (MOS) is a typical subjective evaluation metric for speech\nsynthesis systems. Since collecting MOS is time-consuming, it would be\ndesirable if there are accurate MOS prediction models for automatic evaluation.\nIn this work, we propose DDOS, a novel MOS prediction model. DDOS utilizes\ndomain adaptive pre-training to further pre-train self-supervised learning\nmodels on synthetic speech. And a proposed module is added to model the opinion\nscore distribution of each utterance. With the proposed components, DDOS\noutperforms previous works on BVCC dataset. And the zero shot transfer result\non BC2019 dataset is significantly improved. DDOS also wins second place in\nInterspeech 2022 VoiceMOS challenge in terms of system-level score.", "published": "2022-04-07 05:04:10", "link": "http://arxiv.org/abs/2204.03219v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Real Conversational Data for Multi-Channel Continuous Speech\n  Separation", "abstract": "Existing multi-channel continuous speech separation (CSS) models are heavily\ndependent on supervised data - either simulated data which causes data mismatch\nbetween the training and real-data testing, or the real transcribed overlapping\ndata, which is difficult to be acquired, hindering further improvements in the\nconversational/meeting transcription tasks. In this paper, we propose a\nthree-stage training scheme for the CSS model that can leverage both supervised\ndata and extra large-scale unsupervised real-world conversational data. The\nscheme consists of two conventional training approaches -- pre-training using\nsimulated data and ASR-loss-based training using transcribed data -- and a\nnovel continuous semi-supervised training between the two, in which the CSS\nmodel is further trained by using real data based on the teacher-student\nlearning framework. We apply this scheme to an array-geometry-agnostic CSS\nmodel, which can use the multi-channel data collected from any microphone\narray. Large-scale meeting transcription experiments are carried out on both\nMicrosoft internal meeting data and the AMI meeting corpus. The steady\nimprovement by each training stage has been observed, showing the effect of the\nproposed method that enables leveraging real conversational data for CSS model\ntraining.", "published": "2022-04-07 05:45:52", "link": "http://arxiv.org/abs/2204.03232v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility\n  Prediction Model for Hearing Aids", "abstract": "Improving the user's hearing ability to understand speech in noisy\nenvironments is critical to the development of hearing aid (HA) devices. For\nthis, it is important to derive a metric that can fairly predict speech\nintelligibility for HA users. A straightforward approach is to conduct a\nsubjective listening test and use the test results as an evaluation metric.\nHowever, conducting large-scale listening tests is time-consuming and\nexpensive. Therefore, several evaluation metrics were derived as surrogates for\nsubjective listening test results. In this study, we propose a multi-branched\nspeech intelligibility prediction model (MBI-Net), for predicting the\nsubjective intelligibility scores of HA users. MBI-Net consists of two branches\nof models, with each branch consisting of a hearing loss model, a cross-domain\nfeature extraction module, and a speech intelligibility prediction model, to\nprocess speech signals from one channel. The outputs of the two branches are\nfused through a linear layer to obtain predicted speech intelligibility scores.\nExperimental results confirm the effectiveness of MBI-Net, which produces\nhigher prediction scores than the baseline system in Track 1 and Track 2 on the\nClarity Prediction Challenge 2022 dataset.", "published": "2022-04-07 09:13:44", "link": "http://arxiv.org/abs/2204.03305v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Genre-conditioned Acoustic Models for Automatic Lyrics Transcription of\n  Polyphonic Music", "abstract": "Lyrics transcription of polyphonic music is challenging not only because the\nsinging vocals are corrupted by the background music, but also because the\nbackground music and the singing style vary across music genres, such as pop,\nmetal, and hip hop, which affects lyrics intelligibility of the song in\ndifferent ways. In this work, we propose to transcribe the lyrics of polyphonic\nmusic using a novel genre-conditioned network. The proposed network adopts\npre-trained model parameters, and incorporates the genre adapters between\nlayers to capture different genre peculiarities for lyrics-genre pairs, thereby\nonly requiring lightweight genre-specific parameters for training. Our\nexperiments show that the proposed genre-conditioned network outperforms the\nexisting lyrics transcription systems.", "published": "2022-04-07 09:15:46", "link": "http://arxiv.org/abs/2204.03307v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MTI-Net: A Multi-Target Speech Intelligibility Prediction Model", "abstract": "Recently, deep learning (DL)-based non-intrusive speech assessment models\nhave attracted great attention. Many studies report that these DL-based models\nyield satisfactory assessment performance and good flexibility, but their\nperformance in unseen environments remains a challenge. Furthermore, compared\nto quality scores, fewer studies elaborate deep learning models to estimate\nintelligibility scores. This study proposes a multi-task speech intelligibility\nprediction model, called MTI-Net, for simultaneously predicting human and\nmachine intelligibility measures. Specifically, given a speech utterance,\nMTI-Net is designed to predict human subjective listening test results and word\nerror rate (WER) scores. We also investigate several methods that can improve\nthe prediction performance of MTI-Net. First, we compare different features\n(including low-level features and embeddings from self-supervised learning\n(SSL) models) and prediction targets of MTI-Net. Second, we explore the effect\nof transfer learning and multi-tasking learning on training MTI-Net. Finally,\nwe examine the potential advantages of fine-tuning SSL embeddings. Experimental\nresults demonstrate the effectiveness of using cross-domain features,\nmulti-task learning, and fine-tuning SSL embeddings. Furthermore, it is\nconfirmed that the intelligibility and WER scores predicted by MTI-Net are\nhighly correlated with the ground-truth scores.", "published": "2022-04-07 09:17:04", "link": "http://arxiv.org/abs/2204.03310v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised learning for robust voice cloning", "abstract": "Voice cloning is a difficult task which requires robust and informative\nfeatures incorporated in a high quality TTS system in order to effectively copy\nan unseen speaker's voice. In our work, we utilize features learned in a\nself-supervised framework via the Bootstrap Your Own Latent (BYOL) method,\nwhich is shown to produce high quality speech representations when specific\naudio augmentations are applied to the vanilla algorithm. We further extend the\naugmentations in the training procedure to aid the resulting features to\ncapture the speaker identity and to make them robust to noise and acoustic\nconditions. The learned features are used as pre-trained utterance-level\nembeddings and as inputs to a Non-Attentive Tacotron based architecture, aiming\nto achieve multispeaker speech synthesis without utilizing additional speaker\nfeatures. This method enables us to train our model in an unlabeled\nmultispeaker dataset as well as use unseen speaker embeddings to copy a\nspeaker's voice. Subjective and objective evaluations are used to validate the\nproposed model, as well as the robustness to the acoustic conditions of the\ntarget utterance.", "published": "2022-04-07 13:05:24", "link": "http://arxiv.org/abs/2204.03421v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Heterogeneous Target Speech Separation", "abstract": "We introduce a new paradigm for single-channel target source separation where\nthe sources of interest can be distinguished using non-mutually exclusive\nconcepts (e.g., loudness, gender, language, spatial location, etc). Our\nproposed heterogeneous separation framework can seamlessly leverage datasets\nwith large distribution shifts and learn cross-domain representations under a\nvariety of concepts used as conditioning. Our experiments show that training\nseparation models with heterogeneous conditions facilitates the generalization\nto new concepts with unseen out-of-domain data while also performing\nsubstantially higher than single-domain specialist models. Notably, such\ntraining leads to more robust learning of new harder source separation\ndiscriminative concepts and can yield improvements over permutation invariant\ntraining with oracle source selection. We analyze the intrinsic behavior of\nsource separation training with heterogeneous metadata and propose ways to\nalleviate emerging problems with challenging separation conditions. We release\nthe collection of preparation recipes for all datasets used to further promote\nresearch towards this challenging task.", "published": "2022-04-07 17:14:20", "link": "http://arxiv.org/abs/2204.03594v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
