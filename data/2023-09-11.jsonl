{"title": "Two is Better Than One: Answering Complex Questions by Multiple\n  Knowledge Sources with Generalized Links", "abstract": "Incorporating multiple knowledge sources is proven to be beneficial for\nanswering complex factoid questions. To utilize multiple knowledge bases (KB),\nprevious works merge all KBs into a single graph via entity alignment and\nreduce the problem to question-answering (QA) over the fused KB. In reality,\nvarious link relations between KBs might be adopted in QA over multi-KBs. In\naddition to the identity between the alignable entities (i.e. full link),\nunalignable entities expressing the different aspects or types of an abstract\nconcept may also be treated identical in a question (i.e. partial link). Hence,\nthe KB fusion in prior works fails to represent all types of links, restricting\ntheir ability to comprehend multi-KBs for QA. In this work, we formulate the\nnovel Multi-KB-QA task that leverages the full and partial links among multiple\nKBs to derive correct answers, a benchmark with diversified link and query\ntypes is also constructed to efficiently evaluate Multi-KB-QA performance.\nFinally, we propose a method for Multi-KB-QA that encodes all link relations in\nthe KB embedding to score and rank candidate answers. Experiments show that our\nmethod markedly surpasses conventional KB-QA systems in Multi-KB-QA, justifying\nthe necessity of devising this task.", "published": "2023-09-11 02:31:41", "link": "http://arxiv.org/abs/2309.05201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Artificially Real to Real: Leveraging Pseudo Data from Large\n  Language Models for Low-Resource Molecule Discovery", "abstract": "Molecule discovery serves as a cornerstone in numerous scientific domains,\nfueling the development of new materials and innovative drug designs. Recent\ndevelopments of in-silico molecule discovery have highlighted the promising\nresults of cross-modal techniques, which bridge molecular structures with their\ndescriptive annotations. However, these cross-modal methods frequently\nencounter the issue of data scarcity, hampering their performance and\napplication. In this paper, we address the low-resource challenge by utilizing\nartificially-real data generated by Large Language Models (LLMs). We first\nintroduce a retrieval-based prompting strategy to construct high-quality pseudo\ndata, then explore the optimal method to effectively leverage this pseudo data.\nExperiments show that using pseudo data for domain adaptation outperforms all\nexisting methods, while also requiring a smaller model scale, reduced data size\nand lower training cost, highlighting its efficiency. Furthermore, our method\nshows a sustained improvement as the volume of pseudo data increases, revealing\nthe great potential of pseudo data in advancing low-resource cross-modal\nmolecule discovery. Our code and data are available at\nhttps://github.com/SCIR-HI/ArtificiallyR2R.", "published": "2023-09-11 02:35:36", "link": "http://arxiv.org/abs/2309.05203v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Impact of Post-Training Quantization on Large Language\n  Models", "abstract": "Large language models (LLMs) are rapidly increasing in size, with the number\nof parameters becoming a key factor in the success of many commercial models,\nsuch as ChatGPT, Claude, and Bard. Even the recently released publicly\naccessible models for commercial usage, such as Falcon and Llama2, come\nequipped with billions of parameters. This significant increase in the number\nof parameters makes deployment and operation very costly. The remarkable\nprogress in the field of quantization for large neural networks in general and\nLLMs in particular, has made these models more accessible by enabling them to\nbe deployed on consumer-grade GPUs. Quantized models generally demonstrate\ncomparable performance levels to their unquantized base counterparts.\nNonetheless, there exists a notable gap in our comprehensive understanding of\nhow these quantized models respond to hyperparameters, such as temperature, max\nnew tokens, and topk, particularly for next word prediction. The present\nanalysis reveals that nf4 and fp4 are equally proficient 4-bit quantization\ntechniques, characterized by similar attributes such as inference speed, memory\nconsumption, and the quality of generated content. the study identifies nf4 as\ndisplaying greater resilience to temperature variations in the case of the\nllama2 series of models at lower temperature, while fp4 and fp4-dq proves to be\na more suitable choice for falcon series of models. It is noteworthy that, in\ngeneral, 4-bit quantized models of varying sizes exhibit higher sensitivity to\ntemperature in the range of 0.5 to 0.8, unlike their unquantized counterparts.\nAdditionally, int8 quantization is associated with significantly slower\ninference speeds, whereas unquantized bfloat16 models consistently yield the\nfastest inference speeds across models of all sizes.", "published": "2023-09-11 02:58:32", "link": "http://arxiv.org/abs/2309.05210v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Law of Numbers: Evidence from China's Real Estate", "abstract": "The renowned proverb, Numbers do not lie, underscores the reliability and\ninsight that lie beneath numbers, a concept of undisputed importance,\nespecially in economics and finance etc. Despite the prosperity of Benford's\nLaw in the first digit analysis, its scope fails to remain comprehensiveness\nwhen it comes to deciphering the laws of number. This paper delves into number\nlaws by taking the financial statements of China real estate as a\nrepresentative, quantitatively study not only the first digit, but also depict\nthe other two dimensions of numbers: frequency and length. The research\noutcomes transcend mere reservations about data manipulation and open the door\nto discussions surrounding number diversity and the delineation of the usage\ninsights. This study wields both economic significance and the capacity to\nfoster a deeper comprehension of numerical phenomena.", "published": "2023-09-11 03:54:38", "link": "http://arxiv.org/abs/2309.05221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minuteman: Machine and Human Joining Forces in Meeting Summarization", "abstract": "Many meetings require creating a meeting summary to keep everyone up to date.\nCreating minutes of sufficient quality is however very cognitively demanding.\nAlthough we currently possess capable models for both audio speech recognition\n(ASR) and summarization, their fully automatic use is still problematic. ASR\nmodels frequently commit errors when transcribing named entities while the\nsummarization models tend to hallucinate and misinterpret the transcript. We\npropose a novel tool -- Minuteman -- to enable efficient semi-automatic meeting\nminuting. The tool provides a live transcript and a live meeting summary to the\nusers, who can edit them in a collaborative manner, enabling correction of ASR\nerrors and imperfect summary points in real time. The resulting application\neases the cognitive load of the notetakers and allows them to easily catch up\nif they missed a part of the meeting due to absence or a lack of focus. We\nconduct several tests of the application in varied settings, exploring the\nworthiness of the concept and the possible user strategies.", "published": "2023-09-11 07:10:47", "link": "http://arxiv.org/abs/2309.05272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity\n  Recognition", "abstract": "Transfer learning has led to large gains in performance for nearly all NLP\ntasks while making downstream models easier and faster to train. This has also\nbeen extended to low-resourced languages, with some success. We investigate the\nproperties of cross-lingual transfer learning between ten low-resourced\nlanguages, from the perspective of a named entity recognition task. We\nspecifically investigate how much adaptive fine-tuning and the choice of\ntransfer language affect zero-shot transfer performance. We find that models\nthat perform well on a single language often do so at the expense of\ngeneralising to others, while models with the best generalisation to other\nlanguages suffer in individual language performance. Furthermore, the amount of\ndata overlap between the source and target datasets is a better predictor of\ntransfer performance than either the geographical or genetic distance between\nthe languages.", "published": "2023-09-11 08:56:47", "link": "http://arxiv.org/abs/2309.05311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experimenting with UD Adaptation of an Unsupervised Rule-based Approach\n  for Sentiment Analysis of Mexican Tourist Texts", "abstract": "This paper summarizes the results of experimenting with Universal\nDependencies (UD) adaptation of an Unsupervised, Compositional and Recursive\n(UCR) rule-based approach for Sentiment Analysis (SA) submitted to the Shared\nTask at Rest-Mex 2023 (Team Olga/LyS-SALSA) (within the IberLEF 2023\nconference). By using basic syntactic rules such as rules of modification and\nnegation applied on words from sentiment dictionaries, our approach exploits\nsome advantages of an unsupervised method for SA: (1) interpretability and\nexplainability of SA, (2) robustness across datasets, languages and domains and\n(3) usability by non-experts in NLP. We compare our approach with other\nunsupervised approaches of SA that in contrast to our UCR rule-based approach\nuse simple heuristic rules to deal with negation and modification. Our results\nshow a considerable improvement over these approaches. We discuss future\nimprovements of our results by using modality features as another shifting rule\nof polarity and word disambiguation techniques to identify the right sentiment\nwords.", "published": "2023-09-11 08:57:02", "link": "http://arxiv.org/abs/2309.05312v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded\n  Instruction Wrapping", "abstract": "The improvement of LLMs' instruction-following capabilities relies heavily on\nthe availability of high-quality instruction-response pairs. Unfortunately, the\ncurrent methods used to collect the pairs suffer from either unaffordable labor\ncosts or severe hallucinations in the self-generation of LLM. To tackle these\nchallenges, this paper proposes a scalable solution. It involves training LLMs\nto generate instruction-response pairs based on human-written documents, rather\nthan relying solely on self-generation without context. Our proposed method not\nonly exploits the advantages of human-written documents in reducing\nhallucinations but also utilizes an LLM to wrap the expression of documents,\nwhich enables us to bridge the gap between various document styles and the\nstandard AI response. Experiments demonstrate that our method outperforms\nexisting typical methods on multiple benchmarks. In particular, compared to the\nbest-performing baseline, the LLM trained using our generated dataset exhibits\na 10\\% relative improvement in performance on AlpacaEval, despite utilizing\nonly 1/5 of its training data. Furthermore, a comprehensive manual evaluation\nvalidates the quality of the data we generated. Our trained wrapper is publicly\navailable at https://github.com/Bahuia/Dog-Instruct.", "published": "2023-09-11 13:41:18", "link": "http://arxiv.org/abs/2309.05447v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Deductive Competence of Large Language Models", "abstract": "The development of highly fluent large language models (LLMs) has prompted\nincreased interest in assessing their reasoning and problem-solving\ncapabilities. We investigate whether several LLMs can solve a classic type of\ndeductive reasoning problem from the cognitive science literature. The tested\nLLMs have limited abilities to solve these problems in their conventional form.\nWe performed follow up experiments to investigate if changes to the\npresentation format and content improve model performance. We do find\nperformance differences between conditions; however, they do not improve\noverall performance. Moreover, we find that performance interacts with\npresentation format and content in unexpected ways that differ from human\nperformance. Overall, our results suggest that LLMs have unique reasoning\nbiases that are only partially predicted from human reasoning performance and\nthe human-generated language corpora that informs them.", "published": "2023-09-11 13:47:07", "link": "http://arxiv.org/abs/2309.05452v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flesch or Fumble? Evaluating Readability Standard Alignment of\n  Instruction-Tuned Language Models", "abstract": "Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL)\nand the Common European Framework of Reference for Languages (CEFR) exist to\nguide teachers and educators to properly assess the complexity of educational\nmaterials before administering them for classroom use. In this study, we select\na diverse set of open and closed-source instruction-tuned language models and\ninvestigate their performances in writing story completions and simplifying\nnarratives--tasks that teachers perform--using standard-guided prompts\ncontrolling text readability. Our extensive findings provide empirical proof of\nhow globally recognized models like ChatGPT may be considered less effective\nand may require more refined prompts for these generative tasks compared to\nother open-sourced models such as BLOOMZ and FlanT5--which have shown promising\nresults.", "published": "2023-09-11 13:50:38", "link": "http://arxiv.org/abs/2309.05454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Learning with Minimum Instruction to Extract Social\n  Determinants and Family History from Clinical Notes using GPT Model", "abstract": "Demographics, Social determinants of health, and family history documented in\nthe unstructured text within the electronic health records are increasingly\nbeing studied to understand how this information can be utilized with the\nstructured data to improve healthcare outcomes. After the GPT models were\nreleased, many studies have applied GPT models to extract this information from\nthe narrative clinical notes. Different from the existing work, our research\nfocuses on investigating the zero-shot learning on extracting this information\ntogether by providing minimum information to the GPT model. We utilize\nde-identified real-world clinical notes annotated for demographics, various\nsocial determinants, and family history information. Given that the GPT model\nmight provide text different from the text in the original data, we explore two\nsets of evaluation metrics, including the traditional NER evaluation metrics\nand semantic similarity evaluation metrics, to completely understand the\nperformance. Our results show that the GPT-3.5 method achieved an average of\n0.975 F1 on demographics extraction, 0.615 F1 on social determinants\nextraction, and 0.722 F1 on family history extraction. We believe these results\ncan be further improved through model fine-tuning or few-shots learning.\nThrough the case studies, we also identified the limitations of the GPT models,\nwhich need to be addressed in future research.", "published": "2023-09-11 14:16:27", "link": "http://arxiv.org/abs/2309.05475v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrisisTransformers: Pre-trained language models and sentence encoders\n  for crisis-related social media texts", "abstract": "Social media platforms play an essential role in crisis communication, but\nanalyzing crisis-related social media texts is challenging due to their\ninformal nature. Transformer-based pre-trained models like BERT and RoBERTa\nhave shown success in various NLP tasks, but they are not tailored for\ncrisis-related texts. Furthermore, general-purpose sentence encoders are used\nto generate sentence embeddings, regardless of the textual complexities in\ncrisis-related texts. Advances in applications like text classification,\nsemantic search, and clustering contribute to the effective processing of\ncrisis-related texts, which is essential for emergency responders to gain a\ncomprehensive view of a crisis event, whether historical or real-time. To\naddress these gaps in crisis informatics literature, this study introduces\nCrisisTransformers, an ensemble of pre-trained language models and sentence\nencoders trained on an extensive corpus of over 15 billion word tokens from\ntweets associated with more than 30 crisis events, including disease outbreaks,\nnatural disasters, conflicts, and other critical incidents. We evaluate\nexisting models and CrisisTransformers on 18 crisis-specific public datasets.\nOur pre-trained models outperform strong baselines across all datasets in\nclassification tasks, and our best-performing sentence encoder improves the\nstate-of-the-art by 17.43% in sentence encoding tasks. Additionally, we\ninvestigate the impact of model initialization on convergence and evaluate the\nsignificance of domain-specific models in generating semantically meaningful\nsentence embeddings. The models are publicly available at:\nhttps://huggingface.co/crisistransformers", "published": "2023-09-11 14:36:16", "link": "http://arxiv.org/abs/2309.05494v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-Range Transformer Architectures for Document Understanding", "abstract": "Since their release, Transformers have revolutionized many fields from\nNatural Language Understanding to Computer Vision. Document Understanding (DU)\nwas not left behind with first Transformer based models for DU dating from late\n2019. However, the computational complexity of the self-attention operation\nlimits their capabilities to small sequences. In this paper we explore multiple\nstrategies to apply Transformer based models to long multi-page documents. We\nintroduce 2 new multi-modal (text + layout) long-range models for DU. They are\nbased on efficient implementations of Transformers for long sequences.\nLong-range models can process whole documents at once effectively and are less\nimpaired by the document's length. We compare them to LayoutLM, a classical\nTransformer adapted for DU and pre-trained on millions of documents. We further\npropose 2D relative attention bias to guide self-attention towards relevant\ntokens without harming model efficiency. We observe improvements on multi-page\nbusiness documents on Information Retrieval for a small performance cost on\nsmaller sequences. Relative 2D attention revealed to be effective on dense text\nfor both normal and long-range models.", "published": "2023-09-11 14:45:24", "link": "http://arxiv.org/abs/2309.05503v1", "categories": ["cs.CL", "68T01", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Effective Proxy for Human Labeling: Ensemble Disagreement Scores in\n  Large Language Models for Industrial NLP", "abstract": "Large language models (LLMs) have demonstrated significant capability to\ngeneralize across a large number of NLP tasks. For industry applications, it is\nimperative to assess the performance of the LLM on unlabeled production data\nfrom time to time to validate for a real-world setting. Human labeling to\nassess model error requires considerable expense and time delay. Here we\ndemonstrate that ensemble disagreement scores work well as a proxy for human\nlabeling for language models in zero-shot, few-shot, and fine-tuned settings,\nper our evaluation on keyphrase extraction (KPE) task. We measure fidelity of\nthe results by comparing to true error measured from human labeled ground\ntruth. We contrast with the alternative of using another LLM as a source of\nmachine labels, or silver labels. Results across various languages and domains\nshow disagreement scores provide a better estimation of model performance with\nmean average error (MAE) as low as 0.4% and on average 13.8% better than using\nsilver labels.", "published": "2023-09-11 17:07:01", "link": "http://arxiv.org/abs/2309.05619v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction\n  Tuning", "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs)\nspecifically tailored for general math problem-solving. The MAmmoTH models are\ntrained on MathInstruct, our meticulously curated instruction tuning dataset.\nMathInstruct is compiled from 13 math datasets with intermediate rationales,\nsix of which have rationales newly curated by us. It presents a unique hybrid\nof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also\nensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT\nnot only unleashes the potential of tool use but also allows different thought\nprocesses for different math problems. As a result, the MAmmoTH series\nsubstantially outperform existing open-source models on nine mathematical\nreasoning datasets across all scales with an average accuracy gain between 16%\nand 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a\ncompetition-level dataset), which exceeds the best open-source 7B model\n(WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH,\neven surpassing GPT-4's CoT result. Our work underscores the importance of\ndiverse problem coverage and the use of hybrid rationales in developing\nsuperior math generalist models.", "published": "2023-09-11 17:47:22", "link": "http://arxiv.org/abs/2309.05653v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hi Model, generating 'nice' instead of 'good' is not as bad as\n  generating 'rice'! Towards Context and Semantic Infused Dialogue Generation\n  Loss Function and Evaluation Metric", "abstract": "Over the past two decades, dialogue modeling has made significant strides,\nmoving from simple rule-based responses to personalized and persuasive response\ngeneration. However, despite these advancements, the objective functions and\nevaluation metrics for dialogue generation have remained stagnant. These\nlexical-based metrics, e.g., cross-entropy and BLEU, have two key limitations:\n(a) word-to-word matching without semantic consideration: It assigns the same\ncredit for failure to generate \"nice\" and \"rice\" for \"good\", (b) missing\ncontext attribute for evaluating the generated response: Even if a generated\nresponse is relevant to the ongoing dialogue context, it may still be penalized\nfor not matching the gold utterance provided in the corpus. In this paper, we\nfirst investigate these limitations comprehensively and propose a new loss\nfunction called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss\nfunction. We also formulate an evaluation metric called Dialuation,\nincorporating both context and semantic relevance. We experimented with both\nnon-pretrained and pre-trained models on two dialogue corpora, encompassing\ntask-oriented and open-domain scenarios. We found that the dialogue generation\nmodels trained with SemTextualLogueloss attained superior performance compared\nto the traditional cross-entropy loss function. The findings establish that the\neffective training of a dialogue generation model hinges significantly on\nincorporating semantics and context. This pattern is also mirrored in the\nintroduced Dialuation metric, where the consideration of both context and\nsemantics correlates more strongly with human evaluation compared to\ntraditional metrics.", "published": "2023-09-11 20:16:38", "link": "http://arxiv.org/abs/2309.05804v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying and Attributing the Hallucination of Large Language Models\n  via Association Analysis", "abstract": "Although demonstrating superb performance on various NLP tasks, large\nlanguage models (LLMs) still suffer from the hallucination problem, which\nthreatens the reliability of LLMs. To measure the level of hallucination of\nLLMs, previous works first categorize the hallucination according to the\nphenomenon similarity, then quantify the proportion that model outputs contain\nhallucinatory contents. However, such hallucination rates could easily be\ndistorted by confounders. Moreover, such hallucination rates could not reflect\nthe reasons for the hallucination, as similar hallucinatory phenomena may\noriginate from different sources. To address these issues, we propose to\ncombine the hallucination level quantification and hallucination reason\ninvestigation through an association analysis, which builds the relationship\nbetween the hallucination rate of LLMs with a set of risk factors. In this way,\nwe are able to observe the hallucination level under each value of each risk\nfactor, examining the contribution and statistical significance of each risk\nfactor, meanwhile excluding the confounding effect of other factors.\nAdditionally, by recognizing the risk factors according to a taxonomy of model\ncapability, we reveal a set of potential deficiencies in commonsense\nmemorization, relational reasoning, and instruction following, which may\nfurther provide guidance for the pretraining and supervised fine-tuning process\nof LLMs to mitigate the hallucination.", "published": "2023-09-11 03:35:00", "link": "http://arxiv.org/abs/2309.05217v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Detecting Natural Language Biases with Prompt-based Learning", "abstract": "In this project, we want to explore the newly emerging field of prompt\nengineering and apply it to the downstream task of detecting LM biases. More\nconcretely, we explore how to design prompts that can indicate 4 different\ntypes of biases: (1) gender, (2) race, (3) sexual orientation, and (4)\nreligion-based. Within our project, we experiment with different manually\ncrafted prompts that can draw out the subtle biases that may be present in the\nlanguage model. We apply these prompts to multiple variations of popular and\nwell-recognized models: BERT, RoBERTa, and T5 to evaluate their biases. We\nprovide a comparative analysis of these models and assess them using a two-fold\nmethod: use human judgment to decide whether model predictions are biased and\nutilize model-level judgment (through further prompts) to understand if a model\ncan self-diagnose the biases of its own prediction.", "published": "2023-09-11 04:20:36", "link": "http://arxiv.org/abs/2309.05227v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CONFLATOR: Incorporating Switching Point based Rotatory Positional\n  Encodings for Code-Mixed Language Modeling", "abstract": "The mixing of two or more languages is called Code-Mixing (CM). CM is a\nsocial norm in multilingual societies. Neural Language Models (NLMs) like\ntransformers have been effective on many NLP tasks. However, NLM for CM is an\nunder-explored area. Though transformers are capable and powerful, they cannot\nalways encode positional information since they are non-recurrent. Therefore,\nto enrich word information and incorporate positional information, positional\nencoding is defined. We hypothesize that Switching Points (SPs), i.e.,\njunctions in the text where the language switches (L1 -> L2 or L2 -> L1), pose\na challenge for CM Language Models (LMs), and hence give special emphasis to\nSPs in the modeling process. We experiment with several positional encoding\nmechanisms and show that rotatory positional encodings along with switching\npoint information yield the best results.\n  We introduce CONFLATOR: a neural language modeling approach for code-mixed\nlanguages. CONFLATOR tries to learn to emphasize switching points using smarter\npositional encoding, both at unigram and bigram levels. CONFLATOR outperforms\nthe state-of-the-art on two tasks based on code-mixed Hindi and English\n(Hinglish): (i) sentiment analysis and (ii) machine translation.", "published": "2023-09-11 07:02:13", "link": "http://arxiv.org/abs/2309.05270v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Information Extraction on Business Documents with Specific\n  Pre-Training Tasks", "abstract": "Transformer-based Language Models are widely used in Natural Language\nProcessing related tasks. Thanks to their pre-training, they have been\nsuccessfully adapted to Information Extraction in business documents. However,\nmost pre-training tasks proposed in the literature for business documents are\ntoo generic and not sufficient to learn more complex structures. In this paper,\nwe use LayoutLM, a language model pre-trained on a collection of business\ndocuments, and introduce two new pre-training tasks that further improve its\ncapacity to extract relevant information. The first is aimed at better\nunderstanding the complex layout of documents, and the second focuses on\nnumeric values and their order of magnitude. These tasks force the model to\nlearn better-contextualized representations of the scanned documents. We\nfurther introduce a new post-processing algorithm to decode BIESO tags in\nInformation Extraction that performs better with complex entities. Our method\nsignificantly improves extraction performance on both public (from 93.88 to\n95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed of\nexpense receipts, invoices, and purchase orders.", "published": "2023-09-11 13:05:23", "link": "http://arxiv.org/abs/2309.05429v1", "categories": ["cs.CL", "cs.AI", "68T01", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient\n  MoE for Instruction Tuning", "abstract": "The Mixture of Experts (MoE) is a widely known neural architecture where an\nensemble of specialized sub-models optimizes overall performance with a\nconstant computational cost. However, conventional MoEs pose challenges at\nscale due to the need to store all experts in memory. In this paper, we push\nMoE to the limit. We propose extremely parameter-efficient MoE by uniquely\ncombining MoE architecture with lightweight experts.Our MoE architecture\noutperforms standard parameter-efficient fine-tuning (PEFT) methods and is on\npar with full fine-tuning by only updating the lightweight experts -- less than\n1% of an 11B parameters model. Furthermore, our method generalizes to unseen\ntasks as it does not depend on any prior task knowledge. Our research\nunderscores the versatility of the mixture of experts architecture, showcasing\nits ability to deliver robust performance even when subjected to rigorous\nparameter constraints. Our code used in all the experiments is publicly\navailable here: https://github.com/for-ai/parameter-efficient-moe.", "published": "2023-09-11 13:31:00", "link": "http://arxiv.org/abs/2309.05444v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textbooks Are All You Need II: phi-1.5 technical report", "abstract": "We continue the investigation into the power of smaller Transformer-based\nlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parameter\nmodel that can produce coherent English -- and the follow-up work on\n\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\nclose to the state-of-the-art. The latter work proposed to use existing Large\nLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhance\nthe learning process compared to traditional web data. We follow the\n``Textbooks Are All You Need\" approach, focusing this time on common sense\nreasoning in natural language, and create a new 1.3 billion parameter model\nnamed \\textbf{phi-1.5}, with performance on natural language tasks comparable\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\nreasoning tasks such as grade-school mathematics and basic coding. More\ngenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\nboth good -- such as the ability to ``think step by step\" or perform some\nrudimentary in-context learning -- and bad, including hallucinations and the\npotential for toxic and biased generations -- encouragingly though, we are\nseeing improvement on that front thanks to the absence of web data. We\nopen-source \\textbf{phi-1.5} to promote further research on these urgent\ntopics.", "published": "2023-09-11 14:01:45", "link": "http://arxiv.org/abs/2309.05463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personality Detection and Analysis using Twitter Data", "abstract": "Personality types are important in various fields as they hold relevant\ninformation about the characteristics of a human being in an explainable\nformat. They are often good predictors of a person's behaviors in a particular\nenvironment and have applications ranging from candidate selection to marketing\nand mental health. Recently automatic detection of personality traits from\ntexts has gained significant attention in computational linguistics. Most\npersonality detection and analysis methods have focused on small datasets\nmaking their experimental observations often limited. To bridge this gap, we\nfocus on collecting and releasing the largest automatically curated dataset for\nthe research community which has 152 million tweets and 56 thousand data points\nfor the Myers-Briggs personality type (MBTI) prediction task. We perform a\nseries of extensive qualitative and quantitative studies on our dataset to\nanalyze the data patterns in a better way and infer conclusions. We show how\nour intriguing analysis results often follow natural intuition. We also perform\na series of ablation studies to show how the baselines perform for our dataset.", "published": "2023-09-11 14:39:04", "link": "http://arxiv.org/abs/2309.05497v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource\n  Languages through Data Enrichment", "abstract": "In recent years, natural language processing has gained significant\npopularity in various sectors, including the legal domain. This paper presents\nNeCo Team's solutions to the Vietnamese text processing tasks provided in the\nAutomated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on\nlegal domain knowledge acquisition for low-resource languages through data\nenrichment. Our methods for the legal document retrieval task employ a\ncombination of similarity ranking and deep learning models, while for the\nsecond task, which requires extracting an answer from a relevant legal article\nin response to a question, we propose a range of adaptive techniques to handle\ndifferent question types. Our approaches achieve outstanding results on both\ntasks of the competition, demonstrating the potential benefits and\neffectiveness of question answering systems in the legal field, particularly\nfor low-resource languages.", "published": "2023-09-11 14:43:45", "link": "http://arxiv.org/abs/2309.05500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task", "abstract": "The evolution of Generative Pre-trained Transformer (GPT) models has led to\nsignificant advancements in various natural language processing applications,\nparticularly in legal textual entailment. We present an analysis of GPT-3.5\n(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent\nbenchmark in this domain. The study encompasses data from Heisei 18 (2006) to\nReiwa 3 (2021), exploring the models' abilities to discern entailment\nrelationships within Japanese statute law across different periods. Our\npreliminary experimental results unveil intriguing insights into the models'\nstrengths and weaknesses in handling legal textual entailment tasks, as well as\nthe patterns observed in model performance. In the context of proprietary\nmodels with undisclosed architectures and weights, black-box analysis becomes\ncrucial for evaluating their capabilities. We discuss the influence of training\ndata distribution and the implications on the models' generalizability. This\nanalysis serves as a foundation for future research, aiming to optimize\nGPT-based models and enable their successful adoption in legal information\nextraction and entailment applications.", "published": "2023-09-11 14:43:54", "link": "http://arxiv.org/abs/2309.05501v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Pre-trained Model Prompting in Multimodal Stock Volume\n  Movement Prediction", "abstract": "Multimodal stock trading volume movement prediction with stock-related news\nis one of the fundamental problems in the financial area. Existing multimodal\nworks that train models from scratch face the problem of lacking universal\nknowledge when modeling financial news. In addition, the models ability may be\nlimited by the lack of domain-related knowledge due to insufficient data in the\ndatasets. To handle this issue, we propose the Prompt-based MUltimodal Stock\nvolumE prediction model (ProMUSE) to process text and time series modalities.\nWe use pre-trained language models for better comprehension of financial news\nand adopt prompt learning methods to leverage their capability in universal\nknowledge to model textual information. Besides, simply fusing two modalities\ncan cause harm to the unimodal representations. Thus, we propose a novel\ncross-modality contrastive alignment while reserving the unimodal heads beside\nthe fusion head to mitigate this problem. Extensive experiments demonstrate\nthat our proposed ProMUSE outperforms existing baselines. Comprehensive\nanalyses further validate the effectiveness of our architecture compared to\npotential variants and learning mechanisms.", "published": "2023-09-11 16:47:01", "link": "http://arxiv.org/abs/2309.05608v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Large Language Model for Science: A Study on P vs. NP", "abstract": "In this work, we use large language models (LLMs) to augment and accelerate\nresearch on the P versus NP problem, one of the most important open problems in\ntheoretical computer science and mathematics. Specifically, we propose Socratic\nreasoning, a general framework that promotes in-depth thinking with LLMs for\ncomplex problem-solving. Socratic reasoning encourages LLMs to recursively\ndiscover, solve, and integrate problems while facilitating self-evaluation and\nrefinement. Our pilot study on the P vs. NP problem shows that GPT-4\nsuccessfully produces a proof schema and engages in rigorous reasoning\nthroughout 97 dialogue turns, concluding \"P $\\neq$ NP\", which is in alignment\nwith (Xu and Zhou, 2023). The investigation uncovers novel insights within the\nextensive solution space of LLMs, shedding light on LLM for Science.", "published": "2023-09-11 17:49:27", "link": "http://arxiv.org/abs/2309.05689v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenges in Annotating Datasets to Quantify Bias in Under-represented\n  Society", "abstract": "Recent advances in artificial intelligence, including the development of\nhighly sophisticated large language models (LLM), have proven beneficial in\nmany real-world applications. However, evidence of inherent bias encoded in\nthese LLMs has raised concerns about equity. In response, there has been an\nincrease in research dealing with bias, including studies focusing on\nquantifying bias and developing debiasing techniques. Benchmark bias datasets\nhave also been developed for binary gender classification and ethical/racial\nconsiderations, focusing predominantly on American demographics. However, there\nis minimal research in understanding and quantifying bias related to\nunder-represented societies. Motivated by the lack of annotated datasets for\nquantifying bias in under-represented societies, we endeavoured to create\nbenchmark datasets for the New Zealand (NZ) population. We faced many\nchallenges in this process, despite the availability of three annotators. This\nresearch outlines the manual annotation process, provides an overview of the\nchallenges we encountered and lessons learnt, and presents recommendations for\nfuture research.", "published": "2023-09-11 22:24:39", "link": "http://arxiv.org/abs/2309.08624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Applying BioBERT to Extract Germline Gene-Disease Associations for\n  Building a Knowledge Graph from the Biomedical Literature", "abstract": "Published biomedical information has and continues to rapidly increase. The\nrecent advancements in Natural Language Processing (NLP), have generated\nconsiderable interest in automating the extraction, normalization, and\nrepresentation of biomedical knowledge about entities such as genes and\ndiseases. Our study analyzes germline abstracts in the construction of\nknowledge graphs of the of the immense work that has been done in this area for\ngenes and diseases. This paper presents SimpleGermKG, an automatic knowledge\ngraph construction approach that connects germline genes and diseases. For the\nextraction of genes and diseases, we employ BioBERT, a pre-trained BERT model\non biomedical corpora. We propose an ontology-based and rule-based algorithm to\nstandardize and disambiguate medical terms. For semantic relationships between\narticles, genes, and diseases, we implemented a part-whole relation approach to\nconnect each entity with its data source and visualize them in a graph-based\nknowledge representation. Lastly, we discuss the knowledge graph applications,\nlimitations, and challenges to inspire the future research of germline corpora.\nOur knowledge graph contains 297 genes, 130 diseases, and 46,747 triples.\nGraph-based visualizations are used to show the results.", "published": "2023-09-11 18:05:12", "link": "http://arxiv.org/abs/2309.13061v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning", "abstract": "Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving substantial memory and time costs compared to vanilla\nPT and its variants, without changing trainable parameter sizes. Through\nextensive experiments on 23 natural language processing (NLP) and\nvision-language (VL) tasks, we demonstrate that DePT outperforms\nstate-of-the-art PEFT approaches, including the full fine-tuning baseline, in\nsome scenarios. Additionally, we empirically show that DEPT grows more\nefficient as the model size increases. Our further study reveals that DePT\nintegrates seamlessly with parameter-efficient transfer learning in the\nfew-shot learning setting and highlights its adaptability to various model\narchitectures and sizes.", "published": "2023-09-11 00:02:05", "link": "http://arxiv.org/abs/2309.05173v5", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Writing with Language Models Reduce Content Diversity?", "abstract": "Large language models (LLMs) have led to a surge in collaborative writing\nwith model assistance. As different users incorporate suggestions from the same\nmodel, there is a risk of decreased diversity in the produced content,\npotentially limiting diverse perspectives in public discourse. In this work, we\nmeasure the impact of co-writing on diversity via a controlled experiment,\nwhere users write argumentative essays in three setups -- using a base LLM\n(GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We\ndevelop a set of diversity metrics and find that writing with InstructGPT (but\nnot the GPT3) results in a statistically significant reduction in diversity.\nSpecifically, it increases the similarity between the writings of different\nauthors and reduces the overall lexical and content diversity. We additionally\nfind that this effect is mainly attributable to InstructGPT contributing less\ndiverse text to co-written essays. In contrast, the user-contributed text\nremains unaffected by model collaboration. This suggests that the recent\nimprovement in generation quality from adapting models to human feedback might\ncome at the cost of more homogeneous and less diverse content.", "published": "2023-09-11 02:16:47", "link": "http://arxiv.org/abs/2309.05196v3", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of\n  SSWP", "abstract": "In expressive and controllable Text-to-Speech (TTS), explicit prosodic\nfeatures significantly improve the naturalness and controllability of\nsynthesised speech. However, manual prosody annotation is labor-intensive and\ninconsistent. To address this issue, a two-stage automatic annotation pipeline\nis novelly proposed in this paper. In the first stage, we use contrastive\npretraining of Speech-Silence and Word-Punctuation (SSWP) pairs to enhance\nprosodic information in latent representations. In the second stage, we build a\nmulti-modal prosody annotator, comprising pretrained encoders, a text-speech\nfusing scheme, and a sequence classifier. Experiments on English prosodic\nboundaries demonstrate that our method achieves state-of-the-art (SOTA)\nperformance with 0.72 and 0.93 f1 score for Prosodic Word and Prosodic Phrase\nboundary respectively, while bearing remarkable robustness to data scarcity.", "published": "2023-09-11 12:50:28", "link": "http://arxiv.org/abs/2309.05423v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Panoptic Vision-Language Feature Fields", "abstract": "Recently, methods have been proposed for 3D open-vocabulary semantic\nsegmentation. Such methods are able to segment scenes into arbitrary classes\nbased on text descriptions provided during runtime. In this paper, we propose\nto the best of our knowledge the first algorithm for open-vocabulary panoptic\nsegmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature\nFields (PVLFF), learns a semantic feature field of the scene by distilling\nvision-language features from a pretrained 2D model, and jointly fits an\ninstance feature field through contrastive learning using 2D instance segments\non input frames. Despite not being trained on the target classes, our method\nachieves panoptic segmentation performance similar to the state-of-the-art\nclosed-set 3D systems on the HyperSim, ScanNet and Replica dataset and\nadditionally outperforms current 3D open-vocabulary systems in terms of\nsemantic segmentation. We ablate the components of our method to demonstrate\nthe effectiveness of our model architecture. Our code will be available at\nhttps://github.com/ethz-asl/pvlff.", "published": "2023-09-11 13:41:27", "link": "http://arxiv.org/abs/2309.05448v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for\n  Self-supervised Representations of French Speech", "abstract": "Self-supervised learning (SSL) is at the origin of unprecedented improvements\nin many different domains including computer vision and natural language\nprocessing. Speech processing drastically benefitted from SSL as most of the\ncurrent domain-related tasks are now being approached with pre-trained models.\nThis work introduces LeBenchmark 2.0 an open-source framework for assessing and\nbuilding SSL-equipped French speech technologies. It includes documented,\nlarge-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous\nspeech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to\none billion learnable parameters shared with the community, and an evaluation\nprotocol made of six downstream tasks to complement existing benchmarks.\nLeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for\nspeech with the investigation of frozen versus fine-tuned downstream models,\ntask-agnostic versus task-specific pre-trained models as well as a discussion\non the carbon footprint of large-scale model training. Overall, the newly\nintroduced models trained on 14,000 hours of French speech outperform\nmultilingual and previous LeBenchmark SSL models across the benchmark but also\nrequired up to four times more energy for pre-training.", "published": "2023-09-11 14:13:09", "link": "http://arxiv.org/abs/2309.05472v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Optimize Weight Rounding via Signed Gradient Descent for the\n  Quantization of LLMs", "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nlanguage-related tasks, but their deployment poses significant challenges due\nto substantial memory and storage requirements. Weight-only quantization has\nemerged as a promising solution, significantly reducing memory and storage\nneeds without sacrificing too much performance. In this study, we introduce\nSignRound, a method that leverages signed gradient descent (SignSGD) to\noptimize rounding values and weight clipping in just 200 steps. SignRound\nintegrates the advantages of Quantization-Aware Training (QAT) and\nPost-Training Quantization (PTQ), delivering exceptional results across 2 to 4\nbits while minimizing tuning costs and avoiding additional inference overhead.\nFor example, SignRound achieved absolute average accuracy improvements ranging\nfrom 6.91% to 33.22% at 2bits, as measured by the average zero-shot accuracy\nacross 11 tasks. It also demonstrates strong generalization in recent models,\nachieving near-lossless 4-bit quantization in most scenarios. The source code\nis publicly available at https://github.com/intel/auto-round.", "published": "2023-09-11 14:58:23", "link": "http://arxiv.org/abs/2309.05516v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NExT-GPT: Any-to-Any Multimodal LLM", "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/", "published": "2023-09-11 15:02:25", "link": "http://arxiv.org/abs/2309.05519v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "PAI-Diffusion: Constructing and Serving a Family of Open Chinese\n  Diffusion Models for Text-to-image Synthesis on the Cloud", "abstract": "Text-to-image synthesis for the Chinese language poses unique challenges due\nto its large vocabulary size, and intricate character relationships. While\nexisting diffusion models have shown promise in generating images from textual\ndescriptions, they often neglect domain-specific contexts and lack robustness\nin handling the Chinese language. This paper introduces PAI-Diffusion, a\ncomprehensive framework that addresses these limitations. PAI-Diffusion\nincorporates both general and domain-specific Chinese diffusion models,\nenabling the generation of contextually relevant images. It explores the\npotential of using LoRA and ControlNet for fine-grained image style transfer\nand image editing, empowering users with enhanced control over image\ngeneration. Moreover, PAI-Diffusion seamlessly integrates with Alibaba Cloud's\nMachine Learning Platform for AI, providing accessible and scalable solutions.\nAll the Chinese diffusion model checkpoints, LoRAs, and ControlNets, including\ndomain-specific ones, are publicly available. A user-friendly Chinese WebUI and\nthe diffusers-api elastic inference toolkit, also open-sourced, further\nfacilitate the easy deployment of PAI-Diffusion models in various environments,\nmaking it a valuable resource for Chinese text-to-image synthesis.", "published": "2023-09-11 15:18:28", "link": "http://arxiv.org/abs/2309.05534v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Kani: A Lightweight and Highly Hackable Framework for Building Language\n  Model Applications", "abstract": "Language model applications are becoming increasingly popular and complex,\noften including features like tool usage and retrieval augmentation. However,\nexisting frameworks for such applications are often opinionated, deciding for\ndevelopers how their prompts ought to be formatted and imposing limitations on\ncustomizability and reproducibility. To solve this we present Kani: a\nlightweight, flexible, and model-agnostic open-source framework for building\nlanguage model applications. Kani helps developers implement a variety of\ncomplex features by supporting the core building blocks of chat interaction:\nmodel interfacing, chat management, and robust function calling. All Kani core\nfunctions are easily overridable and well documented to empower developers to\ncustomize functionality for their own needs. Kani thus serves as a useful tool\nfor researchers, hobbyists, and industry professionals alike to accelerate\ntheir development while retaining interoperability and fine-grained control.", "published": "2023-09-11 15:27:59", "link": "http://arxiv.org/abs/2309.05542v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.SE"}
{"title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language\n  Models", "abstract": "Nowadays, the versatile capabilities of Pre-trained Large Language Models\n(LLMs) have attracted much attention from the industry. However, some vertical\ndomains are more interested in the in-domain capabilities of LLMs. For the\nNetworks domain, we present NetEval, an evaluation set for measuring the\ncomprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is\ndesigned for evaluating the commonsense knowledge and inference ability in\nNetOps in a multi-lingual context. NetEval consists of 5,732 questions about\nNetOps, covering five different sub-domains of NetOps. With NetEval, we\nsystematically evaluate the NetOps capability of 26 publicly available LLMs.\nThe results show that only GPT-4 can achieve a performance competitive to\nhumans. However, some open models like LLaMA 2 demonstrate significant\npotential.", "published": "2023-09-11 15:45:40", "link": "http://arxiv.org/abs/2309.05557v3", "categories": ["cs.CL", "cs.AI", "cs.NI"], "primary_category": "cs.CL"}
{"title": "ITI-GEN: Inclusive Text-to-Image Generation", "abstract": "Text-to-image generative models often reflect the biases of the training\ndata, leading to unequal representations of underrepresented groups. This study\ninvestigates inclusive text-to-image generative models that generate images\nbased on human-written prompts and ensure the resulting images are uniformly\ndistributed across attributes of interest. Unfortunately, directly expressing\nthe desired attributes in the prompt often leads to sub-optimal results due to\nlinguistic ambiguity or model misrepresentation. Hence, this paper proposes a\ndrastically different approach that adheres to the maxim that \"a picture is\nworth a thousand words\". We show that, for some attributes, images can\nrepresent concepts more expressively than text. For instance, categories of\nskin tones are typically hard to specify by text but can be easily represented\nby example images. Building upon these insights, we propose a novel approach,\nITI-GEN, that leverages readily available reference images for Inclusive\nText-to-Image GENeration. The key idea is learning a set of prompt embeddings\nto generate images that can effectively represent all desired attribute\ncategories. More importantly, ITI-GEN requires no model fine-tuning, making it\ncomputationally efficient to augment existing text-to-image models. Extensive\nexperiments demonstrate that ITI-GEN largely improves over state-of-the-art\nmodels to generate inclusive images from a prompt. Project page:\nhttps://czhang0528.github.io/iti-gen.", "published": "2023-09-11 15:54:30", "link": "http://arxiv.org/abs/2309.05569v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Memory Injections: Correcting Multi-Hop Reasoning Failures during\n  Inference in Transformer-Based Language Models", "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Large Language Models (LLMs) struggle to\nperform such reasoning consistently. Here we propose an approach to pinpoint\nand rectify multi-hop reasoning failures through targeted memory injections on\nLLM attention heads. First, we analyze the per-layer activations of GPT-2\nmodels in response to single and multi-hop prompts. We then propose a mechanism\nthat allows users to inject pertinent prompt-specific information, which we\nrefer to as \"memories,\" at critical LLM locations during inference. By thus\nenabling the LLM to incorporate additional relevant information during\ninference, we enhance the quality of multi-hop prompt completions. We show\nempirically that a simple, efficient, and targeted memory injection into a key\nattention layer can often increase the probability of the desired next token in\nmulti-hop tasks, by up to 424%.", "published": "2023-09-11 16:39:30", "link": "http://arxiv.org/abs/2309.05605v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hypothesis Search: Inductive Reasoning with Language Models", "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify\nunderlying principles from a few examples, which robustly generalize to novel\nscenarios. Recent work evaluates large language models (LLMs) on inductive\nreasoning tasks by directly prompting them yielding \"in context learning.\" This\nworks well for straightforward inductive tasks but performs poorly on complex\ntasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we\npropose to improve the inductive reasoning ability of LLMs by generating\nexplicit hypotheses at multiple levels of abstraction: we prompt the LLM to\npropose multiple abstract hypotheses about the problem, in natural language,\nthen implement the natural language hypotheses as concrete Python programs.\nThese programs can be verified by running on observed examples and generalized\nto novel inputs. To reduce the hypothesis search space, we explore steps to\nfilter the set of hypotheses to implement: we either ask the LLM to summarize\nthem into a smaller set of hypotheses or ask human annotators to select a\nsubset. We verify our pipeline's effectiveness on the ARC visual inductive\nreasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS,\nand list transformation dataset List Functions. On a random 100-problem subset\nof ARC, our automated pipeline using LLM summaries achieves 30% accuracy,\noutperforming the direct prompting baseline (accuracy of 17%). With the minimal\nhuman input of selecting from LLM-generated candidates, performance is boosted\nto 33%. Our ablations show that both abstract hypothesis generation and\nconcrete program representations benefit LLMs on inductive reasoning tasks.", "published": "2023-09-11 17:56:57", "link": "http://arxiv.org/abs/2309.05660v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation\n  with GPT-4 in Cloud Incident Root Cause Analysis", "abstract": "Major cloud providers have employed advanced AI-based solutions like large\nlanguage models to aid humans in identifying the root causes of cloud\nincidents. Despite the growing prevalence of AI-driven assistants in the root\ncause analysis process, their effectiveness in assisting on-call engineers is\nconstrained by low accuracy due to the intrinsic difficulty of the task, a\npropensity for LLM-based approaches to hallucinate, and difficulties in\ndistinguishing these well-disguised hallucinations. To address this challenge,\nwe propose to perform confidence estimation for the predictions to help on-call\nengineers make decisions on whether to adopt the model prediction. Considering\nthe black-box nature of many LLM-based root cause predictors, fine-tuning or\ntemperature-scaling-based approaches are inapplicable. We therefore design an\ninnovative confidence estimation framework based on prompting\nretrieval-augmented large language models (LLMs) that demand a minimal amount\nof information from the root cause predictor. This approach consists of two\nscoring phases: the LLM-based confidence estimator first evaluates its\nconfidence in making judgments in the face of the current incident that\nreflects its ``grounded-ness\" level in reference data, then rates the root\ncause prediction based on historical references. An optimization step combines\nthese two scores for a final confidence assignment. We show that our method is\nable to produce calibrated confidence estimates for predicted root causes,\nvalidate the usefulness of retrieved historical data and the prompting strategy\nas well as the generalizability across different root cause prediction models.\nOur study takes an important move towards reliably and effectively embedding\nLLMs into cloud incident management systems.", "published": "2023-09-11 21:24:00", "link": "http://arxiv.org/abs/2309.05833v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Unsupervised Bias Detection in College Student Newspapers", "abstract": "This paper presents a pipeline with minimal human influence for scraping and\ndetecting bias on college newspaper archives. This paper introduces a framework\nfor scraping complex archive sites that automated tools fail to grab data from,\nand subsequently generates a dataset of 14 student papers with 23,154 entries.\nThis data can also then be queried by keyword to calculate bias by comparing\nthe sentiment of a large language model summary to the original article. The\nadvantages of this approach are that it is less comparative than reconstruction\nbias and requires less labelled data than generating keyword sentiment. Results\nare calculated on politically charged words as well as control words to show\nhow conclusions can be drawn. The complete method facilitates the extraction of\nnuanced insights with minimal assumptions and categorizations, paving the way\nfor a more objective understanding of bias within student newspaper sources.", "published": "2023-09-11 06:51:09", "link": "http://arxiv.org/abs/2309.06557v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.2.8; J.4"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Compiler Optimization", "abstract": "We explore the novel application of Large Language Models to code\noptimization. We present a 7B-parameter transformer model trained from scratch\nto optimize LLVM assembly for code size. The model takes as input unoptimized\nassembly and outputs a list of compiler options to best optimize the program.\nCrucially, during training, we ask the model to predict the instruction counts\nbefore and after optimization, and the optimized code itself. These auxiliary\nlearning tasks significantly improve the optimization performance of the model\nand improve the model's depth of understanding.\n  We evaluate on a large suite of test programs. Our approach achieves a 3.0%\nimprovement in reducing instruction counts over the compiler, outperforming two\nstate-of-the-art baselines that require thousands of compilations. Furthermore,\nthe model shows surprisingly strong code reasoning abilities, generating\ncompilable code 91% of the time and perfectly emulating the output of the\ncompiler 70% of the time.", "published": "2023-09-11 22:11:46", "link": "http://arxiv.org/abs/2309.07062v1", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "Enhancing Speaker Diarization with Large Language Models: A Contextual\n  Beam Search Approach", "abstract": "Large language models (LLMs) have shown great promise for capturing\ncontextual information in natural language processing tasks. We propose a novel\napproach to speaker diarization that incorporates the prowess of LLMs to\nexploit contextual cues in human dialogues. Our method builds upon an\nacoustic-based speaker diarization system by adding lexical information from an\nLLM in the inference stage. We model the multi-modal decoding process\nprobabilistically and perform joint acoustic and lexical beam search to\nincorporate cues from both modalities: audio and text. Our experiments\ndemonstrate that infusing lexical knowledge from the LLM into an acoustics-only\ndiarization system improves overall speaker-attributed word error rate\n(SA-WER). The experimental results show that LLMs can provide complementary\ninformation to acoustic models for the speaker diarization task via proposed\nbeam search decoding approach showing up to 39.8% relative delta-SA-WER\nimprovement from the baseline system. Thus, we substantiate that the proposed\ntechnique is able to exploit contextual information that is inaccessible to\nacoustics-only systems which is represented by speaker embeddings. In addition,\nthese findings point to the potential of using LLMs to improve speaker\ndiarization and other speech processing tasks by capturing semantic and\ncontextual cues.", "published": "2023-09-11 05:47:56", "link": "http://arxiv.org/abs/2309.05248v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards generalisable and calibrated synthetic speech detection with\n  self-supervised representations", "abstract": "Generalisation -- the ability of a model to perform well on unseen data -- is\ncrucial for building reliable deepfake detectors. However, recent studies have\nshown that the current audio deepfake models fall short of this desideratum. In\nthis work we investigate the potential of pretrained self-supervised\nrepresentations in building general and calibrated audio deepfake detection\nmodels. We show that large frozen representations coupled with a simple\nlogistic regression classifier are extremely effective in achieving strong\ngeneralisation capabilities: compared to the RawNet2 model, this approach\nreduces the equal error rate from 30.9% to 8.8% on a benchmark of eight\ndeepfake datasets, while learning less than 2k parameters. Moreover, the\nproposed method produces considerably more reliable predictions compared to\nprevious approaches making it more suitable for realistic use.", "published": "2023-09-11 11:11:28", "link": "http://arxiv.org/abs/2309.05384v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus", "abstract": "Multi-Modal automatic speech recognition (ASR) techniques aim to leverage\nadditional modalities to improve the performance of speech recognition systems.\nWhile existing approaches primarily focus on video or contextual information,\nthe utilization of extra supplementary textual information has been overlooked.\nRecognizing the abundance of online conference videos with slides, which\nprovide rich domain-specific information in the form of text and images, we\nrelease SlideSpeech, a large-scale audio-visual corpus enriched with slides.\nThe corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality\ntranscribed speech. Moreover, the corpus contains a significant amount of\nreal-time synchronized slides. In this work, we present the pipeline for\nconstructing the corpus and propose baseline methods for utilizing text\ninformation in the visual slide context. Through the application of keyword\nextraction and contextual ASR methods in the benchmark system, we demonstrate\nthe potential of improving speech recognition performance by incorporating\ntextual information from supplementary video slides.", "published": "2023-09-11 11:56:44", "link": "http://arxiv.org/abs/2309.05396v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Undecidability Results and Their Relevance in Modern Music Making", "abstract": "This paper delves into the intersection of computational theory and music,\nexamining the concept of undecidability and its significant, yet overlooked,\nimplications within the realm of modern music composition and production. It\nposits that undecidability, a principle traditionally associated with\ntheoretical computer science, extends its relevance to the music industry. The\nstudy adopts a multidimensional approach, focusing on five key areas: (1) the\nTuring completeness of Ableton, a widely used digital audio workstation, (2)\nthe undecidability of satisfiability in sound creation utilizing an array of\neffects, (3) the undecidability of constraints on polymeters in musical\ncompositions, (4) the undecidability of satisfiability in just intonation\nharmony constraints, and (5) the undecidability of \"new ordering systems\". In\naddition to providing theoretical proof for these assertions, the paper\nelucidates the practical relevance of these concepts for practitioners outside\nthe field of theoretical computer science. The ultimate aim is to foster a new\nunderstanding of undecidability in music, highlighting its broader\napplicability and potential to influence contemporary computer-assisted (and\ntraditional) music making.", "published": "2023-09-11 16:23:43", "link": "http://arxiv.org/abs/2309.05595v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Kernel Interpolation of Incident Sound Field in Region Including\n  Scattering Objects", "abstract": "A method for estimating the incident sound field inside a region containing\nscattering objects is proposed. The sound field estimation method has various\napplications, such as spatial audio capturing and spatial active noise control;\nhowever, most existing methods do not take into account the presence of\nscatterers within the target estimation region. Although several techniques\nexist that employ knowledge or measurements of the properties of the scattering\nobjects, it is usually difficult to obtain them precisely in advance, and their\nproperties may change during the estimation process. Our proposed method is\nbased on the kernel ridge regression of the incident field, with a separation\nfrom the scattering field represented by a spherical wave function expansion,\nthus eliminating the need for prior modeling or measurements of the scatterers.\nMoreover, we introduce a weighting matrix to induce smoothness of the\nscattering field in the angular direction, which alleviates the effect of the\ntruncation order of the expansion coefficients on the estimation accuracy.\nExperimental results indicate that the proposed method achieves a higher level\nof estimation accuracy than the kernel ridge regression without separation.", "published": "2023-09-11 17:26:00", "link": "http://arxiv.org/abs/2309.05634v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Natural Language Supervision for General-Purpose Audio Representations", "abstract": "Audio-Language models jointly learn multimodal text and audio representations\nthat enable Zero-Shot inference. Models rely on the encoders to create powerful\nrepresentations of the input and generalize to multiple tasks ranging from\nsounds, music, and speech. Although models have achieved remarkable\nperformance, there is still a performance gap with task-specific models. In\nthis paper, we propose a Contrastive Language-Audio Pretraining model that is\npretrained with a diverse collection of 4.6M audio-text pairs employing two\ninnovative encoders for Zero-Shot inference. To learn audio representations, we\ntrained an audio encoder on 22 audio tasks, instead of the standard training of\nsound event classification. To learn language representations, we trained an\nautoregressive decoder-only model instead of the standard encoder-only models.\nThen, the audio and language representations are brought into a joint\nmultimodal space using Contrastive Learning. We used our encoders to improve\nthe downstream performance by a margin. We extensively evaluated the\ngeneralization of our representations on 26 downstream tasks, the largest in\nthe literature. Our model achieves state of the art results in several tasks\nleading the way towards general-purpose audio representations.", "published": "2023-09-11 18:50:21", "link": "http://arxiv.org/abs/2309.05767v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Addressing Feature Imbalance in Sound Source Separation", "abstract": "Neural networks often suffer from a feature preference problem, where they\ntend to overly rely on specific features to solve a task while disregarding\nother features, even if those neglected features are essential for the task.\nFeature preference problems have primarily been investigated in classification\ntask. However, we observe that feature preference occurs in high-dimensional\nregression task, specifically, source separation. To mitigate feature\npreference in source separation, we propose FEAture BAlancing by Suppressing\nEasy feature (FEABASE). This approach enables efficient data utilization by\nlearning hidden information about the neglected feature. We evaluate our method\nin a multi-channel source separation task, where feature preference between\nspatial feature and timbre feature appears.", "published": "2023-09-11 08:11:27", "link": "http://arxiv.org/abs/2309.05287v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Applied design thinking in urban air mobility: creating the airtaxi\n  cabin design of the future from a user perspective", "abstract": "In the course of developing digital and future aviation cabin concepts at the\nGerman Aerospace Center, the exploration of user-centered and\nacceptance-enhancing methods plays a central role. The challenge here is to\nidentify the flexible range of requirements of different user groups for a\npreviously non-existent transport concept, to translate these into a concept\nand to generate a rapid evaluation process by the user groups. Therefore, this\npaper aims to demonstrate the application of the user-centered Design Thinking\nmethod in the design of cabin for future air taxis. Based on the Design\nThinking approach and its iterative process steps, the direct implementation is\ndescribed on the combined airport shuttle and intracity UAM concept. The main\nfocus is on the identification of key user requirements by means of a focus\ngroup study and the evaluation of initial cabin designs and key ideas by means\nof an online survey. Consequently, the creative design process of a digital\nprototype will be presented. In addition to an increased awareness and\nacceptance among the population towards a novel mode of transportation, the\napplication of the Design Thinking methodology offers a flexible and\nuser-centered approach for further testing and simulation scenarios.", "published": "2023-09-11 09:53:44", "link": "http://arxiv.org/abs/2309.05353v1", "categories": ["cs.HC", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.HC"}
{"title": "EDAC: Efficient Deployment of Audio Classification Models For COVID-19\n  Detection", "abstract": "The global spread of COVID-19 had severe consequences for public health and\nthe world economy. The quick onset of the pandemic highlighted the potential\nbenefits of cheap and deployable pre-screening methods to monitor the\nprevalence of the disease in a population. Various researchers made use of\nmachine learning methods in an attempt to detect COVID-19. The solutions\nleverage various input features, such as CT scans or cough audio signals, with\nstate-of-the-art results arising from deep neural network architectures.\nHowever, larger models require more compute; a pertinent consideration when\ndeploying to the edge. To address this, we first recreated two models that use\ncough audio recordings to detect COVID-19. Through applying network pruning and\nquantisation, we were able to compress these two architectures without reducing\nthe model's predictive performance. Specifically, we were able to achieve an\n105.76x and an 19.34x reduction in the compressed model file size with\ncorresponding 1.37x and 1.71x reductions in the inference times of the two\nmodels.", "published": "2023-09-11 10:07:51", "link": "http://arxiv.org/abs/2309.05357v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio\n  Representation", "abstract": "This paper describes a system developed for the GENEA (Generation and\nEvaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Our\nsolution builds on an existing diffusion-based motion synthesis model. We\npropose a contrastive speech and motion pretraining (CSMP) module, which learns\na joint embedding for speech and gesture with the aim to learn a semantic\ncoupling between these modalities. The output of the CSMP module is used as a\nconditioning signal in the diffusion-based gesture synthesis model in order to\nachieve semantically-aware co-speech gesture generation. Our entry achieved\nhighest human-likeness and highest speech appropriateness rating among the\nsubmitted entries. This indicates that our system is a promising approach to\nachieve human-like co-speech gestures in agents that carry semantic meaning.", "published": "2023-09-11 13:51:06", "link": "http://arxiv.org/abs/2309.05455v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T42", "I.2.6; I.2.7"], "primary_category": "eess.AS"}
{"title": "Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant\n  Everyday Functioning", "abstract": "Detection of subtle deficits in everyday functioning due to cognitive\nimpairment is important for early detection of neurodegenerative diseases,\nparticularly Alzheimer's disease. However, current standards for assessment of\neveryday functioning are based on qualitative, subjective ratings. Speech has\nbeen shown to provide good objective markers for cognitive impairments, but the\nassociation with cognition-relevant everyday functioning remains\nuninvestigated. In this study, we demonstrate the feasibility of using a\nsmartwatch-based application to collect acoustic features as objective markers\nfor detecting deficits in everyday functioning. We collected voice data during\nthe performance of cognitive tasks and daily conversation, as possible\napplication scenarios, from 54 older adults, along with a measure of everyday\nfunctioning. Machine learning models using acoustic features could detect\nindividuals with deficits in everyday functioning with up to 77.8% accuracy,\nwhich was higher than the 68.5% accuracy with standard neuropsychological\ntests. We also identified common acoustic features for robustly discriminating\ndeficits in everyday functioning across both types of voice data (cognitive\ntasks and daily conversation). Our results suggest that common acoustic\nfeatures extracted from different types of voice data can be used as markers\nfor deficits in everyday functioning.", "published": "2023-09-11 19:12:09", "link": "http://arxiv.org/abs/2309.05777v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Optimizing Audio Augmentations for Contrastive Learning of\n  Health-Related Acoustic Signals", "abstract": "Health-related acoustic signals, such as cough and breathing sounds, are\nrelevant for medical diagnosis and continuous health monitoring. Most existing\nmachine learning approaches for health acoustics are trained and evaluated on\nspecific tasks, limiting their generalizability across various healthcare\napplications. In this paper, we leverage a self-supervised learning framework,\nSimCLR with a Slowfast NFNet backbone, for contrastive learning of health\nacoustics. A crucial aspect of optimizing Slowfast NFNet for this application\nlies in identifying effective audio augmentations. We conduct an in-depth\nanalysis of various audio augmentation strategies and demonstrate that an\nappropriate augmentation strategy enhances the performance of the Slowfast\nNFNet audio encoder across a diverse set of health acoustic tasks. Our findings\nreveal that when augmentations are combined, they can produce synergistic\neffects that exceed the benefits seen when each is applied individually.", "published": "2023-09-11 22:03:34", "link": "http://arxiv.org/abs/2309.05843v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Instabilities in Convnets for Raw Audio", "abstract": "What makes waveform-based deep learning so hard? Despite numerous attempts at\ntraining convolutional neural networks (convnets) for filterbank design, they\noften fail to outperform hand-crafted baselines. These baselines are linear\ntime-invariant systems: as such, they can be approximated by convnets with wide\nreceptive fields. Yet, in practice, gradient-based optimization leads to\nsuboptimal approximations. In our article, we approach this phenomenon from the\nperspective of initialization. We present a theory of large deviations for the\nenergy response of FIR filterbanks with random Gaussian weights. We find that\ndeviations worsen for large filters and locally periodic input signals, which\nare both typical for audio signal processing applications. Numerical\nsimulations align with our theory and suggest that the condition number of a\nconvolutional layer follows a logarithmic scaling law between the number and\nlength of the filters, which is reminiscent of discrete wavelet bases.", "published": "2023-09-11 22:34:06", "link": "http://arxiv.org/abs/2309.05855v4", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion", "abstract": "This paper presents a novel hybrid Automatic Speech Recognition (ASR) system\ndesigned specifically for resource-constrained robots. The proposed approach\ncombines Hidden Markov Models (HMMs) with deep learning models and leverages\nsocket programming to distribute processing tasks effectively. In this\narchitecture, the HMM-based processing takes place within the robot, while a\nseparate PC handles the deep learning model. This synergy between HMMs and deep\nlearning enhances speech recognition accuracy significantly. We conducted\nexperiments across various robotic platforms, demonstrating real-time and\nprecise speech recognition capabilities. Notably, the system exhibits\nadaptability to changing acoustic conditions and compatibility with low-power\nhardware, making it highly effective in environments with limited computational\nresources. This hybrid ASR paradigm opens up promising possibilities for\nseamless human-robot interaction. In conclusion, our research introduces a\npioneering dimension to ASR techniques tailored for robotics. By employing\nsocket programming to distribute processing tasks across distinct devices and\nstrategically combining HMMs with deep learning models, our hybrid ASR system\nshowcases its potential to enable robots to comprehend and respond to spoken\nlanguage adeptly, even in environments with restricted computational resources.\nThis paradigm sets a innovative course for enhancing human-robot interaction\nacross a wide range of real-world scenarios.", "published": "2023-09-11 15:28:19", "link": "http://arxiv.org/abs/2309.07164v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "62M09 (Primary) 62F10, 62F12 (Secondary)", "I.2.7; I.2.9"], "primary_category": "eess.AS"}
{"title": "Hierarchical Audio-Visual Information Fusion with Multi-label Joint\n  Decoding for MER 2023", "abstract": "In this paper, we propose a novel framework for recognizing both discrete and\ndimensional emotions. In our framework, deep features extracted from foundation\nmodels are used as robust acoustic and visual representations of raw video.\nThree different structures based on attention-guided feature gathering (AFG)\nare designed for deep feature fusion. Then, we introduce a joint decoding\nstructure for emotion classification and valence regression in the decoding\nstage. A multi-task loss based on uncertainty is also designed to optimize the\nwhole process. Finally, by combining three different structures on the\nposterior probability level, we obtain the final predictions of discrete and\ndimensional emotions. When tested on the dataset of multimodal emotion\nrecognition challenge (MER 2023), the proposed framework yields consistent\nimprovements in both emotion classification and valence regression. Our final\nsystem achieves state-of-the-art performance and ranks third on the leaderboard\non MER-MULTI sub-challenge.", "published": "2023-09-11 03:19:10", "link": "http://arxiv.org/abs/2309.07925v1", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
