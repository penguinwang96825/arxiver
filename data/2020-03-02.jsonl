{"title": "Learning from Easy to Complex: Adaptive Multi-curricula Learning for\n  Neural Dialogue Generation", "abstract": "Current state-of-the-art neural dialogue systems are mainly data-driven and\nare trained on human-generated responses. However, due to the subjectivity and\nopen-ended nature of human conversations, the complexity of training dialogues\nvaries greatly. The noise and uneven complexity of query-response pairs impede\nthe learning efficiency and effects of the neural dialogue generation models.\nWhat is more, so far, there are no unified dialogue complexity measurements,\nand the dialogue complexity embodies multiple aspects of\nattributes---specificity, repetitiveness, relevance, etc. Inspired by human\nbehaviors of learning to converse, where children learn from easy dialogues to\ncomplex ones and dynamically adjust their learning progress, in this paper, we\nfirst analyze five dialogue attributes to measure the dialogue complexity in\nmultiple perspectives on three publicly available corpora. Then, we propose an\nadaptive multi-curricula learning framework to schedule a committee of the\norganized curricula. The framework is established upon the reinforcement\nlearning paradigm, which automatically chooses different curricula at the\nevolving learning process according to the learning status of the neural\ndialogue generation model. Extensive experiments conducted on five\nstate-of-the-art models demonstrate its learning efficiency and effectiveness\nwith respect to 13 automatic evaluation metrics and human judgments.", "published": "2020-03-02 03:09:28", "link": "http://arxiv.org/abs/2003.00639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Example-Guided Text Generation using Generative Adversarial\n  Transformers", "abstract": "We introduce a language generative model framework for generating a styled\nparagraph based on a context sentence and a style reference example. The\nframework consists of a style encoder and a texts decoder. The style encoder\nextracts a style code from the reference example, and the text decoder\ngenerates texts based on the style code and the context. We propose a novel\nobjective function to train our framework. We also investigate different\nnetwork design choices. We conduct extensive experimental validation with\ncomparison to strong baselines to validate the effectiveness of the proposed\nframework using a newly collected dataset with diverse text styles. Both code\nand dataset will be released upon publication.", "published": "2020-03-02 05:40:57", "link": "http://arxiv.org/abs/2003.00674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identification of primary and collateral tracks in stuttered speech", "abstract": "Disfluent speech has been previously addressed from two main perspectives:\nthe clinical perspective focusing on diagnostic, and the Natural Language\nProcessing (NLP) perspective aiming at modeling these events and detect them\nfor downstream tasks. In addition, previous works often used different metrics\ndepending on whether the input features are text or speech, making it difficult\nto compare the different contributions. Here, we introduce a new evaluation\nframework for disfluency detection inspired by the clinical and NLP perspective\ntogether with the theory of performance from \\cite{clark1996using} which\ndistinguishes between primary and collateral tracks. We introduce a novel\nforced-aligned disfluency dataset from a corpus of semi-directed interviews,\nand present baseline results directly comparing the performance of text-based\nfeatures (word and span information) and speech-based (acoustic-prosodic\ninformation). Finally, we introduce new audio features inspired by the\nword-based span features. We show experimentally that using these features\noutperformed the baselines for speech-based predictions on the present dataset.", "published": "2020-03-02 16:50:33", "link": "http://arxiv.org/abs/2003.01018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Sample Distillation", "abstract": "In the past decade, there has been substantial progress at training\nincreasingly deep neural networks. Recent advances within the teacher--student\ntraining paradigm have established that information about past training updates\nshow promise as a source of guidance during subsequent training steps. Based on\nthis notion, in this paper, we propose Long Short-Term Sample Distillation, a\nnovel training policy that simultaneously leverages multiple phases of the\nprevious training process to guide the later training updates to a neural\nnetwork, while efficiently proceeding in just one single generation pass. With\nLong Short-Term Sample Distillation, the supervision signal for each sample is\ndecomposed into two parts: a long-term signal and a short-term one. The\nlong-term teacher draws on snapshots from several epochs ago in order to\nprovide steadfast guidance and to guarantee teacher--student differences, while\nthe short-term one yields more up-to-date cues with the goal of enabling\nhigher-quality updates. Moreover, the teachers for each sample are unique, such\nthat, overall, the model learns from a very diverse set of teachers.\nComprehensive experimental results across a range of vision and NLP tasks\ndemonstrate the effectiveness of this new training method.", "published": "2020-03-02 10:03:14", "link": "http://arxiv.org/abs/2003.00739v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PhoBERT: Pre-trained language models for Vietnamese", "abstract": "We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the\nfirst public large-scale monolingual language models pre-trained for\nVietnamese. Experimental results show that PhoBERT consistently outperforms the\nrecent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and\nimproves the state-of-the-art in multiple Vietnamese-specific NLP tasks\nincluding Part-of-speech tagging, Dependency parsing, Named-entity recognition\nand Natural language inference. We release PhoBERT to facilitate future\nresearch and downstream applications for Vietnamese NLP. Our PhoBERT models are\navailable at https://github.com/VinAIResearch/PhoBERT", "published": "2020-03-02 10:21:17", "link": "http://arxiv.org/abs/2003.00744v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-View Learning for Vision-and-Language Navigation", "abstract": "Learning to navigate in a visual environment following natural language\ninstructions is a challenging task because natural language instructions are\nhighly variable, ambiguous, and under-specified. In this paper, we present a\nnovel training paradigm, Learn from EveryOne (LEO), which leverages multiple\ninstructions (as different views) for the same trajectory to resolve language\nambiguity and improve generalization. By sharing parameters across\ninstructions, our approach learns more effectively from limited training data\nand generalizes better in unseen environments. On the recent Room-to-Room (R2R)\nbenchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent\nas the base agent (25.3% $\\rightarrow$ 41.4%) in Success Rate weighted by Path\nLength (SPL). Further, LEO is complementary to most existing models for\nvision-and-language navigation, allowing for easy integration with the existing\ntechniques, leading to LEO+, which creates the new state of the art, pushing\nthe R2R benchmark to 62% (9% absolute improvement).", "published": "2020-03-02 13:07:46", "link": "http://arxiv.org/abs/2003.00857v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pathological speech detection using x-vector embeddings", "abstract": "The potential of speech as a non-invasive biomarker to assess a speaker's\nhealth has been repeatedly supported by the results of multiple works, for both\nphysical and psychological conditions. Traditional systems for speech-based\ndisease classification have focused on carefully designed knowledge-based\nfeatures. However, these features may not represent the disease's full\nsymptomatology, and may even overlook its more subtle manifestations. This has\nprompted researchers to move in the direction of general speaker\nrepresentations that inherently model symptoms, such as Gaussian Supervectors,\ni-vectors and, x-vectors. In this work, we focus on the latter, to assess their\napplicability as a general feature extraction method to the detection of\nParkinson's disease (PD) and obstructive sleep apnea (OSA). We test our\napproach against knowledge-based features and i-vectors, and report results for\ntwo European Portuguese corpora, for OSA and PD, as well as for an additional\nSpanish corpus for PD. Both x-vector and i-vector models were trained with an\nout-of-domain European Portuguese corpus. Our results show that x-vectors are\nable to perform better than knowledge-based features in same-language corpora.\nMoreover, while x-vectors performed similarly to i-vectors in matched\nconditions, they significantly outperform them when domain-mismatch occurs.", "published": "2020-03-02 13:10:18", "link": "http://arxiv.org/abs/2003.00864v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The STEM-ECR Dataset: Grounding Scientific Entity References in STEM\n  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources", "abstract": "We introduce the STEM (Science, Technology, Engineering, and Medicine)\nDataset for Scientific Entity Extraction, Classification, and Resolution,\nversion 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to\nprovide a benchmark for the evaluation of scientific entity extraction,\nclassification, and resolution tasks in a domain-independent fashion. It\ncomprises abstracts in 10 STEM disciplines that were found to be the most\nprolific ones on a major publishing platform. We describe the creation of such\na multidisciplinary corpus and highlight the obtained findings in terms of the\nfollowing features: 1) a generic conceptual formalism for scientific entities\nin a multidisciplinary scientific context; 2) the feasibility of the\ndomain-independent human annotation of scientific entities under such a generic\nformalism; 3) a performance benchmark obtainable for automatic extraction of\nmultidisciplinary scientific entities using BERT-based neural models; 4) a\ndelineated 3-step entity resolution procedure for human annotation of the\nscientific entities via encyclopedic entity linking and lexicographic word\nsense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic\nlinks and lexicographic senses for our entities. Our findings cumulatively\nindicate that human annotation and automatic learning of multidisciplinary\nscientific concepts as well as their semantic disambiguation in a wide-ranging\nsetting as STEM is reasonable.", "published": "2020-03-02 16:35:17", "link": "http://arxiv.org/abs/2003.01006v4", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Natural Language Processing Advancements By Deep Learning: A Survey", "abstract": "Natural Language Processing (NLP) helps empower intelligent machines by\nenhancing a better understanding of the human language for linguistic-based\nhuman-computer communication. Recent developments in computational power and\nthe advent of large amounts of linguistic data have heightened the need and\ndemand for automating semantic analysis using data-driven approaches. The\nutilization of data-driven strategies is pervasive now due to the significant\nimprovements demonstrated through the usage of deep learning methods in areas\nsuch as Computer Vision, Automatic Speech Recognition, and in particular, NLP.\nThis survey categorizes and addresses the different aspects and applications of\nNLP that have benefited from deep learning. It covers core NLP tasks and\napplications and describes how deep learning methods and models advance these\nareas. We further analyze and compare different approaches and state-of-the-art\nmodels.", "published": "2020-03-02 21:32:05", "link": "http://arxiv.org/abs/2003.01200v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer++", "abstract": "Recent advancements in attention mechanisms have replaced recurrent neural\nnetworks and its variants for machine translation tasks. Transformer using\nattention mechanism solely achieved state-of-the-art results in sequence\nmodeling. Neural machine translation based on the attention mechanism is\nparallelizable and addresses the problem of handling long-range dependencies\namong words in sentences more effectively than recurrent neural networks. One\nof the key concepts in attention is to learn three matrices, query, key, and\nvalue, where global dependencies among words are learned through linearly\nprojecting word embeddings through these matrices. Multiple query, key, value\nmatrices can be learned simultaneously focusing on a different subspace of the\nembedded dimension, which is called multi-head in Transformer. We argue that\ncertain dependencies among words could be learned better through an\nintermediate context than directly modeling word-word dependencies. This could\nhappen due to the nature of certain dependencies or lack of patterns that lend\nthem difficult to be modeled globally using multi-head self-attention. In this\nwork, we propose a new way of learning dependencies through a context in\nmulti-head using convolution. This new form of multi-head attention along with\nthe traditional form achieves better results than Transformer on the WMT 2014\nEnglish-to-German and English-to-French translation tasks. We also introduce a\nframework to learn POS tagging and NER information during the training of\nencoder which further improves results achieving a new state-of-the-art of 32.1\nBLEU, better than existing best by 1.4 BLEU, on the WMT 2014 English-to-German\nand 44.6 BLEU, better than existing best by 1.1 BLEU, on the WMT 2014\nEnglish-to-French translation tasks. We call this Transformer++.", "published": "2020-03-02 13:00:16", "link": "http://arxiv.org/abs/2003.04974v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Toward Interpretability of Dual-Encoder Models for Dialogue Response\n  Suggestions", "abstract": "This work shows how to improve and interpret the commonly used dual encoder\nmodel for response suggestion in dialogue. We present an attentive dual encoder\nmodel that includes an attention mechanism on top of the extracted word-level\nfeatures from two encoders, one for context and one for label respectively. To\nimprove the interpretability in the dual encoder models, we design a novel\nregularization loss to minimize the mutual information between unimportant\nwords and desired labels, in addition to the original attention method, so that\nimportant words are emphasized while unimportant words are de-emphasized. This\ncan help not only with model interpretability, but can also further improve\nmodel accuracy. We propose an approximation method that uses a neural network\nto calculate the mutual information. Furthermore, by adding a residual layer\nbetween raw word embeddings and the final encoded context feature, word-level\ninterpretability is preserved at the final prediction of the model. We compare\nthe proposed model with existing methods for the dialogue response task on two\npublic datasets (Persona and Ubuntu). The experiments demonstrate the\neffectiveness of the proposed model in terms of better Recall@1 accuracy and\nvisualized interpretability.", "published": "2020-03-02 21:26:06", "link": "http://arxiv.org/abs/2003.04998v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Inferring the location of reflecting surfaces exploiting loudspeaker\n  directivity", "abstract": "Accurate sound field reproduction in rooms is often limited by the lack of\nknowledge of the room characteristics. Information about the room shape or\nnearby reflecting boundaries can, in principle, be used to improve the accuracy\nof the reproduction. In this paper, we propose a method to infer the location\nof nearby reflecting boundaries from measurements on a microphone array. As\nopposed to traditional methods, we explicitly exploit the loudspeaker\ndirectivity model (beyond omnidirectional radiation) and the microphone array\ngeometry. This approach does not require noiseless timing information of the\nechoes as input, nor a tailored loudspeaker-wall-microphone measurement step.\nSimulations show the proposed model outperforms current methods that disregard\ndirectivity in reverberant environments.", "published": "2020-03-02 17:42:54", "link": "http://arxiv.org/abs/2003.01117v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel Singing Voice Separation by Deep Neural Network Informed\n  DOA Constrained CNMF", "abstract": "This work addresses the problem of multichannel source separation combining\ntwo powerful approaches, multichannel spectral factorization with recent\nmonophonic deep-learning (DL) based spectrum inference. Individual source\nspectra at different channels are estimated with a Masker-Denoiser Twin Network\n(MaD TwinNet), able to model long-term temporal patterns of a musical piece.\nThe monophonic source spectrograms are used within a spatial covariance mixing\nmodel based on Complex Non-Negative Matrix Factorization (CNMF) that predicts\nthe spatial characteristics of each source. The proposed framework is evaluated\non the task of singing voice separation with a large multichannel dataset.\nExperimental results show that our joint DL+CNMF method outperforms both the\nindividual monophonic DL-based separation and the multichannel CNMF baseline\nmethods.", "published": "2020-03-02 19:46:53", "link": "http://arxiv.org/abs/2003.01162v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised learning of glottal pulse positions in a neural\n  analysis-synthesis framework", "abstract": "This article investigates into recently emerging approaches that use deep\nneural networks for the estimation of glottal closure instants (GCI). We build\nupon our previous approach that used synthetic speech exclusively to create\nperfectly annotated training data and that had been shown to compare favourably\nwith other training approaches using electroglottograph (EGG) signals. Here we\nintroduce a semi-supervised training strategy that allows refining the\nestimator by means of an analysis-synthesis setup using real speech signals,\nfor which GCI ground truth does not exist. Evaluation of the analyser is\nperformed by means of comparing the GCI extracted from the glottal flow signal\ngenerated by the analyser with the GCI extracted from EGG on the CMU arctic\ndataset, where EGG signals were recorded in addition to speech. We observe that\n(1.) the artificial increase of the diversity of pulse shapes that has been\nused in our previous construction of the synthetic database is beneficial, (2.)\ntraining the GCI network in the analysis-synthesis setup allows achieving a\nvery significant improvement of the GCI analyser, (3.) additional\nregularisation strategies allow improving the final analysis network when\ntrained in the analysis-synthesis setup.", "published": "2020-03-02 22:18:10", "link": "http://arxiv.org/abs/2003.01220v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Uniform Array with Broadband Beamforming for Arbitrary Beam Patterns", "abstract": "Broadband beamforming is a technique to obtain the signal with a wide range\nof frequencies. It maintains the signal integrity and spatial selectivity over\nfrequencies. This is important in several applications such as microphone\narray, sonar array or radar where the operation range of the signal is several\noctaves. Using uniform array for broadband beamforming is the old topic but\nthere is no available design method for the arbitrary beam patterns, except the\noptimization methods. In this paper, we present a new method based on geometry\ntranslation and coordinate transformation to design broadband beamformer for\narbitrary beam patterns. The new method uses less computation time than the\noptimization methods and it could help to find a better configuration of the\narray such as fewer sensors, smaller size.", "published": "2020-03-02 16:19:04", "link": "http://arxiv.org/abs/2003.00991v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "One or Two Components? The Scattering Transform Answers", "abstract": "With the aim of constructing a biologically plausible model of machine\nlistening, we study the representation of a multicomponent stationary signal by\na wavelet scattering network. First, we show that renormalizing second-order\nnodes by their first-order parents gives a simple numerical criterion to assess\nwhether two neighboring components will interfere psychoacoustically. Secondly,\nwe run a manifold learning algorithm (Isomap) on scattering coefficients to\nvisualize the similarity space underlying parametric additive synthesis.\nThirdly, we generalize the \"one or two components\" framework to three sine\nwaves or more, and prove that the effective scattering depth of a Fourier\nseries grows in logarithmic proportion to its bandwidth.", "published": "2020-03-02 17:15:06", "link": "http://arxiv.org/abs/2003.01037v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
