{"title": "\"Killing Me\" Is Not a Spoiler: Spoiler Detection Model using Graph\n  Neural Networks with Dependency Relation-Aware Attention Mechanism", "abstract": "Several machine learning-based spoiler detection models have been proposed\nrecently to protect users from spoilers on review websites. Although dependency\nrelations between context words are important for detecting spoilers, current\nattention-based spoiler detection models are insufficient for utilizing\ndependency relations. To address this problem, we propose a new spoiler\ndetection model called SDGNN that is based on syntax-aware graph neural\nnetworks. In the experiments on two real-world benchmark datasets, we show that\nour SDGNN outperforms the existing spoiler detection models.", "published": "2021-01-15 05:34:17", "link": "http://arxiv.org/abs/2101.05972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-grained decomposition and fine-grained interaction for multi-hop\n  question answering", "abstract": "Recent advances regarding question answering and reading comprehension have\nresulted in models that surpass human performance when the answer is contained\nin a single, continuous passage of text, requiring only single-hop reasoning.\nHowever, in actual scenarios, lots of complex queries require multi-hop\nreasoning. The key to the Question Answering task is semantic feature\ninteraction between documents and questions, which is widely processed by\nBi-directional Attention Flow (Bi-DAF), but Bi-DAF generally captures only the\nsurface semantics of words in complex questions and fails to capture implied\nsemantic feature of intermediate answers. As a result, Bi-DAF partially ignores\npart of the contexts related to the question and cannot extract the most\nimportant parts of multiple documents. In this paper we propose a new model\narchitecture for multi-hop question answering, by applying two completion\nstrategies: (1) Coarse-Grain complex question Decomposition (CGDe) strategy are\nintroduced to decompose complex question into simple ones under the condition\nof without any additional annotations (2) Fine-Grained Interaction (FGIn)\nstrategy are introduced to better represent each word in the document and\nextract more comprehensive and accurate sentences related to the inference\npath. The above two strategies are combined and tested on the SQuAD and\nHotpotQA datasets, and the experimental results show that our method\noutperforms state-of-the-art baselines.", "published": "2021-01-15 06:56:34", "link": "http://arxiv.org/abs/2101.05988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Walk in Wild: An Ensemble Approach for Hostility Detection in Hindi\n  Posts", "abstract": "As the reach of the internet increases, pejorative terms started flooding\nover social media platforms. This leads to the necessity of identifying hostile\ncontent on social media platforms. Identification of hostile contents on\nlow-resource languages like Hindi poses different challenges due to its diverse\nsyntactic structure compared to English. In this paper, we develop a simple\nensemble based model on pre-trained mBERT and popular classification algorithms\nlike Artificial Neural Network (ANN) and XGBoost for hostility detection in\nHindi posts. We formulated this problem as binary classification (hostile and\nnon-hostile class) and multi-label multi-class classification problem (for more\nfine-grained hostile classes). We received third overall rank in the\ncompetition and weighted F1-scores of ~0.969 and ~0.61 on the binary and\nmulti-label multi-class classification tasks respectively.", "published": "2021-01-15 07:49:27", "link": "http://arxiv.org/abs/2101.06004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multitask Learning with Dependency Parsing for Japanese\n  Semantic Role Labeling Improves Performance of Argument Identification", "abstract": "With the advent of FrameNet and PropBank, many semantic role labeling (SRL)\nsystems have been proposed in English. Although research on Japanese predicate\nargument structure analysis (PASA) has been conducted, most studies focused on\nsurface cases. There are only few previous works on Japanese SRL for deep\ncases, and their models' accuracies are low. Therefore, we propose a\nhierarchical multitask learning method with dependency parsing (DP) and show\nthat our model achieves state-of-the-art results in Japanese SRL. Also, we\nconduct experiments with a joint model that performs both argument\nidentification and argument classification simultaneously. The result suggests\nthat multitasking with DP is mainly effective for argument identification.", "published": "2021-01-15 11:41:20", "link": "http://arxiv.org/abs/2101.06071v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Post-editing and Machine Translation on Creativity and\n  Reading Experience", "abstract": "This article presents the results of a study involving the translation of a\nfictional story from English into Catalan in three modalities:\nmachine-translated (MT), post-edited (MTPE) and translated without aid (HT).\nEach translation was analysed to evaluate its creativity. Subsequently, a\ncohort of 88 Catalan participants read the story in a randomly assigned\nmodality and completed a survey. The results show that HT presented a higher\ncreativity score if compared to MTPE and MT. HT also ranked higher in narrative\nengagement, and translation reception, while MTPE ranked marginally higher in\nenjoyment. HT and MTPE show no statistically significant differences in any\ncategory, whereas MT does in all variables tested. We conclude that creativity\nis highest when professional translators intervene in the process, especially\nwhen working without any aid. We hypothesize that creativity in translation\ncould be the factor that enhances reading engagement and the reception of\ntranslated literary texts.", "published": "2021-01-15 14:11:11", "link": "http://arxiv.org/abs/2101.06125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset:\n  Collection, Insights and Improvements", "abstract": "Truly real-life data presents a strong, but exciting challenge for sentiment\nand emotion research. The high variety of possible `in-the-wild' properties\nmakes large datasets such as these indispensable with respect to building\nrobust machine learning models. A sufficient quantity of data covering a deep\nvariety in the challenges of each modality to force the exploratory analysis of\nthe interplay of all modalities has not yet been made available in this\ncontext. In this contribution, we present MuSe-CaR, a first of its kind\nmultimodal dataset. The data is publicly available as it recently served as the\ntesting bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on\nthe tasks of emotion, emotion-target engagement, and trustworthiness\nrecognition by means of comprehensively integrating the audio-visual and\nlanguage modalities. Furthermore, we give a thorough overview of the dataset in\nterms of collection and annotation, including annotation tiers not used in this\nyear's MuSe 2020. In addition, for one of the sub-challenges - predicting the\nlevel of trustworthiness - no participant outperformed the baseline model, and\nso we propose a simple, but highly efficient Multi-Head-Attention network that\nexceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %\nimprovement).", "published": "2021-01-15 10:40:37", "link": "http://arxiv.org/abs/2101.06053v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Unstructured Knowledge Access in Task-oriented Dialog Modeling using\n  Language Inference, Knowledge Retrieval and Knowledge-Integrative Response\n  Generation", "abstract": "Dialog systems enriched with external knowledge can handle user queries that\nare outside the scope of the supporting databases/APIs. In this paper, we\nfollow the baseline provided in DSTC9 Track 1 and propose three subsystems,\nKDEAK, KnowleDgEFactor, and Ens-GPT, which form the pipeline for a\ntask-oriented dialog system capable of accessing unstructured knowledge.\nSpecifically, KDEAK performs knowledge-seeking turn detection by formulating\nthe problem as natural language inference using knowledge from dialogs,\ndatabases and FAQs. KnowleDgEFactor accomplishes the knowledge selection task\nby formulating a factorized knowledge/document retrieval problem with three\nmodules performing domain, entity and knowledge level analyses. Ens-GPT\ngenerates a response by first processing multiple knowledge snippets, followed\nby an ensemble algorithm that decides if the response should be solely derived\nfrom a GPT2-XL model, or regenerated in combination with the top-ranking\nknowledge snippet. Experimental results demonstrate that the proposed pipeline\nsystem outperforms the baseline and generates high-quality responses, achieving\nat least 58.77% improvement on BLEU-4 score.", "published": "2021-01-15 11:24:32", "link": "http://arxiv.org/abs/2101.06066v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Supervision Signals for Style Transfer Models", "abstract": "Text style transfer has gained increasing attention from the research\ncommunity over the recent years. However, the proposed approaches vary in many\nways, which makes it hard to assess the individual contribution of the model\ncomponents. In style transfer, the most important component is the optimization\ntechnique used to guide the learning in the absence of parallel training data.\nIn this work we empirically compare the dominant optimization paradigms which\nprovide supervision signals during training: backtranslation, adversarial\ntraining and reinforcement learning. We find that backtranslation has\nmodel-specific limitations, which inhibits training style transfer models.\nReinforcement learning shows the best performance gains, while adversarial\ntraining, despite its popularity, does not offer an advantage over the latter\nalternative. In this work we also experiment with Minimum Risk Training, a\npopular technique in the machine translation community, which, to our\nknowledge, has not been empirically evaluated in the task of style transfer. We\nfill this research gap and empirically show its efficacy.", "published": "2021-01-15 15:33:30", "link": "http://arxiv.org/abs/2101.06172v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored\n  Search", "abstract": "Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.", "published": "2021-01-15 23:12:47", "link": "http://arxiv.org/abs/2101.06323v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer\n  on DuoRC", "abstract": "The purpose of this project is to evaluate three language models named BERT,\nALBERT, and LongFormer on the Question Answering dataset called DuoRC. The\nlanguage model task has two inputs, a question, and a context. The context is a\nparagraph or an entire document while the output is the answer based on the\ncontext. The goal is to perform grid search hyperparameter fine-tuning using\nDuoRC. Pretrained weights of the models are taken from the Huggingface library.\nDifferent sets of hyperparameters are used to fine-tune the models using two\nversions of DuoRC which are the SelfRC and the ParaphraseRC. The results show\nthat the ALBERT (pretrained using the SQuAD1 dataset) has an F1 score of 76.4\nand an accuracy score of 68.52 after fine-tuning on the SelfRC dataset. The\nLongformer model (pretrained using the SQuAD and SelfRC datasets) has an F1\nscore of 52.58 and an accuracy score of 46.60 after fine-tuning on the\nParaphraseRC dataset. The current results outperformed the results from the\nprevious model by DuoRC.", "published": "2021-01-15 23:28:32", "link": "http://arxiv.org/abs/2101.06326v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with\n  Learned Step Size Quantization", "abstract": "Recently, transformer-based language models such as BERT have shown\ntremendous performance improvement for a range of natural language processing\ntasks. However, these language models usually are computation expensive and\nmemory intensive during inference. As a result, it is difficult to deploy them\non resource-restricted devices. To improve the inference performance, as well\nas reduce the model size while maintaining the model accuracy, we propose a\nnovel quantization method named KDLSQ-BERT that combines knowledge distillation\n(KD) with learned step size quantization (LSQ) for language model quantization.\nThe main idea of our method is that the KD technique is leveraged to transfer\nthe knowledge from a \"teacher\" model to a \"student\" model when exploiting LSQ\nto quantize that \"student\" model during the quantization training process.\nExtensive experiment results on GLUE benchmark and SQuAD demonstrate that our\nproposed KDLSQ-BERT not only performs effectively when doing different bit\n(e.g. 2-bit $\\sim$ 8-bit) quantization, but also outperforms the existing BERT\nquantization methods, and even achieves comparable performance as the\nfull-precision base-line model while obtaining 14.9x compression ratio. Our\ncode will be public available.", "published": "2021-01-15 02:21:28", "link": "http://arxiv.org/abs/2101.05938v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hostility Detection and Covid-19 Fake News Detection in Social Media", "abstract": "Withtheadventofsocialmedia,therehasbeenanextremely rapid increase in the\ncontent shared online. Consequently, the propagation of fake news and hostile\nmessages on social media platforms has also skyrocketed. In this paper, we\naddress the problem of detecting hostile and fake content in the Devanagari\n(Hindi) script as a multi-class, multi-label problem. Using NLP techniques, we\nbuild a model that makes use of an abusive language detector coupled with\nfeatures extracted via Hindi BERT and Hindi FastText models and metadata. Our\nmodel achieves a 0.97 F1 score on coarse grain evaluation on Hostility\ndetection task. Additionally, we built models to identify fake news related to\nCovid-19 in English tweets. We leverage entity information extracted from the\ntweets along with textual representations learned from word embeddings and\nachieve a 0.93 F1 score on the English fake news detection task.", "published": "2021-01-15 03:24:36", "link": "http://arxiv.org/abs/2101.05953v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Annotation of epidemiological information in animal disease-related news\n  articles: guidelines", "abstract": "This paper describes a method for annotation of epidemiological information\nin animal disease-related news articles. The annotation guidelines are generic\nand aim to embrace all animal or zoonotic infectious diseases, regardless of\nthe pathogen involved or its way of transmission (e.g. vector-borne, airborne,\nby contact). The framework relies on the successive annotation of all the\nsentences from a news article. The annotator evaluates the sentences in a\nspecific epidemiological context, corresponding to the publication of the news\narticle.", "published": "2021-01-15 14:48:01", "link": "http://arxiv.org/abs/2101.06150v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AMFFCN: Attentional Multi-layer Feature Fusion Convolution Network for\n  Audio-visual Speech Enhancement", "abstract": "Audio-visual speech enhancement system is regarded to be one of promising\nsolutions for isolating and enhancing speech of desired speaker. Conventional\nmethods focus on predicting clean speech spectrum via a naive convolution\nneural network based encoder-decoder architecture, and these methods a) not\nadequate to use data fully and effectively, b) cannot process features\nselectively. The proposed model addresses these drawbacks, by a) applying a\nmodel that fuses audio and visual features layer by layer in encoding phase,\nand that feeds fused audio-visual features to each corresponding decoder layer,\nand more importantly, b) introducing soft threshold attention into the model to\nselect the informative modality softly. This paper proposes attentional\naudio-visual multi-layer feature fusion model, in which soft threshold\nattention unit are applied on feature mapping at every layer of decoder. The\nproposed model demonstrates the superior performance of the network against the\nstate-of-the-art models.", "published": "2021-01-15 05:59:12", "link": "http://arxiv.org/abs/2101.06268v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-layer Feature Fusion Convolution Network for Audio-visual Speech\n  Enhancement", "abstract": "Speech enhancement can potentially benefit from the visual information from\nthe target speaker, such as lip movement and facial expressions, because the\nvisual aspect of speech is essentially unaffected by acoustic environment. In\nthis paper, we address the problem of enhancing corrupted speech signal from\nvideos by using audio-visual (AV) neural processing. Most of recent AV speech\nenhancement approaches separately process the acoustic and visual features and\nfuse them via a simple concatenation operation. Although this strategy is\nconvenient and easy to implement, it comes with two major drawbacks: 1)\nevidence in speech perception suggests that in humans the AV integration occurs\nat a very early stage, in contrast to previous models that process the two\nmodalities separately at early stage and combine them only at a later stage,\nthus making the system less robust, and 2) a simple concatenation does not\nallow to control how the information from the acoustic and the visual\nmodalities is treated. To overcome these drawbacks, we propose a multi-layer\nfeature fusion convolution network (MFFCN), which separately process acoustic\nand visual modalities for preserving each modality features while fusing both\nmodalities' features layer by layer in encoding phase for enjoying the human AV\nspeech perception. In addition, considering the balance between the two\nmodalities, we design channel and spectral attention mechanisms to provide\nadditional flexibility in dealing with different types of information expanding\nthe representational ability of the convolution neural network. Experimental\nresults show that the proposed MFFCN demonstrates the performance of the\nnetwork superior to the state-of-the-art models.", "published": "2021-01-15 05:52:41", "link": "http://arxiv.org/abs/2101.05975v3", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
