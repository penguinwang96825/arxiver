{"title": "SparseLLM: Towards Global Pruning for Pre-trained Language Models", "abstract": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose SparseLLM, a novel framework that redefines the\nglobal pruning process into manageable, coordinated subproblems, allowing for\nresource-efficient optimization with global optimality. SparseLLM's approach,\nwhich conceptualizes LLMs as a chain of modular functions and leverages\nauxiliary variables for problem decomposition, not only facilitates a pragmatic\napplication on LLMs but also demonstrates significant performance improvements,\nparticularly in high-sparsity regimes where it surpasses current\nstate-of-the-art methods.", "published": "2024-02-28 00:09:07", "link": "http://arxiv.org/abs/2402.17946v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models\n  Exhibit Gender Performance Gaps", "abstract": "Current automatic speech recognition (ASR) models are designed to be used\nacross many languages and tasks without substantial changes. However, this\nbroad language coverage hides performance gaps within languages, for example,\nacross genders. Our study systematically evaluates the performance of two\nwidely used multilingual ASR models on three datasets, encompassing 19\nlanguages from eight language families and two speaking conditions. Our\nfindings reveal clear gender disparities, with the advantaged group varying\nacross languages and models. Surprisingly, those gaps are not explained by\nacoustic or lexical properties. However, probing internal model states reveals\na correlation with gendered performance gap. That is, the easier it is to\ndistinguish speaker gender in a language using probes, the more the gap\nreduces, favoring female speakers. Our results show that gender disparities\npersist even in state-of-the-art models. Our findings have implications for the\nimprovement of multilingual ASR systems, underscoring the importance of\naccessibility to training data and nuanced evaluation to predict and mitigate\ngender gaps. We release all code and artifacts at\nhttps://github.com/g8a9/multilingual-asr-gender-gap.", "published": "2024-02-28 00:24:29", "link": "http://arxiv.org/abs/2402.17954v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborative decoding of critical tokens for boosting factuality of\n  large language models", "abstract": "The most common training pipeline for large language models includes\npretraining, finetuning and aligning phases, with their respective resulting\nmodels, such as the pretrained model and the finetuned model. Finetuned and\naligned models show improved abilities of instruction following and safe\ngeneration, however their abilities to stay factual about the world are\nimpacted by the finetuning process. Furthermore, the common practice of using\nsampling during generation also increases chances of hallucination. In this\nwork, we introduce a collaborative decoding framework to harness the high\nfactuality within pretrained models through the concept of critical tokens. We\nfirst design a critical token classifier to decide which model to use for the\nnext token, and subsequently generates the next token using different decoding\nstrategies. Experiments with different models and datasets show that our\ndecoding framework is able to reduce model hallucination significantly,\nshowcasing the importance of the collaborative decoding framework.", "published": "2024-02-28 01:53:37", "link": "http://arxiv.org/abs/2402.17982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hire a Linguist!: Learning Endangered Languages with In-Context\n  Linguistic Descriptions", "abstract": "How can large language models (LLMs) process and translate endangered\nlanguages? Many languages lack a large corpus to train a decent LLM; therefore\nexisting LLMs rarely perform well in unseen, endangered languages. On the\ncontrary, we observe that 2000 endangered languages, though without a large\ncorpus, have a grammar book or a dictionary. We propose LINGOLLM, a\ntraining-free approach to enable an LLM to process unseen languages that hardly\noccur in its pre-training. Our key insight is to demonstrate linguistic\nknowledge of an unseen language in an LLM's prompt, including a dictionary, a\ngrammar book, and morphologically analyzed input text. We implement LINGOLLM on\ntop of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks\nacross 8 endangered or low-resource languages. Our results show that LINGOLLM\nelevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language\ndirections. Our findings demonstrate the tremendous value of linguistic\nknowledge in the age of LLMs for endangered languages. Our data, code, and\nmodel generations can be found at https://github.com/LLiLab/llm4endangeredlang.", "published": "2024-02-28 03:44:01", "link": "http://arxiv.org/abs/2402.18025v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crisis talk: analysis of the public debate around the energy crisis and\n  cost of living", "abstract": "A prominent media topic in the UK in the early 2020s is the energy crisis\naffecting the UK and most of Europe. It brings into a single public debate\nissues of energy dependency and sustainability, fair distribution of economic\nburdens and cost of living, as well as climate change, risk, and\nsustainability. In this paper, we investigate the public discourse around the\nenergy crisis and cost of living to identify how these pivotal and\ncontradictory issues are reconciled in this debate and to identify which social\nactors are involved and the role they play. We analyse a document corpus\nretrieved from UK newspapers from January 2014 to March 2023. We apply a\nvariety of natural language processing and data visualisation techniques to\nidentify key topics, novel trends, critical social actors, and the role they\nplay in the debate, along with the sentiment associated with those actors and\ntopics. We combine automated techniques with manual discourse analysis to\nexplore and validate the insights revealed in this study. The findings verify\nthe utility of these techniques by providing a flexible and scalable pipeline\nfor discourse analysis and providing critical insights for cost of living -\nenergy crisis nexus research.", "published": "2024-02-28 04:42:59", "link": "http://arxiv.org/abs/2402.18043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore", "abstract": "Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.", "published": "2024-02-28 04:43:46", "link": "http://arxiv.org/abs/2402.18045v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Truthfulness in Large Language Model Generations with\n  Local Intrinsic Dimension", "abstract": "We study how to characterize and predict the truthfulness of texts generated\nfrom large language models (LLMs), which serves as a crucial step in building\ntrust between humans and LLMs. Although several approaches based on entropy or\nverbalized uncertainty have been proposed to calibrate model predictions, these\nmethods are often intractable, sensitive to hyperparameters, and less reliable\nwhen applied in generative tasks with LLMs. In this paper, we suggest\ninvestigating internal activations and quantifying LLM's truthfulness using the\nlocal intrinsic dimension (LID) of model activations. Through experiments on\nfour question answering (QA) datasets, we demonstrate the effectiveness\nohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally,\nwe study intrinsic dimensions in LLMs and their relations with model layers,\nautoregressive language modeling, and the training of LLMs, revealing that\nintrinsic dimensions can be a powerful approach to understanding LLMs.", "published": "2024-02-28 04:56:21", "link": "http://arxiv.org/abs/2402.18048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualizing Generated Citation Texts", "abstract": "Abstractive citation text generation is usually framed as an infilling task,\nwhere a sequence-to-sequence model is trained to generate a citation given a\nreference paper and the context window around the target; the generated\ncitation should be a brief discussion of the reference paper as it relates to\nthe citing context. However, examining a recent LED-based citation generation\nsystem, we find that many of the generated citations are generic summaries of\nthe reference papers main contribution, ignoring the citation contexts focus on\na different topic. To address this problem, we propose a simple modification to\nthe citation text generation task: the generation target is not only the\ncitation itself, but the entire context window, including the target citation.\nThis approach can be easily applied to any abstractive citation generation\nsystem, and our experimental results show that training in this way is\npreferred by human readers and allows the generation model to make use of\ncontextual clues about what topic to discuss and what stance to take.", "published": "2024-02-28 05:24:21", "link": "http://arxiv.org/abs/2402.18054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions", "abstract": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exams or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets.\\footnote{Datasets and code are available at\n\\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical\nChallenge consists of questions based on challenging clinical cases, while\nMedbullets comprises simulated clinical questions. Both datasets are structured\nas multiple-choice question-answering tasks, accompanied by expert-written\nexplanations. We evaluate seven LLMs on the two datasets using various prompts.\nExperiments demonstrate that our datasets are harder than previous benchmarks.\nIn-depth automatic and human evaluations of model-generated explanations\nprovide insights into the promise and deficiency of LLMs for explainable\nmedical QA.", "published": "2024-02-28 05:44:41", "link": "http://arxiv.org/abs/2402.18060v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Efficacy of Grammar Error Correction: A Human Evaluation\n  Approach in the Japanese Context", "abstract": "In this study, we evaluated the performance of the state-of-the-art sequence\ntagging grammar error detection and correction model (SeqTagger) using Japanese\nuniversity students' writing samples. With an automatic annotation toolkit,\nERRANT, we first evaluated SeqTagger's performance on error correction with\nhuman expert correction as the benchmark. Then a human-annotated approach was\nadopted to evaluate Seqtagger's performance in error detection using a subset\nof the writing dataset. Results indicated a precision of 63.66% and a recall of\n20.19% for error correction in the full dataset. For the subset, after manual\nexclusion of irrelevant errors such as semantic and mechanical ones, the model\nshows an adjusted precision of 97.98% and an adjusted recall of 42.98% for\nerror detection, indicating the model's high accuracy but also its\nconservativeness. Thematic analysis on errors undetected by the model revealed\nthat determiners and articles, especially the latter, were predominant.\nSpecifically, in terms of context-independent errors, the model occasionally\noverlooked basic ones and faced challenges with overly erroneous or complex\nstructures. Meanwhile, context-dependent errors, notably those related to tense\nand noun number, as well as those possibly influenced by the students' first\nlanguage (L1), remained particularly challenging.", "published": "2024-02-28 06:43:43", "link": "http://arxiv.org/abs/2402.18101v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Concepts of Human Value in Large Language Models:\n  Is Value Alignment Consistent, Transferable and Controllable across\n  Languages?", "abstract": "Prior research has revealed that certain abstract concepts are linearly\nrepresented as directions in the representation space of LLMs, predominantly\ncentered around English. In this paper, we extend this investigation to a\nmultilingual context, with a specific focus on human values-related concepts\n(i.e., value concepts) due to their significance for AI safety. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality (e.g., monolingual, bilingual and\nmultilingual), we first empirically confirm the presence of value concepts\nwithin LLMs in a multilingual format. Further analysis on the cross-lingual\ncharacteristics of these concepts reveals 3 traits arising from language\nresource disparities: cross-lingual inconsistency, distorted linguistic\nrelationships, and unidirectional cross-lingual transfer between high- and\nlow-resource languages, all in terms of value concepts. Moreover, we validate\nthe feasibility of cross-lingual control over value alignment capabilities of\nLLMs, leveraging the dominant language as a source language. Ultimately,\nrecognizing the significant impact of LLMs' multilinguality on our results, we\nconsolidate our findings and provide prudent suggestions on the composition of\nmultilingual data for LLMs pre-training.", "published": "2024-02-28 07:18:39", "link": "http://arxiv.org/abs/2402.18120v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Intrinsic Dimension via Information Bottleneck for Explainable\n  Aspect-based Sentiment Analysis", "abstract": "Gradient-based explanation methods are increasingly used to interpret neural\nmodels in natural language processing (NLP) due to their high fidelity. Such\nmethods determine word-level importance using dimension-level gradient values\nthrough a norm function, often presuming equal significance for all gradient\ndimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA),\nour preliminary research suggests that only specific dimensions are pertinent.\nTo address this, we propose the Information Bottleneck-based Gradient\n(\\texttt{IBG}) explanation framework for ABSA. This framework leverages an\ninformation bottleneck to refine word embeddings into a concise intrinsic\ndimension, maintaining essential features and omitting unrelated information.\nComprehensive tests show that our \\texttt{IBG} approach considerably improves\nboth the models' performance and interpretability by identifying\nsentiment-aware features.", "published": "2024-02-28 08:11:05", "link": "http://arxiv.org/abs/2402.18145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIKO: Multimodal Intention Knowledge Distillation from Large Language\n  Models for Social-Media Commonsense Discovery", "abstract": "Social media has become a ubiquitous tool for connecting with others, staying\nupdated with news, expressing opinions, and finding entertainment. However,\nunderstanding the intention behind social media posts remains challenging due\nto the implicitness of intentions in social media posts, the need for\ncross-modality understanding of both text and images, and the presence of noisy\ninformation such as hashtags, misspelled words, and complicated abbreviations.\nTo address these challenges, we present MIKO, a Multimodal Intention Kowledge\nDistillatiOn framework that collaboratively leverages a Large Language Model\n(LLM) and a Multimodal Large Language Model (MLLM) to uncover users'\nintentions. Specifically, we use an MLLM to interpret the image and an LLM to\nextract key information from the text and finally instruct the LLM again to\ngenerate intentions. By applying MIKO to publicly available social media\ndatasets, we construct an intention knowledge base featuring 1,372K intentions\nrooted in 137,287 posts. We conduct a two-stage annotation to verify the\nquality of the generated knowledge and benchmark the performance of widely used\nLLMs for intention generation. We further apply MIKO to a sarcasm detection\ndataset and distill a student model to demonstrate the downstream benefits of\napplying intention knowledge.", "published": "2024-02-28 08:57:42", "link": "http://arxiv.org/abs/2402.18169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Pre-Training Graph Neural Networks for Context-Based Fake\n  News Detection: An Evaluation of Current Strategies and Resource Limitations", "abstract": "Pre-training of neural networks has recently revolutionized the field of\nNatural Language Processing (NLP) and has before demonstrated its effectiveness\nin computer vision. At the same time, advances around the detection of fake\nnews were mainly driven by the context-based paradigm, where different types of\nsignals (e.g. from social media) form graph-like structures that hold\ncontextual information apart from the news article to classify. We propose to\nmerge these two developments by applying pre-training of Graph Neural Networks\n(GNNs) in the domain of context-based fake news detection. Our experiments\nprovide an evaluation of different pre-training strategies for graph-based\nmisinformation detection and demonstrate that transfer learning does currently\nnot lead to significant improvements over training a model from scratch in the\ndomain. We argue that a major current issue is the lack of suitable large-scale\nresources that can be used for pre-training.", "published": "2024-02-28 09:10:25", "link": "http://arxiv.org/abs/2402.18179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering and Ranking: Diversity-preserved Instruction Selection\n  through Expert-aligned Quality Estimation", "abstract": "With contributions from the open-source community, a vast amount of\ninstruction tuning (IT) data has emerged. Given the significant resource\nallocation required for training and evaluating models, it is advantageous to\nhave an efficient method for selecting high-quality IT data. However, existing\nmethods for instruction data selection have limitations such as relying on\nfragile external APIs, being affected by biases in GPT models, or reducing the\ndiversity of the selected instruction dataset. In this paper, we propose an\nindustrial-friendly, expert-aligned and diversity-preserved instruction data\nselection method: Clustering and Ranking (CaR). CaR employs a two-step process:\nfirst, it ranks instruction pairs using a high-accuracy (84.25%) scoring model\naligned with expert preferences; second, it preserves dataset diversity through\nclustering. In our experiment, CaR efficiently selected a mere 1.96% of\nAlpaca's IT data, yet the resulting AlpaCaR model surpassed Alpaca's\nperformance by an average of 32.1% in GPT-4 evaluations. Moreover, we find that\ndata selecting is a consistent paradigm whether the pre-trained model is more\ncapable or the model parameters scaling up. Our approach employs compact models\nwith 550M parameters and incurs just 11.2% of the financial outlay of current\nmethods, enhancing its industrial deployability.", "published": "2024-02-28 09:27:29", "link": "http://arxiv.org/abs/2402.18191v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity\n  Recognition", "abstract": "Named entity recognition is one of the cornerstones of Danish NLP, essential\nfor language technology applications within both industry and research.\nHowever, Danish NER is inhibited by a lack of available datasets. As a\nconsequence, no current models are capable of fine-grained named entity\nrecognition, nor have they been evaluated for potential generalizability issues\nacross datasets and domains. To alleviate these limitations, this paper\nintroduces: 1) DANSK: a named entity dataset providing for high-granularity\ntagging as well as within-domain evaluation of models across a diverse set of\ndomains; 2) DaCy 2.6.0 that includes three generalizable models with\nfine-grained annotation; and 3) an evaluation of current state-of-the-art\nmodels' ability to generalize across domains. The evaluation of existing and\nnew models revealed notable performance discrepancies across domains, which\nshould be addressed within the field. Shortcomings of the annotation quality of\nthe dataset and its impact on model training and evaluation are also discussed.\nDespite these limitations, we advocate for the use of the new dataset DANSK\nalongside further work on the generalizability within Danish NER.", "published": "2024-02-28 10:01:00", "link": "http://arxiv.org/abs/2402.18209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in\n  Conversational History", "abstract": "With the recent emergence of powerful instruction-tuned large language models\n(LLMs), various helpful conversational Artificial Intelligence (AI) systems\nhave been deployed across many applications. When prompted by users, these AI\nsystems successfully perform a wide range of tasks as part of a conversation.\nTo provide some sort of memory and context, such approaches typically condition\ntheir output on the entire conversational history. Although this sensitivity to\nthe conversational history can often lead to improved performance on subsequent\ntasks, we find that performance can in fact also be negatively impacted, if\nthere is a task-switch. To the best of our knowledge, our work makes the first\nattempt to formalize the study of such vulnerabilities and interference of\ntasks in conversational LLMs caused by task-switches in the conversational\nhistory. Our experiments across 5 datasets with 15 task switches using popular\nLLMs reveal that many of the task-switches can lead to significant performance\ndegradation.", "published": "2024-02-28 10:19:05", "link": "http://arxiv.org/abs/2402.18216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Open-Ended Text Generation via Adaptive Decoding", "abstract": "Current language models decode text token by token according to probabilistic\ndistribution, and determining the appropriate candidates for the next token is\ncrucial to ensure generation quality. This study introduces adaptive decoding,\na mechanism that dynamically empowers language models to ascertain a sensible\ncandidate set during generation. Specifically, we introduce an entropy-based\nmetric called confidence and conceptualize determining the optimal candidate\nset as a confidence-increasing process. The rationality of including a token in\nthe candidate set is assessed by leveraging the increment of confidence.\nExperimental results reveal that our method balances diversity and coherence\nwell. The human evaluation shows that our method can generate human-preferred\ntext. Additionally, our method can potentially improve the reasoning ability of\nlanguage models.", "published": "2024-02-28 10:38:21", "link": "http://arxiv.org/abs/2402.18223v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning", "abstract": "Instruction Fine-tuning~(IFT) is a critical phase in building large language\nmodels~(LLMs). Previous works mainly focus on the IFT's role in the transfer of\nbehavioral norms and the learning of additional world knowledge. However, the\nunderstanding of the underlying mechanisms of IFT remains significantly\nlimited. In this paper, we design a knowledge intervention framework to\ndecouple the potential underlying factors of IFT, thereby enabling individual\nanalysis of different factors. Surprisingly, our experiments reveal that\nattempting to learn additional world knowledge through IFT often struggles to\nyield positive impacts and can even lead to markedly negative effects. Further,\nwe discover that maintaining internal knowledge consistency before and after\nIFT is a critical factor for achieving successful IFT. Our findings reveal the\nunderlying mechanisms of IFT and provide robust support for some very recent\nand potential future works.", "published": "2024-02-28 11:16:00", "link": "http://arxiv.org/abs/2402.18243v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A BiRGAT Model for Multi-intent Spoken Language Understanding with\n  Hierarchical Semantic Frames", "abstract": "Previous work on spoken language understanding (SLU) mainly focuses on\nsingle-intent settings, where each input utterance merely contains one user\nintent. This configuration significantly limits the surface form of user\nutterances and the capacity of output semantics. In this work, we first propose\na Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue\nSystem, called MIVS. The target semantic frame is organized in a 3-layer\nhierarchical structure to tackle the alignment and assignment problems in\nmulti-intent cases. Accordingly, we devise a BiRGAT model to encode the\nhierarchy of ontology items, the backbone of which is a dual relational graph\nattention network. Coupled with the 3-way pointer-generator decoder, our method\noutperforms traditional sequence labeling and classification-based schemes by a\nlarge margin.", "published": "2024-02-28 11:39:26", "link": "http://arxiv.org/abs/2402.18258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario", "abstract": "It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary.", "published": "2024-02-28 11:51:56", "link": "http://arxiv.org/abs/2402.18264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Understanding of Contrastive Sentence Representation\n  Learning: A Unified Paradigm for Gradient", "abstract": "Sentence Representation Learning (SRL) is a crucial task in Natural Language\nProcessing (NLP), where contrastive Self-Supervised Learning (SSL) is currently\na mainstream approach. However, the reasons behind its remarkable effectiveness\nremain unclear. Specifically, many studies have investigated the similarities\nbetween contrastive and non-contrastive SSL from a theoretical perspective.\nSuch similarities can be verified in classification tasks, where the two\napproaches achieve comparable performance. But in ranking tasks (i.e., Semantic\nTextual Similarity (STS) in SRL), contrastive SSL significantly outperforms\nnon-contrastive SSL. Therefore, two questions arise: First, *what commonalities\nenable various contrastive losses to achieve superior performance in STS?*\nSecond, *how can we make non-contrastive SSL also effective in STS?* To address\nthese questions, we start from the perspective of gradients and discover that\nfour effective contrastive losses can be integrated into a unified paradigm,\nwhich depends on three components: the **Gradient Dissipation**, the\n**Weight**, and the **Ratio**. Then, we conduct an in-depth analysis of the\nroles these components play in optimization and experimentally demonstrate\ntheir significance for model performance. Finally, by adjusting these\ncomponents, we enable non-contrastive SSL to achieve outstanding performance in\nSTS.", "published": "2024-02-28 12:17:40", "link": "http://arxiv.org/abs/2402.18281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning\n  with Large Language Models", "abstract": "Recent approaches in domain-specific named entity recognition (NER), such as\nbiomedical NER, have shown remarkable advances. However, they still lack of\nfaithfulness, producing erroneous predictions. We assume that knowledge of\nentities can be useful in verifying the correctness of the predictions. Despite\nthe usefulness of knowledge, resolving such errors with knowledge is\nnontrivial, since the knowledge itself does not directly indicate the\nground-truth label. To this end, we propose VerifiNER, a post-hoc verification\nframework that identifies errors from existing NER methods using knowledge and\nrevises them into more faithful predictions. Our framework leverages the\nreasoning abilities of large language models to adequately ground on knowledge\nand the contextual information in the verification process. We validate\neffectiveness of VerifiNER through extensive experiments on biomedical\ndatasets. The results suggest that VerifiNER can successfully verify errors\nfrom existing models as a model-agnostic approach. Further analyses on\nout-of-domain and low-resource settings show the usefulness of VerifiNER on\nreal-world applications.", "published": "2024-02-28 14:49:05", "link": "http://arxiv.org/abs/2402.18374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure\n  Knowledge in English-Centric Large Language Models", "abstract": "Despite the predominance of English in their training data, English-centric\nLarge Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability\nto perform multilingual tasks, raising questions about the depth and nature of\ntheir cross-lingual capabilities. This paper introduces the decomposed\nprompting approach to probe the linguistic structure understanding of these\nLLMs in sequence labeling tasks. Diverging from the single text-to-text prompt,\nour method generates for each token of the input sentence an individual prompt\nwhich asks for its linguistic label. We assess our method on the Universal\nDependencies part-of-speech tagging dataset for 38 languages, utilizing both\nEnglish-centric and multilingual LLMs. Our findings show that decomposed\nprompting surpasses the iterative prompting baseline in efficacy and efficiency\nunder zero- and few-shot settings. Further analysis reveals the influence of\nevaluation methods and the use of instructions in prompts. Our multilingual\ninvestigation shows that English-centric language models perform better on\naverage than multilingual models. Our study offers insights into the\nmultilingual transferability of English-centric LLMs, contributing to the\nunderstanding of their multilingual linguistic knowledge.", "published": "2024-02-28 15:15:39", "link": "http://arxiv.org/abs/2402.18397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Diverse Modeling Contexts with Collaborating Learning for\n  Neural Machine Translation", "abstract": "Autoregressive (AR) and Non-autoregressive (NAR) models are two types of\ngenerative models for Neural Machine Translation (NMT). AR models predict\ntokens in a word-by-word manner and can effectively capture the distribution of\nreal translations. NAR models predict tokens by extracting bidirectional\ncontextual information which can improve the inference speed but they suffer\nfrom performance degradation. Previous works utilized AR models to enhance NAR\nmodels by reducing the training data's complexity or incorporating the global\ninformation into AR models by virtue of NAR models. However, those investigated\nmethods only take advantage of the contextual information of a single type of\nmodel while neglecting the diversity in the contextual information that can be\nprovided by different types of models. In this paper, we propose a novel\ngeneric collaborative learning method, DCMCL, where AR and NAR models are\ntreated as collaborators instead of teachers and students. To hierarchically\nleverage the bilateral contextual information, token-level mutual learning and\nsequence-level contrastive learning are adopted between AR and NAR models.\nExtensive experiments on four widely used benchmarks show that the proposed\nDCMCL method can simultaneously improve both AR and NAR models with up to 1.38\nand 2.98 BLEU scores respectively, and can also outperform the current\nbest-unified model with up to 0.97 BLEU scores for both AR and NAR decoding.", "published": "2024-02-28 15:55:02", "link": "http://arxiv.org/abs/2402.18428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Task Prompting Elicits Embeddings from Large Language Models", "abstract": "We introduce a new unsupervised text embedding method, Meta-Task Prompting\nwith Explicit One-Word Limitation (MetaEOL), for generating high-quality\nsentence embeddings from Large Language Models (LLMs) without the need for\nmodel fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to\nproduce embeddings through a series of carefully designed prompts that address\nmultiple representational aspects. Our comprehensive experiments demonstrate\nthat embeddings averaged from various meta-tasks are versatile embeddings that\nyield competitive performance on Semantic Textual Similarity (STS) benchmarks\nand excel in downstream tasks, surpassing contrastive-trained models. Our\nfindings suggest a new scaling law, offering a versatile and resource-efficient\napproach for embedding generation across diverse scenarios.", "published": "2024-02-28 16:35:52", "link": "http://arxiv.org/abs/2402.18458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewsQs: Multi-Source Question Generation for the Inquiring Mind", "abstract": "We present NewsQs (news-cues), a dataset that provides question-answer pairs\nfor multiple news documents. To create NewsQs, we augment a traditional\nmulti-document summarization dataset with questions automatically generated by\na T5-Large model fine-tuned on FAQ-style news articles from the News On the Web\ncorpus. We show that fine-tuning a model with control codes produces questions\nthat are judged acceptable more often than the same model without them as\nmeasured through human evaluation. We use a QNLI model with high correlation\nwith human annotations to filter our data. We release our final dataset of\nhigh-quality questions, answers, and document clusters as a resource for future\nwork in query-based multi-document summarization.", "published": "2024-02-28 16:59:35", "link": "http://arxiv.org/abs/2402.18479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware\n  Classification", "abstract": "Employing Large Language Models (LLM) in various downstream applications such\nas classification is crucial, especially for smaller companies lacking the\nexpertise and resources required for fine-tuning a model. Fairness in LLMs\nhelps ensure inclusivity, equal representation based on factors such as race,\ngender and promotes responsible AI deployment. As the use of LLMs has become\nincreasingly prevalent, it is essential to assess whether LLMs can generate\nfair outcomes when subjected to considerations of fairness. In this study, we\nintroduce a framework outlining fairness regulations aligned with various\nfairness definitions, with each definition being modulated by varying degrees\nof abstraction. We explore the configuration for in-context learning and the\nprocedure for selecting in-context demonstrations using RAG, while\nincorporating fairness rules into the process. Experiments conducted with\ndifferent LLMs indicate that GPT-4 delivers superior results in terms of both\naccuracy and fairness compared to other models. This work is one of the early\nattempts to achieve fairness in prediction tasks by utilizing LLMs through\nin-context learning.", "published": "2024-02-28 17:29:27", "link": "http://arxiv.org/abs/2402.18502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability", "abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large\nlanguage models' (LLMs) ability to follow complex, domain-specific formats, a\ncrucial yet underexamined capability for their application as AI agents.\nDespite LLMs' advancements, existing benchmarks fail to assess their\nformat-following proficiency adequately. FoFo fills this gap with a diverse\nrange of real-world formats and instructions, developed through an AI-Human\ncollaborative method. Our evaluation across both open-source (e.g., Llama 2,\nWizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three\nkey findings: open-source models significantly lag behind closed-source ones in\nformat adherence; LLMs' format-following performance is independent of their\ncontent generation quality; and LLMs' format proficiency varies across\ndifferent domains. These insights suggest the need for specialized tuning for\nformat-following skills and highlight FoFo's role in guiding the selection of\ndomain-specific AI agents. FoFo is released here at\nhttps://github.com/SalesforceAIResearch/FoFo.", "published": "2024-02-28 19:23:27", "link": "http://arxiv.org/abs/2402.18667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RORA: Robust Free-Text Rationale Evaluation", "abstract": "Free-text rationales play a pivotal role in explainable NLP, bridging the\nknowledge and reasoning gaps behind a model's decision-making. However, due to\nthe diversity of potential reasoning paths and a corresponding lack of\ndefinitive ground truth, their evaluation remains a challenge. Existing\nevaluation metrics rely on the degree to which a rationale supports a target\nlabel, but we find these fall short in evaluating rationales that inadvertently\nleak the labels. To address this problem, we propose RORA, a Robust free-text\nRationale evaluation against label leakage. RORA quantifies the new information\nsupplied by a rationale to justify the label. This is achieved by assessing the\nconditional V-information \\citep{hewitt-etal-2021-conditional} with a\npredictive family robust against leaky features that can be exploited by a\nsmall model. RORA consistently outperforms existing approaches in evaluating\nhuman-written, synthetic, or model-generated rationales, particularly\ndemonstrating robustness against label leakage. We also show that RORA aligns\nwell with human judgment, providing a more reliable and accurate measurement\nacross diverse free-text rationales.", "published": "2024-02-28 19:46:21", "link": "http://arxiv.org/abs/2402.18678v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Much Annotation is Needed to Compare Summarization Models?", "abstract": "Modern instruction-tuned models have become highly capable in text generation\ntasks such as summarization, and are expected to be released at a steady pace.\nIn practice one may now wish to choose confidently, but with minimal effort,\nthe best performing summarization model when applied to a new domain or\npurpose. In this work, we empirically investigate the test sample size\nnecessary to select a preferred model in the context of news summarization.\nEmpirical results reveal that comparative evaluation converges quickly for both\nautomatic and human evaluation, with clear preferences for a system emerging\nfrom under 100 examples. The human preference data allows us to quantify how\nwell automatic scores can reproduce preference rankings across a variety of\ndownstream summarization tasks. We find that, while automatic metrics are\nstable at smaller sample sizes, only some automatic metrics are able to\nmoderately predict model win rates according to human preference.", "published": "2024-02-28 23:34:51", "link": "http://arxiv.org/abs/2402.18756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Iterative Associative Memory Model for Empathetic Response Generation", "abstract": "Empathetic response generation aims to comprehend the cognitive and emotional\nstates in dialogue utterances and generate proper responses. Psychological\ntheories posit that comprehending emotional and cognitive states necessitates\niteratively capturing and understanding associated words across dialogue\nutterances. However, existing approaches regard dialogue utterances as either a\nlong sequence or independent utterances for comprehension, which are prone to\noverlook the associated words between them. To address this issue, we propose\nan Iterative Associative Memory Model (IAMM) for empathetic response\ngeneration. Specifically, we employ a novel second-order interaction attention\nmechanism to iteratively capture vital associated words between dialogue\nutterances and situations, dialogue history, and a memory module (for storing\nassociated words), thereby accurately and nuancedly comprehending the\nutterances. We conduct experiments on the Empathetic-Dialogue dataset. Both\nautomatic and human evaluations validate the efficacy of the model. Variant\nexperiments on LLMs also demonstrate that attending to associated words\nimproves empathetic comprehension and expression.", "published": "2024-02-28 00:49:06", "link": "http://arxiv.org/abs/2402.17959v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "3MVRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document\n  Understanding", "abstract": "This paper presents a groundbreaking multimodal, multi-task, multi-teacher\njoint-grained knowledge distillation model for visually-rich form document\nunderstanding. The model is designed to leverage insights from both\nfine-grained and coarse-grained levels by facilitating a nuanced correlation\nbetween token and entity representations, addressing the complexities inherent\nin form documents. Additionally, we introduce new inter-grained and\ncross-grained loss functions to further refine diverse multi-teacher knowledge\ndistillation transfer process, presenting distribution gaps and a harmonised\nunderstanding of form documents. Through a comprehensive evaluation across\npublicly available form document understanding datasets, our proposed model\nconsistently outperforms existing baselines, showcasing its efficacy in\nhandling the intricate structures and content of visually complex form\ndocuments.", "published": "2024-02-28 01:56:00", "link": "http://arxiv.org/abs/2402.17983v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Sentiment Consolidation Framework for Meta-Review Generation", "abstract": "Modern natural language generation systems with Large Language Models (LLMs)\nexhibit the capability to generate a plausible summary of multiple documents;\nhowever, it is uncertain if they truly possess the capability of information\nconsolidation to generate summaries, especially on documents with opinionated\ninformation. We focus on meta-review generation, a form of sentiment\nsummarisation for the scientific domain. To make scientific sentiment\nsummarization more grounded, we hypothesize that human meta-reviewers follow a\nthree-layer framework of sentiment consolidation to write meta-reviews. Based\non the framework, we propose novel prompting methods for LLMs to generate\nmeta-reviews and evaluation metrics to assess the quality of generated\nmeta-reviews. Our framework is validated empirically as we find that prompting\nLLMs based on the framework -- compared with prompting them with simple\ninstructions -- generates better meta-reviews.", "published": "2024-02-28 02:40:09", "link": "http://arxiv.org/abs/2402.18005v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems", "abstract": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.", "published": "2024-02-28 03:16:44", "link": "http://arxiv.org/abs/2402.18013v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Mirror Cognitive Language Processing?", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.", "published": "2024-02-28 03:38:20", "link": "http://arxiv.org/abs/2402.18023v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Corpus-Steered Query Expansion with Large Language Models", "abstract": "Recent studies demonstrate that query expansions generated by large language\nmodels (LLMs) can considerably enhance information retrieval systems by\ngenerating hypothetical documents that answer the queries as expansions.\nHowever, challenges arise from misalignments between the expansions and the\nretrieval corpus, resulting in issues like hallucinations and outdated\ninformation due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo\nRelevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to\npromote the incorporation of knowledge embedded within the corpus. CSQE\nutilizes the relevance assessing capability of LLMs to systematically identify\npivotal sentences in the initially-retrieved documents. These corpus-originated\ntexts are subsequently used to expand the query together with LLM-knowledge\nempowered expansions, improving the relevance prediction between the query and\nthe target documents. Extensive experiments reveal that CSQE exhibits strong\nperformance without necessitating any training, especially with queries for\nwhich LLMs lack knowledge.", "published": "2024-02-28 03:58:58", "link": "http://arxiv.org/abs/2402.18031v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption", "abstract": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods,\nlow-rank adaptation (LoRA) is commonly applied to fine-tune large language\nmodels (LLMs). However, updating the weights of LoRA blocks effectively and\nexpeditiously is challenging due to the long calculation path in the original\nmodel. To address this, we propose ResLoRA, an improved framework of LoRA. By\nadding residual paths during training and using merging approaches to eliminate\nthese extra paths during inference, our method can achieve better results in\nfewer training steps without any extra trainable parameters or inference cost\ncompared to LoRA. The experiments on NLG, NLU, and text-to-image tasks\ndemonstrate the effectiveness of our method. To the best of our knowledge,\nResLoRA is the first work that combines the residual path with LoRA. The code\nof our method is available at\nhttps://github.com/microsoft/LMOps/tree/main/reslora .", "published": "2024-02-28 04:33:20", "link": "http://arxiv.org/abs/2402.18039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Datasets for Large Language Models: A Comprehensive Survey", "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.", "published": "2024-02-28 04:35:51", "link": "http://arxiv.org/abs/2402.18041v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEGAnno+: A Human-LLM Collaborative Annotation System", "abstract": "Large language models (LLMs) can label data faster and cheaper than humans\nfor various NLP tasks. Despite their prowess, LLMs may fall short in\nunderstanding of complex, sociocultural, or domain-specific context,\npotentially leading to incorrect annotations. Therefore, we advocate a\ncollaborative approach where humans and LLMs work together to produce reliable\nand high-quality labels. We present MEGAnno+, a human-LLM collaborative\nannotation system that offers effective LLM agent and annotation management,\nconvenient and robust LLM annotation, and exploratory verification of LLM\nlabels by humans.", "published": "2024-02-28 04:58:07", "link": "http://arxiv.org/abs/2402.18050v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "On the use of Silver Standard Data for Zero-shot Classification Tasks in\n  Information Extraction", "abstract": "The superior performance of supervised classification methods in the\ninformation extraction (IE) area heavily relies on a large amount of gold\nstandard data. Recent zero-shot classification methods converted the task to\nother NLP tasks (e.g., textual entailment) and used off-the-shelf models of\nthese NLP tasks to directly perform inference on the test data without using a\nlarge amount of IE annotation data. A potentially valuable by-product of these\nmethods is the large-scale silver standard data, i.e., pseudo-labeled data by\nthe off-the-shelf models of other NLP tasks. However, there is no further\ninvestigation into the use of these data. In this paper, we propose a new\nframework, Clean-LaVe, which aims to utilize silver standard data to enhance\nthe zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining\nsilver data; (2) Identifying relatively clean data from silver data; (3)\nFinetuning the off-the-shelf model using clean data; (4) Inference on the test\ndata. The experimental results show that Clean-LaVe can outperform the baseline\nby 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation\nclassification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot\ncross-lingual relation classification task, and by 8% on ACE05-E+ in the\nzero-shot event argument classification task. The code is share in\nhttps://github.com/wjw136/Clean_LaVe.git.", "published": "2024-02-28 05:45:37", "link": "http://arxiv.org/abs/2402.18061v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Editing Factual Knowledge and Explanatory Ability of Medical Large\n  Language Models", "abstract": "Model editing aims to precisely alter the behaviors of large language models\n(LLMs) in relation to specific knowledge, while leaving unrelated knowledge\nintact. This approach has proven effective in addressing issues of\nhallucination and outdated information in LLMs. However, the potential of using\nmodel editing to modify knowledge in the medical field remains largely\nunexplored, even though resolving hallucination is a pressing need in this\narea. Our observations indicate that current methods face significant\nchallenges in dealing with specialized and complex knowledge in medical domain.\nTherefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for\nmedical model editing. MedLaSA harnesses the strengths of both adding extra\nparameters and locate-then-edit methods for medical model editing. We utilize\ncausal tracing to identify the association of knowledge in neurons across\ndifferent layers, and generate a corresponding scale set from the association\nvalue for each piece of knowledge. Subsequently, we incorporate scalable\nadapters into the dense layers of LLMs. These adapters are assigned scaling\nvalues based on the corresponding specific knowledge, which allows for the\nadjustment of the adapter's weight and rank. The more similar the content, the\nmore consistent the scale between them. This ensures precise editing of\nsemantically identical knowledge while avoiding impact on unrelated knowledge.\nTo evaluate the editing impact on the behaviours of LLMs, we propose two model\nediting studies for medical domain: (1) editing factual knowledge for medical\nspecialization and (2) editing the explanatory ability for complex knowledge.\nWe build two novel medical benchmarking datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting unrelated\nknowledge.", "published": "2024-02-28 06:40:57", "link": "http://arxiv.org/abs/2402.18099v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation", "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising\nlanguage generation capabilities, particularly in performing tasks like complex\nreasoning and creative writing. Consequently, distillation through imitation of\nteacher responses has emerged as a popular technique to transfer knowledge from\nLLMs to more accessible, Small Language Models (SLMs). While this works well\nfor simpler tasks, there is a substantial performance gap on tasks requiring\nintricate language comprehension and creativity, such as humor generation. We\nhypothesize that this gap may stem from the fact that creative tasks might be\nhard to learn by imitation alone and explore whether an approach, involving\nsupplementary guidance from the teacher, could yield higher performance. To\naddress this, we study the effect of assigning a dual role to the LLM - as a\n\"teacher\" generating data, as well as a \"critic\" evaluating the student's\nperformance. Our experiments on humor generation reveal that the incorporation\nof feedback significantly narrows the performance gap between SLMs and their\nlarger counterparts compared to merely relying on imitation. As a result, our\nresearch highlights the potential of using feedback as an additional dimension\nto data when transferring complex language abilities via distillation.", "published": "2024-02-28 07:02:38", "link": "http://arxiv.org/abs/2402.18113v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniVS: Unified and Universal Video Segmentation with Prompts as Queries", "abstract": "Despite the recent advances in unified image segmentation (IS), developing a\nunified video segmentation (VS) model remains a challenge. This is mainly\nbecause generic category-specified VS tasks need to detect all objects and\ntrack them across consecutive frames, while prompt-guided VS tasks require\nre-identifying the target with visual/text prompts throughout the entire video,\nmaking it hard to handle the different tasks with the same architecture. We\nmake an attempt to address these issues and present a novel unified VS\narchitecture, namely UniVS, by using prompts as queries. UniVS averages the\nprompt features of the target from previous frames as its initial query to\nexplicitly decode masks, and introduces a target-wise prompt cross-attention\nlayer in the mask decoder to integrate prompt features in the memory pool. By\ntaking the predicted masks of entities from previous frames as their visual\nprompts, UniVS converts different VS tasks into prompt-guided target\nsegmentation, eliminating the heuristic inter-frame matching process. Our\nframework not only unifies the different VS tasks but also naturally achieves\nuniversal training and testing, ensuring robust performance across different\nscenarios. UniVS shows a commendable balance between performance and\nuniversality on 10 challenging VS benchmarks, covering video instance,\nsemantic, panoptic, object, and referring segmentation tasks. Code can be found\nat \\url{https://github.com/MinghanLi/UniVS}.", "published": "2024-02-28 07:05:27", "link": "http://arxiv.org/abs/2402.18115v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for\n  Aminoacian", "abstract": "This study assesses four cutting-edge language models in the underexplored\nAminoacian language. Through evaluation, it scrutinizes their adaptability,\neffectiveness, and limitations in text generation, semantic coherence, and\ncontextual understanding. Uncovering insights into these models' performance in\na low-resourced language, this research pioneers pathways to bridge linguistic\ngaps. By offering benchmarks and understanding challenges, it lays groundwork\nfor future advancements in natural language processing, aiming to elevate the\napplicability of language models in similar linguistic landscapes, marking a\nsignificant step toward inclusivity and progress in language technology.", "published": "2024-02-28 07:22:13", "link": "http://arxiv.org/abs/2402.18121v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cause and Effect: Can Large Language Models Truly Understand Causality?", "abstract": "With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.", "published": "2024-02-28 08:02:14", "link": "http://arxiv.org/abs/2402.18139v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Quantized Large Language Models", "abstract": "Post-training quantization (PTQ) has emerged as a promising technique to\nreduce the cost of large language models (LLMs). Specifically, PTQ can\neffectively mitigate memory consumption and reduce computational overhead in\nLLMs. To meet the requirements of both high efficiency and performance across\ndiverse scenarios, a comprehensive evaluation of quantized LLMs is essential to\nguide the selection of quantization methods. This paper presents a thorough\nevaluation of these factors by evaluating the effect of PTQ on Weight,\nActivation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon,\nBloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with\nparameters ranging from 125M to 180B. The evaluation encompasses five types of\ntasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context\ntasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization\nmethods to demonstrate their applicability. Based on the extensive experiments,\nwe systematically summarize the effect of quantization, provide recommendations\nto apply quantization techniques, and point out future directions. The code can\nbe found in https://github.com/thu-nics/qllm-eval.", "published": "2024-02-28 08:43:05", "link": "http://arxiv.org/abs/2402.18158v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prospect Personalized Recommendation on Large Language Model-based Agent\n  Platform", "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges\nus to inspect the information system infrastructure to support Agent-level\ninformation processing and to adapt to the characteristics of Large Language\nModel (LLM)-based Agents, such as interactivity. In this work, we envisage the\nprospect of the recommender system on LLM-based Agent platforms and introduce a\nnovel recommendation paradigm called Rec4Agentverse, comprised of Agent Items\nand Agent Recommender. Rec4Agentverse emphasizes the collaboration between\nAgent Items and Agent Recommender, thereby promoting personalized information\nservices and enhancing the exchange of information beyond the traditional\nuser-recommender feedback loop. Additionally, we prospect the evolution of\nRec4Agentverse and conceptualize it into three stages based on the enhancement\nof the interaction and information exchange among Agent Items, Agent\nRecommender, and the user. A preliminary study involving several cases of\nRec4Agentverse validates its significant potential for application. Lastly, we\ndiscuss potential issues and promising directions for future research.", "published": "2024-02-28 11:12:17", "link": "http://arxiv.org/abs/2402.18240v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Generalist Prompting for Large Language Models by Mental Models", "abstract": "Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.", "published": "2024-02-28 11:29:09", "link": "http://arxiv.org/abs/2402.18252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multimodal Pre-training for Visually Rich Webpage\n  Understanding", "abstract": "The growing prevalence of visually rich documents, such as webpages and\nscanned/digital-born documents (images, PDFs, etc.), has led to increased\ninterest in automatic document understanding and information extraction across\nacademia and industry. Although various document modalities, including image,\ntext, layout, and structure, facilitate human information retrieval, the\ninterconnected nature of these modalities presents challenges for neural\nnetworks. In this paper, we introduce WebLM, a multimodal pre-training network\ndesigned to address the limitations of solely modeling text and structure\nmodalities of HTML in webpages. Instead of processing document images as\nunified natural images, WebLM integrates the hierarchical structure of document\nimages to enhance the understanding of markup-language-based documents.\nAdditionally, we propose several pre-training tasks to model the interaction\namong text, structure, and image modalities effectively. Empirical results\ndemonstrate that the pre-trained WebLM significantly surpasses previous\nstate-of-the-art pre-trained models across several webpage understanding tasks.\nThe pre-trained models and code are available at\nhttps://github.com/X-LANCE/weblm.", "published": "2024-02-28 11:50:36", "link": "http://arxiv.org/abs/2402.18262v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Survey on Neural Question Generation: Methods, Applications, and\n  Prospects", "abstract": "In this survey, we present a detailed examination of the advancements in\nNeural Question Generation (NQG), a field leveraging neural network techniques\nto generate relevant questions from diverse inputs like knowledge bases, texts,\nand images. The survey begins with an overview of NQG's background,\nencompassing the task's problem formulation, prevalent benchmark datasets,\nestablished evaluation metrics, and notable applications. It then methodically\nclassifies NQG approaches into three predominant categories: structured NQG,\nwhich utilizes organized data sources, unstructured NQG, focusing on more\nloosely structured inputs like texts or visual content, and hybrid NQG, drawing\non diverse input modalities. This classification is followed by an in-depth\nanalysis of the distinct neural network models tailored for each category,\ndiscussing their inherent strengths and potential limitations. The survey\nculminates with a forward-looking perspective on the trajectory of NQG,\nidentifying emergent research trends and prospective developmental paths.\nAccompanying this survey is a curated collection of related research papers,\ndatasets and codes, systematically organized on Github, providing an extensive\nreference for those delving into NQG.", "published": "2024-02-28 11:57:12", "link": "http://arxiv.org/abs/2402.18267v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the\n  Key?", "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion\nimproves the reasoning abilities of LLMs. In this work, we reevaluate this\nclaim through systematic experiments, where we propose a novel group discussion\nframework to enrich the set of discussion mechanisms. Interestingly, our\nresults show that a single-agent LLM with strong prompts can achieve almost the\nsame performance as the best existing discussion approach on a wide range of\nreasoning tasks and backbone LLMs. We observe that the multi-agent discussion\nperforms better than a single agent only when there is no demonstration in the\nprompt. Further study reveals the common interaction mechanisms of LLMs during\nthe discussion.", "published": "2024-02-28 12:04:05", "link": "http://arxiv.org/abs/2402.18272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of\n  Pre-trained Language Models with Proximal Policy Optimization", "abstract": "Wide usage of ChatGPT has highlighted the potential of reinforcement learning\nfrom human feedback. However, its training pipeline relies on manual ranking, a\nresource-intensive process. To reduce labor costs, we propose a self-supervised\ntext ranking approach for applying Proximal-Policy-Optimization to fine-tune\nlanguage models while eliminating the need for human annotators. Our method\nbegins with probabilistic sampling to encourage a language model to generate\ndiverse responses for each input. We then employ TextRank and ISODATA\nalgorithms to rank and cluster these responses based on their semantics.\nSubsequently, we construct a reward model to learn the rank and optimize our\ngenerative policy. Our experimental results, conducted using two language\nmodels on three tasks, demonstrate that the models trained by our method\nconsiderably outperform baselines regarding BLEU, GLEU, and METEOR scores.\nFurthermore, our manual evaluation shows that our ranking results exhibit a\nremarkably high consistency with that of humans. This research significantly\nreduces training costs of proximal policy-guided models and demonstrates the\npotential for self-correction of language models.", "published": "2024-02-28 12:24:07", "link": "http://arxiv.org/abs/2402.18284v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to think step-by-step: A mechanistic understanding of\n  chain-of-thought reasoning", "abstract": "Despite superior reasoning prowess demonstrated by Large Language Models\n(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails\naround the internal mechanisms of the models that facilitate CoT generation.\nThis work investigates the neural sub-structures within LLMs that manifest CoT\nreasoning from a mechanistic point of view. From an analysis of Llama-2 7B\napplied to multistep reasoning over fictional ontologies, we demonstrate that\nLLMs deploy multiple parallel pathways of answer generation for step-by-step\nreasoning. These parallel pathways provide sequential answers from the input\nquestion context as well as the generated CoT. We observe a functional rift in\nthe middle layers of the LLM. Token representations in the initial half remain\nstrongly biased towards the pretraining prior, with the in-context prior taking\nover in the later half. This internal phase shift manifests in different\nfunctional components: attention heads that write the answer token appear in\nthe later half, attention heads that move information along ontological\nrelationships appear in the initial half, and so on. To the best of our\nknowledge, this is the first attempt towards mechanistic investigation of CoT\nreasoning in LLMs.", "published": "2024-02-28 13:14:20", "link": "http://arxiv.org/abs/2402.18312v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task\n  Adaptation", "abstract": "We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.", "published": "2024-02-28 13:54:57", "link": "http://arxiv.org/abs/2402.18334v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems\n  in Commonsense Reasoning", "abstract": "Large language models exhibit high-level commonsense reasoning abilities,\nespecially with enhancement methods like Chain-of-Thought (CoT). However, we\nfind these CoT-like methods lead to a considerable number of originally correct\nanswers turning wrong, which we define as the Toxic CoT problem. To interpret\nand mitigate this problem, we first utilize attribution tracing and causal\ntracing methods to probe the internal working mechanism of the LLM during CoT\nreasoning. Through comparisons, we prove that the model exhibits information\nloss from the question over the shallow attention layers when generating\nrationales or answers. Based on the probing findings, we design a novel method\ncalled RIDERS (Residual decodIng and sERial-position Swap), which compensates\nfor the information deficit in the model from both decoding and serial-position\nperspectives. Through extensive experiments on multiple commonsense reasoning\nbenchmarks, we validate that this method not only significantly eliminates\nToxic CoT problems (decreased by 23.6%), but also effectively improves the\nmodel's overall commonsense reasoning performance (increased by 5.5%).", "published": "2024-02-28 14:09:02", "link": "http://arxiv.org/abs/2402.18344v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tokenization Is More Than Compression", "abstract": "Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.", "published": "2024-02-28 14:52:15", "link": "http://arxiv.org/abs/2402.18376v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language\n  Models for Conversational Multi-Doc QA", "abstract": "Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.", "published": "2024-02-28 15:05:43", "link": "http://arxiv.org/abs/2402.18385v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for\n  Enhanced Reasoning and Communication", "abstract": "Natural language (NL) has long been the predominant format for human\ncognition and communication, and by extension, has been similarly pivotal in\nthe development and application of Large Language Models (LLMs). Yet, besides\nNL, LLMs have seen various non-NL formats during pre-training, such as code and\nlogical expression. NL's status as the optimal format for LLMs, particularly in\nsingle-LLM reasoning and multi-agent communication, has not been thoroughly\nexamined. In this work, we challenge the default use of NL by exploring the\nutility of non-NL formats in these contexts. We show that allowing LLMs to\nautonomously select the most suitable format before reasoning or communicating\nleads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs,\nand up to a 72.7\\% reduction in token usage in multi-agent communication, all\nwhile maintaining communicative effectiveness. Our comprehensive analysis\nfurther reveals that LLMs can devise a format from limited task instructions\nand that the devised format is effectively transferable across different LLMs.\nIntriguingly, the structured communication format decided by LLMs exhibits\nnotable parallels with established agent communication languages, suggesting a\nnatural evolution towards efficient, structured communication in agent\ncommunication. Our code is released at\n\\url{https://github.com/thunlp/AutoForm}.", "published": "2024-02-28 16:07:54", "link": "http://arxiv.org/abs/2402.18439v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Represent Beliefs of Self and Others", "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.", "published": "2024-02-28 17:25:59", "link": "http://arxiv.org/abs/2402.18496v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Simple linear attention language models balance the recall-throughput\n  tradeoff", "abstract": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.", "published": "2024-02-28 19:28:27", "link": "http://arxiv.org/abs/2402.18668v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding Language Models for Visual Entity Recognition", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition.\nOur model extends an autoregressive Multi-modal Large Language Model by\nemploying retrieval augmented constrained generation. It mitigates low\nperformance on out-of-domain entities while excelling in queries that require\nvisually-situated reasoning. Our method learns to distinguish similar entities\nwithin a vast label space by contrastively training on hard negative pairs in\nparallel with a sequence-to-sequence objective without an external retriever.\nDuring inference, a list of retrieved candidate answers explicitly guides\nlanguage generation by removing invalid decoding paths. The proposed method\nachieves significant improvements across different dataset splits in the\nrecently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises\nfrom 32.7% to 61.5%. It also demonstrates superior performance on the unseen\nand query splits by a substantial double-digit margin.", "published": "2024-02-28 20:22:17", "link": "http://arxiv.org/abs/2402.18695v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains", "abstract": "We introduce a new, extensive multidimensional quality metrics (MQM)\nannotated dataset covering 11 language pairs in the biomedical domain. We use\nthis dataset to investigate whether machine translation (MT) metrics which are\nfine-tuned on human-generated MT quality judgements are robust to domain shifts\nbetween training and inference. We find that fine-tuned metrics exhibit a\nsubstantial performance drop in the unseen domain scenario relative to metrics\nthat rely on the surface form, as well as pre-trained metrics which are not\nfine-tuned on MT quality judgments.", "published": "2024-02-28 23:01:24", "link": "http://arxiv.org/abs/2402.18747v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TroubleLLM: Align to Red Team Expert", "abstract": "Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.", "published": "2024-02-28 03:40:46", "link": "http://arxiv.org/abs/2403.00829v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MedAide: Leveraging Large Language Models for On-Premise Medical\n  Assistance on Edge Devices", "abstract": "Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.", "published": "2024-02-28 08:30:49", "link": "http://arxiv.org/abs/2403.00830v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "CLLMs: Consistency Large Language Models", "abstract": "Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.", "published": "2024-02-28 20:17:04", "link": "http://arxiv.org/abs/2403.00835v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient\n  Fine-Tuning of Large Language Models", "abstract": "In addressing the computational and memory demands of fine-tuning Large\nLanguage Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter\nAdaptation), a novel approach utilizing randomized half-selective parameter\nfreezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently\nbalances pre-trained knowledge retention and adaptability for task-specific\noptimizations. Through a randomized mechanism, LoRA-SP determines which\nparameters to update or freeze, significantly reducing computational and memory\nrequirements without compromising model performance. We evaluated LoRA-SP\nacross several benchmark NLP tasks, demonstrating its ability to achieve\ncompetitive performance with substantially lower resource consumption compared\nto traditional full-parameter fine-tuning and other parameter-efficient\ntechniques. LoRA-SP innovative approach not only facilitates the deployment of\nadvanced NLP models in resource-limited settings but also opens new research\navenues into effective and efficient model adaptation strategies.", "published": "2024-02-28 06:50:10", "link": "http://arxiv.org/abs/2403.08822v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for\n  Large Language Models", "abstract": "Transformer-based Language Models have become ubiquitous in Natural Language\nProcessing (NLP) due to their impressive performance on various tasks. However,\nexpensive training as well as inference remains a significant impediment to\ntheir widespread applicability. While enforcing sparsity at various levels of\nthe model architecture has found promise in addressing scaling and efficiency\nissues, there remains a disconnect between how sparsity affects network\ntopology. Inspired by brain neuronal networks, we explore sparsity approaches\nthrough the lens of network topology. Specifically, we exploit mechanisms seen\nin biological networks, such as preferential attachment and redundant synapse\npruning, and show that principled, model-agnostic sparsity approaches are\nperformant and efficient across diverse NLP tasks, spanning both classification\n(such as natural language inference) and generation (summarization, machine\ntranslation), despite our sole objective not being optimizing performance.\nNeuroPrune is competitive with (or sometimes superior to) baselines on\nperformance and can be up to $10$x faster in terms of training time for a given\nlevel of sparsity, simultaneously exhibiting measurable improvements in\ninference time in many cases.", "published": "2024-02-28 22:21:47", "link": "http://arxiv.org/abs/2404.01306v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "All in an Aggregated Image for In-Image Learning", "abstract": "This paper introduces a new in-context learning (ICL) mechanism called\nIn-Image Learning (I$^2$L) that combines demonstration examples, visual cues,\nand chain-of-thought reasoning into an aggregated image to enhance the\ncapabilities of Large Multimodal Models (e.g., GPT-4V) in multimodal reasoning\ntasks. Unlike previous approaches that rely on converting images to text or\nincorporating visual input into language models, I$^2$L consolidates all\ninformation into an aggregated image and leverages image processing,\nunderstanding, and reasoning abilities. This has several advantages: it reduces\ninaccurate textual descriptions of complex images, provides flexibility in\npositioning demonstration examples, and avoids multiple input images and\nlengthy prompts. We also introduce I$^2$L-Hybrid, a method that combines the\nstrengths of I$^2$L with other ICL methods. Specifically, it uses an automatic\nstrategy to select the most suitable method (I$^2$L or another certain ICL\nmethod) for a specific task instance. We conduct extensive experiments to\nassess the effectiveness of I$^2$L and I$^2$L-Hybrid on MathVista, which covers\na variety of complex multimodal reasoning tasks. Additionally, we investigate\nthe influence of image resolution, the number of demonstration examples in a\nsingle image, and the positions of these demonstrations in the aggregated image\non the effectiveness of I$^2$L. Our code is publicly available at\nhttps://github.com/AGI-Edgerunners/IIL.", "published": "2024-02-28 01:32:59", "link": "http://arxiv.org/abs/2402.17971v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large\n  Language Models with Per-tensor Quantization", "abstract": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross various tasks. However, the latency of inference and the large GPU\nmemory consumption of LLMs restrict their deployment performance. Recently,\nthere have been some efficient attempts to quantize LLMs, yet inference with\nlarge batch size or long sequence still has the issue of being compute-bound.\nFine-grained quantization methods have showcased their proficiency in achieving\nlow-bit quantization for LLMs, while requiring FP16 data type for linear layer\ncomputations, which is time-consuming when dealing with large batch size or\nlong sequence. In this paper, we introduce a method called FlattenQuant, which\nsignificantly reduces the maximum value of the tensor by flattening the large\nchannels in the tensor, to achieve low bit per-tensor quantization with minimal\naccuracy loss. Our experiments show that FlattenQuant can directly use 4 bits\nto achieve 48.29% of the linear layer calculation in LLMs, with the remaining\nlayers using 8 bits. The 4-bit matrix multiplication introduced in the\nFlattenQuant method can effectively address the compute-bound caused by large\nmatrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$\nmemory reduction for LLMs with negligible loss in accuracy.", "published": "2024-02-28 02:00:34", "link": "http://arxiv.org/abs/2402.17985v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Token-Specific Watermarking with Enhanced Detectability and Semantic\n  Coherence for Large Language Models", "abstract": "Large language models generate high-quality responses with potential\nmisinformation, underscoring the need for regulation by distinguishing\nAI-generated and human-written texts. Watermarking is pivotal in this context,\nwhich involves embedding hidden markers in texts during the LLM inference\nphase, which is imperceptible to humans. Achieving both the detectability of\ninserted watermarks and the semantic quality of generated texts is challenging.\nWhile current watermarking algorithms have made promising progress in this\ndirection, there remains significant scope for improvement. To address these\nchallenges, we introduce a novel multi-objective optimization (MOO) approach\nfor watermarking that utilizes lightweight networks to generate token-specific\nwatermarking logits and splitting ratios. By leveraging MOO to optimize for\nboth detection and semantic objective functions, our method simultaneously\nachieves detectability and semantic integrity. Experimental results show that\nour method outperforms current watermarking techniques in enhancing the\ndetectability of texts generated by LLMs while maintaining their semantic\ncoherence. Our code is available at https://github.com/mignonjia/TS_watermark.", "published": "2024-02-28 05:43:22", "link": "http://arxiv.org/abs/2402.18059v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Polos: Multimodal Metric Learning from Human Feedback for Image\n  Captioning", "abstract": "Establishing an automatic evaluation metric that closely aligns with human\njudgments is essential for effectively developing image captioning models.\nRecent data-driven metrics have demonstrated a stronger correlation with human\njudgments than classic metrics such as CIDEr; however they lack sufficient\ncapabilities to handle hallucinations and generalize across diverse images and\ntexts partially because they compute scalar similarities merely using\nembeddings learned from tasks unrelated to image captioning evaluation. In this\nstudy, we propose Polos, a supervised automatic evaluation metric for image\ncaptioning models. Polos computes scores from multimodal inputs, using a\nparallel feature extraction mechanism that leverages embeddings trained through\nlarge-scale contrastive learning. To train Polos, we introduce Multimodal\nMetric Learning from Human Feedback (M$^2$LHF), a framework for developing\nmetrics based on human feedback. We constructed the Polaris dataset, which\ncomprises 131K human judgments from 550 evaluators, which is approximately ten\ntimes larger than standard datasets. Our approach achieved state-of-the-art\nperformance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and\nthe Polaris dataset, thereby demonstrating its effectiveness and robustness.", "published": "2024-02-28 06:24:39", "link": "http://arxiv.org/abs/2402.18091v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.", "published": "2024-02-28 08:24:38", "link": "http://arxiv.org/abs/2402.18150v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and\n  Mitigating Knowledge Conflicts in Language Models", "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a\nremarkable capability to expand the internal memory boundaries of language\nmodels (LMs) by providing external context. However, internal memory and\nexternal context inevitably clash, leading to knowledge conflicts within LMs.\nIn this paper, we aim to interpret the mechanism of knowledge conflicts through\nthe lens of information flow, and then mitigate conflicts by precise\ninterventions at the pivotal point. We find there are some attention heads with\nopposite effects in the later layers, where memory heads can recall knowledge\nfrom internal memory, and context heads can retrieve knowledge from external\ncontext. Moreover, we reveal that the pivotal point at which knowledge\nconflicts emerge in LMs is the integration of inconsistent information flows by\nmemory heads and context heads. Inspired by the insights, we propose a novel\nmethod called Pruning Head via PatH PatcHing (PH3), which can efficiently\nmitigate knowledge conflicts by pruning conflicting attention heads without\nupdating model parameters. PH3 can flexibly control eight LMs to use internal\nmemory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3\ncan also improve the performance of LMs on open-domain QA tasks. We also\nconduct extensive experiments to demonstrate the cross-model, cross-relation,\nand cross-format generalization of our method.", "published": "2024-02-28 08:34:41", "link": "http://arxiv.org/abs/2402.18154v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "From Summary to Action: Enhancing Large Language Models for Complex\n  Tasks with Open World APIs", "abstract": "The distinction between humans and animals lies in the unique ability of\nhumans to use and create tools. Tools empower humans to overcome physiological\nlimitations, fostering the creation of magnificent civilizations. Similarly,\nenabling foundational models like Large Language Models (LLMs) with the\ncapacity to learn external tool usage may serve as a pivotal step toward\nrealizing artificial general intelligence. Previous studies in this field have\npredominantly pursued two distinct approaches to augment the tool invocation\ncapabilities of LLMs. The first approach emphasizes the construction of\nrelevant datasets for model fine-tuning. The second approach, in contrast, aims\nto fully exploit the inherent reasoning abilities of LLMs through in-context\nlearning strategies. In this work, we introduce a novel tool invocation\npipeline designed to control massive real-world APIs. This pipeline mirrors the\nhuman task-solving process, addressing complicated real-life user queries. At\neach step, we guide LLMs to summarize the achieved results and determine the\nnext course of action. We term this pipeline `from Summary to action', Sum2Act\nfor short. Empirical evaluations of our Sum2Act pipeline on the ToolBench\nbenchmark show significant performance improvements, outperforming established\nmethods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in\nenhancing LLMs for complex real-world tasks.", "published": "2024-02-28 08:42:23", "link": "http://arxiv.org/abs/2402.18157v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "CogBench: a large language model walks into a psychology lab", "abstract": "Large language models (LLMs) have significantly advanced the field of\nartificial intelligence. Yet, evaluating them comprehensively remains\nchallenging. We argue that this is partly due to the predominant focus on\nperformance metrics in most benchmarks. This paper introduces CogBench, a\nbenchmark that includes ten behavioral metrics derived from seven cognitive\npsychology experiments. This novel approach offers a toolkit for phenotyping\nLLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse\ndataset. We analyze this data using statistical multilevel modeling techniques,\naccounting for the nested dependencies among fine-tuned versions of specific\nLLMs. Our study highlights the crucial role of model size and reinforcement\nlearning from human feedback (RLHF) in improving performance and aligning with\nhuman behavior. Interestingly, we find that open-source models are less\nrisk-prone than proprietary models and that fine-tuning on code does not\nnecessarily enhance LLMs' behavior. Finally, we explore the effects of\nprompt-engineering techniques. We discover that chain-of-thought prompting\nimproves probabilistic reasoning, while take-a-step-back prompting fosters\nmodel-based behaviors.", "published": "2024-02-28 10:43:54", "link": "http://arxiv.org/abs/2402.18225v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploration of Adapter for Noise Robust Automatic Speech Recognition", "abstract": "Adapting an automatic speech recognition (ASR) system to unseen noise\nenvironments is crucial. Integrating adapters into neural networks has emerged\nas a potent technique for transfer learning. This study thoroughly investigates\nadapter-based ASR adaptation in noisy environments. We conducted experiments\nusing the CHiME--4 dataset. The results show that inserting the adapter in the\nshallow layer yields superior effectiveness, and there is no significant\ndifference between adapting solely within the shallow layer and adapting across\nall layers. The simulated data helps the system to improve its performance\nunder real noise conditions. Nonetheless, when the amount of data is the same,\nthe real data is more effective than the simulated data. Multi-condition\ntraining is still useful for adapter training. Furthermore, integrating\nadapters into speech enhancement-based ASR systems yields substantial\nimprovements.", "published": "2024-02-28 12:06:08", "link": "http://arxiv.org/abs/2402.18275v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.", "published": "2024-02-28 15:28:36", "link": "http://arxiv.org/abs/2402.18409v4", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Can GPT Improve the State of Prior Authorization via Guideline Based\n  Automated Question Answering?", "abstract": "Health insurance companies have a defined process called prior authorization\n(PA) which is a health plan cost-control process that requires doctors and\nother healthcare professionals to get clearance in advance from a health plan\nbefore performing a particular procedure on a patient in order to be eligible\nfor payment coverage. For health insurance companies, approving PA requests for\npatients in the medical domain is a time-consuming and challenging task. One of\nthose key challenges is validating if a request matches up to certain criteria\nsuch as age, gender, etc. In this work, we evaluate whether GPT can validate\nnumerous key factors, in turn helping health plans reach a decision drastically\nfaster. We frame it as a question answering task, prompting GPT to answer a\nquestion from patient electronic health record. We experiment with different\nconventional prompting techniques as well as introduce our own novel prompting\ntechnique. Moreover, we report qualitative assessment by humans on the natural\nlanguage generation outputs from our approach. Results show that our method\nachieves superior performance with the mean weighted F1 score of 0.61 as\ncompared to its standard counterparts.", "published": "2024-02-28 15:39:53", "link": "http://arxiv.org/abs/2402.18419v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Classification in Low and Moderate Resource Languages", "abstract": "It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.", "published": "2024-02-28 15:46:09", "link": "http://arxiv.org/abs/2402.18424v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HOP to the Next Tasks and Domains for Continual Learning in NLP", "abstract": "Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and\ndomains) by transferring knowledge acquired on previous problems, whilst\navoiding forgetting of past ones. Different from previous approaches which\nfocused on CL for one NLP task or domain in a specific use-case, in this paper,\nwe address a more general CL setting to learn from a sequence of problems in a\nunique framework. Our method, HOP, permits to hop across tasks and domains by\naddressing the CL problem along three directions: (i) we employ a set of\nadapters to generalize a large pre-trained model to unseen problems, (ii) we\ncompute high-order moments over the distribution of embedded representations to\ndistinguish independent and correlated statistics across different tasks and\ndomains, (iii) we process this enriched information with auxiliary heads\nspecialized for each end problem. Extensive experimental campaign on 4 NLP\napplications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of\nour HOP.", "published": "2024-02-28 16:21:02", "link": "http://arxiv.org/abs/2402.18449v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context\n  Retrieval", "abstract": "This paper investigates the gap in representation powers of Recurrent Neural\nNetworks (RNNs) and Transformers in the context of solving algorithmic\nproblems. We focus on understanding whether RNNs, known for their memory\nefficiency in handling long sequences, can match the performance of\nTransformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.\nOur theoretical analysis reveals that CoT improves RNNs but is insufficient to\nclose the gap with Transformers. A key bottleneck lies in the inability of RNNs\nto perfectly retrieve information from the context, even with CoT: for several\ntasks that explicitly or implicitly require this capability, such as\nassociative recall and determining if a graph is a tree, we prove that RNNs are\nnot expressive enough to solve the tasks while Transformers can solve them with\nease. Conversely, we prove that adopting techniques to enhance the in-context\nretrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)\nand adding a single Transformer layer, can elevate RNNs to be capable of\nsolving all polynomial-time solvable problems with CoT, hence closing the\nrepresentation gap with Transformers.", "published": "2024-02-28 17:38:06", "link": "http://arxiv.org/abs/2402.18510v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates", "abstract": "Public LLMs such as the Llama 2-Chat underwent alignment training and were\nconsidered safe. Recently Qi et al. [2024] reported that even benign\nfine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the\nmodels. The current paper is about methods and best practices to mitigate such\nloss of alignment. We focus on the setting where a public model is fine-tuned\nbefore serving users for specific usage, where the model should improve on the\ndownstream task while maintaining alignment. Through extensive experiments on\nseveral chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct\nv0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt\ntemplates used during fine-tuning and inference play a crucial role in\npreserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''\n(PTST) strategy -- fine-tune models without a safety prompt, but include it at\ntest time. This seemingly counterintuitive strategy incorporates an intended\ndistribution shift to encourage alignment preservation. Fine-tuning experiments\non GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the\nrise of unsafe behaviors.", "published": "2024-02-28 18:23:49", "link": "http://arxiv.org/abs/2402.18540v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Implicit Optimization Bias of Next-Token Prediction in Linear Models", "abstract": "We initiate an investigation into the optimization properties of next-token\nprediction (NTP), the dominant training paradigm for modern language models.\nSpecifically, we study the structural properties of the solutions selected by\ngradient-based optimizers among the many possible minimizers of the NTP\nobjective. By framing NTP as cross-entropy minimization across distinct\ncontexts, each tied with a sparse conditional probability distribution across a\nfinite vocabulary of tokens, we introduce \"NTP-separability conditions\" that\nenable reaching the data-entropy lower bound. With this setup, and focusing on\nlinear models with fixed context embeddings, we characterize the optimization\nbias of gradient descent (GD): Within the data subspace defined by the sparsity\npatterns of distinct contexts, GD selects parameters that equate the logits'\ndifferences of in-support tokens to their log-odds. In the orthogonal subspace,\nthe GD parameters diverge in norm and select the direction that maximizes a\nmargin specific to NTP. These findings extend previous research on implicit\nbias in one-hot classification to the NTP setting, highlighting key differences\nand prompting further research into the optimization and generalization\nproperties of NTP, irrespective of the specific architecture used to generate\nthe context embeddings.", "published": "2024-02-28 18:34:53", "link": "http://arxiv.org/abs/2402.18551v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Approaching Human-Level Forecasting with Language Models", "abstract": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.", "published": "2024-02-28 18:54:18", "link": "http://arxiv.org/abs/2402.18563v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional\n  Preference Alignment with Multi-Objective Rewards", "abstract": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).", "published": "2024-02-28 18:58:25", "link": "http://arxiv.org/abs/2402.18571v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task", "abstract": "Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression (SR). Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and Genetic Programming (GP) and\nReinforcement Learning algorithms were used to solve it. However, GP is\nsensitive to hyperparameters, and these two types of algorithms are\ninefficient. To solve this problem, researchers treat the mapping from data to\nexpressions as a translation problem. And the corresponding large-scale\npre-trained model is introduced. However, the data and expression skeletons do\nnot have very clear word correspondences as the two languages do. Instead, they\nare more like two modalities (e.g., image and text). Therefore, in this paper,\nwe proposed MMSR. The SR problem is solved as a pure multi-modal problem, and\ncontrastive learning is also introduced in the training process for modal\nalignment to facilitate later modal feature fusion. It is worth noting that to\nbetter promote the modal feature fusion, we adopt the strategy of training\ncontrastive learning loss and other losses at the same time, which only needs\none-step training, instead of training contrastive learning loss first and then\ntraining other losses. Because our experiments prove training together can make\nthe feature extraction module and feature fusion module wearing-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench. Our code is open source at\nhttps://github.com/1716757342/MMSR", "published": "2024-02-28 08:29:42", "link": "http://arxiv.org/abs/2402.18603v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models and Games: A Survey and Roadmap", "abstract": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.", "published": "2024-02-28 19:09:08", "link": "http://arxiv.org/abs/2402.18659v5", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning to Compress Prompt in Natural Language Formats", "abstract": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.", "published": "2024-02-28 20:41:21", "link": "http://arxiv.org/abs/2402.18700v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Priority Sampling of Large Language Models for Compilers", "abstract": "Large language models show great potential in generating and optimizing code.\nWidely used sampling methods such as Nucleus Sampling increase the diversity of\ngeneration but often produce repeated samples for low temperatures and\nincoherent samples for high temperatures. Furthermore, the temperature\ncoefficient has to be tuned for each task, limiting its usability. We present\nPriority Sampling, a simple and deterministic sampling technique that produces\nunique samples ordered by the model's confidence. Each new sample expands the\nunexpanded token with the highest probability in the augmented search tree.\nAdditionally, Priority Sampling supports generation based on regular expression\nthat provides a controllable and structured exploration process. Priority\nSampling outperforms Nucleus Sampling for any number of samples, boosting the\nperformance of the original model from 2.87% to 5% improvement over -Oz.\nMoreover, it outperforms the autotuner used for the generation of labels for\nthe training of the original model in just 30 samples.", "published": "2024-02-28 22:27:49", "link": "http://arxiv.org/abs/2402.18734v1", "categories": ["cs.LG", "cs.CL", "cs.PF"], "primary_category": "cs.LG"}
{"title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word\n  Pre-Training", "abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for\nboth protein-centric and protein-language tasks. ProtLLM features a unique\ndynamic protein mounting mechanism, enabling it to handle complex inputs where\nthe natural language text is interspersed with an arbitrary number of proteins.\nBesides, we propose the protein-as-word language modeling approach to train\nProtLLM. By developing a specialized protein vocabulary, we equip the model\nwith the capability to predict not just natural language but also proteins from\na vast pool of candidates. Additionally, we construct a large-scale interleaved\nprotein-text dataset, named InterPT, for pre-training. This dataset\ncomprehensively encompasses both (1) structured data sources like protein\nannotations and (2) unstructured data sources like biological research papers,\nthereby endowing ProtLLM with crucial knowledge for understanding proteins. We\nevaluate ProtLLM on classic supervised protein-centric tasks and explore its\nnovel protein-language applications. Experimental results demonstrate that\nProtLLM not only achieves superior performance against protein-specialized\nbaselines on protein-centric tasks but also induces zero-shot and in-context\nlearning capabilities on protein-language tasks.", "published": "2024-02-28 01:29:55", "link": "http://arxiv.org/abs/2403.07920v1", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices", "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.", "published": "2024-02-28 03:20:27", "link": "http://arxiv.org/abs/2403.07921v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Social Intelligence Data Infrastructure: Structuring the Present and\n  Navigating the Future", "abstract": "As Natural Language Processing (NLP) systems become increasingly integrated\ninto human social life, these technologies will need to increasingly rely on\nsocial intelligence. Although there are many valuable datasets that benchmark\nisolated dimensions of social intelligence, there does not yet exist any body\nof work to join these threads into a cohesive subfield in which researchers can\nquickly identify research gaps and future directions. Towards this goal, we\nbuild a Social AI Data Infrastructure, which consists of a comprehensive social\nAI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows\nus to analyze existing dataset efforts, and also evaluate language models'\nperformance in different social intelligence aspects. Our analyses demonstrate\nits utility in enabling a thorough understanding of current data landscape and\nproviding a holistic perspective on potential directions for future dataset\ndevelopment. We show there is a need for multifaceted datasets, increased\ndiversity in language and culture, more long-tailed social situations, and more\ninteractive data in future social intelligence data efforts.", "published": "2024-02-28 00:22:42", "link": "http://arxiv.org/abs/2403.14659v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference\n  Learning", "abstract": "Multimodal pretraining is an effective strategy for the trinity of goals of\nrepresentation learning in autonomous robots: 1) extracting both local and\nglobal task progressions; 2) enforcing temporal consistency of visual\nrepresentation; 3) capturing trajectory-level language grounding. Most existing\nmethods approach these via separate objectives, which often reach sub-optimal\nsolutions. In this paper, we propose a universal unified objective that can\nsimultaneously extract meaningful task progression information from image\nsequences and seamlessly align them with language instructions. We discover\nthat via implicit preferences, where a visual trajectory inherently aligns\nbetter with its corresponding language instruction than mismatched pairs, the\npopular Bradley-Terry model can transform into representation learning through\nproper reward reparameterizations. The resulted framework, DecisionNCE, mirrors\nan InfoNCE-style objective but is distinctively tailored for decision-making\ntasks, providing an embodied representation learning framework that elegantly\nextracts both local and global task progression features, with temporal\nconsistency enforced through implicit time contrastive learning, while ensuring\ntrajectory-level instruction grounding via multimodal joint encoding.\nEvaluation on both simulated and real robots demonstrates that DecisionNCE\neffectively facilitates diverse downstream policy learning tasks, offering a\nversatile solution for unified representation and reward learning. Project\nPage: https://2toinf.github.io/DecisionNCE/", "published": "2024-02-28 07:58:24", "link": "http://arxiv.org/abs/2402.18137v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Why does music source separation benefit from cacophony?", "abstract": "In music source separation, a standard training data augmentation procedure\nis to create new training samples by randomly combining instrument stems from\ndifferent songs. These random mixes have mismatched characteristics compared to\nreal music, e.g., the different stems do not have consistent beat or tonality,\nresulting in a cacophony. In this work, we investigate why random mixing is\neffective when training a state-of-the-art music source separation model in\nspite of the apparent distribution shift it creates. Additionally, we examine\nwhy performance levels off despite potentially limitless combinations, and\nexamine the sensitivity of music source separation performance to differences\nin beat and tonality of the instrumental sources in a mixture.", "published": "2024-02-28 15:27:58", "link": "http://arxiv.org/abs/2402.18407v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car\n  Manufacturing", "abstract": "This paper proposes a method for Acoustic Constrained Segmentation (ACS) in\naudio recordings of vehicles driven through a production test track, delimiting\nthe boundaries of surface types in the track. ACS is a variant of classical\nacoustic segmentation where the sequence of labels is known, contiguous and\ninvariable, which is especially useful in this work as the test track has a\nstandard configuration of surface types. The proposed ConvDTW-ACS method\nutilizes a Convolutional Neural Network for classifying overlapping image\nchunks extracted from the full audio spectrogram. Then, our custom Dynamic Time\nWarping algorithm aligns the sequence of predicted probabilities to the\nsequence of surface types in the track, from which timestamps of the surface\ntype boundaries can be extracted. The method was evaluated on a real-world\ndataset collected from the Ford Manufacturing Plant in Valencia (Spain),\nachieving a mean error of 166 milliseconds when delimiting, within the audio,\nthe boundaries of the surfaces in the track. The results demonstrate the\neffectiveness of the proposed method in accurately segmenting different surface\ntypes, which could enable the development of more specialized AI systems to\nimprove the quality inspection process.", "published": "2024-02-28 09:48:43", "link": "http://arxiv.org/abs/2402.18204v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mixer is more than just a model", "abstract": "Recently, MLP structures have regained popularity, with MLP-Mixer standing\nout as a prominent example. In the field of computer vision, MLP-Mixer is noted\nfor its ability to extract data information from both channel and token\nperspectives, effectively acting as a fusion of channel and token information.\nIndeed, Mixer represents a paradigm for information extraction that amalgamates\nchannel and token information. The essence of Mixer lies in its ability to\nblend information from diverse perspectives, epitomizing the true concept of\n\"mixing\" in the realm of neural network architectures. Beyond channel and token\nconsiderations, it is possible to create more tailored mixers from various\nperspectives to better suit specific task requirements. This study focuses on\nthe domain of audio recognition, introducing a novel model named Audio\nSpectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates\ninsights from both time and frequency domains. Experimental results demonstrate\nthat ASM-RH is particularly well-suited for audio data and yields promising\noutcomes across multiple classification tasks. The models and optimal weights\nfiles will be published.", "published": "2024-02-28 02:45:58", "link": "http://arxiv.org/abs/2402.18007v2", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Improvement Of Audiovisual Quality Estimation Using A Nonlinear\n  Autoregressive Exogenous Neural Network And Bitstream Parameters", "abstract": "With the increasing demand for audiovisual services, telecom service\nproviders and application developers are compelled to ensure that their\nservices provide the best possible user experience. Particularly, services such\nas videoconferencing are very sensitive to network conditions. Therefore, their\nperformance should be monitored in real time in order to adjust parameters to\nany network perturbation. In this paper, we developed a parametric model for\nestimating the perceived audiovisual quality in videoconference services. Our\nmodel is developed with the nonlinear autoregressive exogenous (NARX) recurrent\nneural network and estimates the perceived quality in terms of mean opinion\nscore (MOS). We validate our model using the publicly available INRS bitstream\naudiovisual quality dataset. This dataset contains bitstream parameters such as\nloss per frame, bit rate and video duration. We compare the proposed model\nagainst state-of-the-art methods based on machine learning and show our model\nto outperform these methods in terms of mean square error (MSE=0.150) and\nPearson correlation coefficient (R=0.931)", "published": "2024-02-28 05:25:57", "link": "http://arxiv.org/abs/2402.18056v1", "categories": ["eess.IV", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
{"title": "PITCH: AI-assisted Tagging of Deepfake Audio Calls using\n  Challenge-Response", "abstract": "The rise of AI voice-cloning technology, particularly audio Real-time\nDeepfakes (RTDFs), has intensified social engineering attacks by enabling\nreal-time voice impersonation that bypasses conventional enrollment-based\nauthentication. To address this, we propose PITCH, a robust challenge-response\nmethod to detect and tag interactive deepfake audio calls. We developed a\ncomprehensive taxonomy of audio challenges based on the human auditory system,\nlinguistics, and environmental factors, yielding 20 prospective challenges.\nThese were tested against leading voice-cloning systems using a novel dataset\ncomprising 18,600 original and 1.6 million deepfake samples from 100 users.\nPITCH's prospective challenges enhanced machine detection capabilities to 88.7%\nAUROC score on the full unbalanced dataset, enabling us to shortlist 10\nfunctional challenges that balance security and usability.\n  For human evaluation and subsequent analyses, we filtered a challenging,\nbalanced subset. On this subset, human evaluators independently scored 72.6%\naccuracy, while machines achieved 87.7%. Acknowledging that call environments\nrequire higher human control, we aided call receivers in making decisions with\nthem using machines. Our solution uses an early warning system to tag\nsuspicious incoming calls as \"Deepfake-likely.\" Contrary to prior findings, we\ndiscovered that integrating human intuition with machine precision offers\ncomplementary advantages. Our solution gave users maximum control and boosted\ndetection accuracy to 84.5%. Evidenced by this jump in accuracy, PITCH\ndemonstrated the potential for AI-assisted pre-screening in call verification\nprocesses, offering an adaptable and usable approach to combat real-time\nvoice-cloning attacks. Code to reproduce and access data at\n\\url{https://github.com/mittalgovind/PITCH-Deepfakes}.", "published": "2024-02-28 06:17:55", "link": "http://arxiv.org/abs/2402.18085v3", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous\n  Driving", "abstract": "This paper introduces the task of Auditory Referring Multi-Object Tracking\n(AR-MOT), which dynamically tracks specific objects in a video sequence based\non audio expressions and appears as a challenging problem in autonomous\ndriving. Due to the lack of semantic modeling capacity in audio and video,\nexisting works have mainly focused on text-based multi-object tracking, which\noften comes at the cost of tracking quality, interaction efficiency, and even\nthe safety of assistance systems, limiting the application of such methods in\nautonomous driving. In this paper, we delve into the problem of AR-MOT from the\nperspective of audio-video fusion and audio-video tracking. We put forward\nEchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.\nThe dual streams are intertwined with our Bidirectional Frequency-domain\nCross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and\nvideo features from both frequency- and spatiotemporal domains. Moreover, we\npropose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract\nhomogeneous semantic features between expressions and visual objects by\nlearning homogeneous features between different audio and video objects\neffectively. Aside from the architectural design, we establish the first set of\nlarge-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.\nExtensive experiments on the established benchmarks demonstrate the\neffectiveness of the proposed EchoTrack and its components. The source code and\ndatasets are available at https://github.com/lab206/EchoTrack.", "published": "2024-02-28 12:50:16", "link": "http://arxiv.org/abs/2402.18302v2", "categories": ["cs.CV", "cs.RO", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
