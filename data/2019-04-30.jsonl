{"title": "A self-attention based deep learning method for lesion attribute\n  detection from CT reports", "abstract": "In radiology, radiologists not only detect lesions from the medical image,\nbut also describe them with various attributes such as their type, location,\nsize, shape, and intensity. While these lesion attributes are rich and useful\nin many downstream clinical applications, how to extract them from the\nradiology reports is less studied. This paper outlines a novel deep learning\nmethod to automatically extract attributes of lesions of interest from the\nclinical text. Different from classical CNN models, we integrated the\nmulti-head self-attention mechanism to handle the long-distance information in\nthe sentence, and to jointly correlate different portions of sentence\nrepresentation subspaces in parallel. Evaluation on an in-house corpus\ndemonstrates that our method can achieve high performance with 0.848 in\nprecision, 0.788 in recall, and 0.815 in F-score. The new method and\nconstructed corpus will enable us to build automatic systems with a\nhigher-level understanding of the radiological world.", "published": "2019-04-30 02:18:23", "link": "http://arxiv.org/abs/1904.13018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Entity Recognition with Reduced False Negatives and Large\n  Type Coverage", "abstract": "Fine-grained Entity Recognition (FgER) is the task of detecting and\nclassifying entity mentions to a large set of types spanning diverse domains\nsuch as biomedical, finance and sports. We observe that when the type set spans\nseveral domains, detection of entity mention becomes a limitation for\nsupervised learning models. The primary reason being lack of dataset where\nentity boundaries are properly annotated while covering a large spectrum of\nentity types. Our work directly addresses this issue. We propose Heuristics\nAllied with Distant Supervision (HAnDS) framework to automatically construct a\nquality dataset suitable for the FgER task. HAnDS framework exploits the high\ninterlink among Wikipedia and Freebase in a pipelined manner, reducing\nannotation errors introduced by naively using distant supervision approach.\nUsing HAnDS framework, we create two datasets, one suitable for building FgER\nsystems recognizing up to 118 entity types based on the FIGER type hierarchy\nand another for up to 1115 entity types based on the TypeNet hierarchy. Our\nextensive empirical experimentation warrants the quality of the generated\ndatasets. Along with this, we also provide a manually annotated dataset for\nbenchmarking FgER systems.", "published": "2019-04-30 11:51:52", "link": "http://arxiv.org/abs/1904.13178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastContext: an efficient and scalable implementation of the ConText\n  algorithm", "abstract": "Objective: To develop and evaluate FastContext, an efficient, scalable\nimplementation of the ConText algorithm suitable for very large-scale clinical\nnatural language processing. Background: The ConText algorithm performs with\nstate-of-art accuracy in detecting the experiencer, negation status, and\ntemporality of concept mentions in clinical narratives. However, the speed\nlimitation of its current implementations hinders its use in big data\nprocessing. Methods: We developed FastContext through hashing the ConText's\nrules, then compared its speed and accuracy with JavaConText and\nGeneralConText, two widely used Java implementations. Results: FastContext ran\ntwo orders of magnitude faster and was less decelerated by rule increase than\nthe other two implementations used in this study for comparison. Additionally,\nFastContext consistently gained accuracy improvement as the rules increased\n(the desired outcome of adding new rules), while the other two implementations\ndid not. Conclusions: FastContext is an efficient, scalable implementation of\nthe popular ConText algorithm, suitable for natural language applications on\nvery large clinical corpora.", "published": "2019-04-30 19:57:47", "link": "http://arxiv.org/abs/1905.00079v1", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Semi-Unsupervised Lifelong Learning for Sentiment Classification: Less\n  Manual Data Annotation and More Self-Studying", "abstract": "Lifelong machine learning is a novel machine learning paradigm which can\ncontinually accumulate knowledge during learning. The knowledge extracting and\nreusing abilities enable the lifelong machine learning to solve the related\nproblems. The traditional approaches like Na\\\"ive Bayes and some neural network\nbased approaches only aim to achieve the best performance upon a single task.\nUnlike them, the lifelong machine learning in this paper focuses on how to\naccumulate knowledge during learning and leverage them for further tasks.\nMeanwhile, the demand for labelled data for training also is significantly\ndecreased with the knowledge reusing. This paper suggests that the aim of the\nlifelong learning is to use less labelled data and computational cost to\nachieve the performance as well as or even better than the supervised learning.", "published": "2019-04-30 23:56:54", "link": "http://arxiv.org/abs/1905.01988v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word\n  Vectors", "abstract": "Recent literature suggests that averaged word vectors followed by simple\npost-processing outperform many deep learning methods on semantic textual\nsimilarity tasks. Furthermore, when averaged word vectors are trained\nsupervised on large corpora of paraphrases, they achieve state-of-the-art\nresults on standard STS benchmarks. Inspired by these insights, we push the\nlimits of word embeddings even further. We propose a novel fuzzy bag-of-words\n(FBoW) representation for text that contains all the words in the vocabulary\nsimultaneously but with different degrees of membership, which are derived from\nsimilarities between word vectors. We show that max-pooled word vectors are\nonly a special case of fuzzy BoW and should be compared via fuzzy Jaccard index\nrather than cosine similarity. Finally, we propose DynaMax, a completely\nunsupervised and non-parametric similarity measure that dynamically extracts\nand max-pools good features depending on the sentence pair. This method is both\nefficient and easy to implement, yet outperforms current baselines on STS tasks\nby a large margin and is even competitive with supervised word vectors trained\nto directly optimise cosine similarity.", "published": "2019-04-30 14:08:37", "link": "http://arxiv.org/abs/1904.13264v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Personalized Ranking in eCommerce Search", "abstract": "We address the problem of personalization in the context of eCommerce search.\nSpecifically, we develop personalization ranking features that use in-session\ncontext to augment a generic ranker optimized for conversion and relevance. We\nuse a combination of latent features learned from item co-clicks in historic\nsessions and content-based features that use item title and price.\nPersonalization in search has been discussed extensively in the existing\nliterature. The novelty of our work is combining and comparing content-based\nand content-agnostic features and showing that they complement each other to\nresult in a significant improvement of the ranker. Moreover, our technique does\nnot require an explicit re-ranking step, does not rely on learning user\nprofiles from long term search behavior, and does not involve complex modeling\nof query-item-user features. Our approach captures item co-click propensity\nusing lightweight item embeddings. We experimentally show that our technique\nsignificantly outperforms a generic ranker in terms of Mean Reciprocal Rank\n(MRR). We also provide anecdotal evidence for the semantic similarity captured\nby the item embeddings on the eBay search engine.", "published": "2019-04-30 18:29:28", "link": "http://arxiv.org/abs/1905.00052v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Many-to-Many Voice Conversion with Out-of-Dataset Speaker Support", "abstract": "We present a Cycle-GAN based many-to-many voice conversion method that can\nconvert between speakers that are not in the training set. This property is\nenabled through speaker embeddings generated by a neural network that is\njointly trained with the Cycle-GAN. In contrast to prior work in this domain,\nour method enables conversion between an out-of-dataset speaker and a target\nspeaker in either direction and does not require re-training. Out-of-dataset\nspeaker conversion quality is evaluated using an independently trained speaker\nidentification model, and shows good style conversion characteristics for\npreviously unheard speakers. Subjective tests on human listeners show style\nconversion quality for in-dataset speakers is comparable to the\nstate-of-the-art baseline model.", "published": "2019-04-30 15:35:25", "link": "http://arxiv.org/abs/1905.02525v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Coherent and Engaging Spoken Dialog Response Generation Using\n  Automatic Conversation Evaluators", "abstract": "Encoder-decoder based neural architectures serve as the basis of\nstate-of-the-art approaches in end-to-end open domain dialog systems. Since\nmost of such systems are trained with a maximum likelihood~(MLE) objective they\nsuffer from issues such as lack of generalizability and the generic response\nproblem, i.e., a system response that can be an answer to a large number of\nuser utterances, e.g., \"Maybe, I don't know.\" Having explicit feedback on the\nrelevance and interestingness of a system response at each turn can be a useful\nsignal for mitigating such issues and improving system quality by selecting\nresponses from different approaches. Towards this goal, we present a system\nthat evaluates chatbot responses at each dialog turn for coherence and\nengagement. Our system provides explicit turn-level dialog quality feedback,\nwhich we show to be highly correlated with human evaluation. To show that\nincorporating this feedback in the neural response generation models improves\ndialog quality, we present two different and complementary mechanisms to\nincorporate explicit feedback into a neural response generation model:\nreranking and direct modification of the loss function during training. Our\nstudies show that a response generation model that incorporates these combined\nfeedback mechanisms produce more engaging and coherent responses in an\nopen-domain spoken dialog setting, significantly improving the response quality\nusing both automatic and human evaluation.", "published": "2019-04-30 02:03:05", "link": "http://arxiv.org/abs/1904.13015v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "English Broadcast News Speech Recognition by Humans and Machines", "abstract": "With recent advances in deep learning, considerable attention has been given\nto achieving automatic speech recognition performance close to human\nperformance on tasks like conversational telephone speech (CTS) recognition. In\nthis paper we evaluate the usefulness of these proposed techniques on broadcast\nnews (BN), a similar challenging task. We also perform a set of recognition\nmeasurements to understand how close the achieved automatic speech recognition\nresults are to human performance on this task. On two publicly available BN\ntest sets, DEV04F and RT04, our speech recognition system using LSTM and\nresidual network based acoustic models with a combination of n-gram and neural\nnetwork language models performs at 6.5% and 5.9% word error rate. By achieving\nnew performance milestones on these test sets, our experiments show that\ntechniques developed on other related tasks, like CTS, can be transferred to\nachieve similar performance. In contrast, the best measured human recognition\nperformance on these test sets is much lower, at 3.6% and 2.8% respectively,\nindicating that there is still room for new techniques and improvements in this\nspace, to reach human performance levels.", "published": "2019-04-30 13:59:18", "link": "http://arxiv.org/abs/1904.13258v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Model Comparison for Semantic Grouping", "abstract": "We introduce a probabilistic framework for quantifying the semantic\nsimilarity between two groups of embeddings. We formulate the task of semantic\nsimilarity as a model comparison task in which we contrast a generative model\nwhich jointly models two sentences versus one that does not. We illustrate how\nthis framework can be used for the Semantic Textual Similarity tasks using\nclear assumptions about how the embeddings of words are generated. We apply\nmodel comparison that utilises information criteria to address some of the\nshortcomings of Bayesian model comparison, whilst still penalising model\ncomplexity. We achieve competitive results by applying the proposed framework\nwith an appropriate choice of likelihood on the STS datasets.", "published": "2019-04-30 15:37:16", "link": "http://arxiv.org/abs/1904.13323v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition", "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have\ngained significant interest in the research community. While previous\narchitecture choices revolve around time-delay neural networks (TDNN) and long\nshort-term memory (LSTM) recurrent neural networks, we propose to use\nself-attention via the Transformer architecture as an alternative. Our analysis\nshows that deep Transformer networks with high learning capacity are able to\nexceed performance from previous end-to-end approaches and even match the\nconventional hybrid systems. Moreover, we trained very deep models with up to\n48 Transformer layers for both encoder and decoders combined with stochastic\nresidual connections, which greatly improve generalizability and training\nefficiency. The resulting models outperform all previous end-to-end ASR\napproaches on the Switchboard benchmark. An ensemble of these models achieve\n9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This\nfinding brings our end-to-end models to competitive levels with previous hybrid\nsystems. Further, with model ensembling the Transformers can outperform certain\nhybrid systems, which are more complicated in terms of both structure and\ntraining procedure.", "published": "2019-04-30 17:20:32", "link": "http://arxiv.org/abs/1904.13377v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text", "abstract": "Sequence-to-sequence automatic speech recognition (ASR) models require large\nquantities of data to attain high performance. For this reason, there has been\na recent surge in interest for unsupervised and semi-supervised training in\nsuch models. This work builds upon recent results showing notable improvements\nin semi-supervised training using cycle-consistency and related techniques.\nSuch techniques derive training procedures and losses able to leverage unpaired\nspeech and/or text data by combining ASR with Text-to-Speech (TTS) models. In\nparticular, this work proposes a new semi-supervised loss combining an\nend-to-end differentiable ASR$\\rightarrow$TTS loss with TTS$\\rightarrow$ASR\nloss. The method is able to leverage both unpaired speech and text data to\noutperform recently proposed related techniques in terms of \\%WER. We provide\nextensive results analyzing the impact of data quantity and speech and text\nmodalities and show consistent gains across WSJ and Librispeech corpora. Our\ncode is provided in ESPnet to reproduce the experiments.", "published": "2019-04-30 16:13:41", "link": "http://arxiv.org/abs/1905.01152v2", "categories": ["eess.AS", "cs.CL", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning for Audio Signal Processing", "abstract": "Given the recent surge in developments of deep learning, this article\nprovides a review of the state-of-the-art deep learning techniques for audio\nsignal processing. Speech, music, and environmental sound processing are\nconsidered side-by-side, in order to point out similarities and differences\nbetween the domains, highlighting general methods, problems, key references,\nand potential for cross-fertilization between areas. The dominant feature\nrepresentations (in particular, log-mel spectra and raw waveform) and deep\nlearning models are reviewed, including convolutional neural networks, variants\nof the long short-term memory architecture, as well as more audio-specific\nneural network models. Subsequently, prominent deep learning application areas\nare covered, i.e. audio recognition (automatic speech recognition, music\ninformation retrieval, environmental sound detection, localization and\ntracking) and synthesis and transformation (source separation, audio\nenhancement, generative models for speech, sound, and music synthesis).\nFinally, key issues and future questions regarding deep learning applied to\naudio signal processing are identified.", "published": "2019-04-30 19:57:33", "link": "http://arxiv.org/abs/1905.00078v2", "categories": ["cs.SD", "eess.AS", "stat.ML", "I.2.6; H.5.1"], "primary_category": "cs.SD"}
{"title": "Interfacing PDM MEMS microphones with PFM spiking systems: Application\n  for Neuromorphic Auditory Sensors", "abstract": "In neuromorphic engineering, computation is commonly performed\nasynchronously, mimicking the way in which nervous systems process information:\nspike by spike. The Neuromorphic Auditory Sensor (NAS) has been implemented\nunder this principle: applying different spike-based Signal Processing blocks.\nComputation in the spike domain requires the conversion of signals from analog\nor digital representation to the spike domain, which could present a speed\nconstraint in many cases. This paper presents a spike-based system to convert\naudio information from low-power pulse density modulation (PDM)\nMicroElectroMechanical Systems (MEMS) microphones into rate coded spike\nfrequencies. These spikes represent the input signal of the NAS, avoiding the\nanalog or digital to spike conversion, and therefore improving the time\nresponse of the NAS. This conversion has been done in VHDL as an interface for\nPDM microphones, converting their pulses into temporal distributed spikes\nfollowing a pulse frequency modulation (PFM) scheme with an accurate\nInter-Spike-Interval, known as \"PDM to spikes interface\" (PSI). This was tested\nin two scenarios, first as a stand-alone circuit for its characterization, and\nthen integrated with a NAS for verification. The PSI achieves a Total Harmonic\nDistortion (THD) of -39.51dB and a Signal-to-Noise Ratio (SNR) of 59.12dB,\ndemands less than 1\\% of the resources of a Spartan-6 FPGA and has a power\nconsumption below 5mW.", "published": "2019-04-30 14:04:16", "link": "http://arxiv.org/abs/1905.00390v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement", "abstract": "In a noisy environment, a lossy speech signal can be automatically restored\nby a listener if he/she knows the language well. That is, with the built-in\nknowledge of a \"language model\", a listener may effectively suppress noise\ninterference and retrieve the target speech signals. Accordingly, we argue that\nfamiliarity with the underlying linguistic content of spoken utterances\nbenefits speech enhancement (SE) in noisy environments. In this study, in\naddition to the conventional modeling for learning the acoustic noisy-clean\nspeech mapping, an abstract symbolic sequential modeling is incorporated into\nthe SE framework. This symbolic sequential modeling can be regarded as a\n\"linguistic constraint\" in learning the acoustic noisy-clean speech mapping\nfunction. In this study, the symbolic sequences for acoustic signals are\nobtained as discrete representations with a Vector Quantized Variational\nAutoencoder algorithm. The obtained symbols are able to capture high-level\nphoneme-like content from speech signals. The experimental results demonstrate\nthat the proposed framework can obtain notable performance improvement in terms\nof perceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI) on the TIMIT dataset.", "published": "2019-04-30 10:31:22", "link": "http://arxiv.org/abs/1904.13142v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Performing Structured Improvisations with pre-trained Deep Learning\n  Models", "abstract": "The quality of outputs produced by deep generative models for music have seen\na dramatic improvement in the last few years. However, most deep learning\nmodels perform in \"offline\" mode, with few restrictions on the processing time.\nIntegrating these types of models into a live structured performance poses a\nchallenge because of the necessity to respect the beat and harmony. Further,\nthese deep models tend to be agnostic to the style of a performer, which often\nrenders them impractical for live performance. In this paper we propose a\nsystem which enables the integration of out-of-the-box generative models by\nleveraging the musician's creativity and expertise.", "published": "2019-04-30 14:50:12", "link": "http://arxiv.org/abs/1904.13285v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
