{"title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "abstract": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.", "published": "2025-04-24 17:39:25", "link": "http://arxiv.org/abs/2504.17768v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "abstract": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other.", "published": "2025-04-24 17:16:24", "link": "http://arxiv.org/abs/2504.17753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Performance Biases of Large Language Models in Education", "abstract": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.", "published": "2025-04-24 16:32:31", "link": "http://arxiv.org/abs/2504.17720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Safety in Large Reasoning Models: A Survey", "abstract": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models.", "published": "2025-04-24 16:11:01", "link": "http://arxiv.org/abs/2504.17704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "abstract": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.", "published": "2025-04-24 15:55:10", "link": "http://arxiv.org/abs/2504.17685v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "abstract": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.", "published": "2025-04-24 15:45:05", "link": "http://arxiv.org/abs/2504.17674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "abstract": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "published": "2025-04-24 15:39:46", "link": "http://arxiv.org/abs/2504.17671v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "abstract": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.", "published": "2025-04-24 15:34:24", "link": "http://arxiv.org/abs/2504.17665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "abstract": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders.", "published": "2025-04-24 15:23:47", "link": "http://arxiv.org/abs/2504.17653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "abstract": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications.", "published": "2025-04-24 14:03:53", "link": "http://arxiv.org/abs/2504.17574v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "abstract": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M", "published": "2025-04-24 13:57:53", "link": "http://arxiv.org/abs/2504.17565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "abstract": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.", "published": "2025-04-24 13:56:43", "link": "http://arxiv.org/abs/2504.17562v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HalluLens: LLM Hallucination Benchmark", "abstract": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "published": "2025-04-24 13:40:27", "link": "http://arxiv.org/abs/2504.17550v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "abstract": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.", "published": "2025-04-24 12:15:46", "link": "http://arxiv.org/abs/2504.17480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "abstract": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.", "published": "2025-04-24 11:28:40", "link": "http://arxiv.org/abs/2504.17449v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "abstract": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.", "published": "2025-04-24 11:14:13", "link": "http://arxiv.org/abs/2504.17445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "abstract": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona.", "published": "2025-04-24 09:15:58", "link": "http://arxiv.org/abs/2504.17390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "abstract": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.", "published": "2025-04-24 08:27:48", "link": "http://arxiv.org/abs/2504.17366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "abstract": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.", "published": "2025-04-24 08:27:42", "link": "http://arxiv.org/abs/2504.17365v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "abstract": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.", "published": "2025-04-24 08:21:04", "link": "http://arxiv.org/abs/2504.17360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "abstract": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain.", "published": "2025-04-24 08:14:36", "link": "http://arxiv.org/abs/2504.17353v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "abstract": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.", "published": "2025-04-24 07:48:26", "link": "http://arxiv.org/abs/2504.17332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "abstract": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.", "published": "2025-04-24 07:12:37", "link": "http://arxiv.org/abs/2504.17311v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "abstract": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality.", "published": "2025-04-24 07:08:13", "link": "http://arxiv.org/abs/2504.17309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "abstract": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair.", "published": "2025-04-24 06:10:40", "link": "http://arxiv.org/abs/2504.17279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "abstract": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.", "published": "2025-04-24 05:48:57", "link": "http://arxiv.org/abs/2504.17264v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "abstract": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks.", "published": "2025-04-24 05:02:26", "link": "http://arxiv.org/abs/2504.17252v1", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "abstract": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations.", "published": "2025-04-24 04:22:00", "link": "http://arxiv.org/abs/2504.17238v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "abstract": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation.", "published": "2025-04-24 03:18:16", "link": "http://arxiv.org/abs/2504.17220v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "abstract": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support.", "published": "2025-04-24 02:25:06", "link": "http://arxiv.org/abs/2504.17200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "abstract": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.", "published": "2025-04-24 01:57:01", "link": "http://arxiv.org/abs/2504.17192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "abstract": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/.", "published": "2025-04-24 17:46:29", "link": "http://arxiv.org/abs/2504.17771v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "abstract": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.", "published": "2025-04-24 17:09:59", "link": "http://arxiv.org/abs/2504.17751v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "abstract": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.", "published": "2025-04-24 16:33:56", "link": "http://arxiv.org/abs/2504.17721v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "abstract": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.", "published": "2025-04-24 16:19:13", "link": "http://arxiv.org/abs/2504.17717v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "abstract": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.", "published": "2025-04-24 16:10:29", "link": "http://arxiv.org/abs/2504.17703v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "abstract": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/", "published": "2025-04-24 16:04:00", "link": "http://arxiv.org/abs/2504.17696v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "abstract": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.", "published": "2025-04-24 15:47:20", "link": "http://arxiv.org/abs/2504.17677v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance", "abstract": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.", "published": "2025-04-24 15:45:40", "link": "http://arxiv.org/abs/2504.17675v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC"}
{"title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "abstract": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.", "published": "2025-04-24 15:38:20", "link": "http://arxiv.org/abs/2504.17669v1", "categories": ["cs.MA", "cs.AI", "cs.ET"], "primary_category": "cs.MA"}
{"title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "abstract": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.", "published": "2025-04-24 15:31:46", "link": "http://arxiv.org/abs/2504.17663v1", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "abstract": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.", "published": "2025-04-24 15:25:37", "link": "http://arxiv.org/abs/2504.17655v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "abstract": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.", "published": "2025-04-24 15:11:41", "link": "http://arxiv.org/abs/2504.17641v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates", "abstract": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.", "published": "2025-04-24 14:46:20", "link": "http://arxiv.org/abs/2504.17624v1", "categories": ["q-bio.BM", "cs.AI"], "primary_category": "q-bio.BM"}
{"title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "abstract": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.", "published": "2025-04-24 14:43:55", "link": "http://arxiv.org/abs/2504.17619v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Decentralized Time Series Classification with ROCKET Features", "abstract": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.", "published": "2025-04-24 14:41:50", "link": "http://arxiv.org/abs/2504.17617v1", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "primary_category": "cs.LG"}
{"title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "abstract": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.", "published": "2025-04-24 14:34:41", "link": "http://arxiv.org/abs/2504.17609v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "abstract": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.", "published": "2025-04-24 13:41:27", "link": "http://arxiv.org/abs/2504.17551v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Auditing the Ethical Logic of Generative AI Models", "abstract": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.", "published": "2025-04-24 13:32:30", "link": "http://arxiv.org/abs/2504.17544v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "abstract": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.", "published": "2025-04-24 13:32:11", "link": "http://arxiv.org/abs/2504.17540v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste", "abstract": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.", "published": "2025-04-24 13:32:03", "link": "http://arxiv.org/abs/2504.17539v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "abstract": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.", "published": "2025-04-24 13:20:32", "link": "http://arxiv.org/abs/2504.17534v1", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "primary_category": "cs.LG"}
{"title": "Towards Machine-Generated Code for the Resolution of User Intentions", "abstract": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.", "published": "2025-04-24 13:19:17", "link": "http://arxiv.org/abs/2504.17531v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "abstract": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.", "published": "2025-04-24 13:16:21", "link": "http://arxiv.org/abs/2504.17528v1", "categories": ["cs.LG", "cs.AI", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "abstract": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.", "published": "2025-04-24 12:38:03", "link": "http://arxiv.org/abs/2504.17497v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "abstract": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.", "published": "2025-04-24 12:34:43", "link": "http://arxiv.org/abs/2504.17493v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "abstract": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.", "published": "2025-04-24 12:32:13", "link": "http://arxiv.org/abs/2504.17490v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "abstract": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.", "published": "2025-04-24 12:07:14", "link": "http://arxiv.org/abs/2504.17474v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "abstract": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.", "published": "2025-04-24 12:03:15", "link": "http://arxiv.org/abs/2504.17471v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "abstract": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.", "published": "2025-04-24 11:52:13", "link": "http://arxiv.org/abs/2504.17461v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "abstract": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG", "published": "2025-04-24 11:19:18", "link": "http://arxiv.org/abs/2504.17447v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt", "abstract": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.", "published": "2025-04-24 10:38:55", "link": "http://arxiv.org/abs/2504.17428v1", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.GL", "D.2.7; D.2.9"], "primary_category": "cs.SE"}
{"title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code", "abstract": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.", "published": "2025-04-24 10:30:40", "link": "http://arxiv.org/abs/2504.17426v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation", "abstract": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.", "published": "2025-04-24 10:26:14", "link": "http://arxiv.org/abs/2504.17424v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.", "published": "2025-04-24 10:24:35", "link": "http://arxiv.org/abs/2504.17421v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.", "published": "2025-04-24 09:53:49", "link": "http://arxiv.org/abs/2504.17404v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation", "abstract": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.", "published": "2025-04-24 09:47:14", "link": "http://arxiv.org/abs/2504.17402v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "abstract": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.", "published": "2025-04-24 09:46:15", "link": "http://arxiv.org/abs/2504.17401v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "abstract": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.", "published": "2025-04-24 09:25:29", "link": "http://arxiv.org/abs/2504.17393v1", "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "cs.CY"}
{"title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "abstract": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.", "published": "2025-04-24 09:08:24", "link": "http://arxiv.org/abs/2504.17384v1", "categories": ["physics.geo-ph", "cs.AI"], "primary_category": "physics.geo-ph"}
{"title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "abstract": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.", "published": "2025-04-24 08:16:36", "link": "http://arxiv.org/abs/2504.17356v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "abstract": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.", "published": "2025-04-24 08:16:13", "link": "http://arxiv.org/abs/2504.17355v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems", "abstract": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.", "published": "2025-04-24 08:15:46", "link": "http://arxiv.org/abs/2504.17354v1", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE"}
{"title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks", "abstract": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.", "published": "2025-04-24 08:04:08", "link": "http://arxiv.org/abs/2504.17346v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "abstract": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.", "published": "2025-04-24 07:48:09", "link": "http://arxiv.org/abs/2504.17331v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "abstract": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.", "published": "2025-04-24 07:17:59", "link": "http://arxiv.org/abs/2504.17315v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "abstract": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.", "published": "2025-04-24 07:00:38", "link": "http://arxiv.org/abs/2504.17306v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications", "abstract": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.", "published": "2025-04-24 06:59:16", "link": "http://arxiv.org/abs/2504.17304v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining", "abstract": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.", "published": "2025-04-24 06:43:29", "link": "http://arxiv.org/abs/2504.17295v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning", "abstract": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.", "published": "2025-04-24 06:20:08", "link": "http://arxiv.org/abs/2504.17282v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "abstract": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.", "published": "2025-04-24 06:07:14", "link": "http://arxiv.org/abs/2504.17277v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Symbolic Representation for Any-to-Any Generative Tasks", "abstract": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.", "published": "2025-04-24 05:35:47", "link": "http://arxiv.org/abs/2504.17261v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations", "abstract": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.", "published": "2025-04-24 05:19:47", "link": "http://arxiv.org/abs/2504.17255v1", "categories": ["eess.IV", "cs.AI", "physics.optics"], "primary_category": "eess.IV"}
{"title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "abstract": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.", "published": "2025-04-24 04:53:04", "link": "http://arxiv.org/abs/2504.17247v1", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "abstract": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.", "published": "2025-04-24 04:41:35", "link": "http://arxiv.org/abs/2504.17243v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "abstract": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.", "published": "2025-04-24 03:17:57", "link": "http://arxiv.org/abs/2504.17219v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "primary_category": "cs.LG"}
{"title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "abstract": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods.", "published": "2025-04-24 02:54:40", "link": "http://arxiv.org/abs/2504.17213v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "abstract": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.", "published": "2025-04-24 02:53:22", "link": "http://arxiv.org/abs/2504.17210v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model", "abstract": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.", "published": "2025-04-24 02:15:45", "link": "http://arxiv.org/abs/2504.17198v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "primary_category": "cs.SE"}
{"title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "abstract": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.", "published": "2025-04-24 01:34:12", "link": "http://arxiv.org/abs/2504.17180v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "abstract": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.", "published": "2025-04-24 01:31:13", "link": "http://arxiv.org/abs/2504.17179v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "primary_category": "cs.AI"}
{"title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "abstract": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]", "published": "2025-04-24 01:09:51", "link": "http://arxiv.org/abs/2504.17170v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC"}
{"title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "abstract": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.", "published": "2025-04-24 00:47:31", "link": "http://arxiv.org/abs/2504.17162v1", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "primary_category": "cs.CV"}
{"title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "abstract": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.", "published": "2025-04-24 00:41:59", "link": "http://arxiv.org/abs/2504.17160v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "abstract": "Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .", "published": "2025-04-24 17:59:59", "link": "http://arxiv.org/abs/2504.17791v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Dynamic Camera Poses and Where to Find Them", "abstract": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.", "published": "2025-04-24 17:59:56", "link": "http://arxiv.org/abs/2504.17788v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "abstract": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.", "published": "2025-04-24 17:59:56", "link": "http://arxiv.org/abs/2504.17789v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The Fourth Monocular Depth Estimation Challenge", "abstract": "This paper presents the results of the fourth edition of the Monocular Depth\nEstimation Challenge (MDEC), which focuses on zero-shot generalization to the\nSYNS-Patches benchmark, a dataset featuring challenging environments in both\nnatural and indoor settings. In this edition, we revised the evaluation\nprotocol to use least-squares alignment with two degrees of freedom to support\ndisparity and affine-invariant predictions. We also revised the baselines and\nincluded popular off-the-shelf methods: Depth Anything v2 and Marigold. The\nchallenge received a total of 24 submissions that outperformed the baselines on\nthe test set; 10 of these included a report describing their approach, with\nmost leading methods relying on affine-invariant predictions. The challenge\nwinners improved the 3D F-Score over the previous edition's best result,\nraising it from 22.58% to 23.05%.", "published": "2025-04-24 17:59:52", "link": "http://arxiv.org/abs/2504.17787v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Step1X-Edit: A Practical Framework for General Image Editing", "abstract": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "published": "2025-04-24 17:25:12", "link": "http://arxiv.org/abs/2504.17761v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "abstract": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs.", "published": "2025-04-24 16:48:45", "link": "http://arxiv.org/abs/2504.17735v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model", "abstract": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.", "published": "2025-04-24 16:46:32", "link": "http://arxiv.org/abs/2504.17732v1", "categories": ["cs.CV", "I.4.4"], "primary_category": "cs.CV"}
{"title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos", "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat", "published": "2025-04-24 16:42:37", "link": "http://arxiv.org/abs/2504.17728v1", "categories": ["cs.GR", "cs.CV", "cs.MM"], "primary_category": "cs.GR"}
{"title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields", "abstract": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.", "published": "2025-04-24 16:15:02", "link": "http://arxiv.org/abs/2504.17712v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "abstract": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner.", "published": "2025-04-24 16:14:16", "link": "http://arxiv.org/abs/2504.17710v1", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "primary_category": "physics.plasm-ph"}
{"title": "PICO: Reconstructing 3D People In Contact with Objects", "abstract": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.", "published": "2025-04-24 16:03:11", "link": "http://arxiv.org/abs/2504.17695v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BIM-Constrained Optimization for Accurate Localization and Deviation Correction in Construction Monitoring", "abstract": "Augmented reality (AR) applications for construction monitoring rely on\nreal-time environmental tracking to visualize architectural elements. However,\nconstruction sites present significant challenges for traditional tracking\nmethods due to featureless surfaces, dynamic changes, and drift accumulation,\nleading to misalignment between digital models and the physical world. This\npaper proposes a BIM-aware drift correction method to address these challenges.\nInstead of relying solely on SLAM-based localization, we align ``as-built\"\ndetected planes from the real-world environment with ``as-planned\"\narchitectural planes in BIM. Our method performs robust plane matching and\ncomputes a transformation (TF) between SLAM (S) and BIM (B) origin frames using\noptimization techniques, minimizing drift over time. By incorporating BIM as\nprior structural knowledge, we can achieve improved long-term localization and\nenhanced AR visualization accuracy in noisy construction environments. The\nmethod is evaluated through real-world experiments, showing significant\nreductions in drift-induced errors and optimized alignment consistency. On\naverage, our system achieves a reduction of 52.24% in angular deviations and a\nreduction of 60.8% in the distance error of the matched walls compared to the\ninitial manual alignment by the user.", "published": "2025-04-24 16:02:02", "link": "http://arxiv.org/abs/2504.17693v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "DiMeR: Disentangled Mesh Reconstruction Model", "abstract": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.", "published": "2025-04-24 15:39:20", "link": "http://arxiv.org/abs/2504.17670v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CLIPSE -- a minimalistic CLIP-based image search engine for research", "abstract": "A brief overview of CLIPSE, a self-hosted image search engine with the main\napplication of research, is provided. In general, CLIPSE uses CLIP embeddings\nto process the images and also the text queries. The overall framework is\ndesigned with simplicity to enable easy extension and usage. Two benchmark\nscenarios are described and evaluated, covering indexing and querying time. It\nis shown that CLIPSE is capable of handling smaller datasets; for larger\ndatasets, a distributed approach with several instances should be considered.", "published": "2025-04-24 15:13:37", "link": "http://arxiv.org/abs/2504.17643v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Guide to Structureless Visual Localization", "abstract": "Visual localization algorithms, i.e., methods that estimate the camera pose\nof a query image in a known scene, are core components of many applications,\nincluding self-driving cars and augmented / mixed reality systems.\nState-of-the-art visual localization algorithms are structure-based, i.e., they\nstore a 3D model of the scene and use 2D-3D correspondences between the query\nimage and 3D points in the model for camera pose estimation. While such\napproaches are highly accurate, they are also rather inflexible when it comes\nto adjusting the underlying 3D model after changes in the scene. Structureless\nlocalization approaches represent the scene as a database of images with known\nposes and thus offer a much more flexible representation that can be easily\nupdated by adding or removing images. Although there is a large amount of\nliterature on structure-based approaches, there is significantly less work on\nstructureless methods. Hence, this paper is dedicated to providing the, to the\nbest of our knowledge, first comprehensive discussion and comparison of\nstructureless methods. Extensive experiments show that approaches that use a\nhigher degree of classical geometric reasoning generally achieve higher pose\naccuracy. In particular, approaches based on classical absolute or\nsemi-generalized relative pose estimation outperform very recent methods based\non pose regression by a wide margin. Compared with state-of-the-art\nstructure-based approaches, the flexibility of structureless methods comes at\nthe cost of (slightly) lower pose accuracy, indicating an interesting direction\nfor future work.", "published": "2025-04-24 15:08:36", "link": "http://arxiv.org/abs/2504.17636v1", "categories": ["cs.CV", "I.2.10; I.4.8; I.4.9"], "primary_category": "cs.CV"}
{"title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization", "abstract": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.", "published": "2025-04-24 14:50:10", "link": "http://arxiv.org/abs/2504.17628v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Improving Open-World Object Localization by Discovering Background", "abstract": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.", "published": "2025-04-24 14:48:46", "link": "http://arxiv.org/abs/2504.17626v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "abstract": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature.", "published": "2025-04-24 14:43:07", "link": "http://arxiv.org/abs/2504.17618v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Tamper-evident Image using JPEG Fixed Points", "abstract": "An intriguing phenomenon about JPEG compression has been observed since two\ndecades ago- after repeating JPEG compression and decompression, it leads to a\nstable image that does not change anymore, which is a fixed point. In this\nwork, we prove the existence of fixed points in the essential JPEG procedures.\nWe analyze JPEG compression and decompression processes, revealing the\nexistence of fixed points that can be reached within a few iterations. These\nfixed points are diverse and preserve the image's visual quality, ensuring\nminimal distortion. This result is used to develop a method to create a\ntamper-evident image from the original authentic image, which can expose\ntampering operations by showing deviations from the fixed point image.", "published": "2025-04-24 14:22:13", "link": "http://arxiv.org/abs/2504.17594v1", "categories": ["cs.CV", "I.4.7"], "primary_category": "cs.CV"}
{"title": "RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network", "abstract": "The integration of dual-modal features has been pivotal in advancing\nRGB-Depth (RGB-D) tracking. However, current trackers are less efficient and\nfocus solely on single-level features, resulting in weaker robustness in fusion\nand slower speeds that fail to meet the demands of real-world applications. In\nthis paper, we introduce a novel network, denoted as HMAD (Hierarchical\nModality Aggregation and Distribution), which addresses these challenges. HMAD\nleverages the distinct feature representation strengths of RGB and depth\nmodalities, giving prominence to a hierarchical approach for feature\ndistribution and fusion, thereby enhancing the robustness of RGB-D tracking.\nExperimental results on various RGB-D datasets demonstrate that HMAD achieves\nstate-of-the-art performance. Moreover, real-world experiments further validate\nHMAD's capacity to effectively handle a spectrum of tracking challenges in\nreal-time scenarios.", "published": "2025-04-24 14:22:13", "link": "http://arxiv.org/abs/2504.17595v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images", "abstract": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments.", "published": "2025-04-24 14:12:57", "link": "http://arxiv.org/abs/2504.17582v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task", "abstract": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.", "published": "2025-04-24 13:37:25", "link": "http://arxiv.org/abs/2504.17547v1", "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering", "abstract": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.", "published": "2025-04-24 13:32:58", "link": "http://arxiv.org/abs/2504.17545v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection", "abstract": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.", "published": "2025-04-24 13:10:32", "link": "http://arxiv.org/abs/2504.17525v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting", "abstract": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.", "published": "2025-04-24 13:08:36", "link": "http://arxiv.org/abs/2504.17524v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios", "abstract": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.", "published": "2025-04-24 13:03:13", "link": "http://arxiv.org/abs/2504.17522v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation", "abstract": "To segment medical images with distribution shifts, domain generalization\n(DG) has emerged as a promising setting to train models on source domains that\ncan generalize to unseen target domains. Existing DG methods are mainly based\non CNN or ViT architectures. Recently, advanced state space models, represented\nby Mamba, have shown promising results in various supervised medical image\nsegmentation. The success of Mamba is primarily owing to its ability to capture\nlong-range dependencies while keeping linear complexity with input sequence\nlength, making it a promising alternative to CNNs and ViTs. Inspired by the\nsuccess, in the paper, we explore the potential of the Mamba architecture to\naddress distribution shifts in DG for medical image segmentation. Specifically,\nwe propose a novel Mamba-based framework, Mamba-Sea, incorporating\nglobal-to-local sequence augmentation to improve the model's generalizability\nunder domain shift issues. Our Mamba-Sea introduces a global augmentation\nmechanism designed to simulate potential variations in appearance across\ndifferent sites, aiming to suppress the model's learning of domain-specific\ninformation. At the local level, we propose a sequence-wise augmentation along\ninput sequences, which perturbs the style of tokens within random continuous\nsub-sequences by modeling and resampling style statistics associated with\ndomain shifts. To our best knowledge, Mamba-Sea is the first work to explore\nthe generalization of Mamba for medical image segmentation, providing an\nadvanced and promising Mamba-based architecture with strong robustness to\ndomain shifts. Remarkably, our proposed method is the first to surpass a Dice\ncoefficient of 90% on the Prostate dataset, which exceeds previous SOTA of\n88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.", "published": "2025-04-24 12:57:25", "link": "http://arxiv.org/abs/2504.17515v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation", "abstract": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.", "published": "2025-04-24 12:44:51", "link": "http://arxiv.org/abs/2504.17502v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks", "abstract": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems.", "published": "2025-04-24 11:42:10", "link": "http://arxiv.org/abs/2504.17457v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding", "abstract": "Humans can resort to long-form inspection to build intuition on predicting\nthe 3D configurations of unseen objects. The more we observe the object motion,\nthe better we get at predicting its 3D state immediately. Existing systems\neither optimize underlying representations from multi-view observations or\ntrain a feed-forward predictor from supervised datasets. We introduce\nPredict-Optimize-Distill (POD), a self-improving framework that interleaves\nprediction and optimization in a mutually reinforcing cycle to achieve better\n4D object understanding with increasing observation time. Given a multi-view\nobject scan and a long-form monocular video of human-object interaction, POD\niteratively trains a neural network to predict local part poses from RGB\nframes, uses this predictor to initialize a global optimization which refines\noutput poses through inverse rendering, then finally distills the results of\noptimization back into the model by generating synthetic self-labeled training\ndata from novel viewpoints. Each iteration improves both the predictive model\nand the optimized motion trajectory, creating a virtuous cycle that bootstraps\nits own training data to learn about the pose configurations of an object. We\nalso introduce a quasi-multiview mining strategy for reducing depth ambiguity\nby leveraging long video. We evaluate POD on 14 real-world and 5 synthetic\nobjects with various joint types, including revolute and prismatic joints as\nwell as multi-body configurations where parts detach or reattach independently.\nPOD demonstrates significant improvement over a pure optimization baseline\nwhich gets stuck in local minima, particularly for longer videos. We also find\nthat POD's performance improves with both video length and successive\niterations of the self-improving cycle, highlighting its ability to scale\nperformance with additional observations and looped refinement.", "published": "2025-04-24 11:03:15", "link": "http://arxiv.org/abs/2504.17441v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs", "abstract": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.", "published": "2025-04-24 10:51:52", "link": "http://arxiv.org/abs/2504.17432v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models", "abstract": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/", "published": "2025-04-24 10:12:40", "link": "http://arxiv.org/abs/2504.17414v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception", "abstract": "Collective Perception (CP) has emerged as a promising approach to overcome\nthe limitations of individual perception in the context of autonomous driving.\nVarious approaches have been proposed to realize collective perception;\nhowever, the Sensor2Sensor domain gap that arises from the utilization of\ndifferent sensor systems in Connected and Automated Vehicles (CAVs) remains\nmostly unaddressed. This is primarily due to the paucity of datasets containing\nheterogeneous sensor setups among the CAVs. The recently released SCOPE\ndatasets address this issue by providing data from three different LiDAR\nsensors for each CAV. This study is the first to tackle the Sensor2Sensor\ndomain gap in vehicle to vehicle (V2V) collective perception. First, we present\nour sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the\nSensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is\nconducted. S2S-Net demonstrates the capability to maintain very high\nperformance in unseen sensor domains and achieved state-of-the-art results on\nthe SCOPE dataset.", "published": "2025-04-24 09:38:59", "link": "http://arxiv.org/abs/2504.17399v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models", "abstract": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.", "published": "2025-04-24 09:37:02", "link": "http://arxiv.org/abs/2504.17397v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting", "abstract": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.", "published": "2025-04-24 09:31:08", "link": "http://arxiv.org/abs/2504.17395v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology", "abstract": "Multiple instance learning (MIL) is a promising approach for weakly\nsupervised classification in pathology using whole slide images (WSIs).\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\nInstance Learning (ABMIL) typically disregard spatial interactions among\npatches that are crucial to pathological diagnosis. Recent advancements, such\nas Transformer based MIL (TransMIL), have incorporated spatial context and\ninter-patch relationships. However, it remains unclear whether explicitly\nmodeling patch relationships yields similar performance gains in ABMIL, which\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\nTransformer-based layers, introducing a fundamental architectural shift at the\ncost of substantially increased computational complexity. In this work, we\nenhance the ABMIL framework by integrating interaction-aware representations to\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\ncaptures inter-instance dependencies while preserving computational efficiency.\nExperimental results on two publicly available datasets for tumor subtyping in\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\nover ABMIL, with minimal or no additional computational overhead. These\nfindings underscore the importance of incorporating patch interactions within\nMIL frameworks.", "published": "2025-04-24 08:53:46", "link": "http://arxiv.org/abs/2504.17379v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset", "abstract": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\napp.deepscenario.com, facilitating research in motion prediction, behavior\nmodeling, and safety validation.", "published": "2025-04-24 08:43:48", "link": "http://arxiv.org/abs/2504.17371v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "I-INR: Iterative Implicit Neural Representations", "abstract": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.", "published": "2025-04-24 08:27:22", "link": "http://arxiv.org/abs/2504.17364v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition", "abstract": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.", "published": "2025-04-24 08:10:10", "link": "http://arxiv.org/abs/2504.17349v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos", "abstract": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.", "published": "2025-04-24 07:59:46", "link": "http://arxiv.org/abs/2504.17343v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Class-Conditional Distribution Balancing for Group Robust Classification", "abstract": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision.", "published": "2025-04-24 07:15:53", "link": "http://arxiv.org/abs/2504.17314v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy", "abstract": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints.", "published": "2025-04-24 06:14:01", "link": "http://arxiv.org/abs/2504.17280v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation", "abstract": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.", "published": "2025-04-24 05:54:56", "link": "http://arxiv.org/abs/2504.17269v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Precision Neural Network Quantization via Learnable Adaptive Modules", "abstract": "Quantization Aware Training (QAT) is a neural network quantization technique\nthat compresses model size and improves operational efficiency while\neffectively maintaining model performance. The paradigm of QAT is to introduce\nfake quantization operators during the training process, allowing the model to\nautonomously compensate for information loss caused by quantization. Making\nquantization parameters trainable can significantly improve the performance of\nQAT, but at the cost of compromising the flexibility during inference,\nespecially when dealing with activation values with substantially different\ndistributions. In this paper, we propose an effective learnable adaptive neural\nnetwork quantization method, called Adaptive Step Size Quantization (ASQ), to\nresolve this conflict. Specifically, the proposed ASQ method first dynamically\nadjusts quantization scaling factors through a trained module capable of\naccommodating different activations. Then, to address the rigid resolution\nissue inherent in Power of Two (POT) quantization, we propose an efficient\nnon-uniform quantization scheme. We utilize the Power Of Square root of Two\n(POST) as the basis for exponential quantization, effectively handling the\nbell-shaped distribution of neural network weights across various bit-widths\nwhile maintaining computational efficiency through a Look-Up Table method\n(LUT). Extensive experimental results demonstrate that the proposed ASQ method\nis superior to the state-of-the-art QAT approaches. Notably that the ASQ is\neven competitive compared to full precision baselines, with its 4-bit quantized\nResNet34 model improving accuracy by 1.2\\% on ImageNet.", "published": "2025-04-24 05:46:25", "link": "http://arxiv.org/abs/2504.17263v1", "categories": ["cs.CV", "cs.CC"], "primary_category": "cs.CV"}
{"title": "Group Downsampling with Equivariant Anti-aliasing", "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks", "published": "2025-04-24 05:29:51", "link": "http://arxiv.org/abs/2504.17258v1", "categories": ["cs.LG", "cs.CV", "math.GR"], "primary_category": "cs.LG"}
{"title": "DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks", "abstract": "Diffusion models have shown remarkable progress in various generative tasks\nsuch as image and video generation. This paper studies the problem of\nleveraging pretrained diffusion models for performing discriminative tasks.\nSpecifically, we extend the discriminative capability of pretrained frozen\ngenerative diffusion models from the classification task to the more complex\nobject detection task, by \"inverting\" a pretrained layout-to-image diffusion\nmodel. To this end, a gradient-based discrete optimization approach for\nreplacing the heavy prediction enumeration process, and a prior distribution\nmodel for making more accurate use of the Bayes' rule, are proposed\nrespectively. Empirical results show that this method is on par with basic\ndiscriminative object detection baselines on COCO dataset. In addition, our\nmethod can greatly speed up the previous diffusion-based method for\nclassification without sacrificing accuracy. Code and models are available at\nhttps://github.com/LiYinqi/DIVE .", "published": "2025-04-24 05:13:27", "link": "http://arxiv.org/abs/2504.17253v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment", "abstract": "The rapid advancement of artificial intelligence and widespread use of\nsmartphones have resulted in an exponential growth of image data, both real\n(camera-captured) and virtual (AI-generated). This surge underscores the\ncritical need for robust image quality assessment (IQA) methods that accurately\nreflect human visual perception. Traditional IQA techniques primarily rely on\nspatial features - such as signal-to-noise ratio, local structural distortions,\nand texture inconsistencies - to identify artifacts. While effective for\nunprocessed or conventionally altered images, these methods fall short in the\ncontext of modern image post-processing powered by deep neural networks (DNNs).\nThe rise of DNN-based models for image generation, enhancement, and restoration\nhas significantly improved visual quality, yet made accurate assessment\nincreasingly complex. To address this, we propose a novel IQA approach that\nbridges the gap between deep learning methods and human perception. Our model\ndisentangles deep features into high-level semantic information and low-level\nperceptual details, treating each stream separately. These features are then\ncombined with conventional IQA metrics to provide a more comprehensive\nevaluation framework. This hybrid design enables the model to assess both\nglobal context and intricate image details, better reflecting the human visual\nprocess, which first interprets overall structure before attending to\nfine-grained elements. The final stage employs a multilayer perceptron (MLP) to\nmap the integrated features into a concise quality score. Experimental results\ndemonstrate that our method achieves improved consistency with human perceptual\njudgments compared to existing IQA models.", "published": "2025-04-24 04:06:07", "link": "http://arxiv.org/abs/2504.17234v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Range Image-Based Implicit Neural Compression for LiDAR Point Clouds", "abstract": "This paper presents a novel scheme to efficiently compress Light Detection\nand Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,\nand such archives pave the way for a detailed understanding of the\ncorresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight\nformat for representing 3D LiDAR observations. Although conventional image\ncompression techniques can be adapted to improve compression efficiency for\nRIs, their practical performance is expected to be limited due to differences\nin bit precision and the distinct pixel value distribution characteristics\nbetween natural images and RIs. We propose a novel implicit neural\nrepresentation~(INR)--based RI compression method that effectively handles\nfloating-point valued pixels. The proposed method divides RIs into depth and\nmask images and compresses them using patch-wise and pixel-wise INR\narchitectures with model pruning and quantization, respectively. Experiments on\nthe KITTI dataset show that the proposed method outperforms existing image,\npoint cloud, RI, and INR-based compression methods in terms of 3D\nreconstruction and detection quality at low bitrates and decoding latency.", "published": "2025-04-24 03:41:57", "link": "http://arxiv.org/abs/2504.17229v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual and textual prompts for enhancing emotion recognition in video", "abstract": "Vision Large Language Models (VLLMs) exhibit promising potential for\nmulti-modal understanding, yet their application to video-based emotion\nrecognition remains limited by insufficient spatial and contextual awareness.\nTraditional approaches, which prioritize isolated facial features, often\nneglect critical non-verbal cues such as body language, environmental context,\nand social interactions, leading to reduced robustness in real-world scenarios.\nTo address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel\nframework that enhances zero-shot emotion recognition by integrating spatial\nannotations (e.g., bounding boxes, facial landmarks), physiological signals\n(facial action units), and contextual cues (body posture, scene dynamics,\nothers' emotions) into a unified prompting strategy. SoVTP preserves holistic\nscene information while enabling fine-grained analysis of facial muscle\nmovements and interpersonal dynamics. Extensive experiments show that SoVTP\nachieves substantial improvements over existing visual prompting methods,\ndemonstrating its effectiveness in enhancing VLLMs' video emotion recognition\ncapabilities.", "published": "2025-04-24 03:26:30", "link": "http://arxiv.org/abs/2504.17224v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion", "abstract": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.", "published": "2025-04-24 03:23:35", "link": "http://arxiv.org/abs/2504.17223v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation", "abstract": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.", "published": "2025-04-24 02:41:34", "link": "http://arxiv.org/abs/2504.17207v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "abstract": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.", "published": "2025-04-24 01:23:00", "link": "http://arxiv.org/abs/2504.17177v1", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "primary_category": "cs.CV"}
{"title": "PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition", "abstract": "Electroencephalography (EEG) signals provide a promising and involuntary\nreflection of brain activity related to emotional states, offering significant\nadvantages over behavioral cues like facial expressions. However, EEG signals\nare often noisy, affected by artifacts, and vary across individuals,\ncomplicating emotion recognition. While multimodal approaches have used\nPeripheral Physiological Signals (PPS) like GSR to complement EEG, they often\noverlook the dynamic synchronization and consistent semantics between the\nmodalities. Additionally, the temporal dynamics of emotional fluctuations\nacross different time resolutions in PPS remain underexplored. To address these\nchallenges, we propose PhysioSync, a novel pre-training framework leveraging\ntemporal and cross-modal contrastive learning, inspired by physiological\nsynchronization phenomena. PhysioSync incorporates Cross-Modal Consistency\nAlignment (CM-CA) to model dynamic relationships between EEG and complementary\nPPS, enabling emotion-related synchronizations across modalities. Besides, it\nintroduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to\ncapture emotional synchronization at different temporal resolutions within\nmodalities. After pre-training, cross-resolution and cross-modal features are\nhierarchically fused and fine-tuned to enhance emotion recognition. Experiments\non DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance\nunder uni-modal and cross-modal conditions, highlighting its effectiveness for\nEEG-centered emotion recognition.", "published": "2025-04-24 00:48:03", "link": "http://arxiv.org/abs/2504.17163v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Boundedness and Separation in the Graph Covering Number Framework", "abstract": "For a graph class $\\mathcal G$ and a graph $H$, the four $\\mathcal\nG$-covering numbers of $H$, namely global ${\\rm cn}_{g}^{\\mathcal{G}}(H)$,\nunion ${\\rm cn}_{u}^{\\mathcal{G}}(H)$, local ${\\rm cn}_{l}^{\\mathcal{G}}(H)$,\nand folded ${\\rm cn}_{f}^{\\mathcal{G}}(H)$, each measure in a slightly\ndifferent way how well $H$ can be covered with graphs from $\\mathcal G$. For\nevery $\\mathcal G$ and $H$ it holds \\[\n  {\\rm cn}_{g}^{\\mathcal{G}}(H) \\geq {\\rm cn}_{u}^{\\mathcal{G}}(H) \\geq {\\rm\ncn}_{l}^{\\mathcal{G}}(H) \\geq {\\rm cn}_{f}^{\\mathcal{G}}(H) \\] and in general\neach inequality can be arbitrarily far apart. We investigate structural\nproperties of graph classes $\\mathcal G$ and $\\mathcal H$ such that for all\ngraphs $H \\in \\mathcal{H}$, a larger $\\mathcal G$-covering number of $H$ can be\nbounded in terms of a smaller $\\mathcal G$-covering number of $H$. For example,\nwe prove that if $\\mathcal G$ is hereditary and the chromatic number of graphs\nin $\\mathcal H$ is bounded, then there exists a function $f$ (called a binding\nfunction) such that for all $H \\in \\mathcal{H}$ it holds ${\\rm\ncn}_{u}^{\\mathcal{G}}(H) \\leq f({\\rm cn}_{g}^{\\mathcal{G}}(H))$.\n  For $\\mathcal G$ we consider graph classes that are component-closed,\nhereditary, monotone, sparse, or of bounded chromatic number. For $\\mathcal H$\nwe consider graph classes that are sparse, $M$-minor-free, of bounded chromatic\nnumber, or of bounded treewidth. For each combination and every pair of\n$\\mathcal G$-covering numbers, we either give a binding function $f$ or provide\nan example of such $\\mathcal{G},\\mathcal{H}$ for which no binding function\nexists.", "published": "2025-04-24 11:43:53", "link": "http://arxiv.org/abs/2504.17458v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Morphisms and BWT-run Sensitivity", "abstract": "We study how the application of injective morphisms affects the number $r$ of\nequal-letter runs in the Burrows-Wheeler Transform (BWT). This parameter has\nemerged as a key repetitiveness measure in compressed indexing. We focus on the\nnotion of BWT-run sensitivity after application of an injective morphism. For\nbinary alphabets, we characterize the class of morphisms that preserve the\nnumber of BWT-runs up to a bounded additive increase, by showing that it\ncoincides with the known class of primitivity-preserving morphisms, which are\nthose that map primitive words to primitive words. We further prove that\ndeciding whether a given binary morphism has bounded BWT-run sensitivity is\npossible in polynomial time with respect to the total length of the images of\nthe two letters. Additionally, we explore new structural and combinatorial\nproperties of synchronizing and recognizable morphisms. These results establish\nnew connections between BWT-based compressibility, code theory, and symbolic\ndynamics.", "published": "2025-04-24 11:13:13", "link": "http://arxiv.org/abs/2504.17443v1", "categories": ["cs.FL", "cs.DM", "cs.DS", "math.CO"], "primary_category": "cs.FL"}
{"title": "Vertex evaluation of multiplex graphs using Forman Curvature", "abstract": "Identifying vertices that play a central role is a fundamental problem in\nnetwork analysis. Although traditional centrality measures have been widely\nused for this purpose, the growing complexity of contemporary networks\nnecessitates more sophisticated indicators. Forman curvature has recently\nemerged as a promising approach. In this paper, we define Forman curvature for\nmultilayer networks, a class of complex networks characterized by multiple\ntypes of connections or layers between nodes, which are increasingly used to\nmodel intricate real-world phenomena. We establish the key properties of Forman\ncurvature in the context of multilayer networks and demonstrate its utility for\nidentifying vertices that hold central positions within these networks.\nFurthermore, we show that Forman curvature can also serve as an effective tool\nfor the structural classification of entire multilayer networks.", "published": "2025-04-24 06:28:14", "link": "http://arxiv.org/abs/2504.17286v1", "categories": ["math.CO", "cs.DM", "05C50, 05C85"], "primary_category": "math.CO"}
{"title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction", "abstract": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN.", "published": "2025-04-24 16:08:52", "link": "http://arxiv.org/abs/2504.17699v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "abstract": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.", "published": "2025-04-24 13:17:18", "link": "http://arxiv.org/abs/2504.17529v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Replication and Exploration of Generative Retrieval over Dynamic Corpora", "abstract": "Generative retrieval (GR) has emerged as a promising paradigm in information\nretrieval (IR). However, most existing GR models are developed and evaluated\nusing a static document collection, and their performance in dynamic corpora\nwhere document collections evolve continuously is rarely studied. In this\npaper, we first reproduce and systematically evaluate various representative GR\napproaches over dynamic corpora. Through extensive experiments, we reveal that\nexisting GR models with \\textit{text-based} docids show superior generalization\nto unseen documents. We observe that the more fine-grained the docid design in\nthe GR model, the better its performance over dynamic corpora, surpassing BM25\nand even being comparable to dense retrieval methods. While GR models with\n\\textit{numeric-based} docids show high efficiency, their performance drops\nsignificantly over dynamic corpora. Furthermore, our experiments find that the\nunderperformance of numeric-based docids is partly due to their excessive\ntendency toward the initial document set, which likely results from overfitting\non the training set. We then conduct an in-depth analysis of the\nbest-performing GR methods. We identify three critical advantages of text-based\ndocids in dynamic corpora: (i) Semantic alignment with language models'\npretrained knowledge, (ii) Fine-grained docid design, and (iii) High lexical\ndiversity. Building on these insights, we finally propose a novel multi-docid\ndesign that leverages both the efficiency of numeric-based docids and the\neffectiveness of text-based docids, achieving improved performance in dynamic\ncorpus without requiring additional retraining. Our work offers empirical\nevidence for advancing GR methods over dynamic corpora and paves the way for\ndeveloping more generalized yet efficient GR models in real-world search\nengines.", "published": "2025-04-24 13:01:23", "link": "http://arxiv.org/abs/2504.17519v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Adaptive Orchestration of Modular Generative Information Access Systems", "abstract": "Advancements in large language models (LLMs) have driven the emergence of\ncomplex new systems to provide access to information, that we will collectively\nrefer to as modular generative information access (GenIA) systems. They\nintegrate a broad and evolving range of specialized components, including LLMs,\nretrieval models, and a heterogeneous set of sources and tools. While\nmodularity offers flexibility, it also raises critical challenges: How can we\nsystematically characterize the space of possible modules and their\ninteractions? How can we automate and optimize interactions among these\nheterogeneous components? And, how do we enable this modular system to\ndynamically adapt to varying user query requirements and evolving module\ncapabilities? In this perspective paper, we argue that the architecture of\nfuture modular generative information access systems will not just assemble\npowerful components, but enable a self-organizing system through real-time\nadaptive orchestration -- where components' interactions are dynamically\nconfigured for each user input, maximizing information relevance while\nminimizing computational overhead. We give provisional answers to the questions\nraised above with a roadmap that depicts the key principles and methods for\ndesigning such an adaptive modular system. We identify pressing challenges, and\npropose avenues for addressing them in the years ahead. This perspective urges\nthe IR community to rethink modular system designs for developing adaptive,\nself-optimizing, and future-ready architectures that evolve alongside their\nrapidly advancing underlying technologies.", "published": "2025-04-24 11:35:43", "link": "http://arxiv.org/abs/2504.17454v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for Conversational Recommendation", "abstract": "Conversational recommender systems aim to provide personalized\nrecommendations by analyzing and utilizing contextual information related to\ndialogue. However, existing methods typically model the dialogue context as a\nwhole, neglecting the inherent complexity and entanglement within the dialogue.\nSpecifically, a dialogue comprises both focus information and background\ninformation, which mutually influence each other. Current methods tend to model\nthese two types of information mixedly, leading to misinterpretation of users'\nactual needs, thereby lowering the accuracy of recommendations. To address this\nissue, this paper proposes a novel model to introduce contextual\ndisentanglement for improving conversational recommender systems, named\nDisenCRS. The proposed model DisenCRS employs a dual disentanglement framework,\nincluding self-supervised contrastive disentanglement and counterfactual\ninference disentanglement, to effectively distinguish focus information and\nbackground information from the dialogue context under unsupervised conditions.\nMoreover, we design an adaptive prompt learning module to automatically select\nthe most suitable prompt based on the specific dialogue context, fully\nleveraging the power of large language models. Experimental results on two\nwidely used public datasets demonstrate that DisenCRS significantly outperforms\nexisting conversational recommendation models, achieving superior performance\non both item recommendation and response generation tasks.", "published": "2025-04-24 10:33:26", "link": "http://arxiv.org/abs/2504.17427v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent", "abstract": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories.", "published": "2025-04-24 07:52:09", "link": "http://arxiv.org/abs/2504.17334v1", "categories": ["cs.HC", "cs.IR"], "primary_category": "cs.HC"}
{"title": "Quantum Error Correction with Girth-16 Non-Binary LDPC Codes via Affine Permutation Construction", "abstract": "We propose a method for constructing quantum error-correcting codes based on\nnon-binary low-density parity-check codes with girth 16. In conventional\nconstructions using circulant permutation matrices, the girth is upper-bounded\nby 12, which limits the suppression of harmful short cycles. Our construction\nemploys affine permutation matrices and a randomized sequential selection\nprocedure designed to eliminate short cycles, which are known to limit decoding\nperformance.\n  Joint belief propagation decoding is applied over depolarizing channels.\nNumerical experiments confirm that the proposed codes reduce the number of\nlow-weight codewords in $C_X \\setminus C_Z^\\perp$ and $C_Z \\setminus\nC_X^\\perp$, and thus have the potential to suppress error floors. In addition,\nwe obtain a significantly improved upper bound on the minimum distance, which\nwe conjecture to be tight.", "published": "2025-04-24 17:59:58", "link": "http://arxiv.org/abs/2504.17790v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "UNILoc: Unified Localization Combining Model-Based Geometry and Unsupervised Learning", "abstract": "Accurate mobile device localization is critical for emerging 5G/6G\napplications such as autonomous vehicles and augmented reality. In this paper,\nwe propose a unified localization method that integrates model-based and\nmachine learning (ML)-based methods to reap their respective advantages by\nexploiting available map information. In order to avoid supervised learning, we\ngenerate training labels automatically via optimal transport (OT) by fusing\ngeometric estimates with building layouts. Ray-tracing based simulations are\ncarried out to demonstrate that the proposed method significantly improves\npositioning accuracy for both line-of-sight (LoS) users (compared to ML-based\nmethods) and non-line-of-sight (NLoS) users (compared to model-based methods).\nRemarkably, the unified method is able to achieve competitive overall\nperformance with the fully-supervised fingerprinting, while eliminating the\nneed for cumbersome labeled data measurement and collection.", "published": "2025-04-24 15:45:54", "link": "http://arxiv.org/abs/2504.17676v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "DTECM: Digital Twin Enabled Channel Measurement and Modeling in Terahertz Urban Macrocell", "abstract": "In this work, in the THz UMa, extensive channel measurements are conducted\nand an accurate channel model is developed by combining ray-tracing, computer\nvision (CV), and statistical methods. Specifically, substantial channel\nmeasurement campaigns with distances up to 410~m are conducted at 220~GHz, with\nnanosecond-level absolute time synchronization. Based on the measurement\nresults, the propagation phenomena are analyzed in detail and the channel\ncharacteristics are calculated and statistically modeled. Furthermore, a\ndigital twin enabled channel model (DTECM) is proposed, which generates THz\nchannel responses in a hybrid manner. Specifically, the dominant paths are\ngenerated deterministically by using the ray-tracing technique and CV methods.\nApart from the path gains determined by ray-tracing, the additional foliage\nloss is accurately modeled based on foliage information extracted from\npanoramic pictures. To maintain a low computational complexity for the DTECM,\nnon-dominant paths are then generated statistically. Numeric results reveal\nthat compared to the traditional statistical channel models, the DTECM reduces\nthe path loss modeling error from 14~dB to 4~dB, showing its great superiority.\nFurthermore, a preliminary link performance evaluation using the DTECM\nindicates that THz UMa is feasible, though requiring high antenna gains and\ncoverage extension techniques to achieve high spectral efficiencies and wide\ncoverage.", "published": "2025-04-24 15:40:57", "link": "http://arxiv.org/abs/2504.17673v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sparsity-Exploiting Channel Estimation For Unsourced Random Access With Fluid Antenna", "abstract": "This work explores the channel estimation (CE) problem in uplink transmission\nfor unsourced random access (URA) with a fluid antenna receiver. The additional\nspatial diversity in a fluid antenna system (FAS) addresses the needs of URA\ndesign in multiple-input and multiple-output (MIMO) systems. We present two CE\nstrategies based on the activation of different FAS ports, namely alternate\nports and partial ports CE. Both strategies facilitate the estimation of\nchannel coefficients and angles of arrival (AoAs). Additionally, we discuss how\nto refine channel estimation by leveraging the sparsity of finite scatterers.\nSpecifically, the proposed partial ports CE strategy is implemented using a\nregularized estimator, and we optimize the estimator's parameter to achieve the\ndesired AoA precision and refinement. Extensive numerical results demonstrate\nthe feasibility of the proposed strategies, and a comparison with a\nconventional receiver using half-wavelength antennas highlights the promising\nfuture of integrating URA and FAS.", "published": "2025-04-24 15:07:12", "link": "http://arxiv.org/abs/2504.17634v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Integrated Sensing and Communications for Unsourced Random Access: A Spectrum Sharing Compressive Sensing Approach", "abstract": "This paper addresses the unsourced/uncoordinated random access problem in an\nintegrated sensing and communications (ISAC) system, with a focus on uplink\nmultiple access code design. Recent theoretical advancements highlight that an\nISAC system will be overwhelmed by the increasing number of active devices,\ndriven by the growth of massive machine-type communication (mMTC). To meet the\ndemands of future mMTC network, fundamental solutions are required that ensure\nrobust capacity while maintaining favorable energy and spectral efficiency. One\npromising approach to support emerging massive connectivity is the development\nof systems based on the unsourced ISAC (UNISAC) framework. This paper proposes\na spectrum-sharing compressive sensing-based UNISAC (SSCS-UNISAC) and offers\ninsights into the practical design of UNISAC multiple access codes. In this\nframework, both communication signals (data transmission) and sensing signals\n(e.g., radar echoes) overlap within finite channel uses and are transmitted via\nthe proposed UNISAC protocol. The proposed decoder exhibits robust performance,\nproviding 20-30 dB capacity gains compared to conventional protocols such as\nTDMA and ALOHA. Numerical results validate the promising performance of the\nproposed scheme.", "published": "2025-04-24 14:53:28", "link": "http://arxiv.org/abs/2504.17629v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "MacWilliams Theory over Zk and nu-functions over Lattices", "abstract": "Continuing previous works on MacWilliams theory over codes and lattices, a\ngeneralization of the MacWilliams theory over $\\mathbb{Z}_k$ for $m$ codes is\nestablished, and the complete weight enumerator MacWilliams identity also holds\nfor codes over the finitely generated rings $\\mathbb{Z}_k[\\xi]$. In the context\nof lattices, the analogy of the MacWilliams identity associated with\nnu-function was conjectured by Sol\\'{e} in 1995, and we present a new formula\nfor nu-function over the lattices associated with a ternary code, which is\nrather different from the original conjecture. Furthermore, we provide many\ncounterexamples to show that the Sol\\'{e} conjecture never holds in the general\ncase, except for the lattices associated with a binary code.", "published": "2025-04-24 14:19:51", "link": "http://arxiv.org/abs/2504.17589v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Secure Network Function Computation for Linear Functions, Part II: Target-Function Security", "abstract": "In this Part II of a two-part paper, we put forward secure network function\ncomputation, where in a directed acyclic network, a sink node is required to\ncompute a target function of which the inputs are generated as source messages\nat multiple source nodes, while a wiretapper, who can access any one but not\nmore than one wiretap set in a given collection of wiretap sets, is not allowed\nto obtain any information about a security function of the source messages. In\nPart I of the two-part paper, we have investigated securely computing linear\nfunctions with the wiretapper who can eavesdrop any edge subset up to a certain\nsize r, referred to as the security level, where the security function is the\nidentity function. The notion of this security is called source security. In\nthe current paper, we consider another interesting model which is the same as\nthe above one except that the security function is identical to the target\nfunction, i.e., we need to protect the information on the target function from\nbeing leaked to the wiretapper. The notion of this security is called\ntarget-function security. We first prove a non-trivial upper bound on the\nsecure computing capacity, which is applicable to arbitrary network topologies\nand arbitrary security levels. In particular, when the security level r is\nequal to 0, the upper bound reduces to the computing capacity without security\nconsideration. Further, from an algebraic point of view, we prove two\nequivalent conditions for target-function security and source security for the\nexistence of the corresponding linear function-computing secure network codes.\nWith them, for any linear function over a given finite field, we develop a code\nconstruction of linear secure network codes for target-function security and\nthus obtain a lower bound on the secure computing capacity; and also generalize\nthe code construction developed in Part I for source security.", "published": "2025-04-24 12:57:20", "link": "http://arxiv.org/abs/2504.17514v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Subcode Ensemble Decoding of Polar Codes", "abstract": "In the short block length regime, pre-transformed polar codes together with\nsuccessive cancellation list (SCL) decoding possess excellent error correction\ncapabilities. However, in practice, the list size is limited due to the\nsuboptimal scaling of the required area in hardware implementations.\nAutomorphism ensemble decoding (AED) can improve performance for a fixed list\nsize by running multiple parallel SCL decodings on permuted received words,\nyielding a list of estimates from which the final estimate is selected. Yet,\nAED is limited to appropriately designed polar codes. Subcode ensemble decoding\n(ScED) was recently proposed for low-density parity-check codes and does not\nimpose such design constraints. It uses multiple decodings in different\nsubcodes, ensuring that the selected subcodes jointly cover the original code.\nWe extend ScED to polar codes by expressing polar subcodes through suitable\npre-transformations (PTs). To this end, we describe a framework classifying\npre-transformations for pre-transformed polar codes based on their role in\nencoding and decoding. Within this framework, we propose a new type of PT\nenabling ScED for polar codes, analyze its properties, and discuss how to\nconstruct an efficient ensemble.", "published": "2025-04-24 12:55:23", "link": "http://arxiv.org/abs/2504.17511v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware", "abstract": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34.", "published": "2025-04-24 09:49:18", "link": "http://arxiv.org/abs/2504.17403v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.LG"}
{"title": "Error Exponents for DNA Storage Codes with a Variable Number of Reads", "abstract": "In this paper, we study error exponents for a concatataned coding based class\nof DNA storage codes in which the number of reads performed can be variable.\nThat is, the decoder can sequentially perform reads and choose whether to\noutput the final decision or take more reads, and we are interested in\nminimizing the average number of reads performed rather than a fixed\npre-specified value. We show that this flexibility leads to a considerable\nreduction in the error probability compared to a fixed number of reads, not\nonly in terms of constants in the error exponent but also in the scaling laws.\nThis is shown via an achievability result for a suitably-designed protocol, and\nin certain parameter regimes we additionally establish a matching converse that\nholds for all protocols within a broader concatenated coding based class.", "published": "2025-04-24 07:56:21", "link": "http://arxiv.org/abs/2504.17337v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Service Rate Regions of MDS Codes & Fractional Matchings in Quasi-uniform Hypergraphs", "abstract": "The service rate region (SRR) has emerged as a critical performance metric\nfor distributed systems that store data redundantly. It measures the system's\nability to serve multiple users concurrently. Mathematically, the SRR is a\npolytope in R^k where each dimension corresponds to the service request rate of\none of the k data objects. This paper focuses on systems employing a class of\nMaximum Distance Separable (MDS) codes. For each code in the class, we\ncharacterize the k axes intercept points of its SRR, and the smallest standard\nsimplex that includes the SRR. We use these results to show that the SRR grows\nwith the increasing number of systematic columns in the generator matrices. We\nestablish a graph-theoretic framework associating this SRR problem with\nfractional matchings in quasi-uniform hypergraphs. Identifying the SRR polytope\nis equivalent to determining a particular image of the fractional-matching\npolytope. We introduce a notion of Greedy Matching and show that it is\nsufficient to focus on these matchings to characterize the SRR rather than the\nentire matching polytope. With these tools, we determine the SRR of a large\nsubset of the considered class of codes. Our results generalize previous\ncharacterizations of systematic and non-systematic MDS-coded systems, offering\na unified framework for analyzing service rate regions of codes.", "published": "2025-04-24 04:44:37", "link": "http://arxiv.org/abs/2504.17244v1", "categories": ["cs.IT", "math.CO", "math.IT"], "primary_category": "cs.IT"}
{"title": "Quantum-Enhanced Change Detection and Joint Communication-Detection", "abstract": "Quick detection of transmittance changes in optical channel is crucial for\nsecure communication. We demonstrate that pre-shared entanglement using\ntwo-mode squeezed vacuum states significantly reduces detection latency\ncompared to classical and entanglement-augmented coherent-state probes. The\nchange detection latency is inversely proportional to the quantum relative\nentropy (QRE), which goes to infinity in the absence of thermal noise,\nsuggesting idealized instantaneous detection. However, in realistic scenarios,\nwe show that QRE scales logarithmically with the inverse of the thermal noise\nmean photon number. We propose a receiver that achieves this scaling and\nquantify its performance gains over existing methods. Additionally, we explore\nthe fundamental trade-off between communication capacity and change detection\nlatency, highlighting how pre-shared entanglement enhances both.", "published": "2025-04-24 04:17:55", "link": "http://arxiv.org/abs/2504.17237v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space", "abstract": "We establish a single-letter characterization of the fundamental\ndistortion-rate-perception tradeoff with limited common randomness under the\nsquared error distortion measure and the squared Wasserstein-2 perception\nmeasure. Moreover, it is shown that this single-letter characterization can be\nexplicitly evaluated for the Gaussian source. Various notions of universal\nrepresentation are also clarified.", "published": "2025-04-24 04:13:56", "link": "http://arxiv.org/abs/2504.17236v1", "categories": ["cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "P$_\\ell$-Kyber: Packing $\\ell$ Plaintexts and Lattice Coding for Kyber", "abstract": "In this work, we propose a joint design of encoding and encryption processes\nfor KEMs like Kyber, without assuming the independence of the decoding noise\nentries. Our design features two techniques: ciphertext packing and lattice\npacking. First, we extend the Peikert-Vaikuntanathan-Waters (PVW) method to the\nKyber: $\\ell$ plaintexts are packed into a single ciphertext. This scheme is\nreferred to as P$_\\ell$-Kyber. We prove that the P$_\\ell$-Kyber is IND-CCA\nsecure under the M-LWE hardness assumption. We show that the decryption\ndecoding noise entries across the $\\ell$ plaintexts (also known as layers) are\nmutually independent. Second, we propose a cross-layer lattice encoding scheme\nfor the P$_\\ell$-Kyber, where every $\\ell$ cross-layer information symbols are\nencoded to a lattice point. This way we obtain a \\emph{coded} P$_\\ell$-Kyber,\nwhere the decoding noise entries for each lattice point are mutually\nindependent. Therefore, the decryption failure rate (DFR) analysis does not\nrequire the assumption of independence among the decryption decoding noise\nentries. Both DFR and communication cost (CER) are greatly decreased thanks to\nciphertext packing and lattice packing. Finally, we demonstrate that with\n$\\ell=24$ and Leech lattice encoder, the proposed coded P$_\\ell$-KYBER1024\nachieves DFR $<2^{-281}$ and CER $ = 4.6$, i.e., a decrease of CER by $90\\%$\ncompared to KYBER1024.", "published": "2025-04-24 01:39:36", "link": "http://arxiv.org/abs/2504.17185v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "abstract": "Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io.", "published": "2025-04-24 17:58:21", "link": "http://arxiv.org/abs/2504.17782v1", "categories": ["cs.SD", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language Models", "abstract": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios.", "published": "2025-04-24 17:56:22", "link": "http://arxiv.org/abs/2504.17780v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency", "abstract": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing.", "published": "2025-04-24 17:10:18", "link": "http://arxiv.org/abs/2504.17752v1", "categories": ["cs.LG", "cs.ET", "eess.SP", "physics.app-ph"], "primary_category": "cs.LG"}
{"title": "MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction", "abstract": "Graph Neural Networks (GNNs) have been widely used for various learning\ntasks, ranging from node classification to link prediction. They have\ndemonstrated excellent performance in multiple domains involving\ngraph-structured data. However, an important category of learning tasks, namely\nlink weight prediction, has received less emphasis due to its increased\ncomplexity compared to binary link classification. Link weight prediction\nbecomes even more challenging when considering multilayer networks, where nodes\ncan be interconnected across multiple layers. To address these challenges, we\npropose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),\nwhich spatially embeds information across multiple layers to predict interlayer\nlink weights. The MSGCN model generalizes spatial graph convolution to\nmultiplex networks and captures the geometric structure of nodes across\nmultiple layers. Extensive experiments using data with known interlayer link\ninformation show that the MSGCN model has robust, accurate, and generalizable\nlink weight prediction performance across a wide variety of multiplex network\nstructures.", "published": "2025-04-24 17:08:16", "link": "http://arxiv.org/abs/2504.17749v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Embedding Empirical Distributions for Computing Optimal Transport Maps", "abstract": "Distributional data have become increasingly prominent in modern signal\nprocessing, highlighting the necessity of computing optimal transport (OT) maps\nacross multiple probability distributions. Nevertheless, recent studies on\nneural OT methods predominantly focused on the efficient computation of a\nsingle map between two distributions. To address this challenge, we introduce a\nnovel approach to learning transport maps for new empirical distributions.\nSpecifically, we employ the transformer architecture to produce embeddings from\ndistributional data of varying length; these embeddings are then fed into a\nhypernetwork to generate neural OT maps. Various numerical experiments were\nconducted to validate the embeddings and the generated OT maps. The model\nimplementation and the code are provided on\nhttps://github.com/jiangmingchen/HOTET.", "published": "2025-04-24 16:52:48", "link": "http://arxiv.org/abs/2504.17740v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Interpretable Early Detection of Parkinson's Disease through Speech Analysis", "abstract": "Parkinson's disease is a progressive neurodegenerative disorder affecting\nmotor and non-motor functions, with speech impairments among its earliest\nsymptoms. Speech impairments offer a valuable diagnostic opportunity, with\nmachine learning advances providing promising tools for timely detection. In\nthis research, we propose a deep learning approach for early Parkinson's\ndisease detection from speech recordings, which also highlights the vocal\nsegments driving predictions to enhance interpretability. This approach seeks\nto associate predictive speech patterns with articulatory features, providing a\nbasis for interpreting underlying neuromuscular impairments. We evaluated our\napproach using the Italian Parkinson's Voice and Speech Database, containing\n831 audio recordings from 65 participants, including both healthy individuals\nand patients. Our approach showed competitive classification performance\ncompared to state-of-the-art methods, while providing enhanced interpretability\nby identifying key speech features influencing predictions.", "published": "2025-04-24 16:50:52", "link": "http://arxiv.org/abs/2504.17739v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework", "abstract": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment.", "published": "2025-04-24 16:36:19", "link": "http://arxiv.org/abs/2504.17723v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Evaluating Uncertainty in Deep Gaussian Processes", "abstract": "Reliable uncertainty estimates are crucial in modern machine learning. Deep\nGaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs\nhierarchically, offering promising methods for uncertainty quantification\ngrounded in Bayesian principles. However, their empirical calibration and\nrobustness under distribution shift relative to baselines like Deep Ensembles\nremain understudied. This work evaluates these models on regression (CASP\ndataset) and classification (ESR dataset) tasks, assessing predictive\nperformance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)\nand Expected Calibration Error (ECE), alongside robustness under various\nsynthetic feature-level distribution shifts. Results indicate DSPPs provide\nstrong in-distribution calibration leveraging their sigma point approximations.\nHowever, compared to Deep Ensembles, which demonstrated superior robustness in\nboth per- formance and calibration under the tested shifts, the GP-based\nmethods showed vulnerabilities, exhibiting particular sensitivity in the\nobserved metrics. Our findings underscore ensembles as a robust baseline,\nsuggesting that while deep GP methods offer good in-distribution calibration,\ntheir practical robustness under distribution shift requires careful\nevaluation. To facilitate reproducibility, we make our code available at\nhttps://github.com/matthjs/xai-gp.", "published": "2025-04-24 16:31:55", "link": "http://arxiv.org/abs/2504.17719v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation", "abstract": "Intelligent condition monitoring of wind turbines is essential for reducing\ndowntimes. Machine learning models trained on wind turbine operation data are\ncommonly used to detect anomalies and, eventually, operation faults. However,\ndata-driven normal behavior models (NBMs) require a substantial amount of\ntraining data, as NBMs trained with scarce data may result in unreliable fault\ndiagnosis. To overcome this limitation, we present a novel generative deep\nlearning approach to make SCADA samples from one wind turbine lacking training\ndata resemble SCADA data from wind turbines with representative training data.\nThrough CycleGAN-based domain mapping, our method enables the application of an\nNBM trained on an existing wind turbine to one with severely limited data. We\ndemonstrate our approach on field data mapping SCADA samples across 7\nsubstantially different WTs. Our findings show significantly improved fault\ndiagnosis in wind turbines with scarce data. Our method achieves the most\nsimilar anomaly scores to an NBM trained with abundant data, outperforming NBMs\ntrained on scarce training data with improvements of +10.3% in F1-score when 1\nmonth of training data is available and +16.8% when 2 weeks are available. The\ndomain mapping approach outperforms conventional fine-tuning at all considered\ndegrees of data scarcity, ranging from 1 to 8 weeks of training data. The\nproposed technique enables earlier and more reliable fault diagnosis in newly\ninstalled wind farms, demonstrating a novel and promising research direction to\nimprove anomaly detection when faced with training data scarcity.", "published": "2025-04-24 16:14:04", "link": "http://arxiv.org/abs/2504.17709v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "On the Generalization of Adversarially Trained Quantum Classifiers", "abstract": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments.", "published": "2025-04-24 15:59:55", "link": "http://arxiv.org/abs/2504.17690v1", "categories": ["quant-ph", "cs.LG"], "primary_category": "quant-ph"}
{"title": "On Multivariate Financial Time Series Classification", "abstract": "This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series.", "published": "2025-04-24 15:33:00", "link": "http://arxiv.org/abs/2504.17664v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models", "abstract": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems.", "published": "2025-04-24 15:29:39", "link": "http://arxiv.org/abs/2504.17660v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation", "abstract": "Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges: on-demand generation of realistic 3D atomic structures\nthat respect the conformational diversity of polymer structures. Generative\nalgorithms for 3D structures of inorganic crystals, bio-polymers, and small\nmolecules exist, but have not addressed synthetic polymers. In this work, we\nintroduce polyGen, the first latent diffusion model designed specifically to\ngenerate realistic polymer structures from minimal inputs such as the repeat\nunit chemistry alone, leveraging a molecular encoding that captures polymer\nconnectivity throughout the architecture. Due to a scarce dataset of only 3855\nDFT-optimized polymer structures, we augment our training with DFT-optimized\nmolecular structures, showing improvement in joint learning between similar\nchemical structures. We also establish structure matching criteria to benchmark\nour approach on this novel problem. polyGen effectively generates diverse\nconformations of both linear chains and complex branched structures, though its\nperformance decreases when handling repeat units with a high atom count. Given\nthese initial results, polyGen represents a paradigm shift in atomic-level\nstructure generation for polymer science-the first proof-of-concept for\npredicting realistic atomic-level polymer conformations while accounting for\ntheir intrinsic structural flexibility.", "published": "2025-04-24 15:26:00", "link": "http://arxiv.org/abs/2504.17656v1", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "cs.CE"}
{"title": "Likelihood-Free Variational Autoencoders", "abstract": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling.", "published": "2025-04-24 14:44:46", "link": "http://arxiv.org/abs/2504.17622v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation", "abstract": "Synthetic Electronic Health Record (EHR) time-series generation is crucial\nfor advancing clinical machine learning models, as it helps address data\nscarcity by providing more training data. However, most existing approaches\nfocus primarily on replicating statistical distributions and temporal\ndependencies of real-world data. We argue that fidelity to observed data alone\ndoes not guarantee better model performance, as common patterns may dominate,\nlimiting the representation of rare but important conditions. This highlights\nthe need for generate synthetic samples to improve performance of specific\nclinical models to fulfill their target outcomes. To address this, we propose\nTarDiff, a novel target-oriented diffusion framework that integrates\ntask-specific influence guidance into the synthetic data generation process.\nUnlike conventional approaches that mimic training data distributions, TarDiff\noptimizes synthetic samples by quantifying their expected contribution to\nimproving downstream model performance through influence functions.\nSpecifically, we measure the reduction in task-specific loss induced by\nsynthetic samples and embed this influence gradient into the reverse diffusion\nprocess, thereby steering the generation towards utility-optimized data.\nEvaluated on six publicly available EHR datasets, TarDiff achieves\nstate-of-the-art performance, outperforming existing methods by up to 20.4% in\nAUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only\npreserves temporal fidelity but also enhances downstream model performance,\noffering a robust solution to data scarcity and class imbalance in healthcare\nanalytics.", "published": "2025-04-24 14:36:10", "link": "http://arxiv.org/abs/2504.17613v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation", "abstract": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.", "published": "2025-04-24 14:26:42", "link": "http://arxiv.org/abs/2504.17601v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Machine Learning Approach for Denoising and Upsampling HRTFs", "abstract": "The demand for realistic virtual immersive audio continues to grow, with\nHead-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how\nsound reaches our ears, reflecting unique anatomical features and enhancing\nspatial perception. It has been shown that personalized HRTFs improve\nlocalization accuracy, but their measurement remains time-consuming and\nrequires a noise-free environment. Although machine learning has been shown to\nreduce the required measurement points and, thus, the measurement time, a\ncontrolled environment is still necessary. This paper proposes a method to\naddress this constraint by presenting a novel technique that can upsample\nsparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy\nU-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)\nfor upsampling from three measurement points. The proposed method achieves a\nlog-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of\n0.0070, demonstrating the method's effectiveness in HRTF upsampling.", "published": "2025-04-24 14:17:57", "link": "http://arxiv.org/abs/2504.17586v1", "categories": ["cs.SD", "cs.LG"], "primary_category": "cs.SD"}
{"title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference", "abstract": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.", "published": "2025-04-24 14:14:07", "link": "http://arxiv.org/abs/2504.17584v1", "categories": ["cs.AR", "cs.LG"], "primary_category": "cs.AR"}
{"title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization", "abstract": "Recent research in Cooperative Coevolution~(CC) have achieved promising\nprogress in solving large-scale global optimization problems. However, existing\nCC paradigms have a primary limitation in that they require deep expertise for\nselecting or designing effective variable decomposition strategies. Inspired by\nadvancements in Meta-Black-Box Optimization, this paper introduces LCC, a\npioneering learning-based cooperative coevolution framework that dynamically\nschedules decomposition strategies during optimization processes. The\ndecomposition strategy selector is parameterized through a neural network,\nwhich processes a meticulously crafted set of optimization status features to\ndetermine the optimal strategy for each optimization step. The network is\ntrained via the Proximal Policy Optimization method in a reinforcement learning\nmanner across a collection of representative problems, aiming to maximize the\nexpected optimization performance. Extensive experimental results demonstrate\nthat LCC not only offers certain advantages over state-of-the-art baselines in\nterms of optimization effectiveness and resource consumption, but it also\nexhibits promising transferability towards unseen problems.", "published": "2025-04-24 14:09:22", "link": "http://arxiv.org/abs/2504.17578v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "TileLang: A Composable Tiled Programming Model for AI Systems", "abstract": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development.", "published": "2025-04-24 14:08:49", "link": "http://arxiv.org/abs/2504.17577v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis", "abstract": "Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive.", "published": "2025-04-24 13:58:07", "link": "http://arxiv.org/abs/2504.17568v1", "categories": ["cs.LG", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Quantum Autoencoder for Multivariate Time Series Anomaly Detection", "abstract": "Anomaly Detection (AD) defines the task of identifying observations or events\nthat deviate from typical - or normal - patterns, a critical capability in IT\nsecurity for recognizing incidents such as system misconfigurations, malware\ninfections, or cyberattacks. In enterprise environments like SAP HANA Cloud\nsystems, this task often involves monitoring high-dimensional, multivariate\ntime series (MTS) derived from telemetry and log data. With the advent of\nquantum machine learning offering efficient calculations in high-dimensional\nlatent spaces, many avenues open for dealing with such complex data. One\napproach is the Quantum Autoencoder (QAE), an emerging and promising method\nwith potential for application in both data compression and AD. However, prior\napplications of QAEs to time series AD have been restricted to univariate data,\nlimiting their relevance for real-world enterprise systems. In this work, we\nintroduce a novel QAE-based framework designed specifically for MTS AD towards\nenterprise scale. We theoretically develop and experimentally validate the\narchitecture, demonstrating that our QAE achieves performance competitive with\nneural-network-based autoencoders while requiring fewer trainable parameters.\nWe evaluate our model on datasets that closely reflect SAP system telemetry and\nshow that the proposed QAE is a viable and efficient alternative for\nsemisupervised AD in real-world enterprise settings.", "published": "2025-04-24 13:40:06", "link": "http://arxiv.org/abs/2504.17548v1", "categories": ["quant-ph", "cs.CR", "cs.LG"], "primary_category": "quant-ph"}
{"title": "An introduction to R package `mvs`", "abstract": "In biomedical science, a set of objects or persons can often be described by\nmultiple distinct sets of features obtained from different data sources or\nmodalities (called \"multi-view data\"). Classical machine learning methods\nignore the multi-view structure of such data, limiting model interpretability\nand performance. The R package `mvs` provides methods that were designed\nspecifically for dealing with multi-view data, based on the multi-view stacking\n(MVS) framework. MVS is a form of supervised (machine) learning used to train\nmulti-view classification or prediction models. MVS works by training a\nlearning algorithm on each view separately, estimating the predictive power of\neach view-specific model through cross-validation, and then using another\nlearning algorithm to assign weights to the view-specific models based on their\nestimated predictions. MVS is a form of ensemble learning, dividing the large\nmulti-view learning problem into smaller sub-problems. Most of these\nsub-problems can be solved in parallel, making it computationally attractive.\nAdditionally, the number of features of the sub-problems is greatly reduced\ncompared with the full multi-view learning problem. This makes MVS especially\nuseful when the total number of features is larger than the number of\nobservations (i.e., high-dimensional data). MVS can still be applied even if\nthe sub-problems are themselves high-dimensional by adding suitable penalty\nterms to the learning algorithms. Furthermore, MVS can be used to automatically\nselect the views which are most important for prediction. The R package `mvs`\nmakes fitting MVS models, including such penalty terms, easily and openly\naccessible. `mvs` allows for the fitting of stacked models with any number of\nlevels, with different penalty terms, different outcome distributions, and\nprovides several options for missing data handling.", "published": "2025-04-24 13:34:31", "link": "http://arxiv.org/abs/2504.17546v1", "categories": ["stat.CO", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks", "abstract": "Future networks (including 6G) are poised to accelerate the realisation of\nInternet of Everything. However, it will result in a high demand for computing\nresources to support new services. Mobile Edge Computing (MEC) is a promising\nsolution, enabling to offload computation-intensive tasks to nearby edge\nservers from the end-user devices, thereby reducing latency and energy\nconsumption. However, relying solely on a single MEC server for task offloading\ncan lead to uneven resource utilisation and suboptimal performance in complex\nscenarios. Additionally, traditional task offloading strategies specialise in\ncentralised policy decisions, which unavoidably entail extreme transmission\nlatency and reach computational bottleneck. To fill the gaps, we propose a\nlatency and energy efficient Cooperative Task Offloading framework with\nTransformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent\ndeep reinforcement learning to address these challenges. This approach fosters\nedge-edge cooperation and decreases the synchronous waiting time by performing\nasynchronous training, optimising task offloading, and resource allocation\nacross distributed networks. The performance evaluation demonstrates that the\nproposed CTO-TP algorithm reduces up to 80% overall system latency and 87%\nenergy consumption compared to the baseline schemes.", "published": "2025-04-24 13:12:12", "link": "http://arxiv.org/abs/2504.17526v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity", "abstract": "To jointly tackle the challenges of data and node heterogeneity in\ndecentralized learning, we propose a distributed strong lottery ticket\nhypothesis (DSLTH), based on which a communication-efficient personalized\nlearning algorithm is developed. In the proposed method, each local model is\nrepresented as the Hadamard product of global real-valued parameters and a\npersonalized binary mask for pruning. The local model is learned by updating\nand fusing the personalized binary masks while the real-valued parameters are\nfixed among different agents. To further reduce the complexity of hardware\nimplementation, we incorporate a group sparse regularization term in the loss\nfunction, enabling the learned local model to achieve structured sparsity.\nThen, a binary mask aggregation algorithm is designed by introducing an\nintermediate aggregation tensor and adding a personalized fine-tuning step in\neach iteration, which constrains model updates towards the local data\ndistribution. The proposed method effectively leverages the relativity among\nagents while meeting personalized requirements in heterogeneous node\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\nas the foundation of the proposed method. Numerical simulations confirm the\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\nalgorithm.", "published": "2025-04-24 13:02:54", "link": "http://arxiv.org/abs/2504.17520v1", "categories": ["cs.LG", "cs.DC", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data", "abstract": "We study how the degree of nonlinearity in the input data affects the optimal\ndesign of reservoir computers, focusing on how closely the model's nonlinearity\nshould align with that of the data. By reducing minimal RCs to a single tunable\nnonlinearity parameter, we explore how the predictive performance varies with\nthe degree of nonlinearity in the reservoir. To provide controlled testbeds, we\ngeneralize to the fractional Halvorsen system, a novel chaotic system with\nfractional exponents. Our experiments reveal that the prediction performance is\nmaximized when the reservoir's nonlinearity matches the nonlinearity present in\nthe data. In cases where multiple nonlinearities are present in the data, we\nfind that the correlation dimension of the predicted signal is reconstructed\ncorrectly when the smallest nonlinearity is matched. We use this observation to\npropose a method for estimating the minimal nonlinearity in unknown time series\nby sweeping the reservoir exponent and identifying the transition to a\nsuccessful reconstruction. Applying this method to both synthetic and\nreal-world datasets, including financial time series, we demonstrate its\npractical viability. Finally, we transfer these insights to classical RC by\naugmenting traditional architectures with fractional, generalized reservoir\nstates. This yields performance gains, particularly in resource-constrained\nscenarios such as physical reservoirs, where increasing reservoir size is\nimpractical or economically unviable. Our work provides a principled route\ntoward tailoring RCs to the intrinsic complexity of the systems they aim to\nmodel.", "published": "2025-04-24 12:47:21", "link": "http://arxiv.org/abs/2504.17503v1", "categories": ["cs.LG", "nlin.CD"], "primary_category": "cs.LG"}
{"title": "Prototype-enhanced prediction in graph neural networks for climate applications", "abstract": "Data-driven emulators are increasingly being used to learn and emulate\nphysics-based simulations, reducing computational expense and run time. Here,\nwe present a structured way to improve the quality of these high-dimensional\nemulated outputs, through the use of prototypes: an approximation of the\nemulator's output passed as an input, which informs the model and leads to\nbetter predictions. We demonstrate our approach to emulate atmospheric\ndispersion, key for greenhouse gas emissions monitoring, by comparing a\nbaseline model to models trained using prototypes as an additional input. The\nprototype models achieve better performance, even with few prototypes and even\nif they are chosen at random, but we show that choosing the prototypes through\ndata-driven methods (k-means) can lead to almost 10\\% increased performance in\nsome metrics.", "published": "2025-04-24 12:34:23", "link": "http://arxiv.org/abs/2504.17492v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning", "abstract": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings.", "published": "2025-04-24 11:28:00", "link": "http://arxiv.org/abs/2504.17448v1", "categories": ["cs.LG", "cs.DB", "cs.DC"], "primary_category": "cs.LG"}
{"title": "HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time", "abstract": "Finding the initial depth-to-water table (DTWT) configuration of a catchment\nis a critical challenge when simulating the hydrological cycle with integrated\nmodels, significantly impacting simulation outcomes. Traditionally, this\ninvolves iterative spin-up computations, where the model runs under constant\natmospheric settings until steady-state is achieved. These so-called model\nspin-ups are computationally expensive, often requiring many years of simulated\ntime, particularly when the initial DTWT configuration is far from steady\nstate.\n  To accelerate the model spin-up process we developed HydroStartML, a machine\nlearning emulator trained on steady-state DTWT configurations across the\ncontiguous United States. HydroStartML predicts, based on available data like\nconductivity and surface slopes, a DTWT configuration of the respective\nwatershed, which can be used as an initial DTWT.\n  Our results show that initializing spin-up computations with HydroStartML\npredictions leads to faster convergence than with other initial configurations\nlike spatially constant DTWTs. The emulator accurately predicts configurations\nclose to steady state, even for terrain configurations not seen in training,\nand allows especially significant reductions in computational spin-up effort in\nregions with deep DTWTs. This work opens the door for hybrid approaches that\nblend machine learning and traditional simulation, enhancing predictive\naccuracy and efficiency in hydrology for improving water resource management\nand understanding complex environmental interactions.", "published": "2025-04-24 10:24:34", "link": "http://arxiv.org/abs/2504.17420v1", "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "physics.geo-ph"}
{"title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration", "abstract": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second.", "published": "2025-04-24 08:50:01", "link": "http://arxiv.org/abs/2504.17376v1", "categories": ["cs.AR", "cs.LG"], "primary_category": "cs.AR"}
{"title": "Doubly Adaptive Social Learning", "abstract": "In social learning, a network of agents assigns probability scores (beliefs)\nto some hypotheses of interest, which rule the generation of local streaming\ndata observed by each agent. Belief formation takes place by means of an\niterative two-step procedure where: i) the agents update locally their beliefs\nby using some likelihood model; and ii) the updated beliefs are combined with\nthe beliefs of the neighboring agents, using a pooling rule. This procedure can\nfail to perform well in the presence of dynamic drifts, leading the agents to\nincorrect decision making. Here, we focus on the fully online setting where\nboth the true hypothesis and the likelihood models can change over time. We\npropose the doubly adaptive social learning ($\\text{A}^2\\text{SL}$) strategy,\nwhich infuses social learning with the necessary adaptation capabilities. This\ngoal is achieved by exploiting two adaptation stages: i) a stochastic gradient\ndescent update to learn and track the drifts in the decision model; ii) and an\nadaptive belief update to track the true hypothesis changing over time. These\nstages are controlled by two adaptation parameters that govern the evolution of\nthe error probability for each agent. We show that all agents learn\nconsistently for sufficiently small adaptation parameters, in the sense that\nthey ultimately place all their belief mass on the true hypothesis. In\nparticular, the probability of choosing the wrong hypothesis converges to\nvalues on the order of the adaptation parameters. The theoretical analysis is\nillustrated both on synthetic data and by applying the $\\text{A}^2\\text{SL}$\nstrategy to a social learning problem in the online setting using real data.", "published": "2025-04-24 08:43:09", "link": "http://arxiv.org/abs/2504.17370v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space", "abstract": "We present Dargana, a fine-tuned variant of the EarthPT time-series\nfoundation model that achieves specialisation using <3% of its pre-training\ndata volume and 5% of its pre-training compute. Dargana is fine-tuned to\ngenerate regularly updated classification of tree canopy cover at 10m\nresolution, distinguishing conifer and broadleaved tree types. Using Cornwall,\nUK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a\nPR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine\nstructures like hedgerows and coppice below the training sample limit, and can\ntrack temporal changes to canopy cover such as new woodland establishment. Our\nresults demonstrate how pre-trained Large Observation Models like EarthPT can\nbe specialised for granular, dynamic land cover monitoring from space,\nproviding a valuable, scalable tool for natural capital management and\nconservation.", "published": "2025-04-24 07:23:27", "link": "http://arxiv.org/abs/2504.17321v1", "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "physics.geo-ph"}
{"title": "Machine learning-based condition monitoring of powertrains in modern electric drives", "abstract": "The recent technological advances in digitalization have revolutionized the\nindustrial sector. Leveraging data analytics has now enabled the collection of\ndeep insights into the performance and, as a result, the optimization of\nassets. Industrial drives, for example, already accumulate all the necessary\ninformation to control electric machines. These signals include but are not\nlimited to currents, frequency, and temperature. Integrating machine learning\n(ML) models responsible for predicting the evolution of those directly\ncollected or implicitly derived parameters enhances the smartness of industrial\nsystems even further. In this article, data already residing in most modern\nelectric drives has been used to develop a data-driven thermal model of a power\nmodule. A test bench has been designed and used specifically for training and\nvalidating the thermal digital twin undergoing various static and dynamic\noperating profiles. Different approaches, from traditional linear models to\ndeep neural networks, have been implemented to emanate the best ML model for\nestimating the case temperature of a power module. Several evaluation metrics\nwere then used to assess the investigated methods' performance and\nimplementation in industrial embedded systems.", "published": "2025-04-24 06:59:38", "link": "http://arxiv.org/abs/2504.17305v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes", "abstract": "Backdoor attacks on text classifiers can cause them to predict a predefined\nlabel when a particular \"trigger\" is present. Prior attacks often rely on\ntriggers that are ungrammatical or otherwise unusual, leading to conspicuous\nattacks. As a result, human annotators, who play a critical role in curating\ntraining data in practice, can easily detect and filter out these unnatural\ntexts during manual inspection, reducing the risk of such attacks. We argue\nthat a key criterion for a successful attack is for text with and without\ntriggers to be indistinguishable to humans. However, prior work neither\ndirectly nor comprehensively evaluated attack subtlety and invisibility with\nhuman involvement. We bridge the gap by conducting thorough human evaluations\nto assess attack subtlety. We also propose \\emph{AttrBkd}, consisting of three\nrecipes for crafting subtle yet effective trigger attributes, such as\nextracting fine-grained attributes from existing baseline backdoor attacks. Our\nhuman evaluations find that AttrBkd with these baseline-derived attributes is\noften more effective (higher attack success rate) and more subtle (fewer\ninstances detected by humans) than the original baseline backdoor attacks,\ndemonstrating that backdoor attacks can bypass detection by being inconspicuous\nand appearing natural even upon close inspection, while still remaining\neffective. Our human annotation also provides information not captured by\nautomated metrics used in prior work, and demonstrates the misalignment of\nthese metrics with human judgment.", "published": "2025-04-24 06:50:59", "link": "http://arxiv.org/abs/2504.17300v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks", "abstract": "Recent research has witnessed the remarkable progress of Graph Neural\nNetworks (GNNs) in the realm of graph data representation. However, GNNs still\nencounter the challenge of structural imbalance. Prior solutions to this\nproblem did not take graph heterophily into account, namely that connected\nnodes process distinct labels or features, thus resulting in a deficiency in\neffectiveness. Upon verifying the impact of heterophily on solving the\nstructural imbalance problem, we propose to rectify the heterophily first and\nthen transfer homophilic knowledge. To the end, we devise a method named HeRB\n(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two\ninnovative components: 1) A heterophily-lessening augmentation module which\nserves to reduce inter-class edges and increase intra-class edges; 2) A\nhomophilic knowledge transfer mechanism to convey homophilic information from\nhead nodes to tail nodes. Experimental results demonstrate that HeRB achieves\nsuperior performance on two homophilic and six heterophilic benchmark datasets,\nand the ablation studies further validate the efficacy of two proposed\ncomponents.", "published": "2025-04-24 06:04:59", "link": "http://arxiv.org/abs/2504.17276v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy", "abstract": "We consider the problem of recovering latent information from graphs under\n$\\varepsilon$-edge local differential privacy where the presence of\nrelationships/edges between two users/vertices remains confidential, even from\nthe data curator. For the class of generalized random dot-product graphs, we\nshow that a standard local differential privacy mechanism induces a specific\ngeometric distortion in the latent positions. Leveraging this insight, we show\nthat consistent recovery of the latent positions is achievable by appropriately\nadjusting the statistical inference procedure for the privatized graph.\nFurthermore, we prove that our procedure is nearly minimax-optimal under local\nedge differential privacy constraints. Lastly, we show that this framework\nallows for consistent recovery of geometric and topological information\nunderlying the latent positions, as encoded in their persistence diagrams. Our\nresults extend previous work from the private community detection literature to\na substantially richer class of models and inferential tasks.", "published": "2025-04-24 06:02:02", "link": "http://arxiv.org/abs/2504.17274v1", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68P27, 62H22, 62C20, 62R07"], "primary_category": "cs.LG"}
{"title": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification", "abstract": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems.", "published": "2025-04-24 03:57:27", "link": "http://arxiv.org/abs/2504.17232v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services", "abstract": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets.", "published": "2025-04-24 02:27:17", "link": "http://arxiv.org/abs/2504.17203v1", "categories": ["cs.DB", "cs.LG"], "primary_category": "cs.DB"}
{"title": "A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation", "abstract": "In intelligent transportation systems (ITS), traffic management departments\nrely on sensors, cameras, and GPS devices to collect real-time traffic data.\nTraffic speed data is often incomplete due to sensor failures, data\ntransmission delays, or occlusions, resulting in missing speed data in certain\nroad segments. Currently, tensor decomposition based methods are extensively\nutilized, they mostly rely on the $L_2$-norm to construct their learning\nobjectives, which leads to reduced robustness in the algorithms. To address\nthis, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which\ncombines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,\nthereby achieving both high accuracy and robust performance in imputing missing\ntime-varying traffic speed data. TATSI adopts a single latent factor-dependent,\nnonnegative, and multiplicative update (SLF-NMU) approach, which serves as an\nefficient solver for performing nonnegative latent factor analysis (LFA) on a\ntensor. Empirical studies on three real-world time-varying traffic speed\ndatasets demonstrate that, compared with state-of-the-art traffic speed\npredictors, TATSI more precisely captures temporal patterns, thereby yielding\nthe most accurate imputations for missing traffic speed data.", "published": "2025-04-24 02:00:41", "link": "http://arxiv.org/abs/2504.17196v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "abstract": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline.", "published": "2025-04-24 01:16:40", "link": "http://arxiv.org/abs/2504.17173v1", "categories": ["cs.HC", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Causal rule ensemble approach for multi-arm data", "abstract": "Heterogeneous treatment effect (HTE) estimation is critical in medical\nresearch. It provides insights into how treatment effects vary among\nindividuals, which can provide statistical evidence for precision medicine.\nWhile most existing methods focus on binary treatment situations, real-world\napplications often involve multiple interventions. However, current HTE\nestimation methods are primarily designed for binary comparisons and often rely\non black-box models, which limit their applicability and interpretability in\nmulti-arm settings. To address these challenges, we propose an interpretable\nmachine learning framework for HTE estimation in multi-arm trials. Our method\nemploys a rule-based ensemble approach consisting of rule generation, rule\nensemble, and HTE estimation, ensuring both predictive accuracy and\ninterpretability. Through extensive simulation studies and real data\napplications, the performance of our method was evaluated against\nstate-of-the-art multi-arm HTE estimation approaches. The results indicate that\nour approach achieved lower bias and higher estimation accuracy compared with\nthose of existing methods. Furthermore, the interpretability of our framework\nallows clearer insights into how covariates influence treatment effects,\nfacilitating clinical decision making. By bridging the gap between accuracy and\ninterpretability, our study contributes a valuable tool for multi-arm HTE\nestimation, supporting precision medicine.", "published": "2025-04-24 01:03:30", "link": "http://arxiv.org/abs/2504.17166v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Multi-Agent, Laxity-Based Aggregation Strategy for Cost-Effective Electric Vehicle Charging and Local Transformer Overload Prevention", "abstract": "The rapid electrification of transportation, driven by stringent\ndecarbonization targets and supportive policies, poses significant challenges\nfor distribution system operators (DSOs). When numerous electric vehicles (EVs)\ncharge concurrently, local transformers risk overloading - a problem that\ncurrent tariff-based strategies do not adequately address. This paper\nintroduces an aggregator-based coordination mechanism that shifts EV charging\nfrom congested to underutilized periods using a rule-based scheduling\nalgorithm. Unlike conventional methods that depend on complex real-time pricing\nsignals or optimization-heavy solutions, the aggregator approach uses a simple\nyet effective \"laxity\" measure to prioritize charging flexibility. To assess\ntechnical and economic viability, a multi-agent simulation was developed to\nreplicate residential user behavior and DSO constraints under the use of a 400\nkVA low-voltage transformer. The results indicate that overloads are completely\neliminated with minimal inconvenience to users, whose increased charging costs\nare offset by the aggregator at an annual total of under DKK 6000 -\nsignificantly lower than the cost of infrastructure reinforcement. This study\ncontributes by (i) quantifying the compensation needed to prevent large-scale\noverloads, (ii) presenting a replicable, computationally feasible, rule-based\naggregator model for DSOs, and (iii) comparing aggregator solutions to costly\ntransformer upgrades, underscoring the aggregator's role as a viable tool for\nfuture distribution systems.", "published": "2025-04-24 14:04:35", "link": "http://arxiv.org/abs/2504.17575v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "AGCo-MATA: Air-Ground Collaborative Multi-Agent Task Allocation in Mobile Crowdsensing", "abstract": "Rapid progress in intelligent unmanned systems has presented new\nopportunities for mobile crowd sensing (MCS). Today, heterogeneous air-ground\ncollaborative multi-agent framework, which comprise unmanned aerial vehicles\n(UAVs) and unmanned ground vehicles (UGVs), have presented superior flexibility\nand efficiency compared to traditional homogeneous frameworks in complex\nsensing tasks. Within this context, task allocation among different agents\nalways play an important role in improving overall MCS quality. In order to\nbetter allocate tasks among heterogeneous collaborative agents, in this paper,\nwe investigated two representative complex multi-agent task allocation\nscenarios with dual optimization objectives: (1) For AG-FAMT (Air-Ground Few\nAgents More Tasks) scenario, the objectives are to maximize the task completion\nwhile minimizing the total travel distance; (2) For AG-MAFT (Air-Ground More\nAgents Few Tasks) scenario, where the agents are allocated based on their\nlocations, has the optimization objectives of minimizing the total travel\ndistance while reducing travel time cost. To achieve this, we proposed a\nMulti-Task Minimum Cost Maximum Flow (MT-MCMF) optimization algorithm tailored\nfor AG-FAMT, along with a multi-objective optimization algorithm called W-ILP\ndesigned for AG-MAFT, with a particular focus on optimizing the charging path\nplanning of UAVs. Our experiments based on a large-scale real-world dataset\ndemonstrated that the proposed two algorithms both outperform baseline\napproaches under varying experimental settings, including task quantity, task\ndifficulty, and task distribution, providing a novel way to improve the overall\nquality of mobile crowdsensing tasks.", "published": "2025-04-24 10:01:54", "link": "http://arxiv.org/abs/2504.17409v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Fully-Mixed Virtual Element Method for the Biot Problem", "abstract": "Poroelasticity describes the interaction of deformation and fluid flow in\nsaturated porous media. A fully-mixed formulation of Biot's poroelasticity\nproblem has the advantage of producing a better approximation of the Darcy\nvelocity and stress field, as well as satisfying local mass and momentum\nconservation. In this work, we focus on a novel four-fields Virtual Element\ndiscretization of Biot's equations. The stress symmetry is strongly imposed in\nthe definition of the discrete space, thus avoiding the use of an additional\nLagrange multiplier. A complete a priori analysis is performed, showing the\nrobustness of the proposed numerical method with respect to limiting material\nproperties. The first order convergence of the lowest-order fully-discrete\nnumerical method, which is obtained by coupling the spatial approximation with\nthe backward Euler time-advancing scheme, is confirmed by a complete 3D\nnumerical validation. A well known poroelasticity benchmark is also considered\nto assess the robustness properties and computational performance.", "published": "2025-04-24 16:42:50", "link": "http://arxiv.org/abs/2504.17729v1", "categories": ["math.NA", "cs.NA", "65M12, 65M60, 74F10, 76S05"], "primary_category": "math.NA"}
{"title": "On Josephy-Halley method for generalized equations", "abstract": "We extend the classical third-order Halley iteration to the setting of\ngeneralized equations of the form \\[ 0 \\in f(x) + F(x), \\] where \\(f\\colon\nX\\longrightarrow Y\\) is twice continuously Fr\\'echet-differentiable on Banach\nspaces and \\(F\\colon X\\tto Y\\) is a set-valued mapping with closed graph.\nBuilding on predictor-corrector framework, our scheme first solves a partially\nlinearized inclusion to produce a predictor \\(u_{k+1}\\), then incorporates\nsecond-order information in a Halley-type corrector step to obtain \\(x_{k+1}\\).\nUnder metric regularity of the linearization at a reference solution and\nH\\\"older continuity of \\(f''\\), we prove that the iterates converge locally\nwith order \\(2+p\\) (cubically when \\(p=1\\)). Moreover, by constructing a\nsuitable scalar majorant function we derive semilocal Kantorovich-type\nconditions guaranteeing well-definedness and R-cubic convergence from an\nexplicit neighbourhood of the initial guess. Numerical experiments-including\none- and two-dimensional test problems confirm the theoretical convergence\nrates and illustrate the efficiency of the Josephy-Halley method compared to\nits Josephy-Newton counterpart.", "published": "2025-04-24 15:18:59", "link": "http://arxiv.org/abs/2504.17649v1", "categories": ["math.NA", "cs.NA", "math.OC", "49J53, 65K15, 90C33"], "primary_category": "math.NA"}
{"title": "Rescaling and unconstrained minimisation of convex quadratic maps", "abstract": "We investigate the properties of a class of piecewise-fractional maps arising\nfrom the introduction of an invariance under rescaling into convex quadratic\nmaps. The subsequent maps are quasiconvex, and pseudoconvex on specific convex\ncones; they can be optimised via exact line search along admissible directions,\nand the iterates then inherit a bidimensional optimality property. We study the\nminimisation of such relaxed maps via coordinate descents with gradient-based\nrules, placing a special emphasis on coordinate directions verifying a\nmaximum-alignment property in the reproducing kernel Hilbert spaces related to\nthe underlying positive-semidefinite matrices. In this setting, we illustrate\nthat accounting for the optimal rescaling of the iterates can in certain\nsituations substantially accelerate the unconstrained minimisation of convex\nquadratic maps.", "published": "2025-04-24 14:22:38", "link": "http://arxiv.org/abs/2504.17596v1", "categories": ["math.OC", "cs.NA", "math.NA", "65F10 (Primary) 90C20, 26B25 (Secondary)"], "primary_category": "math.OC"}
{"title": "On Runge-Kutta methods of order 10", "abstract": "A family of explicit 15-stage Runge-Kutta methods of order 10 is derived.", "published": "2025-04-24 07:45:51", "link": "http://arxiv.org/abs/2504.17329v1", "categories": ["math.NA", "cs.NA", "65L05, 65L06"], "primary_category": "math.NA"}
{"title": "An Adaptive Finite Element DtN Method for the Acoustic-Elastic Interaction Problem in Periodic Structures", "abstract": "Consider a time-harmonic acoustic plane wave incident onto an elastic body\nwith an unbounded periodic surface. The medium above the surface is supposed to\nbe filled with a homogeneous compressible inviscid air/fluid of constant mass\ndensity, while the elastic body is assumed to be isotropic and linear. By\nintroducing the Dirichlet-to-Neumann (DtN) operators for acoustic and elastic\nwaves simultaneously, the model is formulated as an acoustic-elastic\ninteraction problem in periodic structures. Based on a duality argument, an a\nposteriori error estimate is derived for the associated truncated finite\nelement approximation. The a posteriori error estimate consists of the finite\nelement approximation error and the truncation error of two different DtN\noperators, where the latter decays exponentially with respect to the truncation\nparameter. Based on the a posteriori error, an adaptive finite element\nalgorithm is proposed for solving the acoustic-elastic interaction problem in\nperiodic structures. Numerical experiments are presented to demonstrate the\neffectiveness of the proposed algorithm.", "published": "2025-04-24 04:01:09", "link": "http://arxiv.org/abs/2504.17233v1", "categories": ["math.NA", "cs.NA", "65N12, 65N15, 65N30"], "primary_category": "math.NA"}
{"title": "Tokenizing Stock Prices for Enhanced Multi-Step Forecast and Prediction", "abstract": "Effective stock price forecasting (estimating future prices) and prediction\n(estimating future price changes) are pivotal for investors, regulatory\nagencies, and policymakers. These tasks enable informed decision-making, risk\nmanagement, strategic planning, and superior portfolio returns. Despite their\nimportance, forecasting and prediction are challenging due to the dynamic\nnature of stock price data, which exhibit significant temporal variations in\ndistribution and statistical properties. Additionally, while both forecasting\nand prediction targets are derived from the same dataset, their statistical\ncharacteristics differ significantly. Forecasting targets typically follow a\nlog-normal distribution, characterized by significant shifts in mean and\nvariance over time, whereas prediction targets adhere to a normal distribution.\nFurthermore, although multi-step forecasting and prediction offer a broader\nperspective and richer information compared to single-step approaches, it is\nmuch more challenging due to factors such as cumulative errors and long-term\ntemporal variance. As a result, many previous works have tackled either\nsingle-step stock price forecasting or prediction instead. To address these\nissues, we introduce a novel model, termed Patched Channel Integration Encoder\n(PCIE), to tackle both stock price forecasting and prediction. In this model,\nwe utilize multiple stock channels that cover both historical prices and price\nchanges, and design a novel tokenization method to effectively embed these\nchannels in a cross-channel and temporally efficient manner. Specifically, the\ntokenization process involves univariate patching and temporal learning with a\nchannel-mixing encoder to reduce cumulative errors. Comprehensive experiments\nvalidate that PCIE outperforms current state-of-the-art models in forecast and\nprediction tasks.", "published": "2025-04-24 07:15:05", "link": "http://arxiv.org/abs/2504.17313v1", "categories": ["cs.CE", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "Generating Localized Audible Zones Using a Single-Channel Parametric Loudspeaker", "abstract": "Advanced sound zone control (SZC) techniques typically rely on massive\nmulti-channel loudspeaker arrays to create high-contrast personal sound zones,\nmaking single-loudspeaker SZC seem impossible. In this Letter, we challenge\nthis paradigm by introducing the multi-carrier parametric loudspeaker (MCPL),\nwhich enables SZC using only a single loudspeaker. In our approach, distinct\naudio signals are modulated onto separate ultrasonic carrier waves at different\nfrequencies and combined into a single composite signal. This signal is emitted\nby a single-channel ultrasonic transducer, and through nonlinear demodulation\nin air, the audio signals interact to virtually form multi-channel outputs.\nThis novel capability allows the application of existing SZC algorithms\noriginally designed for multi-channel loudspeaker arrays. Simulations validate\nthe effectiveness of our proposed single-channel MCPL, demonstrating its\npotential as a promising alternative to traditional multi-loudspeaker systems\nfor achieving high-contrast SZC. Our work opens new avenues for simplifying SZC\nsystems without compromising performance.", "published": "2025-04-24 11:02:07", "link": "http://arxiv.org/abs/2504.17440v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Unsupervised EEG-based decoding of absolute auditory attention with canonical correlation analysis", "abstract": "We propose a fully unsupervised algorithm that detects from encephalography\n(EEG) recordings when a subject actively listens to sound, versus when the\nsound is ignored. This problem is known as absolute auditory attention decoding\n(aAAD). We propose an unsupervised discriminative CCA model for feature\nextraction and combine it with an unsupervised classifier called minimally\ninformed linear discriminant analysis (MILDA) for aAAD classification.\nRemarkably, the proposed unsupervised algorithm performs significantly better\nthan a state-of-the-art supervised model. A key reason is that the unsupervised\nalgorithm can successfully adapt to the non-stationary test data at a low\ncomputational cost. This opens the door to the analysis of the auditory\nattention of a subject using EEG signals with a model that automatically tunes\nitself to the subject without requiring an arduous supervised training session\nbeforehand.", "published": "2025-04-24 16:36:56", "link": "http://arxiv.org/abs/2504.17724v1", "categories": ["eess.SP", "cs.SD"], "primary_category": "eess.SP"}
{"title": "Semi-Blind Strategies for MMSE Channel Estimation Utilizing Generative Priors", "abstract": "This paper investigates semi-blind channel estimation for massive\nmultiple-input multiple-output (MIMO) systems. To this end, we first estimate a\nsubspace based on all received symbols (pilot and payload) to provide\nadditional information for subsequent channel estimation. We show how this\nadditional information enhances minimum mean square error (MMSE) channel\nestimation. Two variants of the linear MMSE (LMMSE) estimator are formulated,\nwhere the first one solves the estimation within the subspace, and the second\none uses a subspace projection as a preprocessing step. Theoretical derivations\nshow the superior estimation performance of the latter method in terms of mean\nsquare error for uncorrelated Rayleigh fading. Subsequently, we introduce\nparameterizations of this semi-blind LMMSE estimator based on two different\nconditional Gaussian latent models, i.e., the Gaussian mixture model and the\nvariational autoencoder. Both models learn the underlying channel distribution\nof the propagation environment based on training data and serve as generative\npriors for semi-blind channel estimation. Extensive simulations for real-world\nmeasurement data and spatial channel models show the superior performance of\nthe proposed methods compared to state-of-the-art semi-blind channel estimators\nwith respect to the MSE.", "published": "2025-04-24 14:01:27", "link": "http://arxiv.org/abs/2504.17573v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Comparative Analysis of Hybrid Precoding Optimization Approaches for Millimeter Wave Massive MIMO System", "abstract": "The conventional digital beamforming technique needs one radio frequency (RF)\nchain per antenna element. High power consumption, significantly high cost of\nRF chain components per antenna and complex signal processing task at base band\nmakes digital beamforming unsuitable for implementation in massive MIMO system.\nHybrid beamforming takes the benefits of both analog and digital beamforming\nschemes. Near optimal performance can be achieved using very few RF chains in\nhybrid beamforming method. By optimizing the hybrid transmit and receive\nbeamformers, the spectral efficiency of a system can be maximized. The hybrid\nbeamforming optimization problem is non-convex optimization problem due to the\nnon-convexity of the constraints. In this research work, the millimeter wave\nmassive MIMO system performance implementing the hybrid precoding based on the\nconventional methods and deep neural network is studied considering the\nspectral efficiency, bit error rate and complexity. It is shown that the deep\nneural network based approach for hybrid precoding optimization outperforms\nconventional techniques by solving the non-convex optimization problem.\nMoreover, the DNN model accuracy is also analyzed by observing the training\naccuracy and test accuracy.", "published": "2025-04-24 13:03:01", "link": "http://arxiv.org/abs/2504.17521v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On Geometric Shaping for 400 Gbps IM-DD Links with Laser Intensity Noise", "abstract": "We propose geometric shaping for IM-DD links dominated by relative intensity\nnoise (RIN). For 400 Gbps links, our geometrically-shaped constellations result\nin error probability improvements that relaxes the RIN laser design by 3 dB.", "published": "2025-04-24 10:52:13", "link": "http://arxiv.org/abs/2504.17433v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RIS-Assisted Noncoherent Wireless System: Error Analysis with Optimal Receiver and Multi-level ASK", "abstract": "This paper considers a reconfigurable intelligent surface (RIS) aided\nwireless communication system where the transmitter employs one-sided\namplitude-shift keying (ASK) for data modulation and the receiver employs an\noptimal noncoherent maximum-likelihood rule for symbol detection. A novel\nstatistical analysis is presented to approximate the weighted sum of a central\nand a non-central chi-squared random variable, which is used to derive a novel\nclosed-form expression for the symbol error probability (SEP) of the\nnoncoherent system. Furthermore, an optimization problem to minimize the\nsystem's SEP under transmission energy constraint is proposed, and novel\nalgorithms are presented to obtain the optimal ASK constellation minimizing the\nSEP. Numerical results indicate that the noncoherent system achieves superior\nerror performance and higher diversity order with the optimal ASK constellation\ncompared to the traditional equispaced ASK constellation. Further, the optimal\nASK significantly differs from the traditional one, with the difference\nbecoming significant with an increase in the average signal-to-noise ratio and\nat a lower number of RIS's reflecting elements.", "published": "2025-04-24 10:25:19", "link": "http://arxiv.org/abs/2504.17423v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Introducing Combined Effects of Filtering and ASE Noise in Optical Links Supposing Different Equalization Algorithms", "abstract": "This paper presents a comprehensive analytical framework for evaluating\nfiltering penalties in ASE-noise-limited coherent optical links. The model\naccounts for the cumulative effects of cascaded optical filters,\namplifier-induced ASE noise, and transceiver noise, alongside digital\nequalization at the receiver. By developing a generalized channel\nrepresentation, we derive closed-form expressions for signal-to-noise ratio\ndegradation under various equalization strategies, including Zero-Forcing\nEqualizer, Minimum Mean Square Error Equalizer, and Fractionally Spaced\nEqualizer. These models capture the impact of colored noise resulting from\nlinear filtering and provide both time and frequency-domain insights. The\nproposed framework is validated through experimental comparisons using\naccurately modeled optical filters, demonstrating close agreement between\ntheory and practice and offering a robust foundation for system-level\nperformance evaluation in metro-access networks.", "published": "2025-04-24 10:01:02", "link": "http://arxiv.org/abs/2504.17408v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "The Riemannian Means Field Classifier for EEG-Based BCI Data", "abstract": "A substantial amount of research has demonstrated the robustness and accuracy\nof the Riemannian minimum distance to mean (MDM) classifier for all kinds of\nEEG-based brain--computer interfaces (BCIs). This classifier is simple, fully\ndeterministic, robust to noise, computationally efficient, and prone to\ntransfer learning. Its training is very simple, requiring just the computation\nof a geometric mean of a symmetric positive-definite (SPD) matrix per class. We\npropose an improvement of the MDM involving a number of power means of SPD\nmatrices instead of the sole geometric mean. By the analysis of 20 public\ndatabases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI\nparadigm, comprising 587 individuals in total, we show that the proposed\nclassifier clearly outperforms the MDM, approaching the state-of-the art in\nterms of performance while retaining the simplicity and the deterministic\nbehavior. In order to promote reproducible research, our code will be released\nas open source.", "published": "2025-04-24 08:13:56", "link": "http://arxiv.org/abs/2504.17352v1", "categories": ["cs.HC", "eess.SP"], "primary_category": "cs.HC"}
{"title": "CKMDiff: A Generative Diffusion Model for CKM Construction via Inverse Problems with Learned Priors", "abstract": "Channel knowledge map (CKM) is a promising technology to enable\nenvironment-aware wireless communications and sensing with greatly enhanced\nperformance, by offering location-specific channel prior information for future\nwireless networks. One fundamental problem for CKM-enabled wireless systems\nlies in how to construct high-quality and complete CKM for all locations of\ninterest, based on only limited and noisy on-site channel knowledge data. This\nproblem resembles the long-standing ill-posed inverse problem, which tries to\ninfer from a set of limited and noisy observations the cause factors that\nproduced them. By utilizing the recent advances of solving inverse problems\nwith learned priors using generative artificial intelligence (AI), we propose\nCKMDiff, a conditional diffusion model that can be applied to perform various\ntasks for CKM constructions such as denoising, inpainting, and\nsuper-resolution, without having to know the physical environment maps or\ntransceiver locations. Furthermore, we propose an environment-aware data\naugmentation mechanism to enhance the model's ability to learn implicit\nrelations between electromagnetic propagation patterns and spatial-geometric\nfeatures. Extensive numerical results are provided based on the CKMImageNet and\nRadioMapSeer datasets, which demonstrate that the proposed CKMDiff achieves\nstate-of-the-art performance, outperforming various benchmark methods.", "published": "2025-04-24 07:26:18", "link": "http://arxiv.org/abs/2504.17323v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DualAttWaveNet: Multiscale Attention Networks for Satellite Interference Detection", "abstract": "The escalating overlap between non-geostationary orbit (NGSO) and\ngeostationary orbit (GSO) satellite frequency allocations necessitates accurate\ninterference detection methods that address two pivotal technical gaps:\ncomputationally efficient signal analysis for real-time operation, and robust\nanomaly discrimination under varying interference patterns. Existing deep\nlearning approaches employ encoder-decoder anomaly detectors that threshold\ninput-output discrepancies for robustness. While the transformer-based TrID\nmodel achieves state-of-the-art performance (AUC: 0.8318, F1: 0.8321), its\nmulti-head attention incurs prohibitive computation time, and its decoupled\ntraining of time-frequency models overlooks cross-domain dependencies. To\novercome these problems, we propose DualAttWaveNet. A bidirectional attention\nfusion layer dynamically correlates time-domain samples using\nparameter-efficient cross-attention routing. A wavelet-regularized\nreconstruction loss enforces multi-scale consistency. We train the model on\npublic dataset which consists of 48 hours of satellite signals. Experiments\nshow that compared to TrID, DualAttWaveNet improves AUC by 12% and reduces\ninference time by 50% to 540ms per batch while maintaining F1-score.", "published": "2025-04-24 01:47:00", "link": "http://arxiv.org/abs/2504.17187v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Automotive Radar Multi-Frame Track-Before-Detect Algorithm Considering Self-Positioning Errors", "abstract": "This paper presents a method for the joint detection and tracking of weak\ntargets in automotive radars using the multi-frame track-before-detect (MF-TBD)\nprocedure. Generally, target tracking in automotive radars is challenging due\nto radar field of view (FOV) misalignment, nonlinear coordinate conversion, and\nself-positioning errors of the ego-vehicle, which are caused by platform\nmotion. These issues significantly hinder the implementation of MF-TBD in\nautomotive radars. To address these challenges, a new MF-TBD detection\narchitecture is first proposed. It can adaptively adjust the detection\nthreshold value based on the existence of moving targets within the radar FOV.\nSince the implementation of MF-TBD necessitates the inclusion of position,\nvelocity, and yaw angle information of the ego-vehicle, each with varying\ndegrees of measurement error, we further propose a multi-frame energy\nintegration strategy for moving-platform radar and accurately derive the target\nenergy integration path functions. The self-positioning errors of the\nego-vehicle, which are usually not considered in some previous target tracking\napproaches, are well addressed. Numerical simulations and experimental results\nwith real radar data demonstrate large detection and tracking gains over\nstandard automotive radar processing in weak target environments.", "published": "2025-04-24 00:27:25", "link": "http://arxiv.org/abs/2504.17155v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "abstract": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "published": "2025-04-24 15:39:46", "link": "http://arxiv.org/abs/2504.17671v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "abstract": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M", "published": "2025-04-24 13:57:53", "link": "http://arxiv.org/abs/2504.17565v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "abstract": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.", "published": "2025-04-24 08:27:42", "link": "http://arxiv.org/abs/2504.17365v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "abstract": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/", "published": "2025-04-24 16:04:00", "link": "http://arxiv.org/abs/2504.17696v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "abstract": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.", "published": "2025-04-24 15:11:41", "link": "http://arxiv.org/abs/2504.17641v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the Self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.", "published": "2025-04-24 09:53:49", "link": "http://arxiv.org/abs/2504.17404v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "abstract": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.", "published": "2025-04-24 09:08:24", "link": "http://arxiv.org/abs/2504.17384v2", "categories": ["physics.geo-ph", "cs.AI"], "primary_category": "physics.geo-ph"}
{"title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "abstract": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.", "published": "2025-04-24 04:41:35", "link": "http://arxiv.org/abs/2504.17243v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "abstract": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce NeuS-E, a novel zero-training video refinement\npipeline that leverages neuro-symbolic feedback to automatically enhance video\ngeneration, achieving superior alignment with the prompts. Our approach first\nderives the neuro-symbolic feedback by analyzing a formal video representation\nand pinpoints semantically inconsistent events, objects, and their\ncorresponding frames. This feedback then guides targeted edits to the original\nvideo. Extensive empirical evaluations on both open-source and proprietary T2V\nmodels demonstrate that NeuS-E significantly enhances temporal and logical\nalignment across diverse prompts by almost 40%", "published": "2025-04-24 01:34:12", "link": "http://arxiv.org/abs/2504.17180v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology", "abstract": "Multiple instance learning (MIL) is a promising approach for weakly\nsupervised classification in pathology using whole slide images (WSIs).\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\nInstance Learning (ABMIL) typically disregard spatial interactions among\npatches that are crucial to pathological diagnosis. Recent advancements, such\nas Transformer based MIL (TransMIL), have incorporated spatial context and\ninter-patch relationships. However, it remains unclear whether explicitly\nmodeling patch relationships yields similar performance gains in ABMIL, which\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\nTransformer-based layers, introducing a fundamental architectural shift at the\ncost of substantially increased computational complexity. In this work, we\nenhance the ABMIL framework by integrating interaction-aware representations to\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\ncaptures inter-instance dependencies while preserving computational efficiency.\nExperimental results on two publicly available datasets for tumor subtyping in\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\nover ABMIL, with minimal or no additional computational overhead. These\nfindings underscore the importance of incorporating patch interactions within\nMIL frameworks. Our code is available at\n\\href{https://github.com/tueimage/GABMIL}{\\texttt{GABMIL}}.", "published": "2025-04-24 08:53:46", "link": "http://arxiv.org/abs/2504.17379v2", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset", "abstract": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\nhttps://app.deepscenario.com, facilitating research in motion prediction,\nbehavior modeling, and safety validation.", "published": "2025-04-24 08:43:48", "link": "http://arxiv.org/abs/2504.17371v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Class-Conditional Distribution Balancing for Group Robust Classification", "abstract": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision.", "published": "2025-04-24 07:15:53", "link": "http://arxiv.org/abs/2504.17314v2", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction", "abstract": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN.", "published": "2025-04-24 16:08:52", "link": "http://arxiv.org/abs/2504.17699v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "abstract": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages.", "published": "2025-04-24 23:00:46", "link": "http://arxiv.org/abs/2504.17974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning", "abstract": "Collaboration is ubiquitous and essential in day-to-day life -- from\nexchanging ideas, to delegating tasks, to generating plans together. This work\nstudies how LLMs can adaptively collaborate to perform complex embodied\nreasoning tasks. To this end we introduce MINDcraft, an easily extensible\nplatform built to enable LLM agents to control characters in the open-world\ngame of Minecraft; and MineCollab, a benchmark to test the different dimensions\nof embodied and collaborative reasoning. An experimental study finds that the\nprimary bottleneck in collaborating effectively for current state-of-the-art\nagents is efficient natural language communication, with agent performance\ndropping as much as 15% when they are required to communicate detailed task\ncompletion plans. We conclude that existing LLM agents are ill-optimized for\nmulti-agent collaboration, especially in embodied scenarios, and highlight the\nneed to employ methods beyond in-context and imitation learning. Our website\ncan be found here: https://mindcraft-minecollab.github.io/", "published": "2025-04-24 21:28:16", "link": "http://arxiv.org/abs/2504.17950v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "abstract": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.", "published": "2025-04-24 20:51:20", "link": "http://arxiv.org/abs/2504.17934v1", "categories": ["cs.HC", "cs.CL", "cs.CR"], "primary_category": "cs.HC"}
{"title": "CAMU: Context Augmentation for Meme Understanding", "abstract": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task.", "published": "2025-04-24 19:27:55", "link": "http://arxiv.org/abs/2504.17902v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Token Sequence Compression for Efficient Multimodal Computing", "abstract": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.", "published": "2025-04-24 19:11:10", "link": "http://arxiv.org/abs/2504.17892v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval", "abstract": "This paper concerns corpus poisoning attacks in dense information retrieval,\nwhere an adversary attempts to compromise the ranking performance of a search\nalgorithm by injecting a small number of maliciously generated documents into\nthe corpus. Our work addresses two limitations in the current literature.\nFirst, attacks that perform adversarial gradient-based word substitution search\ndo so in the discrete lexical space, while retrieval itself happens in the\ncontinuous embedding space. We thus propose an optimization method that\noperates in the embedding space directly. Specifically, we train a perturbation\nmodel with the objective of maintaining the geometric distance between the\noriginal and adversarial document embeddings, while also maximizing the\ntoken-level dissimilarity between the original and adversarial documents.\nSecond, it is common for related work to have a strong assumption that the\nadversary has prior knowledge about the queries. In this paper, we focus on a\nmore challenging variant of the problem where the adversary assumes no prior\nknowledge about the query distribution (hence, unsupervised). Our core\ncontribution is an adversarial corpus attack that is fast and effective. We\npresent comprehensive experimental results on both in- and out-of-domain\ndatasets, focusing on two related tasks: a top-1 attack and a corpus poisoning\nattack. We consider attacks under both a white-box and a black-box setting.\nNotably, our method can generate successful adversarial examples in under two\nminutes per target document; four times faster compared to the fastest\ngradient-based word substitution methods in the literature with the same\nhardware. Furthermore, our adversarial generation method generates text that is\nmore likely to occur under the distribution of natural text (low perplexity),\nand is therefore more difficult to detect.", "published": "2025-04-24 18:46:11", "link": "http://arxiv.org/abs/2504.17884v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection", "abstract": "Spoilers in movie reviews are important on platforms like IMDb and Rotten\nTomatoes, offering benefits and drawbacks. They can guide some viewers' choices\nbut also affect those who prefer no plot details in advance, making effective\nspoiler detection essential. Existing spoiler detection methods mainly analyze\nreview text, often overlooking the impact of movie genres and user bias,\nlimiting their effectiveness. To address this, we analyze movie review data,\nfinding genre-specific variations in spoiler rates and identifying that certain\nusers are more likely to post spoilers. Based on these findings, we introduce a\nnew spoiler detection framework called GUSD (The code is available at\nhttps://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler\nDetection), which incorporates genre-specific data and user behavior bias. User\nbias is calculated through dynamic graph modeling of review history.\nAdditionally, the R2GFormer module combines RetGAT (Retentive Graph Attention\nNetwork) for graph information and GenreFormer for genre-specific aggregation.\nThe GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to\nspecialized experts based on genre. Extensive testing on benchmark datasets\nshows that GUSD achieves state-of-the-art results. This approach advances\nspoiler detection by addressing genre and user-specific patterns, enhancing\nuser experience on movie review platforms.", "published": "2025-04-24 15:34:35", "link": "http://arxiv.org/abs/2504.17834v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm", "abstract": "AI-driven semi-autonomous robotic surgery is essential for addressing the\nmedical challenges of long-duration interplanetary missions, where limited crew\nsizes and communication delays restrict traditional surgical approaches.\nCurrent robotic surgery systems require full surgeon control, demanding\nextensive expertise and limiting feasibility in space. We propose a novel\nadaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle\navoidance and collaborative control in a two-degree-of-freedom robotic arm\nmodeled on the Miniaturized Robotic-Assisted surgical system. It was found that\nthe Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent\nimprovement to path search time and 43 percent improvement to path cost.", "published": "2025-04-24 23:19:27", "link": "http://arxiv.org/abs/2504.17979v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.", "published": "2025-04-24 22:27:50", "link": "http://arxiv.org/abs/2504.17967v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content", "abstract": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.", "published": "2025-04-24 22:24:14", "link": "http://arxiv.org/abs/2504.17964v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing", "abstract": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.", "published": "2025-04-24 20:40:29", "link": "http://arxiv.org/abs/2504.17929v1", "categories": ["cs.AI", "cs.AR"], "primary_category": "cs.AI"}
{"title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "abstract": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.", "published": "2025-04-24 20:24:31", "link": "http://arxiv.org/abs/2504.17921v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies", "abstract": "Task and motion planning is a well-established approach for solving\nlong-horizon robot planning problems. However, traditional methods assume that\neach task-level robot action, or skill, can be reduced to kinematic motion\nplanning. In this work, we address the challenge of planning with both\nkinematic skills and closed-loop motor controllers that go beyond kinematic\nconsiderations. We propose a novel method that integrates these controllers\ninto motion planning using Composable Interaction Primitives (CIPs), enabling\nthe use of diverse, non-composable pre-learned skills in hierarchical robot\nplanning. Toward validating our Task and Skill Planning (TASP) approach, we\ndescribe ongoing robot experiments in real-world scenarios designed to\ndemonstrate how CIPs can allow a mobile manipulator robot to effectively\ncombine motion planning with general-purpose skills to accomplish complex\ntasks.", "published": "2025-04-24 19:22:50", "link": "http://arxiv.org/abs/2504.17901v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm", "abstract": "In the looming post-quantum era, traditional cryptographic systems are\nincreasingly vulnerable to quantum computing attacks that can compromise their\nmathematical foundations. To address this critical challenge, we propose\ncrypto-ncRNA-a bio-convergent cryptographic framework that leverages the\ndynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,\nquantum-resistant keys and produce unpredictable ciphertexts. The framework\nemploys a novel, multi-stage process: encoding plaintext into RNA sequences,\npredicting and manipulating RNA secondary structures using advanced algorithms,\nand deriving cryptographic keys through the intrinsic physical unclonability of\nRNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's\nencryption speed is marginally lower than that of AES, it significantly\noutperforms RSA in terms of efficiency and scalability while achieving a 100%\npass rate on the NIST SP 800-22 randomness tests. These results demonstrate\nthat crypto-ncRNA offers a promising and robust approach for securing digital\ninfrastructures against the evolving threats posed by quantum computing.", "published": "2025-04-24 18:30:35", "link": "http://arxiv.org/abs/2504.17878v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Flow Matching Ergodic Coverage", "abstract": "Ergodic coverage effectively generates exploratory behaviors for embodied\nagents by aligning the spatial distribution of the agent's trajectory with a\ntarget distribution, where the difference between these two distributions is\nmeasured by the ergodic metric. However, existing ergodic coverage methods are\nconstrained by the limited set of ergodic metrics available for control\nsynthesis, fundamentally limiting their performance. In this work, we propose\nan alternative approach to ergodic coverage based on flow matching, a technique\nwidely used in generative inference for efficient and scalable sampling. We\nformally derive the flow matching problem for ergodic coverage and show that it\nis equivalent to a linear quadratic regulator problem with a closed-form\nsolution. Our formulation enables alternative ergodic metrics from generative\ninference that overcome the limitations of existing ones. These metrics were\npreviously infeasible for control synthesis but can now be supported with no\ncomputational overhead. Specifically, flow matching with the Stein variational\ngradient flow enables control synthesis directly over the score function of the\ntarget distribution, improving robustness to the unnormalized distributions; on\nthe other hand, flow matching with the Sinkhorn divergence flow enables an\noptimal transport-based ergodic metric, improving coverage performance on\nnon-smooth distributions with irregular supports. We validate the improved\nperformance and competitive computational efficiency of our method through\ncomprehensive numerical benchmarks and across different nonlinear dynamics. We\nfurther demonstrate the practicality of our method through a series of drawing\nand erasing tasks on a Franka robot.", "published": "2025-04-24 18:18:35", "link": "http://arxiv.org/abs/2504.17872v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "CaRL: Learning Scalable Planning Policies with Simple Rewards", "abstract": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.", "published": "2025-04-24 17:56:01", "link": "http://arxiv.org/abs/2504.17838v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI", "abstract": "Large Language Models (LLMs) are transforming geospatial artificial\nintelligence (GeoAI), offering new capabilities in data processing, spatial\nanalysis, and decision support. This paper examines the open-source paradigm's\npivotal role in this transformation. While proprietary LLMs offer\naccessibility, they often limit the customization, interoperability, and\ntransparency vital for specialized geospatial tasks. Conversely, open-source\nalternatives significantly advance Geographic Information Science (GIScience)\nby fostering greater adaptability, reproducibility, and community-driven\ninnovation. Open frameworks empower researchers to tailor solutions, integrate\ncutting-edge methodologies (e.g., reinforcement learning, advanced spatial\nindexing), and align with FAIR principles. However, the growing reliance on any\nLLM necessitates careful consideration of security vulnerabilities, ethical\nrisks, and robust governance for AI-generated geospatial outputs. Ongoing\ndebates on accessibility, regulation, and misuse underscore the critical need\nfor responsible AI development strategies. This paper argues that GIScience\nadvances best not through a single model type, but by cultivating a diverse,\ninteroperable ecosystem combining open-source foundations for innovation,\nbespoke geospatial models, and interdisciplinary collaboration. By critically\nevaluating the opportunities and challenges of open-source LLMs within the\nbroader GeoAI landscape, this work contributes to a nuanced discourse on\nleveraging AI to effectively advance spatial research, policy, and\ndecision-making in an equitable, sustainable, and scientifically rigorous\nmanner.", "published": "2025-04-24 13:20:17", "link": "http://arxiv.org/abs/2504.17833v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "abstract": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS.", "published": "2025-04-24 21:56:53", "link": "http://arxiv.org/abs/2504.17954v1", "categories": ["cs.GR", "cs.CV", "cs.LG"], "primary_category": "cs.GR"}
{"title": "Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data", "abstract": "Accurate myocardial image registration is essential for cardiac strain\nanalysis and disease diagnosis. However, spectral bias in neural networks\nimpedes modeling high-frequency deformations, producing inaccurate,\nbiomechanically implausible results, particularly in pathological data. This\npaper addresses spectral bias in physics-informed neural networks (PINNs) by\nintegrating Fourier Feature mappings and introducing modulation strategies into\na PINN framework. Experiments on two distinct datasets demonstrate that the\nproposed methods enhance the PINN's ability to capture complex, high-frequency\ndeformations in cardiomyopathies, achieving superior registration accuracy\nwhile maintaining biomechanical plausibility - thus providing a foundation for\nscalable cardiac image registration and generalization across multiple patients\nand pathologies.", "published": "2025-04-24 21:18:11", "link": "http://arxiv.org/abs/2504.17945v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis", "abstract": "Monitoring calf body weight (BW) before weaning is essential for assessing\ngrowth, feed efficiency, health, and weaning readiness. However, labor, time,\nand facility constraints limit BW collection. Additionally, Holstein calf coat\npatterns complicate image-based BW estimation, and few studies have explored\nnon-contact measurements taken at early time points for predicting later BW.\nThe objectives of this study were to (1) develop deep learning-based\nsegmentation models for extracting calf body metrics, (2) compare deep learning\nsegmentation with threshold-based methods, and (3) evaluate BW prediction using\nsingle-time-point cross-validation with linear regression (LR) and extreme\ngradient boosting (XGBoost) and multiple-time-point cross-validation with LR,\nXGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63)\nand Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves\nbeing weighed manually. Results showed that You Only Look Once version 8\n(YOLOv8) deep learning segmentation (intersection over union = 0.98)\noutperformed threshold-based methods (0.89). In single-time-point\ncross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean\nabsolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate\nlongitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight\nthe potential of deep learning for automated BW prediction, enhancing farm\nmanagement.", "published": "2025-04-24 21:08:31", "link": "http://arxiv.org/abs/2504.17943v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Masked strategies for images with small objects", "abstract": "The hematology analytics used for detection and classification of small blood\ncomponents is a significant challenge. In particular, when objects exists as\nsmall pixel-sized entities in a large context of similar objects. Deep learning\napproaches using supervised models with pre-trained weights, such as residual\nnetworks and vision transformers have demonstrated success for many\napplications. Unfortunately, when applied to images outside the domain of\nlearned representations, these methods often result with less than acceptable\nperformance. A strategy to overcome this can be achieved by using\nself-supervised models, where representations are learned and weights are then\napplied for downstream applications. Recently, masked autoencoders have proven\nto be effective to obtain representations that captures global context\ninformation. By masking regions of an image and having the model learn to\nreconstruct both the masked and non-masked regions, weights can be used for\nvarious applications. However, if the sizes of the objects in images are less\nthan the size of the mask, the global context information is lost, making it\nalmost impossible to reconstruct the image. In this study, we investigated the\neffect of mask ratios and patch sizes for blood components using a MAE to\nobtain learned ViT encoder representations. We then applied the encoder weights\nto train a U-Net Transformer for semantic segmentation to obtain both local and\nglobal contextual information. Our experimental results demonstrates that both\nsmaller mask ratios and patch sizes improve the reconstruction of images using\na MAE. We also show the results of semantic segmentation with and without\npre-trained weights, where smaller-sized blood components benefited with\npre-training. Overall, our proposed method offers an efficient and effective\nstrategy for the segmentation and classification of small objects.", "published": "2025-04-24 20:52:23", "link": "http://arxiv.org/abs/2504.17935v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Material Identification Via RFID For Smart Shopping", "abstract": "Cashierless stores rely on computer vision and RFID tags to associate\nshoppers with items, but concealed items placed in backpacks, pockets, or bags\ncreate challenges for theft prevention. We introduce a system that turns\nexisting RFID tagged items into material sensors by exploiting how different\ncontainers attenuate and scatter RF signals. Using RSSI and phase angle, we\ntrained a neural network to classify seven common containers. In a simulated\nretail environment, the model achieves 89% accuracy with one second samples and\n74% accuracy from single reads. Incorporating distance measurements, our system\nachieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at\naisle or doorway choke points, the system can flag suspicious events in real\ntime, prompting camera screening or staff intervention. By combining material\nidentification with computer vision tracking, our system provides proactive\nloss prevention for cashierless retail while utilizing existing infrastructure.", "published": "2025-04-24 19:18:55", "link": "http://arxiv.org/abs/2504.17898v1", "categories": ["eess.SP", "cs.CV", "J.0; J.7; B.0"], "primary_category": "eess.SP"}
{"title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing", "abstract": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques.", "published": "2025-04-24 19:14:50", "link": "http://arxiv.org/abs/2504.17894v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light", "abstract": "We present Phaser, a flexible system that directs narrow-beam laser light to\nmoving robots for concurrent wireless power delivery and communication. We\ndesign a semi-automatic calibration procedure to enable fusion of\nstereo-vision-based 3D robot tracking with high-power beam steering, and a\nlow-power optical communication scheme that reuses the laser light as a data\nchannel. We fabricate a Phaser prototype using off-the-shelf hardware and\nevaluate its performance with battery-free autonomous robots. Phaser delivers\noptical power densities of over 110 mW/cm$^2$ and error-free data to mobile\nrobots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\\% less\ncurrent than Bluetooth Low Energy). We demonstrate Phaser fully powering\ngram-scale battery-free robots to nearly 2x higher speeds than prior work while\nsimultaneously controlling them to navigate around obstacles and along paths.\nCode, an open-source design guide, and a demonstration video of Phaser is\navailable at https://mobilex.cs.columbia.edu/phaser.", "published": "2025-04-24 18:08:38", "link": "http://arxiv.org/abs/2504.17865v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Fast Autoregressive Models for Continuous Latent Generation", "abstract": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling.", "published": "2025-04-24 13:57:08", "link": "http://arxiv.org/abs/2504.18391v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Non-distributive Lattices, Stable Matchings, and Linear Optimization", "abstract": "We show that all finite lattices, including non-distributive lattices, arise\nas stable matching lattices under standard assumptions on choice functions. In\nthe process, we introduce new tools to reason on general lattices for\noptimization purposes: the partial representation of a lattice, which partially\nextends Birkhoff's representation theorem to non-distributive lattices; the\ndistributive closure of a lattice, which gives such a partial representation;\nand join constraints, which can be added to the distributive closure to obtain\na representation for the original lattice. Then, we use these techniques to\nshow that the minimum cost stable matching problem under the same standard\nassumptions on choice functions is NP-hard, by establishing a connection with\nantimatroid theory.", "published": "2025-04-24 20:16:25", "link": "http://arxiv.org/abs/2504.17916v1", "categories": ["cs.DM", "econ.TH"], "primary_category": "cs.DM"}
{"title": "Searching in trees with heavy group sets of fixed size", "abstract": "We consider the following generalization of the binary search problem: A\nsearcher is required to find a hidden element $x$ in a tree $T$. To do so, they\niteratively perform queries to an oracle about a chosen vertex $v$. After each\nsuch call, the oracle responds whether the target was found and if not, the\nsearcher receives as a reply the neighbor of $v$ that lays on the shortest path\ntowards $x$. Additionally, each vertex $v$ may have a different query cost\n$w(v)$. The goal is to find the optimal querying strategy for the searcher\nwhich minimizes the worst case query cost required to find $x$. The problem is\nknown to be NP-hard even in restricted classes of trees such as bounded\ndiameter spiders [Cicalese et al. 2016] and no constant factor approximation\nalgorithm is known for the general case. Inspired by recent studies\n[Dereniowski et al. 2022, Dereniowski et al. 2024], instead of restricted\nclasses of trees, we explore restrictions on the weight function. We introduce\nthe concept of a heavy group set of a vertex $HG(v,w)$. We show that if for\nevery $v\\in T$: $|HG(v,w)|\\leq k$ an $O(\\log\\log n)$-approximation can be found\nwithin $2^{O(\\log^2k)}\\cdot\\text{poly}(n)$ time.", "published": "2025-04-24 18:50:09", "link": "http://arxiv.org/abs/2504.17887v1", "categories": ["cs.DS", "cs.DM", "G.2.2"], "primary_category": "cs.DS"}
{"title": "Toward Low-Latency Services over PON using OCDMA Private Networks", "abstract": "An low-latency service scheme is proposed over Passive Optical Network (PON).\nThe Optical Code Division Multiplexing Access (OCDMA) technique is used to\ndefine multiple private networks serving as Virtual GE-PON that mimic the\nservice-based VLAN (S-VLAN) in the optical domain.", "published": "2025-04-24 22:53:07", "link": "http://arxiv.org/abs/2504.17973v1", "categories": ["cs.NI", "cs.IT", "eess.SP", "math.IT", "physics.optics"], "primary_category": "cs.NI"}
{"title": "Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models", "abstract": "The ability to predict trajectories of surrounding agents and obstacles is a\ncrucial component in many robotic applications. Data-driven approaches are\ncommonly adopted for state prediction in scenarios where the underlying\ndynamics are unknown. However, the performance, reliability, and uncertainty of\ndata-driven predictors become compromised when encountering out-of-distribution\nobservations relative to the training data. In this paper, we introduce a\nPlug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address\nthis challenge. Our method employs conformal prediction to identify outlier\ndynamics and, in that case, switches from a nominal predictor to a\nphysics-consistent model, namely distributed Port-Hamiltonian systems (dPHS).\nWe leverage Gaussian processes to model the energy function of the dPHS,\nenabling not only the learning of system dynamics but also the quantification\nof predictive uncertainty through its Bayesian nature. In this way, the\nproposed framework produces reliable physics-informed predictions even for the\nout-of-distribution scenarios.", "published": "2025-04-24 22:25:51", "link": "http://arxiv.org/abs/2504.17966v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Mathematics of Continual Learning", "abstract": "Continual learning is an emerging subject in machine learning that aims to\nsolve multiple tasks presented sequentially to the learner without forgetting\npreviously learned tasks. Recently, many deep learning based approaches have\nbeen proposed for continual learning, however the mathematical foundations\nbehind existing continual learning methods remain underdeveloped. On the other\nhand, adaptive filtering is a classic subject in signal processing with a rich\nhistory of mathematically principled methods. However, its role in\nunderstanding the foundations of continual learning has been underappreciated.\nIn this tutorial, we review the basic principles behind both continual learning\nand adaptive filtering, and present a comparative analysis that highlights\nmultiple connections between them. These connections allow us to enhance the\nmathematical foundations of continual learning based on existing results for\nadaptive filtering, extend adaptive filtering insights using existing continual\nlearning methods, and discuss a few research directions for continual learning\nsuggested by the historical developments in adaptive filtering.", "published": "2025-04-24 22:23:41", "link": "http://arxiv.org/abs/2504.17963v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CIVIL: Causal and Intuitive Visual Imitation Learning", "abstract": "Today's robots learn new tasks by imitating human examples. However, this\nstandard approach to visual imitation learning is fundamentally limited: the\nrobot observes what the human does, but not why the human chooses those\nbehaviors. Without understanding the features that factor into the human's\ndecisions, robot learners often misinterpret the data and fail to perform the\ntask when the environment changes. We therefore propose a shift in perspective:\ninstead of asking human teachers just to show what actions the robot should\ntake, we also enable humans to indicate task-relevant features using markers\nand language prompts. Our proposed algorithm, CIVIL, leverages this augmented\ndata to filter the robot's visual observations and extract a feature\nrepresentation that causally informs human actions. CIVIL then applies these\ncausal features to train a transformer-based policy that emulates human\nbehaviors without being confused by visual distractors. Our simulations,\nreal-world experiments, and user study demonstrate that robots trained with\nCIVIL can learn from fewer human demonstrations and perform better than\nstate-of-the-art baselines, especially in previously unseen scenarios. See\nvideos at our project website: https://civil2025.github.io", "published": "2025-04-24 22:08:29", "link": "http://arxiv.org/abs/2504.17959v1", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions", "abstract": "Phishing detection on Ethereum has increasingly leveraged advanced machine\nlearning techniques to identify fraudulent transactions. However, limited\nattention has been given to understanding the effectiveness of feature\nselection strategies and the role of graph-based models in enhancing detection\naccuracy. In this paper, we systematically examine these issues by analyzing\nand contrasting explicit transactional features and implicit graph-based\nfeatures, both experimentally and analytically. We explore how different\nfeature sets impact the performance of phishing detection models, particularly\nin the context of Ethereum's transactional network. Additionally, we address\nkey challenges such as class imbalance and dataset composition and their\ninfluence on the robustness and precision of detection methods. Our findings\ndemonstrate the advantages and limitations of each feature type, while also\nproviding a clearer understanding of how feature affect model resilience and\ngeneralization in adversarial environments.", "published": "2025-04-24 21:41:00", "link": "http://arxiv.org/abs/2504.17953v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Causality-Driven Neural Network Repair: Challenges and Opportunities", "abstract": "Deep Neural Networks (DNNs) often rely on statistical correlations rather\nthan causal reasoning, limiting their robustness and interpretability. While\ntesting methods can identify failures, effective debugging and repair remain\nchallenging. This paper explores causal inference as an approach primarily for\nDNN repair, leveraging causal debugging, counterfactual analysis, and\nstructural causal models (SCMs) to identify and correct failures. We discuss in\nwhat ways these techniques support fairness, adversarial robustness, and\nbackdoor mitigation by providing targeted interventions. Finally, we discuss\nkey challenges, including scalability, generalization, and computational\nefficiency, and outline future directions for integrating causality-driven\ninterventions to enhance DNN reliability.", "published": "2025-04-24 21:22:00", "link": "http://arxiv.org/abs/2504.17946v1", "categories": ["cs.LG", "D.2.2; I.2.6"], "primary_category": "cs.LG"}
{"title": "A computational model of infant sensorimotor exploration in the mobile paradigm", "abstract": "We present a computational model of the mechanisms that may determine\ninfants' behavior in the \"mobile paradigm\". This paradigm has been used in\ndevelopmental psychology to explore how infants learn the sensory effects of\ntheir actions. In this paradigm, a mobile (an articulated and movable object\nhanging above an infant's crib) is connected to one of the infant's limbs,\nprompting the infant to preferentially move that \"connected\" limb. This ability\nto detect a \"sensorimotor contingency\" is considered to be a foundational\ncognitive ability in development. To understand how infants learn sensorimotor\ncontingencies, we built a model that attempts to replicate infant behavior. Our\nmodel incorporates a neural network, action-outcome prediction, exploration,\nmotor noise, preferred activity level, and biologically-inspired motor control.\nWe find that simulations with our model replicate the classic findings in the\nliterature showing preferential movement of the connected limb. An interesting\nobservation is that the model sometimes exhibits a burst of movement after the\nmobile is disconnected, casting light on a similar occasional finding in\ninfants. In addition to these general findings, the simulations also replicate\ndata from two recent more detailed studies using a connection with the mobile\nthat was either gradual or all-or-none. A series of ablation studies further\nshows that the inclusion of mechanisms of action-outcome prediction,\nexploration, motor noise, and biologically-inspired motor control was essential\nfor the model to correctly replicate infant behavior. This suggests that these\ncomponents are also involved in infants' sensorimotor learning.", "published": "2025-04-24 21:02:06", "link": "http://arxiv.org/abs/2504.17939v1", "categories": ["q-bio.NC", "cs.LG", "92-10", "J.4"], "primary_category": "q-bio.NC"}
{"title": "Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G", "abstract": "The Quality of Experience (QoE) is the users satisfaction while streaming a\nvideo session over an over-the-top (OTT) platform like YouTube. QoE of YouTube\nreflects the smooth streaming session without any buffering and quality shift\nevents. One of the most important factors nowadays affecting QoE of YouTube is\nfrequent shifts from higher to lower resolutions and vice versa. These shifts\nensure a smooth streaming session; however, it might get a lower mean opinion\nscore. For instance, dropping from 1080p to 480p during a video can preserve\ncontinuity but might reduce the viewers enjoyment. Over time, OTT platforms are\nlooking for alternative ways to boost user experience instead of relying on\ntraditional Quality of Service (QoS) metrics such as bandwidth, latency, and\nthroughput. As a result, we look into the relationship between quality shifting\nin YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our\nfindings state that these channel metrics positively correlate with shifts.\nThus, in real-time, OTT can only rely on them to predict video streaming\nsessions into lower- and higher-resolution categories, thus providing more\nresources to improve user experience. Using traditional Machine Learning (ML)\nclassifiers, we achieved an accuracy of 77-percent, while using only RSRP,\nRSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency\nnetworks promise enhanced streaming capabilities, the proposed methodology can\nbe used to improve OTT services.", "published": "2025-04-24 21:00:43", "link": "http://arxiv.org/abs/2504.17938v1", "categories": ["cs.MM", "cs.LG"], "primary_category": "cs.MM"}
{"title": "Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques", "abstract": "Digital systems find it challenging to keep up with cybersecurity threats.\nThe daily emergence of more than 560,000 new malware strains poses significant\nhazards to the digital ecosystem. The traditional malware detection methods\nfail to operate properly and yield high false positive rates with low accuracy\nof the protection system. This study explores the ways in which malware can be\ndetected using these machine learning (ML) and deep learning (DL) approaches to\naddress those shortcomings. This study also includes a systematic comparison of\nthe performance of some of the widely used ML models, such as random forest,\nmulti-layer perceptron (MLP), and deep neural network (DNN), for determining\nthe effectiveness of the domain of modern malware threat systems. We use a\nconsiderable-sized database from Kaggle, which has undergone optimized feature\nselection and preprocessing to improve model performance. Our finding suggests\nthat the DNN model outperformed the other traditional models with the highest\ntraining accuracy of 99.92% and an almost perfect AUC score. Furthermore, the\nfeature selection and preprocessing can help improve the capabilities of\ndetection. This research makes an important contribution by analyzing the\nperformance of the model on the performance metrics and providing insight into\nthe effectiveness of the advanced detection techniques to build more robust and\nmore reliable cybersecurity solutions against the growing malware threats.", "published": "2025-04-24 20:40:51", "link": "http://arxiv.org/abs/2504.17930v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity", "abstract": "Long-term time series forecasting plays a pivotal role in various real-world\napplications. Despite recent advancements and the success of different\narchitectures, forecasting is often challenging due to non-stationary nature of\nthe real-world data, which frequently exhibit distribution shifts and temporal\nchanges in statistical properties like mean and variance over time. Previous\nstudies suggest that this inherent variability complicates forecasting,\nlimiting the performance of many models by leading to loss of non-stationarity\nand resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To\naddress this challenge, we introduce a novel architecture, ChoronoAdaptive\nNetwork (CANet), inspired by style-transfer techniques. The core of CANet is\nthe Non-stationary Adaptive Normalization module, seamlessly integrating the\nStyle Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and\nBelongie, 2017). The Style Blending Gate preserves and reintegrates\nnon-stationary characteristics, such as mean and standard deviation, by\nblending internal and external statistics, preventing over-stationarization\nwhile maintaining essential temporal dependencies. Coupled with AdaIN, which\ndynamically adapts the model to statistical changes, this approach enhances\npredictive accuracy under non-stationary conditions. CANet also employs\nmulti-resolution patching to handle short-term fluctuations and long-term\ntrends, along with Fourier analysis-based adaptive thresholding to reduce\nnoise. A Stacked Kronecker Product Layer further optimizes the model's\nefficiency while maintaining high performance. Extensive experiments on\nreal-world datasets validate CANet's superiority over state-of-the-art methods,\nachieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is\npublicly available at https://github.com/mertsonmezer/CANet.", "published": "2025-04-24 20:05:33", "link": "http://arxiv.org/abs/2504.17913v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection", "abstract": "Epilepsy, affecting approximately 50 million people globally, is\ncharacterized by abnormal brain activity and remains challenging to treat. The\ndiagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where\nspecialists manually analyze epileptiform patterns across pre-ictal, ictal,\npost-ictal, and interictal periods. However, the manual analysis of EEG signals\nis prone to variability between experts, emphasizing the need for automated\nsolutions. Although previous studies have explored preprocessing techniques and\nmachine learning approaches for seizure detection, there is a gap in\nunderstanding how the representation of EEG data (time, frequency, or\ntime-frequency domains) impacts the predictive performance of deep learning\nmodels. This work addresses this gap by systematically comparing deep neural\nnetworks trained on EEG data in these three domains. Through the use of\nstatistical tests, we identify the optimal data representation and model\narchitecture for epileptic seizure detection. The results demonstrate that\nfrequency-domain data achieves detection metrics exceeding 97\\%, providing a\nrobust foundation for more accurate and reliable seizure detection systems.", "published": "2025-04-24 19:50:48", "link": "http://arxiv.org/abs/2504.17908v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Do We Need Transformers to Play FPS Video Games?", "abstract": "In this paper, we explore the Transformer based architectures for\nreinforcement learning in both online and offline settings within the Doom game\nenvironment. Our investigation focuses on two primary approaches: Deep\nTransformer Q- learning Networks (DTQN) for online learning and Decision\nTransformers (DT) for offline reinforcement learning. DTQN leverages the\nsequential modelling capabilities of Transformers to enhance Q-learning in\npartially observable environments,while Decision Transformers repurpose\nsequence modelling techniques to enable offline agents to learn from past\ntrajectories without direct interaction with the environment. We conclude that\nwhile Transformers might have performed well in Atari games, more traditional\nmethods perform better than Transformer based method in both the settings in\nthe VizDoom environment.", "published": "2025-04-24 19:10:55", "link": "http://arxiv.org/abs/2504.17891v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses", "abstract": "Data reduction with uncertainty quantification plays a key role in various\nmulti-task learning applications, where large numbers of responses and features\nare present. To this end, a general framework of high-dimensional\nmanifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou,\nFan and Lv (2024) for interpretable multi-task learning inference focusing on\nthe left factor vectors and singular values exploiting the latent singular\nvalue decomposition (SVD) structure. Yet, designing a valid inference procedure\non the latent right factor vectors is not straightforward from that of the left\nones and can be even more challenging due to asymmetry of left and right\nsingular vectors in the response matrix. To tackle these issues, in this paper\nwe suggest a new method of high-dimensional manifold-based SOFAR inference for\nlatent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The\nfirst variant deals with strongly orthogonal factors by coupling left singular\nvectors with the design matrix and then appropriately rescaling them to\ngenerate new Stiefel manifolds. The second variant handles the more general\nweakly orthogonal factors by employing the hard-thresholded SOFARI estimates\nand delicately incorporating approximation errors into the distribution. Both\nvariants produce bias-corrected estimators for the latent right factor vectors\nthat enjoy asymptotically normal distributions with justified asymptotic\nvariance estimates. We demonstrate the effectiveness of the newly suggested\nmethod using extensive simulation studies and an economic application.", "published": "2025-04-24 18:21:19", "link": "http://arxiv.org/abs/2504.17874v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures", "abstract": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API.", "published": "2025-04-24 18:01:36", "link": "http://arxiv.org/abs/2504.17857v1", "categories": ["cs.LG", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Model Error Covariance Estimation for Weak Constraint Data Assimilation", "abstract": "State estimates from weak constraint 4D-Var data assimilation can vary\nsignificantly depending on the data and model error covariances. As a result,\nthe accuracy of these estimates heavily depends on the correct specification of\nboth model and observational data error covariances. In this work, we assume\nthat the data error is known and and focus on estimating the model error\ncovariance by framing weak constraint 4D-Var as a regularized inverse problem,\nwhere the inverse model error covariance serves as the regularization matrix.\nWe consider both isotropic and non-isotropic forms of the model error\ncovariance. Using the representer method, we reduce the 4D-Var problem from\nstate space to data space, enabling the efficient application of regularization\nparameter selection techniques. The Representer method also provides an\nanalytic expression for the optimal state estimate, allowing us to derive\nmatrix expressions for the three regularization parameter selection methods\ni.e. the L-curve, generalized cross-validation (GCV), and the Chi-square\nmethod. We validate our approach by assimilating simulated data into a 1D\ntransport equation modeling wildfire smoke transport under various\nobservational noise and forward model perturbations. In these experiments the\ngoal is to identify the model error covariances that accurately capture the\ninfluence of observational data versus model predictions on assimilated state\nestimates. The regularization parameter selection methods successfully estimate\nhyperparameters for both isotropic and non-isotropic model error covariances,\nthat reflect whether the first guess model predictions are more or less\nreliable than the observational data. The results further indicate that\nisotropic variances are sufficient when the first guess is more accurate than\nthe data whereas non-isotropic covariances are preferred when the observational\ndata is more reliable.", "published": "2025-04-24 19:22:10", "link": "http://arxiv.org/abs/2504.17900v1", "categories": ["stat.ME", "cs.NA", "math.NA", "math.OC", "65K10, 65F22"], "primary_category": "stat.ME"}
{"title": "Multivariate Newton Interpolation in Downward Closed Spaces Reaches the Optimal Geometric Approximation Rates for Bos--Levenberg--Trefethen Functions", "abstract": "We extend the univariate Newton interpolation algorithm to arbitrary spatial\ndimensions and for any choice of downward-closed polynomial space, while\npreserving its quadratic runtime and linear storage cost. The generalisation\nsupports any choice of the provided notion of non-tensorial unisolvent\ninterpolation nodes, whose number coincides with the dimension of the\nchosen-downward closed space. Specifically, we prove that by selecting\nLeja-ordered Chebyshev-Lobatto or Leja nodes, the optimal geometric\napproximation rates for a class of analytic functions -- termed\nBos--Levenberg--Trefethen functions -- are achieved and extend to the\nderivatives of the interpolants. In particular, choosing Euclidean degree\nresults in downward-closed spaces whose dimension only grows sub-exponentially\nwith spatial dimension, while delivering approximation rates close to, or even\nmatching those of the tensorial maximum-degree case, mitigating the curse of\ndimensionality. Several numerical experiments demonstrate the performance of\nthe resulting multivariate Newton interpolation compared to state-of-the-art\nalternatives and validate our theoretical results.", "published": "2025-04-24 19:20:31", "link": "http://arxiv.org/abs/2504.17899v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "POD-ROM methods: error analysis for continuous parametrized approximations", "abstract": "This paper studies the numerical approximation of parametric time-dependent\npartial differential equations (PDEs) by proper orthogonal decomposition\nreduced order models (POD-ROMs). Although many papers in the literature\nconsider reduced order models for parametric equations, a complete error\nanalysis of the methods is still a challenge. We introduce and analyze in this\npaper a new POD method based on finite differences (respect to time and respect\nto the parameters that may be considered). We obtain a priori bounds for the\nnew method valid for any value of time in a given time interval and any value\nof the parameter in a given parameter interval. Our design of the new POD\nmethod allow us to prove pointwise-in-time error estimates as opposed to\naverage error bounds obtained typically in POD methods. Most of the papers\nconcerning POD methods for parametric equations are just based on the snapshots\ncomputed at different times and parameter values instead of their difference\nquotients. We show that the error analysis of the present paper can also cover\nthe error analysis of that case (that we call standard). Some numerical\nexperiments compare our new approach with the standard one and support the\nerror analysis.", "published": "2025-04-24 19:17:14", "link": "http://arxiv.org/abs/2504.17895v1", "categories": ["math.NA", "cs.NA", "65"], "primary_category": "math.NA"}
{"title": "Efficient iterative techniques for solving tensor problems with the T-product", "abstract": "This paper presents iterative methods for solving tensor equations involving\nthe T-product. The proposed approaches apply tensor computations without matrix\nconstruction. For each initial tensor, these algorithms solve related problems\nin a finite number of iterations, with negligible errors. The theoretical\nanalysis is validated by numerical examples that demonstrate the practicality\nand effectiveness of these algorithms.", "published": "2025-04-24 18:06:06", "link": "http://arxiv.org/abs/2504.17861v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Learning Enhanced Ensemble Filters", "abstract": "The filtering distribution in hidden Markov models evolves according to the\nlaw of a mean-field model in state--observation space. The ensemble Kalman\nfilter (EnKF) approximates this mean-field model with an ensemble of\ninteracting particles, employing a Gaussian ansatz for the joint distribution\nof the state and observation at each observation time. These methods are\nrobust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed\nby approximating the mean-field evolution using a novel form of neural operator\ntaking probability distributions as input: a Measure Neural Mapping (MNM). A\nMNM is used to design a novel approach to filtering, the MNM-enhanced ensemble\nfilter (MNMEF), which is defined in both the mean-fieldlimit and for\ninteracting ensemble particle approximations. The ensemble approach uses\nempirical measures as input to the MNM and is implemented using the set\ntransformer, which is invariant to ensemble permutation and allows for\ndifferent ensemble sizes. The derivation of methods from a mean-field\nformulation allows a single parameterization of the algorithm to be deployed at\ndifferent ensemble sizes. In practice fine-tuning of a small number of\nparameters, for specific ensemble sizes, further enhances the accuracy of the\nscheme. The promise of the approach is demonstrated by its superior\nroot-mean-square-error performance relative to leading methods in filtering the\nLorenz 96 and Kuramoto-Sivashinsky models.", "published": "2025-04-24 17:48:03", "link": "http://arxiv.org/abs/2504.17836v1", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "physics.comp-ph"], "primary_category": "stat.ML"}
{"title": "STNet: Prediction of Underwater Sound Speed Profiles with An Advanced Semi-Transformer Neural Network", "abstract": "Real time acquisition of accurate underwater sound velocity profile (SSP) is\ncrucial for tracking the propagation trajectory of underwater acoustic signals,\nmaking it play a key role in ocean communication positioning. SSPs can be\ndirectly measured by instruments or inverted leveraging sound field data.\nAlthough measurement techniques provide a good accuracy, they are constrained\nby limited spatial coverage and require substantial time investment. The\ninversion method based on real-time measurement of acoustic field data improves\noperational efficiency, but loses the accuracy of SSP estimation and suffers\nfrom limited spatial applicability due to its stringent requirements for ocean\nobservation infrastructure. To achieve accurate long-term ocean SSP estimation\nindependent of real-time underwater data measurements, we propose a\nSemi-Transformer neural network (STNet) specifically designed for simulating\nsound velocity distribution patterns from the perspective of time series\nprediction. The proposed network architecture incorporates an optimized\nself-attention mechanism to effectively capture long-range temporal\ndependencies within historical sound velocity time-series data, facilitating\naccurate estimation of current SSPs or prediction of future SSPs. Through\narchitectural optimization of the Transformer framework and integration of a\ntime encoding mechanism, STNet could effectively improve computational\nefficiency. Comparative experimental results reveal that STNet outperforms\nstate-of-the-art models in predictive accuracy and maintain good computational\nefficiency, demonstrating its potential for enabling accurate long-term\nfull-depth ocean SSP forecasting.", "published": "2025-04-24 20:01:16", "link": "http://arxiv.org/abs/2504.17912v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Quaternion Domain Super MDS for 3D Localization", "abstract": "We propose a novel low-complexity three-dimensional (3D) localization\nalgorithm for wireless sensor networks, termed quaternion-domain super\nmultidimensional scaling (QD-SMDS). This algorithm reformulates the\nconventional SMDS, which was originally developed in the real domain, into the\nquaternion domain. By representing 3D coordinates as quaternions, the method\nenables the construction of a rank-1 Gram edge kernel (GEK) matrix that\nintegrates both relative distance and angular (phase) information between\nnodes, maximizing the noise reduction effect achieved through low-rank\ntruncation via singular value decomposition (SVD). The simulation results\nindicate that the proposed method demonstrates a notable enhancement in\nlocalization accuracy relative to the conventional SMDS algorithm, particularly\nin scenarios characterized by substantial measurement errors.", "published": "2025-04-24 19:01:33", "link": "http://arxiv.org/abs/2504.17890v1", "categories": ["eess.SP", "cs.RO", "math.MG"], "primary_category": "eess.SP"}
{"title": "Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection", "abstract": "Spoilers in movie reviews are important on platforms like IMDb and Rotten\nTomatoes, offering benefits and drawbacks. They can guide some viewers' choices\nbut also affect those who prefer no plot details in advance, making effective\nspoiler detection essential. Existing spoiler detection methods mainly analyze\nreview text, often overlooking the impact of movie genres and user bias,\nlimiting their effectiveness. To address this, we analyze movie review data,\nfinding genre-specific variations in spoiler rates and identifying that certain\nusers are more likely to post spoilers. Based on these findings, we introduce a\nnew spoiler detection framework called GUSD (The code is available at\nhttps://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler\nDetection), which incorporates genre-specific data and user behavior bias. User\nbias is calculated through dynamic graph modeling of review history.\nAdditionally, the R2GFormer module combines RetGAT (Retentive Graph Attention\nNetwork) for graph information and GenreFormer for genre-specific aggregation.\nThe GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to\nspecialized experts based on genre. Extensive testing on benchmark datasets\nshows that GUSD achieves state-of-the-art results. This approach advances\nspoiler detection by addressing genre and user-specific patterns, enhancing\nuser experience on movie review platforms.", "published": "2025-04-24 15:34:35", "link": "http://arxiv.org/abs/2504.17834v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "abstract": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins. Code is available at:\nhttps://github.com/going-doer/Paper2Code.", "published": "2025-04-24 01:57:01", "link": "http://arxiv.org/abs/2504.17192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "abstract": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce Hamlet, a novel hybrid control system for agile badminton\nrobots. Specifically, we propose a model-based strategy for chassis locomotion\nwhich provides a base for arm policy. We introduce a physics-informed \"IL+RL\"\ntraining framework for learning-based arm policy. In this train framework, a\nmodel-based strategy with privileged information is used to guide arm policy\ntraining during both IL and RL phases. In addition, we train the critic model\nduring IL phase to alleviate the performance drop issue when transitioning from\nIL to RL. We present results on our self-engineered badminton robot, achieving\n94.5% success rate against the serving machine and 90.7% success rate against\nhuman players. Our system can be easily generalized to other agile mobile\nmanipulation tasks such as agile catching and table tennis. Our project\nwebsite: https://dreamstarring.github.io/HAMLET/.", "published": "2025-04-24 17:46:29", "link": "http://arxiv.org/abs/2504.17771v2", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "abstract": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.", "published": "2025-04-24 17:09:59", "link": "http://arxiv.org/abs/2504.17751v2", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "abstract": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.", "published": "2025-04-24 17:59:56", "link": "http://arxiv.org/abs/2504.17789v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Step1X-Edit: A Practical Framework for General Image Editing", "abstract": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "published": "2025-04-24 17:25:12", "link": "http://arxiv.org/abs/2504.17761v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Quantum Error Correction with Girth-16 Non-Binary LDPC Codes via Affine Permutation Construction", "abstract": "We propose a method for constructing quantum error-correcting codes based on\nnon-binary low-density parity-check codes with Tanner graph girth 16. While\nconventional constructions using circulant permutation matrices are limited to\ngirth 12, our method employs affine permutation matrices and a randomized\nsequential selection procedure to eliminate short cycles and achieve girth 16.\n  Numerical experiments show that the proposed codes significantly reduce the\nnumber of low-weight codewords. Joint belief propagation decoding over\ndepolarizing channels reveals that although a slight degradation appears in the\nwaterfall region, a substantial improvement is achieved in the error floor\nperformance.\n  We also evaluated the minimum distance and found that the proposed codes\nachieve a larger upper bound compared to conventional constructions.", "published": "2025-04-24 17:59:58", "link": "http://arxiv.org/abs/2504.17790v2", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "UNILoc: Unified Localization Combining Model-Based Geometry and Unsupervised Learning", "abstract": "Accurate mobile device localization is critical for emerging 5G/6G\napplications such as autonomous vehicles and augmented reality. In this paper,\nwe propose a unified localization method that integrates model-based and\nmachine learning (ML)-based methods to reap their respective advantages by\nexploiting available map information. In order to avoid supervised learning, we\ngenerate training labels automatically via optimal transport (OT) by fusing\ngeometric estimates with building layouts. Ray-tracing based simulations are\ncarried out to demonstrate that the proposed method significantly improves\npositioning accuracy for both line-of-sight (LoS) users (compared to ML-based\nmethods) and non-line-of-sight (NLoS) users (compared to model-based methods).\nRemarkably, the unified method is able to achieve competitive overall\nperformance with the fully-supervised fingerprinting, while eliminating the\nneed for cumbersome labeled data measurement and collection.", "published": "2025-04-24 15:45:54", "link": "http://arxiv.org/abs/2504.17676v2", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Likelihood-Free Variational Autoencoders", "abstract": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose EnVAE, a novel likelihood-free\ngenerative framework that has a deterministic decoder and employs the energy\nscore--a proper scoring rule--to build the reconstruction loss. This enables\nlikelihood-free inference without requiring explicit parametric density\nfunctions. To address the computational inefficiency of the energy score, we\nintroduce a fast variant, FEnVAE, based on the local smoothness of the decoder\nand the sharpness of the posterior distribution of latent variables. This\nyields an efficient single-sample training objective that integrates seamlessly\ninto existing VAE pipelines with minimal overhead. Empirical results on\nstandard benchmarks demonstrate that EnVAE achieves superior reconstruction and\ngeneration quality compared to likelihood-based baselines. Our framework offers\na general, scalable, and statistically principled alternative for flexible and\nnonparametric distribution learning in generative modeling.", "published": "2025-04-24 14:44:46", "link": "http://arxiv.org/abs/2504.17622v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Quaternion Domain Super MDS for 3D Localization", "abstract": "We propose a novel low-complexity three-dimensional (3D) localization\nalgorithm for wireless sensor networks, termed quaternion-domain super\nmultidimensional scaling (QD-SMDS). This algorithm reformulates the\nconventional SMDS, which was originally developed in the real domain, into the\nquaternion domain. By representing 3D coordinates as quaternions, the method\nenables the construction of a rank-1 Gram edge kernel (GEK) matrix that\nintegrates both relative distance and angular (phase) information between\nnodes, maximizing the noise reduction effect achieved through low-rank\ntruncation via singular value decomposition (SVD). The simulation results\nindicate that the proposed method demonstrates a notable enhancement in\nlocalization accuracy relative to the conventional SMDS algorithm, particularly\nin scenarios characterized by substantial measurement errors.", "published": "2025-04-24 19:01:33", "link": "http://arxiv.org/abs/2504.17890v2", "categories": ["eess.SP", "cs.RO", "math.MG"], "primary_category": "eess.SP"}
{"title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach", "abstract": "Quantum computing education faces significant challenges due to its\ncomplexity and the limitations of current tools; this paper introduces a novel\nIntelligent Teaching Assistant for quantum computing education and details its\nevolutionary design process. The system combines a knowledge-graph-augmented\narchitecture with two specialized Large Language Model (LLM) agents: a Teaching\nAgent for dynamic interaction, and a Lesson Planning Agent for lesson plan\ngeneration. The system is designed to adapt to individual student needs, with\ninteractions meticulously tracked and stored in a knowledge graph. This graph\nrepresents student actions, learning resources, and relationships, aiming to\nenable reasoning about effective learning pathways. We describe the\nimplementation of the system, highlighting the challenges encountered and the\nsolutions implemented, including introducing a dual-agent architecture where\ntasks are separated, all coordinated through a central knowledge graph that\nmaintains system awareness, and a user-facing tag system intended to mitigate\nLLM hallucination and improve user control. Preliminary results illustrate the\nsystem's potential to capture rich interaction data, dynamically adapt lesson\nplans based on student feedback via a tag system in simulation, and facilitate\ncontext-aware tutoring through the integrated knowledge graph, though\nsystematic evaluation is required.", "published": "2025-04-24 21:53:34", "link": "http://arxiv.org/abs/2504.18603v1", "categories": ["cs.CY", "cs.AI", "cs.MA"], "primary_category": "cs.CY"}
{"title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "abstract": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems.", "published": "2025-04-24 18:49:55", "link": "http://arxiv.org/abs/2504.19940v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "QuantBench: Benchmarking AI Methods for Quantitative Investment", "abstract": "The field of artificial intelligence (AI) in quantitative investment has seen\nsignificant advancements, yet it lacks a standardized benchmark aligned with\nindustry practices. This gap hinders research progress and limits the practical\napplication of academic innovations. We present QuantBench, an industrial-grade\nbenchmark platform designed to address this critical need. QuantBench offers\nthree key strengths: (1) standardization that aligns with quantitative\ninvestment industry practices, (2) flexibility to integrate various AI\nalgorithms, and (3) full-pipeline coverage of the entire quantitative\ninvestment process. Our empirical studies using QuantBench reveal some critical\nresearch directions, including the need for continual learning to address\ndistribution shifts, improved methods for modeling relational financial data,\nand more robust approaches to mitigate overfitting in low signal-to-noise\nenvironments. By providing a common ground for evaluation and fostering\ncollaboration between researchers and practitioners, QuantBench aims to\naccelerate progress in AI for quantitative investment, similar to the impact of\nbenchmark platforms in computer vision and natural language processing.", "published": "2025-04-24 18:47:22", "link": "http://arxiv.org/abs/2504.18600v1", "categories": ["q-fin.CP", "cs.AI", "cs.CE"], "primary_category": "q-fin.CP"}
{"title": "Training Large Language Models to Reason via EM Policy Gradient", "abstract": "Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's\nR1, have demonstrated strong reasoning capacities and problem-solving skills\nacquired through large-scale reinforcement learning (RL), with wide\napplications in mathematics, coding, science, intelligent agents, and virtual\nassistants. In this work, we introduce an off-policy reinforcement learning\nalgorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing\nexpected return over reasoning trajectories. We frame the reasoning task as an\nExpectation-Maximization (EM) optimization problem, alternating between\nsampling diverse rationale trajectories and performing reward-guided\nfine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and\nheuristic clipping, our method provides a simpler, more principled off-policy\npolicy gradient approach, eliminating these complexities while maintaining\nstrong performance. We evaluate the effectiveness of EM Policy Gradient on the\nGSM8K and MATH (HARD) datasets, where it achieves performance comparable to or\nslightly surpassing the state-of-the-art GRPO, while offering additional\nadvantages in scalability, simplicity, and reasoning conciseness. Moreover,\nmodels fine-tuned with our method exhibit cognitive behaviors, such as\nsub-problem decomposition, self-verification, and backtracking, highlighting\nits potential to enhance both the interpretability and robustness of LLM\nreasoning.", "published": "2025-04-24 01:31:05", "link": "http://arxiv.org/abs/2504.18587v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics", "abstract": "GenGrid is a novel comprehensive open-source, distributed platform intended\nfor conducting extensive swarm robotic experiments. The modular platform is\ndesigned to run swarm robotics experiments that are compatible with different\ntypes of mobile robots ranging from Colias, Kilobot, and E puck. The platform\noffers programmable control over the experimental setup and its parameters and\nacts as a tool to collect swarm robot data, including localization, sensory\nfeedback, messaging, and interaction. GenGrid is designed as a modular grid of\nattachable computing nodes that offers bidirectional communication between the\nrobotic agent and grid nodes and within grids. The paper describes the hardware\nand software architecture design of the GenGrid system. Further, it discusses\nsome common experimental studies covering multi-robot and swarm robotics to\nshowcase the platform's use. GenGrid of 25 homogeneous cells with identical\nsensing and communication characteristics with a footprint of 37.5 cm X 37.5\ncm, exhibits multiple capabilities with minimal resources. The open-source\nhardware platform is handy for running swarm experiments, including robot\nhopping based on multiple gradients, collective transport, shepherding,\ncontinuous pheromone deposition, and subsequent evaporation. The low-cost,\nmodular, and open-source platform is significant in the swarm robotics research\ncommunity, which is currently driven by commercial platforms that allow minimal\nmodifications.", "published": "2025-04-24 17:30:38", "link": "http://arxiv.org/abs/2504.20071v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives", "abstract": "Electroencephalogram (EEG) signals play a crucial role in understanding brain\nactivity and diagnosing neurological disorders. This review focuses on the\nrecent development of EEG foundation models(EEG-FMs), which have shown great\npotential in processing and analyzing EEG data. We discuss various EEG-FMs,\nincluding their architectures, pre-training strategies, their pre-training and\ndownstream datasets and other details. The review also highlights the\nchallenges and future directions in this field, aiming to provide a\ncomprehensive overview for researchers and practitioners interested in EEG\nanalysis and related EEG-FMs.", "published": "2025-04-24 14:14:17", "link": "http://arxiv.org/abs/2504.20069v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "abstract": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "published": "2025-04-24 05:57:03", "link": "http://arxiv.org/abs/2505.00018v1", "categories": ["cs.AI", "cs.HC", "cs.MA"], "primary_category": "cs.AI"}
{"title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "abstract": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1.", "published": "2025-04-24 15:25:52", "link": "http://arxiv.org/abs/2505.00725v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "abstract": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.", "published": "2025-04-24 13:17:18", "link": "http://arxiv.org/abs/2504.17529v2", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "abstract": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.", "published": "2025-04-24 15:38:20", "link": "http://arxiv.org/abs/2504.17669v2", "categories": ["cs.MA", "cs.AI", "cs.ET"], "primary_category": "cs.MA"}
