{"title": "Sticking to the Facts: Confident Decoding for Faithful Data-to-Text\n  Generation", "abstract": "We address the issue of hallucination in data-to-text generation, i.e.,\nreducing the generation of text that is unsupported by the source. We\nconjecture that hallucination can be caused by an encoder-decoder model\ngenerating content phrases without attending to the source; so we propose a\nconfidence score to ensure that the model attends to the source whenever\nnecessary, as well as a variational Bayes training framework that can learn the\nscore from data. Experiments on the WikiBio (Lebretet al., 2016) dataset show\nthat our approach is more faithful to the source than existing state-of-the-art\napproaches, according to both PARENT score (Dhingra et al., 2019) and human\nevaluation. We also report strong results on the WebNLG (Gardent et al., 2017)\ndataset.", "published": "2019-10-19 03:00:46", "link": "http://arxiv.org/abs/1910.08684v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MonaLog: a Lightweight System for Natural Language Inference Based on\n  Monotonicity", "abstract": "We present a new logic-based inference engine for natural language inference\n(NLI) called MonaLog, which is based on natural logic and the monotonicity\ncalculus. In contrast to existing logic-based approaches, our system is\nintentionally designed to be as lightweight as possible, and operates using a\nsmall set of well-known (surface-level) monotonicity facts about quantifiers,\nlexical items and tokenlevel polarity information. Despite its simplicity, we\nfind our approach to be competitive with other logic-based NLI models on the\nSICK benchmark. We also use MonaLog in combination with the current\nstate-of-the-art model BERT in a variety of settings, including for\ncompositional data augmentation. We show that MonaLog is capable of generating\nlarge amounts of high-quality training data for BERT, improving its accuracy on\nSICK.", "published": "2019-10-19 13:45:30", "link": "http://arxiv.org/abs/1910.08772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Question Generation with Reinforcement Learning Based\n  Graph-to-Sequence Model", "abstract": "Natural question generation (QG) aims to generate questions from a passage\nand an answer. In this paper, we propose a novel reinforcement learning (RL)\nbased graph-to-sequence (Graph2Seq) model for QG. Our model consists of a\nGraph2Seq generator where a novel Bidirectional Gated Graph Neural Network is\nproposed to embed the passage, and a hybrid evaluator with a mixed objective\ncombining both cross-entropy and RL losses to ensure the generation of\nsyntactically and semantically valid text. The proposed model outperforms\nprevious state-of-the-art methods by a large margin on the SQuAD dataset.", "published": "2019-10-19 20:05:44", "link": "http://arxiv.org/abs/1910.08832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyphrase Extraction from Scholarly Articles as Sequence Labeling using\n  Contextualized Embeddings", "abstract": "In this paper, we formulate keyphrase extraction from scholarly articles as a\nsequence labeling task solved using a BiLSTM-CRF, where the words in the input\ntext are represented using deep contextualized embeddings. We evaluate the\nproposed architecture using both contextualized and fixed word embedding models\non three different benchmark datasets (Inspec, SemEval 2010, SemEval 2017) and\ncompare with existing popular unsupervised and supervised techniques. Our\nresults quantify the benefits of (a) using contextualized embeddings (e.g.\nBERT) over fixed word embeddings (e.g. Glove); (b) using a BiLSTM-CRF\narchitecture with contextualized word embeddings over fine-tuning the\ncontextualized word embedding model directly, and (c) using genre-specific\ncontextualized embeddings (SciBERT). Through error analysis, we also provide\nsome insights into why particular models work better than others. Lastly, we\npresent a case study where we analyze different self-attention layers of the\ntwo best models (BERT and SciBERT) to better understand the predictions made by\neach for the task of keyphrase extraction.", "published": "2019-10-19 20:42:59", "link": "http://arxiv.org/abs/1910.08840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Improved Historical Embedding without Alignment", "abstract": "Many words have evolved in meaning as a result of cultural and social change.\nUnderstanding such changes is crucial for modelling language and cultural\nevolution. Low-dimensional embedding methods have shown promise in detecting\nwords' meaning change by encoding them into dense vectors. However, when\nexploring semantic change of words over time, these methods require the\nalignment of word embeddings across different time periods. This process is\ncomputationally expensive, prohibitively time consuming and suffering from\ncontextual variability. In this paper, we propose a new and scalable method for\nencoding words from different time periods into one dense vector space. This\ncan greatly improve performance when it comes to identifying words that have\nchanged in meaning over time. We evaluated our method on dataset from Google\nBooks N-gram. Our method outperformed three other popular methods in terms of\nthe number of words correctly identified to have changed in meaning.\nAdditionally, we provide an intuitive visualization of the semantic evolution\nof some words extracted by our method", "published": "2019-10-19 03:32:16", "link": "http://arxiv.org/abs/1910.08692v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks on Spoofing Countermeasures of automatic speaker\n  verification", "abstract": "High-performance spoofing countermeasure systems for automatic speaker\nverification (ASV) have been proposed in the ASVspoof 2019 challenge. However,\nthe robustness of such systems under adversarial attacks has not been studied\nyet. In this paper, we investigate the vulnerability of spoofing\ncountermeasures for ASV under both white-box and black-box adversarial attacks\nwith the fast gradient sign method (FGSM) and the projected gradient descent\n(PGD) method. We implement high-performing countermeasure models in the\nASVspoof 2019 challenge and conduct adversarial attacks on them. We compare\nperformance of black-box attacks across spoofing countermeasure models with\ndifferent network architectures and different amount of model parameters. The\nexperimental results show that all implemented countermeasure models are\nvulnerable to FGSM and PGD attacks under the scenario of white-box attack. The\nmore dangerous black-box attacks also prove to be effective by the experimental\nresults.", "published": "2019-10-19 07:28:39", "link": "http://arxiv.org/abs/1910.08716v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "XL-Editor: Post-editing Sentences with XLNet", "abstract": "While neural sequence generation models achieve initial success for many NLP\napplications, the canonical decoding procedure with left-to-right generation\norder (i.e., autoregressive) in one-pass can not reflect the true nature of\nhuman revising a sentence to obtain a refined result. In this work, we propose\nXL-Editor, a novel training framework that enables state-of-the-art generalized\nautoregressive pretraining methods, XLNet specifically, to revise a given\nsentence by the variable-length insertion probability. Concretely, XL-Editor\ncan (1) estimate the probability of inserting a variable-length sequence into a\nspecific position of a given sentence; (2) execute post-editing operations such\nas insertion, deletion, and replacement based on the estimated variable-length\ninsertion probability; (3) complement existing sequence-to-sequence models to\nrefine the generated sequences. Empirically, we first demonstrate better\npost-editing capabilities of XL-Editor over XLNet on the text insertion and\ndeletion tasks, which validates the effectiveness of our proposed framework.\nFurthermore, we extend XL-Editor to the unpaired text style transfer task,\nwhere transferring the target style onto a given sentence can be naturally\nviewed as post-editing the sentence into the target style. XL-Editor achieves\nsignificant improvement in style transfer accuracy and also maintains coherent\nsemantic of the original sentence, showing the broad applicability of our\nmethod.", "published": "2019-10-19 21:39:03", "link": "http://arxiv.org/abs/1910.10479v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-channel Time-Varying Covariance Matrix Model for Late\n  Reverberation Reduction", "abstract": "In this paper, a multi-channel time-varying covariance matrix model for late\nreverberation reduction is proposed. Reflecting that variance of the late\nreverberation is time-varying and it depends on past speech source variance,\nthe proposed model is defined as convolution of a speech source variance with a\nmulti-channel time-invariant covariance matrix of late reverberation. The\nmulti-channel time-invariant covariance matrix can be interpreted as a\ncovariance matrix of a multi-channel acoustic transfer function (ATF). An\nadvantageous point of the covariance matrix model against a deterministic ATF\nmodel is that the covariance matrix model is robust against fluctuation of the\nATF. We propose two covariance matrix models. The first model is a covariance\nmatrix model of late reverberation in the original microphone input signal. The\nsecond one is a covariance matrix model of late reverberation in an extended\nmicrophone input signal which includes not only current microphone input signal\nbut also past microphone input signal. The second one considers correlation\nbetween the current microphone input signal and the past microphone input\nsignal. Experimental results show that the proposed method effectively reduces\nreverberation especially in a time-varying ATF scenario and the second model is\nshown to be more effective than the first model.", "published": "2019-10-19 06:15:29", "link": "http://arxiv.org/abs/1910.08710v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency-Sliding Generalized Cross-Correlation: A Sub-band Time Delay\n  Estimation Approach", "abstract": "The generalized cross correlation (GCC) is regarded as the most popular\napproach for estimating the time difference of arrival (TDOA) between the\nsignals received at two sensors. Time delay estimates are obtained by\nmaximizing the GCC output, where the direct-path delay is usually observed as a\nprominent peak. Moreover, GCCs play also an important role in steered response\npower (SRP) localization algorithms, where the SRP functional can be written as\nan accumulation of the GCCs computed from multiple sensor pairs. Unfortunately,\nthe accuracy of TDOA estimates is affected by multiple factors, including\nnoise, reverberation and signal bandwidth. In this paper, a sub-band approach\nfor time delay estimation aimed at improving the performance of the\nconventional GCC is presented. The proposed method is based on the extraction\nof multiple GCCs corresponding to different frequency bands of the cross-power\nspectrum phase in a sliding-window fashion. The major contributions of this\npaper include: 1) a sub-band GCC representation of the cross-power spectrum\nphase that, despite having a reduced temporal resolution, provides a more\nsuitable representation for estimating the true TDOA; 2) such matrix\nrepresentation is shown to be rank one in the ideal noiseless case, a property\nthat is exploited in more adverse scenarios to obtain a more robust and\naccurate GCC; 3) we propose a set of low-rank approximation alternatives for\nprocessing the sub-band GCC matrix, leading to better TDOA estimates and source\nlocalization performance. An extensive set of experiments is presented to\ndemonstrate the validity of the proposed approach.", "published": "2019-10-19 20:40:10", "link": "http://arxiv.org/abs/1910.08838v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BUT System Description for DIHARD Speech Diarization Challenge 2019", "abstract": "This paper describes the systems developed by the BUT team for the four\ntracks of the second DIHARD speech diarization challenge. For tracks 1 and 2\nthe systems were based on performing agglomerative hierarchical clustering\n(AHC) over x-vectors, followed by the Bayesian Hidden Markov Model (HMM) with\neigenvoice priors applied at x-vector level followed by the same approach\napplied at frame level. For tracks 3 and 4, the systems were based on\nperforming AHC using x-vectors extracted on all channels.", "published": "2019-10-19 21:37:20", "link": "http://arxiv.org/abs/1910.08847v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual\n  Zeroshot Classification and Retrieval of Videos", "abstract": "We present an audio-visual multimodal approach for the task of zeroshot\nlearning (ZSL) for classification and retrieval of videos. ZSL has been studied\nextensively in the recent past but has primarily been limited to visual\nmodality and to images. We demonstrate that both audio and visual modalities\nare important for ZSL for videos. Since a dataset to study the task is\ncurrently not available, we also construct an appropriate multimodal dataset\nwith 33 classes containing 156,416 videos, from an existing large scale audio\nevent dataset. We empirically show that the performance improves by adding\naudio modality for both tasks of zeroshot classification and retrieval, when\nusing multimodal extensions of embedding learning methods. We also propose a\nnovel method to predict the `dominant' modality using a jointly learned\nmodality attention network. We learn the attention in a semi-supervised setting\nand thus do not require any additional explicit labelling for the modalities.\nWe provide qualitative validation of the modality specific attention, which\nalso successfully generalizes to unseen test classes.", "published": "2019-10-19 09:39:28", "link": "http://arxiv.org/abs/1910.08732v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Label-efficient audio classification through multitask learning and\n  self-supervision", "abstract": "While deep learning has been incredibly successful in modeling tasks with\nlarge, carefully curated labeled datasets, its application to problems with\nlimited labeled data remains a challenge. The aim of the present work is to\nimprove the label efficiency of large neural networks operating on audio data\nthrough a combination of multitask learning and self-supervised learning on\nunlabeled data. We trained an end-to-end audio feature extractor based on\nWaveNet that feeds into simple, yet versatile task-specific neural networks. We\ndescribe several easily implemented self-supervised learning tasks that can\noperate on any large, unlabeled audio corpus. We demonstrate that, in scenarios\nwith limited labeled training data, one can significantly improve the\nperformance of three different supervised classification tasks individually by\nup to 6% through simultaneous training with these additional self-supervised\ntasks. We also show that incorporating data augmentation into our multitask\nsetting leads to even further gains in performance.", "published": "2019-10-19 00:58:30", "link": "http://arxiv.org/abs/1910.12587v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
