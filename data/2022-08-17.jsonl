{"title": "What Artificial Neural Networks Can Tell Us About Human Language\n  Acquisition", "abstract": "Rapid progress in machine learning for natural language processing has the\npotential to transform debates about how humans learn language. However, the\nlearning environments and biases of current artificial learners and humans\ndiverge in ways that weaken the impact of the evidence obtained from learning\nsimulations. For example, today's most effective neural language models are\ntrained on roughly one thousand times the amount of linguistic data available\nto a typical child. To increase the relevance of learnability results from\ncomputational models, we need to train model learners without significant\nadvantages over humans. If an appropriate model successfully acquires some\ntarget linguistic knowledge, it can provide a proof of concept that the target\nis learnable in a hypothesized human learning scenario. Plausible model\nlearners will enable us to carry out experimental manipulations to make causal\ninferences about variables in the learning environment, and to rigorously test\npoverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in\nhumans on the basis of speculations about learnability. Comparable experiments\nwill never be possible with human subjects due to practical and ethical\nconsiderations, making model learners an indispensable resource. So far,\nattempts to deprive current models of unfair advantages obtain sub-human\nresults for key grammatical behaviors such as acceptability judgments. But\nbefore we can justifiably conclude that language learning requires more prior\ndomain-specific knowledge than current models possess, we must first explore\nnon-linguistic inputs in the form of multimodal stimuli and multi-agent\ninteraction as ways to make our learners more efficient at learning from\nlimited linguistic input.", "published": "2022-08-17 00:12:37", "link": "http://arxiv.org/abs/2208.07998v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeeperDive: The Unreasonable Effectiveness of Weak Supervision in\n  Document Understanding A Case Study in Collaboration with UiPath Inc", "abstract": "Weak supervision has been applied to various Natural Language Understanding\ntasks in recent years. Due to technical challenges with scaling weak\nsupervision to work on long-form documents, spanning up to hundreds of pages,\napplications in the document understanding space have been limited. At Lexion,\nwe built a weak supervision-based system tailored for long-form (10-200 pages\nlong) PDF documents. We use this platform for building dozens of language\nunderstanding models and have applied it successfully to various domains, from\ncommercial agreements to corporate formation documents.\n  In this paper, we demonstrate the effectiveness of supervised learning with\nweak supervision in a situation with limited time, workforce, and training\ndata. We built 8 high quality machine learning models in the span of one week,\nwith the help of a small team of just 3 annotators working with a dataset of\nunder 300 documents. We share some details about our overall architecture, how\nwe utilize weak supervision, and what results we are able to achieve. We also\ninclude the dataset for researchers who would like to experiment with alternate\napproaches or refine ours.\n  Furthermore, we shed some light on the additional complexities that arise\nwhen working with poorly scanned long-form documents in PDF format, and some of\nthe techniques that help us achieve state-of-the-art performance on such data.", "published": "2022-08-17 00:28:07", "link": "http://arxiv.org/abs/2208.08000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Named Entity Recognition with Entity-level Prototypical Network\n  Enhanced by Dispersedly Distributed Prototypes", "abstract": "Few-shot named entity recognition (NER) enables us to build a NER system for\na new domain using very few labeled examples. However, existing prototypical\nnetworks for this task suffer from roughly estimated label dependency and\nclosely distributed prototypes, thus often causing misclassifications. To\naddress the above issues, we propose EP-Net, an Entity-level Prototypical\nNetwork enhanced by dispersedly distributed prototypes. EP-Net builds\nentity-level prototypes and considers text spans to be candidate entities, so\nit no longer requires the label dependency. In addition, EP-Net trains the\nprototypes from scratch to distribute them dispersedly and aligns spans to\nprototypes in the embedding space using a space projection. Experimental\nresults on two evaluation tasks and the Few-NERD settings demonstrate that\nEP-Net consistently outperforms the previous strong models in terms of overall\nperformance. Extensive analyses further validate the effectiveness of EP-Net.", "published": "2022-08-17 02:09:15", "link": "http://arxiv.org/abs/2208.08023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Context-Aware Approach for Textual Adversarial Attack through\n  Probability Difference Guided Beam Search", "abstract": "Textual adversarial attacks expose the vulnerabilities of text classifiers\nand can be used to improve their robustness. Existing context-aware methods\nsolely consider the gold label probability and use the greedy search when\nsearching an attack path, often limiting the attack efficiency. To tackle these\nissues, we propose PDBS, a context-aware textual adversarial attack model using\nProbability Difference guided Beam Search. The probability difference is an\noverall consideration of all class label probabilities, and PDBS uses it to\nguide the selection of attack paths. In addition, PDBS uses the beam search to\nfind a successful attack path, thus avoiding suffering from limited search\nspace. Extensive experiments and human evaluation demonstrate that PDBS\noutperforms previous best models in a series of evaluation metrics, especially\nbringing up to a +19.5% attack success rate. Ablation studies and qualitative\nanalyses further confirm the efficiency of PDBS.", "published": "2022-08-17 02:19:12", "link": "http://arxiv.org/abs/2208.08029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation", "abstract": "This paper introduces a novel Self-supervised Fine-grained Dialogue\nEvaluation framework (SelF-Eval). The core idea is to model the correlation\nbetween turn quality and the entire dialogue quality. We first propose a novel\nautomatic data construction method that can automatically assign fine-grained\nscores for arbitrarily dialogue data. Then we train \\textbf{SelF-Eval} with a\nmulti-level contrastive learning schema which helps to distinguish different\nscore levels. Experimental results on multiple benchmarks show that SelF-Eval\nis highly consistent with human evaluations and better than the\nstate-of-the-art models. We give a detailed analysis of the experiments in this\npaper. Our code is available on GitHub.", "published": "2022-08-17 06:12:09", "link": "http://arxiv.org/abs/2208.08094v5", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for\n  Curriculum Data Augmentation", "abstract": "Curriculum Data Augmentation (CDA) improves neural models by presenting\nsynthetic data with increasing difficulties from easy to hard. However,\ntraditional CDA simply treats the ratio of word perturbation as the difficulty\nmeasure and goes through the curriculums only once. This paper presents\n\\textbf{PCC}: \\textbf{P}araphrasing with Bottom-k Sampling and \\textbf{C}yclic\nLearning for \\textbf{C}urriculum Data Augmentation, a novel CDA framework via\nparaphrasing, which exploits the textual paraphrase similarity as the\ncurriculum difficulty measure. We propose a curriculum-aware paraphrase\ngeneration module composed of three units: a paraphrase candidate generator\nwith bottom-k sampling, a filtering mechanism and a difficulty measure. We also\npropose a cyclic learning strategy that passes through the curriculums multiple\ntimes. The bottom-k sampling is proposed to generate super-hard instances for\nthe later curriculums. Experimental results on few-shot text classification as\nwell as dialogue generation indicate that PCC surpasses competitive baselines.\nHuman evaluation and extensive case studies indicate that bottom-k sampling\neffectively generates super-hard instances, and PCC significantly improves the\nbaseline dialogue agent.", "published": "2022-08-17 06:56:32", "link": "http://arxiv.org/abs/2208.08110v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differential Privacy in Natural Language Processing: The Story So Far", "abstract": "As the tide of Big Data continues to influence the landscape of Natural\nLanguage Processing (NLP), the utilization of modern NLP methods has grounded\nitself in this data, in order to tackle a variety of text-based tasks. These\nmethods without a doubt can include private or otherwise personally\nidentifiable information. As such, the question of privacy in NLP has gained\nfervor in recent years, coinciding with the development of new\nPrivacy-Enhancing Technologies (PETs). Among these PETs, Differential Privacy\nboasts several desirable qualities in the conversation surrounding data\nprivacy. Naturally, the question becomes whether Differential Privacy is\napplicable in the largely unstructured realm of NLP. This topic has sparked\nnovel research, which is unified in one basic goal: how can one adapt\nDifferential Privacy to NLP methods? This paper aims to summarize the\nvulnerabilities addressed by Differential Privacy, the current thinking, and\nabove all, the crucial next steps that must be considered.", "published": "2022-08-17 08:15:44", "link": "http://arxiv.org/abs/2208.08140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Compositionality with Formal Languages", "abstract": "Recombining known primitive concepts into larger novel combinations is a\nquintessentially human cognitive capability. Whether large neural models in NLP\ncan acquire this ability while learning from data is an open question. In this\npaper, we investigate this problem from the perspective of formal languages. We\nuse deterministic finite-state transducers to make an unbounded number of\ndatasets with controllable properties governing compositionality. By randomly\nsampling over many transducers, we explore which of their properties contribute\nto learnability of a compositional relation by a neural network. We find that\nthe models either learn the relations completely or not at all. The key is\ntransition coverage, setting a soft learnability limit at 400 examples per\ntransition.", "published": "2022-08-17 10:03:18", "link": "http://arxiv.org/abs/2208.08195v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization\n  Framework based on Semantic Blocks", "abstract": "Unsupervised summarization methods have achieved remarkable results by\nincorporating representations from pre-trained language models. However,\nexisting methods fail to consider efficiency and effectiveness at the same time\nwhen the input document is extremely long. To tackle this problem, in this\npaper, we proposed an efficient Coarse-to-Fine Facet-Aware Ranking (C2F-FAR)\nframework for unsupervised long document summarization, which is based on the\nsemantic block. The semantic block refers to continuous sentences in the\ndocument that describe the same facet. Specifically, we address this problem by\nconverting the one-step ranking method into the hierarchical multi-granularity\ntwo-stage ranking. In the coarse-level stage, we propose a new segment\nalgorithm to split the document into facet-aware semantic blocks and then\nfilter insignificant blocks. In the fine-level stage, we select salient\nsentences in each block and then extract the final summary from selected\nsentences. We evaluate our framework on four long document summarization\ndatasets: Gov-Report, BillSum, arXiv, and PubMed. Our C2F-FAR can achieve new\nstate-of-the-art unsupervised summarization results on Gov-Report and BillSum.\nIn addition, our method speeds up 4-28 times more than previous\nmethods.\\footnote{\\url{https://github.com/xnliang98/c2f-far}}", "published": "2022-08-17 12:18:36", "link": "http://arxiv.org/abs/2208.08253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction", "abstract": "Target-oriented Opinion Words Extraction (TOWE) is a fine-grained sentiment\nanalysis task that aims to extract the corresponding opinion words of a given\nopinion target from the sentence. Recently, deep learning approaches have made\nremarkable progress on this task. Nevertheless, the TOWE task still suffers\nfrom the scarcity of training data due to the expensive data annotation\nprocess. Limited labeled data increase the risk of distribution shift between\ntest data and training data. In this paper, we propose exploiting massive\nunlabeled data to reduce the risk by increasing the exposure of the model to\nvarying distribution shifts. Specifically, we propose a novel Multi-Grained\nConsistency Regularization (MGCR) method to make use of unlabeled data and\ndesign two filters specifically for TOWE to filter noisy data at different\ngranularity. Extensive experimental results on four TOWE benchmark datasets\nindicate the superiority of MGCR compared with current state-of-the-art\nmethods. The in-depth analysis also demonstrates the effectiveness of the\ndifferent-granularity filters. Our codes are available at\nhttps://github.com/TOWESSL/TOWESSL.", "published": "2022-08-17 13:19:26", "link": "http://arxiv.org/abs/2208.08280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Embeddings for Text", "abstract": "We propose a new kind of embedding for natural language text that deeply\nrepresents semantic meaning. Standard text embeddings use the outputs from\nhidden layers of a pretrained language model. In our method, we let a language\nmodel learn from the text and then literally pick its brain, taking the actual\nweights of the model's neurons to generate a vector. We call this\nrepresentation of the text a neural embedding. We confirm the ability of this\nrepresentation to reflect semantics of the text by an analysis of its behavior\non several datasets, and by a comparison of neural embedding with state of the\nart sentence embeddings.", "published": "2022-08-17 16:26:13", "link": "http://arxiv.org/abs/2208.08386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Medication Changes in Clinical Narratives using Pre-trained\n  Language Models", "abstract": "An accurate and detailed account of patient medications, including medication\nchanges within the patient timeline, is essential for healthcare providers to\nprovide appropriate patient care. Healthcare providers or the patients\nthemselves may initiate changes to patient medication. Medication changes take\nmany forms, including prescribed medication and associated dosage modification.\nThese changes provide information about the overall health of the patient and\nthe rationale that led to the current care. Future care can then build on the\nresulting state of the patient. This work explores the automatic extraction of\nmedication change information from free-text clinical notes. The Contextual\nMedication Event Dataset (CMED) is a corpus of clinical notes with annotations\nthat characterize medication changes through multiple change-related\nattributes, including the type of change (start, stop, increase, etc.),\ninitiator of the change, temporality, change likelihood, and negation. Using\nCMED, we identify medication mentions in clinical text and propose three novel\nhigh-performing BERT-based systems that resolve the annotated medication change\ncharacteristics. We demonstrate that our proposed systems improve medication\nchange classification performance over the initial work exploring CMED.", "published": "2022-08-17 17:22:48", "link": "http://arxiv.org/abs/2208.08417v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoMent: An Emotion Annotated Mental Health Corpus from two South Asian\n  Countries", "abstract": "People often utilise online media (e.g., Facebook, Reddit) as a platform to\nexpress their psychological distress and seek support. State-of-the-art NLP\ntechniques demonstrate strong potential to automatically detect mental health\nissues from text. Research suggests that mental health issues are reflected in\nemotions (e.g., sadness) indicated in a person's choice of language. Therefore,\nwe developed a novel emotion-annotated mental health corpus (EmoMent),\nconsisting of 2802 Facebook posts (14845 sentences) extracted from two South\nAsian countries - Sri Lanka and India. Three clinical psychology postgraduates\nwere involved in annotating these posts into eight categories, including\n'mental illness' (e.g., depression) and emotions (e.g., 'sadness', 'anger').\nEmoMent corpus achieved 'very good' inter-annotator agreement of 98.3% (i.e. %\nwith two or more agreement) and Fleiss' Kappa of 0.82. Our RoBERTa based models\nachieved an F1 score of 0.76 and a macro-averaged F1 score of 0.77 for the\nfirst task (i.e. predicting a mental health condition from a post) and the\nsecond task (i.e. extent of association of relevant posts with the categories\ndefined in our taxonomy), respectively.", "published": "2022-08-17 18:59:36", "link": "http://arxiv.org/abs/2208.08486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Encoder for Social Science", "abstract": "High-quality text data has become an important data source for social\nscientists. We have witnessed the success of pretrained deep neural network\nmodels, such as BERT and RoBERTa, in recent social science research. In this\npaper, we propose a compact pretrained deep neural network, Transformer Encoder\nfor Social Science (TESS), explicitly designed to tackle text processing tasks\nin social science research. Using two validation tests, we demonstrate that\nTESS outperforms BERT and RoBERTa by 16.7% on average when the number of\ntraining samples is limited (<1,000 training instances). The results display\nthe superiority of TESS over BERT and RoBERTa on social science text processing\ntasks. Lastly, we discuss the limitation of our model and present advice for\nfuture researchers.", "published": "2022-08-17 01:01:25", "link": "http://arxiv.org/abs/2208.08005v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EGCR: Explanation Generation for Conversational Recommendation", "abstract": "Growing attention has been paid in Conversational Recommendation System\n(CRS), which works as a conversation-based and recommendation task-oriented\ntool to provide items of interest and explore user preference. However,\nexisting work in CRS fails to explicitly show the reasoning logic to users and\nthe whole CRS still remains a black box. Therefore we propose a novel\nend-to-end framework named Explanation Generation for Conversational\nRecommendation (EGCR) based on generating explanations for conversational\nagents to explain why they make the action. EGCR incorporates user reviews to\nenhance the item representation and increase the informativeness of the whole\nconversation. To the best of our knowledge, this is the first framework for\nexplainable conversational recommendation on real-world datasets. Moreover, we\nevaluate EGCR on one benchmark conversational recommendation datasets and\nachieve better performance on both recommendation accuracy and conversation\nquality than other state-of-the art models. Finally, extensive experiments\ndemonstrate that generated explanations are not only having high quality and\nexplainability, but also making CRS more trustworthy. We will make our code\navailable to contribute to the CRS community", "published": "2022-08-17 02:30:41", "link": "http://arxiv.org/abs/2208.08035v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Sequence Tagging based Framework for Few-Shot Relation Extraction", "abstract": "Relation Extraction (RE) refers to extracting the relation triples in the\ninput text. Existing neural work based systems for RE rely heavily on manually\nlabeled training data, but there are still a lot of domains where sufficient\nlabeled data does not exist. Inspired by the distance-based few-shot named\nentity recognition methods, we put forward the definition of the few-shot RE\ntask based on the sequence tagging joint extraction approaches, and propose a\nfew-shot RE framework for the task. Besides, we apply two actual sequence\ntagging models to our framework (called Few-shot TPLinker and Few-shot BiTT),\nand achieves solid results on two few-shot RE tasks constructed from a public\ndataset.", "published": "2022-08-17 03:54:22", "link": "http://arxiv.org/abs/2208.08053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constrained Few-Shot Learning: Human-Like Low Sample Complexity Learning\n  and Non-Episodic Text Classification", "abstract": "Few-shot learning (FSL) is an emergent paradigm of learning that attempts to\nlearn to reason with low sample complexity to mimic the way humans learn,\ngeneralise and extrapolate from only a few seen examples. While FSL attempts to\nmimic these human characteristics, fundamentally, the task of FSL as\nconventionally formulated using meta-learning with episodic-based training does\nnot in actuality align with how humans acquire and reason with knowledge. FSL\nwith episodic training, while only requires $K$ instances of each test class,\nstill requires a large number of labelled training instances from disjoint\nclasses. In this paper, we introduce the novel task of constrained few-shot\nlearning (CFSL), a special case of FSL where $M$, the number of instances of\neach training class is constrained such that $M \\leq K$ thus applying a similar\nrestriction during FSL training and test. We propose a method for CFSL\nleveraging Cat2Vec using a novel categorical contrastive loss inspired by\ncognitive theories such as fuzzy trace theory and prototype theory.", "published": "2022-08-17 06:05:41", "link": "http://arxiv.org/abs/2208.08089v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Attention for Vision-and-Language Tasks", "abstract": "Attention mechanism has been used as an important component across\nVision-and-Language(VL) tasks in order to bridge the semantic gap between\nvisual and textual features. While attention has been widely used in VL tasks,\nit has not been examined the capability of different attention alignment\ncalculation in bridging the semantic gap between visual and textual clues. In\nthis research, we conduct a comprehensive analysis on understanding the role of\nattention alignment by looking into the attention score calculation methods and\ncheck how it actually represents the visual region's and textual token's\nsignificance for the global assessment. We also analyse the conditions which\nattention score calculation mechanism would be more (or less) interpretable,\nand which may impact the model performance on three different VL tasks,\nincluding visual question answering, text-to-image generation, text-and-image\nmatching (both sentence and image retrieval). Our analysis is the first of its\nkind and provides useful insights of the importance of each attention alignment\nscore calculation when applied at the training phase of VL tasks, commonly\nignored in attention-based cross modal models, and/or pretrained models. Our\ncode is available at: https://github.com/adlnlp/Attention_VL", "published": "2022-08-17 06:45:07", "link": "http://arxiv.org/abs/2208.08104v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Long Documents with Different Position-Aware Attentions", "abstract": "Despite several successes in document understanding, the practical task for\nlong document understanding is largely under-explored due to several challenges\nin computation and how to efficiently absorb long multimodal input. Most\ncurrent transformer-based approaches only deal with short documents and employ\nsolely textual information for attention due to its prohibitive computation and\nmemory limit. To address those issues in long document understanding, we\nexplore different approaches in handling 1D and new 2D position-aware attention\nwith essentially shortened context. Experimental results show that our proposed\nmodels have advantages for this task based on various evaluation metrics.\nFurthermore, our model makes changes only to the attention and thus can be\neasily adapted to any transformer-based architecture.", "published": "2022-08-17 10:13:15", "link": "http://arxiv.org/abs/2208.08201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Role of Negative Precedent in Legal Outcome Prediction", "abstract": "Every legal case sets a precedent by developing the law in one of the\nfollowing two ways. It either expands its scope, in which case it sets positive\nprecedent, or it narrows it, in which case it sets negative precedent. Legal\noutcome prediction, the prediction of positive outcome, is an increasingly\npopular task in AI. In contrast, we turn our focus to negative outcomes here,\nand introduce a new task of negative outcome prediction. We discover an\nasymmetry in existing models' ability to predict positive and negative\noutcomes. Where the state-of-the-art outcome prediction model we used predicts\npositive outcomes at 75.06 F1, it predicts negative outcomes at only 10.09 F1,\nworse than a random baseline. To address this performance gap, we develop two\nnew models inspired by the dynamics of a court process. Our first model\nsignificantly improves positive outcome prediction score to 77.15 F1 and our\nsecond model more than doubles the negative outcome prediction performance to\n24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals\nthat there is still much room for improvement for outcome prediction models.", "published": "2022-08-17 11:12:50", "link": "http://arxiv.org/abs/2208.08225v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Summarizing Patients Problems from Hospital Progress Notes Using\n  Pre-trained Sequence-to-Sequence Models", "abstract": "Automatically summarizing patients' main problems from daily progress notes\nusing natural language processing methods helps to battle against information\nand cognitive overload in hospital settings and potentially assists providers\nwith computerized diagnostic decision support. Problem list summarization\nrequires a model to understand, abstract, and generate clinical documentation.\nIn this work, we propose a new NLP task that aims to generate a list of\nproblems in a patient's daily care plan using input from the provider's\nprogress notes during hospitalization. We investigate the performance of T5 and\nBART, two state-of-the-art seq2seq transformer architectures, in solving this\nproblem. We provide a corpus built on top of progress notes from publicly\navailable electronic health record progress notes in the Medical Information\nMart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain\ntext, and we experiment with a data augmentation method and a domain adaptation\npre-training method to increase exposure to medical vocabulary and knowledge.\nEvaluation methods include ROUGE, BERTScore, cosine similarity on sentence\nembedding, and F-score on medical concepts. Results show that T5 with domain\nadaptive pre-training achieves significant performance gains compared to a\nrule-based system and general domain pre-trained language models, indicating a\npromising direction for tackling the problem summarization task.", "published": "2022-08-17 17:07:35", "link": "http://arxiv.org/abs/2208.08408v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ShortcutLens: A Visual Analytics Approach for Exploring Shortcuts in\n  Natural Language Understanding Dataset", "abstract": "Benchmark datasets play an important role in evaluating Natural Language\nUnderstanding (NLU) models. However, shortcuts -- unwanted biases in the\nbenchmark datasets -- can damage the effectiveness of benchmark datasets in\nrevealing models' real capabilities. Since shortcuts vary in coverage,\nproductivity, and semantic meaning, it is challenging for NLU experts to\nsystematically understand and avoid them when creating benchmark datasets. In\nthis paper, we develop a visual analytics system, ShortcutLens, to help NLU\nexperts explore shortcuts in NLU benchmark datasets. The system allows users to\nconduct multi-level exploration of shortcuts. Specifically, Statistics View\nhelps users grasp the statistics such as coverage and productivity of shortcuts\nin the benchmark dataset. Template View employs hierarchical and interpretable\ntemplates to summarize different types of shortcuts. Instance View allows users\nto check the corresponding instances covered by the shortcuts. We conduct case\nstudies and expert interviews to evaluate the effectiveness and usability of\nthe system. The results demonstrate that ShortcutLens supports users in gaining\na better understanding of benchmark dataset issues through shortcuts, inspiring\nthem to create challenging and pertinent benchmark datasets.", "published": "2022-08-17 01:24:55", "link": "http://arxiv.org/abs/2208.08010v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "The Conversational Short-phrase Speaker Diarization (CSSD) Task:\n  Dataset, Evaluation Metric and Baselines", "abstract": "The conversation scenario is one of the most important and most challenging\nscenarios for speech processing technologies because people in conversation\nrespond to each other in a casual style. Detecting the speech activities of\neach person in a conversation is vital to downstream tasks, like natural\nlanguage processing, machine translation, etc. People refer to the detection\ntechnology of \"who speak when\" as speaker diarization (SD). Traditionally,\ndiarization error rate (DER) has been used as the standard evaluation metric of\nSD systems for a long time. However, DER fails to give enough importance to\nshort conversational phrases, which are short but important on the semantic\nlevel. Also, a carefully and accurately manually-annotated testing dataset\nsuitable for evaluating the conversational SD technologies is still unavailable\nin the speech community. In this paper, we design and describe the\nConversational Short-phrases Speaker Diarization (CSSD) task, which consists of\ntraining and testing datasets, evaluation metric and baselines. In the dataset\naspect, despite the previously open-sourced 180-hour conversational\nMagicData-RAMC dataset, we prepare an individual 20-hour conversational speech\ntest dataset with carefully and artificially verified speakers timestamps\nannotations for the CSSD task. In the metric aspect, we design the new\nconversational DER (CDER) evaluation metric, which calculates the SD accuracy\nat the utterance level. In the baseline aspect, we adopt a commonly used\nmethod: Variational Bayes HMM x-vector system, as the baseline of the CSSD\ntask. Our evaluation metric is publicly available at\nhttps://github.com/SpeechClub/CDER_Metric.", "published": "2022-08-17 03:26:23", "link": "http://arxiv.org/abs/2208.08042v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NECE: Narrative Event Chain Extraction Toolkit", "abstract": "To understand a narrative, it is essential to comprehend the temporal event\nflows, especially those associated with main characters; however, this can be\nchallenging with lengthy and unstructured narrative texts. To address this, we\nintroduce NECE, an open-access, document-level toolkit that automatically\nextracts and aligns narrative events in the temporal order of their occurrence.\nThrough extensive evaluations, we show the high quality of the NECE toolkit and\ndemonstrates its downstream application in analyzing narrative bias regarding\ngender. We also openly discuss the shortcomings of the current approach, and\npotential of leveraging generative models in future works. Lastly the NECE\ntoolkit includes both a Python library and a user-friendly web interface, which\noffer equal access to professionals and layman audience alike, to visualize\nevent chain, obtain narrative flows, or study narrative bias.", "published": "2022-08-17 04:30:58", "link": "http://arxiv.org/abs/2208.08063v5", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems", "abstract": "Vision-Transformers are widely used in various vision tasks. Meanwhile, there\nis another line of works starting with the MLP-mixer trying to achieve similar\nperformance using mlp-based architectures. Interestingly, until now those\nmlp-based architectures have not been adapted for NLP tasks. Additionally,\nuntil now, mlp-based architectures have failed to achieve state-of-the-art\nperformance in vision tasks. In this paper, we analyze the expressive power of\nmlp-based architectures in modeling dependencies between multiple different\ninputs simultaneously, and show an exponential gap between the attention and\nthe mlp-based mechanisms. Our results suggest a theoretical explanation for the\nmlp inability to compete with attention-based mechanisms in NLP problems, they\nalso suggest that the performance gap in vision tasks may be due to the mlp\nrelative weakness in modeling dependencies between multiple different\nlocations, and that combining smart input permutations with mlp architectures\nmay not be enough to close the performance gap alone.", "published": "2022-08-17 09:59:22", "link": "http://arxiv.org/abs/2208.08191v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ask Question First for Enhancing Lifelong Language Learning", "abstract": "Lifelong language learning aims to stream learning NLP tasks while retaining\nknowledge of previous tasks. Previous works based on the language model and\nfollowing data-free constraint approaches have explored formatting all data as\n\"begin token (\\textit{B}) + context (\\textit{C}) + question (\\textit{Q}) +\nanswer (\\textit{A})\" for different tasks. However, they still suffer from\ncatastrophic forgetting and are exacerbated when the previous task's pseudo\ndata is insufficient for the following reasons: (1) The model has difficulty\ngenerating task-corresponding pseudo data, and (2) \\textit{A} is prone to error\nwhen \\textit{A} and \\textit{C} are separated by \\textit{Q} because the\ninformation of the \\textit{C} is diminished before generating \\textit{A}.\nTherefore, we propose the Ask Question First and Replay Question (AQF-RQ),\nincluding a novel data format \"\\textit{BQCA}\" and a new training task to train\npseudo questions of previous tasks. Experimental results demonstrate that\nAQF-RQ makes it easier for the model to generate more pseudo data that match\ncorresponding tasks, and is more robust to both sufficient and insufficient\npseudo-data when the task boundary is both clear and unclear. AQF-RQ can\nachieve only 0.36\\% lower performance than multi-task learning.", "published": "2022-08-17 15:58:33", "link": "http://arxiv.org/abs/2208.08367v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Computational Interface to Translate Strategic Intent from\n  Unstructured Language in a Low-Data Setting", "abstract": "Many real-world tasks involve a mixed-initiative setup, wherein humans and AI\nsystems collaboratively perform a task. While significant work has been\nconducted towards enabling humans to specify, through language, exactly how an\nagent should complete a task (i.e., low-level specification), prior work lacks\non interpreting the high-level strategic intent of the human commanders.\nParsing strategic intent from language will allow autonomous systems to\nindependently operate according to the user's plan without frequent guidance or\ninstruction. In this paper, we build a computational interface capable of\ntranslating unstructured language strategies into actionable intent in the form\nof goals and constraints. Leveraging a game environment, we collect a dataset\nof over 1000 examples, mapping language strategies to the corresponding goals\nand constraints, and show that our model, trained on this dataset,\nsignificantly outperforms human interpreters in inferring strategic intent\n(i.e., goals and constraints) from language (p < 0.05). Furthermore, we show\nthat our model (125M parameters) significantly outperforms ChatGPT for this\ntask (p < 0.05) in a low-data setting.", "published": "2022-08-17 16:11:07", "link": "http://arxiv.org/abs/2208.08374v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "On the evolution of research in hypersonics: application of natural\n  language processing and machine learning", "abstract": "Research and development in hypersonics have progressed significantly in\nrecent years, with various military and commercial applications being\ndemonstrated increasingly. Public and private organizations in several\ncountries have been investing in hypersonics, with the aim to overtake their\ncompetitors and secure/improve strategic advantage and deterrence. For these\norganizations, being able to identify emerging technologies in a timely and\nreliable manner is paramount. Recent advances in information technology have\nmade it possible to analyze large amounts of data, extract hidden patterns, and\nprovide decision-makers with new insights. In this study, we focus on\nscientific publications about hypersonics within the period of 2000-2020, and\nemploy natural language processing and machine learning to characterize the\nresearch landscape by identifying 12 key latent research themes and analyzing\ntheir temporal evolution. Our publication similarity analysis revealed patterns\nthat are indicative of cycles during two decades of research. The study offers\na comprehensive analysis of the research field and the fact that the research\nthemes are algorithmically extracted removes subjectivity from the exercise and\nenables consistent comparisons between topics and between time intervals.", "published": "2022-08-17 19:57:31", "link": "http://arxiv.org/abs/2208.08507v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Robustness of End-to-End Neural Models for Automatic Speech\n  Recognition", "abstract": "We investigate robustness properties of pre-trained neural models for\nautomatic speech recognition. Real life data in machine learning is usually\nvery noisy and almost never clean, which can be attributed to various factors\ndepending on the domain, e.g. outliers, random noise and adversarial noise.\nTherefore, the models we develop for various tasks should be robust to such\nkinds of noisy data, which led to the thriving field of robust machine\nlearning. We consider this important issue in the setting of automatic speech\nrecognition. With the increasing popularity of pre-trained models, it's an\nimportant question to analyze and understand the robustness of such models to\nnoise. In this work, we perform a robustness analysis of the pre-trained neural\nmodels wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.\nWe use different kinds of noising mechanisms and measure the model performances\nas quantified by the inference time and the standard Word Error Rate metric. We\nalso do an in-depth layer-wise analysis of the wav2vec2 model when injecting\nnoise in between layers, enabling us to predict at a high level what each layer\nlearns. Finally for this model, we visualize the propagation of errors across\nthe layers and compare how it behaves on clean versus noisy data. Our\nexperiments conform the predictions of Pasad et al. [2021] and also raise\ninteresting directions for future work.", "published": "2022-08-17 20:00:54", "link": "http://arxiv.org/abs/2208.08509v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Getting Quechua Closer to Final Users through Knowledge Graphs", "abstract": "Quechua language and Quechua knowledge gather millions of people around the\nworld, especially in several countries in South America. Unfortunately, there\nare only a few resources available to Quechua communities, and they are mainly\nstored in PDF format. In this paper, the Quechua Knowledge Graph is envisioned\nand generated as an effort to get Quechua closer to the Quechua communities,\nresearchers, and technology developers. Currently, there are 553636 triples\nstored in the Quechua Knowledge Graph, which is accessible on the Web,\nretrievable by machines, and curated by users. To showcase the deployment of\nthe Quechua Knowledge Graph, use cases and future work are described.", "published": "2022-08-17 21:21:26", "link": "http://arxiv.org/abs/2208.12608v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Multimodal Lecture Presentations Dataset: Understanding Multimodality in\n  Educational Slides", "abstract": "Lecture slide presentations, a sequence of pages that contain text and\nfigures accompanied by speech, are constructed and presented carefully in order\nto optimally transfer knowledge to students. Previous studies in multimedia and\npsychology attribute the effectiveness of lecture presentations to their\nmultimodal nature. As a step toward developing AI to aid in student learning as\nintelligent teacher assistants, we introduce the Multimodal Lecture\nPresentations dataset as a large-scale benchmark testing the capabilities of\nmachine learning models in multimodal understanding of educational content. Our\ndataset contains aligned slides and spoken language, for 180+ hours of video\nand 9000+ slides, with 10 lecturers from various subjects (e.g., computer\nscience, dentistry, biology). We introduce two research tasks which are\ndesigned as stepping stones towards AI agents that can explain (automatically\ncaptioning a lecture presentation) and illustrate (synthesizing visual figures\nto accompany spoken explanations) educational content. We provide manual\nannotations to help implement these two research tasks and evaluate\nstate-of-the-art models on them. Comparing baselines and human student\nperformances, we find that current models struggle in (1) weak crossmodal\nalignment between slides and spoken text, (2) learning novel visual mediums,\n(3) technical language, and (4) long-range sequences. Towards addressing this\nissue, we also introduce PolyViLT, a multimodal transformer trained with a\nmulti-instance learning loss that is more effective than current approaches. We\nconclude by shedding light on the challenges and opportunities in multimodal\nunderstanding of educational presentations.", "published": "2022-08-17 05:30:18", "link": "http://arxiv.org/abs/2208.08080v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.AI"}
{"title": "HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create\n  Customized Content with Models", "abstract": "Controlling the text generated by language models and customizing the content\nhas been a long-standing challenge. Existing prompting techniques proposed in\npursuit of providing control are task-specific and lack generality; this\nprovides overwhelming choices for non-expert users to find a suitable method\nfor their task. The effort associated with those techniques, such as in writing\nexamples, explanations, instructions, etc. further limits their adoption among\nnon-expert users. In this paper, we propose a simple prompting strategy HELP ME\nTHINK where we encourage GPT3 to help non-expert users by asking a set of\nrelevant questions and leveraging user answers to execute the task. We\ndemonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.\nSpecifically, we focus on tasks that are hard for average humans and require\nsignificant thinking to perform. We hope our work will encourage the\ndevelopment of unconventional ways to harness the power of large language\nmodels.", "published": "2022-08-17 11:20:41", "link": "http://arxiv.org/abs/2208.08232v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions", "abstract": "Bootstrapping from pre-trained language models has been proven to be an\nefficient approach for building vision-language models (VLM) for tasks such as\nimage captioning or visual question answering. However, outputs of these models\nrarely align with user's rationales for specific answers. In order to improve\nthis alignment and reinforce commonsense reasons, we propose a tuning paradigm\nbased on human interactions with machine-generated data. Our ILLUME executes\nthe following loop: Given an image-question-answer prompt, the VLM samples\nmultiple candidate rationales, and a human critic provides feedback via\npreference selection, used for fine-tuning. This loop increases the training\ndata and gradually carves out the VLM's rationalization capabilities that are\naligned with human intent. Our exhaustive experiments demonstrate that ILLUME\nis competitive with standard supervised finetuning while using significantly\nfewer training data and only requiring minimal feedback.", "published": "2022-08-17 11:41:43", "link": "http://arxiv.org/abs/2208.08241v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Disentangled Speaker Representation Learning via Mutual Information\n  Minimization", "abstract": "Domain mismatch problem caused by speaker-unrelated feature has been a major\ntopic in speaker recognition. In this paper, we propose an explicit\ndisentanglement framework to unravel speaker-relevant features from\nspeaker-unrelated features via mutual information (MI) minimization. To achieve\nour goal of minimizing MI between speaker-related and speaker-unrelated\nfeatures, we adopt a contrastive log-ratio upper bound (CLUB), which exploits\nthe upper bound of MI. Our framework is constructed in a 3-stage structure.\nFirst, in the front-end encoder, input speech is encoded into shared initial\nembedding. Next, in the decoupling block, shared initial embedding is split\ninto separate speaker-related and speaker-unrelated embeddings. Finally,\ndisentanglement is conducted by MI minimization in the last stage. Experiments\non Far-Field Speaker Verification Challenge 2022 (FFSVC2022) demonstrate that\nour proposed framework is effective for disentanglement. Also, to utilize\ndomain-unknown datasets containing numerous speakers, we pre-trained the\nfront-end encoder with VoxCeleb datasets. We then fine-tuned the speaker\nembedding model in the disentanglement framework with FFSVC 2022 dataset. The\nexperimental results show that fine-tuning with a disentanglement framework on\na existing pre-trained model is valid and can further improve performance.", "published": "2022-08-17 01:26:22", "link": "http://arxiv.org/abs/2208.08012v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domestic sound event detection by shift consistency mean-teacher\n  training and adversarial domain adaptation", "abstract": "Semi-supervised learning and domain adaptation techniques have drawn\nincreasing attention in the field of domestic sound event detection thanks to\nthe availability of large amounts of unlabeled data and the relative ease to\ngenerate synthetic strongly-labeled data. In a previous work, several\nsemi-supervised learning strategies were designed to boost the performance of a\nmean-teacher model. Namely, these strategies include shift consistency training\n(SCT), interpolation consistency training (ICT), and pseudo-labeling. However,\nadversarial domain adaptation (ADA) did not seem to improve the event detection\naccuracy further when we attempt to compensate for the domain gap between\nsynthetic and real data. In this research, we empirically found that ICT tends\nto pull apart the distributions of synthetic and real data in t-SNE plots.\nTherefore, ICT is abandoned while SCT, in contrast, is applied to train both\nthe student and the teacher models. With these modifications, the system\nsuccessfully integrates with an ADA network, and we achieve 47.2% in the F1\nscore on the DCASE 2020 task 4 dataset, which is 2.1% higher than what was\nreported in the previous work.", "published": "2022-08-17 07:57:12", "link": "http://arxiv.org/abs/2208.08131v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extract fundamental frequency based on CNN combined with PYIN", "abstract": "This paper refers to the extraction of multiple fundamental frequencies\n(multiple F0) based on PYIN, an algorithm for extracting the fundamental\nfrequency (F0) of monophonic music, and a trained convolutional neural networks\n(CNN) model, where a pitch salience function of the input signal is produced to\nestimate the multiple F0. The implementation of these two algorithms and their\ncorresponding advantages and disadvantages are discussed in this article.\nAnalysing the different performance of these two methods, PYIN is applied to\nsupplement the F0 extracted from the trained CNN model to combine the\nadvantages of these two algorithms. For evaluation, four pieces played by two\nviolins are used, and the performance of the models are evaluated accoring to\nthe flatness of the F0 curve extracted. The result shows the combined model\noutperforms the original algorithms when extracting F0 from monophonic music\nand polyphonic music.", "published": "2022-08-17 15:34:54", "link": "http://arxiv.org/abs/2208.08354v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep\n  Learning", "abstract": "The selective fixed-filter active noise control (SFANC) method selecting the\nbest pre-trained control filters for various types of noise can achieve a fast\nresponse time. However, it may lead to large steady-state errors due to\ninaccurate filter selection and the lack of adaptability. In comparison, the\nfiltered-X normalized least-mean-square (FxNLMS) algorithm can obtain lower\nsteady-state errors through adaptive optimization. Nonetheless, its slow\nconvergence has a detrimental effect on dynamic noise attenuation. Therefore,\nthis paper proposes a hybrid SFANC-FxNLMS approach to overcome the adaptive\nalgorithm's slow convergence and provide a better noise reduction level than\nthe SFANC method. A lightweight one-dimensional convolutional neural network\n(1D CNN) is designed to automatically select the most suitable pre-trained\ncontrol filter for each frame of the primary noise. Meanwhile, the FxNLMS\nalgorithm continues to update the coefficients of the chosen pre-trained\ncontrol filter at the sampling rate. Owing to the effective combination of the\ntwo algorithms, experimental results show that the hybrid SFANC-FxNLMS\nalgorithm can achieve a rapid response time, a low noise reduction error, and a\nhigh degree of robustness.", "published": "2022-08-17 05:42:39", "link": "http://arxiv.org/abs/2208.08082v1", "categories": ["eess.SY", "cs.LG", "cs.SD", "cs.SY", "eess.AS"], "primary_category": "eess.SY"}
