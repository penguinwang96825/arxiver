{"title": "How Curiosity can be modeled for a Clickbait Detector", "abstract": "The impact of continually evolving digital technologies and the proliferation of communications and content has now been widely acknowledged to be central to understanding our world. What is less acknowledged is that this is based on the successful arousing of curiosity both at the collective and individual levels. Advertisers, communication professionals and news editors are in constant competition to capture attention of the digital population perennially shifty and distracted. This paper, tries to understand how curiosity works in the digital world by attempting the first ever work done on quantifying human curiosity, basing itself on various theories drawn from humanities and social sciences. Curious communication pushes people to spot, read and click the message from their social feed or any other form of online presentation. Our approach focuses on measuring the strength of the stimulus to generate reader curiosity by using unsupervised and supervised machine learning algorithms, but is also informed by philosophical, psychological, neural and cognitive studies on this topic. Manually annotated news headlines - clickbaits - have been selected for the study, which are known to have drawn huge reader response. A binary classifier was developed based on human curiosity (unlike the work done so far using words and other linguistic features). Our classifier shows an accuracy of 97% . This work is part of the research in computational humanities on digital politics quantifying the emotions of curiosity and outrage on digital media.", "published": "2018-06-11 19:39:42", "link": "http://arxiv.org/abs/1806.04212v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Compression of phase-only holograms with JPEG standard and deep learning", "abstract": "It is a critical issue to reduce the enormous amount of data in the processing, storage and transmission of a hologram in digital format. In photograph compression, the JPEG standard is commonly supported by almost every system and device. It will be favorable if JPEG standard is applicable to hologram compression, with advantages of universal compatibility. However, the reconstructed image from a JPEG compressed hologram suffers from severe quality degradation since some high frequency features in the hologram will be lost during the compression process. In this work, we employ a deep convolutional neural network to reduce the artifacts in a JPEG compressed hologram. Simulation and experimental results reveal that our proposed \"JPEG + deep learning\" hologram compression scheme can achieve satisfactory reconstruction results for a computer-generated phase-only hologram after compression.", "published": "2018-06-11 05:11:58", "link": "http://arxiv.org/abs/1806.03811v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Learning to Decompose and Disentangle Representations for Video Prediction", "abstract": "Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we would intuitively do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.", "published": "2018-06-11 18:12:59", "link": "http://arxiv.org/abs/1806.04166v2", "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Physical Representation-based Predicate Optimization for a Visual Analytics Database", "abstract": "Querying the content of images, video, and other non-textual data sources requires expensive content extraction methods. Modern extraction techniques are based on deep convolutional neural networks (CNNs) and can classify objects within images with astounding accuracy. Unfortunately, these methods are slow: processing a single image can take about 10 milliseconds on modern GPU-based hardware. As massive video libraries become ubiquitous, running a content-based query over millions of video frames is prohibitive.\n  One promising approach to reduce the runtime cost of queries of visual content is to use a hierarchical model, such as a cascade, where simple cases are handled by an inexpensive classifier. Prior work has sought to design cascades that optimize the computational cost of inference by, for example, using smaller CNNs. However, we observe that there are critical factors besides the inference time that dramatically impact the overall query time. Notably, by treating the physical representation of the input image as part of our query optimization---that is, by including image transforms, such as resolution scaling or color-depth reduction, within the cascade---we can optimize data handling costs and enable drastically more efficient classifier cascades.\n  In this paper, we propose Tahoma, which generates and evaluates many potential classifier cascades that jointly optimize the CNN architecture and input data representation. Our experiments on a subset of ImageNet show that Tahoma's input transformations speed up cascades by up to 35 times. We also find up to a 98x speedup over the ResNet50 classifier with no loss in accuracy, and a 280x speedup if some accuracy is sacrificed.", "published": "2018-06-11 20:28:12", "link": "http://arxiv.org/abs/1806.04226v3", "categories": ["cs.DB", "cs.CV"], "primary_category": "cs.DB"}
{"title": "Time-inhomogeneous polynomial processes", "abstract": "Time homogeneous polynomial processes are Markov processes whose moments can be calculated easily through matrix exponentials. In this work, we develop a notion of time inhomogeneous polynomial processes where the coeffiecients of the process may depend on time. A full characterization of this model class is given by means of their semimartingale characteristics. We show that in general, the computation of moments by matrix exponentials is no longer possible. As an alternative we explore a connection to Magnus series for fast numerical approximations.\n  Time-inhomogeneity is important in a number of applications: in term-structure models, this allows a perfect calibration to available prices. In electricity markets, seasonality comes naturally into play and have to be captured by the used models. The model class studied in this work extends existing models, for example Sato processes and time-inhomogeneous affine processes.", "published": "2018-06-11 09:54:53", "link": "http://arxiv.org/abs/1806.03887v1", "categories": ["math.PR", "q-fin.MF"], "primary_category": "math.PR"}
