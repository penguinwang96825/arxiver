{"title": "Supervised Similarity for High-Yield Corporate Bonds with Quantum Cognition Machine Learning", "abstract": "We investigate the application of quantum cognition machine learning (QCML),\na novel paradigm for both supervised and unsupervised learning tasks rooted in\nthe mathematical formalism of quantum theory, to distance metric learning in\ncorporate bond markets. Compared to equities, corporate bonds are relatively\nilliquid and both trade and quote data in these securities are relatively\nsparse. Thus, a measure of distance/similarity among corporate bonds is\nparticularly useful for a variety of practical applications in the trading of\nilliquid bonds, including the identification of similar tradable alternatives,\npricing securities with relatively few recent quotes or trades, and explaining\nthe predictions and performance of ML models based on their training data.\nPrevious research has explored supervised similarity learning based on\nclassical tree-based models in this context; here, we explore the application\nof the QCML paradigm for supervised distance metric learning in the same\ncontext, showing that it outperforms classical tree-based models in high-yield\n(HY) markets, while giving comparable or better performance (depending on the\nevaluation metric) in investment grade (IG) markets.", "published": "2025-02-03 16:28:44", "link": "http://arxiv.org/abs/2502.01495v1", "categories": ["q-fin.ST", "q-fin.CP", "q-fin.RM", "q-fin.TR", "stat.ML"], "primary_category": "q-fin.ST"}
{"title": "Exploratory Utility Maximization Problem with Tsallis Entropy", "abstract": "We study expected utility maximization problem with constant relative risk\naversion utility function in a complete market under the reinforcement learning\nframework. To induce exploration, we introduce the Tsallis entropy regularizer,\nwhich generalizes the commonly used Shannon entropy. Unlike the classical\nMerton's problem, which is always well-posed and admits closed-form solutions,\nwe find that the utility maximization exploratory problem is ill-posed in\ncertain cases, due to over-exploration. With a carefully selected primary\ntemperature function, we investigate two specific examples, for which we fully\ncharacterize their well-posedness and provide semi-closed-form solutions. It is\ninteresting to find that one example has the well-known Gaussian distribution\nas the optimal strategy, while the other features the rare Wigner semicircle\ndistribution, which is equivalent to a scaled Beta distribution. The means of\nthe two optimal exploratory policies coincide with that of the classical\ncounterpart. In addition, we examine the convergence of the value function and\noptimal exploratory strategy as the exploration vanishes. Finally, we design a\nreinforcement learning algorithm and conduct numerical experiments to\ndemonstrate the advantages of reinforcement learning.", "published": "2025-02-03 11:39:12", "link": "http://arxiv.org/abs/2502.01269v1", "categories": ["cs.LG", "q-fin.MF"], "primary_category": "cs.LG"}
{"title": "Regression and Forecasting of U.S. Stock Returns Based on LSTM", "abstract": "This paper analyses the investment returns of three stock sectors, Manuf,\nHitec, and Other, in the U.S. stock market, based on the Fama-French\nthree-factor model, the Carhart four-factor model, and the Fama-French\nfive-factor model, in order to test the validity of the Fama-French\nthree-factor model, the Carhart four-factor model, and the Fama-French\nfive-factor model for the three sectors of the market. French five-factor model\nfor the three sectors of the market. Also, the LSTM model is used to explore\nthe additional factors affecting stock returns. The empirical results show that\nthe Fama-French five-factor model has better validity for the three segments of\nthe market under study, and the LSTM model has the ability to capture the\nfactors affecting the returns of certain industries, and can better regress and\npredict the stock returns of the relevant industries. Keywords- Fama-French\nmodel; Carhart model; Factor model; LSTM model.", "published": "2025-02-03 19:26:44", "link": "http://arxiv.org/abs/2502.05210v2", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "An End-To-End LLM Enhanced Trading System", "abstract": "This project introduces an end-to-end trading system that leverages Large\nLanguage Models (LLMs) for real-time market sentiment analysis. By synthesizing\ndata from financial news and social media, the system integrates\nsentiment-driven insights with technical indicators to generate actionable\ntrading signals. FinGPT serves as the primary model for sentiment analysis,\nensuring domain-specific accuracy, while Kubernetes is used for scalable and\nefficient deployment.", "published": "2025-02-03 17:57:04", "link": "http://arxiv.org/abs/2502.01574v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with\n  Decision Tree Branching", "abstract": "The goal of conversational product search (CPS) is to develop an intelligent,\nchat-based shopping assistant that can directly interact with customers to\nunderstand shopping intents, ask clarification questions, and find relevant\nproducts. However, training such assistants is hindered mainly due to the lack\nof reliable and large-scale datasets. Prior human-annotated CPS datasets are\nextremely small in size and lack integration with real-world product search\nsystems. We propose a novel approach, TRACER, which leverages large language\nmodels (LLMs) to generate realistic and natural conversations for different\nshopping domains. TRACER's novelty lies in grounding the generation to dialogue\nplans, which are product search trajectories predicted from a decision tree\nmodel, that guarantees relevant product discovery in the shortest number of\nsearch conditions. We also release the first target-oriented CPS dataset Wizard\nof Shopping (WoS), containing highly natural and coherent conversations (3.6k)\nfrom three shopping domains. Finally, we demonstrate the quality and\neffectiveness of WoS via human evaluations and downstream tasks.", "published": "2025-02-03 00:27:13", "link": "http://arxiv.org/abs/2502.00969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Hierarchical Merging for Long Document Summarization", "abstract": "Hierarchical Merging is a technique commonly used to summarize very long\ntexts ($>$100K tokens) by breaking down the input into smaller sections,\nsummarizing those sections individually, and then merging or combining those\nsummaries into a final coherent summary. Although it helps address the\nlimitations of large language models (LLMs) with fixed input length\nconstraints, the recursive merging process can amplify LLM hallucinations,\nincreasing the risk of factual inaccuracies. In this paper, we seek to mitigate\nhallucinations by enriching hierarchical merging with context from the source\ndocument. Specifically, we propose different approaches to contextual\naugmentation ranging from \\emph{replacing} intermediate summaries with relevant\ninput context, to \\emph{refining} them while using the context as supporting\nevidence, and \\emph{aligning} them implicitly (via citations) to the input.\nExperimental results on datasets representing legal and narrative domains show\nthat contextual augmentation consistently outperforms zero-shot and\nhierarchical merging baselines for the Llama 3.1 model family. Our analysis\nfurther reveals that refinement methods tend to perform best when paired with\nextractive summarization for identifying relevant input.", "published": "2025-02-03 01:14:31", "link": "http://arxiv.org/abs/2502.00977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised Analogical Learning using Language Models", "abstract": "Large language models have been shown to suffer from reasoning inconsistency\nissues. That is, they fail more in situations unfamiliar to the training data,\neven though exact or very similar reasoning paths exist in more common cases\nthat they can successfully solve. Such observations motivate us to propose\nmethods that encourage models to understand the high-level and abstract\nreasoning processes during training instead of only the final answer. This way,\nmodels can transfer the exact solution to similar cases, regardless of their\nrelevance to the pre-training data distribution. In this work, we propose SAL,\na self-supervised analogical learning framework. SAL mimics the human analogy\nprocess and trains models to explicitly transfer high-quality symbolic\nsolutions from cases that they know how to solve to other rare cases in which\nthey tend to fail more. We show that the resulting models after SAL learning\noutperform base language models on a wide range of reasoning benchmarks, such\nas StrategyQA, GSM8K, and HotpotQA, by 2% to 20%. At the same time, we show\nthat our model is more generalizable and controllable through analytical\nstudies.", "published": "2025-02-03 02:31:26", "link": "http://arxiv.org/abs/2502.00996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately,\nwhich is inefficient in cases where the information required to answer a query\nis localized within the context. We present dynamic context cutoff, a\nhuman-inspired method enabling LLMs to self-terminate processing upon acquiring\nsufficient task-relevant information. Through analysis of model internals, we\ndiscover that specific attention heads inherently encode \"sufficiency signals\"\n- detectable through lightweight classifiers - that predict when critical\ninformation has been processed. This reveals a new efficiency paradigm: models'\ninternal understanding naturally dictates processing needs rather than external\ncompression heuristics. Comprehensive experiments across six QA datasets (up to\n40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B0-70B) demonstrate\n1.33x average token reduction while improving accuracy by 1.3%. Furthermore,\nour method demonstrates better performance with the same rate of token\nreduction compared to other context efficiency methods. Additionally, we\nobserve an emergent scaling phenomenon: while smaller models require require\nprobing for sufficiency detection, larger models exhibit intrinsic\nself-assessment capabilities through prompting.", "published": "2025-02-03 03:38:29", "link": "http://arxiv.org/abs/2502.01025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARA: Parameter-Efficient Fine-tuning with Prompt Aware Representation\n  Adjustment", "abstract": "In the realm of parameter-efficient fine-tuning (PEFT) methods, while options\nlike LoRA are available, there is a persistent demand in the industry for a\nPEFT approach that excels in both efficiency and performance within the context\nof single-backbone multi-tenant applications. This paper introduces a new and\nstraightforward PEFT technique, termed \\underline{P}rompt \\underline{A}ware\n\\underline{R}epresentation \\underline{A}djustment (PARA). The core of our\nproposal is to integrate a lightweight vector generator within each Transformer\nlayer. This generator produces vectors that are responsive to input prompts,\nthereby adjusting the hidden representations accordingly. Our extensive\nexperimentation across diverse tasks has yielded promising results. Firstly,\nthe PARA method has been shown to surpass current PEFT benchmarks in terms of\nperformance, despite having a similar number of adjustable parameters.\nSecondly, it has proven to be more efficient than LoRA in the single-backbone\nmulti-tenant scenario, highlighting its significant potential for industrial\nadoption.", "published": "2025-02-03 04:06:03", "link": "http://arxiv.org/abs/2502.01033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Prefer What They Know: Relative Confidence Estimation\n  via Confidence Preferences", "abstract": "Language models (LMs) should provide reliable confidence estimates to help\nusers detect mistakes in their outputs and defer to human experts when\nnecessary. Asking a language model to assess its confidence (\"Score your\nconfidence from 0-1.\") is a natural way of evaluating its uncertainty. However,\nmodels struggle to provide absolute assessments of confidence (i.e. judging\nconfidence in answering a question independent of other questions) and the\ncoarse-grained scores they produce are not useful for evaluating the\ncorrectness of their answers. We propose relative confidence estimation, where\nwe match up questions against each other and ask the model to make relative\njudgments of confidence (\"Which question are you more confident in answering\ncorrectly?\"). Treating each question as a \"player\" in a series of matchups\nagainst other questions and the model's preferences as match outcomes, we can\nuse rank aggregation methods like Elo rating and Bradley-Terry to translate the\nmodel's confidence preferences into confidence scores. We evaluate relative\nconfidence estimation against absolute confidence estimation and\nself-consistency confidence methods on five state-of-the-art LMs -- GPT-4,\nGPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14\nchallenging STEM, social science, and commonsense reasoning question answering\ntasks. Our results demonstrate that relative confidence estimation consistently\nprovides more reliable confidence scores than absolute confidence estimation,\nwith average gains of 3.5% in selective classification AUC over direct absolute\nconfidence estimation methods and 1.7% over self-consistency approaches across\nall models and datasets.", "published": "2025-02-03 07:43:27", "link": "http://arxiv.org/abs/2502.01126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVE: COntext and VEracity prediction for out-of-context images", "abstract": "Images taken out of their context are the most prevalent form of multimodal\nmisinformation. Debunking them requires (1) providing the true context of the\nimage and (2) checking the veracity of the image's caption. However, existing\nautomated fact-checking methods fail to tackle both objectives explicitly. In\nthis work, we introduce COVE, a new method that predicts first the true COntext\nof the image and then uses it to predict the VEracity of the caption. COVE\nbeats the SOTA context prediction model on all context items, often by more\nthan five percentage points. It is competitive with the best veracity\nprediction models on synthetic data and outperforms them on real-world data,\nshowing that it is beneficial to combine the two tasks sequentially. Finally,\nwe conduct a human study that reveals that the predicted context is a reusable\nand interpretable artifact to verify new out-of-context captions for the same\nimage. Our code and data are made available.", "published": "2025-02-03 09:33:58", "link": "http://arxiv.org/abs/2502.01194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OCR Error Post-Correction with LLMs in Historical Documents: No Free\n  Lunches", "abstract": "Optical Character Recognition (OCR) systems often introduce errors when\ntranscribing historical documents, leaving room for post-correction to improve\ntext quality. This study evaluates the use of open-weight LLMs for OCR error\ncorrection in historical English and Finnish datasets. We explore various\nstrategies, including parameter optimization, quantization, segment length\neffects, and text continuation methods. Our results demonstrate that while\nmodern LLMs show promise in reducing character error rates (CER) in English, a\npractically useful performance for Finnish was not reached. Our findings\nhighlight the potential and limitations of LLMs in scaling OCR post-correction\nfor large historical corpora.", "published": "2025-02-03 09:55:31", "link": "http://arxiv.org/abs/2502.01205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling change in neural dynamics during phonetic accommodation", "abstract": "Short-term phonetic accommodation is a fundamental driver behind accent\nchange, but how does real-time input from another speaker's voice shape the\nspeech planning representations of an interlocutor? We advance a computational\nmodel of change in phonetic representations during phonetic accommodation,\ngrounded in dynamic neural field equations for movement planning and memory\ndynamics. We test the model's ability to capture empirical patterns from an\nexperimental study where speakers shadowed a model talker with a different\naccent from their own. The experimental data shows vowel-specific degrees of\nconvergence during shadowing, followed by return to baseline (or minor\ndivergence) post-shadowing. The model can reproduce these phenomena by\nmodulating the magnitude of inhibitory memory dynamics, which may reflect\nresistance to accommodation due to phonological and/or sociolinguistic\npressures. We discuss the implications of these results for the relation\nbetween short-term phonetic accommodation and longer-term patterns of sound\nchange.", "published": "2025-02-03 10:00:29", "link": "http://arxiv.org/abs/2502.01210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Main Predicate and Their Arguments as Explanation Signals For Intent\n  Classification", "abstract": "Intent classification is crucial for conversational agents (chatbots), and\ndeep learning models perform well in this area. However, little research has\nbeen done on the explainability of intent classification due to the absence of\nsuitable benchmark data. Human annotation of explanation signals in text\nsamples is time-consuming and costly. However, from inspection of data on\nintent classification, we see that, more often than not, the main verb denotes\nthe action, and the direct object indicates the domain of conversation, serving\nas explanation signals for intent. This observation enables us to hypothesize\nthat the main predicate in the text utterances, along with the arguments of the\nmain predicate, can serve as explanation signals. Leveraging this, we introduce\na new technique to automatically augment text samples from intent\nclassification datasets with word-level explanations. We mark main predicates\n(primarily verbs) and their arguments (dependency relations) as explanation\nsignals in benchmark intent classification datasets ATIS and SNIPS, creating a\nunique 21k-instance dataset for explainability. Further, we experiment with\ndeep learning and language models. We observe that models that work well for\nclassification do not perform well in explainability metrics like plausibility\nand faithfulness. We also observe that guiding models to focus on explanation\nsignals from our dataset during training improves the plausibility Token F1\nscore by 3-4%, improving the model's reasoning.", "published": "2025-02-03 11:39:26", "link": "http://arxiv.org/abs/2502.01270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding", "abstract": "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.", "published": "2025-02-03 13:34:51", "link": "http://arxiv.org/abs/2502.01341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations", "abstract": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommendation systems, yet their susceptibility to adversarial manipulation\nposes critical challenges, particularly in real-world commercial applications.\nOur approach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making these adversarial\nmanipulations hard to detect. In this work, we investigate cognitive biases as\nblack-box adversarial strategies, drawing parallels between their effects on\nLLMs and human purchasing behavior. Through extensive experiments on LLMs of\nvarying scales, we reveal significant vulnerabilities in their use as\nrecommenders, providing critical insights into safeguarding these systems.", "published": "2025-02-03 13:39:28", "link": "http://arxiv.org/abs/2502.01349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Tool and Dataset for Fact-Checking Podcasts", "abstract": "Podcasts are a popular medium on the web, featuring diverse and multilingual\ncontent that often includes unverified claims. Fact-checking podcasts is a\nchallenging task, requiring transcription, annotation, and claim verification,\nall while preserving the contextual details of spoken content. Our tool offers\na novel approach to tackle these challenges by enabling real-time annotation of\npodcasts during playback. This unique capability allows users to listen to the\npodcast and annotate key elements, such as check-worthy claims, claim spans,\nand contextual errors, simultaneously. By integrating advanced transcription\nmodels like OpenAI's Whisper and leveraging crowdsourced annotations, we create\nhigh-quality datasets to fine-tune multilingual transformer models such as\nXLM-RoBERTa for tasks like claim detection and stance classification.\nFurthermore, we release the annotated podcast transcripts and sample\nannotations with preliminary experiments.", "published": "2025-02-03 14:34:17", "link": "http://arxiv.org/abs/2502.01402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorization Inheritance in Sequence-Level Knowledge Distillation for\n  Neural Machine Translation", "abstract": "In this work, we explore how instance-level memorization in the teacher\nNeural Machine Translation (NMT) model gets inherited by the student model in\nsequence-level knowledge distillation (SeqKD). We find that despite not\ndirectly seeing the original training data, students memorize more than\nbaseline models (models of the same size, trained on the original data) -- 3.4%\nfor exact matches and 57% for extractive memorization -- and show increased\nhallucination rates. Further, under this SeqKD setting, we also characterize\nhow students behave on specific training data subgroups, such as subgroups with\nlow quality and specific counterfactual memorization (CM) scores, and find that\nstudents exhibit amplified denoising on low-quality subgroups. Finally, we\npropose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD\nto reduce memorization and hallucinations. Overall, we recommend caution when\napplying SeqKD: students inherit both their teachers' superior performance and\ntheir fault modes, thereby requiring active monitoring.", "published": "2025-02-03 16:26:06", "link": "http://arxiv.org/abs/2502.01491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question\n  Answering", "abstract": "Large language models (LLMs) are prone to hallucinations in\nquestion-answering (QA) tasks when faced with ambiguous questions. Users often\nassume that LLMs share their cognitive alignment, a mutual understanding of\ncontext, intent, and implicit details, leading them to omit critical\ninformation in the queries. However, LLMs generate responses based on\nassumptions that can misalign with user intent, which may be perceived as\nhallucinations if they misalign with the user's intent. Therefore, identifying\nthose implicit assumptions is crucial to resolve ambiguities in QA. Prior work,\nsuch as AmbigQA, reduces ambiguity in queries via human-annotated\nclarifications, which is not feasible in real application. Meanwhile, ASQA\ncompiles AmbigQA's short answers into long-form responses but inherits human\nbiases and fails capture explicit logical distinctions that differentiates the\nanswers. We introduce Conditional Ambiguous Question-Answering (CondAmbigQA), a\nbenchmark with 200 ambiguous queries and condition-aware evaluation metrics.\nOur study pioneers the concept of ``conditions'' in ambiguous QA tasks, where\nconditions stand for contextual constraints or assumptions that resolve\nambiguities. The retrieval-based annotation strategy uses retrieved Wikipedia\nfragments to identify possible interpretations for a given query as its\nconditions and annotate the answers through those conditions. Such a strategy\nminimizes human bias introduced by different knowledge levels among annotators.\nBy fixing retrieval results, CondAmbigQA evaluates how RAG systems leverage\nconditions to resolve ambiguities. Experiments show that models considering\nconditions before answering improve performance by $20\\%$, with an additional\n$5\\%$ gain when conditions are explicitly provided. These results underscore\nthe value of conditional reasoning in QA, offering researchers tools to\nrigorously evaluate ambiguity resolution.", "published": "2025-02-03 17:01:51", "link": "http://arxiv.org/abs/2502.01523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massive Values in Self-Attention Modules are the Key to Contextual\n  Knowledge Understanding", "abstract": "Large language models (LLMs) have achieved remarkable success in contextual\nknowledge understanding. In this paper, we show that these concentrated massive\nvalues consistently emerge in specific regions of attention queries (Q) and\nkeys (K) while not having such patterns in values (V) in various modern\ntransformer-based LLMs (Q, K, and V mean the representations output by the\nquery, key, and value layers respectively). Through extensive experiments, we\nfurther demonstrate that these massive values play a critical role in\ninterpreting contextual knowledge (knowledge obtained from the current context\nwindow) rather than in retrieving parametric knowledge stored within the\nmodel's parameters. Our further investigation of quantization strategies\nreveals that ignoring these massive values leads to a pronounced drop in\nperformance on tasks requiring rich contextual understanding, aligning with our\nanalysis. Finally, we trace the emergence of concentrated massive values and\nfind that such concentration is caused by Rotary Positional Encoding (RoPE),\nwhich has appeared since the first layers. These findings shed new light on how\nQ and K operate in LLMs and offer practical insights for model design and\noptimization. The Code is Available at\nhttps://github.com/MingyuJ666/Rope_with_LLM.", "published": "2025-02-03 17:47:03", "link": "http://arxiv.org/abs/2502.01563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReGLA: Refining Gated Linear Attention", "abstract": "Recent advancements in Large Language Models (LLMs) have set themselves apart\nwith their exceptional performance in complex language modelling tasks.\nHowever, these models are also known for their significant computational and\nstorage requirements, primarily due to the quadratic computation complexity of\nsoftmax attention. To mitigate this issue, linear attention has been designed\nto reduce the quadratic space-time complexity that is inherent in standard\ntransformers. In this work, we embarked on a comprehensive exploration of three\nkey components that substantially impact the performance of the Gated Linear\nAttention module: feature maps, normalization, and the gating mechanism. We\ndeveloped a feature mapping function to address some crucial issues that\nprevious suggestions overlooked. Then we offered further rationale for the\nintegration of normalization layers to stabilize the training process.\nMoreover, we explored the saturation phenomenon of the gating mechanism and\naugmented it with a refining module. We conducted extensive experiments and\nshowed our architecture outperforms previous Gated Linear Attention mechanisms\nin extensive tasks including training from scratch and post-linearization with\ncontinual pre-training.", "published": "2025-02-03 18:03:13", "link": "http://arxiv.org/abs/2502.01578v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FutureVision: A methodology for the investigation of future cognition", "abstract": "This paper presents a methodology combining multimodal semantic analysis with\nan eye-tracking experimental protocol to investigate the cognitive effort\ninvolved in understanding the communication of future scenarios. To demonstrate\nthe methodology, we conduct a pilot study examining how visual fixation\npatterns vary during the evaluation of valence and counterfactuality in\nfictional ad pieces describing futuristic scenarios, using a portable eye\ntracker. Participants eye movements are recorded while evaluating the stimuli\nand describing them to a conversation partner. Gaze patterns are analyzed\nalongside semantic representations of the stimuli and participants\ndescriptions, constructed from a frame semantic annotation of both linguistic\nand visual modalities. Preliminary results show that far-future and pessimistic\nscenarios are associated with longer fixations and more erratic saccades,\nsupporting the hypothesis that fractures in the base spaces underlying the\ninterpretation of future scenarios increase cognitive load for comprehenders.", "published": "2025-02-03 18:29:06", "link": "http://arxiv.org/abs/2502.01597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Focus: Contextual Distraction Curse in Large Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have revolutionized\ngenerative systems, achieving excellent performance across diverse domains.\nAlthough these models perform well in controlled environments, their real-world\napplications frequently encounter inputs containing both essential and\nirrelevant details. Our investigation has revealed a critical vulnerability in\nLLMs, which we term Contextual Distraction Vulnerability (CDV). This phenomenon\narises when models fail to maintain consistent performance on questions\nmodified with semantically coherent but irrelevant context. To systematically\ninvestigate this vulnerability, we propose an efficient tree-based search\nmethodology to automatically generate CDV examples. Our approach successfully\ngenerates CDV examples across four datasets, causing an average performance\ndegradation of approximately 45% in state-of-the-art LLMs. To address this\ncritical issue, we explore various mitigation strategies and find that\npost-targeted training approaches can effectively enhance model robustness\nagainst contextual distractions. Our findings highlight the fundamental nature\nof CDV as an ability-level challenge rather than a knowledge-level issue since\nmodels demonstrate the necessary knowledge by answering correctly in the\nabsence of distractions. This calls the community's attention to address CDV\nduring model development to ensure reliability. The code is available at\nhttps://github.com/wyf23187/LLM_CDV.", "published": "2025-02-03 18:43:36", "link": "http://arxiv.org/abs/2502.01609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Human-Like Internally", "abstract": "Recent cognitive modeling studies have reported that larger language models\n(LMs) exhibit a poorer fit to human reading behavior, leading to claims of\ntheir cognitive implausibility. In this paper, we revisit this argument through\nthe lens of mechanistic interpretability and argue that prior conclusions were\nskewed by an exclusive focus on the final layers of LMs. Our analysis reveals\nthat next-word probabilities derived from internal layers of larger LMs align\nwith human sentence processing data as well as, or better than, those from\nsmaller LMs. This alignment holds consistently across behavioral (self-paced\nreading times, gaze durations, MAZE task processing times) and\nneurophysiological (N400 brain potentials) measures, challenging earlier mixed\nresults and suggesting that the cognitive plausibility of larger LMs has been\nunderestimated. Furthermore, we first identify an intriguing relationship\nbetween LM layers and human measures: earlier layers correspond more closely\nwith fast gaze durations, while later layers better align with relatively\nslower signals such as N400 potentials and MAZE processing times. Our work\nopens new avenues for interdisciplinary research at the intersection of\nmechanistic interpretability and cognitive modeling.", "published": "2025-02-03 18:48:32", "link": "http://arxiv.org/abs/2502.01615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Lexical Projection in Large Language Models: A Novel Approach to\n  Implicit Representation Refinement", "abstract": "Generating semantically coherent text requires a robust internal\nrepresentation of linguistic structures, which traditional embedding techniques\noften fail to capture adequately. A novel approach, Latent Lexical Projection\n(LLP), is introduced to refine lexical representations through a structured\ntransformation into a latent space, thereby enhancing the alignment between\ninput embeddings and their contextual meanings. The method integrates an\noptimized projection mechanism within an existing language model architecture,\nenabling more accurate token selection while maintaining syntactic integrity.\nEvaluations across multiple benchmarks indicate a reduction in perplexity and\nan increase in BLEU scores, suggesting improvements in predictive accuracy and\nfluency. The analysis of lexical diversity reveals a more varied vocabulary in\ngenerated text, addressing common issues of redundancy and repetitive phrase\nstructures. Further assessments of entropy distributions demonstrate a decline\nin uncertainty during decoding, reflecting enhanced confidence in word\nselection. Additionally, long-range dependency retention exhibits measurable\ngains, with increased classification accuracy at extended token distances.\nComputational efficiency remains within manageable constraints, despite the\nadded projection mechanism, highlighting the practicality of LLP for\nintegration into existing architectures.", "published": "2025-02-03 23:18:53", "link": "http://arxiv.org/abs/2502.01882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via\n  Multimodal Feedback", "abstract": "Scientific data visualization is pivotal for transforming raw data into\ncomprehensible visual representations, enabling pattern recognition,\nforecasting, and the presentation of data-driven insights. However, novice\nusers often face difficulties due to the complexity of selecting appropriate\ntools and mastering visualization techniques. Large Language Models (LLMs) have\nrecently demonstrated potential in assisting code generation, though they\nstruggle with accuracy and require iterative debugging. In this paper, we\npropose PlotGen, a novel multi-agent framework aimed at automating the creation\nof precise scientific visualizations. PlotGen orchestrates multiple LLM-based\nagents, including a Query Planning Agent that breaks down complex user requests\ninto executable steps, a Code Generation Agent that converts pseudocode into\nexecutable Python code, and three retrieval feedback agents - a Numeric\nFeedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that\nleverage multimodal LLMs to iteratively refine the data accuracy, textual\nlabels, and visual correctness of generated plots via self-reflection.\nExtensive experiments show that PlotGen outperforms strong baselines, achieving\na 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user\ntrust in LLM-generated visualizations and improved novice productivity due to a\nreduction in debugging time needed for plot errors.", "published": "2025-02-03 02:00:29", "link": "http://arxiv.org/abs/2502.00988v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual\n  Attribution", "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but\noften generate unverified hallucinated responses. Existing answer attribution\nmethods struggle to ground responses in source charts due to limited\nvisual-semantic context, complex visual-text alignment requirements, and\ndifficulties in bounding box prediction across complex layouts. We present\nChartCitor, a multi-agent framework that provides fine-grained bounding box\ncitations by identifying supporting evidence within chart images. The system\norchestrates LLM agents to perform chart-to-table extraction, answer\nreformulation, table augmentation, evidence retrieval through pre-filtering and\nre-ranking, and table-to-chart mapping. ChartCitor outperforms existing\nbaselines across different chart types. Qualitative user studies show that\nChartCitor helps increase user trust in Generative AI by providing enhanced\nexplainability for LLM-assisted chart QA and enables professionals to be more\nproductive.", "published": "2025-02-03 02:00:51", "link": "http://arxiv.org/abs/2502.00989v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs", "abstract": "The recent success of specialized Large Language Models (LLMs) in domains\nsuch as mathematical reasoning and coding has led to growing interest in\nmethods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)\nmodel, with the goal of enhancing performance in each domain while retaining\neffectiveness on general tasks. However, the effective merging of expert models\nremains an open challenge, especially for models with highly divergent weight\nparameters or different architectures. State-of-the-art MoE merging methods\nonly work with homogeneous model architectures and rely on simple unweighted\naveraging to merge expert layers, which does not address parameter interference\nand requires extensive fine-tuning of the merged MoE to restore performance. To\naddress these limitations, this paper introduces new MoE merging techniques,\nincluding strategies to mitigate parameter interference, routing heuristics to\nreduce the need for MoE fine-tuning, and a novel method for merging experts\nwith different architectures. Extensive experiments across multiple domains\ndemonstrate the effectiveness of our proposed methods, reducing fine-tuning\ncosts, improving performance over state-of-the-art methods, and expanding the\napplicability of MoE merging.", "published": "2025-02-03 02:34:46", "link": "http://arxiv.org/abs/2502.00997v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Large Vision-Language Models with Internal\n  Fact-based Contrastive Decoding", "abstract": "Large Visual Language Models (LVLMs) integrate visual and linguistic\nmodalities, exhibiting exceptional performance across various multimodal tasks.\nNevertheless, LVLMs remain vulnerable to the issue of object hallucinations.\nPrevious efforts to mitigate this issue focus on supervised fine-tuning (SFT)\nor incorporating external knowledge, both of which entail significant costs\nrelated to training and the acquisition of external data. To address these\nchallenges, we propose a novel model-agnostic approach termed Internal\nFact-based Contrastive Decoding (IFCD), designed to mitigate and suppress\nhallucinations during the inference process of LVLMs by exploiting the LVLMs'\nown hallucinations. IFCD is grounded in experimental observations that\nalterations to the LVLMs' internal representations tend to amplify\nhallucinations caused by language bias. By contrasting disturbed distribution,\nIFCD calibrates the LVLMs' output and effectively removes the hallucinatory\nlogits from the final predictions. Experimental results validate that IFCD\nsignificantly alleviates both object-level and attribute-level hallucinations\nwhile achieving an average 9% accuracy improvement on POPE and 8% accuracy\nimprovement on MME object hallucinations subset compared with direct decoding,\nrespectively.", "published": "2025-02-03 05:08:35", "link": "http://arxiv.org/abs/2502.01056v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Knowledge Synthesis of Photosynthesis Research Using a Large Language\n  Model", "abstract": "The development of biological data analysis tools and large language models\n(LLMs) has opened up new possibilities for utilizing AI in plant science\nresearch, with the potential to contribute significantly to knowledge\nintegration and research gap identification. Nonetheless, current LLMs struggle\nto handle complex biological data and theoretical models in photosynthesis\nresearch and often fail to provide accurate scientific contexts. Therefore,\nthis study proposed a photosynthesis research assistant (PRAG) based on\nOpenAI's GPT-4o with retrieval-augmented generation (RAG) techniques and prompt\noptimization. Vector databases and an automated feedback loop were used in the\nprompt optimization process to enhance the accuracy and relevance of the\nresponses to photosynthesis-related queries. PRAG showed an average improvement\nof 8.7% across five metrics related to scientific writing, with a 25.4%\nincrease in source transparency. Additionally, its scientific depth and domain\ncoverage were comparable to those of photosynthesis research papers. A\nknowledge graph was used to structure PRAG's responses with papers within and\noutside the database, which allowed PRAG to match key entities with 63% and\n39.5% of the database and test papers, respectively. PRAG can be applied for\nphotosynthesis research and broader plant science domains, paving the way for\nmore in-depth data analysis and predictive capabilities.", "published": "2025-02-03 05:10:19", "link": "http://arxiv.org/abs/2502.01059v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation", "abstract": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.", "published": "2025-02-03 05:25:09", "link": "http://arxiv.org/abs/2502.01068v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Classic4Children: Adapting Chinese Literary Classics for Children with\n  Large Language Model", "abstract": "Chinese literary classics hold significant cultural and educational value,\noffering deep insights into morality, history, and human nature. These works\noften include classical Chinese and complex narratives, making them difficult\nfor children to read. To bridge this gap, we introduce a child-friendly\nliterary adaptation (CLA) task to adapt the Chinese literary classic into\nengaging and accessible text for children. However, recent large language\nmodels (LLMs) overlook children's reading preferences (\\ie, vivid character\nportrayals, concise narrative structures, and appropriate readability), which\nposes challenges in CLA. In this paper, we propose a method called\nInstructChild, which augments the LLM with these preferences for adaptation.\nSpecifically, we first obtain the characters' personalities and narrative\nstructure as additional information for fine-grained instruction tuning. Then,\nwe devise a readability metric as the reward to align the LLM with the\nchildren's reading level. Finally, a lookahead decoding strategy is applied to\nimprove the readability of the generated text during inference. To support the\nevaluation of CLA task, we construct the Classic4Children dataset, which\ncomprises both the original and child-friendly versions of the Four Great\nClassical Novels of Chinese literature. Experimental results show that our\nInstructChild significantly improves automatic and human evaluation\nperformance.", "published": "2025-02-03 06:23:35", "link": "http://arxiv.org/abs/2502.01090v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian\n  Language", "abstract": "In the era of pervasive internet use and the dominance of social networks,\nresearchers face significant challenges in Persian text mining including the\nscarcity of adequate datasets in Persian and the inefficiency of existing\nlanguage models. This paper specifically tackles these challenges, aiming to\namplify the efficiency of language models tailored to the Persian language.\nFocusing on enhancing the effectiveness of sentiment analysis, our approach\nemploys an aspect-based methodology utilizing the ParsBERT model, augmented\nwith a relevant lexicon. The study centers on sentiment analysis of user\nopinions extracted from the Persian website 'Digikala.' The experimental\nresults not only highlight the proposed method's superior semantic capabilities\nbut also showcase its efficiency gains with an accuracy of 88.2% and an F1\nscore of 61.7. The importance of enhancing language models in this context lies\nin their pivotal role in extracting nuanced sentiments from user-generated\ncontent, ultimately advancing the field of sentiment analysis in Persian text\nmining by increasing efficiency and accuracy.", "published": "2025-02-03 06:25:06", "link": "http://arxiv.org/abs/2502.01091v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Single Model Ensemble Framework for Neural Machine Translation using\n  Pivot Translation", "abstract": "Despite the significant advances in neural machine translation, performance\nremains subpar for low-resource language pairs. Ensembling multiple systems is\na widely adopted technique to enhance performance, often accomplished by\ncombining probability distributions. However, the previous approaches face the\nchallenge of high computational costs for training multiple models.\nFurthermore, for black-box models, averaging token-level probabilities at each\ndecoding step is not feasible. To address the problems of multi-model ensemble\nmethods, we present a pivot-based single model ensemble. The proposed strategy\nconsists of two steps: pivot-based candidate generation and post-hoc\naggregation. In the first step, we generate candidates through pivot\ntranslation. This can be achieved with only a single model and facilitates\nknowledge transfer from high-resource pivot languages, resulting in candidates\nthat are not only diverse but also more accurate. Next, in the aggregation\nstep, we select k high-quality candidates from the generated candidates and\nmerge them to generate a final translation that outperforms the existing\ncandidates. Our experimental results show that our method produces translations\nof superior quality by leveraging candidates from pivot translation to capture\nthe subtle nuances of the source sentence.", "published": "2025-02-03 09:17:45", "link": "http://arxiv.org/abs/2502.01182v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Almost Surely Safe Alignment of Large Language Models at Inference-Time", "abstract": "Even highly capable large language models (LLMs) can produce biased or unsafe\nresponses, and alignment techniques, such as RLHF, aimed at mitigating this\nissue, are expensive and prone to overfitting as they retrain the LLM. This\npaper introduces a novel inference-time alignment approach that ensures LLMs\ngenerate safe responses almost surely, i.e., with a probability approaching\none. We achieve this by framing the safe generation of inference-time responses\nas a constrained Markov decision process within the LLM's latent space.\nCrucially, we augment a safety state that tracks the evolution of safety\nconstraints and enables us to demonstrate formal safety guarantees upon solving\nthe MDP in the latent space. Building on this foundation, we propose\nInferenceGuard, a practical implementation that safely aligns LLMs without\nmodifying the model weights. Empirically, we demonstrate InferenceGuard\neffectively balances safety and task performance, outperforming existing\ninference-time alignment methods in generating safe and aligned responses.", "published": "2025-02-03 09:59:32", "link": "http://arxiv.org/abs/2502.01208v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts", "abstract": "Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.", "published": "2025-02-03 10:24:55", "link": "http://arxiv.org/abs/2502.01220v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OphthBench: A Comprehensive Benchmark for Evaluating Large Language\n  Models in Chinese Ophthalmology", "abstract": "Large language models (LLMs) have shown significant promise across various\nmedical applications, with ophthalmology being a notable area of focus. Many\nophthalmic tasks have shown substantial improvement through the integration of\nLLMs. However, before these models can be widely adopted in clinical practice,\nevaluating their capabilities and identifying their limitations is crucial. To\naddress this research gap and support the real-world application of LLMs, we\nintroduce the OphthBench, a specialized benchmark designed to assess LLM\nperformance within the context of Chinese ophthalmic practices. This benchmark\nsystematically divides a typical ophthalmic clinical workflow into five key\nscenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each\nscenario, we developed multiple tasks featuring diverse question types,\nresulting in a comprehensive benchmark comprising 9 tasks and 591 questions.\nThis comprehensive framework allows for a thorough assessment of LLMs'\ncapabilities and provides insights into their practical application in Chinese\nophthalmology. Using this benchmark, we conducted extensive experiments and\nanalyzed the results from 39 popular LLMs. Our evaluation highlights the\ncurrent gap between LLM development and its practical utility in clinical\nsettings, providing a clear direction for future advancements. By bridging this\ngap, we aim to unlock the potential of LLMs and advance their development in\nophthalmology.", "published": "2025-02-03 11:04:51", "link": "http://arxiv.org/abs/2502.01243v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probabilistic adaptation of language comprehension for individual\n  speakers: Evidence from neural oscillations", "abstract": "Listeners adapt language comprehension based on their mental representations\nof speakers, but how these representations are dynamically updated remains\nunclear. We investigated whether listeners probabilistically adapt their\ncomprehension based on the likelihood of speakers producing\nstereotype-incongruent utterances. Our findings reveal two potential\nmechanisms: a speaker-general mechanism that adjusts overall expectations about\nspeaker-content relationships, and a speaker-specific mechanism that updates\nindividual speaker models. In two EEG experiments, participants heard speakers\nmake stereotype-congruent or incongruent utterances, with incongruency base\nrate manipulated between blocks. In Experiment 1, speaker incongruency\nmodulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations:\nincongruent utterances decreased oscillatory power in low base rate condition\nbut increased it in high base rate condition. The theta effect varied with\nlisteners' openness trait: less open participants showed theta increases to\nspeaker-incongruencies, suggesting maintenance of speaker-specific information,\nwhile more open participants showed theta decreases, indicating flexible model\nupdating. In Experiment 2, we dissociated base rate from the target speaker by\nmanipulating the overall base rate using an alternative non-target speaker.\nOnly the high-beta effect persisted, showing power decrease for\nspeaker-incongruencies in low base rate condition but no effect in high base\nrate condition. The high-beta oscillations might reflect the speaker-general\nadjustment, while theta oscillations may index the speaker-specific model\nupdating. These findings provide evidence for how language processing is shaped\nby social cognition in real time.", "published": "2025-02-03 12:19:20", "link": "http://arxiv.org/abs/2502.01299v1", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance\n  When Using LLM Agents As A Daily Assistant", "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs)\nhave continued to impact our everyday lives. Equipped with external tools that\nare designed for a specific purpose (e.g., for flight booking or an alarm\nclock), LLM agents exercise an increasing capability to assist humans in their\ndaily work. Although LLM agents have shown a promising blueprint as daily\nassistants, there is a limited understanding of how they can provide daily\nassistance based on planning and sequential decision making capabilities. We\ndraw inspiration from recent work that has highlighted the value of\n'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks.\nWe conducted an empirical study (N = 248) of LLM agents as daily assistants in\nsix commonly occurring tasks with different levels of risk typically associated\nwith them (e.g., flight ticket booking and credit card payments). To ensure\nuser agency and control over the LLM agent, we adopted LLM agents in a\nplan-then-execute manner, wherein the agents conducted step-wise planning and\nstep-by-step execution in a simulation environment. We analyzed how user\ninvolvement at each stage affects their trust and collaborative team\nperformance. Our findings demonstrate that LLM agents can be a double-edged\nsword -- (1) they can work well when a high-quality plan and necessary user\ninvolvement in execution are available, and (2) users can easily mistrust the\nLLM agents with plans that seem plausible. We synthesized key insights for\nusing LLM agents as daily assistants to calibrate user trust and achieve better\noverall task outcomes. Our work has important implications for the future\ndesign of daily assistants and human-AI collaboration with LLM agents.", "published": "2025-02-03 14:23:22", "link": "http://arxiv.org/abs/2502.01390v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Originality in scientific titles and abstracts can predict citation\n  count", "abstract": "In this research-in-progress paper, we apply a computational measure\ncorrelating with originality from creativity science: Divergent Semantic\nIntegration (DSI), to a selection of 99,557 scientific abstracts and titles\nselected from the Web of Science. We observe statistically significant\ndifferences in DSI between subject and field of research, and a slight rise in\nDSI over time. We model the base 10 logarithm of the citation count after 5\nyears with DSI and find a statistically significant positive correlation in all\nfields of research with an adjusted $R^2$ of 0.13.", "published": "2025-02-03 14:55:52", "link": "http://arxiv.org/abs/2502.01417v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Emergent Stack Representations in Modeling Counter Languages Using\n  Transformers", "abstract": "Transformer architectures are the backbone of most modern language models,\nbut understanding the inner workings of these models still largely remains an\nopen problem. One way that research in the past has tackled this problem is by\nisolating the learning capabilities of these architectures by training them\nover well-understood classes of formal languages. We extend this literature by\nanalyzing models trained over counter languages, which can be modeled using\ncounter variables. We train transformer models on 4 counter languages, and\nequivalently formulate these languages using stacks, whose depths can be\nunderstood as the counter values. We then probe their internal representations\nfor stack depths at each input token to show that these models when trained as\nnext token predictors learn stack-like representations. This brings us closer\nto understanding the algorithmic details of how transformers learn languages\nand helps in circuit discovery.", "published": "2025-02-03 15:11:49", "link": "http://arxiv.org/abs/2502.01432v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs", "abstract": "Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations.\n  We validate our framework with a manually annotated ground truth, and\nevaluate it through a large-scale study with 782 Custom GPTs across three\ncategories: Romantic, Cybersecurity, and Academic GPTs. Our manual annotation\nprocess achieved an F1 score of 0.975 in identifying policy violations,\nconfirming the reliability of the framework's assessments. The results reveal\nthat 58.7% of the analyzed models exhibit indications of non-compliance,\nexposing weaknesses in the GPT store's review and approval processes.\nFurthermore, our findings indicate that a model's popularity does not correlate\nwith compliance, and non-compliance issues largely stem from behaviors\ninherited from base models rather than user-driven customizations. We believe\nthis approach is extendable to other chatbot platforms and policy domains,\nimproving LLM-based systems safety.", "published": "2025-02-03 15:19:28", "link": "http://arxiv.org/abs/2502.01436v1", "categories": ["cs.CL", "cs.AI", "I.2.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal\n  Unalignment for Large Language Model", "abstract": "Large language models have been widely applied, but can inadvertently encode\nsensitive or harmful information, raising significant safety concerns. Machine\nunlearning has emerged to alleviate this concern; however, existing\ntraining-time unlearning approaches, relying on coarse-grained loss\ncombinations, have limitations in precisely separating knowledge and balancing\nremoval effectiveness with model utility. In contrast, we propose Fine-grained\nActivation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel\nrepresentation-guided unlearning approach that leverages information-theoretic\nguidance for efficient parameter selection, employs contrastive mechanisms to\nenhance representation separation, and projects conflict gradients onto\northogonal subspaces to resolve conflicts between forgetting and retention\nobjectives. Extensive experiments demonstrate that FALCON achieves superior\nunlearning effectiveness while maintaining model utility, exhibiting robust\nresistance against knowledge recovery attempts.", "published": "2025-02-03 16:05:15", "link": "http://arxiv.org/abs/2502.01472v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining Context Length Scaling and Bounds for Language Models", "abstract": "Long Context Language Models have drawn great attention in the past few\nyears. There has been work discussing the impact of long context on Language\nModel performance: some find that long irrelevant context could harm\nperformance, while some experimentally summarize loss reduction by relevant\nlong context as Scaling Laws. This calls for a more thorough understanding on\nhow long context impact Language Modeling. In this work, we (1) propose a clean\nand effective theoretical framework on explaining the impact of context length\nto Language Modeling, from an Intrinsic Space perspective; and (2) conduct\nexperiments on natural language and synthetic data, validating our proposed\ntheoretical assumptions and deductions. Our theoretical framework can provide\npractical insights such as establishing that training dataset size dictates an\noptimal context length and bounds context length scaling for certain case. We\nhope our work may inspire new long context Language Models, as well as future\nwork studying Physics for Language Models. Code for our experiments is\navailable at this url: https://github.com/JingzheShi/NLPCtlScalingAndBounds.", "published": "2025-02-03 16:16:15", "link": "http://arxiv.org/abs/2502.01481v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What is a Number, That a Large Language Model May Know It?", "abstract": "Numbers are a basic part of how humans represent and describe the world\naround them. As a consequence, learning effective representations of numbers is\ncritical for the success of large language models as they become more\nintegrated into everyday decisions. However, these models face a challenge:\ndepending on context, the same sequence of digit tokens, e.g., 911, can be\ntreated as a number or as a string. What kind of representations arise from\nthis duality, and what are its downstream implications? Using a\nsimilarity-based prompting technique from cognitive science, we show that LLMs\nlearn representational spaces that blend string-like and numerical\nrepresentations. In particular, we show that elicited similarity judgments from\nthese models over integer pairs can be captured by a combination of Levenshtein\nedit distance and numerical Log-Linear distance, suggesting an entangled\nrepresentation. In a series of experiments we show how this entanglement is\nreflected in the latent embeddings, how it can be reduced but not entirely\neliminated by context, and how it can propagate into a realistic decision\nscenario. These results shed light on a representational tension in transformer\nmodels that must learn what a number is from text input.", "published": "2025-02-03 17:17:26", "link": "http://arxiv.org/abs/2502.01540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Theory of Mind Enables the Invention of Writing Systems", "abstract": "Abstract symbolic writing systems are semiotic codes that are ubiquitous in\nmodern society but are otherwise absent in the animal kingdom. Anthropological\nevidence suggests that the earliest forms of some writing systems originally\nconsisted of iconic pictographs, which signify their referent via visual\nresemblance. While previous studies have examined the emergence and,\nseparately, the evolution of pictographic writing systems through a\ncomputational lens, most employ non-naturalistic methodologies that make it\ndifficult to draw clear analogies to human and animal cognition. We develop a\nmulti-agent reinforcement learning testbed for emergent communication called a\nSignification Game, and formulate a model of inferential communication that\nenables agents to leverage visual theory of mind to communicate actions using\npictographs. Our model, which is situated within a broader formalism for animal\ncommunication, sheds light on the cognitive and cultural processes that led to\nthe development of early writing systems.", "published": "2025-02-03 17:50:37", "link": "http://arxiv.org/abs/2502.01568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-TA: An LLM-Enhanced Thematic Analysis Pipeline for Transcripts from\n  Parents of Children with Congenital Heart Disease", "abstract": "Thematic Analysis (TA) is a fundamental method in healthcare research for\nanalyzing transcript data, but it is resource-intensive and difficult to scale\nfor large, complex datasets. This study investigates the potential of large\nlanguage models (LLMs) to augment the inductive TA process in high-stakes\nhealthcare settings. Focusing on interview transcripts from parents of children\nwith Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital\nheart disease, we propose an LLM-Enhanced Thematic Analysis (LLM-TA) pipeline.\nOur pipeline integrates an affordable state-of-the-art LLM (GPT-4o mini),\nLangChain, and prompt engineering with chunking techniques to analyze nine\ndetailed transcripts following the inductive TA framework. We evaluate the\nLLM-generated themes against human-generated results using thematic similarity\nmetrics, LLM-assisted assessments, and expert reviews. Results demonstrate that\nour pipeline outperforms existing LLM-assisted TA methods significantly. While\nthe pipeline alone has not yet reached human-level quality in inductive TA, it\nshows great potential to improve scalability, efficiency, and accuracy while\nreducing analyst workload when working collaboratively with domain experts. We\nprovide practical recommendations for incorporating LLMs into high-stakes TA\nworkflows and emphasize the importance of close collaboration with domain\nexperts to address challenges related to real-world applicability and dataset\ncomplexity. https://github.com/jiaweixu98/LLM-TA", "published": "2025-02-03 18:51:46", "link": "http://arxiv.org/abs/2502.01620v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scaling Embedding Layers in Language Models", "abstract": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.", "published": "2025-02-03 18:59:32", "link": "http://arxiv.org/abs/2502.01637v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging", "abstract": "Machine learning models are routinely trained on a mixture of different data\ndomains. Different domain weights yield very different downstream performances.\nWe propose the Soup-of-Experts, a novel architecture that can instantiate a\nmodel at test time for any domain weights with minimal computational cost and\nwithout re-training the model. Our architecture consists of a bank of expert\nparameters, which are linearly combined to instantiate one model. We learn the\nlinear combination coefficients as a function of the input domain weights. To\ntrain this architecture, we sample random domain weights, instantiate the\ncorresponding model, and backprop through one batch of data sampled with these\ndomain weights. We demonstrate how our approach obtains small specialized\nmodels on several language modeling tasks quickly. Soup-of-Experts are\nparticularly appealing when one needs to ship many different specialist models\nquickly under a model size constraint.", "published": "2025-02-03 20:33:20", "link": "http://arxiv.org/abs/2502.01804v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative\n  Large Language Models", "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical\nchallenge for their reliable deployment in real-world applications. To address\nthis, we introduce SelfCheckAgent, a novel framework integrating three\ndifferent agents: the Symbolic Agent, the Specialized Detection Agent, and the\nContextual Consistency Agent. These agents provide a robust multi-dimensional\napproach to hallucination detection. Notable results include the Contextual\nConsistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve\noutstanding performance on the WikiBio dataset, with NonFactual hallucination\ndetection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On\nthe AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89%\nbut reveals trade-offs in Factual with 30.58% and Ranking with 30.68%,\nunderscoring the complexity of hallucination detection in the complex\nmathematical domains. The framework also incorporates a triangulation strategy,\nwhich increases the strengths of the SelfCheckAgent, yielding significant\nimprovements in real-world hallucination identification. The comparative\nanalysis demonstrates SelfCheckAgent's applicability across diverse domains,\npositioning it as a crucial advancement for trustworthy LLMs. These findings\nhighlight the potentiality of consistency-driven methodologies in detecting\nhallucinations in LLMs.", "published": "2025-02-03 20:42:32", "link": "http://arxiv.org/abs/2502.01812v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training and Evaluating with Human Label Variation: An Empirical Study", "abstract": "Human label variation (HLV) challenges the standard assumption that a\nlabelled instance has a single ground truth, instead embracing the natural\nvariation in human annotation to train and evaluate models. While various\ntraining methods and metrics for HLV have been proposed, it is still unclear\nwhich methods and metrics perform best in what settings. We propose new\nevaluation metrics for HLV leveraging fuzzy set theory. Since these new\nproposed metrics are differentiable, we then in turn experiment with employing\nthese metrics as training objectives. We conduct an extensive study over 6 HLV\ndatasets testing 14 training methods and 6 evaluation metrics. We find that\ntraining on either disaggregated annotations or soft labels performs best\nacross metrics, outperforming training using the proposed training objectives\nwith differentiable metrics. We also show that our proposed soft metric is more\ninterpretable and correlates best with human preference.", "published": "2025-02-03 23:49:20", "link": "http://arxiv.org/abs/2502.01891v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance,\n  Efficiency, and Few-Shot Learning", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced sentiment analysis capabilities. However, the trade-offs between model\nperformance, efficiency, and explainability of some latest models remain\nunderexplored. This study presents the first comprehensive evaluation of the\nDeepSeek-R1 series of models, reasoning open-source LLMs, for sentiment\nanalysis, comparing them against OpenAI's GPT-4 and GPT-4-mini. We\nsystematically analyze their performance under few-shot prompting conditions,\nscaling up to 50-shot configurations to assess in-context learning\neffectiveness. Our experiments reveal that DeepSeek-R1 demonstrates competitive\naccuracy, particularly in multi-class sentiment tasks, while offering enhanced\ninterpretability through its detailed reasoning process. Additionally, we\nhighlight the impact of increasing few-shot examples on model performance and\ndiscuss key trade-offs between explainability and computational efficiency.", "published": "2025-02-03 07:17:46", "link": "http://arxiv.org/abs/2503.11655v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RandLoRA: Full-rank parameter-efficient fine-tuning of large models", "abstract": "Low-Rank Adaptation (LoRA) and its variants have shown impressive results in\nreducing the number of trainable parameters and memory requirements of large\ntransformer networks while maintaining fine-tuning performance. The low-rank\nnature of the weight update inherently limits the representation power of\nfine-tuned models, however, thus potentially compromising performance on\ncomplex tasks. This raises a critical question: when a performance gap between\nLoRA and standard fine-tuning is observed, is it due to the reduced number of\ntrainable parameters or the rank deficiency? This paper aims to answer this\nquestion by introducing RandLoRA, a parameter-efficient method that performs\nfull-rank updates using a learned linear combinations of low-rank,\nnon-trainable random matrices. Our method limits the number of trainable\nparameters by restricting optimization to diagonal scaling matrices applied to\nthe fixed random matrices. This allows us to effectively overcome the low-rank\nlimitations while maintaining parameter and memory efficiency during training.\nThrough extensive experimentation across vision, language, and vision-language\nbenchmarks, we systematically evaluate the limitations of LoRA and existing\nrandom basis methods. Our findings reveal that full-rank updates are beneficial\nacross vision and language tasks individually, and even more so for\nvision-language tasks, where RandLoRA significantly reduces -- and sometimes\neliminates -- the performance gap between standard fine-tuning and LoRA,\ndemonstrating its efficacy.", "published": "2025-02-03 01:59:45", "link": "http://arxiv.org/abs/2502.00987v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning\n  Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles", "abstract": "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large\nLanguage Models towards advanced reasoning capabilities. Notably, o3\noutperformed humans in novel problem-solving and skill acquisition on the\nAbstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI).\nHowever, this benchmark is limited to symbolic patterns, whereas humans often\nperceive and reason about multimodal scenarios involving both vision and\nlanguage data. Thus, there is an urgent need to investigate advanced reasoning\ncapabilities in multimodal tasks. To this end, we track the evolution of the\nGPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring\nfine-grained visual perception with abstract or algorithmic reasoning. The\nsuperior performance of o1 comes at nearly 750 times the computational cost of\nGPT-4o, raising concerns about its efficiency. Our results reveal a clear\nupward trend in reasoning capabilities across model iterations, with notable\nperformance jumps across GPT-series models and subsequently to o1. Nonetheless,\nwe observe that the o1 model still struggles with simple multimodal puzzles\nrequiring abstract reasoning. Furthermore, its performance in algorithmic\npuzzles remains poor. We plan to continuously track new models in the series\nand update our results in this paper accordingly. All resources used in this\nevaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", "published": "2025-02-03 05:47:04", "link": "http://arxiv.org/abs/2502.01081v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Tool Unlearning for Tool-Augmented LLMs", "abstract": "Tool-augmented large language models (LLMs) are often trained on datasets of\nquery-response pairs, which embed the ability to use tools or APIs directly\ninto the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to\nforget learned tools due to security vulnerabilities, privacy regulations, or\ntool deprecations. However, ``tool unlearning'' has not been investigated in\nunlearning literature. We introduce this novel task, which requires addressing\ndistinct challenges compared to traditional unlearning: knowledge removal\nrather than forgetting individual samples, the high cost of optimizing LLMs,\nand the need for principled evaluation metrics. To bridge these gaps, we\npropose ToolDelete, the first approach for unlearning tools from tool-augmented\nLLMs. It implements three key properties to address the above challenges for\neffective tool unlearning and introduces a new membership inference attack\n(MIA) model for effective evaluation. Extensive experiments on multiple tool\nlearning datasets and tool-augmented LLMs show that ToolDelete effectively\nunlearns randomly selected tools, while preserving the LLM's knowledge on\nnon-deleted tools and maintaining performance on general tasks.", "published": "2025-02-03 05:50:55", "link": "http://arxiv.org/abs/2502.01083v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning", "abstract": "We investigate the logical reasoning capabilities of large language models\n(LLMs) and their scalability in complex non-monotonic reasoning. To this end,\nwe introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM\nreasoning performance on logic grid puzzles derived from constraint\nsatisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with\ncontrollable and quantifiable complexity, facilitating a systematic study of\nthe scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By\nencompassing a broad range of search space complexities and diverse logical\nconstraints, ZebraLogic provides a structured environment to evaluate reasoning\nunder increasing difficulty.\n  Our results reveal a significant decline in accuracy as problem complexity\ngrows -- a phenomenon we term the curse of complexity. This limitation persists\neven with larger models and increased inference-time computation, suggesting\ninherent constraints in current LLM reasoning capabilities. Additionally, we\nexplore strategies to enhance logical reasoning, including Best-of-N sampling,\nbacktracking mechanisms, and self-verification prompts. Our findings offer\ncritical insights into the scalability of LLM reasoning, highlight fundamental\nlimitations, and outline potential directions for improvement.", "published": "2025-02-03 06:44:49", "link": "http://arxiv.org/abs/2502.01100v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) has proven effective in integrating\nknowledge into large language models (LLMs). However, conventional RAGs\nstruggle to capture complex relationships between pieces of knowledge, limiting\ntheir performance in intricate reasoning that requires integrating knowledge\nfrom multiple sources. Recently, graph-enhanced retrieval augmented generation\n(GraphRAG) builds graph structure to explicitly model these relationships,\nenabling more effective and efficient retrievers. Nevertheless, its performance\nis still hindered by the noise and incompleteness within the graph structure.\nTo address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for\nretrieval augmented generation. GFM-RAG is powered by an innovative graph\nneural network that reasons over graph structure to capture complex\nquery-knowledge relationships. The GFM with 8M parameters undergoes a two-stage\ntraining process on large-scale datasets, comprising 60 knowledge graphs with\nover 14M triples and 700k documents. This results in impressive performance and\ngeneralizability for GFM-RAG, making it the first graph foundation model\napplicable to unseen datasets for retrieval without any fine-tuning required.\nExtensive experiments on three multi-hop QA datasets and seven domain-specific\nRAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance\nwhile maintaining efficiency and alignment with neural scaling laws,\nhighlighting its potential for further improvement.", "published": "2025-02-03 07:04:29", "link": "http://arxiv.org/abs/2502.01113v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment\n  after Instruction Tuning", "abstract": "Large language models (LLMs) have emerged as powerful tools for addressing a\nwide range of general inquiries and tasks. Despite this, fine-tuning aligned\nLLMs on smaller, domain-specific datasets, critical to adapting them to\nspecialized tasks, can inadvertently degrade their safety alignment, even when\nthe datasets are benign. This phenomenon makes models more susceptible to\nproviding inappropriate responses. In this study, we systematically examine the\nfactors contributing to safety alignment degradation in benign fine-tuning\nscenarios. Our analysis identifies three critical factors affecting aligned\nLLMs: answer structure, identity calibration, and role-play. Additionally, we\nevaluate the reliability of state-of-the-art reward models (RMs), which are\noften used to guide alignment processes. Our findings reveal that these RMs\nfrequently fail to accurately reflect human preferences regarding safety,\nunderscoring their limitations in practical applications. By uncovering these\nchallenges, our work highlights the complexities of maintaining safety\nalignment during fine-tuning and offers guidance to help developers balance\nutility and safety in LLMs. Datasets and fine-tuning code used in our\nexperiments can be found in\nhttps://github.com/GuanlinLee/llm_instruction_tuning.", "published": "2025-02-03 07:09:09", "link": "http://arxiv.org/abs/2502.01116v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable potential in reasoning\nwhile they still suffer from severe factual hallucinations due to timeliness,\naccuracy, and coverage of parametric knowledge. Meanwhile, integrating\nreasoning with retrieval-augmented generation (RAG) remains challenging due to\nineffective task decomposition and redundant retrieval, which can introduce\nnoise and degrade response quality. In this paper, we propose DeepRAG, a\nframework that models retrieval-augmented reasoning as a Markov Decision\nProcess (MDP), enabling strategic and adaptive retrieval. By iteratively\ndecomposing queries, DeepRAG dynamically determines whether to retrieve\nexternal knowledge or rely on parametric reasoning at each step. Experiments\nshow that DeepRAG improves retrieval efficiency while improving answer accuracy\nby 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented\nreasoning.", "published": "2025-02-03 08:22:45", "link": "http://arxiv.org/abs/2502.01142v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Jailbreaking with Universal Multi-Prompts", "abstract": "Large language models (LLMs) have seen rapid development in recent years,\nrevolutionizing various applications and significantly enhancing convenience\nand productivity. However, alongside their impressive capabilities, ethical\nconcerns and new types of attacks, such as jailbreaking, have emerged. While\nmost prompting techniques focus on optimizing adversarial inputs for individual\ncases, resulting in higher computational costs when dealing with large\ndatasets. Less research has addressed the more general setting of training a\nuniversal attacker that can transfer to unseen tasks. In this paper, we\nintroduce JUMP, a prompt-based method designed to jailbreak LLMs using\nuniversal multi-prompts. We also adapt our approach for defense, which we term\nDUMP. Experimental results demonstrate that our method for optimizing universal\nmulti-prompts outperforms existing techniques.", "published": "2025-02-03 08:44:24", "link": "http://arxiv.org/abs/2502.01154v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning", "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting techniques, which modify the activations of specific model components.\nThese methods, due to their extremely small parameter counts, show promise for\nsmall datasets. However, their performance is highly dependent on identifying\nthe correct modules to edit and often lacks stability across different\ndatasets. In this paper, we propose Joint Localization and Activation Editing\n(JoLA), a method that jointly learns (1) which heads in the Transformer to edit\n(2) whether the intervention should be additive, multiplicative, or both and\n(3) the intervention parameters themselves - the vectors applied as additive\noffsets or multiplicative scalings to the head output. Through evaluations on\nthree benchmarks spanning commonsense reasoning, natural language\nunderstanding, and natural language generation, we demonstrate that JoLA\nconsistently outperforms existing methods.", "published": "2025-02-03 09:13:09", "link": "http://arxiv.org/abs/2502.01179v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skewed Memorization in Large Language Models: Quantification and\n  Decomposition", "abstract": "Memorization in Large Language Models (LLMs) poses privacy and security\nrisks, as models may unintentionally reproduce sensitive or copyrighted data.\nExisting analyses focus on average-case scenarios, often neglecting the highly\nskewed distribution of memorization. This paper examines memorization in LLM\nsupervised fine-tuning (SFT), exploring its relationships with training\nduration, dataset size, and inter-sample similarity. By analyzing memorization\nprobabilities over sequence lengths, we link this skewness to the token\ngeneration process, offering insights for estimating memorization and comparing\nit to established metrics. Through theoretical analysis and empirical\nevaluation, we provide a comprehensive understanding of memorization behaviors\nand propose strategies to detect and mitigate risks, contributing to more\nprivacy-preserving LLMs.", "published": "2025-02-03 09:23:53", "link": "http://arxiv.org/abs/2502.01187v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Eliciting Language Model Behaviors with Investigator Agents", "abstract": "Language models exhibit complex, diverse behaviors when prompted with\nfree-form text, making it difficult to characterize the space of possible\noutputs. We study the problem of behavior elicitation, where the goal is to\nsearch for prompts that induce specific target behaviors (e.g., hallucinations\nor harmful responses) from a target language model. To navigate the\nexponentially large space of possible prompts, we train investigator models to\nmap randomly-chosen target behaviors to a diverse distribution of outputs that\nelicit them, similar to amortized Bayesian inference. We do this through\nsupervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe\ntraining objective to iteratively discover diverse prompting strategies. Our\ninvestigator models surface a variety of effective and human-interpretable\nprompts leading to jailbreaks, hallucinations, and open-ended aberrant\nbehaviors, obtaining a 100% attack success rate on a subset of AdvBench\n(Harmful Behaviors) and an 85% hallucination rate.", "published": "2025-02-03 10:52:44", "link": "http://arxiv.org/abs/2502.01236v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PSSD: Making Large Language Models Self-denial via Human Psyche\n  Structure", "abstract": "The enhance of accuracy in reasoning results of LLMs arouses the community's\ninterests, wherein pioneering studies investigate post-hoc strategies to\nrectify potential mistakes. Despite extensive efforts, they are all stuck in a\nstate of resource competition demanding significant time and computing\nexpenses. The cause of the situation lies in the failure of identifying the\nfundamental feature of the solutions in this line, coined as the self-denial of\nLLMs. In other words, LLMs should confidently determine the potential existence\nof mistakes and carefully execute the targeted correction. As the whole\nprocedure conducts within LLMs, supporting and persuasive references are hard\nto acquire, while the absence of specific steps towards refining hidden\nmistakes persists even when errors are acknowledged. In response to the\nchallenges, we present PSSD, which refers to and implements the human psyche\nstructure such that three distinct and interconnected roles contribute to human\nreasoning. Specifically, PSSD leverages the recent multi-agent paradigm, and is\nfurther enhanced with three innovatively conceived roles: (1) the\nintuition-based id role that provides initial attempts based on benign LLMs;\n(2) the rule-driven superego role that summarizes rules to regulate the above\nattempts, and returns specific key points as guidance; and (3) the\nscript-centric ego role that absorbs all procedural information to generate\nexecutable script for the final answer prediction. Extensive experiments\ndemonstrate that the proposed design not only better enhance reasoning\ncapabilities, but also seamlessly integrate with current models, leading to\nsuperior performance.", "published": "2025-02-03 13:37:21", "link": "http://arxiv.org/abs/2502.01344v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods", "abstract": "Discrete diffusion models have recently gained significant attention due to\ntheir ability to process complex discrete structures for language modeling.\nHowever, fine-tuning these models with policy gradient methods, as is commonly\ndone in Reinforcement Learning from Human Feedback (RLHF), remains a\nchallenging task. We propose an efficient, broadly applicable, and\ntheoretically justified policy gradient algorithm, called Score Entropy Policy\nOptimization (SEPO), for fine-tuning discrete diffusion models over\nnon-differentiable rewards. Our numerical experiments across several discrete\ngenerative tasks demonstrate the scalability and efficiency of our method. Our\ncode is available at https://github.com/ozekri/SEPO", "published": "2025-02-03 14:20:19", "link": "http://arxiv.org/abs/2502.01384v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models", "abstract": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research.", "published": "2025-02-03 14:21:42", "link": "http://arxiv.org/abs/2502.01386v2", "categories": ["cs.CL", "cs.CR", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models", "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD.", "published": "2025-02-03 14:34:37", "link": "http://arxiv.org/abs/2502.01403v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GRADIEND: Monosemantic Feature Learning within Neural Networks Applied\n  to Gender Debiasing of Transformer Models", "abstract": "AI systems frequently exhibit and amplify social biases, including gender\nbias, leading to harmful consequences in critical areas. This study introduces\na novel encoder-decoder approach that leverages model gradients to learn a\nsingle monosemantic feature neuron encoding gender information. We show that\nour method can be used to debias transformer-based language models, while\nmaintaining other capabilities. We demonstrate the effectiveness of our\napproach across multiple encoder-only based models and highlight its potential\nfor broader applications.", "published": "2025-02-03 14:38:27", "link": "http://arxiv.org/abs/2502.01406v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Process Reinforcement through Implicit Rewards", "abstract": "Dense process rewards have proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling of large language models\n(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for the reinforcement learning\n(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of training process reward models (PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable to reward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using only policy rollouts and outcome labels through\nimplict process rewards. PRIME combines well with various advantage functions\nand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting from\nQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkey reasoning benchmarks over the SFT model. Notably, our resulting model,\nEurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning\nbenchmarks with 10% of its training data.", "published": "2025-02-03 15:43:48", "link": "http://arxiv.org/abs/2502.01456v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hybrid Machine Learning Model for Detecting Bangla Smishing Text Using\n  BERT and Character-Level CNN", "abstract": "Smishing is a social engineering attack using SMS containing malicious\ncontent to deceive individuals into disclosing sensitive information or\ntransferring money to cybercriminals. Smishing attacks have surged by 328%,\nposing a major threat to mobile users, with losses exceeding \\$54.2 million in\n2019. Despite its growing prevalence, the issue remains significantly\nunder-addressed. This paper presents a novel hybrid machine learning model for\ndetecting Bangla smishing texts, combining Bidirectional Encoder\nRepresentations from Transformers (BERT) with Convolutional Neural Networks\n(CNNs) for enhanced character-level analysis.\n  Our model addresses multi-class classification by distinguishing between\nNormal, Promotional, and Smishing SMS. Unlike traditional binary classification\nmethods, our approach integrates BERT's contextual embeddings with CNN's\ncharacter-level features, improving detection accuracy. Enhanced by an\nattention mechanism, the model effectively prioritizes crucial text segments.\nOur model achieves 98.47% accuracy, outperforming traditional classifiers, with\nhigh precision and recall in Smishing detection, and strong performance across\nall categories.", "published": "2025-02-03 16:51:58", "link": "http://arxiv.org/abs/2502.01518v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Efficiently Integrate Large Language Models with Visual Perception: A\n  Survey from the Training Paradigm Perspective", "abstract": "The integration of vision-language modalities has been a significant focus in\nmultimodal learning, traditionally relying on Vision-Language Pretrained\nModels. However, with the advent of Large Language Models (LLMs), there has\nbeen a notable shift towards incorporating LLMs with vision modalities.\nFollowing this, the training paradigms for incorporating vision modalities into\nLLMs have evolved. Initially, the approach was to integrate the modalities\nthrough pretraining the modality integrator, named Single-stage Tuning. It has\nsince branched out into methods focusing on performance enhancement, denoted as\nTwo-stage Tuning, and those prioritizing parameter efficiency, referred to as\nDirect Adaptation. However, existing surveys primarily address the latest\nVision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in\nunderstanding the evolution of training paradigms and their unique\nparameter-efficient considerations. This paper categorizes and reviews 34 VLLMs\nfrom top conferences, journals, and highly cited Arxiv papers, focusing on\nparameter efficiency during adaptation from the training paradigm perspective.\nWe first introduce the architecture of LLMs and parameter-efficient learning\nmethods, followed by a discussion on vision encoders and a comprehensive\ntaxonomy of modality integrators. We then review three training paradigms and\ntheir efficiency considerations, summarizing benchmarks in the VLLM field. To\ngain deeper insights into their effectiveness in parameter efficiency, we\ncompare and discuss the experimental results of representative models, among\nwhich the experiment of the Direct Adaptation paradigm is replicated. Providing\ninsights into recent developments and practical uses, this survey is a vital\nguide for researchers and practitioners navigating the efficient integration of\nvision modalities into LLMs.", "published": "2025-02-03 17:01:59", "link": "http://arxiv.org/abs/2502.01524v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The in-context inductive biases of vision-language models differ across\n  modalities", "abstract": "Inductive biases are what allow learners to make guesses in the absence of\nconclusive evidence. These biases have often been studied in cognitive science\nusing concepts or categories -- e.g. by testing how humans generalize a new\ncategory from a few examples that leave the category boundary ambiguous. We use\nthese approaches to study generalization in foundation models during in-context\nlearning. Modern foundation models can condition on both vision and text, and\ndifferences in how they interpret and learn from these different modalities is\nan emerging area of study. Here, we study how their generalizations vary by the\nmodality in which stimuli are presented, and the way the stimuli are described\nin text. We study these biases with three different experimental paradigms,\nacross three different vision-language models. We find that the models\ngenerally show some bias towards generalizing according to shape over color.\nThis shape bias tends to be amplified when the examples are presented visually.\nBy contrast, when examples are presented in text, the ordering of adjectives\naffects generalization. However, the extent of these effects vary across models\nand paradigms. These results help to reveal how vision-language models\nrepresent different types of inputs in context, and may have practical\nimplications for the use of vision-language models.", "published": "2025-02-03 17:11:03", "link": "http://arxiv.org/abs/2502.01530v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "abstract": "Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between data\ngenerator LLM and judge LLM: being the same model, having an inheritance\nrelationship, and belonging to the same model family. Through extensive\nexperiments, we empirically confirm the bias of judges towards their related\nstudent models caused by preference leakage across multiple LLM baselines and\nbenchmarks. Further analysis suggests that preference leakage is a pervasive\nissue that is harder to detect compared to previously identified biases in\nLLM-as-a-judge scenarios. All of these findings imply that preference leakage\nis a widespread and challenging problem in the area of LLM-as-a-judge. We\nrelease all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage.", "published": "2025-02-03 17:13:03", "link": "http://arxiv.org/abs/2502.01534v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VisTA: Vision-Text Alignment Model with Contrastive Learning using\n  Multimodal Data for Evidence-Driven, Reliable, and Explainable Alzheimer's\n  Disease Diagnosis", "abstract": "Objective: Assessing Alzheimer's disease (AD) using high-dimensional\nradiology images is clinically important but challenging. Although Artificial\nIntelligence (AI) has advanced AD diagnosis, it remains unclear how to design\nAI models embracing predictability and explainability. Here, we propose VisTA,\na multimodal language-vision model assisted by contrastive learning, to\noptimize disease prediction and evidence-based, interpretable explanations for\nclinical decision-making.\n  Methods: We developed VisTA (Vision-Text Alignment Model) for AD diagnosis.\nArchitecturally, we built VisTA from BiomedCLIP and fine-tuned it using\ncontrastive learning to align images with verified abnormalities and their\ndescriptions. To train VisTA, we used a constructed reference dataset\ncontaining images, abnormality types, and descriptions verified by medical\nexperts. VisTA produces four outputs: predicted abnormality type, similarity to\nreference cases, evidence-driven explanation, and final AD diagnoses. To\nillustrate VisTA's efficacy, we reported accuracy metrics for abnormality\nretrieval and dementia prediction. To demonstrate VisTA's explainability, we\ncompared its explanations with human experts' explanations.\n  Results: Compared to 15 million images used for baseline pretraining, VisTA\nonly used 170 samples for fine-tuning and obtained significant improvement in\nabnormality retrieval and dementia prediction. For abnormality retrieval, VisTA\nreached 74% accuracy and an AUC of 0.87 (26% and 0.74, respectively, from\nbaseline models). For dementia prediction, VisTA achieved 88% accuracy and an\nAUC of 0.82 (30% and 0.57, respectively, from baseline models). The generated\nexplanations agreed strongly with human experts' and provided insights into the\ndiagnostic process. Taken together, VisTA optimize prediction, clinical\nreasoning, and explanation.", "published": "2025-02-03 17:15:01", "link": "http://arxiv.org/abs/2502.01535v1", "categories": ["cs.CV", "cs.CL", "q-bio.QM"], "primary_category": "cs.CV"}
{"title": "Scalable Language Models with Posterior Inference of Latent Thought\n  Vectors", "abstract": "We propose a novel family of language models, Latent-Thought Language Models\n(LTMs), which incorporate explicit latent thought vectors that follow an\nexplicit prior model in latent space. These latent thought vectors guide the\nautoregressive generation of ground tokens through a Transformer decoder.\nTraining employs a dual-rate optimization process within the classical\nvariational Bayes framework: fast learning of local variational parameters for\nthe posterior distribution of latent vectors, and slow learning of global\ndecoder parameters. Empirical studies reveal that LTMs possess additional\nscaling dimensions beyond traditional LLMs, yielding a structured design space.\nHigher sample efficiency can be achieved by increasing training compute per\ntoken, with further gains possible by trading model size for more inference\nsteps. Designed based on these scaling properties, LTMs demonstrate superior\nsample and parameter efficiency compared to conventional autoregressive models\nand discrete diffusion models. They significantly outperform these counterparts\nin validation perplexity and zero-shot language modeling. Additionally, LTMs\nexhibit emergent few-shot in-context reasoning capabilities that scale with\nmodel and latent size, and achieve competitive performance in conditional and\nunconditional text generation.", "published": "2025-02-03 17:50:34", "link": "http://arxiv.org/abs/2502.01567v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Unit Tests for Automated Debugging", "abstract": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B.", "published": "2025-02-03 18:51:43", "link": "http://arxiv.org/abs/2502.01619v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Lifelong Sequential Knowledge Editing without Model Degradation", "abstract": "Prior work in parameter-modifying knowledge editing has shown that\nlarge-scale sequential editing leads to significant model degradation. In this\npaper, we study the reasons behind this and scale sequential knowledge editing\nto 10,000 sequential edits, while maintaining the downstream performance of the\noriginal model. We first show that locate-then-edit knowledge editing methods\nlead to overfitting on the edited facts. We also show that continuous knowledge\nediting using these methods leads to disproportionate growth in the norm of the\nedited matrix. We then provide a crucial insight into the inner workings of\nlocate-then-edit methods. We show that norm-growth is a hidden trick employed\nby these methods that gives larger importance to the output activations\nproduced from the edited layers. With this \"importance hacking\", the edited\nlayers provide a much larger contributions to the model's output. To mitigate\nthese issues, we present ENCORE - Early stopping and Norm-Constrained Robust\nknowledge Editing. ENCORE controls for overfitting and the disproportionate\nnorm-growth to enable long-term sequential editing, where we are able to\nperform up to 10,000 sequential edits without loss of downstream performance.\nENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on\nLlama3-8B.", "published": "2025-02-03 18:59:14", "link": "http://arxiv.org/abs/2502.01636v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BARE: Combining Base and Instruction-Tuned Language Models for Better\n  Synthetic Data Generation", "abstract": "As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs. A\ncommon assumption about synthetic data is that sampling from instruct-tuned\nmodels is sufficient; however, these models struggle to produce diverse\noutputs-a key requirement for generalization. Despite various prompting\nmethods, in this work we show that achieving meaningful diversity from\ninstruct-tuned models remains challenging. In contrast, we find base models\nwithout post-training exhibit greater diversity, but are less capable at\ninstruction following and hence of lower quality. Leveraging this insight, we\npropose Base-Refine (BARE), a synthetic data generation method that combines\nthe diversity of base models with the quality of instruct-tuned models through\na two-stage process. With minimal few-shot examples and curation, BARE\ngenerates diverse and high-quality datasets, improving downstream task\nperformance. We show that fine-tuning with as few as 1,000 BARE-generated\nsamples can reach performance comparable to the best similarly sized models on\nLiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves\na 101% improvement over instruct-only data on GSM8K and a 18.4% improvement\nover SOTA methods on RAFT.", "published": "2025-02-03 00:12:40", "link": "http://arxiv.org/abs/2502.01697v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large\n  Language Model Fine-Tuning", "abstract": "Fine-tuning large language models (LLMs) is often constrained by the\ncomputational costs of processing massive datasets. We propose \\textbf{QLESS}\n(Quantized Low-rank Gradient Similarity Search), which integrates gradient\nquantization with the LESS framework to enable memory-efficient data valuation\nand selection. QLESS employs a two-step compression process: first, it obtains\nlow-dimensional gradient representations through LoRA-based random projection;\nthen, it quantizes these gradients to low-bitwidth representations. Experiments\non multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH,\nTyDiQA) show that QLESS achieves comparable data selection performance to LESS\nwhile reducing memory usage by up to 16x. Even 1-bit gradient quantization\npreserves data valuation quality. These findings underscore QLESS as a\npractical, scalable approach to identifying informative examples within strict\nmemory constraints.", "published": "2025-02-03 10:52:32", "link": "http://arxiv.org/abs/2502.01703v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Comply: Learning Sentences with Complex Weights inspired by Fruit Fly\n  Olfaction", "abstract": "Biologically inspired neural networks offer alternative avenues to model data\ndistributions. FlyVec is a recent example that draws inspiration from the fruit\nfly's olfactory circuit to tackle the task of learning word embeddings.\nSurprisingly, this model performs competitively even against deep learning\napproaches specifically designed to encode text, and it does so with the\nhighest degree of computational efficiency. We pose the question of whether\nthis performance can be improved further. For this, we introduce Comply. By\nincorporating positional information through complex weights, we enable a\nsingle-layer neural network to learn sequence representations. Our experiments\nshow that Comply not only supersedes FlyVec but also performs on par with\nsignificantly larger state-of-the-art models. We achieve this without\nadditional parameters. Comply yields sparse contextual representations of\nsentences that can be interpreted explicitly from the neuron weights.", "published": "2025-02-03 13:30:44", "link": "http://arxiv.org/abs/2502.01706v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis", "abstract": "Most progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveraging\nautomated large-scale test-case synthesis to enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we construct\npreference pairs based on pass rates over sampled programs to train reward\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\nFurthermore, we conduct reinforcement learning with both reward models and\ntest-case pass rewards, leading to consistent improvements across HumanEval,\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential of reinforcement learning in coder models.", "published": "2025-02-03 18:46:04", "link": "http://arxiv.org/abs/2502.01718v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Evaluation of Large Language Models via Coupled Token Generation", "abstract": "State of the art large language models rely on randomization to respond to a\nprompt. As an immediate consequence, a model may respond differently to the\nsame prompt if asked multiple times. In this work, we argue that the evaluation\nand ranking of large language models should control for the randomization\nunderpinning their functioning. Our starting point is the development of a\ncausal model for coupled autoregressive generation, which allows different\nlarge language models to sample responses with the same source of randomness.\nBuilding upon our causal model, we first show that, on evaluations based on\nbenchmark datasets, coupled autoregressive generation leads to the same\nconclusions as vanilla autoregressive generation but using provably fewer\nsamples. However, we further show that, on evaluations based on (human)\npairwise comparisons, coupled and vanilla autoregressive generation can\nsurprisingly lead to different rankings when comparing more than two models,\neven with an infinite amount of samples. This suggests that the apparent\nadvantage of a model over others in existing evaluation protocols may not be\ngenuine but rather confounded by the randomness inherent to the generation\nprocess. To illustrate and complement our theoretical results, we conduct\nexperiments with several large language models from the Llama family. We find\nthat, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the\nsame conclusions as vanilla autoregressive generation. Further, using data from\nthe LMSYS Chatbot Arena platform, we find that the win-rates derived from\npairwise comparisons by a strong large language model to prompts differ under\ncoupled and vanilla autoregressive generation.", "published": "2025-02-03 19:01:17", "link": "http://arxiv.org/abs/2502.01754v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Bob Dylan: A Computational Perspective", "abstract": "Cass Sunstein's essay 'On Bob Dylan' describes Dylan's 'dishabituating' style\n-- a constant refusal to conform to expectation and a penchant for reinventing\nhis musical and lyrical identity. In this paper, I extend Sunstein's\nobservations through a large-scale computational analysis of Dylan's lyrics\nfrom 1962 to 2012. Using o3-mini-high (a large language model), I extract\nconcept-to-concept relationships from the lyrics and construct directed\nknowledge graphs that capture Dylan's thematic structure. I then quantify\nshifts in sentiment, metaphorical expression, thematic diversity, and network\ncomplexity over time. The results indicate that Dylan's lyrics increasingly\nrely on metaphor, display an evolving sentiment profile, and exhibit heightened\ndishabituation -- measured here as a growing variance in the network centrality\nof key concepts. I also find that references to movement, protest, and mythic\nimagery fluctuate in ways that align with well-known phases of Dylan's career,\nreflecting the dynamic and unpredictable quality of his art. These findings not\nonly deepen our empirical understanding of Sunstein's thesis but also introduce\na novel computational method for analyzing an artist's evolution-offering\nbroader applicability to the study of cultural and creative change.", "published": "2025-02-03 19:25:08", "link": "http://arxiv.org/abs/2502.01772v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech\n  Recognition", "abstract": "Modern deep learning models often achieve high overall performance, but\nconsistently fail on specific subgroups. Group distributionally robust\noptimization (group DRO) addresses this problem by minimizing the worst-group\nloss, but it fails when group losses misrepresent performance differences\nbetween groups. This is common in domains like speech, where the widely used\nconnectionist temporal classification (CTC) loss scales with input length and\nvaries with linguistic and acoustic properties, leading to spurious differences\nbetween group losses. We present CTC-DRO, which addresses the shortcomings of\nthe group DRO objective by smoothing the group weight update to prevent\noveremphasis on consistently high-loss groups, while using input length-matched\nbatching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of\nmultilingual automatic speech recognition (ASR) across five language sets from\nthe ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and\nCTC-based baseline models, reducing the worst-language error by up to 47.1% and\nthe average error by up to 32.9%. CTC-DRO can be applied to ASR with minimal\ncomputational costs, and offers the potential for reducing group disparities in\nother domains with similar challenges.", "published": "2025-02-03 19:29:42", "link": "http://arxiv.org/abs/2502.01777v2", "categories": ["cs.LG", "cs.CL", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Tutorial on Using Machine Learning and Deep Learning Models for Mental\n  Illness Detection", "abstract": "Social media has become an important source for understanding mental health,\nproviding researchers with a way to detect conditions like depression from\nuser-generated posts. This tutorial provides practical guidance to address\ncommon challenges in applying machine learning and deep learning methods for\nmental health detection on these platforms. It focuses on strategies for\nworking with diverse datasets, improving text preprocessing, and addressing\nissues such as imbalanced data and model evaluation. Real-world examples and\nstep-by-step instructions demonstrate how to apply these techniques\neffectively, with an emphasis on transparency, reproducibility, and ethical\nconsiderations. By sharing these approaches, this tutorial aims to help\nresearchers build more reliable and widely applicable models for mental health\nresearch, contributing to better tools for early detection and intervention.", "published": "2025-02-03 06:43:12", "link": "http://arxiv.org/abs/2502.04342v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logits are All We Need to Adapt Closed Models", "abstract": "Many commercial Large Language Models (LLMs) are often closed-source,\nlimiting developers to prompt tuning for aligning content generation with\nspecific applications. While these models currently do not provide access to\ntoken logits, we argue that if such access were available, it would enable more\npowerful adaptation techniques beyond prompt engineering. In this paper, we\npropose a token-level probability reweighting framework that, given access to\nlogits and a small amount of task-specific data, can effectively steer\nblack-box LLMs toward application-specific content generation. Our approach\nviews next-token prediction through the lens of supervised classification. We\nshow that aligning black-box LLMs with task-specific data can be formulated as\na label noise correction problem, leading to \\emph{Plugin} model -- an\nautoregressive probability reweighting model that operates solely on logits. We\nprovide theoretical justification for why reweighting logits alone is\nsufficient for task adaptation. Extensive experiments with multiple datasets,\nLLMs, and reweighting models demonstrate the effectiveness of our method,\nadvocating for broader access to token logits in closed-source models.", "published": "2025-02-03 22:24:22", "link": "http://arxiv.org/abs/2502.06806v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Competitive Programming with Large Reasoning Models", "abstract": "We show that reinforcement learning applied to large language models (LLMs)\nsignificantly boosts performance on complex coding and reasoning tasks.\nAdditionally, we compare two general-purpose reasoning models - OpenAI o1 and\nan early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses\nhand-engineered inference strategies designed for competing in the 2024\nInternational Olympiad in Informatics (IOI). We competed live at IOI 2024 with\no1-ioi and, using hand-crafted test-time strategies, placed in the 49th\npercentile. Under relaxed competition constraints, o1-ioi achieved a gold\nmedal. However, when evaluating later models such as o3, we find that o3\nachieves gold without hand-crafted domain-specific strategies or relaxed\nconstraints. Our findings show that although specialized pipelines such as\no1-ioi yield solid improvements, the scaled-up, general-purpose o3 model\nsurpasses those results without relying on hand-crafted inference heuristics.\nNotably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces\nrating on par with elite human competitors. Overall, these results indicate\nthat scaling general-purpose reinforcement learning, rather than relying on\ndomain-specific techniques, offers a robust path toward state-of-the-art AI in\nreasoning domains, such as competitive programming.", "published": "2025-02-03 23:00:15", "link": "http://arxiv.org/abs/2502.06807v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Standardizing Intelligence: Aligning Generative AI for Regulatory and\n  Operational Compliance", "abstract": "Technical standards, or simply standards, are established documented\nguidelines and rules that facilitate the interoperability, quality, and\naccuracy of systems and processes. In recent years, we have witnessed an\nemerging paradigm shift where the adoption of generative AI (GenAI) models has\nincreased tremendously, spreading implementation interests across\nstandard-driven industries, including engineering, legal, healthcare, and\neducation. In this paper, we assess the criticality levels of different\nstandards across domains and sectors and complement them by grading the current\ncompliance capabilities of state-of-the-art GenAI models. To support the\ndiscussion, we outline possible challenges and opportunities with integrating\nGenAI for standard compliance tasks while also providing actionable\nrecommendations for entities involved with developing and using standards.\nOverall, we argue that aligning GenAI with standards through computational\nmethods can help strengthen regulatory and operational compliance. We\nanticipate this area of research will play a central role in the management,\noversight, and trustworthiness of larger, more powerful GenAI-based systems in\nthe near future.", "published": "2025-02-03 16:55:01", "link": "http://arxiv.org/abs/2503.04736v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "From Divergence to Consensus: Evaluating the Role of Large Language\n  Models in Facilitating Agreement through Adaptive Strategies", "abstract": "Achieving consensus in group decision-making often involves overcoming\nsignificant challenges, particularly in reconciling diverse perspectives and\nmitigating biases that hinder agreement. Traditional methods relying on human\nfacilitators are often constrained by scalability and efficiency, especially in\nlarge-scale, fast-paced discussions. To address these challenges, this study\nproposes a novel framework employing large language models (LLMs) as automated\nfacilitators within a custom-built multi-user chat system. Leveraging cosine\nsimilarity as a core metric, this approach evaluates the ability of three\nstate-of-the-art LLMs- ChatGPT 4.0, Mistral Large 2, and AI21 Jamba Instruct-\nto synthesize consensus proposals that align with participants' viewpoints.\nUnlike conventional techniques, the system integrates adaptive facilitation\nstrategies, including clarifying misunderstandings, summarizing discussions,\nand proposing compromises, enabling the LLMs to iteratively refine consensus\nproposals based on user feedback. Experimental results demonstrate the\nsuperiority of ChatGPT 4.0, which achieves higher alignment with participant\nopinions, requiring fewer iterations to reach consensus compared to its\ncounterparts. Moreover, analysis reveals the nuanced performance of the models\nacross various sustainability-focused discussion topics, such as climate\naction, quality education, good health and well-being, and access to clean\nwater and sanitation. These findings highlight the transformative potential of\nLLM-driven facilitation for improving collective decision-making processes and\nunderscore the importance of advancing evaluation metrics and cross-cultural\nadaptability in future research.", "published": "2025-02-03 09:59:55", "link": "http://arxiv.org/abs/2503.15521v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Learnable polynomial, trigonometric, and tropical activations", "abstract": "This paper investigates scalable neural networks with learnable activation\nfunctions based on orthogonal function bases and tropical polynomials,\ntargeting ImageNet-1K classification and next token prediction on OpenWebText.\nTraditional activations, such as ReLU, are static. In contrast, learnable\nactivations enable the network to adapt dynamically during training. However,\nstability issues, such as vanishing or exploding gradients, arise with improper\nvariance management in deeper networks. To remedy this, we propose an\ninitialization scheme that single-handedly preserves unitary variance in\ntransformers and convolutional networks, ensuring stable gradient flow even in\ndeep architectures. Extensive experiments demonstrate that networks with\nHermite, Fourier, and Tropical-based learnable activations significantly\nimprove over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in\ntrain and test, highlighting the viability of learnable activations in\nlarge-scale tasks. The activation functions developed here are the subject of a\nlibrary coded entirely in pure PyTorch: torchortho, available at\nhttps://github.com/K-H-Ismail/torchortho.", "published": "2025-02-03 11:13:58", "link": "http://arxiv.org/abs/2502.01247v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "math.AG"], "primary_category": "cs.LG"}
{"title": "Meursault as a Data Point", "abstract": "In an era dominated by datafication, the reduction of human experiences to\nquantifiable metrics raises profound philosophical and ethical questions. This\npaper explores these issues through the lens of Meursault, the protagonist of\nAlbert Camus' The Stranger, whose emotionally detached existence epitomizes the\nexistential concept of absurdity. Using natural language processing (NLP)\ntechniques including emotion detection (BERT), sentiment analysis (VADER), and\nnamed entity recognition (spaCy)-this study quantifies key events and behaviors\nin Meursault's life. Our analysis reveals the inherent limitations of applying\nalgorithmic models to complex human experiences, particularly those rooted in\nexistential alienation and moral ambiguity. By examining how modern AI tools\nmisinterpret Meursault's actions and emotions, this research underscores the\nbroader ethical dilemmas of reducing nuanced human narratives to data points,\nchallenging the foundational assumptions of our data-driven society. The\nfindings presented in this paper serve as a critique of the increasing reliance\non data-driven narratives and advocate for incorporating humanistic values in\nartificial intelligence.", "published": "2025-02-03 13:56:48", "link": "http://arxiv.org/abs/2502.01364v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Multimodal Inverse Attention Network with Intrinsic Discriminant Feature\n  Exploitation for Fake News Detection", "abstract": "Multimodal fake news detection has garnered significant attention due to its\nprofound implications for social security. While existing approaches have\ncontributed to understanding cross-modal consistency, they often fail to\nleverage modal-specific representations and explicit discrepant features. To\naddress these limitations, we propose a Multimodal Inverse Attention Network\n(MIAN), a novel framework that explores intrinsic discriminative features based\non news content to advance fake news detection. Specifically, MIAN introduces a\nhierarchical learning module that captures diverse intra-modal relationships\nthrough local-to-global and local-to-local interactions, thereby generating\nenhanced unimodal representations to improve the identification of fake news at\nthe intra-modal level. Additionally, a cross-modal interaction module employs a\nco-attention mechanism to establish and model dependencies between the refined\nunimodal representations, facilitating seamless semantic integration across\nmodalities. To explicitly extract inconsistency features, we propose an inverse\nattention mechanism that effectively highlights the conflicting patterns and\nsemantic deviations introduced by fake news in both intra- and inter-modality.\nExtensive experiments on benchmark datasets demonstrate that MIAN significantly\noutperforms state-of-the-art methods, underscoring its pivotal contribution to\nadvancing social security through enhanced multimodal fake news detection.", "published": "2025-02-03 07:58:22", "link": "http://arxiv.org/abs/2502.01699v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Emotional Face-to-Speech", "abstract": "How much can we infer about an emotional voice solely from an expressive\nface? This intriguing question holds great potential for applications such as\nvirtual character dubbing and aiding individuals with expressive language\ndisorders. Existing face-to-speech methods offer great promise in capturing\nidentity characteristics but struggle to generate diverse vocal styles with\nemotional expression. In this paper, we explore a new task, termed emotional\nface-to-speech, aiming to synthesize emotional speech directly from expressive\nfacial cues. To that end, we introduce DEmoFace, a novel generative framework\nthat leverages a discrete diffusion transformer (DiT) with curriculum learning,\nbuilt upon a multi-level neural audio codec. Specifically, we propose\nmultimodal DiT blocks to dynamically align text and speech while tailoring\nvocal styles based on facial emotion and identity. To enhance training\nefficiency and generation quality, we further introduce a coarse-to-fine\ncurriculum learning algorithm for multi-level token processing. In addition, we\ndevelop an enhanced predictor-free guidance to handle diverse conditioning\nscenarios, enabling multi-conditional generation and disentangling complex\nattributes effectively. Extensive experimental results demonstrate that\nDEmoFace generates more natural and consistent speech compared to baselines,\neven surpassing speech-driven methods. Demos are shown at\nhttps://demoface-ai.github.io/.", "published": "2025-02-03 04:48:50", "link": "http://arxiv.org/abs/2502.01046v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Autoregressive Modeling with Stochastic Monotonic Alignment\n  for Speech Synthesis", "abstract": "We propose a novel autoregressive modeling approach for speech synthesis,\ncombining a variational autoencoder (VAE) with a multi-modal latent space and\nan autoregressive model that uses Gaussian Mixture Models (GMM) as the\nconditional probability distribution. Unlike previous methods that rely on\nresidual vector quantization, our model leverages continuous speech\nrepresentations from the VAE's latent space, greatly simplifying the training\nand inference pipelines. We also introduce a stochastic monotonic alignment\nmechanism to enforce strict monotonic alignments. Our approach significantly\noutperforms the state-of-the-art autoregressive model VALL-E in both subjective\nand objective evaluations, achieving these results with only 10.3\\% of VALL-E's\nparameters. This demonstrates the potential of continuous speech language\nmodels as a more efficient alternative to existing quantization-based speech\nlanguage models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.", "published": "2025-02-03 05:53:59", "link": "http://arxiv.org/abs/2502.01084v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech\n  Recognition", "abstract": "Backdoor attacks have posed a significant threat to the security of deep\nneural networks (DNNs). Despite considerable strides in developing defenses\nagainst backdoor attacks in the visual domain, the specialized defenses for the\naudio domain remain empty. Furthermore, the defenses adapted from the visual to\naudio domain demonstrate limited effectiveness. To fill this gap, we propose\nGradient Norm-based FineTuning (GN-FT), a novel defense strategy against the\nattacks in the audio domain, based on the observation from the corresponding\nbackdoored models. Specifically, we first empirically find that the backdoored\nneurons exhibit greater gradient values compared to other neurons, while clean\nneurons stay the lowest. On this basis, we fine-tune the backdoored model by\nincorporating the gradient norm regularization, aiming to weaken and reduce the\nbackdoored neurons. We further approximate the loss computation for lower\nimplementation costs. Extensive experiments on two speech recognition datasets\nacross five models demonstrate the superior performance of our proposed method.\nTo the best of our knowledge, this work is the first specialized and effective\ndefense against backdoor attacks in the audio domain.", "published": "2025-02-03 08:42:46", "link": "http://arxiv.org/abs/2502.01152v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech\n  Recognition", "abstract": "Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions.", "published": "2025-02-03 17:29:52", "link": "http://arxiv.org/abs/2502.01547v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Differentiable Alignment Framework for Sequence-to-Sequence Modeling\n  via Optimal Transport", "abstract": "Accurate sequence-to-sequence (seq2seq) alignment is critical for\napplications like medical speech analysis and language learning tools relying\non automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR\nsystems, such as the Connectionist Temporal Classification (CTC) and\ntransducer-based models, suffer from peaky behavior and alignment inaccuracies.\nIn this paper, we propose a novel differentiable alignment framework based on\none-dimensional optimal transport, enabling the model to learn a single\nalignment and perform ASR in an E2E manner. We introduce a pseudo-metric,\ncalled Sequence Optimal Transport Distance (SOTD), over the sequence space and\ndiscuss its theoretical properties. Based on the SOTD, we propose Optimal\nTemporal Transport Classification (OTTC) loss for ASR and contrast its behavior\nwith CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show\nthat our method considerably improves alignment performance, though with a\ntrade-off in ASR performance when compared to CTC. We believe this work opens\nnew avenues for seq2seq alignment research, providing a solid foundation for\nfurther exploration and development within the community.", "published": "2025-02-03 18:20:29", "link": "http://arxiv.org/abs/2502.01588v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adapter-Based Multi-Agent AVSR Extension for Pre-Trained ASR Models", "abstract": "We present an approach to Audio-Visual Speech Recognition that builds on a\npre-trained Whisper model. To infuse visual information into this audio-only\nmodel, we extend it with an AV fusion module and LoRa adapters, one of the most\nup-to-date adapter approaches. One advantage of adapter-based approaches, is\nthat only a relatively small number of parameters are trained, while the basic\nmodel remains unchanged. Common AVSR approaches train single models to handle\nseveral noise categories and noise levels simultaneously. Taking advantage of\nthe lightweight nature of adapter approaches, we train noise-scenario-specific\nadapter-sets, each covering individual noise-categories or a specific\nnoise-level range. The most suitable adapter-set is selected by previously\nclassifying the noise-scenario. This enables our models to achieve an optimum\ncoverage across different noise-categories and noise-levels, while training\nonly a minimum number of parameters.\n  Compared to a full fine-tuning approach with SOTA performance our models\nachieve almost comparable results over the majority of the tested\nnoise-categories and noise-levels, with up to 88.5% less trainable parameters.\nOur approach can be extended by further noise-specific adapter-sets to cover\nadditional noise scenarios. It is also possible to utilize the underlying\npowerful ASR model when no visual information is available, as it remains\nunchanged.", "published": "2025-02-03 14:54:47", "link": "http://arxiv.org/abs/2502.01709v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Active Speech Cancellation with Multi-Band Mamba Network", "abstract": "We present a novel deep learning network for Active Speech Cancellation\n(ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively\ncanceling both noise and speech signals. The proposed Multi-Band Mamba\narchitecture segments input audio into distinct frequency bands, enabling\nprecise anti-signal generation and improved phase alignment across frequencies.\nAdditionally, we introduce an optimization-driven loss function that provides\nnear-optimal supervisory signals for anti-signal generation. Experimental\nresults demonstrate substantial performance gains, achieving up to 7.2dB\nimprovement in ANC scenarios and 6.2dB in ASC, significantly outperforming\nexisting methods. Audio samples are available at\nhttps://mishalydev.github.io/DeepASC-Demo", "published": "2025-02-03 09:22:26", "link": "http://arxiv.org/abs/2502.01185v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
