{"title": "Challenging Language-Dependent Segmentation for Arabic: An Application\n  to Machine Translation and Part-of-Speech Tagging", "abstract": "Word segmentation plays a pivotal role in improving any Arabic NLP\napplication. Therefore, a lot of research has been spent in improving its\naccuracy. Off-the-shelf tools, however, are: i) complicated to use and ii)\ndomain/dialect dependent. We explore three language-independent alternatives to\nmorphological segmentation using: i) data-driven sub-word units, ii) characters\nas a unit of learning, and iii) word embeddings learned using a character CNN\n(Convolution Neural Network). On the tasks of Machine Translation and POS\ntagging, we found these methods to achieve close to, and occasionally surpass\nstate-of-the-art performance. In our analysis, we show that a neural machine\ntranslation system is sensitive to the ratio of source and target tokens, and a\nratio close to 1 or greater, gives optimal performance.", "published": "2017-09-02 18:45:48", "link": "http://arxiv.org/abs/1709.00616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "abstract": "Words in some natural languages can have a composite structure. Elements of\nthis structure include the root (that could also be composite), prefixes and\nsuffixes with which various nuances and relations to other words can be\nexpressed. Thus, in order to build a proper word representation one must take\ninto account its internal structure. From a corpus of texts we extract a set of\nfrequent subwords and from the latter set we select patterns, i.e. subwords\nwhich encapsulate information on character $n$-gram regularities. The selection\nis made using the pattern-based Conditional Random Field model with $l_1$\nregularization. Further, for every word we construct a new sequence over an\nalphabet of patterns. The new alphabet's symbols confine a local statistical\ncontext stronger than the characters, therefore they allow better\nrepresentations in ${\\mathbb{R}}^n$ and are better building blocks for word\nrepresentation. In the task of subword-aware language modeling, pattern-based\nmodels outperform character-based analogues by 2-20 perplexity points. Also, a\nrecurrent neural network in which a word is represented as a sum of embeddings\nof its patterns is on par with a competitive and significantly more\nsophisticated character-based convolutional architecture.", "published": "2017-09-02 07:00:22", "link": "http://arxiv.org/abs/1709.00541v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor\n  Detection", "abstract": "The ubiquity of metaphor in our everyday communication makes it an important\nproblem for natural language understanding. Yet, the majority of metaphor\nprocessing systems to date rely on hand-engineered features and there is still\nno consensus in the field as to which features are optimal for this task. In\nthis paper, we present the first deep learning architecture designed to capture\nmetaphorical composition. Our results demonstrate that it outperforms the\nexisting approaches in the metaphor identification task.", "published": "2017-09-02 13:13:06", "link": "http://arxiv.org/abs/1709.00575v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
