{"title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of\n  Knowledge Graph", "abstract": "Word embedding, which refers to low-dimensional dense vector representations\nof natural words, has demonstrated its power in many natural language\nprocessing tasks. However, it may suffer from the inaccurate and incomplete\ninformation contained in the free text corpus as training data. To tackle this\nchallenge, there have been quite a few works that leverage knowledge graphs as\nan additional information source to improve the quality of word embedding.\nAlthough these works have achieved certain success, they have neglected some\nimportant facts about knowledge graphs: (i) many relationships in knowledge\ngraphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many},\nrather than simply \\emph{one-to-one}; (ii) most head entities and tail entities\nin knowledge graphs come from very different semantic spaces. To address these\nissues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet\nmodels the relationships between head and tail entities after transforming them\nwith different low-rank projection matrices. The low-rank projection can allow\nnon \\emph{one-to-one} relationships between entities, while different\nprojection matrices for head and tail entities allow them to originate in\ndifferent semantic spaces. The experimental results demonstrate that ProjectNet\nyields more accurate word embedding than previous works, thus leads to clear\nimprovements in various natural language processing tasks.", "published": "2015-05-19 07:08:10", "link": "http://arxiv.org/abs/1505.04891v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Named Entity Recognition with Neural Character Embeddings", "abstract": "Most state-of-the-art named entity recognition (NER) systems rely on\nhandcrafted features and on the output of other NLP tasks such as\npart-of-speech (POS) tagging and text chunking. In this work we propose a\nlanguage-independent NER system that uses automatically learned features only.\nOur approach is based on the CharWNN deep neural network, which uses word-level\nand character-level representations (embeddings) to perform sequential\nclassification. We perform an extensive number of experiments using two\nannotated corpora in two different languages: HAREM I corpus, which contains\ntexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in\nSpanish. Our experimental results shade light on the contribution of neural\ncharacter embeddings for NER. Moreover, we demonstrate that the same neural\nnetwork which has been successfully applied to POS tagging can also achieve\nstate-of-the-art results for language-independet NER, using the same\nhyperparameters, and without any handcrafted features. For the HAREM I corpus,\nCharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score\nfor the total scenario (ten NE classes), and by 7.2 points in the F1 for the\nselective scenario (five NE classes).", "published": "2015-05-19 14:21:37", "link": "http://arxiv.org/abs/1505.05008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for\n  Richer Image-to-Sentence Models", "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based\nimage description. This paper presents Flickr30k Entities, which augments the\n158k captions from Flickr30k with 244k coreference chains, linking mentions of\nthe same entities across different captions for the same image, and associating\nthem with 276k manually annotated bounding boxes. Such annotations are\nessential for continued progress in automatic image description and grounded\nlanguage understanding. They enable us to define a new benchmark for\nlocalization of textual entity mentions in an image. We present a strong\nbaseline for this task that combines an image-text embedding, detectors for\ncommon objects, a color classifier, and a bias towards selecting larger\nobjects. While our baseline rivals in accuracy more complex state-of-the-art\nmodels, we show that its gains cannot be easily parlayed into improvements on\nsuch tasks as image-sentence retrieval, thus underlining the limitations of\ncurrent methods and the need for further research.", "published": "2015-05-19 04:46:03", "link": "http://arxiv.org/abs/1505.04870v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
