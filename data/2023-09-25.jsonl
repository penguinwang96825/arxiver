{"title": "Multiple evolutionary pressures shape identical consonant avoidance in\n  the world's languages", "abstract": "Languages disfavor word forms containing sequences of similar or identical\nconsonants, due to the biomechanical and cognitive difficulties posed by\npatterns of this sort. However, the specific evolutionary processes responsible\nfor this phenomenon are not fully understood. Words containing sequences of\nidentical consonants may be more likely to arise than those without; processes\nof word form mutation may be more likely to remove than create sequences of\nidentical consonants in word forms; finally, words containing identical\nconsonants may die out more frequently than those without. Phylogenetic\nanalyses of the evolution of homologous word forms indicate that words with\nidentical consonants arise less frequently than those without, and processes\nwhich mutate word forms are more likely to remove sequences of identical\nconsonants than introduce them. However, words with identical consonants do not\ndie out more frequently than those without. Further analyses reveal that forms\nwith identical consonants are replaced in basic meaning functions more\nfrequently than words without. Taken together, results suggest that the under\nrepresentation of sequences of identical consonants is overwhelmingly a\nbyproduct of constraints on word form coinage, though processes related to word\nusage also serve to ensure that such patterns are infrequent in more salient\nvocabulary items. These findings clarify previously unknown aspects of\nprocesses of lexical evolution and competition that take place during language\nchange, optimizing communicative systems.", "published": "2023-09-25 10:16:30", "link": "http://arxiv.org/abs/2309.14006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Temporal Bias in Abusive Language Detection", "abstract": "The use of abusive language online has become an increasingly pervasive\nproblem that damages both individuals and society, with effects ranging from\npsychological harm right through to escalation to real-life violence and even\ndeath. Machine learning models have been developed to automatically detect\nabusive language, but these models can suffer from temporal bias, the\nphenomenon in which topics, language use or social norms change over time. This\nstudy aims to investigate the nature and impact of temporal bias in abusive\nlanguage detection across various languages and explore mitigation methods. We\nevaluate the performance of models on abusive data sets from different time\nperiods. Our results demonstrate that temporal bias is a significant challenge\nfor abusive language detection, with models trained on historical data showing\na significant drop in performance over time. We also present an extensive\nlinguistic analysis of these abusive data sets from a diachronic perspective,\naiming to explore the reasons for language evolution and performance decline.\nThis study sheds light on the pervasive issue of temporal bias in abusive\nlanguage detection across languages, offering crucial insights into language\nevolution and temporal bias mitigation.", "published": "2023-09-25 13:59:39", "link": "http://arxiv.org/abs/2309.14146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards End-User Development for IoT: A Case Study on Semantic Parsing\n  of Cooking Recipes for Programming Kitchen Devices", "abstract": "Semantic parsing of user-generated instructional text, in the way of enabling\nend-users to program the Internet of Things (IoT), is an underexplored area. In\nthis study, we provide a unique annotated corpus which aims to support the\ntransformation of cooking recipe instructions to machine-understandable\ncommands for IoT devices in the kitchen. Each of these commands is a tuple\ncapturing the semantics of an instruction involving a kitchen device in terms\nof \"What\", \"Where\", \"Why\" and \"How\". Based on this corpus, we developed machine\nlearning-based sequence labelling methods, namely conditional random fields\n(CRF) and a neural network model, in order to parse recipe instructions and\nextract our tuples of interest from them. Our results show that while it is\nfeasible to train semantic parsers based on our annotations, most\nnatural-language instructions are incomplete, and thus transforming them into\nformal meaning representation, is not straightforward.", "published": "2023-09-25 14:21:24", "link": "http://arxiv.org/abs/2309.14165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Only 5\\% Attention Is All You Need: Efficient Long-range Document-level\n  Neural Machine Translation", "abstract": "Document-level Neural Machine Translation (DocNMT) has been proven crucial\nfor handling discourse phenomena by introducing document-level context\ninformation. One of the most important directions is to input the whole\ndocument directly to the standard Transformer model. In this case, efficiency\nbecomes a critical concern due to the quadratic complexity of the attention\nmodule. Existing studies either focus on the encoder part, which cannot be\ndeployed on sequence-to-sequence generation tasks, e.g., Machine Translation\n(MT), or suffer from a significant performance drop. In this work, we keep the\ntranslation performance while gaining 20\\% speed up by introducing extra\nselection layer based on lightweight attention that selects a small portion of\ntokens to be attended. It takes advantage of the original attention to ensure\nperformance and dimension reduction to accelerate inference. Experimental\nresults show that our method could achieve up to 95\\% sparsity (only 5\\% tokens\nattended) approximately, and save 93\\% computation cost on the attention module\ncompared with the original Transformer, while maintaining the performance.", "published": "2023-09-25 14:33:47", "link": "http://arxiv.org/abs/2309.14174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing DictaLM -- A Large Generative Language Model for Modern\n  Hebrew", "abstract": "We present DictaLM, a large-scale language model tailored for Modern Hebrew.\nBoasting 7B parameters, this model is predominantly trained on Hebrew-centric\ndata. As a commitment to promoting research and development in the Hebrew\nlanguage, we release both the foundation model and the instruct-tuned model\nunder a Creative Commons license. Concurrently, we introduce DictaLM-Rab,\nanother foundation model geared towards Rabbinic/Historical Hebrew. These\nfoundation models serve as ideal starting points for fine-tuning various\nHebrew-specific tasks, such as instruction, Q&A, sentiment analysis, and more.\nThis release represents a preliminary step, offering an initial Hebrew LLM\nmodel for the Hebrew NLP community to experiment with.", "published": "2023-09-25 22:42:09", "link": "http://arxiv.org/abs/2309.14568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot\n  Compression", "abstract": "Low Rank Decomposition of matrix - splitting a large matrix into a product of\ntwo smaller matrix offers a means for compression that reduces the parameters\nof a model without sparsification, and hence delivering more speedup on modern\nhardware. Moreover, unlike quantization, the compressed linear layers remain\nfully differentiable and all the parameters trainable, while being able to\nleverage the existing highly efficient kernels over floating point matrices. We\nstudy the potential to compress Large Language Models (LLMs) for monolingual\nCode generation via Low Rank Decomposition (LoRD) and observe that ranks for\nthe linear layers in these models can be reduced by upto 39.58% with less than\n1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to\ncompress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with\nminimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single\nA100. The compressed models speeds up inference by up to 22.35% with just a\nsingle line of change in code over huggingface's implementation with pytorch\nbackend. Low Rank Decomposition (LoRD) models remain compatible with state of\nthe art near-lossless quantization method such as SpQR, which allows leveraging\nfurther compression gains of quantization. Lastly, QLoRA over Low Rank\nDecomposition (LoRD) model further reduces memory requirements by as much as\n21.2% over vanilla QLoRA while offering similar gains from parameter efficient\nfine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new\nparadigm for LLM compression.", "published": "2023-09-25 10:35:17", "link": "http://arxiv.org/abs/2309.14021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autonomous Vehicles an overview on system, cyber security, risks,\n  issues, and a way forward", "abstract": "This chapter explores the complex realm of autonomous cars, analyzing their\nfundamental components and operational characteristics. The initial phase of\nthe discussion is elucidating the internal mechanics of these automobiles,\nencompassing the crucial involvement of sensors, artificial intelligence (AI)\nidentification systems, control mechanisms, and their integration with\ncloud-based servers within the framework of the Internet of Things (IoT). It\ndelves into practical implementations of autonomous cars, emphasizing their\nutilization in forecasting traffic patterns and transforming the dynamics of\ntransportation. The text also explores the topic of Robotic Process Automation\n(RPA), illustrating the impact of autonomous cars on different businesses\nthrough the automation of tasks. The primary focus of this investigation lies\nin the realm of cybersecurity, specifically in the context of autonomous\nvehicles. A comprehensive analysis will be conducted to explore various risk\nmanagement solutions aimed at protecting these vehicles from potential threats\nincluding ethical, environmental, legal, professional, and social dimensions,\noffering a comprehensive perspective on their societal implications. A\nstrategic plan for addressing the challenges and proposing strategies for\neffectively traversing the complex terrain of autonomous car systems,\ncybersecurity, hazards, and other concerns are some resources for acquiring an\nunderstanding of the intricate realm of autonomous cars and their ramifications\nin contemporary society, supported by a comprehensive compilation of resources\nfor additional investigation.\n  Keywords: RPA, Cyber Security, AV, Risk, Smart Cars", "published": "2023-09-25 15:19:09", "link": "http://arxiv.org/abs/2309.14213v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "Urdu Poetry Generated by Using Deep Learning Techniques", "abstract": "This study provides Urdu poetry generated using different deep-learning\ntechniques and algorithms. The data was collected through the Rekhta website,\ncontaining 1341 text files with several couplets. The data on poetry was not\nfrom any specific genre or poet. Instead, it was a collection of mixed Urdu\npoems and Ghazals. Different deep learning techniques, such as the model\napplied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU),\nhave been used. Natural Language Processing (NLP) may be used in machine\nlearning to understand, analyze, and generate a language humans may use and\nunderstand. Much work has been done on generating poetry for different\nlanguages using different techniques. The collection and use of data were also\ndifferent for different researchers. The primary purpose of this project is to\nprovide a model that generates Urdu poems by using data completely, not by\nsampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as\nin the base paper. The results have shown good accuracy in the poems generated\nby the model.", "published": "2023-09-25 15:44:24", "link": "http://arxiv.org/abs/2309.14233v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event\n  Understanding", "abstract": "Event understanding aims at understanding the content and relationship of\nevents within texts, which covers multiple complicated information extraction\ntasks: event detection, event argument extraction, and event relation\nextraction. To facilitate related research and application, we present an event\nunderstanding toolkit OmniEvent, which features three desiderata: (1)\nComprehensive. OmniEvent supports mainstream modeling paradigms of all the\nevent understanding tasks and the processing of 15 widely-used English and\nChinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous\nevaluation pitfalls reported in Peng et al. (2023), which ensures fair\ncomparisons between different models. (3) Easy-to-use. OmniEvent is designed to\nbe easily used by users with varying needs. We provide off-the-shelf models\nthat can be directly deployed as web services. The modular framework also\nenables users to easily implement and evaluate new event understanding models\nwith OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly\nreleased along with the demonstration website and video\n(https://omnievent.xlore.cn/).", "published": "2023-09-25 16:15:09", "link": "http://arxiv.org/abs/2309.14258v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via\n  Multi-Modal Causal Attention", "abstract": "Most of the existing multi-modal models, hindered by their incapacity to\nadeptly manage interleaved image-and-text inputs in multi-image, multi-round\ndialogues, face substantial constraints in resource allocation for training and\ndata accessibility, impacting their adaptability and scalability across varied\ninteraction realms. To address this, we present the DeepSpeed-VisualChat\nframework, designed to optimize Large Language Models (LLMs) by incorporating\nmulti-modal capabilities, with a focus on enhancing the proficiency of Large\nVision and Language Models in handling interleaved inputs. Our framework is\nnotable for (1) its open-source support for multi-round and multi-image\ndialogues, (2) introducing an innovative multi-modal causal attention\nmechanism, and (3) utilizing data blending techniques on existing datasets to\nassure seamless interactions in multi-round, multi-image conversations.\nCompared to existing frameworks, DeepSpeed-VisualChat shows superior\nscalability up to 70B parameter language model size, representing a significant\nadvancement in multi-modal language models and setting a solid foundation for\nfuture explorations.", "published": "2023-09-25 17:53:29", "link": "http://arxiv.org/abs/2309.14327v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Explainable and Accurate Natural Language Understanding for Voice\n  Assistants and Beyond", "abstract": "Joint intent detection and slot filling, which is also termed as joint NLU\n(Natural Language Understanding) is invaluable for smart voice assistants.\nRecent advancements in this area have been heavily focusing on improving\naccuracy using various techniques. Explainability is undoubtedly an important\naspect for deep learning-based models including joint NLU models. Without\nexplainability, their decisions are opaque to the outside world and hence, have\ntendency to lack user trust. Therefore to bridge this gap, we transform the\nfull joint NLU model to be `inherently' explainable at granular levels without\ncompromising on accuracy. Further, as we enable the full joint NLU model\nexplainable, we show that our extension can be successfully used in other\ngeneral classification tasks. We demonstrate this using sentiment analysis and\nnamed entity recognition.", "published": "2023-09-25 19:30:44", "link": "http://arxiv.org/abs/2309.14485v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When Automated Assessment Meets Automated Content Generation: Examining\n  Text Quality in the Era of GPTs", "abstract": "The use of machine learning (ML) models to assess and score textual data has\nbecome increasingly pervasive in an array of contexts including natural\nlanguage processing, information retrieval, search and recommendation, and\ncredibility assessment of online content. A significant disruption at the\nintersection of ML and text are text-generating large-language models such as\ngenerative pre-trained transformers (GPTs). We empirically assess the\ndifferences in how ML-based scoring models trained on human content assess the\nquality of content generated by humans versus GPTs. To do so, we propose an\nanalysis framework that encompasses essay scoring ML-models, human and\nML-generated essays, and a statistical model that parsimoniously considers the\nimpact of type of respondent, prompt genre, and the ML model used for\nassessment model. A rich testbed is utilized that encompasses 18,460\nhuman-generated and GPT-based essays. Results of our benchmark analysis reveal\nthat transformer pretrained language models (PLMs) more accurately score human\nessay quality as compared to CNN/RNN and feature-based ML methods.\nInterestingly, we find that the transformer PLMs tend to score GPT-generated\ntext 10-15\\% higher on average, relative to human-authored documents.\nConversely, traditional deep learning and feature-based ML models score human\ntext considerably higher. Further analysis reveals that although the\ntransformer PLMs are exclusively fine-tuned on human text, they more\nprominently attend to certain tokens appearing only in GPT-generated text,\npossibly due to familiarity/overlap in pre-training. Our framework and results\nhave implications for text classification settings where automated scoring of\ntext is likely to be disrupted by generative AI.", "published": "2023-09-25 19:32:18", "link": "http://arxiv.org/abs/2309.14488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifying token frequencies using angular Minkowski $p$-distance", "abstract": "Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by\nreplacing Euclidean distance in the definition of cosine dissimilarity with\nother Minkowski $p$-distances. Cosine dissimilarity is frequently used with\ndatasets containing token frequencies, and angular Minkowski $p$-distance may\npotentially be an even better choice for certain tasks. In a case study based\non the 20-newsgroups dataset, we evaluate clasification performance for\nclassical weighted nearest neighbours, as well as fuzzy rough nearest\nneighbours. In addition, we analyse the relationship between the hyperparameter\n$p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the\nchoice of weights and the choice of classifier. We conclude that it is possible\nto obtain substantially higher classification performance with angular\nMinkowski $p$-distance with suitable values for $p$ than with classical cosine\ndissimilarity.", "published": "2023-09-25 19:45:11", "link": "http://arxiv.org/abs/2309.14495v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChatGPT Performance on Standardized Testing Exam -- A Proposed Strategy\n  for Learners", "abstract": "This study explores the problem solving capabilities of ChatGPT and its\nprospective applications in standardized test preparation, focusing on the GRE\nquantitative exam. Prior research has shown great potential for the utilization\nof ChatGPT for academic purposes in revolutionizing the approach to studying\nacross various disciplines. We investigate how ChatGPT performs across various\nquestion types in the GRE quantitative domain, and how modifying question\nprompts impacts its accuracy. More specifically this study addressed two\nresearch questions: 1. How does ChatGPT perform in answering GRE-based\nquantitative questions across various content areas? 2. How does the accuracy\nof ChatGPT vary with modifying the question prompts? The dataset consisting of\n100 randomly selected GRE quantitative questions was collected from the ETS\nofficial guide to GRE test preparation. We used quantitative evaluation to\nanswer our first research question, and t-test to examine the statistical\nassociation between prompt modification and ChatGPT's accuracy. Results show a\nstatistical improvement in the ChatGPT's accuracy after applying instruction\npriming and contextual prompts to the original questions. ChatGPT showed 84%\naccuracy with the modified prompts compared to 69% with the original data. The\nstudy discusses the areas where ChatGPT struggled with certain questions and\nhow modifications can be helpful for preparing for standardized tests like GRE\nand provides future directions for prompt modifications.", "published": "2023-09-25 20:25:29", "link": "http://arxiv.org/abs/2309.14519v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Aligning Large Multimodal Models with Factually Augmented RLHF", "abstract": "Large Multimodal Models (LMM) are built across modalities and the\nmisalignment between two modalities can result in \"hallucination\", generating\ntextual outputs that are not grounded by the multimodal information in context.\nTo address the multimodal misalignment issue, we adapt the Reinforcement\nLearning from Human Feedback (RLHF) from the text domain to the task of\nvision-language alignment, where human annotators are asked to compare two\nresponses and pinpoint the more hallucinated one, and the vision-language model\nis trained to maximize the simulated human rewards. We propose a new alignment\nalgorithm called Factually Augmented RLHF that augments the reward model with\nadditional factual information such as image captions and ground-truth\nmulti-choice options, which alleviates the reward hacking phenomenon in RLHF\nand further improves the performance. We also enhance the GPT-4-generated\ntraining data (for vision instruction tuning) with previously available\nhuman-written image-text pairs to improve the general capabilities of our\nmodel. To evaluate the proposed approach in real-world scenarios, we develop a\nnew evaluation benchmark MMHAL-BENCH with a special focus on penalizing\nhallucinations. As the first LMM trained with RLHF, our approach achieves\nremarkable improvement on the LLaVA-Bench dataset with the 94% performance\nlevel of the text-only GPT-4 (while previous best methods can only achieve the\n87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We\nopensource our code, model, data at https://llava-rlhf.github.io.", "published": "2023-09-25 20:59:33", "link": "http://arxiv.org/abs/2309.14525v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts", "abstract": "In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.", "published": "2023-09-25 09:21:25", "link": "http://arxiv.org/abs/2309.14974v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with\n  Relation-Aware Score Calibration", "abstract": "Document-level relation extraction (DocRE) aims to extract relations of all\nentity pairs in a document. A key challenge in DocRE is the cost of annotating\nsuch data which requires intensive human effort. Thus, we investigate the case\nof DocRE in a low-resource setting, and we find that existing models trained on\nlow data overestimate the NA (\"no relation\") label, causing limited\nperformance. In this work, we approach the problem from a calibration\nperspective and propose PRiSM, which learns to adapt logits based on relation\nsemantic information. We evaluate our method on three DocRE datasets and\ndemonstrate that integrating existing models with PRiSM improves performance by\nas much as 26.38 F1 score, while the calibration error drops as much as 36\ntimes when trained with about 3% of data. The code is publicly available at\nhttps://github.com/brightjade/PRiSM.", "published": "2023-09-25 04:42:39", "link": "http://arxiv.org/abs/2309.13869v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reproducing Whisper-Style Training Using an Open-Source Toolkit and\n  Publicly Available Data", "abstract": "Pre-training speech models on large volumes of data has achieved remarkable\nsuccess. OpenAI Whisper is a multilingual multitask model trained on 680k hours\nof supervised speech data. It generalizes well to various speech recognition\nand translation benchmarks even in a zero-shot setup. However, the full\npipeline for developing such models (from data collection to training) is not\npublicly accessible, which makes it difficult for researchers to further\nimprove its performance and address training-related issues such as efficiency,\nrobustness, fairness, and bias. This work presents an Open Whisper-style Speech\nModel (OWSM), which reproduces Whisper-style training using an open-source\ntoolkit and publicly available data. OWSM even supports more translation\ndirections and can be more efficient to train. We will publicly release all\nscripts used for data preparation, training, inference, and scoring as well as\npre-trained models and training logs to promote open science.", "published": "2023-09-25 05:01:34", "link": "http://arxiv.org/abs/2309.13876v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VidChapters-7M: Video Chapters at Scale", "abstract": "Segmenting long videos into chapters enables users to quickly navigate to the\ninformation of their interest. This important topic has been understudied due\nto the lack of publicly released datasets. To address this issue, we present\nVidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters\nin total. VidChapters-7M is automatically created from videos online in a\nscalable manner by scraping user-annotated chapters and hence without any\nadditional manual annotation. We introduce the following three tasks based on\nthis data. First, the video chapter generation task consists of temporally\nsegmenting the video and generating a chapter title for each segment. To\nfurther dissect the problem, we also define two variants of this task: video\nchapter generation given ground-truth boundaries, which requires generating a\nchapter title given an annotated video segment, and video chapter grounding,\nwhich requires temporally localizing a chapter given its annotated title. We\nbenchmark both simple baselines and state-of-the-art video-language models for\nthese three tasks. We also show that pretraining on VidChapters-7M transfers\nwell to dense video captioning tasks in both zero-shot and finetuning settings,\nlargely improving the state of the art on the YouCook2 and ViTT benchmarks.\nFinally, our experiments reveal that downstream performance scales well with\nthe size of the pretraining dataset. Our dataset, code, and models are publicly\navailable at https://antoyang.github.io/vidchapters.html.", "published": "2023-09-25 08:38:11", "link": "http://arxiv.org/abs/2309.13952v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Connecting Speech Encoder and Large Language Model for ASR", "abstract": "The impressive capability and versatility of large language models (LLMs)\nhave aroused increasing attention in automatic speech recognition (ASR), with\nseveral pioneering studies attempting to build integrated ASR models by\nconnecting a speech encoder with an LLM. This paper presents a comparative\nstudy of three commonly used structures as connectors, including fully\nconnected layers, multi-head cross-attention, and Q-Former. Speech encoders\nfrom the Whisper model series as well as LLMs from the Vicuna model series with\ndifferent model sizes were studied. Experiments were performed on the commonly\nused LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with\nQ-Formers demonstrated consistent and considerable word error rate (WER)\nreductions over LLMs with other connector structures. Q-Former-based LLMs can\ngeneralise well to out-of-domain datasets, where 12% relative WER reductions\nover the Whisper baseline ASR model were achieved on the Eval2000 test set\nwithout using any in-domain training data from Switchboard. Moreover, a novel\nsegment-level Q-Former is proposed to enable LLMs to recognise speech segments\nwith a duration exceeding the limitation of the encoders, which results in 17%\nrelative WER reductions over other connector structures on 90-second-long\nspeech data.", "published": "2023-09-25 08:57:07", "link": "http://arxiv.org/abs/2309.13963v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comprehensive Overview of Named Entity Recognition: Models,\n  Domain-Specific Applications and Challenges", "abstract": "In the domain of Natural Language Processing (NLP), Named Entity Recognition\n(NER) stands out as a pivotal mechanism for extracting structured insights from\nunstructured text. This manuscript offers an exhaustive exploration into the\nevolving landscape of NER methodologies, blending foundational principles with\ncontemporary AI advancements. Beginning with the rudimentary concepts of NER,\nthe study spans a spectrum of techniques from traditional rule-based strategies\nto the contemporary marvels of transformer architectures, particularly\nhighlighting integrations such as BERT with LSTM and CNN. The narrative\naccentuates domain-specific NER models, tailored for intricate areas like\nfinance, legal, and healthcare, emphasizing their specialized adaptability.\nAdditionally, the research delves into cutting-edge paradigms including\nreinforcement learning, innovative constructs like E-NER, and the interplay of\nOptical Character Recognition (OCR) in augmenting NER capabilities. Grounding\nits insights in practical realms, the paper sheds light on the indispensable\nrole of NER in sectors like finance and biomedicine, addressing the unique\nchallenges they present. The conclusion outlines open challenges and avenues,\nmarking this work as a comprehensive guide for those delving into NER research\nand applications.", "published": "2023-09-25 12:23:37", "link": "http://arxiv.org/abs/2309.14084v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Relation between Internal Language Model and Sequence\n  Discriminative Training for Neural Transducers", "abstract": "Internal language model (ILM) subtraction has been widely applied to improve\nthe performance of the RNN-Transducer with external language model (LM) fusion\nfor speech recognition. In this work, we show that sequence discriminative\ntraining has a strong correlation with ILM subtraction from both theoretical\nand empirical points of view. Theoretically, we derive that the global optimum\nof maximum mutual information (MMI) training shares a similar formula as ILM\nsubtraction. Empirically, we show that ILM subtraction and sequence\ndiscriminative training achieve similar effects across a wide range of\nexperiments on Librispeech, including both MMI and minimum Bayes risk (MBR)\ncriteria, as well as neural transducers and LMs of both full and limited\ncontext. The benefit of ILM subtraction also becomes much smaller after\nsequence discriminative training. We also provide an in-depth study to show\nthat sequence discriminative training has a minimal effect on the commonly used\nzero-encoder ILM estimation, but a joint effect on both encoder and prediction\n+ joint network for posterior probability reshaping including both ILM and\nblank suppression.", "published": "2023-09-25 13:35:28", "link": "http://arxiv.org/abs/2309.14130v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction", "abstract": "Large language models (LLMs) can store a vast amount of world knowledge,\noften extractable via question-answering (e.g., \"What is Abraham Lincoln's\nbirthday?\"). However, do they answer such questions based on exposure to\nsimilar questions during training (i.e., cheating), or by genuinely learning to\nextract knowledge from sources like Wikipedia?\n  In this paper, we investigate this issue using a controlled biography\ndataset. We find a strong correlation between the model's ability to extract\nknowledge and various diversity measures of the training data.\n$\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be\nsufficiently augmented (e.g., through paraphrasing, sentence shuffling,\ntranslations) $\\textit{during pretraining}$. Without such augmentation,\nknowledge may be memorized but not extractable, leading to 0% accuracy,\nregardless of subsequent instruction fine-tuning.\n  To understand why this occurs, we employ (nearly) linear probing to\ndemonstrate a strong connection between the observed correlation and how the\nmodel internally encodes knowledge -- whether it is linearly encoded in the\nhidden embeddings of entity names or distributed across other token embeddings\nin the training text.\n  This paper provides $\\textbf{several key recommendations for LLM pretraining\nin the industry}$: (1) rewrite the pretraining data -- using small, auxiliary\nmodels -- to provide knowledge augmentation, and (2) incorporate more\ninstruction-finetuning data into the pretraining stage before it becomes too\nlate.", "published": "2023-09-25 17:37:20", "link": "http://arxiv.org/abs/2309.14316v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion", "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text\ninstructions such as \"articulate slowly with a deep tone\" or \"speak in a\ncheerful boyish voice\". Unlike traditional methods that rely on reference\nutterances to determine the attributes of the converted speech, our model adds\nversatility and specificity to voice conversion. The proposed VC model is a\nneural codec language model which processes a sequence of discrete codes,\nresulting in the code sequence of converted speech. It utilizes text\ninstructions as style prompts to modify the prosody and emotional information\nof the given speech. In contrast to previous approaches, which often rely on\nemploying separate encoders like prosody and content encoders to handle\ndifferent aspects of the source speech, our model handles various information\nof speech in an end-to-end manner. Experiments have demonstrated the impressive\ncapabilities of our model in comprehending instructions and delivering\nreasonable results.", "published": "2023-09-25 17:52:09", "link": "http://arxiv.org/abs/2309.14324v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of\n  Service-oriented Systems", "abstract": "Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the\nopen-world assumption in service-oriented systems. Deep RL was successfully\napplied to problems such as dynamic service composition, job scheduling, and\noffloading, as well as service adaptation. While Deep RL offers many benefits,\nunderstanding the decision-making of Deep RL is challenging because its learned\ndecision-making policy essentially appears as a black box. Yet, understanding\nthe decision-making of Deep RL is key to help service developers perform\ndebugging, support service providers to comply with relevant legal frameworks,\nand facilitate service users to build trust. We introduce Chat4XAI to\nfacilitate the understanding of the decision-making of Deep RL by providing\nnatural-language explanations. Compared with visual explanations, the reported\nbenefits of natural-language explanations include better understandability for\nnon-technical users, increased user acceptance and trust, as well as more\nefficient explanations. Chat4XAI leverages modern AI chatbot technology and\ndedicated prompt engineering. Compared to earlier work on natural-language\nexplanations using classical software-based dialogue systems, using an AI\nchatbot eliminates the need for eliciting and defining potential questions and\nanswers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API\nand evaluate the fidelity and stability of its explanations using an adaptive\nservice exemplar.", "published": "2023-09-25 09:05:36", "link": "http://arxiv.org/abs/2309.14391v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language\n  Models", "abstract": "The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{\\carb}, an end-to-end carbon footprint projection model designed for\nboth dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the\naccuracy of carbon footprint estimations for various LLMs. The source code is\nreleased at \\url{https://github.com/SotaroKaneda/MLCarbon}.", "published": "2023-09-25 14:50:04", "link": "http://arxiv.org/abs/2309.14393v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain\n  Translation", "abstract": "Domain-to-domain translation involves generating a target domain sample given\na condition in the source domain. Most existing methods focus on fixed input\nand output domains, i.e. they only work for specific configurations (i.e. for\ntwo domains, either $D_1\\rightarrow{}D_2$ or $D_2\\rightarrow{}D_1$). This paper\nproposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for\nmulti-domain translation in a semi-supervised context. Unlike previous methods,\nMDD does not require defining input and output domains, allowing translation\nbetween any partition of domains within a set (such as $(D_1,\nD_2)\\rightarrow{}D_3$, $D_2\\rightarrow{}(D_1, D_3)$, $D_3\\rightarrow{}D_1$,\netc. for 3 domains), without the need to train separate models for each domain\nconfiguration. The key idea behind MDD is to leverage the noise formulation of\ndiffusion models by incorporating one noise level per domain, which allows\nmissing domains to be modeled with noise in a natural way. This transforms the\ntraining task from a simple reconstruction task to a domain translation task,\nwhere the model relies on less noisy domains to reconstruct more noisy domains.\nWe present results on a multi-domain (with more than two domains) synthetic\nimage translation dataset with challenging semantic domain inversion.", "published": "2023-09-25 15:31:16", "link": "http://arxiv.org/abs/2309.14394v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seeing and hearing what has not been said; A multimodal client behavior\n  classifier in Motivational Interviewing with interpretable fusion", "abstract": "Motivational Interviewing (MI) is an approach to therapy that emphasizes\ncollaboration and encourages behavioral change. To evaluate the quality of an\nMI conversation, client utterances can be classified using the MISC code as\neither change talk, sustain talk, or follow/neutral talk. The proportion of\nchange talk in a MI conversation is positively correlated with therapy\noutcomes, making accurate classification of client utterances essential. In\nthis paper, we present a classifier that accurately distinguishes between the\nthree MISC classes (change talk, sustain talk, and follow/neutral talk)\nleveraging multimodal features such as text, prosody, facial expressivity, and\nbody expressivity. To train our model, we perform annotations on the publicly\navailable AnnoMI dataset to collect multimodal information, including text,\naudio, facial expressivity, and body expressivity. Furthermore, we identify the\nmost important modalities in the decision-making process, providing valuable\ninsights into the interplay of different modalities during a MI conversation.", "published": "2023-09-25 16:00:06", "link": "http://arxiv.org/abs/2309.14398v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Physics of Language Models: Part 3.2, Knowledge Manipulation", "abstract": "Language models can store vast factual knowledge, yet their ability to\nflexibly use this knowledge for downstream tasks (e.g., via instruction\nfinetuning) remains questionable. This paper investigates four fundamental\nknowledge manipulation tasks: retrieval (e.g., \"What is person A's attribute\nX?\"), classification (e.g., \"Is A's attribute X even or odd?\"), comparison\n(e.g., \"Is A greater than B in attribute X?\"), and inverse search (e.g., \"Which\nperson's attribute X equals T?\").\n  We show that language models excel in knowledge retrieval but struggle even\nin the simplest classification or comparison tasks unless Chain of Thoughts\n(CoTs) are employed during both training and inference. Moreover, their\nperformance in inverse knowledge search is virtually 0%, regardless of the\nprompts. Our primary contribution is a controlled, synthetic experiment that\nconfirms these weaknesses are inherent to language models: they cannot\nefficiently manipulate knowledge from pre-training data, even when such\nknowledge is perfectly stored in the models, despite adequate training and\nsufficient model size. Our findings also apply to modern pretrained language\nmodels such as GPT-4, thus giving rise to many Turing tests to distinguish\nHumans from contemporary AIs.", "published": "2023-09-25 17:50:41", "link": "http://arxiv.org/abs/2309.14402v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme\n  Long Sequence Transformer Models", "abstract": "Computation in a typical Transformer-based large language model (LLM) can be\ncharacterized by batch size, hidden dimension, number of layers, and sequence\nlength. Until now, system works for accelerating LLM training have focused on\nthe first three dimensions: data parallelism for batch size, tensor parallelism\nfor hidden size and pipeline parallelism for model depth or layers. These\nwidely studied forms of parallelism are not targeted or optimized for long\nsequence Transformer models. Given practical application needs for long\nsequence LLM, renewed attentions are being drawn to sequence parallelism.\nHowever, existing works in sequence parallelism are constrained by\nmemory-communication inefficiency, limiting their scalability to long sequence\nlarge models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable\nand effective methodology for enabling highly efficient and scalable LLM\ntraining with extremely long sequence length. DeepSpeed-Ulysses at its core\npartitions input data along the sequence dimension and employs an efficient\nall-to-all collective communication for attention computation. Theoretical\ncommunication analysis shows that whereas other methods incur communication\noverhead as sequence length increases, DeepSpeed-Ulysses maintains constant\ncommunication volume when sequence length and compute devices are increased\nproportionally. Furthermore, experimental evaluations show that\nDeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the\nexisting method SOTA baseline.", "published": "2023-09-25 20:15:57", "link": "http://arxiv.org/abs/2309.14509v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Art or Artifice? Large Language Models and the False Promise of\n  Creativity", "abstract": "Researchers have argued that large language models (LLMs) exhibit\nhigh-quality writing capabilities from blogs to stories. However, evaluating\nobjectively the creativity of a piece of writing is challenging. Inspired by\nthe Torrance Test of Creative Thinking (TTCT), which measures creativity as a\nprocess, we use the Consensual Assessment Technique [3] and propose the\nTorrance Test of Creative Writing (TTCW) to evaluate creativity as a product.\nTTCW consists of 14 binary tests organized into the original dimensions of\nFluency, Flexibility, Originality, and Elaboration. We recruit 10 creative\nwriters and implement a human assessment of 48 stories written either by\nprofessional authors or LLMs using TTCW. Our analysis shows that LLM-generated\nstories pass 3-10X less TTCW tests than stories written by professionals. In\naddition, we explore the use of LLMs as assessors to automate the TTCW\nevaluation, revealing that none of the LLMs positively correlate with the\nexpert assessments.", "published": "2023-09-25 22:02:46", "link": "http://arxiv.org/abs/2309.14556v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Evaluating Cognitive Maps and Planning in Large Language Models with\n  CogEval", "abstract": "Recently an influx of studies claim emergent cognitive abilities in large\nlanguage models (LLMs). Yet, most rely on anecdotes, overlook contamination of\ntraining sets, or lack systematic Evaluation involving multiple tasks, control\nconditions, multiple iterations, and statistical robustness tests. Here we make\ntwo major contributions. First, we propose CogEval, a cognitive\nscience-inspired protocol for the systematic evaluation of cognitive capacities\nin Large Language Models. The CogEval protocol can be followed for the\nevaluation of various abilities. Second, here we follow CogEval to\nsystematically evaluate cognitive maps and planning ability across eight LLMs\n(OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard,\nCohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base\nour task prompts on human experiments, which offer both established construct\nvalidity for evaluating planning, and are absent from LLM training sets. We\nfind that, while LLMs show apparent competence in a few planning tasks with\nsimpler structures, systematic evaluation reveals striking failure modes in\nplanning tasks, including hallucinations of invalid trajectories and getting\ntrapped in loops. These findings do not support the idea of emergent\nout-of-the-box planning ability in LLMs. This could be because LLMs do not\nunderstand the latent relational structures underlying planning problems, known\nas cognitive maps, and fail at unrolling goal-directed trajectories based on\nthe underlying structure. Implications for application and future directions\nare discussed.", "published": "2023-09-25 01:20:13", "link": "http://arxiv.org/abs/2309.15129v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox", "abstract": "Recent advances in Language Model (LM) agents and tool use, exemplified by\napplications like ChatGPT Plugins, enable a rich set of capabilities but also\namplify potential risks - such as leaking private data or causing financial\nlosses. Identifying these risks is labor-intensive, necessitating implementing\nthe tools, setting up the environment for each test scenario manually, and\nfinding risky cases. As tools and agents become more complex, the high cost of\ntesting these agents will make it increasingly difficult to find high-stakes,\nlong-tailed risks. To address these challenges, we introduce ToolEmu: a\nframework that uses an LM to emulate tool execution and enables the testing of\nLM agents against a diverse range of tools and scenarios, without manual\ninstantiation. Alongside the emulator, we develop an LM-based automatic safety\nevaluator that examines agent failures and quantifies associated risks. We test\nboth the tool emulator and evaluator through human evaluation and find that\n68.8% of failures identified with ToolEmu would be valid real-world agent\nfailures. Using our curated initial benchmark consisting of 36 high-stakes\ntools and 144 test cases, we provide a quantitative risk analysis of current LM\nagents and identify numerous failures with potentially severe outcomes.\nNotably, even the safest LM agent exhibits such failures 23.9% of the time\naccording to our evaluator, underscoring the need to develop safer LM agents\nfor real-world deployment.", "published": "2023-09-25 17:08:02", "link": "http://arxiv.org/abs/2309.15817v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Disinformation Detection: An Evolving Challenge in the Age of LLMs", "abstract": "The advent of generative Large Language Models (LLMs) such as ChatGPT has\ncatalyzed transformative advancements across multiple domains. However,\nalongside these advancements, they have also introduced potential threats. One\ncritical concern is the misuse of LLMs by disinformation spreaders, leveraging\nthese models to generate highly persuasive yet misleading content that\nchallenges the disinformation detection system. This work aims to address this\nissue by answering three research questions: (1) To what extent can the current\ndisinformation detection technique reliably detect LLM-generated\ndisinformation? (2) If traditional techniques prove less effective, can LLMs\nthemself be exploited to serve as a robust defense against advanced\ndisinformation? and, (3) Should both these strategies falter, what novel\napproaches can be proposed to counter this burgeoning threat effectively? A\nholistic exploration for the formation and detection of disinformation is\nconducted to foster this line of research.", "published": "2023-09-25 22:12:50", "link": "http://arxiv.org/abs/2309.15847v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Cybersecurity Crisis of Artificial Intelligence: Unrestrained\n  Adoption and Natural Language-Based Attacks", "abstract": "The widespread integration of autoregressive-large language models (AR-LLMs),\nsuch as ChatGPT, across established applications, like search engines, has\nintroduced critical vulnerabilities with uniquely scalable characteristics. In\nthis commentary, we analyse these vulnerabilities, their dependence on natural\nlanguage as a vector of attack, and their challenges to cybersecurity best\npractices. We offer recommendations designed to mitigate these challenges.", "published": "2023-09-25 10:48:46", "link": "http://arxiv.org/abs/2311.09224v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Can LLM-Generated Misinformation Be Detected?", "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.", "published": "2023-09-25 00:45:07", "link": "http://arxiv.org/abs/2309.13788v5", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech\n  Representation Learning", "abstract": "Recent years have witnessed significant advancements in self-supervised\nlearning (SSL) methods for speech-processing tasks. Various speech-based SSL\nmodels have been developed and present promising performance on a range of\ndownstream tasks including speech recognition. However, existing speech-based\nSSL models face a common dilemma in terms of computational cost, which might\nhinder their potential application and in-depth academic research. To address\nthis issue, we first analyze the computational cost of different modules during\nHuBERT pre-training and then introduce a stack of efficiency optimizations,\nwhich is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be\ntrained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without\nperformance degradation, resulting in a 5.2x speedup, compared to the original\nimplementation. Moreover, we explore two well-studied techniques in the\nFast-HuBERT and demonstrate consistent improvements as reported in previous\nwork.", "published": "2023-09-25 04:07:34", "link": "http://arxiv.org/abs/2309.13860v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TouchUp-G: Improving Feature Representation through Graph-Centric\n  Finetuning", "abstract": "How can we enhance the node features acquired from Pretrained Models (PMs) to\nbetter suit downstream graph learning tasks? Graph Neural Networks (GNNs) have\nbecome the state-of-the-art approach for many high-impact, real-world graph\napplications. For feature-rich graphs, a prevalent practice involves utilizing\na PM directly to generate features, without incorporating any domain adaptation\ntechniques. Nevertheless, this practice is suboptimal because the node features\nextracted from PM are graph-agnostic and prevent GNNs from fully utilizing the\npotential correlations between the graph structure and node features, leading\nto a decline in GNNs performance. In this work, we seek to improve the node\nfeatures obtained from a PM for downstream graph tasks and introduce TOUCHUP-G,\nwhich has several advantages. It is (a) General: applicable to any downstream\ngraph task, including link prediction which is often employed in recommender\nsystems; (b) Multi-modal: able to improve raw features of any modality (e.g.\nimages, texts, audio); (c) Principled: it is closely related to a novel metric,\nfeature homophily, which we propose to quantify the potential correlations\nbetween the graph structure and node features and we show that TOUCHUP-G can\neffectively shrink the discrepancy between the graph structure and node\nfeatures; (d) Effective: achieving state-of-the-art results on four real-world\ndatasets spanning different tasks and modalities.", "published": "2023-09-25 05:44:40", "link": "http://arxiv.org/abs/2309.13885v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Analysis and Detection of Pathological Voice using Glottal Source\n  Features", "abstract": "Automatic detection of voice pathology enables objective assessment and\nearlier intervention for the diagnosis. This study provides a systematic\nanalysis of glottal source features and investigates their effectiveness in\nvoice pathology detection. Glottal source features are extracted using glottal\nflows estimated with the quasi-closed phase (QCP) glottal inverse filtering\nmethod, using approximate glottal source signals computed with the zero\nfrequency filtering (ZFF) method, and using acoustic voice signals directly. In\naddition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from\nthe glottal source waveforms computed by QCP and ZFF to effectively capture the\nvariations in glottal source spectra of pathological voice. Experiments were\ncarried out using two databases, the Hospital Universitario Principe de\nAsturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.\nAnalysis of features revealed that the glottal source contains information that\ndiscriminates normal and pathological voice. Pathology detection experiments\nwere carried out using support vector machine (SVM). From the detection\nexperiments it was observed that the performance achieved with the studied\nglottal source features is comparable or better than that of conventional MFCCs\nand perceptual linear prediction (PLP) features. The best detection performance\nwas achieved when the glottal source features were combined with the\nconventional MFCCs and PLP features, which indicates the complementary nature\nof the features.", "published": "2023-09-25 12:14:25", "link": "http://arxiv.org/abs/2309.14080v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Wav2vec-based Detection and Severity Level Classification of Dysarthria\n  from Speech", "abstract": "Automatic detection and severity level classification of dysarthria directly\nfrom acoustic speech signals can be used as a tool in medical diagnosis. In\nthis work, the pre-trained wav2vec 2.0 model is studied as a feature extractor\nto build detection and severity level classification systems for dysarthric\nspeech. The experiments were carried out with the popularly used UA-speech\ndatabase. In the detection experiments, the results revealed that the best\nperformance was obtained using the embeddings from the first layer of the\nwav2vec model that yielded an absolute improvement of 1.23% in accuracy\ncompared to the best performing baseline feature (spectrogram). In the studied\nseverity level classification task, the results revealed that the embeddings\nfrom the final layer gave an absolute improvement of 10.62% in accuracy\ncompared to the best baseline features (mel-frequency cepstral coefficients).", "published": "2023-09-25 13:00:33", "link": "http://arxiv.org/abs/2309.14107v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Online Active Learning For Sound Event Detection", "abstract": "Data collection and annotation is a laborious, time-consuming prerequisite\nfor supervised machine learning tasks. Online Active Learning (OAL) is a\nparadigm that addresses this issue by simultaneously minimizing the amount of\nannotation required to train a classifier and adapting to changes in the data\nover the duration of the data collection process. Prior work has indicated that\nfluctuating class distributions and data drift are still common problems for\nOAL. This work presents new loss functions that address these challenges when\nOAL is applied to Sound Event Detection (SED). Experimental results from the\nSONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL\ncan reduce the time and effort required to train SED classifiers by a factor of\n5 for SONYC, and that the new methods presented here successfully resolve\nissues present in existing OAL methods.", "published": "2023-09-25 18:48:36", "link": "http://arxiv.org/abs/2309.14460v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Watch Your Language: Investigating Content Moderation with Large\n  Language Models", "abstract": "Large language models (LLMs) have exploded in popularity due to their ability\nto perform a wide array of natural language tasks. Text-based content\nmoderation is one LLM use case that has received recent enthusiasm, however,\nthere is little research investigating how LLMs perform in content moderation\nsettings. In this work, we evaluate a suite of commodity LLMs on two common\ncontent moderation tasks: rule-based community moderation and toxic content\ndetection. For rule-based community moderation, we instantiate 95 subcommunity\nspecific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We\nfind that GPT-3.5 is effective at rule-based moderation for many communities,\nachieving a median accuracy of 64% and a median precision of 83%. For toxicity\ndetection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini\nPro, LLAMA 2) and show that LLMs significantly outperform currently widespread\ntoxicity classifiers. However, recent increases in model size add only marginal\nbenefit to toxicity detection, suggesting a potential performance plateau for\nLLMs on toxicity detection tasks. We conclude by outlining avenues for future\nwork in studying LLMs and content moderation.", "published": "2023-09-25 20:23:51", "link": "http://arxiv.org/abs/2309.14517v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.SI"], "primary_category": "cs.HC"}
{"title": "Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images\n  with Improved Face-to-Speech Mapping", "abstract": "Generating speech from a face image is crucial for developing virtual humans\ncapable of interacting using their unique voices, without relying on\npre-recorded human speech. In this paper, we propose Face-StyleSpeech, a\nzero-shot Text-To-Speech (TTS) synthesis model that generates natural speech\nconditioned on a face image rather than reference speech. We hypothesize that\nlearning entire prosodic features from a face image poses a significant\nchallenge. To address this, our TTS model incorporates both face and prosody\nencoders. The prosody encoder is specifically designed to model speech style\ncharacteristics that are not fully captured by the face image, allowing the\nface encoder to focus on extracting speaker-specific features such as timbre.\nExperimental results demonstrate that Face-StyleSpeech effectively generates\nmore natural speech from a face image than baselines, even for unseen faces.\nSamples are available on our demo page.", "published": "2023-09-25 13:46:00", "link": "http://arxiv.org/abs/2311.05844v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Two-Step Approach for Narrowband Source Localization in Reverberant\n  Rooms", "abstract": "This paper presents a two-step approach for narrowband source localization\nwithin reverberant rooms. The first step involves dereverberation by modeling\nthe homogeneous component of the sound field by an equivalent decomposition of\nplanewaves using Iteratively Reweighted Least Squares (IRLS), while the second\nstep focuses on source localization by modeling the dereverberated component as\na sparse representation of point-source distribution using Orthogonal Matching\nPursuit (OMP). The proposed method enhances localization accuracy with fewer\nmeasurements, particularly in environments with strong reverberation. A\nnumerical simulation in a conference room scenario, using a uniform microphone\narray affixed to the wall, demonstrates real-world feasibility. Notably, the\nproposed method and microphone placement effectively localize sound sources\nwithin the 2D-horizontal plane without requiring prior knowledge of boundary\nconditions and room geometry, making it versatile for application in different\nroom types.", "published": "2023-09-25 02:00:40", "link": "http://arxiv.org/abs/2309.13819v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech\n  Data", "abstract": "Recently, the utilization of extensive open-sourced text data has\nsignificantly advanced the performance of text-based large language models\n(LLMs). However, the use of in-the-wild large-scale speech data in the speech\ntechnology community remains constrained. One reason for this limitation is\nthat a considerable amount of the publicly available speech data is compromised\nby background noise, speech overlapping, lack of speech segmentation\ninformation, missing speaker labels, and incomplete transcriptions, which can\nlargely hinder their usefulness. On the other hand, human annotation of speech\ndata is both time-consuming and costly. To address this issue, we introduce an\nautomatic in-the-wild speech data preprocessing framework (AutoPrep) in this\npaper, which is designed to enhance speech quality, generate speaker labels,\nand produce transcriptions automatically. The proposed AutoPrep framework\ncomprises six components: speech enhancement, speech segmentation, speaker\nclustering, target speech extraction, quality filtering and automatic speech\nrecognition. Experiments conducted on the open-sourced WenetSpeech and our\nself-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep\nframework can generate preprocessed data with similar DNSMOS and PDNSMOS scores\ncompared to several open-sourced TTS datasets. The corresponding TTS system can\nachieve up to 0.68 in-domain speaker similarity.", "published": "2023-09-25 07:01:10", "link": "http://arxiv.org/abs/2309.13905v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for\n  Expressive Long-form TTS", "abstract": "Recent advances in text-to-speech, particularly those based on Graph Neural\nNetworks (GNNs), have significantly improved the expressiveness of short-form\nsynthetic speech. However, generating human-parity long-form speech with high\ndynamic prosodic variations is still challenging. To address this problem, we\nexpand the capabilities of GNNs with a hierarchical prosody modeling approach,\nnamed HiGNN-TTS. Specifically, we add a virtual global node in the graph to\nstrengthen the interconnection of word nodes and introduce a contextual\nattention mechanism to broaden the prosody modeling scope of GNNs from\nintra-sentence to inter-sentence. Additionally, we perform hierarchical\nsupervision from acoustic prosody on each node of the graph to capture the\nprosodic variations with a high dynamic range. Ablation studies show the\neffectiveness of HiGNN-TTS in learning hierarchical prosody. Both objective and\nsubjective evaluations demonstrate that HiGNN-TTS significantly improves the\nnaturalness and expressiveness of long-form synthetic speech.", "published": "2023-09-25 07:07:02", "link": "http://arxiv.org/abs/2309.13907v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frame-wise streaming end-to-end speaker diarization with\n  non-autoregressive self-attention-based attractors", "abstract": "This work proposes a frame-wise online/streaming end-to-end neural\ndiarization (FS-EEND) method in a frame-in-frame-out fashion. To frame-wisely\ndetect a flexible number of speakers and extract/update their corresponding\nattractors, we propose to leverage a causal speaker embedding encoder and an\nonline non-autoregressive self-attention-based attractor decoder. A look-ahead\nmechanism is adopted to allow leveraging some future frames for effectively\ndetecting new speakers in real time and adaptively updating speaker attractors.\nThe proposed method processes the audio stream frame by frame, and has a low\ninference latency caused by the look-ahead frames. Experiments show that,\ncompared with the recently proposed block-wise online methods, our method\nFS-EEND achieves state-of-the-art diarization results, with a low inference\nlatency and computational cost.", "published": "2023-09-25 07:33:54", "link": "http://arxiv.org/abs/2309.13916v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Accent Adaptation Through Masked Language Model Correction\n  Of Discrete Self-Supervised Speech Units", "abstract": "Self-supervised pre-trained speech models have strongly improved speech\nrecognition, yet they are still sensitive to domain shifts and accented or\natypical speech. Many of these models rely on quantisation or clustering to\nlearn discrete acoustic units. We propose to correct the discovered discrete\nunits for accented speech back to a standard pronunciation in an unsupervised\nmanner. A masked language model is trained on discrete units from a standard\naccent and iteratively corrects an accented token sequence by masking\nunexpected cluster sequences and predicting their common variant. Small accent\nadapter blocks are inserted in the pre-trained model and fine-tuned by\npredicting the corrected clusters, which leads to an increased robustness of\nthe pre-trained model towards a target accent, and this without supervision. We\nare able to improve a state-of-the-art HuBERT Large model on a downstream\naccented speech recognition task by altering the training regime with the\nproposed method.", "published": "2023-09-25 09:51:59", "link": "http://arxiv.org/abs/2309.13994v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceLens: Controllable Speaker Generation and Editing with Flow", "abstract": "Currently, many multi-speaker speech synthesis and voice conversion systems\naddress speaker variations with an embedding vector. Modeling it directly\nallows new voices outside of training data to be synthesized. GMM based\napproaches such as Tacospawn are favored in literature for this generation\ntask, but there are still some limitations when difficult conditionings are\ninvolved. In this paper, we propose VoiceLens, a semi-supervised flow-based\napproach, to model speaker embedding distributions for multi-conditional\nspeaker generation. VoiceLens maps speaker embeddings into a combination of\nindependent attributes and residual information. It allows new voices\nassociated with certain attributes to be \\textit{generated} for existing TTS\nmodels, and attributes of known voices to be meaningfully \\textit{edited}. We\nshow in this paper, VoiceLens displays an unconditional generation capacity\nthat is similar to Tacospawn while obtaining higher controllability and\nflexibility when used in a conditional manner. In addition, we show\nsynthesizing less noisy speech from known noisy speakers without re-training\nthe TTS model is possible via solely editing their embeddings with a SNR\nconditioned VoiceLens model. Demos are available at\nsos1sos2sixteen.github.io/voicelens.", "published": "2023-09-25 12:37:03", "link": "http://arxiv.org/abs/2309.14094v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Haha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification", "abstract": "It is widely acknowledged that discriminative representation for speaker\nverification can be extracted from verbal speech. However, how much speaker\ninformation that non-verbal vocalization carries is still a puzzle. This paper\nexplores speaker verification based on the most ubiquitous form of non-verbal\nvoice, laughter. First, we use a semi-automatic pipeline to collect a new\nHaha-Pod dataset from open-source podcast media. The dataset contains over 240\nspeakers' laughter clips with corresponding high-quality verbal speech. Second,\nwe propose a Two-Stage Teacher-Student (2S-TS) framework to minimize the\nwithin-speaker embedding distance between verbal and non-verbal (laughter)\nsignals. Considering Haha-Pod as a test set, two trials (S2L-Eval) are designed\nto verify the speaker's identity through laugh sounds. Experimental results\ndemonstrate that our method can significantly improve the performance of the\nS2L-Eval test set with only a minor degradation on the VoxCeleb1 test set. The\nresources for the Haha-Pod dataset can be found at\nhttps://github.com/nevermoreLin/HahaPod.", "published": "2023-09-25 13:04:39", "link": "http://arxiv.org/abs/2309.14109v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker anonymization using neural audio codec language models", "abstract": "The vast majority of approaches to speaker anonymization involve the\nextraction of fundamental frequency estimates, linguistic features and a\nspeaker embedding which is perturbed to obfuscate the speaker identity before\nan anonymized speech waveform is resynthesized using a vocoder. Recent work has\nshown that x-vector transformations are difficult to control consistently:\nother sources of speaker information contained within fundamental frequency and\nlinguistic features are re-entangled upon vocoding, meaning that anonymized\nspeech signals still contain speaker information. We propose an approach based\nupon neural audio codecs (NACs), which are known to generate high-quality\nsynthetic speech when combined with language models. NACs use quantized codes,\nwhich are known to effectively bottleneck speaker-related information: we\ndemonstrate the potential of speaker anonymization systems based on NAC\nlanguage modeling by applying the evaluation framework of the Voice Privacy\nChallenge 2022.", "published": "2023-09-25 13:32:09", "link": "http://arxiv.org/abs/2309.14129v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Domain Adaptation by Self-Supervised Learning for Speaker\n  Verification", "abstract": "In real-world applications, speaker recognition models often face various\ndomain-mismatch challenges, leading to a significant drop in performance.\nAlthough numerous domain adaptation techniques have been developed to address\nthis issue, almost all present methods focus on a simple configuration where\nthe model is trained in one domain and deployed in another. However, real-world\nenvironments are often complex and may contain multiple domains, making the\nmethods designed for one-to-one adaptation suboptimal. In our paper, we propose\na self-supervised learning method to tackle this multi-domain adaptation\nproblem. Building upon the basic self-supervised adaptation algorithm, we\ndesigned three strategies to make it suitable for multi-domain adaptation: an\nin-domain negative sampling strategy, a MoCo-like memory bank scheme, and a\nCORAL-like distribution alignment. We conducted experiments using VoxCeleb2 as\nthe source domain dataset and CN-Celeb1 as the target multi-domain dataset. Our\nresults demonstrate that our method clearly outperforms the basic\nself-supervised adaptation method, which simply treats the data of CN-Celeb1 as\na single domain. Importantly, the improvement is consistent in nearly all\nin-domain tests and cross-domain tests, demonstrating the effectiveness of our\nproposed method.", "published": "2023-09-25 14:02:16", "link": "http://arxiv.org/abs/2309.14149v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of Distribution Alignment in Multi-Genre Speaker\n  Recognition", "abstract": "Multi-genre speaker recognition is becoming increasingly popular due to its\nability to better represent the complexities of real-world applications.\nHowever, a major challenge is the significant shift in the distribution of\nspeaker vectors across different genres. While distribution alignment is a\ncommon approach to address this challenge, previous studies have mainly focused\non aligning a source domain with a target domain, and the performance of\nmulti-genre data is unknown.\n  This paper presents a comprehensive study of mainstream distribution\nalignment methods on multi-genre data, where multiple distributions need to be\naligned. We analyze various methods both qualitatively and quantitatively. Our\nexperiments on the CN-Celeb dataset show that within-between distribution\nalignment (WBDA) performs relatively better. However, we also found that none\nof the investigated methods consistently improved performance in all test\ncases. This suggests that solely aligning the distributions of speaker vectors\nmay not fully address the challenges posed by multi-genre speaker recognition.\nFurther investigation is necessary to develop a more comprehensive solution.", "published": "2023-09-25 14:08:48", "link": "http://arxiv.org/abs/2309.14158v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Impact of Quantization and Pruning of Self-Supervised Speech\n  Models for Downstream Speech Recognition Tasks \"In-the-Wild''", "abstract": "Recent advances with self-supervised learning have allowed speech recognition\nsystems to achieve state-of-the-art (SOTA) word error rates (WER) while\nrequiring only a fraction of the labeled training data needed by its\npredecessors. Notwithstanding, while such models achieve SOTA performance in\nmatched train/test conditions, their performance degrades substantially when\ntested in unseen conditions. To overcome this problem, strategies such as data\naugmentation and/or domain shift training have been explored. Available models,\nhowever, are still too large to be considered for edge speech applications on\nresource-constrained devices, thus model compression tools are needed. In this\npaper, we explore the effects that train/test mismatch conditions have on\nspeech recognition accuracy based on compressed self-supervised speech models.\nIn particular, we report on the effects that parameter quantization and model\npruning have on speech recognition accuracy based on the so-called robust\nwav2vec 2.0 model under noisy, reverberant, and noise-plus-reverberation\nconditions.", "published": "2023-09-25 18:54:16", "link": "http://arxiv.org/abs/2309.14462v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise-Robust DSP-Assisted Neural Pitch Estimation with Very Low\n  Complexity", "abstract": "Pitch estimation is an essential step of many speech processing algorithms,\nincluding speech coding, synthesis, and enhancement. Recently, pitch estimators\nbased on deep neural networks (DNNs) have have been outperforming\nwell-established DSP-based techniques. Unfortunately, these new estimators can\nbe impractical to deploy in real-time systems, both because of their relatively\nhigh complexity, and the fact that some require significant lookahead. We show\nthat a hybrid estimator using a small deep neural network (DNN) with\ntraditional DSP-based features can match or exceed the performance of pure\nDNN-based models, with a complexity and algorithmic delay comparable to\ntraditional DSP-based algorithms. We further demonstrate that this hybrid\napproach can provide benefits for a neural vocoding task.", "published": "2023-09-25 20:14:31", "link": "http://arxiv.org/abs/2309.14507v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NoLACE: Improving Low-Complexity Speech Codec Enhancement Through\n  Adaptive Temporal Shaping", "abstract": "Speech codec enhancement methods are designed to remove distortions added by\nspeech codecs. While classical methods are very low in complexity and add zero\ndelay, their effectiveness is rather limited. Compared to that, DNN-based\nmethods deliver higher quality but they are typically high in complexity and/or\nrequire delay. The recently proposed Linear Adaptive Coding Enhancer (LACE)\naddresses this problem by combining DNNs with classical long-term/short-term\npostfiltering resulting in a causal low-complexity model. A short-coming of the\nLACE model is, however, that quality quickly saturates when the model size is\nscaled up. To mitigate this problem, we propose a novel adatpive temporal\nshaping module that adds high temporal resolution to the LACE model resulting\nin the Non-Linear Adaptive Coding Enhancer (NoLACE). We adapt NoLACE to enhance\nthe Opus codec and show that NoLACE significantly outperforms both the Opus\nbaseline and an enlarged LACE model at 6, 9 and 12 kb/s. We also show that LACE\nand NoLACE are well-behaved when used with an ASR system.", "published": "2023-09-25 20:44:00", "link": "http://arxiv.org/abs/2309.14521v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDTSE: Discriminative Diffusion Model for Target Speech Extraction", "abstract": "Diffusion models have gained attention in speech enhancement tasks, providing\nan alternative to conventional discriminative methods. However, research on\ntarget speech extraction under multi-speaker noisy conditions remains\nrelatively unexplored. Moreover, the superior quality of diffusion methods\ntypically comes at the cost of slower inference speed. In this paper, we\nintroduce the Discriminative Diffusion model for Target Speech Extraction\n(DDTSE). We apply the same forward process as diffusion models and utilize the\nreconstruction loss similar to discriminative methods. Furthermore, we devise a\ntwo-stage training strategy to emulate the inference process during model\ntraining. DDTSE not only works as a standalone system, but also can further\nimprove the performance of discriminative models without additional retraining.\nExperimental results demonstrate that DDTSE not only achieves higher perceptual\nquality but also accelerates the inference process by 3 times compared to the\nconventional diffusion model.", "published": "2023-09-25 04:58:38", "link": "http://arxiv.org/abs/2309.13874v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time Emergency Vehicle Detection using Mel Spectrograms and Regular\n  Expressions", "abstract": "In emergency situations, the high-speed movement of an ambulance through the\ncity streets can be hindered by vehicular traffic. This work presents a method\nfor detecting emergency vehicle sirens in real time. To obtain the audio\nfingerprint of a Hi-Lo siren, DSP and signal symbolization techniques were\napplied, which were contrasted against an audio classifier based on a deep\nneural network, using the same 280 audios of ambient sounds and 52 Hi-Lo siren\naudios dataset. In both methods, some classification accuracy metrics were\nevaluated based on its confusion matrix, resulting in the DSP algorithm having\na slightly lower accuracy than the DNN model, however, it offers a\nself-explanatory, adjustable, portable, high performance and lower energy and\nconsumption that makes it a more viable lower cost ADAS implementation to\nidentify Hi-Lo sirens in real time.", "published": "2023-09-25 07:40:19", "link": "http://arxiv.org/abs/2309.13920v3", "categories": ["cs.SD", "cs.FL", "cs.SC", "eess.AS", "I.5.5"], "primary_category": "cs.SD"}
{"title": "Evaluating Classification Systems Against Soft Labels with Fuzzy\n  Precision and Recall", "abstract": "Classification systems are normally trained by minimizing the cross-entropy\nbetween system outputs and reference labels, which makes the Kullback-Leibler\ndivergence a natural choice for measuring how closely the system can follow the\ndata. Precision and recall provide another perspective for measuring the\nperformance of a classification system. Non-binary references can arise from\nvarious sources, and it is often beneficial to use the soft labels for training\ninstead of the binarized data. However, the existing definitions for precision\nand recall require binary reference labels, and binarizing the data can cause\nerroneous interpretations. We present a novel method to calculate precision,\nrecall and F-score without quantizing the data. The proposed metrics extend the\nwell established metrics as the definitions coincide when used with binary\nlabels. To understand the behavior of the metrics we show simple example cases\nand an evaluation of different sound event detection models trained on real\ndata with soft labels.", "published": "2023-09-25 08:16:01", "link": "http://arxiv.org/abs/2309.13938v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speed Co-Augmentation for Unsupervised Audio-Visual Pre-training", "abstract": "This work aims to improve unsupervised audio-visual pre-training. Inspired by\nthe efficacy of data augmentation in visual contrastive learning, we propose a\nnovel speed co-augmentation method that randomly changes the playback speeds of\nboth audio and video data. Despite its simplicity, the speed co-augmentation\nmethod possesses two compelling attributes: (1) it increases the diversity of\naudio-visual pairs and doubles the size of negative pairs, resulting in a\nsignificant enhancement in the learned representations, and (2) it changes the\nstrict correlation between audio-visual pairs but introduces a partial\nrelationship between the augmented pairs, which is modeled by our proposed\nSoftInfoNCE loss to further boost the performance. Experimental results show\nthat the proposed method significantly improves the learned representations\nwhen compared to vanilla audio-visual contrastive learning.", "published": "2023-09-25 08:22:30", "link": "http://arxiv.org/abs/2309.13942v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Audio classification with Dilated Convolution with Learnable Spacings", "abstract": "Dilated convolution with learnable spacings (DCLS) is a recent convolution\nmethod in which the positions of the kernel elements are learned throughout\ntraining by backpropagation. Its interest has recently been demonstrated in\ncomputer vision (ImageNet classification and downstream tasks). Here we show\nthat DCLS is also useful for audio tagging using the AudioSet classification\nbenchmark. We took two state-of-the-art convolutional architectures using\ndepthwise separable convolutions (DSC), ConvNeXt and ConvFormer, and a hybrid\none using attention in addition, FastViT, and drop-in replaced all the DSC\nlayers by DCLS ones. This significantly improved the mean average precision\n(mAP) with the three architectures without increasing the number of parameters\nand with only a low cost on the throughput. The method code is based on PyTorch\nand is available at https://github.com/K-H-Ismail/DCLS-Audio", "published": "2023-09-25 09:09:54", "link": "http://arxiv.org/abs/2309.13972v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BiSinger: Bilingual Singing Voice Synthesis", "abstract": "Although Singing Voice Synthesis (SVS) has made great strides with\nText-to-Speech (TTS) techniques, multilingual singing voice modeling remains\nrelatively unexplored. This paper presents BiSinger, a bilingual pop SVS system\nfor English and Chinese Mandarin. Current systems require separate models per\nlanguage and cannot accurately represent both Chinese and English, hindering\ncode-switch SVS. To address this gap, we design a shared representation between\nChinese and English singing voices, achieved by using the CMU dictionary with\nmapping rules. We fuse monolingual singing datasets with open-source singing\nvoice conversion techniques to generate bilingual singing voices while also\nexploring the potential use of bilingual speech data. Experiments affirm that\nour language-independent representation and incorporation of related datasets\nenable a single model with enhanced performance in English and code-switch SVS\nwhile maintaining Chinese song performance. Audio samples are available at\nhttps://bisinger-svs.github.io.", "published": "2023-09-25 12:31:05", "link": "http://arxiv.org/abs/2309.14089v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Audio and Speech Understanding", "abstract": "Humans are surrounded by audio signals that include both speech and\nnon-speech sounds. The recognition and understanding of speech and non-speech\naudio events, along with a profound comprehension of the relationship between\nthem, constitute fundamental cognitive capabilities. For the first time, we\nbuild a machine learning model, called LTU-AS, that has a conceptually similar\nuniversal audio perception and advanced reasoning ability. Specifically, by\nintegrating Whisper as a perception module and LLaMA as a reasoning module,\nLTU-AS can simultaneously recognize and jointly understand spoken text, speech\nparalinguistics, and non-speech audio events - almost everything perceivable\nfrom audio signals.", "published": "2023-09-25 17:59:05", "link": "http://arxiv.org/abs/2309.14405v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
