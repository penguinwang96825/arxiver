{"title": "Neural Headline Generation with Sentence-wise Optimization", "abstract": "Recently, neural models have been proposed for headline generation by\nlearning to map documents to headlines with recurrent neural networks.\nNevertheless, as traditional neural network utilizes maximum likelihood\nestimation for parameter optimization, it essentially constrains the expected\ntraining objective within word level rather than sentence level. Moreover, the\nperformance of model prediction significantly relies on training data\ndistribution. To overcome these drawbacks, we employ minimum risk training\nstrategy in this paper, which directly optimizes model parameters in sentence\nlevel with respect to evaluation metrics and leads to significant improvements\nfor headline generation. Experiment results show that our models outperforms\nstate-of-the-art systems on both English and Chinese headline generation tasks.", "published": "2016-04-07 07:47:11", "link": "http://arxiv.org/abs/1604.01904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combinatorial Topic Models using Small-Variance Asymptotics", "abstract": "Topic models have emerged as fundamental tools in unsupervised machine\nlearning. Most modern topic modeling algorithms take a probabilistic view and\nderive inference algorithms based on Latent Dirichlet Allocation (LDA) or its\nvariants. In contrast, we study topic modeling as a combinatorial optimization\nproblem, and propose a new objective function derived from LDA by passing to\nthe small-variance limit. We minimize the derived objective by using ideas from\ncombinatorial optimization, which results in a new, fast, and high-quality\ntopic modeling algorithm. In particular, we show that our results are\ncompetitive with popular LDA-based topic modeling approaches, and also discuss\nthe (dis)similarities between our approach and its probabilistic counterparts.", "published": "2016-04-07 15:04:16", "link": "http://arxiv.org/abs/1604.02027v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for\n  Themselves", "abstract": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model\nthat assumes the generation of each word within a sentence to depend on both\nthe topic of the sentence and the whole history of its preceding words in the\nsentence. Different from conventional topic models that largely ignore the\nsequential order of words or their topic coherence, SLRTM gives full\ncharacterization to them by using a Recurrent Neural Networks (RNN) based\nframework. Experimental results have shown that SLRTM outperforms several\nstrong baselines on various tasks. Furthermore, SLRTM can automatically\ngenerate sentences given a topic (i.e., topics to sentences), which is a key\ntechnology for real world applications such as personalized short text\nconversation.", "published": "2016-04-07 15:29:45", "link": "http://arxiv.org/abs/1604.02038v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation &\n  Prepositional Attachment Resolution in Captioned Scenes", "abstract": "We present an approach to simultaneously perform semantic segmentation and\nprepositional phrase attachment resolution for captioned images. Some\nambiguities in language cannot be resolved without simultaneously reasoning\nabout an associated image. If we consider the sentence \"I shot an elephant in\nmy pajamas\", looking at language alone (and not using common sense), it is\nunclear if it is the person or the elephant wearing the pajamas or both. Our\napproach produces a diverse set of plausible hypotheses for both semantic\nsegmentation and prepositional phrase attachment resolution that are then\njointly reranked to select the most consistent pair. We show that our semantic\nsegmentation and prepositional phrase attachment resolution modules have\ncomplementary strengths, and that joint reasoning produces more accurate\nresults than any module operating in isolation. Multiple hypotheses are also\nshown to be crucial to improved multiple-module reasoning. Our vision and\nlanguage approach significantly outperforms the Stanford Parser (De Marneffe et\nal., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two\ndifferent experiments. We also make small improvements over DeepLab-CRF (Chen\net al., 2015).", "published": "2016-04-07 19:26:56", "link": "http://arxiv.org/abs/1604.02125v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
