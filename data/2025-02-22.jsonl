{"title": "Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning", "abstract": "We study monotonicity testing of high-dimensional distributions on\n$\\{-1,1\\}^n$ in the model of subcube conditioning, suggested and studied by\nCanonne, Ron, and Servedio~\\cite{CRS15} and Bhattacharyya and\nChakraborty~\\cite{BC18}. Previous work shows that the \\emph{sample complexity}\nof monotonicity testing must be exponential in $n$ (Rubinfeld,\nVasilian~\\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld,\nYodpinyanee~\\cite{AGPRY19}). We show that the subcube \\emph{query complexity}\nis $\\tilde{\\Theta}(n/\\varepsilon^2)$, by proving nearly matching upper and\nlower bounds. Our work is the first to use directed isoperimetric inequalities\n(developed for function monotonicity testing) for analyzing a distribution\ntesting algorithm. Along the way, we generalize an inequality of Khot, Minzer,\nand Safra~\\cite{KMS18} to real-valued functions on $\\{-1,1\\}^n$.\n  We also study uniformity testing of distributions that are promised to be\nmonotone, a problem introduced by Rubinfeld, Servedio~\\cite{RS09} , using\nsubcube conditioning. We show that the query complexity is\n$\\tilde{\\Theta}(\\sqrt{n}/\\varepsilon^2)$. Our work proves the lower bound,\nwhich matches (up to poly-logarithmic factors) the uniformity testing upper\nbound for general distributions (Canonne, Chen, Kamath, Levi,\nWaingarten~\\cite{CCKLW21}). Hence, we show that monotonicity does not help,\nbeyond logarithmic factors, in testing uniformity of distributions with subcube\nconditional queries.", "published": "2025-02-22 21:04:22", "link": "http://arxiv.org/abs/2502.16355v1", "categories": ["math.ST", "cs.CC", "cs.DM", "cs.DS", "cs.LG", "stat.TH"], "primary_category": "math.ST"}
{"title": "Optimality and Renormalization imply Statistical Laws", "abstract": "Benford's Law is an important instance of experimental mathematics that\nappears to constrain the information-theoretic behavior of numbers. Elias'\nencoding for integers is a remarkable approach to universality and optimality\nof codes. In the present analysis we seek to deduce a general law and its\nparticular implications for these two cases from optimality and renormalization\nas applied to information-theoretical functionals. Both theoretical and\nexperimental results corroborate our conclusions.", "published": "2025-02-22 18:04:28", "link": "http://arxiv.org/abs/2502.16314v1", "categories": ["cs.IT", "cs.CC", "cs.DM", "math-ph", "math.IT", "math.MP", "H.1.1"], "primary_category": "cs.IT"}
{"title": "Risk Measures for DC Pension Plan Decumulation", "abstract": "As the developed world replaces Defined Benefit (DB) pension plans with\nDefined Contribution (DC) plans, there is a need to develop decumulation\nstrategies for DC plan holders. Optimal decumulation can be viewed as a problem\nin optimal stochastic control. Formulation as a control problem requires\nspecification of an objective function, which in turn requires a definition of\nreward and risk. An intuitive specification of reward is the total withdrawals\nover the retirement period. Most retirees view risk as the possibility of\nrunning out of savings. This paper investigates several possible left tail risk\nmeasures, in conjunction with DC plan decumulation. The risk measures studied\ninclude (i) expected shortfall (ii) linear shortfall and (iii) probability of\nshortfall. We establish that, under certain assumptions, the set of optimal\ncontrols associated with all expected reward and expected shortfall Pareto\nefficient frontier curves is identical to the set of optimal controls for all\nexpected reward and linear shortfall Pareto efficient frontier curves. Optimal\nefficient frontiers are determined computationally for each risk measure, based\non a parametric market model. Robustness of these strategies is determined by\ntesting the strategies out-of-sample using block bootstrapping of historical\ndata.", "published": "2025-02-22 21:40:20", "link": "http://arxiv.org/abs/2502.16364v1", "categories": ["math.OC", "q-fin.CP", "q-fin.MF", "q-fin.RM"], "primary_category": "math.OC"}
{"title": "Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework", "abstract": "We introduce the Contrastive Similarity Space Embedding Algorithm\n(ContraSim), a novel framework for uncovering the global semantic relationships\nbetween daily financial headlines and market movements. ContraSim operates in\ntwo key stages: (I) Weighted Headline Augmentation, which generates augmented\nfinancial headlines along with a semantic fine-grained similarity score, and\n(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version\nof classical self-supervised contrastive learning that uses the similarity\nmetric to create a refined weighted embedding space. This embedding space\nclusters semantically similar headlines together, facilitating deeper market\ninsights. Empirical results demonstrate that integrating ContraSim features\ninto financial forecasting tasks improves classification accuracy from WSJ\nheadlines by 7%. Moreover, leveraging an information density analysis, we find\nthat the similarity spaces constructed by ContraSim intrinsically cluster days\nwith homogeneous market movement directions, indicating that ContraSim captures\nmarket dynamics independent of ground truth labels. Additionally, ContraSim\nenables the identification of historical news days that closely resemble the\nheadlines of the current day, providing analysts with actionable insights to\npredict market trends by referencing analogous past events.", "published": "2025-02-22 00:53:22", "link": "http://arxiv.org/abs/2502.16023v1", "categories": ["q-fin.ST", "cs.LG", "I.2.4; I.2.6; I.5.1; J.1"], "primary_category": "q-fin.ST"}
{"title": "The \"double\" square-root law: Evidence for the mechanical origin of market impact using Tokyo Stock Exchange data", "abstract": "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.", "published": "2025-02-22 14:48:06", "link": "http://arxiv.org/abs/2502.16246v1", "categories": ["q-fin.TR", "cond-mat.stat-mech"], "primary_category": "q-fin.TR"}
