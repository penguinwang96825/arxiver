{"title": "Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning", "abstract": "We study monotonicity testing of high-dimensional distributions on\n$\\{-1,1\\}^n$ in the model of subcube conditioning, suggested and studied by\nCanonne, Ron, and Servedio~\\cite{CRS15} and Bhattacharyya and\nChakraborty~\\cite{BC18}. Previous work shows that the \\emph{sample complexity}\nof monotonicity testing must be exponential in $n$ (Rubinfeld,\nVasilian~\\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld,\nYodpinyanee~\\cite{AGPRY19}). We show that the subcube \\emph{query complexity}\nis $\\tilde{\\Theta}(n/\\varepsilon^2)$, by proving nearly matching upper and\nlower bounds. Our work is the first to use directed isoperimetric inequalities\n(developed for function monotonicity testing) for analyzing a distribution\ntesting algorithm. Along the way, we generalize an inequality of Khot, Minzer,\nand Safra~\\cite{KMS18} to real-valued functions on $\\{-1,1\\}^n$.\n  We also study uniformity testing of distributions that are promised to be\nmonotone, a problem introduced by Rubinfeld, Servedio~\\cite{RS09} , using\nsubcube conditioning. We show that the query complexity is\n$\\tilde{\\Theta}(\\sqrt{n}/\\varepsilon^2)$. Our work proves the lower bound,\nwhich matches (up to poly-logarithmic factors) the uniformity testing upper\nbound for general distributions (Canonne, Chen, Kamath, Levi,\nWaingarten~\\cite{CCKLW21}). Hence, we show that monotonicity does not help,\nbeyond logarithmic factors, in testing uniformity of distributions with subcube\nconditional queries.", "published": "2025-02-22 21:04:22", "link": "http://arxiv.org/abs/2502.16355v1", "categories": ["math.ST", "cs.CC", "cs.DM", "cs.DS", "cs.LG", "stat.TH"], "primary_category": "math.ST"}
{"title": "Optimality and Renormalization imply Statistical Laws", "abstract": "Benford's Law is an important instance of experimental mathematics that\nappears to constrain the information-theoretic behavior of numbers. Elias'\nencoding for integers is a remarkable approach to universality and optimality\nof codes. In the present analysis we seek to deduce a general law and its\nparticular implications for these two cases from optimality and renormalization\nas applied to information-theoretical functionals. Both theoretical and\nexperimental results corroborate our conclusions.", "published": "2025-02-22 18:04:28", "link": "http://arxiv.org/abs/2502.16314v1", "categories": ["cs.IT", "cs.CC", "cs.DM", "math-ph", "math.IT", "math.MP", "H.1.1"], "primary_category": "cs.IT"}
{"title": "Risk Measures for DC Pension Plan Decumulation", "abstract": "As the developed world replaces Defined Benefit (DB) pension plans with\nDefined Contribution (DC) plans, there is a need to develop decumulation\nstrategies for DC plan holders. Optimal decumulation can be viewed as a problem\nin optimal stochastic control. Formulation as a control problem requires\nspecification of an objective function, which in turn requires a definition of\nreward and risk. An intuitive specification of reward is the total withdrawals\nover the retirement period. Most retirees view risk as the possibility of\nrunning out of savings. This paper investigates several possible left tail risk\nmeasures, in conjunction with DC plan decumulation. The risk measures studied\ninclude (i) expected shortfall (ii) linear shortfall and (iii) probability of\nshortfall. We establish that, under certain assumptions, the set of optimal\ncontrols associated with all expected reward and expected shortfall Pareto\nefficient frontier curves is identical to the set of optimal controls for all\nexpected reward and linear shortfall Pareto efficient frontier curves. Optimal\nefficient frontiers are determined computationally for each risk measure, based\non a parametric market model. Robustness of these strategies is determined by\ntesting the strategies out-of-sample using block bootstrapping of historical\ndata.", "published": "2025-02-22 21:40:20", "link": "http://arxiv.org/abs/2502.16364v1", "categories": ["math.OC", "q-fin.CP", "q-fin.MF", "q-fin.RM"], "primary_category": "math.OC"}
{"title": "Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework", "abstract": "We introduce the Contrastive Similarity Space Embedding Algorithm\n(ContraSim), a novel framework for uncovering the global semantic relationships\nbetween daily financial headlines and market movements. ContraSim operates in\ntwo key stages: (I) Weighted Headline Augmentation, which generates augmented\nfinancial headlines along with a semantic fine-grained similarity score, and\n(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version\nof classical self-supervised contrastive learning that uses the similarity\nmetric to create a refined weighted embedding space. This embedding space\nclusters semantically similar headlines together, facilitating deeper market\ninsights. Empirical results demonstrate that integrating ContraSim features\ninto financial forecasting tasks improves classification accuracy from WSJ\nheadlines by 7%. Moreover, leveraging an information density analysis, we find\nthat the similarity spaces constructed by ContraSim intrinsically cluster days\nwith homogeneous market movement directions, indicating that ContraSim captures\nmarket dynamics independent of ground truth labels. Additionally, ContraSim\nenables the identification of historical news days that closely resemble the\nheadlines of the current day, providing analysts with actionable insights to\npredict market trends by referencing analogous past events.", "published": "2025-02-22 00:53:22", "link": "http://arxiv.org/abs/2502.16023v1", "categories": ["q-fin.ST", "cs.LG", "I.2.4; I.2.6; I.5.1; J.1"], "primary_category": "q-fin.ST"}
{"title": "The \"double\" square-root law: Evidence for the mechanical origin of market impact using Tokyo Stock Exchange data", "abstract": "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.", "published": "2025-02-22 14:48:06", "link": "http://arxiv.org/abs/2502.16246v1", "categories": ["q-fin.TR", "cond-mat.stat-mech"], "primary_category": "q-fin.TR"}
{"title": "Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation", "abstract": "OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios.", "published": "2025-02-22 00:50:01", "link": "http://arxiv.org/abs/2502.16022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of\n  Real-World Tasks and Ambiguity in Mental Healthcare", "abstract": "Current medical language model (LM) benchmarks often over-simplify the\ncomplexities of day-to-day clinical practice tasks and instead rely on\nevaluating LMs on multiple-choice board exam questions. Thus, we present an\nexpert-created and annotated dataset spanning five critical domains of\ndecision-making in mental healthcare: treatment, diagnosis, documentation,\nmonitoring, and triage. This dataset - created without any LM assistance - is\ndesigned to capture the nuanced clinical reasoning and daily ambiguities mental\nhealth practitioners encounter, reflecting the inherent complexities of care\ndelivery that are missing from existing datasets. Almost all 203 base questions\nwith five answer options each have had the decision-irrelevant demographic\npatient information removed and replaced with variables (e.g., AGE), and are\navailable for male, female, or non-binary-coded patients. For question\ncategories dealing with ambiguity and multiple valid answer options, we create\na preference dataset with uncertainties from the expert annotations. We outline\na series of intended use cases and demonstrate the usability of our dataset by\nevaluating eleven off-the-shelf and four mental health fine-tuned LMs on\ncategory-specific task accuracy, on the impact of patient demographic\ninformation on decision-making, and how consistently free-form responses\ndeviate from human annotated samples.", "published": "2025-02-22 03:10:16", "link": "http://arxiv.org/abs/2502.16051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "abstract": "Large Language Models (LLMs) have gained increasing attention for their\nremarkable capacity, alongside concerns about safety arising from their\npotential to produce harmful content. Red teaming aims to find prompts that\ncould elicit harmful responses from LLMs, and is essential to discover and\nmitigate safety risks before real-world deployment. However, manual red teaming\nis both time-consuming and expensive, rendering it unscalable. In this paper,\nwe propose RTPE, a scalable evolution framework to evolve red teaming prompts\nacross both breadth and depth dimensions, facilitating the automatic generation\nof numerous high-quality and diverse red teaming prompts. Specifically,\nin-breadth evolving employs a novel enhanced in-context learning method to\ncreate a multitude of quality prompts, whereas in-depth evolving applies\ncustomized transformation operations to enhance both content and form of\nprompts, thereby increasing diversity. Extensive experiments demonstrate that\nRTPE surpasses existing representative automatic red teaming methods on both\nattack success rate and diversity. In addition, based on 4,800 red teaming\nprompts created by RTPE, we further provide a systematic analysis of 8\nrepresentative LLMs across 8 sensitive topics.", "published": "2025-02-22 06:13:19", "link": "http://arxiv.org/abs/2502.16109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting,\n  and Preventing LLM Hallucination", "abstract": "Hallucination is a persistent challenge in large language models (LLMs),\nwhere even with rigorous quality control, models often generate distorted\nfacts. This paradox, in which error generation continues despite high-quality\ntraining data, calls for a deeper understanding of the underlying LLM\nmechanisms. To address it, we propose a novel concept: knowledge overshadowing,\nwhere model's dominant knowledge can obscure less prominent knowledge during\ntext generation, causing the model to fabricate inaccurate details. Building on\nthis idea, we introduce a novel framework to quantify factual hallucinations by\nmodeling knowledge overshadowing. Central to our approach is the log-linear\nlaw, which predicts that the rate of factual hallucination increases linearly\nwith the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length,\nand (3) Model Size. The law provides a means to preemptively quantify\nhallucinations, offering foresight into their occurrence even before model\ntraining or inference. Built on overshadowing effect, we propose a new decoding\nstrategy CoDa, to mitigate hallucinations, which notably enhance model\nfactuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our\nfindings not only deepen understandings of the underlying mechanisms behind\nhallucinations but also provide actionable insights for developing more\npredictable and controllable language models.", "published": "2025-02-22 08:36:06", "link": "http://arxiv.org/abs/2502.16143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Number Representations in LLMs: A Computational Parallel to Human\n  Perception", "abstract": "Humans are believed to perceive numbers on a logarithmic mental number line,\nwhere smaller values are represented with greater resolution than larger ones.\nThis cognitive bias, supported by neuroscience and behavioral studies, suggests\nthat numerical magnitudes are processed in a sublinear fashion rather than on a\nuniform linear scale. Inspired by this hypothesis, we investigate whether large\nlanguage models (LLMs) exhibit a similar logarithmic-like structure in their\ninternal numerical representations. By analyzing how numerical values are\nencoded across different layers of LLMs, we apply dimensionality reduction\ntechniques such as PCA and PLS followed by geometric regression to uncover\nlatent structures in the learned embeddings. Our findings reveal that the\nmodel's numerical representations exhibit sublinear spacing, with distances\nbetween values aligning with a logarithmic scale. This suggests that LLMs, much\nlike humans, may encode numbers in a compressed, non-uniform manner.", "published": "2025-02-22 08:44:29", "link": "http://arxiv.org/abs/2502.16147v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "abstract": "To compare autoregressive language models at scale, we propose using\nlog-likelihood vectors computed on a predefined text set as model features.\nThis approach has a solid theoretical basis: when treated as model coordinates,\ntheir squared Euclidean distance approximates the Kullback-Leibler divergence\nof text-generation probabilities. Our method is highly scalable, with\ncomputational cost growing linearly in both the number of models and text\nsamples, and is easy to implement as the required features are derived from\ncross-entropy loss. Applying this method to over 1,000 language models, we\nconstructed a \"model map,\" providing a new perspective on large-scale model\nanalysis.", "published": "2025-02-22 10:23:36", "link": "http://arxiv.org/abs/2502.16173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OrderSum: Semantic Sentence Ordering for Extractive Summarization", "abstract": "There are two main approaches to recent extractive summarization: the\nsentence-level framework, which selects sentences to include in a summary\nindividually, and the summary-level framework, which generates multiple\ncandidate summaries and ranks them. Previous work in both frameworks has\nprimarily focused on improving which sentences in a document should be included\nin the summary. However, the sentence order of extractive summaries, which is\ncritical for the quality of a summary, remains underexplored. In this paper, we\nintroduce OrderSum, a novel extractive summarization model that semantically\norders sentences within an extractive summary. OrderSum proposes a new\nrepresentation method to incorporate the sentence order into the embedding of\nthe extractive summary, and an objective function to train the model to\nidentify which extractive summary has a better sentence order in the semantic\nspace. Extensive experimental results demonstrate that OrderSum obtains\nstate-of-the-art performance in both sentence inclusion and sentence order for\nextractive summarization. In particular, OrderSum achieves a ROUGE-L score of\n30.52 on CNN/DailyMail, outperforming the previous state-of-the-art model by a\nlarge margin of 2.54.", "published": "2025-02-22 10:51:04", "link": "http://arxiv.org/abs/2502.16180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IPO: Your Language Model is Secretly a Preference Classifier", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.", "published": "2025-02-22 10:59:11", "link": "http://arxiv.org/abs/2502.16182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM\n  Reasoning", "abstract": "Evaluating large language models (LLMs) poses significant challenges,\nparticularly due to issues of data contamination and the leakage of correct\nanswers. To address these challenges, we introduce ThinkBench, a novel\nevaluation framework designed to evaluate LLMs' reasoning capability robustly.\nThinkBench proposes a dynamic data generation method for constructing\nout-of-distribution (OOD) datasets and offers an OOD dataset that contains\n2,912 samples drawn from reasoning tasks. ThinkBench unifies the evaluation of\nreasoning models and non-reasoning models. We evaluate 16 LLMs and 4 PRMs under\nidentical experimental conditions and show that most of the LLMs' performance\nare far from robust and they face a certain level of data leakage. By\ndynamically generating OOD datasets, ThinkBench effectively provides a reliable\nevaluation of LLMs and reduces the impact of data contamination.", "published": "2025-02-22 15:41:51", "link": "http://arxiv.org/abs/2502.16268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LegalBench.PT: A Benchmark for Portuguese Law", "abstract": "The recent application of LLMs to the legal field has spurred the creation of\nbenchmarks across various jurisdictions and languages. However, no benchmark\nhas yet been specifically designed for the Portuguese legal system. In this\nwork, we present LegalBench.PT, the first comprehensive legal benchmark\ncovering key areas of Portuguese law. To develop LegalBench.PT, we first\ncollect long-form questions and answers from real law exams, and then use\nGPT-4o to convert them into multiple-choice, true/false, and matching formats.\nOnce generated, the questions are filtered and processed to improve the quality\nof the dataset. To ensure accuracy and relevance, we validate our approach by\nhaving a legal professional review a sample of the generated questions.\nAlthough the questions are synthetically generated, we show that their basis in\nhuman-created exams and our rigorous filtering and processing methods applied\nresult in a reliable benchmark for assessing LLMs' legal knowledge and\nreasoning abilities. Finally, we evaluate the performance of leading LLMs on\nLegalBench.PT and investigate potential biases in GPT-4o's responses. We also\nassess the performance of Portuguese lawyers on a sample of questions to\nestablish a baseline for model comparison and validate the benchmark.", "published": "2025-02-22 21:07:12", "link": "http://arxiv.org/abs/2502.16357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines", "abstract": "In this work, we study the effect of annotation guidelines -- textual\ndescriptions of event types and arguments, when instruction-tuning large\nlanguage models for event extraction. We conducted a series of experiments with\nboth human-provided and machine-generated guidelines in both full- and low-data\nsettings. Our results demonstrate the promise of annotation guidelines when\nthere is a decent amount of training data and highlight its effectiveness in\nimproving cross-schema generalization and low-frequency event-type performance.", "published": "2025-02-22 22:38:16", "link": "http://arxiv.org/abs/2502.16377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BottleHumor: Self-Informed Humor Explanation using the Information\n  Bottleneck Principle", "abstract": "Humor is prevalent in online communications and it often relies on more than\none modality (e.g., cartoons and memes). Interpreting humor in multimodal\nsettings requires drawing on diverse types of knowledge, including\nmetaphorical, sociocultural, and commonsense knowledge. However, identifying\nthe most useful knowledge remains an open question. We introduce \\method{}, a\nmethod inspired by the information bottleneck principle that elicits relevant\nworld knowledge from vision and language models which is iteratively refined\nfor generating an explanation of the humor in an unsupervised manner. Our\nexperiments on three datasets confirm the advantage of our method over a range\nof baselines. Our method can further be adapted in the future for additional\ntasks that can benefit from eliciting and conditioning on relevant world\nknowledge and open new research avenues in this direction.", "published": "2025-02-22 11:52:39", "link": "http://arxiv.org/abs/2502.18331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for\n  Multimodal Reasoning Models", "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency.", "published": "2025-02-22 01:52:37", "link": "http://arxiv.org/abs/2502.16033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inference Computation Scaling for Feature Augmentation in Recommendation\n  Systems", "abstract": "Large language models have become a powerful method for feature augmentation\nin recommendation systems. However, existing approaches relying on quick\ninference often suffer from incomplete feature coverage and insufficient\nspecificity in feature descriptions, limiting their ability to capture\nfine-grained user preferences and undermining overall performance. Motivated by\nthe recent success of inference scaling in math and coding tasks, we explore\nwhether scaling inference can address these limitations and enhance feature\nquality.\n  Our experiments show that scaling inference leads to significant improvements\nin recommendation performance, with a 12% increase in NDCG@10. The gains can be\nattributed to two key factors: feature quantity and specificity. In particular,\nmodels using extended Chain-of-Thought (CoT) reasoning generate a greater\nnumber of detailed and precise features, offering deeper insights into user\npreferences and overcoming the limitations of quick inference. We further\ninvestigate the factors influencing feature quantity, revealing that model\nchoice and search strategy play critical roles in generating a richer and more\ndiverse feature set. This is the first work to apply inference scaling to\nfeature augmentation in recommendation systems, bridging advances in reasoning\ntasks to enhance personalized recommendation.", "published": "2025-02-22 02:21:06", "link": "http://arxiv.org/abs/2502.16040v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning\n  Trajectories for Complex Problem Solving", "abstract": "Recent agent frameworks and inference-time algorithms often struggle with\ncomplex planning problems due to limitations in verifying generated plans or\nreasoning and varying complexity of instances within a single task. Many\nexisting methods for these tasks either perform task-level verification without\nconsidering constraints or apply inference-time algorithms without adapting to\ninstance-level complexity. To address these limitations, we propose PlanGEN, a\nmodel-agnostic and easily scalable agent framework with three key components:\nconstraint, verification, and selection agents. Specifically, our approach\nproposes constraint-guided iterative verification to enhance performance of\ninference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN\nframework, the selection agent optimizes algorithm choice based on instance\ncomplexity, ensuring better adaptability to complex planning problems.\nExperimental results demonstrate significant improvements over the strongest\nbaseline across multiple benchmarks, achieving state-of-the-art results on\nNATURAL PLAN ($\\sim$8%$\\uparrow$), OlympiadBench ($\\sim$4%$\\uparrow$), DocFinQA\n($\\sim$7%$\\uparrow$), and GPQA ($\\sim$1%$\\uparrow$). Our key finding highlights\nthat constraint-guided iterative verification improves inference-time\nalgorithms, and adaptive selection further boosts performance on complex\nplanning and reasoning problems.", "published": "2025-02-22 06:21:56", "link": "http://arxiv.org/abs/2502.16111v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Chain-of-Description: What I can understand, I can put into words", "abstract": "In this paper, we propose a novel strategy defined as Chain-of-Description\n(CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach\ninvolves having the model first provide a detailed description of the\nmulti-modal input before generating an answer to the question. When applied to\nmodels such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting\nsignificantly enhances performance compared to standard prompting methods. This\nis demonstrated by nearly a 4\\% improvement in the speech category of the audio\nbenchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of\nthe vision benchmark MMMU\\_Pro. Our ablation study further validates the\neffectiveness of CoD Prompting.", "published": "2025-02-22 08:27:31", "link": "http://arxiv.org/abs/2502.16137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Improved Deep Learning Model for Word Embeddings Based Clustering for\n  Large Text Datasets", "abstract": "In this paper, an improved clustering technique for large textual datasets by\nleveraging fine-tuned word embeddings is presented. WEClustering technique is\nused as the base model. WEClustering model is fur-ther improvements\nincorporating fine-tuning contextual embeddings, advanced dimensionality\nreduction methods, and optimization of clustering algorithms. Experimental\nresults on benchmark datasets demon-strate significant improvements in\nclustering metrics such as silhouette score, purity, and adjusted rand index\n(ARI). An increase of 45% and 67% of median silhouette score is reported for\nthe proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based\non Agglomerative models), respec-tively. The proposed technique will help to\nbridge the gap between semantic understanding and statistical robustness for\nlarge-scale text-mining tasks.", "published": "2025-02-22 08:28:41", "link": "http://arxiv.org/abs/2502.16139v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Zero-shot Rare Word Recognition Improvements Through LLM\n  Integration", "abstract": "In this study, we investigate the integration of a large language model (LLM)\nwith an automatic speech recognition (ASR) system, specifically focusing on\nenhancing rare word recognition performance. Using a 190,000-hour dataset\nprimarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling,\nwe demonstrate that the LLM-ASR architecture outperforms traditional\nZipformer-Transducer models in the zero-shot rare word recognition task, after\ntraining on a large dataset. Our analysis reveals that the LLM contributes\nsignificantly to improvements in rare word error rate (R-WER), while the speech\nencoder primarily determines overall transcription performance (Orthographic\nWord Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through\nextensive ablation studies, we highlight the importance of adapter integration\nin aligning speech encoder outputs with the LLM's linguistic capabilities.\nFurthermore, we emphasize the critical role of high-quality labeled data in\nachieving optimal performance. These findings provide valuable insights into\nthe synergy between LLM-based ASR architectures, paving the way for future\nadvancements in large-scale LLM-based speech recognition systems.", "published": "2025-02-22 08:30:38", "link": "http://arxiv.org/abs/2502.16142v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Advanced Text Analytics -- Graph Neural Network for Fake News Detection\n  in Social Media", "abstract": "Traditional Graph Neural Network (GNN) approaches for fake news detection\n(FND) often depend on auxiliary, non-textual data such as user interaction\nhistories or content dissemination patterns. However, these data sources are\nnot always accessible, limiting the effectiveness and applicability of such\nmethods. Additionally, existing models frequently struggle to capture the\ndetailed and intricate relationships within textual information, reducing their\noverall accuracy. In order to address these challenges Advanced Text Analysis\nGraph Neural Network (ATA-GNN) is proposed in this paper. The proposed model is\ndesigned to operate solely on textual data. ATA-GNN employs innovative topic\nmodelling (clustering) techniques to identify typical words for each topic,\nleveraging multiple clustering dimensions to achieve a comprehensive semantic\nunderstanding of the text. This multi-layered design enables the model to\nuncover intricate textual patterns while contextualizing them within a broader\nsemantic framework, significantly enhancing its interpretative capabilities.\nExtensive evaluations on widely used benchmark datasets demonstrate that\nATA-GNN surpasses the performance of current GNN-based FND methods. These\nfindings validate the potential of integrating advanced text clustering within\nGNN architectures to achieve more reliable and text-focused detection\nsolutions.", "published": "2025-02-22 09:17:33", "link": "http://arxiv.org/abs/2502.16157v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ZiGong 1.0: A Large Language Model for Financial Credit", "abstract": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious general Natural Language Processing (NLP) tasks. However, their\neffectiveness in financial credit assessment applications remains suboptimal,\nprimarily due to the specialized financial expertise required for these tasks.\nTo address this limitation, we propose ZiGong, a Mistral-based model enhanced\nthrough multi-task supervised fine-tuning. To specifically combat model\nhallucination in financial contexts, we introduce a novel data pruning\nmethodology. Our approach utilizes a proxy model to score training samples,\nsubsequently combining filtered data with original datasets for model training.\nThis data refinement strategy effectively reduces hallucinations in LLMs while\nmaintaining reliability in downstream financial applications. Experimental\nresults show our method significantly enhances model robustness and prediction\naccuracy in real-world financial scenarios.", "published": "2025-02-22 09:27:56", "link": "http://arxiv.org/abs/2502.16159v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "OmniParser V2: Structured-Points-of-Thought for Unified Visual Text\n  Parsing and Its Generality to Multimodal Large Language Models", "abstract": "Visually-situated text parsing (VsTP) has recently seen notable advancements,\ndriven by the growing demand for automated document understanding and the\nemergence of large language models capable of processing document-based\nquestions. While various methods have been proposed to tackle the complexities\nof VsTP, existing solutions often rely on task-specific architectures and\nobjectives for individual tasks. This leads to modal isolation and complex\nworkflows due to the diversified targets and heterogeneous schemas. In this\npaper, we introduce OmniParser V2, a universal model that unifies VsTP typical\ntasks, including text spotting, key information extraction, table recognition,\nand layout analysis, into a unified framework. Central to our approach is the\nproposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves\nmodel performance across diverse scenarios by leveraging a unified\nencoder-decoder architecture, objective, and input\\&output representation. SPOT\neliminates the need for task-specific architectures and loss functions,\nsignificantly simplifying the processing pipeline. Our extensive evaluations\nacross four tasks on eight different datasets show that OmniParser V2 achieves\nstate-of-the-art or competitive results in VsTP. Additionally, we explore the\nintegration of SPOT within a multimodal large language model structure, further\nenhancing text localization and recognition capabilities, thereby confirming\nthe generality of SPOT prompting technique. The code is available at\n\\href{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}{AdvancedLiterateMachinery}.", "published": "2025-02-22 09:32:01", "link": "http://arxiv.org/abs/2502.16161v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph\n  Question and Answering", "abstract": "Due to the remarkable reasoning ability, Large language models (LLMs) have\ndemonstrated impressive performance in knowledge graph question answering\n(KGQA) tasks, which find answers to natural language questions over knowledge\ngraphs (KGs). To alleviate the hallucinations and lack of knowledge issues of\nLLMs, existing methods often retrieve the question-related information from KGs\nto enrich the input context. However, most methods focus on retrieving the\nrelevant information while ignoring the importance of different types of\nknowledge in reasoning, which degrades their performance. To this end, this\npaper reformulates the KGQA problem as a graphical model and proposes a\nthree-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM)\nfor KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a\nsubgraph related to the question from the original knowledge graph. In the\nsecond stage, EPERM filters out the evidence paths that faithfully support the\nreasoning of the questions, and score their importance in reasoning. Finally,\nEPERM uses the weighted evidence paths to reason the final answer. Since\nconsidering the importance of different structural information in KGs for\nreasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks.\nExtensive experiments on benchmark datasets demonstrate that EPERM achieves\nsuperior performances in KGQA tasks.", "published": "2025-02-22 10:05:22", "link": "http://arxiv.org/abs/2502.16171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking", "abstract": "Complex claim fact-checking performs a crucial role in disinformation\ndetection. However, existing fact-checking methods struggle with claim\nvagueness, specifically in effectively handling latent information and complex\nrelations within claims. Moreover, evidence redundancy, where nonessential\ninformation complicates the verification process, remains a significant issue.\nTo tackle these limitations, we propose Bilateral Defusing Verification\n(BiDeV), a novel fact-checking working-flow framework integrating multiple\nrole-played LLMs to mimic the human-expert fact-checking process. BiDeV\nconsists of two main modules: Vagueness Defusing identifies latent information\nand resolves complex relations to simplify the claim, and Redundancy Defusing\neliminates redundant content to enhance the evidence quality. Extensive\nexperimental results on two widely used challenging fact-checking benchmarks\n(Hover and Feverous-s) demonstrate that our BiDeV can achieve the best\nperformance under both gold and open settings. This highlights the\neffectiveness of BiDeV in handling complex claims and ensuring precise\nfact-checking", "published": "2025-02-22 10:58:40", "link": "http://arxiv.org/abs/2502.16181v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation", "abstract": "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.", "published": "2025-02-22 16:00:01", "link": "http://arxiv.org/abs/2502.16274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interrogating LLM design under a fair learning doctrine", "abstract": "The current discourse on large language models (LLMs) and copyright largely\ntakes a \"behavioral\" perspective, focusing on model outputs and evaluating\nwhether they are substantially similar to training data. However, substantial\nsimilarity is difficult to define algorithmically and a narrow focus on model\noutputs is insufficient to address all copyright risks. In this\ninterdisciplinary work, we take a complementary \"structural\" perspective and\nshift our focus to how LLMs are trained. We operationalize a notion of \"fair\nlearning\" by measuring whether any training decision substantially affected the\nmodel's memorization. As a case study, we deconstruct Pythia, an open-source\nLLM, and demonstrate the use of causal and correlational analyses to make\nfactual determinations about Pythia's training decisions. By proposing a legal\nstandard for fair learning and connecting memorization analyses to this\nstandard, we identify how judges may advance the goals of copyright law through\nadjudication. Finally, we discuss how a fair learning standard might evolve to\nenhance its clarity by becoming more rule-like and incorporating external\ntechnical guidelines.", "published": "2025-02-22 16:41:11", "link": "http://arxiv.org/abs/2502.16290v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Iterative Auto-Annotation for Scientific Named Entity Recognition Using\n  BERT-Based Models", "abstract": "This paper presents an iterative approach to performing Scientific Named\nEntity Recognition (SciNER) using BERT-based models. We leverage transfer\nlearning to fine-tune pretrained models with a small but high-quality set of\nmanually annotated data. The process is iteratively refined by using the\nfine-tuned model to auto-annotate a larger dataset, followed by additional\nrounds of fine-tuning. We evaluated two models, dslim/bert-large-NER and\nbert-largecased, and found that bert-large-cased consistently outperformed the\nformer. Our approach demonstrated significant improvements in prediction\naccuracy and F1 scores, especially for less common entity classes. Future work\ncould include pertaining with unlabeled data, exploring more powerful encoders\nlike RoBERTa, and expanding the scope of manual annotations. This methodology\nhas broader applications in NLP tasks where access to labeled data is limited.", "published": "2025-02-22 17:58:20", "link": "http://arxiv.org/abs/2502.16312v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA\n  Dataset with Answer Plausibility Scores", "abstract": "Large Language Models (LLMs) are revolutionizing information retrieval, with\nchatbots becoming an important source for answering user queries. As by their\ndesign, LLMs prioritize generating correct answers, the value of highly\nplausible yet incorrect answers (candidate answers) tends to be overlooked.\nHowever, such answers can still prove useful, for example, they can play a\ncrucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA\nRobustness Assessment (QARA). Existing QA datasets primarily focus on correct\nanswers without explicit consideration of the plausibility of other candidate\nanswers, limiting opportunity for more nuanced evaluations of models. To\naddress this gap, we introduce PlausibleQA, a large-scale dataset comprising\n10,000 questions and 100,000 candidate answers, each annotated with\nplausibility scores and justifications for their selection. Additionally, the\ndataset includes 900,000 justifications for pairwise comparisons between\ncandidate answers, further refining plausibility assessments. We evaluate\nPlausibleQA through human assessments and empirical experiments, demonstrating\nits utility in MCQA and QARA analysis. Our findings show that\nplausibility-aware approaches are effective for MCQA distractor generation and\nQARA. We release PlausibleQA as a resource for advancing QA research and\nenhancing LLM performance in distinguishing plausible distractors from correct\nanswers.", "published": "2025-02-22 21:14:18", "link": "http://arxiv.org/abs/2502.16358v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Toward a Flexible Framework for Linear Representation Hypothesis Using\n  Maximum Likelihood Estimation", "abstract": "Linear representation hypothesis posits that high-level concepts are encoded\nas linear directions in the representation spaces of LLMs. Park et al. (2024)\nformalize this notion by unifying multiple interpretations of linear\nrepresentation, such as 1-dimensional subspace representation and\ninterventions, using a causal inner product. However, their framework relies on\nsingle-token counterfactual pairs and cannot handle ambiguous contrasting\npairs, limiting its applicability to complex or context-dependent concepts. We\nintroduce a new notion of binary concepts as unit vectors in a canonical\nrepresentation space, and utilize LLMs' (neural) activation differences along\nwith maximum likelihood estimation (MLE) to compute concept directions (i.e.,\nsteering vectors). Our method, Sum of Activation-base Normalized Difference\n(SAND), formalizes the use of activation differences modeled as samples from a\nvon Mises-Fisher (vMF) distribution, providing a principled approach to derive\nconcept directions. We extend the applicability of Park et al. (2024) by\neliminating the dependency on unembedding representations and single-token\npairs. Through experiments with LLaMA models across diverse concepts and\nbenchmarks, we demonstrate that our lightweight approach offers greater\nflexibility, superior performance in activation engineering tasks like\nmonitoring and manipulation.", "published": "2025-02-22 23:56:30", "link": "http://arxiv.org/abs/2502.16385v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Comprehensive Survey of Machine Unlearning Techniques for Large\n  Language Models", "abstract": "This study investigates the machine unlearning techniques within the context\nof large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM\nunlearning offers a principled approach to removing the influence of\nundesirable data (e.g., sensitive or illegal information) from LLMs, while\npreserving their overall utility without requiring full retraining. Despite\ngrowing research interest, there is no comprehensive survey that systematically\norganizes existing work and distills key insights; here, we aim to bridge this\ngap. We begin by introducing the definition and the paradigms of LLM\nunlearning, followed by a comprehensive taxonomy of existing unlearning\nstudies. Next, we categorize current unlearning approaches, summarizing their\nstrengths and limitations. Additionally, we review evaluation metrics and\nbenchmarks, providing a structured overview of current assessment\nmethodologies. Finally, we outline promising directions for future research,\nhighlighting key challenges and opportunities in the field.", "published": "2025-02-22 12:46:14", "link": "http://arxiv.org/abs/2503.01854v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and\n  Satisfaction Problems in MiniZinc", "abstract": "There is growing interest in utilizing large language models (LLMs) as\nco-pilots for combinatorial optimization and constraint programming tasks\nacross various problems. This paper aims to advance this line of research by\nintroducing Text2Zinc}, a cross-domain dataset for capturing optimization and\nsatisfaction problems specified in natural language text. Our work is\ndistinguished from previous attempts by integrating both satisfaction and\noptimization problems within a unified dataset using a solver-agnostic modeling\nlanguage. To achieve this, we leverage MiniZinc's solver-and-paradigm-agnostic\nmodeling capabilities to formulate these problems. Using the Text2Zinc dataset,\nwe conduct comprehensive baseline experiments to compare execution and solution\naccuracy across several methods, including off-the-shelf prompting strategies,\nchain-of-thought reasoning, and a compositional approach. Additionally, we\nexplore the effectiveness of intermediary representations, specifically\nknowledge graphs. Our findings indicate that LLMs are not yet a push-button\ntechnology to model combinatorial problems from text. We hope that Text2Zinc\nserves as a valuable resource for researchers and practitioners to advance the\nfield further.", "published": "2025-02-22 04:13:53", "link": "http://arxiv.org/abs/2503.10642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Echo: A Large Language Model with Temporal Episodic Memory", "abstract": "Research on large language models (LLMs) has shown remarkable performance in\ndomains such as mathematics, programming, and literary creation. However, most\nstudies have focused on semantic memory-based question answering, neglecting\nLLMs' potential to handle episodic memory (EM)-related queries. This oversight\nhas led to suboptimal performance in applications requiring EM, including\nemotional companionship, personal AI assistants, and AI teachers. To address\nthis gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We\npropose a Multi-Agent Data Generation Framework that guides the model in\ngenerating multi-turn, complex scenario episodic memory dialogue data\n(EM-Train). Temporal information is innovatively incorporated into the LLM\ntraining process, and Echo is trained using the EM-Train. Furthermore, We\ndevelop an EM-Test benchmark specifically designed to evaluate LLMs' episodic\nmemory capabilities. The EM-Test assesses performance across various time spans\nand difficulty levels, providing a comprehensive evaluation of multi-turn\nepisodic memory dialogues. Our experiments demonstrate that Echo significantly\noutperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative\nanalysis reveals Echo's potential to exhibit human-like episodic memory\ncapabilities. We will open-source all datasets, code, and model weights.", "published": "2025-02-22 05:25:20", "link": "http://arxiv.org/abs/2502.16090v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of\n  Their Inputs?", "abstract": "Ensuring the safety of the Large Language Model (LLM) is critical, but\ncurrently used methods in most cases sacrifice the model performance to obtain\nincreased safety or perform poorly on data outside of their adaptation\ndistribution. We investigate existing methods for such generalization and find\nthem insufficient. Surprisingly, while even plain LLMs recognize unsafe\nprompts, they may still generate unsafe responses. To avoid performance\ndegradation and preserve safe performance, we advocate for a two-step\nframework, where we first identify unsafe prompts via a lightweight classifier,\nand apply a \"safe\" model only to such prompts. In particular, we explore the\ndesign of the safety detector in more detail, investigating the use of\ndifferent classifier architectures and prompting techniques. Interestingly, we\nfind that the final hidden state for the last token is enough to provide robust\nperformance, minimizing false positives on benign data while performing well on\nmalicious prompt detection. Additionally, we show that classifiers trained on\nthe representations from different model layers perform comparably on the\nlatest model layers, indicating that safety representation is present in the\nLLMs' hidden states at most model stages. Our work is a step towards efficient,\nrepresentation-based safety mechanisms for LLMs.", "published": "2025-02-22 10:31:50", "link": "http://arxiv.org/abs/2502.16174v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Conflicts of Interest in Published NLP Research 2000-2024", "abstract": "Natural Language Processing research is increasingly reliant on large scale\ndata and computational power. Many achievements in the past decade resulted\nfrom collaborations with the tech industry. But an increasing entanglement of\nacademic research and industry interests leads to conflicts of interest. We\nassessed published NLP research from 2000-2024 and labeled author affiliations\nas academic or industry-affiliated to measure conflicts of interest. Overall\n27.65% of the papers contained at least one industry-affiliated author. That\nfigure increased substantially with more than 1 in 3 papers having a conflict\nof interest in 2024. We identify top-tier venues (ACL, EMNLP) as main drivers\nfor that effect. The paper closes with a discussion and a simple, concrete\nsuggestion for the future.", "published": "2025-02-22 12:44:57", "link": "http://arxiv.org/abs/2502.16218v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Dynamic Coalition Structure Detection in Natural Language-based\n  Interactions", "abstract": "In strategic multi-agent sequential interactions, detecting dynamic coalition\nstructures is crucial for understanding how self-interested agents coordinate\nto influence outcomes. However, natural-language-based interactions introduce\nunique challenges to coalition detection due to ambiguity over intents and\ndifficulty in modeling players' subjective perspectives. We propose a new\nmethod that leverages recent advancements in large language models and game\ntheory to predict dynamic multilateral coalition formation in Diplomacy, a\nstrategic multi-agent game where agents negotiate coalitions using natural\nlanguage. The method consists of two stages. The first stage extracts the set\nof agreements discussed by two agents in their private dialogue, by combining a\nparsing-based filtering function with a fine-tuned language model trained to\npredict player intents. In the second stage, we define a new metric using the\nconcept of subjective rationalizability from hypergame theory to evaluate the\nexpected value of an agreement for each player. We then compute this metric for\neach agreement identified in the first stage by assessing the strategic value\nof the agreement for both players and taking into account the subjective belief\nof one player that the second player would honor the agreement. We demonstrate\nthat our method effectively detects potential coalition structures in online\nDiplomacy gameplay by assigning high values to agreements likely to be honored\nand low values to those likely to be violated. The proposed method provides\nfoundational insights into coalition formation in multi-agent environments with\nlanguage-based negotiation and offers key directions for future research on the\nanalysis of complex natural language-based interactions between agents.", "published": "2025-02-22 20:12:52", "link": "http://arxiv.org/abs/2502.16339v1", "categories": ["cs.MA", "cs.CL", "cs.GT"], "primary_category": "cs.MA"}
{"title": "A Framework for Evaluating Vision-Language Model Safety: Building Trust\n  in AI for Public Sector Applications", "abstract": "Vision-Language Models (VLMs) are increasingly deployed in public sector\nmissions, necessitating robust evaluation of their safety and vulnerability to\nadversarial attacks. This paper introduces a novel framework to quantify\nadversarial risks in VLMs. We analyze model performance under Gaussian,\nsalt-and-pepper, and uniform noise, identifying misclassification thresholds\nand deriving composite noise patches and saliency patterns that highlight\nvulnerable regions. These patterns are compared against the Fast Gradient Sign\nMethod (FGSM) to assess their adversarial effectiveness. We propose a new\nVulnerability Score that combines the impact of random noise and adversarial\nattacks, providing a comprehensive metric for evaluating model robustness.", "published": "2025-02-22 21:33:26", "link": "http://arxiv.org/abs/2502.16361v1", "categories": ["cs.CY", "cs.CL", "cs.CV", "I.2.10; I.4.9; K.4.1"], "primary_category": "cs.CY"}
{"title": "A generative approach to LLM harmfulness detection with special red flag\n  tokens", "abstract": "Most safety training methods for large language models (LLMs) based on\nfine-tuning rely on dramatically changing the output distribution of the model\nwhen faced with a harmful request, shifting it from an unsafe answer to a\nrefusal to respond. These methods inherently compromise model capabilities and\nmight make auto-regressive models vulnerable to attacks that make likely an\ninitial token of affirmative response. To avoid that, we propose to expand the\nmodel's vocabulary with a special token we call red flag token (<rf>) and\npropose to fine-tune the model to generate this token at any time harmful\ncontent is generated or about to be generated. This novel safety training\nmethod effectively augments LLMs into generative classifiers of harmfulness at\nall times during the conversation. This method offers several advantages: it\nenables the model to explicitly learn the concept of harmfulness while\nmarginally affecting the generated distribution, thus maintaining the model's\nutility. It also evaluates each generated answer rather than just the input\nprompt and provides a stronger defence against sampling-based attacks. In\naddition, it simplifies the evaluation of the model's robustness and reduces\ncorrelated failures when combined with a classifier. We further show an\nincreased robustness to long contexts, and supervised fine-tuning attacks.", "published": "2025-02-22 21:48:48", "link": "http://arxiv.org/abs/2502.16366v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Knowledge Identification and Fusion for Language Model\n  Continual Learning", "abstract": "Continual learning (CL) is crucial for deploying large language models (LLMs)\nin dynamic real-world environments without costly retraining. While recent\nmodel ensemble and model merging methods guided by parameter importance have\ngained popularity, they often struggle to balance knowledge transfer and\nforgetting, mainly due to the reliance on static importance estimates during\nsequential training. In this paper, we present Recurrent-KIF, a novel CL\nframework for Recurrent Knowledge Identification and Fusion, which enables\ndynamic estimation of parameter importance distributions to enhance knowledge\ntransfer. Inspired by human continual learning, Recurrent-KIF employs an inner\nloop that rapidly adapts to new tasks while identifying important parameters,\ncoupled with an outer loop that globally manages the fusion of new and\nhistorical knowledge through redundant knowledge pruning and key knowledge\nmerging. These inner-outer loops iteratively perform multiple rounds of fusion,\nallowing Recurrent-KIF to leverage intermediate training information and\nadaptively adjust fusion strategies based on evolving importance distributions.\nExtensive experiments on two CL benchmarks with various model sizes (from 770M\nto 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic\nforgetting and enhances knowledge transfer.", "published": "2025-02-22 05:37:27", "link": "http://arxiv.org/abs/2502.17510v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment", "abstract": "With the integration of image modality, the semantic space of multimodal\nlarge language models (MLLMs) is more complex than text-only models, making\ntheir interpretability more challenging and their alignment less stable,\nparticularly susceptible to low-quality data, which can lead to inconsistencies\nbetween modalities, hallucinations, and biased outputs. As a result, developing\ninterpretability methods for MLLMs is crucial for improving alignment quality\nand efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained\nattention for their ability to interpret latent representations. However,\nextending SAEs to multimodal settings presents new challenges due to modality\nfusion and the difficulty of isolating cross-modal representations. To address\nthese challenges, we introduce SAE-V, a mechanistic interpretability framework\nthat extends the SAE paradigm to MLLMs. By identifying and analyzing\ninterpretable features along with their corresponding data, SAE-V enables\nfine-grained interpretation of both model behavior and data quality,\nfacilitating a deeper understanding of cross-modal interactions and alignment\ndynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides\nan intrinsic data filtering mechanism to enhance model alignment without\nrequiring additional models. Specifically, when applied to the alignment\nprocess of MLLMs, SAE-V-based data filtering methods could achieve more than\n110% performance with less than 50% data. Our results highlight SAE-V's ability\nto enhance interpretability and alignment in MLLMs, providing insights into\ntheir internal mechanisms.", "published": "2025-02-22 14:20:07", "link": "http://arxiv.org/abs/2502.17514v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Protecting Users From Themselves: Safeguarding Contextual Privacy in\n  Interactions with Conversational Agents", "abstract": "Conversational agents are increasingly woven into individuals' personal\nlives, yet users often underestimate the privacy risks involved. The moment\nusers share information with these agents (e.g., LLMs), their private\ninformation becomes vulnerable to exposure. In this paper, we characterize the\nnotion of contextual privacy for user interactions with LLMs. It aims to\nminimize privacy risks by ensuring that users (sender) disclose only\ninformation that is both relevant and necessary for achieving their intended\ngoals when interacting with LLMs (untrusted receivers). Through a formative\ndesign user study, we observe how even \"privacy-conscious\" users inadvertently\nreveal sensitive information through indirect disclosures. Based on insights\nfrom this study, we propose a locally-deployable framework that operates\nbetween users and LLMs, and identifies and reformulates out-of-context\ninformation in user prompts. Our evaluation using examples from ShareGPT shows\nthat lightweight models can effectively implement this framework, achieving\nstrong gains in contextual privacy while preserving the user's intended\ninteraction goals through different approaches to classify information relevant\nto the intended goals.", "published": "2025-02-22 09:05:39", "link": "http://arxiv.org/abs/2502.18509v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Analyzing User Perceptions of Large Language Models (LLMs) on Reddit:\n  Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions", "abstract": "While there is an increased discourse on large language models (LLMs) like\nChatGPT and DeepSeek, there is no comprehensive understanding of how users of\nonline platforms, like Reddit, perceive these models. This is an important\nomission because public opinion can influence AI development, trust, and future\npolicy. This study aims at analyzing Reddit discussions about ChatGPT and\nDeepSeek using sentiment and topic modeling to advance the understanding of\nuser attitudes. Some of the significant topics such as trust in AI, user\nexpectations, potential uses of the tools, reservations about AI biases, and\nethical implications of their use are explored in this study. By examining\nthese concerns, the study provides a sense of how public sentiment might shape\nthe direction of AI development going forward. The report also mentions whether\nusers have faith in the technology and what they see as its future. A word\nfrequency approach is used to identify broad topics and sentiment trends. Also,\ntopic modeling through the Latent Dirichlet Allocation (LDA) method identifies\ntop topics in users' language, for example, potential benefits of LLMs, their\ntechnological applications, and their overall social ramifications. The study\naims to inform developers and policymakers by making it easier to see how users\ncomprehend and experience these game-changing technologies.", "published": "2025-02-22 17:00:42", "link": "http://arxiv.org/abs/2502.18513v1", "categories": ["cs.SI", "cs.CL", "cs.HC"], "primary_category": "cs.SI"}
{"title": "Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating\n  Hallucinations in Large Language Models", "abstract": "Large Language Models (LLMs) are known to hallucinate and generate\nnon-factual outputs which can undermine user trust. Traditional methods to\ndirectly mitigate hallucinations, such as representation editing and\ncontrastive decoding, often require additional training data and involve high\nimplementation complexity. While ensemble-based approaches harness multiple\nLLMs to tap into the \"wisdom of crowds\", these methods overlook uncertainties\nin individual model responses. Recent studies reveal that uncertainty\nestimation can enable LLMs to self-assess the likelihood of generating\nhallucinations. In this work, we focus on factoid question answering (QA) and\nobserve that LLMs accuracy and self-assessment capabilities vary widely with\ndifferent models excelling in different scenarios. Leveraging this insight, we\npropose Uncertainty-Aware Fusion (UAF), an ensemble framework to reduces\nhallucinations by strategically combining multiple LLM based on their accuracy\nand self-assessment abilities. Empirical results on several public benchmark\ndatasets show that UAF outperforms state-of-the-art hallucination mitigation\nmethods by $8\\%$ in factual accuracy, while either narrowing or surpassing the\nperformance gap with GPT-4.", "published": "2025-02-22 10:48:18", "link": "http://arxiv.org/abs/2503.05757v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integrating Personality into Digital Humans: A Review of LLM-Driven\n  Approaches for Virtual Reality", "abstract": "The integration of large language models (LLMs) into virtual reality (VR)\nenvironments has opened new pathways for creating more immersive and\ninteractive digital humans. By leveraging the generative capabilities of LLMs\nalongside multimodal outputs such as facial expressions and gestures, virtual\nagents can simulate human-like personalities and emotions, fostering richer and\nmore engaging user experiences. This paper provides a comprehensive review of\nmethods for enabling digital humans to adopt nuanced personality traits,\nexploring approaches such as zero-shot, few-shot, and fine-tuning.\nAdditionally, it highlights the challenges of integrating LLM-driven\npersonality traits into VR, including computational demands, latency issues,\nand the lack of standardized evaluation frameworks for multimodal interactions.\nBy addressing these gaps, this work lays a foundation for advancing\napplications in education, therapy, and gaming, while fostering\ninterdisciplinary collaboration to redefine human-computer interaction in VR.", "published": "2025-02-22 01:33:05", "link": "http://arxiv.org/abs/2503.16457v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Improving Speech Enhancement by Cross- and Sub-band Processing with\n  State Space Model", "abstract": "Recently, the state space model (SSM) represented by Mamba has shown\nremarkable performance in long-term sequence modeling tasks, including speech\nenhancement. However, due to substantial differences in sub-band features,\napplying the same SSM to all sub-bands limits its inference capability.\nAdditionally, when processing each time frame of the time-frequency\nrepresentation, the SSM may forget certain high-frequency information of low\nenergy, making the restoration of structure in the high-frequency bands\nchallenging. For this reason, we propose Cross- and Sub-band Mamba (CSMamba).\nTo assist the SSM in handling different sub-band features flexibly, we propose\na band split block that splits the full-band into four sub-bands with different\nwidths based on their information similarity. We then allocate independent\nweights to each sub-band, thereby reducing the inference burden on the SSM.\nFurthermore, to mitigate the forgetting of low-energy information in the\nhigh-frequency bands by the SSM, we introduce a spectrum restoration block that\nenhances the representation of the cross-band features from multiple\nperspectives. Experimental results on the DNS Challenge 2021 dataset\ndemonstrate that CSMamba outperforms several state-of-the-art (SOTA) speech\nenhancement methods in three objective evaluation metrics with fewer\nparameters.", "published": "2025-02-22 12:19:46", "link": "http://arxiv.org/abs/2502.16207v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multizone sound field reproduction with\n  direction-of-arrival-distribution-based regularization and its application to\n  binaural-centered mode-matching", "abstract": "In higher-order Ambisonics, a framework for sound field reproduction,\nsecondary-source driving signals are generally obtained by regularized mode\nmatching. The authors have proposed a regularization technique based on\ndirection-of-arrival (DoA) distribution of wavefronts in the primary sound\nfield. Such DoA-distribution-based regularization enables a suppression of\nexcessively large driving signal gains for secondary sources that are in the\ndirections far from the primary source direction. This improves the\nreproduction accuracy at regions away from the reproduction center. First, this\nstudy applies the DoA-distribution-based regularization to a multizone sound\nfield reproduction based on the addition theorem. Furthermore, the regularized\nmultizone sound field reproduction is extended to a binaural-centered mode\nmatching (BCMM), which produces two reproduction points, one at each ear, to\navoid a degraded reproduction accuracy due to a shrinking sweet spot at higher\nfrequencies. Free-field and binaural simulations were numerically performed to\nexamine the effectiveness of the DoA-distribution-based regularization on the\nmultizone sound field reproduction and the BCMM.", "published": "2025-02-22 12:37:40", "link": "http://arxiv.org/abs/2502.16213v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "voc2vec: A Foundation Model for Non-Verbal Vocalization", "abstract": "Speech foundation models have demonstrated exceptional capabilities in\nspeech-related tasks. Nevertheless, these models often struggle with non-verbal\naudio data, such as vocalizations, baby crying, etc., which are critical for\nvarious real-world applications. Audio foundation models well handle non-speech\ndata but also fail to capture the nuanced features of non-verbal human sounds.\nIn this work, we aim to overcome the above shortcoming and propose a novel\nfoundation model, termed voc2vec, specifically designed for non-verbal human\ndata leveraging exclusively open-source non-verbal audio datasets. We employ a\ncollection of 10 datasets covering around 125 hours of non-verbal audio.\nExperimental results prove that voc2vec is effective in non-verbal vocalization\nclassification, and it outperforms conventional speech and audio foundation\nmodels. Moreover, voc2vec consistently outperforms strong baselines, namely\nOpenSmile and emotion2vec, on six different benchmark datasets. To the best of\nthe authors' knowledge, voc2vec is the first universal representation model for\nvocalization tasks.", "published": "2025-02-22 17:07:28", "link": "http://arxiv.org/abs/2502.16298v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec", "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.", "published": "2025-02-22 14:25:55", "link": "http://arxiv.org/abs/2502.16240v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
