{"title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "abstract": "The charge prediction task is to determine appropriate charges for a given\ncase, which is helpful for legal assistant systems where the user input is fact\ndescription. We argue that relevant law articles play an important role in this\ntask, and therefore propose an attention-based neural network method to jointly\nmodel the charge prediction task and the relevant article extraction task in a\nunified framework. The experimental results show that, besides providing legal\nbasis, the relevant articles can also clearly improve the charge prediction\nresults, and our full model can effectively predict appropriate charges for\ncases with different expression styles.", "published": "2017-07-28 09:46:29", "link": "http://arxiv.org/abs/1707.09168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving coreference resolution with automatically predicted prosodic\n  information", "abstract": "Adding manually annotated prosodic information, specifically pitch accents\nand phrasing, to the typical text-based feature set for coreference resolution\nhas previously been shown to have a positive effect on German data. Practical\napplications on spoken language, however, would rely on automatically predicted\nprosodic information. In this paper we predict pitch accents (and phrase\nboundaries) using a convolutional neural network (CNN) model from acoustic\nfeatures extracted from the speech signal. After an assessment of the quality\nof these automatic prosodic annotations, we show that they also significantly\nimprove coreference resolution.", "published": "2017-07-28 13:49:46", "link": "http://arxiv.org/abs/1707.09231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online Deception Detection Refueled by Real World Data Collection", "abstract": "The lack of large realistic datasets presents a bottleneck in online\ndeception detection studies. In this paper, we apply a data collection method\nbased on social network analysis to quickly identify high-quality deceptive and\ntruthful online reviews from Amazon. The dataset contains more than 10,000\ndeceptive reviews and is diverse in product domains and reviewers. Using this\ndataset, we explore effective general features for online deception detection\nthat perform well across domains. We demonstrate that with generalized features\n- advertising speak and writing complexity scores - deception detection\nperformance can be further improved by adding additional deceptive reviews from\nassorted domains in training. Finally, reviewer level evaluation gives an\ninteresting insight into different deceptive reviewers' writing styles.", "published": "2017-07-28 20:27:35", "link": "http://arxiv.org/abs/1707.09406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Weakly Supervised Approach to Train Temporal Relation Classifiers and\n  Acquire Regular Event Pairs Simultaneously", "abstract": "Capabilities of detecting temporal relations between two events can benefit\nmany applications. Most of existing temporal relation classifiers were trained\nin a supervised manner. Instead, we explore the observation that regular event\npairs show a consistent temporal relation despite of their various contexts,\nand these rich contexts can be used to train a contextual temporal relation\nclassifier, which can further recognize new temporal relation contexts and\nidentify new regular event pairs. We focus on detecting after and before\ntemporal relations and design a weakly supervised learning approach that\nextracts thousands of regular event pairs and learns a contextual temporal\nrelation classifier simultaneously. Evaluation shows that the acquired regular\nevent pairs are of high quality and contain rich commonsense knowledge and\ndomain specific knowledge. In addition, the weakly supervised trained temporal\nrelation classifier achieves comparable performance with the state-of-the-art\nsupervised systems.", "published": "2017-07-28 20:38:15", "link": "http://arxiv.org/abs/1707.09410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine\n  Comprehension", "abstract": "Machine comprehension(MC) style question answering is a representative\nproblem in natural language processing. Previous methods rarely spend time on\nthe improvement of encoding layer, especially the embedding of syntactic\ninformation and name entity of the words, which are very crucial to the quality\nof encoding. Moreover, existing attention methods represent each query word as\na vector or use a single vector to represent the whole query sentence, neither\nof them can handle the proper weight of the key words in query sentence. In\nthis paper, we introduce a novel neural network architecture called Multi-layer\nEmbedding with Memory Network(MEMEN) for machine reading task. In the encoding\nlayer, we employ classic skip-gram model to the syntactic and semantic\ninformation of the words to train a new kind of embedding layer. We also\npropose a memory network of full-orientation matching of the query and passage\nto catch more pivotal information. Experiments show that our model has\ncompetitive results both from the perspectives of precision and efficiency in\nStanford Question Answering Dataset(SQuAD) among all published results and\nachieves the state-of-the-art results on TriviaQA dataset.", "published": "2017-07-28 03:41:18", "link": "http://arxiv.org/abs/1707.09098v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Putting Self-Supervised Token Embedding on the Tables", "abstract": "Information distribution by electronic messages is a privileged means of\ntransmission for many businesses and individuals, often under the form of\nplain-text tables. As their number grows, it becomes necessary to use an\nalgorithm to extract text and numbers instead of a human. Usual methods are\nfocused on regular expressions or on a strict structure in the data, but are\nnot efficient when we have many variations, fuzzy structure or implicit labels.\nIn this paper we introduce SC2T, a totally self-supervised model for\nconstructing vector representations of tokens in semi-structured messages by\nusing characters and context levels that address these issues. It can then be\nused for an unsupervised labeling of tokens, or be the basis for a\nsemi-supervised information extraction system.", "published": "2017-07-28 09:35:45", "link": "http://arxiv.org/abs/1708.04120v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Counterfactual Learning from Bandit Feedback under Deterministic\n  Logging: A Case Study in Statistical Machine Translation", "abstract": "The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback.", "published": "2017-07-28 06:32:47", "link": "http://arxiv.org/abs/1707.09118v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
