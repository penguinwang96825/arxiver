{"title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent\n  Deliberation", "abstract": "Uncertainty estimation is a significant issue for current large language\nmodels (LLMs) that are generally poorly calibrated and over-confident,\nespecially with reinforcement learning from human feedback (RLHF). Unlike\nhumans, whose decisions and confidences not only stem from intrinsic beliefs\nbut can also be adjusted through daily observations, existing calibration\nmethods for LLMs focus on estimating or eliciting individual confidence without\ntaking full advantage of the \"Collective Wisdom\": the interaction among\nmultiple LLMs that can collectively improve both accuracy and calibration. In\nthis work, we propose Collaborative Calibration, a post-hoc training-free\ncalibration strategy that leverages the collaborative and expressive\ncapabilities of multiple tool-augmented LLM agents in a simulated group\ndeliberation process. We demonstrate the effectiveness of Collaborative\nCalibration on generative QA tasks across various domains, showing its\npotential in harnessing the rationalization of collectively calibrated\nconfidence assessments and improving the reliability of model predictions.", "published": "2024-04-14 02:40:43", "link": "http://arxiv.org/abs/2404.09127v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in\n  Large Language Models", "abstract": "Recent studies suggest that self-reflective prompting can significantly\nenhance the reasoning capabilities of Large Language Models (LLMs). However,\nthe use of external feedback as a stop criterion raises doubts about the true\nextent of LLMs' ability to emulate human-like self-reflection. In this paper,\nwe set out to clarify these capabilities under a more stringent evaluation\nsetting in which we disallow any kind of external feedback. Our findings under\nthis setting show a split: while self-reflection enhances performance in\nTruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up\nanalyses to clarify the contributing factors in these patterns, and find that\nthe influence of self-reflection is impacted both by reliability of accuracy in\nmodels' initial responses, and by overall question difficulty: specifically,\nself-reflection shows the most benefit when models are less likely to be\ncorrect initially, and when overall question difficulty is higher. We also find\nthat self-reflection reduces tendency toward majority voting. Based on our\nfindings, we propose guidelines for decisions on when to implement\nself-reflection. We release the codebase for reproducing our experiments at\nhttps://github.com/yanhong-lbh/LLM-SelfReflection-Eval.", "published": "2024-04-14 02:47:32", "link": "http://arxiv.org/abs/2404.09129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions", "abstract": "Natural Language Processing (NLP) is witnessing a remarkable breakthrough\ndriven by the success of Large Language Models (LLMs). LLMs have gained\nsignificant attention across academia and industry for their versatile\napplications in text generation, question answering, and text summarization. As\nthe landscape of NLP evolves with an increasing number of domain-specific LLMs\nemploying diverse techniques and trained on various corpus, evaluating\nperformance of these models becomes paramount. To quantify the performance,\nit's crucial to have a comprehensive grasp of existing metrics. Among the\nevaluation, metrics which quantifying the performance of LLMs play a pivotal\nrole. This paper offers a comprehensive exploration of LLM evaluation from a\nmetrics perspective, providing insights into the selection and interpretation\nof metrics currently in use. Our main goal is to elucidate their mathematical\nformulations and statistical interpretations. We shed light on the application\nof these metrics using recent Biomedical LLMs. Additionally, we offer a\nsuccinct comparison of these metrics, aiding researchers in selecting\nappropriate metrics for diverse tasks. The overarching goal is to furnish\nresearchers with a pragmatic guide for effective LLM evaluation and metric\nselection, thereby advancing the understanding and application of these large\nlanguage models.", "published": "2024-04-14 03:54:00", "link": "http://arxiv.org/abs/2404.09135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking", "abstract": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism.", "published": "2024-04-14 07:19:27", "link": "http://arxiv.org/abs/2404.09170v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation\n  with Generative Models and Biomedical Knowledge to Enhance Inference\n  Robustness", "abstract": "Safe and reliable natural language inference is critical for extracting\ninsights from clinical trial reports but poses challenges due to biases in\nlarge pre-trained language models. This paper presents a novel data\naugmentation technique to improve model robustness for biomedical natural\nlanguage inference in clinical trials. By generating synthetic examples through\nsemantic perturbations and domain-specific vocabulary replacement and adding a\nnew task for numerical and quantitative reasoning, we introduce greater\ndiversity and reduce shortcut learning. Our approach, combined with multi-task\nlearning and the DeBERTa architecture, achieved significant performance gains\non the NLI4CT 2024 benchmark compared to the original language models. Ablation\nstudies validate the contribution of each augmentation method in improving\nrobustness. Our best-performing model ranked 12th in terms of faithfulness and\n8th in terms of consistency, respectively, out of the 32 participants.", "published": "2024-04-14 10:02:47", "link": "http://arxiv.org/abs/2404.09206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compass: Large Multilingual Language Model for South-east Asia", "abstract": "Large language models have exhibited significant proficiency in languages\nendowed with extensive linguistic resources, such as English and Chinese.\nNevertheless, their effectiveness notably diminishes when applied to languages\ncharacterized by limited linguistic resources, particularly within the\nSoutheast Asian linguistic landscape, such as Indonesian. The scarcity of\nlinguistic resources for these languages presents challenges associated with\ninadequate training, restricted vocabulary coverage, and challenging evaluation\nprocesses. In response to these exigencies, we have introduced CompassLLM, a\nlarge multilingual model specifically tailored for Southeast Asian languages,\nwith the primary aim of supporting the developmental requirements of Shopee.\nOur methodology encompasses several key strategies. To progressively enhance\nmultilingual proficiencies, we implemented a multi-stage pre-training strategy\nintegrated with curriculum learning, gradually intensifying the focus on\nlow-resource languages. Concurrently, to better accommodate low-resource human\ninstructions, we curated and generated a repository of high-quality\nmultilingual human instructions, culminating the CompassLLM-SFT model through\nsupervised instruction fine-tuning. Finally, to reinforce the model's alignment\nwith human preference behaviors, we have embraced the principle of Direct\nPreference Optimization (DPO) to obtain CompassLLM-DPO model. Preliminary\nevaluation of the CompassLLM model yields promising results, with our model\nsurpassing benchmark models like Vicuna-7b-v1.5, Sealion, Falcon and SeaLLM,\nacross diverse evaluation tasks, as verified through both automated and\nhuman-driven assessments. Notably, our model exhibits its superior performance\nin South-east Asia languages, such as Indonesian language.", "published": "2024-04-14 11:48:33", "link": "http://arxiv.org/abs/2404.09220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Data Knowledge Graph Construction for LLM-enabled Educational\n  Question-Answering System: A Case Study at HCMUT", "abstract": "In today's rapidly evolving landscape of Artificial Intelligence, large\nlanguage models (LLMs) have emerged as a vibrant research topic. LLMs find\napplications in various fields and contribute significantly. Despite their\npowerful language capabilities, similar to pre-trained language models (PLMs),\nLLMs still face challenges in remembering events, incorporating new\ninformation, and addressing domain-specific issues or hallucinations. To\novercome these limitations, researchers have proposed Retrieval-Augmented\nGeneration (RAG) techniques, some others have proposed the integration of LLMs\nwith Knowledge Graphs (KGs) to provide factual context, thereby improving\nperformance and delivering more accurate feedback to user queries.\n  Education plays a crucial role in human development and progress. With the\ntechnology transformation, traditional education is being replaced by digital\nor blended education. Therefore, educational data in the digital environment is\nincreasing day by day. Data in higher education institutions are diverse,\ncomprising various sources such as unstructured/structured text, relational\ndatabases, web/app-based API access, etc. Constructing a Knowledge Graph from\nthese cross-data sources is not a simple task. This article proposes a method\nfor automatically constructing a Knowledge Graph from multiple data sources and\ndiscusses some initial applications (experimental trials) of KG in conjunction\nwith LLMs for question-answering tasks.", "published": "2024-04-14 16:34:31", "link": "http://arxiv.org/abs/2404.09296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora", "abstract": "Media Storms, dramatic outbursts of attention to a story, are central\ncomponents of media dynamics and the attention landscape. Despite their\nsignificance, there has been little systematic and empirical research on this\nconcept due to issues of measurement and operationalization. We introduce an\niterative human-in-the-loop method to identify media storms in a large-scale\ncorpus of news articles. The text is first transformed into signals of\ndispersion based on several textual characteristics. In each iteration, we\napply unsupervised anomaly detection to these signals; each anomaly is then\nvalidated by an expert to confirm the presence of a storm, and those results\nare then used to tune the anomaly detection in the next iteration. We\ndemonstrate the applicability of this method in two scenarios: first,\nsupplementing an initial list of media storms within a specific time frame; and\nsecond, detecting media storms in new time periods. We make available a media\nstorm dataset compiled using both scenarios. Both the method and dataset offer\nthe basis for comprehensive empirical research into the concept of media\nstorms, including characterizing them and predicting their outbursts and\ndurations, in mainstream media or social media platforms.", "published": "2024-04-14 16:47:38", "link": "http://arxiv.org/abs/2404.09299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are as persuasive as humans, but how? About the\n  cognitive effort and moral-emotional language of LLM arguments", "abstract": "Large Language Models (LLMs) are already as persuasive as humans. However, we\nknow very little about how they do it. This paper investigates the persuasion\nstrategies of LLMs, comparing them with human-generated arguments. Using a\ndataset of 1,251 participants in an experiment, we analyze the persuasion\nstrategies of LLM-generated and human-generated arguments using measures of\ncognitive effort (lexical and grammatical complexity) and moral-emotional\nlanguage (sentiment and moral analysis). The study reveals that LLMs produce\narguments that require higher cognitive effort, exhibiting more complex\ngrammatical and lexical structures than human counterparts. Additionally, LLMs\ndemonstrate a significant propensity to engage more deeply with moral language,\nutilizing both positive and negative moral foundations more frequently than\nhumans. In contrast with previous research, no significant difference was found\nin the emotional content produced by LLMs and humans. These findings contribute\nto the discourse on AI and persuasion, highlighting the dual potential of LLMs\nto both enhance and undermine informational integrity through communication\nstrategies for digital persuasion.", "published": "2024-04-14 19:01:20", "link": "http://arxiv.org/abs/2404.09329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entropy Guided Extrapolative Decoding to Improve Factuality in Large\n  Language Models", "abstract": "Large language models (LLMs) exhibit impressive natural language capabilities\nbut suffer from hallucination -- generating content ungrounded in the realities\nof training data. Recent work has focused on decoding techniques to improve\nfactuality during inference by leveraging LLMs' hierarchical representation of\nfactual knowledge, manipulating the predicted distributions at inference time.\nCurrent state-of-the-art approaches refine decoding by contrasting early-exit\ndistributions from a lower layer with the final layer to exploit information\nrelated to factuality within the model forward procedure. However, such methods\noften assume the final layer is the most reliable and the lower layer selection\nprocess depends on it. In this work, we first propose extrapolation of critical\ntoken probabilities beyond the last layer for more accurate contrasting. We\nadditionally employ layer-wise entropy-guided lower layer selection, decoupling\nthe selection process from the final layer. Experiments demonstrate strong\nperformance - surpassing state-of-the-art on multiple different datasets by\nlarge margins. Analyses show different kinds of prompts respond to different\nselection strategies.", "published": "2024-04-14 19:45:35", "link": "http://arxiv.org/abs/2404.09338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Data Partitioning Strategy on Model Generalizability: A\n  Case Study of Morphological Segmentation", "abstract": "Recent work to enhance data partitioning strategies for more realistic model\nevaluation face challenges in providing a clear optimal choice. This study\naddresses these challenges, focusing on morphological segmentation and\nsynthesizing limitations related to language diversity, adoption of multiple\ndatasets and splits, and detailed model comparisons. Our study leverages data\nfrom 19 languages, including ten indigenous or endangered languages across 10\nlanguage families with diverse morphological systems (polysynthetic, fusional,\nand agglutinative) and different degrees of data availability. We conduct\nlarge-scale experimentation with varying sized combinations of training and\nevaluation sets as well as new test data. Our results show that, when faced\nwith new test data: (1) models trained from random splits are able to achieve\nhigher numerical scores; (2) model rankings derived from random splits tend to\ngeneralize more consistently.", "published": "2024-04-14 22:22:58", "link": "http://arxiv.org/abs/2404.09371v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Low-Resource Named Entity Recognition with Cross-Lingual,\n  Character-Level Neural Conditional Random Fields", "abstract": "Low-resource named entity recognition is still an open problem in NLP. Most\nstate-of-the-art systems require tens of thousands of annotated sentences in\norder to obtain high performance. However, for most of the world's languages,\nit is unfeasible to obtain such annotation. In this paper, we present a\ntransfer learning scheme, whereby we train character-level neural CRFs to\npredict named entities for both high-resource languages and low resource\nlanguages jointly. Learning character representations for multiple related\nlanguages allows transfer among the languages, improving F1 by up to 9.8 points\nover a loglinear CRF baseline.", "published": "2024-04-14 23:44:49", "link": "http://arxiv.org/abs/2404.09383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subtle Signs of Scribal Intent in the Voynich Manuscript", "abstract": "This study explores the cryptic Voynich Manuscript, by looking for subtle\nsigns of scribal intent hidden in overlooked features of the \"Voynichese\"\nscript. The findings indicate that distributions of tokens within paragraphs\nvary significantly based on positions defined not only by elements intrinsic to\nthe script such as paragraph and line boundaries but also by extrinsic\nelements, namely the hand-drawn illustrations of plants.", "published": "2024-04-14 14:54:01", "link": "http://arxiv.org/abs/2404.13069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToNER: Type-oriented Named Entity Recognition with Generative Language\n  Model", "abstract": "In recent years, the fine-tuned generative models have been proven more\npowerful than the previous tagging-based or span-based models on named entity\nrecognition (NER) task. It has also been found that the information related to\nentities, such as entity types, can prompt a model to achieve NER better.\nHowever, it is not easy to determine the entity types indeed existing in the\ngiven sentence in advance, and inputting too many potential entity types would\ndistract the model inevitably. To exploit entity types' merit on promoting NER\ntask, in this paper we propose a novel NER framework, namely ToNER based on a\ngenerative model. In ToNER, a type matching model is proposed at first to\nidentify the entity types most likely to appear in the sentence. Then, we\nappend a multiple binary classification task to fine-tune the generative\nmodel's encoder, so as to generate the refined representation of the input\nsentence. Moreover, we add an auxiliary task for the model to discover the\nentity types which further fine-tunes the model to output more accurate\nresults. Our extensive experiments on some NER benchmarks verify the\neffectiveness of our proposed strategies in ToNER that are oriented towards\nentity types' exploitation.", "published": "2024-04-14 05:13:37", "link": "http://arxiv.org/abs/2404.09145v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GeMQuAD : Generating Multilingual Question Answering Datasets from Large\n  Language Models using Few Shot Learning", "abstract": "The emergence of Large Language Models (LLMs) with capabilities like\nIn-Context Learning (ICL) has ushered in new possibilities for data generation\nacross various domains while minimizing the need for extensive data collection\nand modeling techniques. Researchers have explored ways to use this generated\nsynthetic data to optimize smaller student models for reduced deployment costs\nand lower latency in downstream tasks. However, ICL-generated data often\nsuffers from low quality as the task specificity is limited with few examples\nused in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning\napproach, extending the WeakDAP framework, applied to a dataset generated\nthrough ICL with just one example in the target language using AlexaTM 20B\nSeq2Seq LLM. Through our approach, we iteratively identify high-quality data to\nenhance model performance, especially for low-resource multilingual setting in\nthe context of Extractive Question Answering task. Our framework outperforms\nthe machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points\nfor Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it\nsurpasses the performance of model trained on an English-only dataset by\n5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the\nsame dataset. Notably, our approach uses a pre-trained LLM for generation with\nno fine-tuning (FT), utilizing just a single annotated example in ICL to\ngenerate data, providing a cost-effective development process.", "published": "2024-04-14 06:55:42", "link": "http://arxiv.org/abs/2404.09163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JaFIn: Japanese Financial Instruction Dataset", "abstract": "We construct an instruction dataset for the large language model (LLM) in the\nJapanese finance domain. Domain adaptation of language models, including LLMs,\nis receiving more attention as language models become more popular. This study\ndemonstrates the effectiveness of domain adaptation through instruction tuning.\nTo achieve this, we propose an instruction tuning data in Japanese called\nJaFIn, the Japanese Financial Instruction Dataset. JaFIn is manually\nconstructed based on multiple data sources, including Japanese government\nwebsites, which provide extensive financial knowledge. We then utilize JaFIn to\napply instruction tuning for several LLMs, demonstrating that our models\nspecialized in finance have better domain adaptability than the original\nmodels. The financial-specialized LLMs created were evaluated using a\nquantitative Japanese financial benchmark and qualitative response comparisons,\nshowing improved performance over the originals.", "published": "2024-04-14 14:01:53", "link": "http://arxiv.org/abs/2404.09260v2", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Self-Selected Attention Span for Accelerating Large Language Model\n  Inference", "abstract": "Large language models (LLMs) can solve challenging tasks. However, their\ninference computation on modern GPUs is highly inefficient due to the\nincreasing number of tokens they must attend to as they generate new ones. To\naddress this inefficiency, we capitalize on LLMs' problem-solving capabilities\nto optimize their own inference-time efficiency. We demonstrate with two\nspecific tasks: (a) evaluating complex arithmetic expressions and (b)\nsummarizing news articles. For both tasks, we create custom datasets to\nfine-tune an LLM. The goal of fine-tuning is twofold: first, to make the LLM\nlearn to solve the evaluation or summarization task, and second, to train it to\nidentify the minimal attention spans required for each step of the task. As a\nresult, the fine-tuned model is able to convert these self-identified minimal\nattention spans into sparse attention masks on-the-fly during inference. We\ndevelop a custom CUDA kernel to take advantage of the reduced context to attend\nto. We demonstrate that using this custom CUDA kernel improves the throughput\nof LLM inference by 28%. Our work presents an end-to-end demonstration showing\nthat training LLMs to self-select their attention spans speeds up\nautoregressive inference in solving real-world tasks.", "published": "2024-04-14 19:36:04", "link": "http://arxiv.org/abs/2404.09336v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding the Role of Temperature in Diverse Question Generation by\n  GPT-4", "abstract": "We conduct a preliminary study of the effect of GPT's temperature parameter\non the diversity of GPT4-generated questions. We find that using higher\ntemperature values leads to significantly higher diversity, with different\ntemperatures exposing different types of similarity between generated sets of\nquestions. We also demonstrate that diverse question generation is especially\ndifficult for questions targeting lower levels of Bloom's Taxonomy.", "published": "2024-04-14 21:38:50", "link": "http://arxiv.org/abs/2404.09366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deceptive Patterns of Intelligent and Interactive Writing Assistants", "abstract": "Large Language Models have become an integral part of new intelligent and\ninteractive writing assistants. Many are offered commercially with a\nchatbot-like UI, such as ChatGPT, and provide little information about their\ninner workings. This makes this new type of widespread system a potential\ntarget for deceptive design patterns. For example, such assistants might\nexploit hidden costs by providing guidance up until a certain point before\nasking for a fee to see the rest. As another example, they might sneak unwanted\ncontent/edits into longer generated or revised text pieces (e.g. to influence\nthe expressed opinion). With these and other examples, we conceptually transfer\nseveral deceptive patterns from the literature to the new context of AI writing\nassistants. Our goal is to raise awareness and encourage future research into\nhow the UI and interaction design of such systems can impact people and their\nwriting.", "published": "2024-04-14 23:05:10", "link": "http://arxiv.org/abs/2404.09375v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Evidence from counterfactual tasks supports emergent analogical\n  reasoning in large language models", "abstract": "We recently reported evidence that large language models are capable of\nsolving a wide range of text-based analogy problems in a zero-shot manner,\nindicating the presence of an emergent capacity for analogical reasoning. Two\nrecent commentaries have challenged these results, citing evidence from\nso-called `counterfactual' tasks in which the standard sequence of the alphabet\nis arbitrarily permuted so as to decrease similarity with materials that may\nhave been present in the language model's training data. Here, we reply to\nthese critiques, clarifying some misunderstandings about the test materials\nused in our original work, and presenting evidence that language models are\nalso capable of generalizing to these new counterfactual task variants.", "published": "2024-04-14 21:51:02", "link": "http://arxiv.org/abs/2404.13070v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Provable Interactive Learning with Hindsight Instruction Feedback", "abstract": "We study interactive learning in a setting where the agent has to generate a\nresponse (e.g., an action or trajectory) given a context and an instruction. In\ncontrast, to typical approaches that train the system using reward or expert\nsupervision on response, we study learning with hindsight instruction where a\nteacher provides an instruction that is most suitable for the agent's generated\nresponse. This hindsight labeling of instruction is often easier to provide\nthan providing expert supervision of the optimal response which may require\nexpert knowledge or can be impractical to elicit. We initiate the theoretical\nanalysis of interactive learning with hindsight labeling. We first provide a\nlower bound showing that in general, the regret of any algorithm must scale\nwith the size of the agent's response space. We then study a specialized\nsetting where the underlying instruction-response distribution can be\ndecomposed as a low-rank matrix. We introduce an algorithm called LORIL for\nthis setting and show that its regret scales as $\\sqrt{T}$ where $T$ is the\nnumber of rounds and depends on the intrinsic rank but does not depend on the\nsize of the agent's response space. We provide experiments in two domains\nshowing that LORIL outperforms baselines even when the low-rank assumption is\nviolated.", "published": "2024-04-14 02:18:07", "link": "http://arxiv.org/abs/2404.09123v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries\n  for DeBERTa Report Analysis", "abstract": "This paper introduces novel methodologies for the Natural Language Inference\nfor Clinical Trials (NLI4CT) task. We present TLDR (T5-generated\nclinical-Language summaries for DeBERTa Report Analysis) which incorporates\nT5-model generated premise summaries for improved entailment and contradiction\nanalysis in clinical NLI tasks. This approach overcomes the challenges posed by\nsmall context windows and lengthy premises, leading to a substantial\nimprovement in Macro F1 scores: a 0.184 increase over truncated premises. Our\ncomprehensive experimental evaluation, including detailed error analysis and\nablations, confirms the superiority of TLDR in achieving consistency and\nfaithfulness in predictions against semantically altered inputs.", "published": "2024-04-14 04:14:30", "link": "http://arxiv.org/abs/2404.09136v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian\n  Language Representation", "abstract": "In the rapidly advancing field of AI and NLP, generative large language\nmodels (LLMs) stand at the forefront of innovation, showcasing unparalleled\nabilities in text understanding and generation. However, the limited\nrepresentation of low-resource languages like Ukrainian poses a notable\nchallenge, restricting the reach and relevance of this technology. Our paper\naddresses this by fine-tuning the open-source Gemma and Mistral LLMs with\nUkrainian datasets, aiming to improve their linguistic proficiency and\nbenchmarking them against other existing models capable of processing Ukrainian\nlanguage. This endeavor not only aims to mitigate language bias in technology\nbut also promotes inclusivity in the digital realm. Our transparent and\nreproducible approach encourages further NLP research and development.\nAdditionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID)\nto aid future efforts in language model fine-tuning. Our research not only\nadvances the field of NLP but also highlights the importance of linguistic\ndiversity in AI, which is crucial for cultural preservation, education, and\nexpanding AI's global utility. Ultimately, we advocate for a future where\ntechnology is inclusive, enabling AI to communicate effectively across all\nlanguages, especially those currently underrepresented.", "published": "2024-04-14 04:25:41", "link": "http://arxiv.org/abs/2404.09138v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds\n  for Tensor Decomposition Based Temporal Knowledge Graph Embedding", "abstract": "Recent studies have highlighted the effectiveness of tensor decomposition\nmethods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we\nfound that inherent heterogeneity among factor tensors in tensor decomposition\nsignificantly hinders the tensor fusion process and further limits the\nperformance of link prediction. To overcome this limitation, we introduce a\nnovel method that maps factor tensors onto a unified smooth Lie group manifold\nto make the distribution of factor tensors approximating homogeneous in tensor\ndecomposition. We provide the theoretical proof of our motivation that\nhomogeneous tensors are more effective than heterogeneous tensors in tensor\nfusion and approximating the target for tensor decomposition based TKGE\nmethods. The proposed method can be directly integrated into existing tensor\ndecomposition based TKGE methods without introducing extra parameters.\nExtensive experiments demonstrate the effectiveness of our method in mitigating\nthe heterogeneity and in enhancing the tensor decomposition based TKGE models.", "published": "2024-04-14 06:10:46", "link": "http://arxiv.org/abs/2404.09155v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TransformerFAM: Feedback attention is working memory", "abstract": "While Transformers have revolutionized deep learning, their quadratic\nattention complexity hinders their ability to process infinitely long inputs.\nWe propose Feedback Attention Memory (FAM), a novel Transformer architecture\nthat leverages a feedback loop to enable the network to attend to its own\nlatent representations. This design fosters the emergence of working memory\nwithin the Transformer, allowing it to process indefinitely long sequences.\nTransformerFAM requires no additional weights, enabling seamless integration\nwith pre-trained models. Our experiments show that TransformerFAM significantly\nimproves Transformer performance on long-context tasks across various model\nsizes (1B, 8B, and 24B). These results showcase the potential to empower Large\nLanguage Models (LLMs) to process sequences of unlimited length.", "published": "2024-04-14 07:43:45", "link": "http://arxiv.org/abs/2404.09173v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring and Improving Drafts in Blockwise Parallel Decoding", "abstract": "Despite the remarkable strides made by autoregressive language models, their\npotential is often hampered by the slow inference speeds inherent in sequential\ntoken generation. Blockwise parallel decoding (BPD) was proposed by Stern et\nal. as a method to improve inference speed of language models by simultaneously\npredicting multiple future tokens, termed block drafts, which are subsequently\nverified and conditionally accepted by the autoregressive model. This paper\ncontributes to the understanding and improvement of block drafts in two ways.\nFirst, we analyze the token distributions produced by multiple prediction\nheads. Secondly, we leverage this analysis to develop algorithms to improve BPD\ninference speed by refining the block drafts using n-gram and neural language\nmodels. Experiments demonstrate that refined block drafts yield a +5-21%\nincrease in block efficiency (i.e., the number of accepted tokens from the\nblock draft) across diverse datasets.", "published": "2024-04-14 11:49:38", "link": "http://arxiv.org/abs/2404.09221v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledgeable Agents by Offline Reinforcement Learning from Large\n  Language Model Rollouts", "abstract": "Reinforcement learning (RL) trains agents to accomplish complex tasks through\nenvironmental interaction data, but its capacity is also limited by the scope\nof the available data. To obtain a knowledgeable agent, a promising approach is\nto leverage the knowledge from large language models (LLMs). Despite previous\nstudies combining LLMs with RL, seamless integration of the two components\nremains challenging due to their semantic gap. This paper introduces a novel\nmethod, Knowledgeable Agents from Language Model Rollouts (KALM), which\nextracts knowledge from LLMs in the form of imaginary rollouts that can be\neasily learned by the agent through offline reinforcement learning methods. The\nprimary challenge of KALM lies in LLM grounding, as LLMs are inherently limited\nto textual data, whereas environmental data often comprise numerical vectors\nunseen to LLMs. To address this, KALM fine-tunes the LLM to perform various\ntasks based on environmental data, including bidirectional translation between\nnatural language descriptions of skills and their corresponding rollout data.\nThis grounding process enhances the LLM's comprehension of environmental\ndynamics, enabling it to generate diverse and meaningful imaginary rollouts\nthat reflect novel skills. Initial empirical evaluations on the CLEVR-Robot\nenvironment demonstrate that KALM enables agents to complete complex\nrephrasings of task goals and extend their capabilities to novel tasks\nrequiring unprecedented optimal behaviors. KALM achieves a success rate of 46%\nin executing tasks with unseen goals, substantially surpassing the 26% success\nrate achieved by baseline methods. Furthermore, KALM effectively enables the\nLLM to comprehend environmental dynamics, resulting in the generation of\nmeaningful imaginary rollouts that reflect novel skills and demonstrate the\nseamless integration of large language models and reinforcement learning.", "published": "2024-04-14 13:19:40", "link": "http://arxiv.org/abs/2404.09248v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Test Code Generation for Telecom Software Systems using Two-Stage\n  Generative Model", "abstract": "In recent years, the evolution of Telecom towards achieving intelligent,\nautonomous, and open networks has led to an increasingly complex Telecom\nSoftware system, supporting various heterogeneous deployment scenarios, with\nmulti-standard and multi-vendor support. As a result, it becomes a challenge\nfor large-scale Telecom software companies to develop and test software for all\ndeployment scenarios. To address these challenges, we propose a framework for\nAutomated Test Generation for large-scale Telecom Software systems. We begin by\ngenerating Test Case Input data for test scenarios observed using a time-series\nGenerative model trained on historical Telecom Network data during field\ntrials. Additionally, the time-series Generative model helps in preserving the\nprivacy of Telecom data. The generated time-series software performance data\nare then utilized with test descriptions written in natural language to\ngenerate Test Script using the Generative Large Language Model. Our\ncomprehensive experiments on public datasets and Telecom datasets obtained from\noperational Telecom Networks demonstrate that the framework can effectively\ngenerate comprehensive test case data input and useful test code.", "published": "2024-04-14 13:25:15", "link": "http://arxiv.org/abs/2404.09249v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "TrafficVLM: A Controllable Visual Language Model for Traffic Video\n  Captioning", "abstract": "Traffic video description and analysis have received much attention recently\ndue to the growing demand for efficient and reliable urban surveillance\nsystems. Most existing methods only focus on locating traffic event segments,\nwhich severely lack descriptive details related to the behaviour and context of\nall the subjects of interest in the events. In this paper, we present\nTrafficVLM, a novel multi-modal dense video captioning model for vehicle ego\ncamera view. TrafficVLM models traffic video events at different levels of\nanalysis, both spatially and temporally, and generates long fine-grained\ndescriptions for the vehicle and pedestrian at different phases of the event.\nWe also propose a conditional component for TrafficVLM to control the\ngeneration outputs and a multi-task fine-tuning paradigm to enhance\nTrafficVLM's learning capability. Experiments show that TrafficVLM performs\nwell on both vehicle and overhead camera views. Our solution achieved\noutstanding results in Track 2 of the AI City Challenge 2024, ranking us third\nin the challenge standings. Our code is publicly available at\nhttps://github.com/quangminhdinh/TrafficVLM.", "published": "2024-04-14 14:51:44", "link": "http://arxiv.org/abs/2404.09275v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Practical Tool Usage for Continually Learning LLMs", "abstract": "Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.", "published": "2024-04-14 19:45:47", "link": "http://arxiv.org/abs/2404.09339v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLeMpower: Understanding Disparities in the Control and Access of Large\n  Language Models", "abstract": "Large Language Models (LLMs) are a powerful technology that augment human\nskill to create new opportunities, akin to the development of steam engines and\nthe internet. However, LLMs come with a high cost. They require significant\ncomputing resources and energy to train and serve. Inequity in their control\nand access has led to concentration of ownership and power to a small\ncollection of corporations. In our study, we collect training and inference\nrequirements for various LLMs. We then analyze the economic strengths of\nnations and organizations in the context of developing and serving these\nmodels. Additionally, we also look at whether individuals around the world can\naccess and use this emerging technology. We compare and contrast these groups\nto show that these technologies are monopolized by a surprisingly few entities.\nWe conclude with a qualitative study on the ethical implications of our\nfindings and discuss future directions towards equity in LLM access.", "published": "2024-04-14 20:49:53", "link": "http://arxiv.org/abs/2404.09356v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET", "K.4.0; K.7.4"], "primary_category": "cs.CY"}
{"title": "Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software\n  Verification and Falsification Approaches", "abstract": "Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners\nhave been \"playing\" with prompts (e.g., In-Context Learning) to see how to make\nthe most of pre-trained Language Models. By homogeneously dissecting more than\na hundred articles, we investigate how software testing and verification\nresearch communities have leveraged LLMs capabilities. First, we validate that\ndownstream tasks are adequate to convey a nontrivial modular blueprint of\nprompt-based proposals in scope. Moreover, we name and classify the concrete\ndownstream tasks we recover in both validation research papers and solution\nproposals. In order to perform classification, mapping, and analysis, we also\ndevelop a novel downstream-task taxonomy. The main taxonomy requirement is to\nhighlight commonalities while exhibiting variation points of task types that\nenable pinpointing emerging patterns in a varied spectrum of Software\nEngineering problems that encompasses testing, fuzzing, fault localization,\nvulnerability detection, static analysis, and program verification approaches.\nAvenues for future research are also discussed based on conceptual clusters\ninduced by the taxonomy.", "published": "2024-04-14 23:45:23", "link": "http://arxiv.org/abs/2404.09384v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "F.3.1; D.2.4; D.2.5; I.2.7"], "primary_category": "cs.SE"}
{"title": "Evaluation and Improvement of Fault Detection for Large Language Models", "abstract": "Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%.", "published": "2024-04-14 07:06:12", "link": "http://arxiv.org/abs/2404.14419v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Text-to-Song: Towards Controllable Music Generation Incorporating Vocals\n  and Accompaniment", "abstract": "A song is a combination of singing voice and accompaniment. However, existing\nworks focus on singing voice synthesis and music generation independently.\nLittle attention was paid to explore song synthesis. In this work, we propose a\nnovel task called text-to-song synthesis which incorporating both vocals and\naccompaniments generation. We develop Melodist, a two-stage text-to-song method\nthat consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A)\nsynthesis. Melodist leverages tri-tower contrastive pretraining to learn more\neffective text representation for controllable V2A synthesis. A Chinese song\ndataset mined from a music website is built up to alleviate data scarcity for\nour research. The evaluation results on our dataset demonstrate that Melodist\ncan synthesize songs with comparable quality and style consistency. Audio\nsamples can be found in https://text2songMelodist.github.io/Sample/.", "published": "2024-04-14 18:00:05", "link": "http://arxiv.org/abs/2404.09313v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "An Experimental Comparison Of Multi-view Self-supervised Methods For\n  Music Tagging", "abstract": "Self-supervised learning has emerged as a powerful way to pre-train\ngeneralizable machine learning models on large amounts of unlabeled data. It is\nparticularly compelling in the music domain, where obtaining labeled data is\ntime-consuming, error-prone, and ambiguous. During the self-supervised process,\nmodels are trained on pretext tasks, with the primary objective of acquiring\nrobust and informative features that can later be fine-tuned for specific\ndownstream tasks. The choice of the pretext task is critical as it guides the\nmodel to shape the feature space with meaningful constraints for information\nencoding. In the context of music, most works have relied on contrastive\nlearning or masking techniques. In this study, we expand the scope of pretext\ntasks applied to music by investigating and comparing the performance of new\nself-supervised methods for music tagging. We open-source a simple ResNet model\ntrained on a diverse catalog of millions of tracks. Our results demonstrate\nthat, although most of these pre-training methods result in similar downstream\nresults, contrastive learning consistently results in better downstream\nperformance compared to other self-supervised pre-training methods. This holds\ntrue in a limited-data downstream context.", "published": "2024-04-14 07:56:08", "link": "http://arxiv.org/abs/2404.09177v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for\n  Parallelized TTS Frontend Modeling", "abstract": "Over the past decade, a series of unflagging efforts have been dedicated to\ndeveloping highly expressive and controllable text-to-speech (TTS) systems. In\ngeneral, the holistic TTS comprises two interconnected components: the frontend\nmodule and the backend module. The frontend excels in capturing linguistic\nrepresentations from the raw text input, while the backend module converts\nlinguistic cues to speech. The research community has shown growing interest in\nthe study of the frontend component, recognizing its pivotal role in\ntext-to-speech systems, including Text Normalization (TN), Prosody Boundary\nPrediction (PBP), and Polyphone Disambiguation (PD). Nonetheless, the\nlimitations posed by insufficient annotated textual data and the reliance on\nhomogeneous text signals significantly undermine the effectiveness of its\nsupervised learning. To evade this obstacle, a novel two-stage TTS frontend\nprediction pipeline, named TAP-FM, is proposed in this paper. Specifically,\nduring the first learning phase, we present a Multi-scale Contrastive\nText-audio Pre-training protocol (MC-TAP), which hammers at acquiring richer\ninsights via multi-granularity contrastive pre-training in an unsupervised\nmanner. Instead of mining homogeneous features in prior pre-training\napproaches, our framework demonstrates the ability to delve deep into both\nglobal and local text-audio semantic and acoustic representations. Furthermore,\na parallelized TTS frontend model is delicately devised to execute TN, PD, and\nPBP prediction tasks, respectively in the second stage. Finally, extensive\nexperiments illustrate the superiority of our proposed method, achieving\nstate-of-the-art performance.", "published": "2024-04-14 08:56:19", "link": "http://arxiv.org/abs/2404.09192v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Face-voice Association in Multilingual Environments (FAME) Challenge\n  2024 Evaluation Plan", "abstract": "The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, the audio-visual systems are one\nof the widely used multimodal systems. In the recent years, associating face\nand voice of a person has gained attention due to presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) Challenge 2024 focuses on exploring face-voice association\nunder a unique condition of multilingual scenario. This condition is inspired\nfrom the fact that half of the world's population is bilingual and most often\npeople communicate under multilingual scenario. The challenge uses a dataset\nnamely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baselines and task details for the FAME Challenge.", "published": "2024-04-14 19:51:32", "link": "http://arxiv.org/abs/2404.09342v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
