{"title": "Speaker Diarization With Lexical Information", "abstract": "This work presents a novel approach to leverage lexical information for\nspeaker diarization. We introduce a speaker diarization system that can\ndirectly integrate lexical as well as acoustic information into a speaker\nclustering process. Thus, we propose an adjacency matrix integration technique\nto integrate word level speaker turn probabilities with speaker embeddings in a\ncomprehensive way. Our proposed method works without any reference transcript.\nWords, and word boundary information are provided by an ASR system. We show\nthat our proposed method improves a baseline speaker diarization system solely\nbased on speaker embeddings, achieving a meaningful improvement on the CALLHOME\nAmerican English Speech dataset.", "published": "2018-11-27 00:58:39", "link": "http://arxiv.org/abs/1811.10761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verb Argument Structure Alternations in Word and Sentence Embeddings", "abstract": "Verbs occur in different syntactic environments, or frames. We investigate\nwhether artificial neural networks encode grammatical distinctions necessary\nfor inferring the idiosyncratic frame-selectional properties of verbs. We\nintroduce five datasets, collectively called FAVA, containing in aggregate\nnearly 10k sentences labeled for grammatical acceptability, illustrating\ndifferent verbal argument structure alternations. We then test whether models\ncan distinguish acceptable English verb-frame combinations from unacceptable\nones using a sentence embedding alone. For converging evidence, we further\nconstruct LaVA, a corresponding word-level dataset, and investigate whether the\nsame syntactic features can be extracted from word embeddings. Our models\nperform reliable classifications for some verbal alternations but not others,\nsuggesting that while these representations do encode fine-grained lexical\ninformation, it is incomplete or can be hard to extract. Further, differences\nbetween the word- and sentence-level models show that some information present\nin word embeddings is not passed on to the down-stream sentence embeddings.", "published": "2018-11-27 02:07:36", "link": "http://arxiv.org/abs/1811.10773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Representation Learning of Cross-lingual Words and Entities via\n  Attentive Distant Supervision", "abstract": "Joint representation learning of words and entities benefits many NLP tasks,\nbut has not been well explored in cross-lingual settings. In this paper, we\npropose a novel method for joint representation learning of cross-lingual words\nand entities. It captures mutually complementary knowledge, and enables\ncross-lingual inferences among knowledge bases and texts. Our method does not\nrequire parallel corpora, and automatically generates comparable data via\ndistant supervision using multi-lingual knowledge bases. We utilize two types\nof regularizers to align cross-lingual words and entities, and design knowledge\nattention and cross-lingual attention to further reduce noises. We conducted a\nseries of experiments on three tasks: word translation, entity relatedness, and\ncross-lingual entity linking. The results, both qualitatively and\nquantitatively, demonstrate the significance of our method.", "published": "2018-11-27 02:24:37", "link": "http://arxiv.org/abs/1811.10776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fact Extraction and VERification (FEVER) Shared Task", "abstract": "We present the results of the first Fact Extraction and VERification (FEVER)\nShared Task. The task challenged participants to classify whether human-written\nfactoid claims could be Supported or Refuted using evidence retrieved from\nWikipedia. We received entries from 23 competing teams, 19 of which scored\nhigher than the previously published baseline. The best performing system\nachieved a FEVER score of 64.21%. In this paper, we present the results of the\nshared task and a summary of the systems, highlighting commonalities and\ninnovations among participating systems.", "published": "2018-11-27 13:32:26", "link": "http://arxiv.org/abs/1811.10971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to detect dysarthria from raw speech", "abstract": "Speech classifiers of paralinguistic traits traditionally learn from diverse\nhand-crafted low-level features, by selecting the relevant information for the\ntask at hand. We explore an alternative to this selection, by learning jointly\nthe classifier, and the feature extraction. Recent work on speech recognition\nhas shown improved performance over speech features by learning from the\nwaveform. We extend this approach to paralinguistic classification and propose\na neural network that can learn a filterbank, a normalization factor and a\ncompression power from the raw speech, jointly with the rest of the\narchitecture. We apply this model to dysarthria detection from sentence-level\naudio recordings. Starting from a strong attention-based baseline on which\nmel-filterbanks outperform standard low-level descriptors, we show that\nlearning the filters or the normalization and compression improves over fixed\nfeatures by 10% absolute accuracy. We also observe a gain over OpenSmile\nfeatures by learning jointly the feature extraction, the normalization, and the\ncompression factor with the architecture. This constitutes a first attempt at\nlearning jointly all these operations from raw audio for a speech\nclassification task.", "published": "2018-11-27 16:51:39", "link": "http://arxiv.org/abs/1811.11101v2", "categories": ["cs.CL", "68T10"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Approaches to Reference Resolution in Dialogue Systems", "abstract": "In the slot-filling paradigm, where a user can refer back to slots in the\ncontext during the conversation, the goal of the contextual understanding\nsystem is to resolve the referring expressions to the appropriate slots in the\ncontext. In this paper, we build on the context carryover\nsystem~\\citep{Naik2018ContextualSC}, which provides a scalable multi-domain\nframework for resolving references. However, scaling this approach across\nlanguages is not a trivial task, due to the large demand on acquisition of\nannotated data in the target language. Our main focus is on cross-lingual\nmethods for reference resolution as a way to alleviate the need for annotated\ndata in the target language. In the cross-lingual setup, we assume there is\naccess to annotated resources as well as a well trained model in the source\nlanguage and little to no annotated data in the target language. In this paper,\nwe explore three different approaches for cross-lingual transfer \\textemdash~\\\ndelexicalization as data augmentation, multilingual embeddings and machine\ntranslation. We compare these approaches both on a low resource setting as well\nas a large resource setting. Our experiments show that multilingual embeddings\nand delexicalization via data augmentation have a significant impact in the low\nresource setting, but the gains diminish as the amount of available data in the\ntarget language increases. Furthermore, when combined with machine translation\nwe can get performance very close to actual live data in the target language,\nwith only 25\\% of the data projected into the target language.", "published": "2018-11-27 18:52:58", "link": "http://arxiv.org/abs/1811.11161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Recognition to Cognition: Visual Commonsense Reasoning", "abstract": "Visual understanding goes well beyond object recognition. With one glance at\nan image, we can effortlessly imagine the world beyond the pixels: for\ninstance, we can infer people's actions, goals, and mental states. While this\ntask is easy for humans, it is tremendously difficult for today's vision\nsystems, requiring higher-order cognition and commonsense reasoning about the\nworld. We formalize this task as Visual Commonsense Reasoning. Given a\nchallenging question about an image, a machine must answer correctly and then\nprovide a rationale justifying its answer.\n  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA\nproblems derived from 110k movie scenes. The key recipe for generating\nnon-trivial and high-quality problems at scale is Adversarial Matching, a new\napproach to transform rich annotations into multiple choice questions with\nminimal bias. Experimental results show that while humans find VCR easy (over\n90% accuracy), state-of-the-art vision models struggle (~45%).\n  To move towards cognition-level understanding, we present a new reasoning\nengine, Recognition to Cognition Networks (R2C), that models the necessary\nlayered inferences for grounding, contextualization, and reasoning. R2C helps\nnarrow the gap between humans and machines (~65%); still, the challenge is far\nfrom solved, and we provide analysis that suggests avenues for future work.", "published": "2018-11-27 06:22:26", "link": "http://arxiv.org/abs/1811.10830v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SOC: hunting the underground inside story of the ethereum Social-network\n  Opinion and Comment", "abstract": "The cryptocurrency is attracting more and more attention because of the\nblockchain technology. Ethereum is gaining a significant popularity in\nblockchain community, mainly due to the fact that it is designed in a way that\nenables developers to write smart contracts and decentralized applications\n(Dapps). There are many kinds of cryptocurrency information on the social\nnetwork. The risks and fraud problems behind it have pushed many countries\nincluding the United States, South Korea, and China to make warnings and set up\ncorresponding regulations. However, the security of Ethereum smart contracts\nhas not gained much attention. Through the Deep Learning approach, we propose a\nmethod of sentiment analysis for Ethereum's community comments. In this\nresearch, we first collected the users' cryptocurrency comments from the social\nnetwork and then fed to our LSTM + CNN model for training. Then we made\nprediction through sentiment analysis. With our research result, we have\ndemonstrated that both the precision and the recall of sentiment analysis can\nachieve 0.80+. More importantly, we deploy our sentiment analysis1 on\nRatingToken and Coin Master (mobile application of Cheetah Mobile Blockchain\nSecurity Center23). We can effectively provide detail information to resolve\nthe risks of being fake and fraud problems.", "published": "2018-11-27 17:54:12", "link": "http://arxiv.org/abs/1811.11136v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Refined WaveNet Vocoder for Variational Autoencoder Based Voice\n  Conversion", "abstract": "This paper presents a refinement framework of WaveNet vocoders for\nvariational autoencoder (VAE) based voice conversion (VC), which reduces the\nquality distortion caused by the mismatch between the training data and testing\ndata. Conventional WaveNet vocoders are trained with natural acoustic features\nbut conditioned on the converted features in the conversion stage for VC, and\nsuch a mismatch often causes significant quality and similarity degradation. In\nthis work, we take advantage of the particular structure of VAEs to refine\nWaveNet vocoders with the self-reconstructed features generated by VAE, which\nare of similar characteristics with the converted features while having the\nsame temporal structure with the target natural features. We analyze these\nfeatures and show that the self-reconstructed features are similar to the\nconverted features. Objective and subjective experimental results demonstrate\nthe effectiveness of our proposed framework.", "published": "2018-11-27 16:26:17", "link": "http://arxiv.org/abs/1811.11078v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-scale Speaker Retrieval on Random Speaker Variability Subspace", "abstract": "This paper describes a fast speaker search system to retrieve segments of the\nsame voice identity in the large-scale data. A recent study shows that Locality\nSensitive Hashing (LSH) enables quick retrieval of a relevant voice in the\nlarge-scale data in conjunction with i-vector while maintaining accuracy. In\nthis paper, we proposed Random Speaker-variability Subspace (RSS) projection to\nmap a data into LSH based hash tables. We hypothesized that rather than\nprojecting on completely random subspace without considering data, projecting\non randomly generated speaker variability space would give more chance to put\nthe same speaker representation into the same hash bins, so we can use less\nnumber of hash tables. Multiple RSS can be generated by randomly selecting a\nsubset of speakers from a large speaker cohort. From the experimental result,\nthe proposed approach shows 100 times and 7 times faster than the linear search\nand LSH, respectively", "published": "2018-11-27 04:52:14", "link": "http://arxiv.org/abs/1811.10812v2", "categories": ["eess.AS", "cs.IR"], "primary_category": "eess.AS"}
{"title": "Noise-tolerant Audio-visual Online Person Verification using an\n  Attention-based Neural Network Fusion", "abstract": "In this paper, we present a multi-modal online person verification system\nusing both speech and visual signals. Inspired by neuroscientific findings on\nthe association of voice and face, we propose an attention-based end-to-end\nneural network that learns multi-sensory associations for the task of person\nverification. The attention mechanism in our proposed network learns to\nconditionally select a salient modality between speech and facial\nrepresentations that provides a balance between complementary inputs. By virtue\nof this capability, the network is robust to missing or corrupted data from\neither modality. In the VoxCeleb2 dataset, we show that our method performs\nfavorably against competing multi-modal methods. Even for extreme cases of\nlarge corruption or an entirely missing modality, our method demonstrates\nrobustness over other unimodal methods.", "published": "2018-11-27 04:58:10", "link": "http://arxiv.org/abs/1811.10813v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Examples of usage of nearfield acoustic holography methods for far field\n  estimations: Part 1. CW signals", "abstract": "The paper is devoted to the usage of nearfield acoustic holography methods\nfor estimating far field of the object. An experiment was carried out in\nanechoic chamber. First, acoustic filed was recorded in a plane that was close\nto source. This signals records were used to reconstruct the far field by\ncomputation routines. Second, the signal in the far field is measured and the\nresults are compared. Several methods are tested and research on possible\nreduction of the microphone array size is carried out. The most significant\nreduction of the measurement facility complexity is usage a linear array in\nstead of the planar array that is made possible due to introduced computation\nroutines", "published": "2018-11-27 20:51:58", "link": "http://arxiv.org/abs/1812.03826v1", "categories": ["eess.AS", "cs.SD", "physics.app-ph", "78.02"], "primary_category": "eess.AS"}
{"title": "Improved Speech Enhancement with the Wave-U-Net", "abstract": "We study the use of the Wave-U-Net architecture for speech enhancement, a\nmodel introduced by Stoller et al for the separation of music vocals and\naccompaniment. This end-to-end learning method for audio source separation\noperates directly in the time domain, permitting the integrated modelling of\nphase information and being able to take large temporal contexts into account.\nOur experiments show that the proposed method improves several metrics, namely\nPESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the\nspeech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a\nreduced number of hidden layers is sufficient for speech enhancement in\ncomparison to the original system designed for singing voice separation in\nmusic. We see this initial result as an encouraging signal to further explore\nspeech enhancement in the time-domain, both as an end in itself and as a\npre-processing step to speech recognition systems.", "published": "2018-11-27 23:11:05", "link": "http://arxiv.org/abs/1811.11307v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
