{"title": "Merging Hazy Sets with m-Schemes: A Geometric Approach to Data Visualization", "abstract": "Many machine learning algorithms try to visualize high dimensional metric\ndata in 2D in such a way that the essential geometric and topological features\nof the data are highlighted. In this paper, we introduce a framework for\naggregating dissimilarity functions that arise from locally adjusting a metric\nthrough density-aware normalization, as employed in the IsUMap method. We\nformalize these approaches as m-schemes, a class of methods closely related to\nt-norms and t-conorms in probabilistic metrics, as well as to composition laws\nin information theory. These m-schemes provide a flexible and theoretically\ngrounded approach to refining distance-based embeddings.", "published": "2025-03-03 15:40:08", "link": "http://arxiv.org/abs/2503.01664v1", "categories": ["cs.LG", "cs.DM", "math.MG", "51F04, 51-04, 18-02, 51H99", "G.2.3; I.2.m"], "primary_category": "cs.LG"}
{"title": "Metric-Based Granular Computing in Networks", "abstract": "Networks can be highly complex systems with numerous interconnected\ncomponents and interactions. Granular computing offers a framework to manage\nthis complexity by decomposing networks into smaller, more manageable\ncomponents, or granules. In this article, we introduce metric-based granular\ncomputing technique to study networks. This technique can be applied to the\nanalysis of networks where granules can represent subsets of nodes or edges and\ntheir interactions can be studied at different levels of granularity. We model\nthe network as an information system and investigate its granular structures\nusing metric representation. We establish that the concepts of reducts in rough\nset theory and resolving sets in networks are equivalent. Through this\nequivalence, we present a novel approach for computing all the minimal\nresolving sets of these networks.", "published": "2025-03-03 07:34:10", "link": "http://arxiv.org/abs/2503.02901v1", "categories": ["cs.SI", "cs.DM", "05C12, 05A18, 05C62"], "primary_category": "cs.SI"}
{"title": "Hybrid Metaheuristic Vehicle Routing Problem for Security Dispatch Operations", "abstract": "This paper investigates the optimization of the Vehicle Routing Problem for\nSecurity Dispatch (VRPSD). VRPSD focuses on security and patrolling\napplications which involve challenging constraints including precise timing and\nstrict time windows. We propose three algorithms based on different\nmetaheuristics, which are Adaptive Large Neighborhood Search (ALNS), Tabu\nSearch (TS), and Threshold Accepting (TA). The first algorithm combines\nsingle-phase ALNS with TA, the second employs a multiphase ALNS with TA, and\nthe third integrates multiphase ALNS, TS, and TA. Experiments are conducted on\nan instance comprising 251 customer requests. The results demonstrate that the\nthird algorithm, the hybrid multiphase ALNS-TS-TA algorithm, delivers the best\nperformance. This approach simultaneously leverages the large-area search\ncapabilities of ALNS for exploration and effectively escapes local optima when\nthe multiphase ALNS is coupled with TS and TA. Furthermore, in our experiments,\nthe hybrid multiphase ALNS-TS-TA algorithm is the only one that shows potential\nfor improving results with increased computation time across all attempts.", "published": "2025-03-03 02:58:49", "link": "http://arxiv.org/abs/2503.01121v1", "categories": ["cs.AI", "cs.DM", "math.OC"], "primary_category": "cs.AI"}
{"title": "The Volterra Stein-Stein model with stochastic interest rates", "abstract": "We introduce the Volterra Stein-Stein model with stochastic interest rates,\nwhere both volatility and interest rates are driven by correlated Gaussian\nVolterra processes. This framework unifies various well-known Markovian and\nnon-Markovian models while preserving analytical tractability for pricing and\nhedging financial derivatives. We derive explicit formulas for pricing\nzero-coupon bond and interest rate cap or floor, along with a semi-explicit\nexpression for the characteristic function of the log-forward index using\nFredholm resolvents and determinants. This allows for fast and efficient\nderivative pricing and calibration via Fourier methods. We calibrate our model\nto market data and observe that our framework is flexible enough to capture key\nempirical features, such as the humped-shaped term structure of ATM implied\nvolatilities for cap options and the concave ATM implied volatility skew term\nstructure (in log-log scale) of the S&P 500 options. Finally, we establish\nconnections between our characteristic function formula and expressions that\ndepend on infinite-dimensional Riccati equations, thereby making the link with\nconventional linear-quadratic models.", "published": "2025-03-03 16:32:35", "link": "http://arxiv.org/abs/2503.01716v1", "categories": ["q-fin.MF", "q-fin.PR"], "primary_category": "q-fin.MF"}
{"title": "\\textsc{Perseus}: Tracing the Masterminds Behind Cryptocurrency Pump-and-Dump Schemes", "abstract": "Masterminds are entities organizing, coordinating, and orchestrating\ncryptocurrency pump-and-dump schemes, a form of trade-based manipulation\nundermining market integrity and causing financial losses for unwitting\ninvestors. Previous research detects pump-and-dump activities in the market,\npredicts the target cryptocurrency, and examines investors and \\ac{osn}\nentities. However, these solutions do not address the root cause of the\nproblem. There is a critical gap in identifying and tracing the masterminds\ninvolved in these schemes. In this research, we develop a detection system\n\\textsc{Perseus}, which collects real-time data from the \\acs{osn} and\ncryptocurrency markets. \\textsc{Perseus} then constructs temporal attributed\ngraphs that preserve the direction of information diffusion and the structure\nof the community while leveraging \\ac{gnn} to identify the masterminds behind\npump-and-dump activities. Our design of \\textsc{Perseus} leads to higher F1\nscores and precision than the \\ac{sota} fraud detection method, achieving fast\ntraining and inferring speeds. Deployed in the real world from February 16 to\nOctober 9 2024, \\textsc{Perseus} successfully detects $438$ masterminds who are\nefficient in the pump-and-dump information diffusion networks. \\textsc{Perseus}\nprovides regulators with an explanation of the risks of masterminds and\noversight capabilities to mitigate the pump-and-dump schemes of cryptocurrency.", "published": "2025-03-03 15:59:40", "link": "http://arxiv.org/abs/2503.01686v1", "categories": ["cs.CY", "cs.LG", "q-fin.TR"], "primary_category": "cs.CY"}
{"title": "A New Traders' Game? -- Response Functions in a Historical Perspective", "abstract": "Traders on financial markets generate non-Markovian effects in various ways,\nparticularly through their competition with one another which can be\ninterpreted as a game between different (types of) traders. To quantify the\nmarket mechanisms, we analyze self-response functions for pairs of different\nstocks and the corresponding trade sign correlators. While the non-Markovian\ndynamics in the self-responses is liquidity-driven, it is expectation-driven in\nthe cross-responses which is related to the emergence of correlations. We study\nthe non-stationarity of theses responses over time. In our previous analysis,\nwe only investigated the crisis year 2008. We now considerably extend this by\nalso analyzing the years 2007, 2014 and 2021. To improve statistics, we also\nwork out averaged response functions for the different years. We find\nsignificant variations over time revealing changes in the traders' game.", "published": "2025-03-03 15:04:46", "link": "http://arxiv.org/abs/2503.01629v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Efficient or Powerful? Trade-offs Between Machine Learning and Deep\n  Learning for Mental Illness Detection on Social Media", "abstract": "Social media platforms provide valuable insights into mental health trends by\ncapturing user-generated discussions on conditions such as depression, anxiety,\nand suicidal ideation. Machine learning (ML) and deep learning (DL) models have\nbeen increasingly applied to classify mental health conditions from textual\ndata, but selecting the most effective model involves trade-offs in accuracy,\ninterpretability, and computational efficiency. This study evaluates multiple\nML models, including logistic regression, random forest, and LightGBM,\nalongside deep learning architectures such as ALBERT and Gated Recurrent Units\n(GRUs), for both binary and multi-class classification of mental health\nconditions. Our findings indicate that ML and DL models achieve comparable\nclassification performance on medium-sized datasets, with ML models offering\ngreater interpretability through variable importance scores, while DL models\nare more robust to complex linguistic patterns. Additionally, ML models require\nexplicit feature engineering, whereas DL models learn hierarchical\nrepresentations directly from text. Logistic regression provides the advantage\nof capturing both positive and negative associations between features and\nmental health conditions, whereas tree-based models prioritize decision-making\npower through split-based feature selection. This study offers empirical\ninsights into the advantages and limitations of different modeling approaches\nand provides recommendations for selecting appropriate methods based on dataset\nsize, interpretability needs, and computational constraints.", "published": "2025-03-03 00:51:41", "link": "http://arxiv.org/abs/2503.01082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precise Localization of Memories: A Fine-grained Neuron-level Knowledge\n  Editing Technique for LLMs", "abstract": "Knowledge editing aims to update outdated information in Large Language\nModels (LLMs). A representative line of study is locate-then-edit methods,\nwhich typically employ causal tracing to identify the modules responsible for\nrecalling factual knowledge about entities. However, we find these methods are\noften sensitive only to changes in the subject entity, leaving them less\neffective at adapting to changes in relations. This limitation results in poor\nediting locality, which can lead to the persistence of irrelevant or inaccurate\nfacts, ultimately compromising the reliability of LLMs. We believe this issue\narises from the insufficient precision of knowledge localization. To address\nthis, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method\nthat enhances editing locality without affecting overall success rates. By\nprecisely identifying and modifying specific neurons within feed-forward\nnetworks, FiNE significantly improves knowledge localization and editing.\nQuantitative experiments demonstrate that FiNE efficiently achieves better\noverall performance compared to existing techniques, providing new insights\ninto the localization and modification of knowledge within LLMs.", "published": "2025-03-03 01:30:28", "link": "http://arxiv.org/abs/2503.01090v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority\n  Languages", "abstract": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems and\nprovides a fine-grained assessment of linguistic and problem-solving skills.\nOur evaluation reveals that LLMs perform poorly on syntax-intensive tasks and\nmulti-script languages. We further demonstrate how MiLiC-Eval can help advance\nLRL research in handling diverse writing systems and understanding the process\nof language adaptation.", "published": "2025-03-03 03:56:03", "link": "http://arxiv.org/abs/2503.01150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Healthcare Text Classification: A Systematic\n  Review", "abstract": "Large Language Models (LLMs) have fundamentally transformed approaches to\nNatural Language Processing (NLP) tasks across diverse domains. In healthcare,\naccurate and cost-efficient text classification is crucial, whether for\nclinical notes analysis, diagnosis coding, or any other task, and LLMs present\npromising potential. Text classification has always faced multiple challenges,\nincluding manual annotation for training, handling imbalanced data, and\ndeveloping scalable approaches. With healthcare, additional challenges are\nadded, particularly the critical need to preserve patients' data privacy and\nthe complexity of the medical terminology. Numerous studies have been conducted\nto leverage LLMs for automated healthcare text classification and contrast the\nresults with existing machine learning-based methods where embedding,\nannotation, and training are traditionally required. Existing systematic\nreviews about LLMs either do not specialize in text classification or do not\nfocus on the healthcare domain. This research synthesizes and critically\nevaluates the current evidence found in the literature regarding the use of\nLLMs for text classification in a healthcare setting. Major databases (e.g.,\nGoogle Scholar, Scopus, PubMed, Science Direct) and other resources were\nqueried, which focused on the papers published between 2018 and 2024 within the\nframework of PRISMA guidelines, which resulted in 65 eligible research\narticles. These were categorized by text classification type (e.g., binary\nclassification, multi-label classification), application (e.g., clinical\ndecision support, public health and opinion analysis), methodology, type of\nhealthcare text, and metrics used for evaluation and validation. This review\nreveals the existing gaps in the literature and suggests future research lines\nthat can be investigated and explored.", "published": "2025-03-03 04:16:13", "link": "http://arxiv.org/abs/2503.01159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEO: Improving Bi-Factorial Preference Alignment with Post-Training\n  Policy Extrapolation", "abstract": "The alignment of large language models with human values presents a critical\nchallenge, particularly when balancing conflicting objectives like helpfulness\nand harmlessness. Existing approaches, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), face notable\nlimitations: RLHF suffers from instability and inefficiency in multi-objective\noptimization, while DPO lacks mechanisms for dynamic trade-offs. To address\nthese challenges, we propose Post-Training Extrapolation Optimization (PEO), a\nnovel and efficient framework for bi-factorial alignment. PEO generates a\nfamily of Pareto-optimal policies in a single training pass by leveraging a\nthree-phase pipeline: (1) aspect-specific learning, (2) generalist\ninitialization via interpolation, and (3) post-training optimization via\nextrapolation. PEO enables dynamic adaptation to diverse user preferences at\ninference time without retraining. Our comprehensive experiments across\nmultiple LLMs demonstrate that PEO achieves superior Pareto fronts compared to\nbaselines, offering improved flexibility and computational efficiency.\nTheoretical analyses further highlight PEO's capacity to overcome optimization\nbottlenecks, paving the way for scalable, personalized alignment.", "published": "2025-03-03 06:56:39", "link": "http://arxiv.org/abs/2503.01233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your Model is Overconfident, and Other Lies We Tell Ourselves", "abstract": "The difficulty intrinsic to a given example, rooted in its inherent\nambiguity, is a key yet often overlooked factor in evaluating neural NLP\nmodels. We investigate the interplay and divergence among various metrics for\nassessing intrinsic difficulty, including annotator dissensus, training\ndynamics, and model confidence. Through a comprehensive analysis using 29\nmodels on three datasets, we reveal that while correlations exist among these\nmetrics, their relationships are neither linear nor monotonic. By disentangling\nthese dimensions of uncertainty, we aim to refine our understanding of data\ncomplexity and its implications for evaluating and improving NLP models.", "published": "2025-03-03 06:59:28", "link": "http://arxiv.org/abs/2503.01235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning", "abstract": "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods.", "published": "2025-03-03 07:59:32", "link": "http://arxiv.org/abs/2503.01275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Tree Extraction from Medical Case Reports: A Novel Task for\n  Experts-like Text Comprehension", "abstract": "Extracting causal relationships from a medical case report is essential for\ncomprehending the case, particularly its diagnostic process. Since the\ndiagnostic process is regarded as a bottom-up inference, causal relationships\nin cases naturally form a multi-layered tree structure. The existing tasks,\nsuch as medical relation extraction, are insufficient for capturing the causal\nrelationships of an entire case, as they treat all relations equally without\nconsidering the hierarchical structure inherent in the diagnostic process.\nThus, we propose a novel task, Causal Tree Extraction (CTE), which receives a\ncase report and generates a causal tree with the primary disease as the root,\nproviding an intuitive understanding of a case's diagnostic process.\nSubsequently, we construct a Japanese case report CTE dataset, J-Casemap,\npropose a generation-based CTE method that outperforms the baseline by 20.2\npoints in the human evaluation, and introduce evaluation metrics that reflect\nclinician preferences. Further experiments also show that J-Casemap enhances\nthe performance of solving other medical tasks, such as question answering.", "published": "2025-03-03 08:40:01", "link": "http://arxiv.org/abs/2503.01302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PROPER: A Progressive Learning Framework for Personalized Large Language\n  Models with Group-Level Adaptation", "abstract": "Personalized large language models (LLMs) aim to tailor their outputs to user\npreferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods\nhave highlighted the effectiveness of adapting population-level LLMs to\npersonalized LLMs by fine-tuning user-specific parameters with user history.\nHowever, user data is typically sparse, making it challenging to adapt LLMs to\nspecific user patterns. To address this challenge, we propose PROgressive\nPERsonalization (PROPER), a novel progressive learning framework inspired by\nmeso-level theory in social science. PROPER bridges population-level and\nuser-level models by grouping users based on preferences and adapting LLMs in\nstages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked\nAdaptation (LoRA), using a user-aware router to assign users to appropriate\ngroups automatically. Additionally, a LoRA-aware router is proposed to\nfacilitate the integration of individual user LoRAs with group-level LoRAs.\nExperimental results show that PROPER significantly outperforms SOTA models\nacross multiple tasks, demonstrating the effectiveness of our approach.", "published": "2025-03-03 08:40:50", "link": "http://arxiv.org/abs/2503.01303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Depression Detection in Clinical Interviews with\n  Personalized Retrieval-Augmented Generation", "abstract": "Depression is a widespread mental health disorder, and clinical interviews\nare the gold standard for assessment. However, their reliance on scarce\nprofessionals highlights the need for automated detection. Current systems\nmainly employ black-box neural networks, which lack interpretability, which is\ncrucial in mental health contexts. Some attempts to improve interpretability\nuse post-hoc LLM generation but suffer from hallucination. To address these\nlimitations, we propose RED, a Retrieval-augmented generation framework for\nExplainable depression Detection. RED retrieves evidence from clinical\ninterview transcripts, providing explanations for predictions. Traditional\nquery-based retrieval systems use a one-size-fits-all approach, which may not\nbe optimal for depression detection, as user backgrounds and situations vary.\nWe introduce a personalized query generation module that combines standard\nqueries with user-specific background inferred by LLMs, tailoring retrieval to\nindividual contexts. Additionally, to enhance LLM performance in social\nintelligence, we augment LLMs by retrieving relevant knowledge from a social\nintelligence datastore using an event-centric retriever. Experimental results\non the real-world benchmark demonstrate RED's effectiveness compared to neural\nnetworks and LLM-based baselines.", "published": "2025-03-03 08:59:34", "link": "http://arxiv.org/abs/2503.01315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models", "abstract": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.", "published": "2025-03-03 09:12:34", "link": "http://arxiv.org/abs/2503.01330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Retrospective Language Agents via Joint Policy Gradient\n  Optimization", "abstract": "In recent research advancements within the community, large language models\n(LLMs) have sparked great interest in creating autonomous agents. However,\ncurrent prompt-based agents often heavily rely on large-scale LLMs. Meanwhile,\nalthough fine-tuning methods significantly enhance the capabilities of smaller\nLLMs, the fine-tuned agents often lack the potential for self-reflection and\nself-improvement. To address these challenges, we introduce a novel agent\nframework named RetroAct, which is a framework that jointly optimizes both\ntask-planning and self-reflective evolution capabilities in language agents.\nSpecifically, we develop a two-stage joint optimization process that integrates\nimitation learning and reinforcement learning, and design an off-policy joint\npolicy gradient optimization algorithm with imitation learning regularization\nto enhance the data efficiency and training stability in agent tasks. RetroAct\nsignificantly improves the performance of open-source models, reduces\ndependency on closed-source LLMs, and enables fine-tuned agents to learn and\nevolve continuously. We conduct extensive experiments across various testing\nenvironments, demonstrating RetroAct has substantial improvements in task\nperformance and decision-making processes.", "published": "2025-03-03 12:54:54", "link": "http://arxiv.org/abs/2503.01490v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh", "abstract": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a\nstate-of-the-art instruction-tuned open generative large language model (LLM)\ndesigned for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM\nadvancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model,\nSherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian,\nand Turkish. With 8 billion parameters, it demonstrates strong knowledge and\nreasoning abilities in Kazakh, significantly outperforming existing open Kazakh\nand multilingual models of similar scale while achieving competitive\nperformance in English. We release Sherkala-Chat (8B) as an open-weight\ninstruction-tuned model and provide a detailed overview of its training,\nfine-tuning, safety alignment, and evaluation, aiming to advance research and\nsupport diverse real-world applications.", "published": "2025-03-03 13:05:48", "link": "http://arxiv.org/abs/2503.01493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SampleMix: A Sample-wise Pre-training Data Mixing Strategey by\n  Coordinating Data Quality and Diversity", "abstract": "Existing pretraining data mixing methods for large language models (LLMs)\ntypically follow a domain-wise methodology, a top-down process that first\ndetermines domain weights and then performs uniform data sampling across each\ndomain. However, these approaches neglect significant inter-domain overlaps and\ncommonalities, failing to control the global diversity of the constructed\ntraining dataset. Further, uniform sampling within domains ignores fine-grained\nsample-specific features, potentially leading to suboptimal data distribution.\nTo address these shortcomings, we propose a novel sample-wise data mixture\napproach based on a bottom-up paradigm. This method performs global\ncross-domain sampling by systematically evaluating the quality and diversity of\neach sample, thereby dynamically determining the optimal domain distribution.\nComprehensive experiments across multiple downstream tasks and perplexity\nassessments demonstrate that SampleMix surpasses existing domain-based methods.\nMeanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the\nbaselines' performance, highlighting the substantial potential of SampleMix to\noptimize pre-training data.", "published": "2025-03-03 13:22:11", "link": "http://arxiv.org/abs/2503.01506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoWit-24: A Richly Annotated Dataset of Wordplay in News Headlines", "abstract": "We present KoWit-24, a dataset with fine-grained annotation of wordplay in\n2,700 Russian news headlines. KoWit-24 annotations include the presence of\nwordplay, its type, wordplay anchors, and words/phrases the wordplay refers to.\nUnlike the majority of existing humor collections of canned jokes, KoWit-24\nprovides wordplay contexts -- each headline is accompanied by the news lead and\nsummary. The most common type of wordplay in the dataset is the transformation\nof collocations, idioms, and named entities -- the mechanism that has been\nunderrepresented in previous humor datasets. Our experiments with five LLMs\nshow that there is ample room for improvement in wordplay detection and\ninterpretation tasks. The dataset and evaluation scripts are available at\nhttps://github.com/Humor-Research/KoWit-24", "published": "2025-03-03 13:24:25", "link": "http://arxiv.org/abs/2503.01510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A\n  Survey", "abstract": "We present a survey of methods for assessing and enhancing the quality of\nonline discussions, focusing on the potential of Large Language Models (LLMs).\nWhile online discourses aim, at least in theory, to foster mutual\nunderstanding, they often devolve into harmful exchanges, such as hate speech,\nthreatening social cohesion and democratic values. Recent advancements in LLMs\nenable facilitation agents that not only moderate content, but also actively\nimprove the quality of interactions. Our survey synthesizes ideas from Natural\nLanguage Processing (NLP) and Social Sciences to provide (a) a new taxonomy on\ndiscussion quality evaluation, (b) an overview of intervention and facilitation\nstrategies, along with a new taxonomy on conversation facilitation datasets,\n(c) an LLM-oriented roadmap of good practices and future research directions,\nfrom technological and societal perspectives.", "published": "2025-03-03 13:26:01", "link": "http://arxiv.org/abs/2503.01513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-creation for Sign Language Processing and Machine Translation", "abstract": "Sign language machine translation (SLMT) -- the task of automatically\ntranslating between sign and spoken languages or between sign languages -- is a\ncomplex task within the field of NLP. Its multi-modal and non-linear nature\nrequire the joint efforts of sign language (SL) linguists, technical experts\nand SL users. Effective user involvement is a challenge that can be addressed\nthrough co-creation. Co-creation has been formally defined in many fields, e.g.\nbusiness, marketing, educational and others, however in NLP and in particular\nin SLMT there is no formal, widely accepted definition. Starting from the\ninception and evolution of co-creation across various fields over time, we\ndevelop a relationship typology to address the collaboration between deaf, Hard\nof Hearing and hearing researchers and the co-creation with SL-users. We\ncompare this new typology to the guiding principles of participatory design for\nNLP. We, then, assess 110 articles from the perspective of involvement of SL\nusers and highlight the lack of involvement of the sign language community or\nusers in decision-making processes required for effective co-creation. Finally,\nwe derive formal guidelines for co-creation for SLMT which take the dynamic\nnature of co-creation throughout the life cycle of a research project into\naccount.", "published": "2025-03-03 13:58:07", "link": "http://arxiv.org/abs/2503.01553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Condensation via Sparsity Induced Regularized Training", "abstract": "As the context window expands, self-attention increasingly dominates the\ntransformer's inference time. Therefore, accelerating attention computation\nwhile minimizing performance degradation is essential for the efficient\ndeployment of Large Language Models (LLMs). In this study we extend a\ntheoretical framework of attention sparsity in LLMs. A customized loss function\nis designed to enforce the sparsity by restricting the number of top elements\nin the attention matrix. We perform an initial set of evaluations with GPT-2 to\nshow the effectiveness of our sparsification approach. The attention matrices\nof the models trained with the proposed loss are both sparse and effective in\ncapturing relevant input dependencies. We now continue working to demonstrate\nthe value of our approach on larger models and different architectures.", "published": "2025-03-03 14:09:13", "link": "http://arxiv.org/abs/2503.01564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-context Learning vs. Instruction Tuning: The Case of Small and\n  Multilingual Language Models", "abstract": "Instruction following is a critical ability for Large Language Models to\nperform downstream tasks. The standard approach to instruction alignment has\nrelied on a specific phase of model tuning over curated instruction datasets,\noptionally complemented with an alignment step over human preferences. Recent\nwork has shown the potential of in-context learning (ICL) alternatives to guide\nbase models towards instruction following. This type of approach is\nparticularly relevant to extend instruction following across languages and\nmodels of varying sizes adapted to different types of usage. In this work we\ncompare ICL and instruction fine-tuning in English, French and Spanish, on\nSmall Language Models, and provide experimental results on applying Direct\nPreference Optimisation (DPO) over base models. Our results show that scenarios\ninvolving multilingual and smaller models result in downgraded ICL instruction\nfollowing performance, only partially mitigated by DPO alignment. This study\naims to further our understanding of current strengths and limitations of\nalternative methods for instruction following.", "published": "2025-03-03 14:47:23", "link": "http://arxiv.org/abs/2503.01611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards\n  Meaningful LLM Evaluation", "abstract": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/", "published": "2025-03-03 14:55:41", "link": "http://arxiv.org/abs/2503.01622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating and Inferring Compositional Structures in Numeral Systems\n  Across Languages", "abstract": "Numeral systems across the world's languages vary in fascinating ways, both\nregarding their synchronic structure and the diachronic processes that\ndetermined how they evolved in their current shape. For a proper comparison of\nnumeral systems across different languages, however, it is important to code\nthem in a standardized form that allows for the comparison of basic properties.\nHere, we present a simple but effective coding scheme for numeral annotation,\nalong with a workflow that helps to code numeral systems in a computer-assisted\nmanner, providing sample data for numerals from 1 to 40 in 25 typologically\ndiverse languages. We perform a thorough analysis of the sample, focusing on\nthe systematic comparison between the underlying and the surface morphological\nstructure. We further experiment with automated models for morpheme\nsegmentation, where we find allomorphy as the major reason for segmentation\nerrors. Finally, we show that subword tokenization algorithms are not viable\nfor discovering morphemes in low-resource scenarios.", "published": "2025-03-03 15:00:36", "link": "http://arxiv.org/abs/2503.01625v2", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Detecting Stylistic Fingerprints of Large Language Models", "abstract": "Large language models (LLMs) have distinct and consistent stylistic\nfingerprints, even when prompted to write in different writing styles.\nDetecting these fingerprints is important for many reasons, among them\nprotecting intellectual property, ensuring transparency regarding AI-generated\ncontent, and preventing the misuse of AI technologies. In this paper, we\npresent a novel method to classify texts based on the stylistic fingerprints of\nthe models that generated them. We introduce an LLM-detection ensemble that is\ncomposed of three classifiers with varied architectures and training data. This\nensemble is trained to classify texts generated by four well-known LLM\nfamilies: Claude, Gemini, Llama, and OpenAI. As this task is highly\ncost-sensitive and might have severe implications, we want to minimize\nfalse-positives and increase confidence. We consider a prediction as valid when\nall three classifiers in the ensemble unanimously agree on the output\nclassification. Our ensemble is validated on a test set of texts generated by\nClaude, Gemini, Llama, and OpenAI models, and achieves extremely high precision\n(0.9988) and a very low false-positive rate (0.0004). Furthermore, we\ndemonstrate the ensemble's ability to distinguish between texts generated by\nseen and unseen models. This reveals interesting stylistic relationships\nbetween models. This approach to stylistic analysis has implications for\nverifying the originality of AI-generated texts and tracking the origins of\nmodel training techniques.", "published": "2025-03-03 15:33:10", "link": "http://arxiv.org/abs/2503.01659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via\n  Fine-Grained Sentence-Level Self-Evolution", "abstract": "Improving context faithfulness in large language models is essential for\ndeveloping trustworthy retrieval augmented generation systems and mitigating\nhallucinations, especially in long-form question answering (LFQA) tasks or\nscenarios involving knowledge conflicts. Existing methods either intervene LLMs\nonly at inference without addressing their inherent limitations or overlook the\npotential for self-improvement. In this paper, we introduce GenDiE (Generate,\nDiscriminate, Evolve), a novel self-evolving framework that enhances context\nfaithfulness through fine-grained sentence-level optimization. GenDiE combines\nboth generative and discriminative training, equipping LLMs with\nself-generation and self-scoring capabilities to facilitate iterative\nself-evolution. This supports both data construction for model alignment and\nscore-guided search during inference. Furthermore, by treating each sentence in\na response as an independent optimization unit, GenDiE effectively addresses\nthe limitations of previous approaches that optimize at the holistic answer\nlevel, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA)\nand ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE\nsurpasses various baselines in both faithfulness and correctness, and exhibits\nrobust performance for domain adaptation.", "published": "2025-03-03 16:08:33", "link": "http://arxiv.org/abs/2503.01695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Learnability of Echo State Neural Language Models at Scale", "abstract": "What is a neural model with minimum architectural complexity that exhibits\nreasonable language learning capability? To explore such a simple but\nsufficient neural language model, we revisit a basic reservoir computing (RC)\nmodel, Echo State Network (ESN), a restricted class of simple Recurrent Neural\nNetworks. Our experiments showed that ESN with a large hidden state is\ncomparable or superior to Transformer in grammaticality judgment tasks when\ntrained with about 100M words, suggesting that architectures as complex as that\nof Transformer may not always be necessary for syntactic learning.", "published": "2025-03-03 16:37:55", "link": "http://arxiv.org/abs/2503.01724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming\n  for Large Language Models", "abstract": "The rapid growth of Large Language Models (LLMs) presents significant\nprivacy, security, and ethical concerns. While much research has proposed\nmethods for defending LLM systems against misuse by malicious actors,\nresearchers have recently complemented these efforts with an offensive approach\nthat involves red teaming, i.e., proactively attacking LLMs with the purpose of\nidentifying their vulnerabilities. This paper provides a concise and practical\noverview of the LLM red teaming literature, structured so as to describe a\nmulti-component system end-to-end. To motivate red teaming we survey the\ninitial safety needs of some high-profile LLMs, and then dive into the\ndifferent components of a red teaming system as well as software packages for\nimplementing them. We cover various attack methods, strategies for\nattack-success evaluation, metrics for assessing experiment outcomes, as well\nas a host of other considerations. Our survey will be useful for any reader who\nwants to rapidly obtain a grasp of the major red teaming concepts for their own\nuse in practical applications.", "published": "2025-03-03 17:04:22", "link": "http://arxiv.org/abs/2503.01742v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boolean-aware Attention for Dense Retrieval", "abstract": "We present Boolean-aware attention, a novel attention mechanism that\ndynamically adjusts token focus based on Boolean operators (e.g., and, or,\nnot). Our model employs specialized Boolean experts, each tailored to amplify\nor suppress attention for operator-specific contexts. A predefined gating\nmechanism activates the corresponding experts based on the detected Boolean\ntype. Experiments on Boolean retrieval datasets demonstrate that integrating\nBoolAttn with BERT greatly enhances the model's capability to process Boolean\nqueries.", "published": "2025-03-03 17:23:08", "link": "http://arxiv.org/abs/2503.01753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism\n  Perspective on Focus Areas", "abstract": "Large Vision Language Models (VLMs) have long struggled with spatial\nreasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as\nrecognizing \"under\" or \"behind\" relationships between only two objects, pose\nsignificant challenges for current VLMs. In this work, we study the spatial\nreasoning challenge from the lens of mechanistic interpretability, diving into\nthe model's internal states to examine the interactions between image and text\ntokens. By tracing attention distribution over the image through out\nintermediate layers, we observe that successful spatial reasoning correlates\nstrongly with the model's ability to align its attention distribution with\nactual object locations, particularly differing between familiar and unfamiliar\nspatial relationships. Motivated by these findings, we propose ADAPTVIS based\non inference-time confidence scores to sharpen the attention on highly relevant\nregions when confident, while smoothing and broadening the attention window to\nconsider a wider context when confidence is lower. This training-free decoding\nmethod shows significant improvement (e.g., up to a 50 absolute point\nimprovement) on spatial reasoning benchmarks such as WhatsUp and VSR with\nnegligible cost. We make code and data publicly available for research purposes\nat https://github.com/shiqichen17/AdaptVis.", "published": "2025-03-03 17:57:03", "link": "http://arxiv.org/abs/2503.01773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for\n  Reasoning Models", "abstract": "We investigate the robustness of reasoning models trained for step-by-step\nproblem solving by introducing query-agnostic adversarial triggers - short,\nirrelevant text that, when appended to math problems, systematically mislead\nmodels to output incorrect answers without altering the problem's semantics. We\npropose CatAttack, an automated iterative attack pipeline for generating\ntriggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully\ntransfer them to more advanced reasoning target models like DeepSeek R1 and\nDeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the\nlikelihood of the target model generating an incorrect answer. For example,\nappending, \"Interesting fact: cats sleep most of their lives,\" to any math\nproblem leads to more than doubling the chances of a model getting the answer\nwrong. Our findings highlight critical vulnerabilities in reasoning models,\nrevealing that even state-of-the-art models remain susceptible to subtle\nadversarial inputs, raising security and reliability concerns. The CatAttack\ntriggers dataset with model responses is available at\nhttps://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers.", "published": "2025-03-03 18:10:54", "link": "http://arxiv.org/abs/2503.01781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-Scale Data Selection for Instruction Tuning", "abstract": "Selecting high-quality training data from a larger pool is a crucial step\nwhen instruction-tuning language models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.\nAutomated data selection approaches for instruction-tuning are typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant of\nrepresentation-based data selection (RDS+), which uses weighted mean pooling of\npretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection.", "published": "2025-03-03 18:37:26", "link": "http://arxiv.org/abs/2503.01807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Language to Cognition: How LLMs Outgrow the Human Language Network", "abstract": "Large language models (LLMs) exhibit remarkable similarity to neural activity\nin the human language network. However, the key properties of language shaping\nbrain-like representations, and their evolution during training as a function\nof different tasks remain unclear. We here benchmark 34 training checkpoints\nspanning 300B tokens across 8 different model sizes to analyze how brain\nalignment relates to linguistic competence. Specifically, we find that brain\nalignment tracks the development of formal linguistic competence -- i.e.,\nknowledge of linguistic rules -- more closely than functional linguistic\ncompetence. While functional competence, which involves world knowledge and\nreasoning, continues to develop throughout training, its relationship with\nbrain alignment is weaker, suggesting that the human language network primarily\nencodes formal linguistic structure rather than broader cognitive functions. We\nfurther show that model size is not a reliable predictor of brain alignment\nwhen controlling for feature size and find that the correlation between\nnext-word prediction, behavioral alignment and brain alignment fades once\nmodels surpass human language proficiency. Finally, using the largest set of\nrigorous neural language benchmarks to date, we show that language brain\nalignment benchmarks remain unsaturated, highlighting opportunities for\nimproving future models. Taken together, our findings suggest that the human\nlanguage network is best modeled by formal, rather than functional, aspects of\nlanguage.", "published": "2025-03-03 18:54:19", "link": "http://arxiv.org/abs/2503.01830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test", "abstract": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE.", "published": "2025-03-03 18:59:04", "link": "http://arxiv.org/abs/2503.01840v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can (A)I Change Your Mind?", "abstract": "The increasing integration of large language model (LLM) based conversational\nagents into everyday life raises critical cognitive and social questions about\ntheir potential to influence human opinions. Although previous studies have\nshown that LLM-based agents can generate persuasive content, these typically\ninvolve controlled, English-language settings. Addressing this, our\npreregistered study explored LLM's persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios, except in static LLM interactions. These\nfindings demonstrate LLM-based agents' robust persuasive capabilities across\ndiverse sources and settings, highlighting their potential impact on shaping\npublic opinions.", "published": "2025-03-03 18:59:54", "link": "http://arxiv.org/abs/2503.01844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One ruler to measure them all: Benchmarking multilingual long-context\n  language models", "abstract": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines.", "published": "2025-03-03 19:12:48", "link": "http://arxiv.org/abs/2503.01996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persuasion at Play: Understanding Misinformation Dynamics in\n  Demographic-Aware Human-LLM Interactions", "abstract": "Existing challenges in misinformation exposure and susceptibility vary across\ndemographic groups, as some populations are more vulnerable to misinformation\nthan others. Large language models (LLMs) introduce new dimensions to these\nchallenges through their ability to generate persuasive content at scale and\nreinforcing existing biases. This study investigates the bidirectional\npersuasion dynamics between LLMs and humans when exposed to misinformative\ncontent. We analyze human-to-LLM influence using human-stance datasets and\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\nmisinformation under persuasion among demographic-oriented LLM agents. Our\nfindings show that demographic factors influence susceptibility to\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\nin human susceptibility. We also find that, similar to human demographic\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\nthe interplay between humans and LLMs, highlighting demographic differences in\nthe context of misinformation and offering insights for future interventions.", "published": "2025-03-03 20:30:22", "link": "http://arxiv.org/abs/2503.02038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Twenty Years of Personality Computing: Threats, Challenges and Future\n  Directions", "abstract": "Personality Computing is a field at the intersection of Personality\nPsychology and Computer Science. Started in 2005, research in the field\nutilizes computational methods to understand and predict human personality\ntraits. The expansion of the field has been very rapid and, by analyzing\ndigital footprints (text, images, social media, etc.), it helped to develop\nsystems that recognize and even replicate human personality. While offering\npromising applications in talent recruiting, marketing and healthcare, the\nethical implications of Personality Computing are significant. Concerns include\ndata privacy, algorithmic bias, and the potential for manipulation by\npersonality-aware Artificial Intelligence. This paper provides an overview of\nthe field, explores key methodologies, discusses the challenges and threats,\nand outlines potential future directions for responsible development and\ndeployment of Personality Computing technologies.", "published": "2025-03-03 22:03:48", "link": "http://arxiv.org/abs/2503.02082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Superficial Self-Improved Reasoners Benefit from Model Merging", "abstract": "As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.", "published": "2025-03-03 22:41:25", "link": "http://arxiv.org/abs/2503.02103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact\n  Embedding in LLMs", "abstract": "This paper presents an extensive examination of Parameter-Efficient\nFine-Tuning (PEFT) for embedding domain specific facts into Large Language\nModels (LLMs), focusing on improving the fine-tuning process by categorizing\nquestion-answer (QA) pairs into Factual and Conceptual classes using a\nBERT-based classifier. Two distinct Llama-2 models are fine-tuned based on\nthese classifications and evaluated using larger models like GPT-3.5 Turbo and\nGemini. Our results indicate that models trained on conceptual datasets\noutperform those trained on factual datasets. Additionally, we compare the\nefficiency of two synthetic fine-tuning dataset generation techniques, D-RAG\nand D-Naive, with D-Naive demonstrating superior performance. Although PEFT has\nshown effectiveness, our research indicates that it may not be the most optimal\nmethod for embedding facts into LLMs. However, it has demonstrated exceptional\nperformance in instruction-based tasks. Our findings are reinforced by a\n1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B\nmodel significantly outperforms the baseline model in generating product\nrecommendations. Our study highlights the importance of QA pair categorization\nand synthetic dataset generation techniques in enhancing the performance of\nLLMs in specific domains.", "published": "2025-03-03 03:26:30", "link": "http://arxiv.org/abs/2503.01131v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity\n  Approach", "abstract": "Chain-of-thought prompting has emerged as a powerful technique for enabling\nlarge language models (LLMs) to solve complex reasoning tasks. However, these\nreasoning chains can be verbose, raising concerns about efficiency. In\nresponse, recent works have sought to decrease response lengths through simple\nprompting strategies (e.g. 'be concise'). In this work, we conduct the first\nsystematic study of the relationship between reasoning length and model\nperformance across a diverse range of compression instructions (e.g. 'use 10\nwords or less' or 'remove all punctuation'). In doing so, we discover a\nuniversal tradeoff between reasoning length and accuracy that persists across\neven very distinct reasoning chains. We demonstrate that this tradeoff emerges\nfrom a sharp threshold behavior at the question level: each task has an\nintrinsic 'token complexity' - a minimal number of tokens required for\nsuccessful problem-solving. We show how token complexity enables us to compute\ninformation-theoretic limits on the accuracy-compression tradeoff, and find\nthat prompt-based compression strategies operate far from these theoretical\nlimits. This suggests there may be significant room for improvement and our\nframework provides a benchmark to help researchers evaluate progress in\nreasoning efficiency. Our work also highlights the importance of adaptive\ncompression -- giving shorter responses for easier questions -- and we show\nthat token complexity is a useful tool for measuring this capability.", "published": "2025-03-03 03:48:20", "link": "http://arxiv.org/abs/2503.01141v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nature-Inspired Population-Based Evolution of Large Language Models", "abstract": "Evolution, the engine behind the survival and growth of life on Earth,\noperates through the population-based process of reproduction. Inspired by this\nprinciple, this paper formally defines a newly emerging problem -- the\npopulation-based evolution of large language models (LLMs) -- and introduces a\nnovel framework. Starting with a population of parent LLMs, our framework\nenables the population to evolve through four key operations: (i) crossover,\nmerging the weights of different parents to create offspring LLMs, (ii)\nmutation, introducing small, random changes to model weights to foster\ndiversity, (iii) selection, prioritizing high-performing models, and (iv)\nsuccession, transferring the learned experience from parent to offspring LLMs.\nWith only 200 samples per new task, the LLM population evolves rapidly to adapt\nto the task at hand, without any gradients. Experiments on 12 datasets show\nthat our framework consistently outperforms existing multi-LLM merging and\nadaptation methods, achieving accuracy gains of up to 54.8% over the best LLM\nin the initial population. Moreover, our framework allows for the evolution of\nLLMs across multiple new tasks simultaneously, scaling effectively with\npopulations of up to 40 LLMs, and even zero-shot generalization to unseen\nheld-out tasks. We have open-sourced the code on GitHub and released the\nweights of 10 parent LLMs, fine-tuned from gemma-2-2b-it, on HuggingFace$,\nenabling reproduction of our proposed framework using just a single 4090 GPU\nwith 24GB memory, without any performance degradation.", "published": "2025-03-03 04:03:31", "link": "http://arxiv.org/abs/2503.01155v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Cancer Type, Stage and Prognosis Assessment from Pathology Reports using\n  LLMs", "abstract": "Large Language Models (LLMs) have shown significant promise across various\nnatural language processing tasks. However, their application in the field of\npathology, particularly for extracting meaningful insights from unstructured\nmedical texts such as pathology reports, remains underexplored and not well\nquantified. In this project, we leverage state-of-the-art language models,\nincluding the GPT family, Mistral models, and the open-source Llama models, to\nevaluate their performance in comprehensively analyzing pathology reports.\nSpecifically, we assess their performance in cancer type identification, AJCC\nstage determination, and prognosis assessment, encompassing both information\nextraction and higher-order reasoning tasks. Based on a detailed analysis of\ntheir performance metrics in a zero-shot setting, we developed two\ninstruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These\nmodels demonstrated superior performance in zero-shot cancer type\nidentification, staging, and prognosis assessment compared to the other models\nevaluated.", "published": "2025-03-03 05:41:16", "link": "http://arxiv.org/abs/2503.01194v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Watch Out Your Album! On the Inadvertent Privacy Memorization in\n  Multi-Modal Large Language Models", "abstract": "Multi-Modal Large Language Models (MLLMs) have exhibited remarkable\nperformance on various vision-language tasks such as Visual Question Answering\n(VQA). Despite accumulating evidence of privacy concerns associated with\ntask-relevant content, it remains unclear whether MLLMs inadvertently memorize\nprivate content that is entirely irrelevant to the training tasks. In this\npaper, we investigate how randomly generated task-irrelevant private content\ncan become spuriously correlated with downstream objectives due to partial\nmini-batch training dynamics, thus causing inadvertent memorization.\nConcretely, we randomly generate task-irrelevant watermarks into VQA\nfine-tuning images at varying probabilities and propose a novel probing\nframework to determine whether MLLMs have inadvertently encoded such content.\nOur experiments reveal that MLLMs exhibit notably different training behaviors\nin partial mini-batch settings with task-irrelevant watermarks embedded.\nFurthermore, through layer-wise probing, we demonstrate that MLLMs trigger\ndistinct representational patterns when encountering previously seen\ntask-irrelevant knowledge, even if this knowledge does not influence their\noutput during prompting. Our code is available at\nhttps://github.com/illusionhi/ProbingPrivacy.", "published": "2025-03-03 06:10:27", "link": "http://arxiv.org/abs/2503.01208v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity\n  Recognition", "abstract": "Incorrect boundary division, complex semantic representation, and differences\nin pronunciation and meaning often lead to errors in Chinese Named Entity\nRecognition(CNER). To address these issues, this paper proposes HREB-CRF\nframework: Hierarchical Reduced-bias EMA with CRF. The proposed method\namplifies word boundaries and pools long text gradients through exponentially\nfixed-bias weighted average of local and global hierarchical attention.\nExperimental results on the MSRA, Resume, and Weibo datasets show excellent in\nF1, outperforming the baseline model by 1.1\\%, 1.6\\%, and 9.8\\%. The\nsignificant improvement in F1 shows evidences of strong effectiveness and\nrobustness of approach in CNER tasks.", "published": "2025-03-03 06:31:52", "link": "http://arxiv.org/abs/2503.01217v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG", "abstract": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.", "published": "2025-03-03 06:40:21", "link": "http://arxiv.org/abs/2503.01222v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generalizable Prompt Learning of CLIP: A Brief Overview", "abstract": "Existing vision-language models (VLMs) such as CLIP have showcased an\nimpressive capability to generalize well across various downstream tasks. These\nmodels leverage the synergy between visual and textual information, enabling\nthem to understand and reason about the content present in images and text in a\nunified manner. This article provides a brief overview of CLIP based on\nfew-shot prompt learning, including experimental data and technical\ncharacteristics of some methods. The purpose of this review is to provide a\nreference for researchers who have just started their research in generalizable\nprompting of CLIP through few-shot training for classification across 15\ndatasets and also to facilitate the integration of this field by researchers in\nother downstream tasks.", "published": "2025-03-03 07:41:41", "link": "http://arxiv.org/abs/2503.01263v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ChatGPT for President! Presupposed content in politicians versus\n  GPT-generated texts", "abstract": "This study examines ChatGPT-4's capability to replicate linguistic strategies\nused in political discourse, focusing on its potential for manipulative\nlanguage generation. As large language models become increasingly popular for\ntext generation, concerns have grown regarding their role in spreading fake\nnews and propaganda. This research compares real political speeches with those\ngenerated by ChatGPT, emphasizing presuppositions (a rhetorical device that\nsubtly influences audiences by packaging some content as already known at the\nmoment of utterance, thus swaying opinions without explicit argumentation).\nUsing a corpus-based pragmatic analysis, this study assesses how well ChatGPT\ncan mimic these persuasive strategies. The findings reveal that although\nChatGPT-generated texts contain many manipulative presuppositions, key\ndifferences emerge in their frequency, form, and function compared with those\nof politicians. For instance, ChatGPT often relies on change-of-state verbs\nused in fixed phrases, whereas politicians use presupposition triggers in more\nvaried and creative ways. Such differences, however, are challenging to detect\nwith the naked eye, underscoring the potential risks posed by large language\nmodels in political and public discourse.Using a corpus-based pragmatic\nanalysis, this study assesses how well ChatGPT can mimic these persuasive\nstrategies. The findings reveal that although ChatGPT-generated texts contain\nmany manipulative presuppositions, key differences emerge in their frequency,\nform, and function compared with those of politicians. For instance, ChatGPT\noften relies on change-of-state verbs used in fixed phrases, whereas\npoliticians use presupposition triggers in more varied and creative ways. Such\ndifferences, however, are challenging to detect with the naked eye,\nunderscoring the potential risks posed by large language models in political\nand public discourse.", "published": "2025-03-03 07:48:04", "link": "http://arxiv.org/abs/2503.01269v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs", "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.", "published": "2025-03-03 08:46:22", "link": "http://arxiv.org/abs/2503.01307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in\n  Language Models", "abstract": "Knowing when to answer or refuse is crucial for safe and reliable\ndecision-making language agents. Although prior work has introduced refusal\nstrategies to boost LMs' reliability, how these models adapt their decisions to\ndifferent risk levels remains underexplored. We formalize the task of\nrisk-aware decision-making, expose critical weaknesses in existing LMs, and\npropose skill-decomposition solutions to mitigate them. Our findings show that\neven cutting-edge LMs--both regular and reasoning models--still require\nexplicit prompt chaining to handle the task effectively, revealing the\nchallenges that must be overcome to achieve truly autonomous decision-making\nagents.", "published": "2025-03-03 09:16:26", "link": "http://arxiv.org/abs/2503.01332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SRAG: Structured Retrieval-Augmented Generation for Multi-Entity\n  Question Answering over Wikipedia Graph", "abstract": "Multi-entity question answering (MEQA) poses significant challenges for large\nlanguage models (LLMs), which often struggle to consolidate scattered\ninformation across multiple documents. An example question might be \"What is\nthe distribution of IEEE Fellows among various fields of study?\", which\nrequires retrieving information from diverse sources e.g., Wikipedia pages. The\neffectiveness of current retrieval-augmented generation (RAG) methods is\nlimited by the LLMs' capacity to aggregate insights from numerous pages. To\naddress this gap, this paper introduces a structured RAG (SRAG) framework that\nsystematically organizes extracted entities into relational tables (e.g.,\ntabulating entities with schema columns like \"name\" and \"field of study\") and\nthen apply table-based reasoning techniques. Our approach decouples retrieval\nand reasoning, enabling LLMs to focus on structured data analysis rather than\nraw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA\ntasks demonstrate that SRAG significantly outperforms state-of-the-art\nlong-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.\nThe results underscore the efficacy of structuring unstructured data to enhance\nLLMs' reasoning capabilities.", "published": "2025-03-03 09:37:33", "link": "http://arxiv.org/abs/2503.01346v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning of Large Language Models via\n  Deconvolution in Subspace", "abstract": "Large language model (LLM) is considered a milestone towards achieving\nArtificial General Intelligence (AGI). With its advanced emergent capabilities,\nit adapt to a wide range of specific applications. Fine-tuning LLMs for various\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) is\nwell-known for its parameter efficiency. It can reduce the number of parameters\nneeded to fine-tune LLMs by several orders of magnitude. However, LoRA-based\napproaches encounter a significant limitation due to the bottleneck imposed by\nrank one decomposition. As the parameters count in LLMs increase, even rank one\ndecomposition might surpass the number of parameters truly necessary for\nhandling more downstream tasks. In this paper, we propose a new method for\nParameter-Efficient Fine-Tuning (PEFT) via deconvolution in subspace, dubbed as\nDCFT. We innovatively use deconvolution to complete details and enhance\nknowledge in subspace incremental matrices, and dynamically control parameters\nby adjusting the kernel size, unconstrained by rank-one decomposition.\nExtensive experiments are conducted to validate the effectiveness of DCFT.\nResults show that compared to LoRA, DCFT achieve an 8$\\times$ reduction in\nparameters, and still achieves highly impressive performance. Our code is\navailable here: https://github.com/Godz-z/DCFT.", "published": "2025-03-03 11:15:50", "link": "http://arxiv.org/abs/2503.01419v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N\n  Sampling in Early Decoding", "abstract": "Test-time scaling improves large language model performance by adding extra\ncompute during decoding. Best-of-N (BoN) sampling serves as a common scaling\ntechnique, broadening the search space for finding better solutions from the\nmodel distribution. However, traditional BoN requires N full generations,\nleading to high GPU memory overhead and time latency. Moreover, some methods\ndepend on reward models, adding computational cost and limiting domain\ngeneralization.\n  In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel\ndecoding method that avoids fully generating all samplings and eliminates the\nneed for reward models. ST-BoN introduces early sampling consistency to\nestimate the most promising sample, truncating suboptimal ones to free memory\nand accelerate inference. This pushes the sampling-efficient test-time scaling.\nCompared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by\nover 90% and time latency by 50%, while achieving comparable or even better\nperformance across reasoning and open-ended domains.", "published": "2025-03-03 11:21:01", "link": "http://arxiv.org/abs/2503.01422v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Hypothesis to Publication: A Comprehensive Survey of AI-Driven\n  Research Support Systems", "abstract": "Research is a fundamental process driving the advancement of human\ncivilization, yet it demands substantial time and effort from researchers. In\nrecent years, the rapid development of artificial intelligence (AI)\ntechnologies has inspired researchers to explore how AI can accelerate and\nenhance research. To monitor relevant advancements, this paper presents a\nsystematic review of the progress in this domain. Specifically, we organize the\nrelevant studies into three main categories: hypothesis formulation, hypothesis\nvalidation, and manuscript publication. Hypothesis formulation involves\nknowledge synthesis and hypothesis generation. Hypothesis validation includes\nthe verification of scientific claims, theorem proving, and experiment\nvalidation. Manuscript publication encompasses manuscript writing and the peer\nreview process. Furthermore, we identify and discuss the current challenges\nfaced in these areas, as well as potential future directions for research.\nFinally, we also offer a comprehensive overview of existing benchmarks and\ntools across various domains that support the integration of AI into the\nresearch process. We hope this paper serves as an introduction for beginners\nand fosters future research. Resources have been made publicly available at\nhttps://github.com/zkzhou126/AI-for-Research.", "published": "2025-03-03 11:27:13", "link": "http://arxiv.org/abs/2503.01424v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Structural Deep Encoding for Table Question Answering", "abstract": "Although Transformers-based architectures excel at processing textual\ninformation, their naive adaptation for tabular data often involves flattening\nthe table structure. This simplification can lead to the loss of essential\ninter-dependencies between rows, columns, and cells, while also posing\nscalability challenges for large tables. To address these issues, prior works\nhave explored special tokens, structured embeddings, and sparse attention\npatterns. In this paper, we conduct a comprehensive analysis of tabular\nencoding techniques, which highlights the crucial role of attention sparsity in\npreserving structural information of tables. We also introduce a set of novel\nsparse attention mask designs for tabular data, that not only enhance\ncomputational efficiency but also preserve structural integrity, leading to\nbetter overall performance.", "published": "2025-03-03 12:16:43", "link": "http://arxiv.org/abs/2503.01457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Data: Towards Better Performing Domain-Specific Small\n  Language Models", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed\non domain-specific data has shown significant promise. However, commercial use\nof such LLMs is limited by the high computational cost required for their\ndeployment at scale. On the other hand, small Language Models (LMs) are much\nmore cost effective but have subpar performance in a similar setup. This paper\npresents our approach to finetuning a small LM, that reaches high accuracy in\nmultiple choice question answering task. We achieve this by improving data\nquality at each stage of the LM training pipeline. In particular, we start with\ndata structuring resulting in extraction of compact, semantically meaningful\ntext chunks used by a retriever. This allows more efficient knowledge digestion\nby the LM. Further, we improve the retrieved context by training a lightweight\nChunk Re-Ranker (CRR) that generates more accurate relative relevance chunk\nscores. Finally, we improve the model generalization ability by merging the\nmodels fine-tuned with different parameters on different data subsets. We\npresent detailed procedure descriptions, and corresponding experimental\nfindings that show the improvements of each one of the proposed techniques.", "published": "2025-03-03 12:19:12", "link": "http://arxiv.org/abs/2503.01464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic\n  Implicit Toxic Language", "abstract": "The rapid development of large language models (LLMs) gives rise to ethical\nconcerns about their performance, while opening new avenues for developing\ntoxic language detection techniques. However, LLMs' unethical output and their\ncapability of detecting toxicity have primarily been tested on language data\nthat do not demand complex meaning inference, such as the biased associations\nof 'he' with programmer and 'she' with household. Nowadays toxic language\nadopts a much more creative range of implicit forms, thanks to advanced\ncensorship. In this study, we collect authentic toxic interactions that evade\nonline censorship and that are verified by human annotators as inference\nintensive. To evaluate and improve LLMs' reasoning of the authentic implicit\ntoxic language, we propose a new prompting method, Pragmatic Inference Chain\n(PIC), drawn on interdisciplinary findings from cognitive science and\nlinguistics. The PIC prompting significantly improves the success rate of\nGPT-4o, Llama-3.1-70B-Instruct, and DeepSeek-v2.5 in identifying implicit toxic\nlanguage, compared to both direct prompting and Chain-of-Thought. In addition,\nit also facilitates the models to produce more explicit and coherent reasoning\nprocesses, hence can potentially be generalized to other inference-intensive\ntasks, e.g., understanding humour and metaphors.", "published": "2025-03-03 13:51:05", "link": "http://arxiv.org/abs/2503.01539v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Large Language Model Pruning using Neuron Semantic\n  Attribution", "abstract": "Model pruning technique is vital for accelerating large language models by\nreducing their size and computational requirements. However, the\ngeneralizability of existing pruning methods across diverse datasets and tasks\nremains unclear. Thus, we conduct extensive evaluations on 24 datasets and 4\ntasks using popular pruning methods. Based on these evaluations, we find and\nthen investigate that calibration set greatly affect the performance of pruning\nmethods. In addition, we surprisingly find a significant performance drop of\nexisting pruning methods in sentiment classification tasks. To understand the\nlink between performance drop and pruned neurons, we propose Neuron Semantic\nAttribution, which learns to associate each neuron with specific semantics.\nThis method first makes the unpruned neurons of LLMs explainable.", "published": "2025-03-03 13:52:17", "link": "http://arxiv.org/abs/2503.01542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "None of the Above, Less of the Right: Parallel Patterns between Humans\n  and LLMs on Multi-Choice Questions Answering", "abstract": "Multiple-choice exam questions with \"None of the above\" (NA) options have\nbeen extensively studied in educational testing, in which existing research\nsuggests that they better assess true knowledge. However, their impact on Large\nLanguage Models (LLMs) evaluation remains underexplored. Through systematic\nexperiments with 28 LLMs on the MMLU benchmark, we examine how NA options\naffect model performance and confidence calibration. Our analysis reveals that\nNA options, when used as the correct answer, lead to a consistent 30-50\\%\nperformance drop across models regardless of scale--suggesting that LLMs lack\nthe meta-cognitive ability to systematically evaluate and reject all given\noptions when none are correct. This degradation shows strong domain dependence,\nwith minimal impact on mathematical reasoning (14.6\\% drop) but severe effects\non tasks requiring uncertainty handling like business ethics (48.1\\% drop). Our\nresults highlight important implications for benchmark design and raise\nquestions about LLMs' ability to handle uncertainty in real-world applications.", "published": "2025-03-03 13:55:29", "link": "http://arxiv.org/abs/2503.01550v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Lost in Moderation: How Commercial Content Moderation APIs Over- and\n  Under-Moderate Group-Targeted Hate Speech and Linguistic Variations", "abstract": "Commercial content moderation APIs are marketed as scalable solutions to\ncombat online hate speech. However, the reliance on these APIs risks both\nsilencing legitimate speech, called over-moderation, and failing to protect\nonline platforms from harmful speech, known as under-moderation. To assess such\nrisks, this paper introduces a framework for auditing black-box NLP systems.\nUsing the framework, we systematically evaluate five widely used commercial\ncontent moderation APIs. Analyzing five million queries based on four datasets,\nwe find that APIs frequently rely on group identity terms, such as ``black'',\nto predict hate speech. While OpenAI's and Amazon's services perform slightly\nbetter, all providers under-moderate implicit hate speech, which uses codified\nmessages, especially against LGBTQIA+ individuals. Simultaneously, they\nover-moderate counter-speech, reclaimed slurs and content related to Black,\nLGBTQIA+, Jewish, and Muslim people. We recommend that API providers offer\nbetter guidance on API implementation and threshold setting and more\ntransparency on their APIs' limitations.\n  Warning: This paper contains offensive and hateful terms and concepts. We\nhave chosen to reproduce these terms for reasons of transparency.", "published": "2025-03-03 14:56:47", "link": "http://arxiv.org/abs/2503.01623v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Automated Annotation of Evolving Corpora for Augmenting Longitudinal\n  Network Data: A Framework Integrating Large Language Models and Expert\n  Knowledge", "abstract": "Longitudinal network data are essential for analyzing political, economic,\nand social systems and processes. In political science, these datasets are\noften generated through human annotation or supervised machine learning applied\nto evolving corpora. However, as semantic contexts shift over time, inferring\ndynamic interaction types on emerging issues among a diverse set of entities\nposes significant challenges, particularly in maintaining timely and consistent\nannotations. This paper presents the Expert-Augmented LLM Annotation (EALA)\napproach, which leverages Large Language Models (LLMs) in combination with\nhistorically annotated data and expert-constructed codebooks to extrapolate and\nextend datasets into future periods. We evaluate the performance and\nreliability of EALA using a dataset of climate negotiations. Our findings\ndemonstrate that EALA effectively predicts nuanced interactions between\nnegotiation parties and captures the evolution of topics over time. At the same\ntime, we identify several limitations inherent to LLM-based annotation,\nhighlighting areas for further improvement. Given the wide availability of\ncodebooks and annotated datasets, EALA holds substantial promise for advancing\nresearch in political science and beyond.", "published": "2025-03-03 15:46:01", "link": "http://arxiv.org/abs/2503.01672v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Using (Not so) Large Language Models for Generating Simulation Models in\n  a Formal DSL -- A Study on Reaction Networks", "abstract": "Formal languages are an integral part of modeling and simulation. They allow\nthe distillation of knowledge into concise simulation models amenable to\nautomatic execution, interpretation, and analysis. However, the arguably most\nhumanly accessible means of expressing models is through natural language,\nwhich is not easily interpretable by computers. Here, we evaluate how a Large\nLanguage Model (LLM) might be used for formalizing natural language into\nsimulation models. Existing studies only explored using very large LLMs, like\nthe commercial GPT models, without fine-tuning model weights. To close this\ngap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned\nto translate natural language descriptions to reaction network models in a\ndomain-specific language, offering a self-hostable, compute-, and memory\nefficient alternative. To this end, we develop a synthetic data generator to\nserve as the basis for fine-tuning and evaluation. Our quantitative evaluation\nshows that our fine-tuned Mistral model can recover the ground truth simulation\nmodel in up to 84.5% of cases. In addition, our small-scale user study\ndemonstrates the model's practical potential for one-time generation as well as\ninteractive modeling in various domains. While promising, in its current form,\nthe fine-tuned small LLM cannot catch up with large LLMs. We conclude that\nhigher-quality training data are required, and expect future small and\nopen-source LLMs to offer new opportunities.", "published": "2025-03-03 15:48:01", "link": "http://arxiv.org/abs/2503.01675v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When an LLM is apprehensive about its answers -- and when its\n  uncertainty is justified", "abstract": "Uncertainty estimation is crucial for evaluating Large Language Models\n(LLMs), particularly in high-stakes domains where incorrect answers result in\nsignificant consequences. Numerous approaches consider this problem, while\nfocusing on a specific type of uncertainty, ignoring others. We investigate\nwhat estimates, specifically token-wise entropy and model-as-judge (MASJ),\nwould work for multiple-choice question-answering tasks for different question\ntopics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of\ndifferent sizes from 1.5B to 72B and $14$ topics. While MASJ performs similarly\nto a random error predictor, the response entropy predicts model error in\nknowledge-dependent domains and serves as an effective indicator of question\ndifficulty: for biology ROC AUC is $0.73$. This correlation vanishes for the\nreasoning-dependent domain: for math questions ROC-AUC is $0.55$. More\nprincipally, we found out that the entropy measure required a reasoning amount.\nThus, data-uncertainty related entropy should be integrated within uncertainty\nestimates frameworks, while MASJ requires refinement. Moreover, existing\nMMLU-Pro samples are biased, and should balance required amount of reasoning\nfor different subdomains to provide a more fair assessment of LLMs performance.", "published": "2025-03-03 16:03:46", "link": "http://arxiv.org/abs/2503.01688v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment", "abstract": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks.", "published": "2025-03-03 16:24:36", "link": "http://arxiv.org/abs/2503.01711v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia", "abstract": "Human readers can efficiently comprehend scrambled words, a phenomenon known\nas Typoglycemia, primarily by relying on word form; if word form alone is\ninsufficient, they further utilize contextual cues for interpretation. While\nadvanced large language models (LLMs) exhibit similar abilities, the underlying\nmechanisms remain unclear. To investigate this, we conduct controlled\nexperiments to analyze the roles of word form and contextual information in\nsemantic reconstruction and examine LLM attention patterns. Specifically, we\nfirst propose SemRecScore, a reliable metric to quantify the degree of semantic\nreconstruction, and validate its effectiveness. Using this metric, we study how\nword form and contextual information influence LLMs' semantic reconstruction\nability, identifying word form as the core factor in this process. Furthermore,\nwe analyze how LLMs utilize word form and find that they rely on specialized\nattention heads to extract and process word form information, with this\nmechanism remaining stable across varying levels of word scrambling. This\ndistinction between LLMs' fixed attention patterns primarily focused on word\nform and human readers' adaptive strategy in balancing word form and contextual\ninformation provides insights into enhancing LLM performance by incorporating\nhuman-like, context-aware mechanisms.", "published": "2025-03-03 16:31:45", "link": "http://arxiv.org/abs/2503.01714v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do GFlowNets Transfer? Case Study on the Game of 24/42", "abstract": "Generating diverse solutions is key to human-like reasoning, yet\nautoregressive language models focus on single accurate responses, limiting\ncreativity. GFlowNets optimize solution generation as a flow network, promising\ngreater diversity. Our case study shows their limited zero-shot transferability\nby fine-tuning small and medium-sized large language models on the Game of 24\nand testing them on the Game of 42 datasets. Results revealed that GFlowNets\nstruggle to maintain solution diversity and accuracy, highlighting key\nlimitations in their cross-task generalization and the need for future research\nin improved transfer learning capabilities.", "published": "2025-03-03 18:43:25", "link": "http://arxiv.org/abs/2503.01819v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Rotary Outliers and Rotary Offset Features in Large Language Models", "abstract": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings. Our\nanalysis reveals consistent patterns within the same model across layers and\nattention heads and across different models and architectures. We present and\napply analysis techniques and show how the queries and keys use RoPE to\nconstruct various attention patterns, including attention sinks. We find and\nanalyze outliers across models in queries and keys and find that they are\nlikely to be found in rotary features with partial cycles. We derive bounds\nthat tell us what rotary frequencies are likely to be selected as outlier\nfeatures and at what minimum angle the query-key rotary pairs in these features\ntend to be above and verify the bounds empirically with models of significant\narchitectural differences.", "published": "2025-03-03 18:55:09", "link": "http://arxiv.org/abs/2503.01832v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom", "abstract": "Distilling advanced Large Language Models' instruction-following capabilities\ninto smaller models using a selected subset has become a mainstream approach in\nmodel training. While existing synthetic instruction data selection strategies\nrely mainly on single-dimensional signals (i.e., reward scores, model\nperplexity), they fail to capture the complexity of instruction-following\nacross diverse fields. Therefore, we investigate more diverse signals to\ncapture comprehensive instruction-response pair characteristics and propose\nthree foundational metrics that leverage Multi-LLM wisdom, informed by (1)\ndiverse LLM responses and (2) reward model assessment. Building upon base\nmetrics, we propose CrowdSelect, an integrated metric incorporating a\nclustering-based approach to maintain response diversity. Our comprehensive\nexperiments demonstrate that our foundation metrics consistently improve\nperformance across 4 base models on MT-bench and Arena-Hard. CrowdSelect,\nefficiently incorporating all metrics, achieves state-of-the-art performance in\nboth Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and\n11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring\nvaluable insights for future research in this direction. Code are available at\nhttps://github.com/listentm/crowdselect.", "published": "2025-03-03 18:56:44", "link": "http://arxiv.org/abs/2503.01836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets and lack of error correction mechanisms during multi-turn\nclarification. We present AskToAct, which addresses these challenges by\nexploiting the structural mapping between queries and their tool invocation\nsolutions. Our key insight is that tool parameters naturally represent explicit\nuser intents. By systematically removing key parameters from queries while\nretaining them as ground truth, we enable automated construction of\nhigh-quality training data. We further enhance model robustness by fine-tuning\non error-correction augmented data using selective masking mechanism, enabling\ndynamic error detection during clarification interactions. Comprehensive\nexperiments demonstrate that AskToAct significantly outperforms existing\napproaches, achieving above 79% accuracy in recovering critical unspecified\nintents and enhancing clarification efficiency by an average of 48.34% while\nmaintaining high accuracy in tool invocation. Our framework exhibits robust\nperformance across varying complexity levels and successfully generalizes to\nentirely unseen APIs without additional training, achieving performance\ncomparable to GPT-4 with substantially fewer computational resources.", "published": "2025-03-03 12:55:49", "link": "http://arxiv.org/abs/2503.01940v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing the Safety of Japanese Large Language Models in\n  Stereotype-Triggering Prompts", "abstract": "In recent years, Large Language Models have attracted growing interest for\ntheir significant potential, though concerns have rapidly emerged regarding\nunsafe behaviors stemming from inherent stereotypes and biases. Most research\non stereotypes in LLMs has primarily relied on indirect evaluation setups, in\nwhich models are prompted to select between pairs of sentences associated with\nparticular social groups. Recently, direct evaluation methods have emerged,\nexamining open-ended model responses to overcome limitations of previous\napproaches, such as annotator biases. Most existing studies have focused on\nEnglish-centric LLMs, whereas research on non-English models, particularly\nJapanese, remains sparse, despite the growing development and adoption of these\nmodels. This study examines the safety of Japanese LLMs when responding to\nstereotype-triggering prompts in direct setups. We constructed 3,612 prompts by\ncombining 301 social group terms, categorized by age, gender, and other\nattributes, with 12 stereotype-inducing templates in Japanese. Responses were\nanalyzed from three foundational models trained respectively on Japanese,\nEnglish, and Chinese language. Our findings reveal that LLM-jp, a Japanese\nnative model, exhibits the lowest refusal rate and is more likely to generate\ntoxic and negative responses compared to other models. Additionally, prompt\nformat significantly influence the output of all models, and the generated\nresponses include exaggerated reactions toward specific social groups, varying\nacross models. These findings underscore the insufficient ethical safety\nmechanisms in Japanese LLMs and demonstrate that even high-accuracy models can\nproduce biased outputs when processing Japanese-language prompts. We advocate\nfor improving safety mechanisms and bias mitigation strategies in Japanese\nLLMs, contributing to ongoing discussions on AI ethics beyond linguistic\nboundaries.", "published": "2025-03-03 19:00:00", "link": "http://arxiv.org/abs/2503.01947v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs", "abstract": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.", "published": "2025-03-03 19:26:04", "link": "http://arxiv.org/abs/2503.02003v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Mind the (Belief) Gap: Group Identity in the World of LLMs", "abstract": "Social biases and belief-driven behaviors can significantly impact Large\nLanguage Models (LLMs) decisions on several tasks. As LLMs are increasingly\nused in multi-agent systems for societal simulations, their ability to model\nfundamental group psychological characteristics remains critical yet\nunder-explored. In this study, we present a multi-agent framework that\nsimulates belief congruence, a classical group psychology theory that plays a\ncrucial role in shaping societal interactions and preferences. Our findings\nreveal that LLMs exhibit amplified belief congruence compared to humans, across\ndiverse contexts. We further investigate the implications of this behavior on\ntwo downstream tasks: (1) misinformation dissemination and (2) LLM learning,\nfinding that belief congruence in LLMs increases misinformation dissemination\nand impedes learning. To mitigate these negative impacts, we propose strategies\ninspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global\ncitizenship framework. Our results show that the best strategies reduce\nmisinformation dissemination by up to 37% and enhance learning by 11%. Bridging\nsocial psychology and AI, our work provides insights to navigate real-world\ninteractions using LLMs while addressing belief-driven biases.", "published": "2025-03-03 19:50:52", "link": "http://arxiv.org/abs/2503.02016v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CareerBERT: Matching Resumes to ESCO Jobs in a Shared Embedding Space\n  for Generic Job Recommendations", "abstract": "The rapidly evolving labor market, driven by technological advancements and\neconomic shifts, presents significant challenges for traditional job matching\nand consultation services. In response, we introduce an advanced support tool\nfor career counselors and job seekers based on CareerBERT, a novel approach\nthat leverages the power of unstructured textual data sources, such as resumes,\nto provide more accurate and comprehensive job recommendations. In contrast to\nprevious approaches that primarily focus on job recommendations based on a\nfixed set of concrete job advertisements, our approach involves the creation of\na corpus that combines data from the European Skills, Competences, and\nOccupations (ESCO) taxonomy and EURopean Employment Services (EURES) job\nadvertisements, ensuring an up-to-date and well-defined representation of\ngeneral job titles in the labor market. Our two-step evaluation approach,\nconsisting of an application-grounded evaluation using EURES job advertisements\nand a human-grounded evaluation using real-world resumes and Human Resources\n(HR) expert feedback, provides a comprehensive assessment of CareerBERT's\nperformance. Our experimental results demonstrate that CareerBERT outperforms\nboth traditional and state-of-the-art embedding approaches while showing robust\neffectiveness in human expert evaluations. These results confirm the\neffectiveness of CareerBERT in supporting career consultants by generating\nrelevant job recommendations based on resumes, ultimately enhancing the\nefficiency of job consultations and expanding the perspectives of job seekers.\nThis research contributes to the field of NLP and job recommendation systems,\noffering valuable insights for both researchers and practitioners in the domain\nof career consulting and job matching.", "published": "2025-03-03 21:14:14", "link": "http://arxiv.org/abs/2503.02056v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Provable Benefits of Task-Specific Prompts for In-context Learning", "abstract": "The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.", "published": "2025-03-03 22:37:03", "link": "http://arxiv.org/abs/2503.02102v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated\n  Information on Retrieval-Augmented Generation", "abstract": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures temporal knowledge evolution in real-world\nfacts. Through comprehensive experiments, we reveal that outdated information\nsignificantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG.", "published": "2025-03-03 06:54:05", "link": "http://arxiv.org/abs/2503.04800v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring and Evaluating Multimodal Knowledge Reasoning Consistency of\n  Multimodal Large Language Models", "abstract": "In recent years, multimodal large language models (MLLMs) have achieved\nsignificant breakthroughs, enhancing understanding across text and vision.\nHowever, current MLLMs still face challenges in effectively integrating\nknowledge across these modalities during multimodal knowledge reasoning,\nleading to inconsistencies in reasoning outcomes. To systematically explore\nthis issue, we propose four evaluation tasks and construct a new dataset. We\nconduct a series of experiments on this dataset to analyze and compare the\nextent of consistency degradation in multimodal knowledge reasoning within\nMLLMs. Based on the experimental results, we identify factors contributing to\nthe observed degradation in consistency. Our research provides new insights\ninto the challenges of multimodal knowledge reasoning and offers valuable\nguidance for future efforts aimed at improving MLLMs.", "published": "2025-03-03 09:01:51", "link": "http://arxiv.org/abs/2503.04801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The order in speech disorder: a scoping review of state of the art\n  machine learning methods for clinical speech classification", "abstract": "Background:Speech patterns have emerged as potential diagnostic markers for\nconditions with varying etiologies. Machine learning (ML) presents an\nopportunity to harness these patterns for accurate disease diagnosis.\n  Objective: This review synthesized findings from studies exploring ML's\ncapability in leveraging speech for the diagnosis of neurological, laryngeal\nand mental disorders.\n  Methods: A systematic examination of 564 articles was conducted with 91\narticles included in the study, which encompassed a wide spectrum of\nconditions, ranging from voice pathologies to mental and neurological\ndisorders. Methods for speech classifications were assessed based on the\nrelevant studies and scored between 0-10 based on the reported diagnostic\naccuracy of their ML models.\n  Results: High diagnostic accuracies were consistently observed for laryngeal\ndisorders, dysarthria, and changes related to speech in Parkinsons disease.\nThese findings indicate the robust potential of speech as a diagnostic tool.\nDisorders like depression, schizophrenia, mild cognitive impairment and\nAlzheimers dementia also demonstrated high accuracies, albeit with some\nvariability across studies. Meanwhile, disorders like OCD and autism\nhighlighted the need for more extensive research to ascertain the relationship\nbetween speech patterns and the respective conditions.\n  Conclusion: ML models utilizing speech patterns demonstrate promising\npotential in diagnosing a range of mental, laryngeal, and neurological\ndisorders. However, the efficacy varies across conditions, and further research\nis needed. The integration of these models into clinical practice could\npotentially revolutionize the evaluation and diagnosis of a number of different\nmedical conditions.", "published": "2025-03-03 11:33:02", "link": "http://arxiv.org/abs/2503.04802v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What do Large Language Models Say About Animals? Investigating Risks of\n  Animal Harm in Generated Text", "abstract": "As machine learning systems become increasingly embedded in human society,\ntheir impact on the natural world continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present the\nAnimal Harm Assessment (AHA), a novel evaluation of risks of animal harm in\nLLM-generated text. Our dataset comprises 1,850 curated questions from Reddit\npost titles and 2,500 synthetic questions based on 50 animal categories (e.g.,\ncats, reptiles) and 50 ethical scenarios, with further 70-30 public-private\nsplit. Scenarios include open-ended questions about how to treat animals,\npractical scenarios with potential animal harm, and willingness-to-pay measures\nfor the prevention of animal harm. Using the LLM-as-a-judge framework, answers\nare evaluated for their potential to increase or decrease harm, and evaluations\nare debiased for the tendency to judge their own outputs more favorably. We\nshow that AHA produces meaningful evaluation results when applied to frontier\nLLMs, revealing significant differences between models, animal categories,\nscenarios, and subreddits. We conclude with future directions for technical\nresearch and the challenges of building evaluations on complex social and moral\ntopics.", "published": "2025-03-03 15:32:18", "link": "http://arxiv.org/abs/2503.04804v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Scientific Reasoning: Assessment of Multimodal Generative LLMs", "abstract": "Large language models (LLMs) can answer questions and reason about complex\ntasks, also from the scientific domain. We assess several multimodal LLMs\n(MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with\nlittle context, and the highest textual similarity to human explanations with\nricher context. Adapter-tuning of smaller MLLMs did not lead to any reliable\nperformance. Training from Gemini outputs consistently underperformed training\nfrom the original data.", "published": "2025-03-03 00:07:22", "link": "http://arxiv.org/abs/2503.01064v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System", "abstract": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.", "published": "2025-03-03 00:14:34", "link": "http://arxiv.org/abs/2503.01066v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "SolBench: A Dataset and Benchmark for Evaluating Functional Correctness\n  in Solidity Code Completion and Repair", "abstract": "Smart contracts are crucial programs on blockchains, and their immutability\npost-deployment makes functional correctness vital. Despite progress in code\ncompletion models, benchmarks for Solidity, the primary smart contract\nlanguage, are lacking. Existing metrics like BLEU do not adequately assess the\nfunctional correctness of generated smart contracts. To fill this gap, we\nintroduce SolBench, a benchmark for evaluating the functional correctness of\nSolidity smart contracts generated by code completion models. SolBench includes\n4,178 functions from 1,155 Ethereum-deployed contracts. Testing advanced models\nrevealed challenges in generating correct code without context, as Solidity\nfunctions rely on context-defined variables and interfaces. To address this, we\npropose a Retrieval-Augmented Code Repair framework. In this framework, an\nexecutor verifies functional correctness, and if necessary, an LLM repairs the\ncode using retrieved snippets informed by executor traces. We conduct a\ncomprehensive evaluation of both closed-source and open-source LLMs across\nvarious model sizes and series to assess their performance in smart contract\ncompletion. The results show that code repair and retrieval techniques\neffectively enhance the correctness of smart contract completion while reducing\ncomputational costs.", "published": "2025-03-03 01:55:20", "link": "http://arxiv.org/abs/2503.01098v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "ReaderLM-v2: Small Language Model for HTML to Markdown and JSON", "abstract": "We present ReaderLM-v2, a compact 1.5 billion parameter language model\ndesigned for efficient web content extraction. Our model processes documents up\nto 512K tokens, transforming messy HTML into clean Markdown or JSON formats\nwith high accuracy -- making it an ideal tool for grounding large language\nmodels. The model's effectiveness results from two key innovations: (1) a\nthree-stage data synthesis pipeline that generates high quality, diverse\ntraining data by iteratively drafting, refining, and critiquing web content\nextraction; and (2) a unified training framework combining continuous\npre-training with multi-objective optimization. Intensive evaluation\ndemonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger\nmodels by 15-20\\% on carefully curated benchmarks, particularly excelling at\ndocuments exceeding 100K tokens, while maintaining significantly lower\ncomputational requirements.", "published": "2025-03-03 03:57:04", "link": "http://arxiv.org/abs/2503.01151v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking\n  Dynamics", "abstract": "The recent wave of audio foundation models (FMs) could provide new\ncapabilities for conversational modeling. However, there have been limited\nefforts to evaluate these audio FMs comprehensively on their ability to have\nnatural and interactive conversations. To engage in meaningful conversation\nwith the end user, we would want the FMs to additionally perform a fluent\nsuccession of turns without too much overlapping speech or long stretches of\nsilence. Inspired by this, we ask whether the recently proposed audio FMs can\nunderstand, predict, and perform turn-taking events? To answer this, we propose\na novel evaluation protocol that can assess spoken dialog system's turn-taking\ncapabilities using a supervised model as a judge that has been trained to\npredict turn-taking events in human-human conversations. Using this protocol,\nwe present the first comprehensive user study that evaluates existing spoken\ndialogue systems on their ability to perform turn-taking events and reveal many\ninteresting insights, such as they sometimes do not understand when to speak\nup, can interrupt too aggressively and rarely backchannel. We further evaluate\nmultiple open-source and proprietary audio FMs accessible through APIs on\ncarefully curated test benchmarks from Switchboard to measure their ability to\nunderstand and predict turn-taking events and identify significant room for\nimprovement. We will open source our evaluation platform to promote the\ndevelopment of advanced conversational AI systems.", "published": "2025-03-03 04:46:04", "link": "http://arxiv.org/abs/2503.01174v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Same Question, Different Words: A Latent Adversarial Framework for\n  Prompt Robustness", "abstract": "Insensitivity to semantically-preserving variations of prompts (paraphrases)\nis crucial for reliable behavior and real-world deployment of large language\nmodels. However, language models exhibit significant performance degradation\nwhen faced with semantically equivalent but differently phrased prompts, and\nexisting solutions either depend on trial-and-error prompt engineering or\nrequire computationally expensive inference-time algorithms. In this study,\nbuilt on the key insight that worst-case prompts exhibit a drift in embedding\nspace, we present Latent Adversarial Paraphrasing (LAP), a dual-loop\nadversarial framework: the inner loop trains a learnable perturbation to serve\nas a \"latent continuous paraphrase\" while preserving semantics through\nLagrangian regulation, and the outer loop optimizes the language model\nparameters on these perturbations. We conduct extensive experiments to\ndemonstrate the effectiveness of LAP across multiple LLM architectures on the\nRobustAlpaca benchmark with a 0.5%-4% absolution improvement on worst-case\nwin-rate compared with vanilla supervised fine-tuning.", "published": "2025-03-03 09:36:50", "link": "http://arxiv.org/abs/2503.01345v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark", "abstract": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments.", "published": "2025-03-03 10:10:30", "link": "http://arxiv.org/abs/2503.01372v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph\n  Question Answering", "abstract": "Question answering (QA) requires accurately aligning user questions with\nstructured queries, a process often limited by the scarcity of high-quality\nquery-natural language (Q-NL) pairs. To overcome this, we present Q-NL\nVerifier, an approach to generating high-quality synthetic pairs of queries and\nNL translations. Our approach relies on large language models (LLMs) to\ngenerate semantically precise natural language paraphrases of structured\nqueries. Building on these synthetic Q-NL pairs, we introduce a learned\nverifier component that automatically determines whether a generated paraphrase\nis semantically equivalent to the original query. Our experiments with the\nwell-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to\nparaphrases from other models and even human-authored translations. Our\napproach strongly aligns with human judgments across varying query complexities\nand outperforms existing NLP metrics in assessing semantic correctness. We also\nintegrate the verifier into QA pipelines, showing that verifier-filtered\nsynthetic data has significantly higher quality in terms of translation\ncorrectness and enhances NL to Q translation accuracy. Lastly, we release an\nupdated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL\npairs and verifier scores, offering a new resource for robust and scalable QA.", "published": "2025-03-03 10:28:24", "link": "http://arxiv.org/abs/2503.01385v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Geo-Semantic-Parsing: AI-powered geoparsing by traversing semantic\n  knowledge graphs", "abstract": "Online social networks convey rich information about geospatial facets of\nreality. However in most cases, geographic information is not explicit and\nstructured, thus preventing its exploitation in real-time applications. We\naddress this limitation by introducing a novel geoparsing and geotagging\ntechnique called Geo-Semantic-Parsing (GSP). GSP identifies location references\nin free text and extracts the corresponding geographic coordinates. To reach\nthis goal, we employ a semantic annotator to identify relevant portions of the\ninput text and to link them to the corresponding entity in a knowledge graph.\nThen, we devise and experiment with several efficient strategies for traversing\nthe knowledge graph, thus expanding the available set of information for the\ngeoparsing task. Finally, we exploit all available information for learning a\nregression model that selects the best entity with which to geotag the input\ntext. We evaluate GSP on a well-known reference dataset including almost 10k\nevent-related tweets, achieving $F1=0.66$. We extensively compare our results\nwith those of 2 baselines and 3 state-of-the-art geoparsing techniques,\nachieving the best performance. On the same dataset, competitors obtain $F1\n\\leq 0.55$. We conclude by providing in-depth analyses of our results, showing\nthat the overall superior performance of GSP is mainly due to a large\nimprovement in recall, with respect to existing techniques.", "published": "2025-03-03 10:30:23", "link": "http://arxiv.org/abs/2503.01386v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese\n  Language", "abstract": "Neural networks have significantly advanced AI applications, yet their\nreal-world adoption remains constrained by high computational demands, hardware\nlimitations, and accessibility challenges. In image captioning, many\nstate-of-the-art models have achieved impressive performances while relying on\nresource-intensive architectures. This made them impractical for deployment on\nresource-constrained devices. This limitation is particularly noticeable for\napplications involving low-resource languages. We demonstrate the case of image\ncaptioning in Assamese language, where lack of effective, scalable systems can\nrestrict the accessibility of AI-based solutions for native Assamese speakers.\nThis work presents AC-Lite, a computationally efficient model for image\ncaptioning in low-resource Assamese language. AC-Lite reduces computational\nrequirements by replacing computation-heavy visual feature extractors like\nFasterRCNN with lightweight ShuffleNetv2x1.5. Additionally, Gated Recurrent\nUnits (GRUs) are used as the caption decoder to further reduce computational\ndemands and model parameters. Furthermore, the integration of bilinear\nattention enhances the model's overall performance. AC-Lite can operate on edge\ndevices, thereby eliminating the need for computation on remote servers. The\nproposed AC-Lite model achieves 82.3 CIDEr score on the COCO-AC dataset with\n1.098 GFLOPs and 25.65M parameters.", "published": "2025-03-03 12:07:52", "link": "http://arxiv.org/abs/2503.01453v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Widening The Distillation Bottleneck for Reasoning Models", "abstract": "Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown\nremarkable reasoning capabilities by scaling test-time compute and generating\nlong Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated\ndata--is a straightforward yet effective method to enhance the reasoning\nabilities of smaller models, but faces a critical bottleneck: we found that\ndistilled long CoT data poses learning difficulty for small models and leads to\nthe inheritance of biases (i.e. over-thinking) when using Supervised\nFine-tuning(SFT) and Reinforcement Learning(RL) methods. To alleviate this\nbottleneck, we propose constructing tree-based CoT data from scratch via Monte\nCarlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches,\nincluding Thoughts Length Balance, Fine-grained DPO, and Joint Post-training\nObjective, to enhance SFT and RL on the construted data.", "published": "2025-03-03 12:17:36", "link": "http://arxiv.org/abs/2503.01461v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction", "abstract": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.", "published": "2025-03-03 12:37:34", "link": "http://arxiv.org/abs/2503.01478v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "abstract": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.", "published": "2025-03-03 13:08:00", "link": "http://arxiv.org/abs/2503.01496v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional Reasoning with Transformers, RNNs, and Chain of Thought", "abstract": "We study and compare the expressive power of transformers, RNNs, and\ntransformers with chain of thought tokens on a simple and natural class of\nproblems we term Compositional Reasoning Questions (CRQ). This family captures\nproblems like evaluating Boolean formulas and multi-step word problems.\nAssuming standard hardness assumptions from circuit complexity and\ncommunication complexity, we prove that none of these three architectures is\ncapable of solving CRQs unless some hyperparameter (depth, embedding dimension,\nand number of chain of thought tokens, respectively) grows with the size of the\ninput. We also provide a construction for each architecture that solves CRQs.\nFor transformers, our construction uses depth that is logarithmic in the\nproblem size. For RNNs, logarithmic embedding dimension is necessary and\nsufficient, so long as the inputs are provided in a certain order. (Otherwise,\na linear dimension is necessary). For transformers with chain of thought, our\nconstruction uses $n$ CoT tokens. These results show that, while CRQs are\ninherently hard, there are several different ways for language models to\novercome this hardness. Even for a single class of problems, each architecture\nhas strengths and weaknesses, and none is strictly better than the others.", "published": "2025-03-03 13:52:45", "link": "http://arxiv.org/abs/2503.01544v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection", "abstract": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.", "published": "2025-03-03 14:26:51", "link": "http://arxiv.org/abs/2503.01586v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain\n  Question Answering", "abstract": "Large language models have recently pushed open domain question answering\n(ODQA) to new frontiers. However, prevailing retriever-reader pipelines often\ndepend on multiple rounds of prompt level instructions, leading to high\ncomputational overhead, instability, and suboptimal retrieval coverage. In this\npaper, we propose EmbQA, an embedding-level framework that alleviates these\nshortcomings by enhancing both the retriever and the reader. Specifically, we\nrefine query representations via lightweight linear layers under an\nunsupervised contrastive learning objective, thereby reordering retrieved\npassages to highlight those most likely to contain correct answers.\nAdditionally, we introduce an exploratory embedding that broadens the model's\nlatent semantic space to diversify candidate generation and employs an\nentropy-based selection mechanism to choose the most confident answer\nautomatically. Extensive experiments across three open-source LLMs, three\nretrieval methods, and four ODQA benchmarks demonstrate that EmbQA\nsubstantially outperforms recent baselines in both accuracy and efficiency.", "published": "2025-03-03 14:41:35", "link": "http://arxiv.org/abs/2503.01606v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing vision-language models in front-end development via data\n  synthesis", "abstract": "Modern front-end (FE) development, especially when leveraging the unique\nfeatures of frameworks like React and Vue, presents distinctive challenges.\nThese include managing modular architectures, ensuring synchronization between\ndata and visual outputs for declarative rendering, and adapting reusable\ncomponents to various scenarios. Such complexities make it particularly\ndifficult for state-of-the-art large vision-language models (VLMs) to generate\naccurate and functional code directly from design images. To address these\nchallenges, we propose a reflective agentic workflow that synthesizes\nhigh-quality image-text data to capture the diverse characteristics of FE\ndevelopment. This workflow automates the extraction of\nself-contained\\footnote{A \\textbf{self-contained} code snippet is one that\nencapsulates all necessary logic, styling, and dependencies, ensuring it\nfunctions independently without requiring external imports or context.} code\nsnippets from real-world projects, renders the corresponding visual outputs,\nand generates detailed descriptions that link design elements to functional\ncode. To further expand the scope and utility of the synthesis, we introduce\nthree data synthesis strategies: Evolution-based synthesis, which enables\nscalable and diverse dataset expansion; Waterfall-Model-based synthesis, which\ngenerates logically coherent code derived from system requirements; and\nAdditive Development synthesis, which iteratively increases the complexity of\nhuman-authored components. We build a large vision-language model, Flame,\ntrained on the synthesized datasets and demonstrate its effectiveness in\ngenerating React code via the $\\text{pass}@k$ metric. Our results suggest that\na code VLM trained to interpret images before code generation may achieve\nbetter performance.", "published": "2025-03-03 14:54:01", "link": "http://arxiv.org/abs/2503.01619v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Emergence of Grammar through Reinforcement Learning", "abstract": "The evolution of grammatical systems of syntactic and semantic composition is\nmodeled here with a novel application of reinforcement learning theory. To test\nthe functionalist thesis that speakers' expressive purposes shape their\nlanguage, we include within the model a probability distribution over different\nmessages that could be expressed in a given context. The proposed learning and\nproduction algorithm then breaks down language learning into a sequence of\nsimple steps, such that each step benefits from the message probabilities. The\nresults are presented in the form of numerical simulations of language\nhistories and analytic proofs. The potential for applying these mathematical\nmodels to the study of natural language is illustrated with two case studies\nfrom the history of English.", "published": "2025-03-03 15:10:46", "link": "http://arxiv.org/abs/2503.01635v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via\n  Symbolic Code Generation", "abstract": "Recent works have shown great potentials of Large Language Models (LLMs) in\nrobot task and motion planning (TAMP). Current LLM approaches generate text- or\ncode-based reasoning chains with sub-goals and action plans. However, they do\nnot fully leverage LLMs' symbolic computing and code generation capabilities.\nMany robot TAMP tasks involve complex optimization under multiple constraints,\nwhere pure textual reasoning is insufficient. While augmenting LLMs with\npredefined solvers and planners improves performance, it lacks generalization\nacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMP\ncapabilities by steering them to generate code as symbolic planners for\noptimization and constraint verification. Unlike prior work that uses code to\ninterface with robot action modules, we steer LLMs to generate code as solvers,\nplanners, and checkers for TAMP tasks requiring symbolic computing, while still\nleveraging textual reasoning to incorporate common sense. With a multi-round\nguidance and answer evolution framework, the proposed Code-as-Symbolic-Planner\nimproves success rates by average 24.1\\% over best baseline methods across\nseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows\nstrong effectiveness and generalizability across discrete and continuous\nenvironments, 2D/3D simulations and real-world settings, as well as single- and\nmulti-robot tasks with diverse requirements. See our project website\nhttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and\ncode.", "published": "2025-03-03 16:13:41", "link": "http://arxiv.org/abs/2503.01700v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAs", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable\nlanguage and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language\nmodel trained on high-quality web and synthetic data, significantly\noutperforming recent open-source models of similar size and matching the\nperformance of models twice its size on math and coding tasks requiring complex\nreasoning. This achievement is driven by a carefully curated synthetic data\nrecipe emphasizing high-quality math and coding datasets. Compared to its\npredecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of\n200K tokens to better support multilingual applications, as well as group query\nattention for more efficient long-sequence generation. Phi-4-Multimodal is a\nmultimodal model that integrates text, vision, and speech/audio input\nmodalities into a single model. Its novel modality extension approach leverages\nLoRA adapters and modality-specific routers to allow multiple inference modes\ncombining various modalities without interference. For example, it now ranks\nfirst in the OpenASR leaderboard to date, although the LoRA component of the\nspeech/audio modality has just 460 million parameters. Phi-4-Multimodal\nsupports scenarios involving (vision + language), (vision + speech), and\n(speech/audio) inputs, outperforming larger vision-language and speech-language\nmodels on a wide range of tasks. Additionally, we experiment to further train\nPhi-4-Mini to enhance its reasoning capabilities. Despite its compact\n3.8-billion-parameter size, this experimental version achieves reasoning\nperformance on par with or surpassing significantly larger models, including\nDeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "published": "2025-03-03 17:05:52", "link": "http://arxiv.org/abs/2503.01743v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAKE: Steering Activations for Knowledge Editing", "abstract": "As Large Langue Models have been shown to memorize real-world facts, the need\nto update this knowledge in a controlled and efficient manner arises. Designed\nwith these constraints in mind, Knowledge Editing (KE) approaches propose to\nalter specific facts in pretrained models. However, they have been shown to\nsuffer from several limitations, including their lack of contextual robustness\nand their failure to generalize to logical implications related to the fact. To\novercome these issues, we propose SAKE, a steering activation method that\nmodels a fact to be edited as a distribution rather than a single prompt.\nLeveraging Optimal Transport, SAKE alters the LLM behavior over a whole\nfact-related distribution, defined as paraphrases and logical implications.\nSeveral numerical experiments demonstrate the effectiveness of this method:\nSAKE is thus able to perform more robust edits than its existing counterparts.", "published": "2025-03-03 17:20:29", "link": "http://arxiv.org/abs/2503.01751v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for\n  Large Language Models", "abstract": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.", "published": "2025-03-03 17:37:16", "link": "http://arxiv.org/abs/2503.01763v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding", "abstract": "Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.", "published": "2025-03-03 18:33:46", "link": "http://arxiv.org/abs/2503.01804v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with\n  Transformers", "abstract": "Transformers have revolutionized the field of machine learning. In\nparticular, they can be used to solve complex algorithmic problems, including\ngraph-based tasks. In such algorithmic tasks a key question is what is the\nminimal size of a transformer that can implement a task. Recent work has begun\nto explore this problem for graph-based tasks, showing that for sub-linear\nembedding dimension (i.e., model width) logarithmic depth suffices. However, an\nopen question, which we address here, is what happens if width is allowed to\ngrow linearly. Here we analyze this setting, and provide the surprising result\nthat with linear width, constant depth suffices for solving a host of\ngraph-based problems. This suggests that a moderate increase in width can allow\nmuch shallower models, which are advantageous in terms of inference time. For\nother problems, we show that quadratic width is required. Our results\ndemonstrate the complex and intriguing landscape of transformer implementations\nof graph-based algorithms. We support our theoretical results with empirical\nevaluations.", "published": "2025-03-03 18:33:58", "link": "http://arxiv.org/abs/2503.01805v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMInit: A Free Lunch from Large Language Models for Selective\n  Initialization of Recommendation", "abstract": "Collaborative filtering models, particularly graph-based approaches, have\ndemonstrated strong performance in capturing user-item interactions for\nrecommendation systems. However, they continue to struggle in cold-start and\ndata-sparse scenarios. The emergence of large language models (LLMs) like GPT\nand LLaMA presents new possibilities for enhancing recommendation performance,\nespecially in cold-start settings. Despite their promise, LLMs pose challenges\nrelated to scalability and efficiency due to their high computational demands\nand limited ability to model complex user-item relationships effectively. In\nthis work, we introduce a novel perspective on leveraging LLMs for CF model\ninitialization. Through experiments, we uncover an embedding collapse issue\nwhen scaling CF models to larger embedding dimensions. To effectively harness\nlarge-scale LLM embeddings, we propose innovative selective initialization\nstrategies utilizing random, uniform, and variance-based index sampling. Our\ncomprehensive evaluation on multiple real-world datasets demonstrates\nsignificant performance gains across various CF models while maintaining a\nlower computational cost compared to existing LLM-based recommendation\napproaches.", "published": "2025-03-03 18:41:59", "link": "http://arxiv.org/abs/2503.01814v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "RSQ: Learning from Important Tokens Leads to Better Quantized LLMs", "abstract": "Layer-wise quantization is a key technique for efficiently compressing large\nmodels without expensive retraining. Previous methods typically quantize the\nweights of each layer by \"uniformly\" optimizing the layer reconstruction loss\nacross all output tokens. However, in this paper, we demonstrate that\nbetter-quantized models can be obtained by prioritizing learning from important\ntokens (e.g. which have large attention scores). Building on this finding, we\npropose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations\n(orthogonal transformation) to the model to mitigate outliers (those with\nexceptionally large magnitude), (2) scales the token feature based on its\nimportance, and (3) quantizes the model using the GPTQ framework with the\nsecond-order statistics computed by scaled tokens. To compute token importance,\nwe explore both heuristic and dynamic strategies. Based on a thorough analysis\nof all approaches, we adopt attention concentration, which uses attention\nscores of each token as its importance, as the best approach. We demonstrate\nthat RSQ consistently outperforms baseline methods across multiple downstream\ntasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally,\nmodels quantized with RSQ achieve superior performance on long-context tasks,\nfurther highlighting its effectiveness. Lastly, RSQ demonstrates\ngeneralizability across various setups, including different model sizes,\ncalibration datasets, bit precisions, and quantization methods.", "published": "2025-03-03 18:46:33", "link": "http://arxiv.org/abs/2503.01820v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Persuade Me if You Can: A Framework for Evaluating Persuasion\n  Effectiveness and Susceptibility Among Large Language Models", "abstract": "Large Language Models (LLMs) demonstrate persuasive capabilities that rival\nhuman-level persuasion. While these capabilities can be used for social good,\nthey also present risks of potential misuse. Moreover, LLMs' susceptibility to\npersuasion raises concerns about alignment with ethical principles. To study\nthese dynamics, we introduce Persuade Me If You Can (PMIYC), an automated\nframework for evaluating persuasion through multi-agent interactions. Here,\nPersuader agents engage in multi-turn conversations with the Persuadee agents,\nallowing us to measure LLMs' persuasive effectiveness and their susceptibility\nto persuasion. We conduct comprehensive evaluations across diverse LLMs,\nensuring each model is assessed against others in both subjective and\nmisinformation contexts. We validate the efficacy of our framework through\nhuman evaluations and show alignment with prior work. PMIYC offers a scalable\nalternative to human annotation for studying persuasion in LLMs. Through PMIYC,\nwe find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness,\noutperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50%\ngreater resistance to persuasion for misinformation compared to Llama-3.3-70B.\nThese findings provide empirical insights into the persuasive dynamics of LLMs\nand contribute to the development of safer AI systems.", "published": "2025-03-03 18:53:21", "link": "http://arxiv.org/abs/2503.01829v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Jailbreaking Safeguarded Text-to-Image Models via Large Language Models", "abstract": "Text-to-Image models may generate harmful content, such as pornographic\nimages, particularly when unsafe prompts are submitted. To address this issue,\nsafety filters are often added on top of text-to-image models, or the models\nthemselves are aligned to reduce harmful outputs. However, these defenses\nremain vulnerable when an attacker strategically designs adversarial prompts to\nbypass these safety guardrails. In this work, we propose PromptTune, a method\nto jailbreak text-to-image models with safety guardrails using a fine-tuned\nlarge language model. Unlike other query-based jailbreak attacks that require\nrepeated queries to the target model, our attack generates adversarial prompts\nefficiently after fine-tuning our AttackLLM. We evaluate our method on three\ndatasets of unsafe prompts and against five safety guardrails. Our results\ndemonstrate that our approach effectively bypasses safety guardrails,\noutperforms existing no-box attacks, and also facilitates other query-based\nattacks.", "published": "2025-03-03 18:58:46", "link": "http://arxiv.org/abs/2503.01839v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI\n  Perspective", "abstract": "Deploying large scale language models on edge devices faces inherent\nchallenges such as high computational demands, energy consumption, and\npotential data privacy risks. This paper introduces the Shakti Small Language\nModels (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these\nconstraints headon. By combining efficient architectures, quantization\ntechniques, and responsible AI principles, the Shakti series enables on-device\nintelligence for smartphones, smart appliances, IoT systems, and beyond. We\nprovide comprehensive insights into their design philosophy, training\npipelines, and benchmark performance on both general tasks (e.g., MMLU,\nHellaswag) and specialized domains (healthcare, finance, and legal). Our\nfindings illustrate that compact models, when carefully engineered and\nfine-tuned, can meet and often exceed expectations in real-world edge-AI\nscenarios.", "published": "2025-03-03 04:53:55", "link": "http://arxiv.org/abs/2503.01933v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM\n  agents", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous\nagents, yet existing benchmarks either focus on single-agent tasks or are\nconfined to narrow domains, failing to capture the dynamics of multi-agent\ncoordination and competition. In this paper, we introduce MultiAgentBench, a\ncomprehensive benchmark designed to evaluate LLM-based multi-agent systems\nacross diverse, interactive scenarios. Our framework measures not only task\ncompletion but also the quality of collaboration and competition using novel,\nmilestone-based key performance indicators. Moreover, we evaluate various\ncoordination protocols (including star, chain, tree, and graph topologies) and\ninnovative strategies such as group discussion and cognitive planning. Notably,\ngpt-4o-mini reaches the average highest task score, graph structure performs\nthe best among coordination protocols in the research scenario, and cognitive\nplanning improves milestone achievement rates by 3%. Code and datasets are\npublic available at https://github.com/MultiagentBench/MARBLE.", "published": "2025-03-03 05:18:50", "link": "http://arxiv.org/abs/2503.01935v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.MA"}
{"title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust\n  Multimodal Document Retrieval", "abstract": "Cross-modal retrieval is gaining increasing efficacy and interest from the\nresearch community, thanks to large-scale training, novel architectural and\nlearning designs, and its application in LLMs and multimodal LLMs. In this\npaper, we move a step forward and design an approach that allows for multimodal\nqueries, composed of both an image and a text, and can search within\ncollections of multimodal documents, where images and text are interleaved. Our\nmodel, ReT, employs multi-level representations extracted from different layers\nof both visual and textual backbones, both at the query and document side. To\nallow for multi-level and cross-modal understanding and feature extraction, ReT\nemploys a novel Transformer-based recurrent cell that integrates both textual\nand visual features at different layers, and leverages sigmoidal gates inspired\nby the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR\nbenchmarks show that ReT achieves state-of-the-art performance across diverse\nsettings. Our source code and trained models are publicly available at\nhttps://github.com/aimagelab/ReT.", "published": "2025-03-03 19:01:17", "link": "http://arxiv.org/abs/2503.01980v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Adaptively evaluating models with task elicitation", "abstract": "Manual curation of evaluation datasets is struggling to keep up with the\nrapidly expanding capabilities and deployment scenarios of language models.\nTowards scalable model profiling, we introduce and validate a framework for\nevaluating LLMs, called Adaptive Evaluations. Adaptive evaluations use\nscaffolded language models (evaluator agents) to search through a target\nmodel's behavior on a domain dataset and create difficult questions (tasks)\nthat can discover and probe the model's failure modes. We find that frontier\nmodels lack consistency when adaptively probed with our framework on a diverse\nsuite of datasets and tasks, including but not limited to legal reasoning,\nforecasting, and online harassment. Generated questions pass human validity\nchecks and often transfer to other models with different capability profiles,\ndemonstrating that adaptive evaluations can also be used to create difficult\ndomain-specific datasets.", "published": "2025-03-03 19:04:10", "link": "http://arxiv.org/abs/2503.01986v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific\n  Text Categorization Using Prompt Engineering", "abstract": "This study examines how large language models categorize sentences from\nscientific papers using prompt engineering. We use two advanced web-based\nmodels, GPT-4o (by OpenAI) and DeepSeek R1, to classify sentences into\npredefined relationship categories. DeepSeek R1 has been tested on benchmark\ndatasets in its technical report. However, its performance in scientific text\ncategorization remains unexplored. To address this gap, we introduce a new\nevaluation method designed specifically for this task. We also compile a\ndataset of cleaned scientific papers from diverse domains. This dataset\nprovides a platform for comparing the two models. Using this dataset, we\nanalyze their effectiveness and consistency in categorization.", "published": "2025-03-03 20:09:35", "link": "http://arxiv.org/abs/2503.02032v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "EPEE: Towards Efficient and Effective Foundation Models in Biomedicine", "abstract": "Foundation models, including language models, e.g., GPT, and vision models,\ne.g., CLIP, have significantly advanced numerous biomedical tasks. Despite\nthese advancements, the high inference latency and the \"overthinking\" issues in\nmodel inference impair the efficiency and effectiveness of foundation models,\nthus limiting their application in real-time clinical settings. To address\nthese challenges, we proposed EPEE (Entropy- and Patience-based Early Exiting),\na novel hybrid strategy designed to improve the inference efficiency of\nfoundation models. The core idea was to leverage the strengths of entropy-based\nand patience-based early exiting methods to overcome their respective\nweaknesses. To evaluate EPEE, we conducted experiments on three core biomedical\ntasks-classification, relation extraction, and event extraction-using four\nfoundation models (BERT, ALBERT, GPT-2, and ViT) across twelve datasets,\nincluding clinical notes and medical images. The results showed that EPEE\nsignificantly reduced inference time while maintaining or improving accuracy,\ndemonstrating its adaptability to diverse datasets and tasks. EPEE addressed\ncritical barriers to deploying foundation models in healthcare by balancing\nefficiency and effectiveness. It potentially provided a practical solution for\nreal-time clinical decision-making with foundation models, supporting reliable\nand efficient workflows.", "published": "2025-03-03 21:11:13", "link": "http://arxiv.org/abs/2503.02053v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Hebbian learning the local structure of language", "abstract": "Learning in the brain is local and unsupervised (Hebbian). We derive the\nfoundations of an effective human language model inspired by these microscopic\nconstraints. It has two parts: (1) a hierarchy of neurons which learns to\ntokenize words from text (whichiswhatyoudowhenyoureadthis); and (2) additional\nneurons which bind the learned symanticless patterns of the tokenizer into a\nsymanticful token (an embedding). The model permits continuous parallel\nlearning without forgetting; and is a powerful tokenizer which performs\nrenormalization group. This allows it to exploit redundancy, such that it\ngenerates tokens which are always decomposable into a basis set (e.g an\nalphabet), and can mix features learned from multiple languages. We find that\nthe structure of this model allows it to learn a natural language morphology\nWITHOUT data. The language data generated by this model predicts the correct\ndistribution of word-forming patterns observed in real languages, and further\ndemonstrates why microscopically human speech is broken up into words. This\nmodel provides the basis for understanding the microscopic origins of language\nand human creativity.", "published": "2025-03-03 21:15:57", "link": "http://arxiv.org/abs/2503.02057v1", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Superscopes: Amplifying Internal Feature Representations for Language\n  Model Interpretation", "abstract": "Understanding and interpreting the internal representations of large language\nmodels (LLMs) remains an open challenge. Patchscopes introduced a method for\nprobing internal activations by patching them into new prompts, prompting\nmodels to self-explain their hidden representations. We introduce Superscopes,\na technique that systematically amplifies superposed features in MLP outputs\n(multilayer perceptron) and hidden states before patching them into new\ncontexts. Inspired by the \"features as directions\" perspective and the\nClassifier-Free Guidance (CFG) approach from diffusion models, Superscopes\namplifies weak but meaningful features, enabling the interpretation of internal\nrepresentations that previous methods failed to explain-all without requiring\nadditional training. This approach provides new insights into how LLMs build\ncontext and represent complex concepts, further advancing mechanistic\ninterpretability.", "published": "2025-03-03 21:58:12", "link": "http://arxiv.org/abs/2503.02078v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forgetting Transformer: Softmax Attention with a Forget Gate", "abstract": "An essential component of modern recurrent sequence models is the forget\ngate. While Transformers do not have an explicit recurrent form, we show that a\nforget gate can be naturally incorporated into Transformers by down-weighting\nthe unnormalized attention scores in a data-dependent way. We name this\nattention mechanism Forgetting Attention and the resulting model the Forgetting\nTransformer (FoX). We show that FoX outperforms the Transformer on long-context\nlanguage modeling, length extrapolation, and short-context downstream tasks,\nwhile performing on par with the Transformer on long-context downstream tasks.\nMoreover, it is compatible with the FlashAttention algorithm and does not\nrequire any positional embeddings. Several analyses, including the\nneedle-in-the-haystack test, show that FoX also retains the Transformer's\nsuperior long-context capabilities over recurrent sequence models such as\nMamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that\nincorporates some common architectural components in recurrent sequence models\nand find it significantly improves the performance of both FoX and the\nTransformer. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.", "published": "2025-03-03 23:35:23", "link": "http://arxiv.org/abs/2503.02130v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Direct Speech to Speech Translation: A Review", "abstract": "Speech to speech translation (S2ST) is a transformative technology that\nbridges global communication gaps, enabling real time multilingual interactions\nin diplomacy, tourism, and international trade. Our review examines the\nevolution of S2ST, comparing traditional cascade models which rely on automatic\nspeech recognition (ASR), machine translation (MT), and text to speech (TTS)\ncomponents with newer end to end and direct speech translation (DST) models\nthat bypass intermediate text representations. While cascade models offer\nmodularity and optimized components, they suffer from error propagation,\nincreased latency, and loss of prosody. In contrast, direct S2ST models retain\nspeaker identity, reduce latency, and improve translation naturalness by\npreserving vocal characteristics and prosody. However, they remain limited by\ndata sparsity, high computational costs, and generalization challenges for\nlow-resource languages. The current work critically evaluates these approaches,\ntheir tradeoffs, and future directions for improving real time multilingual\ncommunication.", "published": "2025-03-03 06:48:22", "link": "http://arxiv.org/abs/2503.04799v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hate Speech and Sentiment of YouTube Video Comments From Public and\n  Private Sources Covering the Israel-Palestine Conflict", "abstract": "This study explores the prevalence of hate speech (HS) and sentiment in\nYouTube video comments concerning the Israel-Palestine conflict by analyzing\ncontent from both public and private news sources. The research involved\nannotating 4983 comments for HS and sentiments (neutral, pro-Israel, and\npro-Palestine). Subsequently, machine learning (ML) models were developed,\ndemonstrating robust predictive capabilities with area under the receiver\noperating characteristic (AUROC) scores ranging from 0.83 to 0.90. These models\nwere applied to the extracted comment sections of YouTube videos from public\nand private sources, uncovering a higher incidence of HS in public sources\n(40.4%) compared to private sources (31.6%). Sentiment analysis revealed a\npredominantly neutral stance in both source types, with more pronounced\nsentiments towards Israel and Palestine observed in public sources. This\ninvestigation highlights the dynamic nature of online discourse surrounding the\nIsrael-Palestine conflict and underscores the potential of moderating content\nin a politically charged environment.", "published": "2025-03-03 09:03:14", "link": "http://arxiv.org/abs/2503.10648v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Diverse and Effective Retrieval-Based Debt Collection System with\n  Expert Knowledge", "abstract": "Designing effective debt collection systems is crucial for improving\noperational efficiency and reducing costs in the financial industry. However,\nthe challenges of maintaining script diversity, contextual relevance, and\ncoherence make this task particularly difficult. This paper presents a debt\ncollection system based on real debtor-collector data from a major commercial\nbank. We construct a script library from real-world debt collection\nconversations, and propose a two-stage retrieval based response system for\ncontextual relevance. Experimental results show that our system improves script\ndiversity, enhances response relevance, and achieves practical deployment\nefficiency through knowledge distillation. This work offers a scalable and\nautomated solution, providing valuable insights for advancing debt collection\npractices in real-world applications.", "published": "2025-03-03 08:56:54", "link": "http://arxiv.org/abs/2504.06273v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers", "abstract": "Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS .", "published": "2025-03-03 04:24:04", "link": "http://arxiv.org/abs/2503.01163v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the\n  Lens of Summarization", "abstract": "With the rapid development of large language models (LLMs), LLM-as-a-judge\nhas emerged as a widely adopted approach for text quality evaluation, including\nhallucination evaluation. While previous studies have focused exclusively on\nsingle-context evaluation (e.g., discourse faithfulness or world factuality),\nreal-world hallucinations typically involve mixed contexts, which remains\ninadequately evaluated. In this study, we use summarization as a representative\ntask to comprehensively evaluate LLMs' capability in detecting mixed-context\nhallucinations, specifically distinguishing between factual and non-factual\nhallucinations. Through extensive experiments across direct generation and\nretrieval-based models of varying scales, our main observations are: (1) LLMs'\nintrinsic knowledge introduces inherent biases in hallucination evaluation; (2)\nThese biases particularly impact the detection of factual hallucinations,\nyielding a significant performance bottleneck; (3) The fundamental challenge\nlies in effective knowledge utilization, balancing between LLMs' intrinsic\nknowledge and external context for accurate mixed-context hallucination\nevaluation.", "published": "2025-03-03 15:42:57", "link": "http://arxiv.org/abs/2503.01670v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linear Representations of Political Perspective Emerge in Large Language\n  Models", "abstract": "Large language models (LLMs) have demonstrated the ability to generate text\nthat realistically reflects a range of different subjective human perspectives.\nThis paper studies how LLMs are seemingly able to reflect more liberal versus\nmore conservative viewpoints among other political perspectives in American\npolitics. We show that LLMs possess linear representations of political\nperspectives within activation space, wherein more similar perspectives are\nrepresented closer together. To do so, we probe the attention heads across the\nlayers of three open transformer-based LLMs (Llama-2-7b-chat,\nMistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from\nthe perspectives of different U.S. lawmakers. We then identify sets of\nattention heads whose activations linearly predict those lawmakers' DW-NOMINATE\nscores, a widely-used and validated measure of political ideology. We find that\nhighly predictive heads are primarily located in the middle layers, often\nspeculated to encode high-level concepts and tasks. Using probes only trained\nto predict lawmakers' ideology, we then show that the same probes can predict\nmeasures of news outlets' slant from the activations of models prompted to\nsimulate text from those news outlets. These linear probes allow us to\nvisualize, interpret, and monitor ideological stances implicitly adopted by an\nLLM as it generates open-ended responses. Finally, we demonstrate that by\napplying linear interventions to these attention heads, we can steer the model\noutputs toward a more liberal or conservative stance. Overall, our research\nsuggests that LLMs possess a high-level linear representation of American\npolitical ideology and that by leveraging recent advances in mechanistic\ninterpretability, we can identify, monitor, and steer the subjective\nperspective underlying generated text.", "published": "2025-03-03 21:59:01", "link": "http://arxiv.org/abs/2503.02080v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End\n  Full-Length Song Generation with Latent Diffusion", "abstract": "Recent advancements in music generation have garnered significant attention,\nyet existing approaches face critical limitations. Some current generative\nmodels can only synthesize either the vocal track or the accompaniment track.\nWhile some models can generate combined vocal and accompaniment, they typically\nrely on meticulously designed multi-stage cascading architectures and intricate\ndata pipelines, hindering scalability. Additionally, most systems are\nrestricted to generating short musical segments rather than full-length songs.\nFurthermore, widely used language model-based methods suffer from slow\ninference speeds. To address these challenges, we propose DiffRhythm, the first\nlatent diffusion-based song generation model capable of synthesizing complete\nsongs with both vocal and accompaniment for durations of up to 4m45s in only\nten seconds, maintaining high musicality and intelligibility. Despite its\nremarkable capabilities, DiffRhythm is designed to be simple and elegant: it\neliminates the need for complex data preparation, employs a straightforward\nmodel structure, and requires only lyrics and a style prompt during inference.\nAdditionally, its non-autoregressive structure ensures fast inference speeds.\nThis simplicity guarantees the scalability of DiffRhythm. Moreover, we release\nthe complete training code along with the pre-trained model on large-scale data\nto promote reproducibility and further research.", "published": "2025-03-03 05:15:34", "link": "http://arxiv.org/abs/2503.01183v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CNN-based Robust Sound Source Localization with SRP-PHAT for the Extreme\n  Edge", "abstract": "Robust sound source localization for environments with noise and\nreverberation are increasingly exploiting deep neural networks fed with various\nacoustic features. Yet, state-of-the-art research mainly focuses on optimizing\nalgorithmic accuracy, resulting in huge models preventing edge-device\ndeployment. The edge, however, urges for real-time low-footprint acoustic\nreasoning for applications such as hearing aids and robot interactions. Hence,\nwe set off from a robust CNN-based model using SRP-PHAT features, Cross3D [16],\nto pursue an efficient yet compact model architecture for the extreme edge. For\nboth the SRP feature representation and neural network, we propose respectively\nour scalable LC-SRP-Edge and Cross3D-Edge algorithms which are optimized\ntowards lower hardware overhead. LC-SRP-Edge halves the complexity and on-chip\nmemory overhead for the sinc interpolation compared to the original LC-SRP\n[19]. Over multiple SRP resolution cases, Cross3D-Edge saves 10.32~73.71%\ncomputational complexity and 59.77~94.66% neural network weights against the\nCross3D baseline. In terms of the accuracy-efficiency tradeoff, the most\nbalanced version (EM) requires only 127.1 MFLOPS computation, 3.71 MByte/s\nbandwidth, and 0.821 MByte on-chip memory in total, while still retaining\ncompetitiveness in state-of-the-art accuracy comparisons. It achieves 8.59\nms/frame end-to-end latency on a Rasberry Pi 4B, which is 7.26x faster than the\ncorresponding baseline.", "published": "2025-03-03 20:50:31", "link": "http://arxiv.org/abs/2503.02046v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity\n  in Speech-Language Pathology", "abstract": "This study explores voice cloning to generate synthetic speech replicating\nthe unique patterns of individuals with dysarthria. Using the TORGO dataset, we\naddress data scarcity and privacy challenges in speech-language pathology. Our\ncontributions include demonstrating that voice cloning preserves dysarthric\nspeech characteristics, analyzing differences between real and synthetic data,\nand discussing implications for diagnostics, rehabilitation, and communication.\nWe cloned voices from dysarthric and control speakers using a commercial\nplatform, ensuring gender-matched synthetic voices. A licensed speech-language\npathologist (SLP) evaluated a subset for dysarthria, speaker gender, and\nsynthetic indicators. The SLP correctly identified dysarthria in all cases and\nspeaker gender in 95% but misclassified 30% of synthetic samples as real,\nindicating high realism. Our results suggest synthetic speech effectively\ncaptures disordered characteristics and that voice cloning has advanced to\nproduce high-quality data resembling real speech, even to trained\nprofessionals. This has critical implications for healthcare, where synthetic\ndata can mitigate data scarcity, protect privacy, and enhance AI-driven\ndiagnostics. By enabling the creation of diverse, high-quality speech datasets,\nvoice cloning can improve generalizable models, personalize therapy, and\nadvance assistive technologies for dysarthria.\n  We publicly release our synthetic dataset to foster further research and\ncollaboration, aiming to develop robust models that improve patient outcomes in\nspeech-language pathology.", "published": "2025-03-03 07:44:49", "link": "http://arxiv.org/abs/2503.01266v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Piano Transcription Based on Consistent Onset and Offset\n  Decoding with Sustain Pedal Detection", "abstract": "This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.", "published": "2025-03-03 09:55:54", "link": "http://arxiv.org/abs/2503.01362v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FlowDec: A flow-based full-band general audio codec with high perceptual\n  quality", "abstract": "We propose FlowDec, a neural full-band audio codec for general audio sampled\nat 48 kHz that combines non-adversarial codec training with a stochastic\npostfilter based on a novel conditional flow matching method. Compared to the\nprior work ScoreDec which is based on score matching, we generalize from speech\nto general audio and move from 24 kbit/s to as low as 4 kbit/s, while improving\noutput quality and reducing the required postfilter DNN evaluations from 60 to\n6 without any fine-tuning or distillation techniques. We provide theoretical\ninsights and geometric intuitions for our approach in comparison to ScoreDec as\nwell as another recent work that uses flow matching, and conduct ablation\nstudies on our proposed components. We show that FlowDec is a competitive\nalternative to the recent GAN-dominated stream of neural codecs, achieving FAD\nscores better than those of the established GAN-based codec DAC and listening\ntest scores that are on par, and producing qualitatively more natural\nreconstructions for speech and harmonic structures in music.", "published": "2025-03-03 12:49:09", "link": "http://arxiv.org/abs/2503.01485v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with\n  Single-Stream Decoupled Speech Tokens", "abstract": "Recent advancements in large language models (LLMs) have driven significant\nprogress in zero-shot text-to-speech (TTS) synthesis. However, existing\nfoundation models rely on multi-stage processing or complex architectures for\npredicting multiple codebooks, limiting efficiency and integration flexibility.\nTo overcome these challenges, we introduce Spark-TTS, a novel system powered by\nBiCodec, a single-stream speech codec that decomposes speech into two\ncomplementary token types: low-bitrate semantic tokens for linguistic content\nand fixed-length global tokens for speaker attributes. This disentangled\nrepresentation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT)\ngeneration approach, enables both coarse-grained control (e.g., gender,\nspeaking style) and fine-grained adjustments (e.g., precise pitch values,\nspeaking rate). To facilitate research in controllable TTS, we introduce\nVoxBox, a meticulously curated 100,000-hour dataset with comprehensive\nattribute annotations. Extensive experiments demonstrate that Spark-TTS not\nonly achieves state-of-the-art zero-shot voice cloning but also generates\nhighly customizable voices that surpass the limitations of reference-based\nsynthesis. Source code, pre-trained models, and audio samples are available at\nhttps://github.com/SparkAudio/Spark-TTS.", "published": "2025-03-03 16:23:10", "link": "http://arxiv.org/abs/2503.01710v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Tuning Whisper for Inclusive Prosodic Stress Analysis", "abstract": "Prosody plays a crucial role in speech perception, influencing both human\nunderstanding and automatic speech recognition (ASR) systems. Despite its\nimportance, prosodic stress remains under-studied due to the challenge of\nefficiently analyzing it. This study explores fine-tuning OpenAI's Whisper\nlarge-v2 ASR model to recognize phrasal, lexical, and contrastive stress in\nspeech. Using a dataset of 66 native English speakers, including male, female,\nneurotypical, and neurodivergent individuals, we assess the model's ability to\ngeneralize stress patterns and classify speakers by neurotype and gender based\non brief speech samples. Our results highlight near-human accuracy in ASR\nperformance across all three stress types and near-perfect precision in\nclassifying gender and neurotype. By improving prosody-aware ASR, this work\ncontributes to equitable and robust transcription technologies for diverse\npopulations.", "published": "2025-03-03 16:48:31", "link": "http://arxiv.org/abs/2503.02907v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
