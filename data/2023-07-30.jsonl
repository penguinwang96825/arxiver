{"title": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing\n  Evaluation for Large Language Models", "abstract": "The field of large language models (LLMs) has made significant progress, and\ntheir knowledge storage capacity is approaching that of human beings.\nFurthermore, advanced techniques, such as prompt learning and reinforcement\nlearning, are being employed to address ethical concerns and hallucination\nproblems associated with LLMs, bringing them closer to aligning with human\nvalues. This situation naturally raises the question of whether LLMs with\nhuman-like abilities possess a human-like personality? In this paper, we aim to\ninvestigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a\nwidespread human personality assessment tool, as an evaluation metric for LLMs.\nSpecifically, extensive experiments will be conducted to explore: 1) the\npersonality types of different LLMs, 2) the possibility of changing the\npersonality types by prompt engineering, and 3) How does the training dataset\naffect the model's personality. Although the MBTI is not a rigorous assessment,\nit can still reflect the similarity between LLMs and human personality. In\npractice, the MBTI has the potential to serve as a rough indicator. Our codes\nare available at\nhttps://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.", "published": "2023-07-30 09:34:35", "link": "http://arxiv.org/abs/2307.16180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving TTS for Shanghainese: Addressing Tone Sandhi via Word\n  Segmentation", "abstract": "Tone is a crucial component of the prosody of Shanghainese, a Wu Chinese\nvariety spoken primarily in urban Shanghai. Tone sandhi, which applies to all\nmulti-syllabic words in Shanghainese, then, is key to natural-sounding speech.\nUnfortunately, recent work on Shanghainese TTS (text-to-speech) such as Apple's\nVoiceOver has shown poor performance with tone sandhi, especially LD\n(left-dominant sandhi). Here I show that word segmentation during text\npreprocessing can improve the quality of tone sandhi production in TTS models.\nSyllables within the same word are annotated with a special symbol, which\nserves as a proxy for prosodic information of the domain of LD. Contrary to the\ncommon practice of using prosodic annotation mainly for static pauses, this\npaper demonstrates that prosodic annotation can also be applied to dynamic\ntonal phenomena. I anticipate this project to be a starting point for bringing\nformal linguistic accounts of Shanghainese into computational projects. Too\nlong have we been using the Mandarin models to approximate Shanghainese, but it\nis a different language with its own linguistic features, and its digitisation\nand revitalisation should be treated as such.", "published": "2023-07-30 10:50:18", "link": "http://arxiv.org/abs/2307.16199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue\n  Information Extraction", "abstract": "This paper focuses on term-status pair extraction from medical dialogues\n(MD-TSPE), which is essential in diagnosis dialogue systems and the automatic\nscribe of electronic medical records (EMRs). In the past few years, works on\nMD-TSPE have attracted increasing research attention, especially after the\nremarkable progress made by generative methods. However, these generative\nmethods output a whole sequence consisting of term-status pairs in one stage\nand ignore integrating prior knowledge, which demands a deeper understanding to\nmodel the relationship between terms and infer the status of each term. This\npaper presents a knowledge-enhanced two-stage generative framework (KTGF) to\naddress the above challenges. Using task-specific prompts, we employ a single\nmodel to complete the MD-TSPE through two phases in a unified generative form:\nwe generate all terms the first and then generate the status of each generated\nterm. In this way, the relationship between terms can be learned more\neffectively from the sequence containing only terms in the first phase, and our\ndesigned knowledge-enhanced prompt in the second phase can leverage the\ncategory and status candidates of the generated term for status generation.\nFurthermore, our proposed special status \"not mentioned\" makes more terms\navailable and enriches the training data in the second phase, which is critical\nin the low-resource setting. The experiments on the Chunyu and CMDD datasets\nshow that the proposed method achieves superior results compared to the\nstate-of-the-art models in the full training and low-resource settings.", "published": "2023-07-30 10:51:32", "link": "http://arxiv.org/abs/2307.16200v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models", "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have\nbeen proposed to mitigate the potential harms of text generated by LLMs,\nincluding fake news and copyright issues. However, current watermark detection\nalgorithms require the secret key used in the watermark generation process,\nmaking them susceptible to security breaches and counterfeiting during public\ndetection. To address this limitation, we propose an unforgeable publicly\nverifiable watermark algorithm named UPV that uses two different neural\nnetworks for watermark generation and detection, instead of using the same key\nat both stages. Meanwhile, the token embedding parameters are shared between\nthe generation and detection networks, which makes the detection network\nachieve a high accuracy very efficiently. Experiments demonstrate that our\nalgorithm attains high detection accuracy and computational efficiency through\nneural networks. Subsequent analysis confirms the high complexity involved in\nforging the watermark from the detection network. Our code is available at\n\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.", "published": "2023-07-30 13:43:27", "link": "http://arxiv.org/abs/2307.16230v7", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Distractor generation for multiple-choice questions with predictive\n  prompting and large language models", "abstract": "Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable\nperformance across various tasks and have garnered significant attention from\nboth researchers and practitioners. However, in an educational context, we\nstill observe a performance gap in generating distractors -- i.e., plausible\nyet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In\nthis study, we propose a strategy for guiding LLMs such as ChatGPT, in\ngenerating relevant distractors by prompting them with question items\nautomatically retrieved from a question bank as well-chosen in-context\nexamples. We evaluate our LLM-based solutions using a quantitative assessment\non an existing test set, as well as through quality annotations by human\nexperts, i.e., teachers. We found that on average 53% of the generated\ndistractors presented to the teachers were rated as high-quality, i.e.,\nsuitable for immediate use as is, outperforming the state-of-the-art model. We\nalso show the gains of our approach 1 in generating high-quality distractors by\ncomparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with\nstatic examples.", "published": "2023-07-30 23:15:28", "link": "http://arxiv.org/abs/2307.16338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension", "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multimodal\nLarge Language Models (MLLMs) have gained prominence as a pivotal research\narea, exhibiting remarkable capability for both comprehension and generation.\nIn this work, we address the evaluation of generative comprehension in MLLMs as\na preliminary step towards a comprehensive assessment of generative models, by\nintroducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple\nchoice questions with accurate human annotations (x 6 larger than existing\nbenchmarks), which spans 12 evaluation dimensions including the comprehension\nof both the image and video modality. We develop an advanced pipeline for\ngenerating multiple-choice questions that target specific evaluation\ndimensions, integrating both automatic filtering and manual verification\nprocesses. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 18 models across all 12\ndimensions, covering both the spatial and temporal understanding. By revealing\nthe limitations of existing MLLMs through evaluation results, we aim for\nSEED-Bench to provide insights for motivating future research. We will launch\nand consistently maintain a leaderboard to provide a platform for the community\nto assess and investigate model capability.", "published": "2023-07-30 04:25:16", "link": "http://arxiv.org/abs/2307.16125v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Toward a Period-Specific Optimized Neural Network for OCR Error\n  Correction of Historical Hebrew Texts", "abstract": "Over the past few decades, large archives of paper-based historical\ndocuments, such as books and newspapers, have been digitized using the Optical\nCharacter Recognition (OCR) technology. Unfortunately, this broadly used\ntechnology is error-prone, especially when an OCRed document was written\nhundreds of years ago. Neural networks have shown great success in solving\nvarious text processing tasks, including OCR post-correction. The main\ndisadvantage of using neural networks for historical corpora is the lack of\nsufficiently large training datasets they require to learn from, especially for\nmorphologically-rich languages like Hebrew. Moreover, it is not clear what are\nthe optimal structure and values of hyperparameters (predefined parameters) of\nneural networks for OCR error correction in Hebrew due to its unique features.\nFurthermore, languages change across genres and periods. These changes may\naffect the accuracy of OCR post-correction neural network models. To overcome\nthese challenges, we developed a new multi-phase method for generating\nartificial training datasets with OCR errors and hyperparameters optimization\nfor building an effective neural network for OCR post-correction in Hebrew.", "published": "2023-07-30 12:40:31", "link": "http://arxiv.org/abs/2307.16213v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing the Neural Network Training for OCR Error Correction of\n  Historical Hebrew Texts", "abstract": "Over the past few decades, large archives of paper-based documents such as\nbooks and newspapers have been digitized using Optical Character Recognition.\nThis technology is error-prone, especially for historical documents. To correct\nOCR errors, post-processing algorithms have been proposed based on natural\nlanguage analysis and machine learning techniques such as neural networks.\nNeural network's disadvantage is the vast amount of manually labeled data\nrequired for training, which is often unavailable. This paper proposes an\ninnovative method for training a light-weight neural network for Hebrew OCR\npost-correction using significantly less manually created data. The main\nresearch goal is to develop a method for automatically generating language and\ntask-specific training data to improve the neural network results for OCR\npost-correction, and to investigate which type of dataset is the most effective\nfor OCR post-correction of historical documents. To this end, a series of\nexperiments using several datasets was conducted. The evaluation corpus was\nbased on Hebrew newspapers from the JPress project. An analysis of historical\nOCRed newspapers was done to learn common language and corpus-specific OCR\nerrors. We found that training the network using the proposed method is more\neffective than using randomly generated errors. The results also show that the\nperformance of the neural network for OCR post-correction strongly depends on\nthe genre and area of the training data. Moreover, neural networks that were\ntrained with the proposed method outperform other state-of-the-art neural\nnetworks for OCR post-correction and complex spellcheckers. These results may\nhave practical implications for many digital humanities projects.", "published": "2023-07-30 12:59:06", "link": "http://arxiv.org/abs/2307.16220v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Hierarchical Multi-label Text Classification: A\n  Survey", "abstract": "Hierarchical multi-label text classification aims to classify the input text\ninto multiple labels, among which the labels are structured and hierarchical.\nIt is a vital task in many real world applications, e.g. scientific literature\narchiving. In this paper, we survey the recent progress of hierarchical\nmulti-label text classification, including the open sourced data sets, the main\nmethods, evaluation metrics, learning strategies and the current challenges. A\nfew future research directions are also listed for community to further improve\nthis field.", "published": "2023-07-30 16:13:00", "link": "http://arxiv.org/abs/2307.16265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LaFiCMIL: Rethinking Large File Classification from the Perspective of\n  Correlated Multiple Instance Learning", "abstract": "Transfomer-based models have significantly advanced natural language\nprocessing, in particular the performance in text classification tasks.\nNevertheless, these models face challenges in processing large files, primarily\ndue to their input constraints, which are generally restricted to hundreds or\nthousands of tokens. Attempts to address this issue in existing models usually\nconsist in extracting only a fraction of the essential information from lengthy\ninputs, while often incurring high computational costs due to their complex\narchitectures. In this work, we address the challenge of classifying large\nfiles from the perspective of correlated multiple instance learning. We\nintroduce LaFiCMIL, a method specifically designed for large file\nclassification. LaFiCMIL is optimized for efficient operation on a single GPU,\nmaking it a versatile solution for binary, multi-class, and multi-label\nclassification tasks. We conducted extensive experiments using seven diverse\nand comprehensive benchmark datasets to assess LaFiCMIL's effectiveness. By\nintegrating BERT for feature extraction, LaFiCMIL demonstrates exceptional\nperformance, setting new benchmarks across all datasets. A notable achievement\nof our approach is its ability to scale BERT to handle nearly 20,000 tokens\nwhile operating on a single GPU with 32GB of memory. This efficiency, coupled\nwith its state-of-the-art performance, highlights LaFiCMIL's potential as a\ngroundbreaking approach in the field of large file classification.", "published": "2023-07-30 18:47:54", "link": "http://arxiv.org/abs/2308.01413v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "User-Controlled Knowledge Fusion in Large Language Models: Balancing\n  Creativity and Hallucination", "abstract": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown\nexponentially due to their capacity to generate diverse, relevant, and creative\nresponses. Despite their strengths, striking a balance between the LLMs'\ncreativity and their faithfulness to external knowledge remains a key\nchallenge. This paper presents an innovative user-controllable mechanism that\nmodulates the balance between an LLM's imaginative capabilities and its\nadherence to factual information. Our approach incorporates a numerical tag\nduring the fine-tuning phase of the LLM's training, representing the degree of\nfaithfulness to the reference knowledge in the generated responses. This degree\nis computed through an automated process that measures lexical overlap using\nROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's\nself-evaluation score. During model inference, users can manipulate this\nnumerical tag, thus controlling the degree of the LLM's reliance on external\nknowledge. We conduct extensive experiments across various scenarios,\ndemonstrating the adaptability of our method and its efficacy in ensuring the\nquality and accuracy of the LLM's responses. The results highlight the\npotential of our approach to enhance the versatility of LLMs while maintaining\na balance between creativity and hallucination.", "published": "2023-07-30 06:06:35", "link": "http://arxiv.org/abs/2307.16139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Around the GLOBE: Numerical Aggregation Question-Answering on\n  Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks", "abstract": "One of the key AI tools for textual corpora exploration is natural language\nquestion-answering (QA). Unlike keyword-based search engines, QA algorithms\nreceive and process natural language questions and produce precise answers to\nthese questions, rather than long lists of documents that need to be manually\nscanned by the users. State-of-the-art QA algorithms based on DNNs were\nsuccessfully employed in various domains. However, QA in the genealogical\ndomain is still underexplored, while researchers in this field (and other\nfields in humanities and social sciences) can highly benefit from the ability\nto ask questions in natural language, receive concrete answers and gain\ninsights hidden within large corpora. While some research has been recently\nconducted for factual QA in the genealogical domain, to the best of our\nknowledge, there is no previous research on the more challenging task of\nnumerical aggregation QA (i.e., answering questions combining aggregation\nfunctions, e.g., count, average, max). Numerical aggregation QA is critical for\ndistant reading and analysis for researchers (and the general public)\ninterested in investigating cultural heritage domains. Therefore, in this\nstudy, we present a new end-to-end methodology for numerical aggregation QA for\ngenealogical trees that includes: 1) an automatic method for training dataset\ngeneration; 2) a transformer-based table selection method, and 3) an optimized\ntransformer-based numerical aggregation QA model. The findings indicate that\nthe proposed architecture, GLOBE, outperforms the state-of-the-art models and\npipelines by achieving 87% accuracy for this task compared to only 21% by\ncurrent state-of-the-art models. This study may have practical implications for\ngenealogical information centers and museums, making genealogical data research\neasy and scalable for experts as well as the general public.", "published": "2023-07-30 12:09:00", "link": "http://arxiv.org/abs/2307.16208v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question Answering with Deep Neural Networks for Semi-Structured\n  Heterogeneous Genealogical Knowledge Graphs", "abstract": "With the rising popularity of user-generated genealogical family trees, new\ngenealogical information systems have been developed. State-of-the-art natural\nquestion answering algorithms use deep neural network (DNN) architecture based\non self-attention networks. However, some of these models use sequence-based\ninputs and are not suitable to work with graph-based structure, while\ngraph-based DNN models rely on high levels of comprehensiveness of knowledge\ngraphs that is nonexistent in the genealogical domain. Moreover, these\nsupervised DNN models require training datasets that are absent in the\ngenealogical domain. This study proposes an end-to-end approach for question\nanswering using genealogical family trees by: 1) representing genealogical data\nas knowledge graphs, 2) converting them to texts, 3) combining them with\nunstructured texts, and 4) training a trans-former-based question answering\nmodel. To evaluate the need for a dedicated approach, a comparison between the\nfine-tuned model (Uncle-BERT) trained on the auto-generated genealogical\ndataset and state-of-the-art question-answering models was per-formed. The\nfindings indicate that there are significant differences between answering\ngenealogical questions and open-domain questions. Moreover, the proposed\nmethodology reduces complexity while increasing accuracy and may have practical\nimplications for genealogical research and real-world projects, making\ngenealogical data accessible to experts as well as the general public.", "published": "2023-07-30 12:49:54", "link": "http://arxiv.org/abs/2307.16214v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Analysis Using Deep Neural Networks in Digital Humanities and\n  Information Science", "abstract": "Combining computational technologies and humanities is an ongoing effort\naimed at making resources such as texts, images, audio, video, and other\nartifacts digitally available, searchable, and analyzable. In recent years,\ndeep neural networks (DNN) dominate the field of automatic text analysis and\nnatural language processing (NLP), in some cases presenting a super-human\nperformance. DNNs are the state-of-the-art machine learning algorithms solving\nmany NLP tasks that are relevant for Digital Humanities (DH) research, such as\nspell checking, language detection, entity extraction, author detection,\nquestion answering, and other tasks. These supervised algorithms learn patterns\nfrom a large number of \"right\" and \"wrong\" examples and apply them to new\nexamples. However, using DNNs for analyzing the text resources in DH research\npresents two main challenges: (un)availability of training data and a need for\ndomain adaptation. This paper explores these challenges by analyzing multiple\nuse-cases of DH studies in recent literature and their possible solutions and\nlays out a practical decision model for DH experts for when and how to choose\nthe appropriate deep learning approaches for their research. Moreover, in this\npaper, we aim to raise awareness of the benefits of utilizing deep learning\nmodels in the DH community.", "published": "2023-07-30 12:54:39", "link": "http://arxiv.org/abs/2307.16217v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mispronunciation detection using self-supervised speech representations", "abstract": "In recent years, self-supervised learning (SSL) models have produced\npromising results in a variety of speech-processing tasks, especially in\ncontexts of data scarcity. In this paper, we study the use of SSL models for\nthe task of mispronunciation detection for second language learners. We compare\ntwo downstream approaches: 1) training the model for phone recognition (PR)\nusing native English data, and 2) training a model directly for the target task\nusing non-native English data. We compare the performance of these two\napproaches for various SSL representations as well as a representation\nextracted from a traditional DNN-based speech recognition model. We evaluate\nthe models on L2Arctic and EpaDB, two datasets of non-native speech annotated\nwith pronunciation labels at the phone level. Overall, we find that using a\ndownstream model trained for the target task gives the best performance and\nthat most upstream models perform similarly for the task.", "published": "2023-07-30 21:20:58", "link": "http://arxiv.org/abs/2307.16324v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Proposing a conceptual framework: social media listening for public\n  health behavior", "abstract": "Existing communications and behavioral theories have been adopted to address\nhealth misinformation. Although various theories and models have been used to\ninvestigate the COVID-19 pandemic, there is no framework specially designed for\nsocial listening or misinformation studies using social media data and natural\nlanguage processing techniques. This study aimed to propose a novel yet\ntheory-based conceptual framework for misinformation research. We collected\ntheories and models used in COVID-19 related studies published in peer-reviewed\njournals. The theories and models ranged from health behaviors, communications,\nto misinformation. They are analyzed and critiqued for their components,\nfollowed by proposing a conceptual framework with a demonstration. We reviewed\nHealth Belief Model, Theory of Planned Behavior/Reasoned Action, Communication\nfor Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory,\nSocial Judgment Theory, Risk Information Seeking and Processing Model,\nBehavioral and Social Drivers, and Hype Loop. Accordingly, we proposed the\nSocial Media Listening for Public Health Behavior Conceptual Framework by not\nonly integrating important attributes of existing theories, but also adding new\nattributes. The proposed conceptual framework was demonstrated in the Freedom\nConvoy social media listening. The proposed conceptual framework can be used to\nbetter understand public discourse on social media, and it can be integrated\nwith other data analyses to gather a more comprehensive picture. The framework\nwill continue to be revised and adopted as health misinformation evolves.", "published": "2023-07-30 03:03:48", "link": "http://arxiv.org/abs/2308.02037v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried\n  by Text", "abstract": "In end-to-end automatic speech recognition system, one of the difficulties\nfor language expansion is the limited paired speech and text training data. In\nthis paper, we propose a novel method to generate augmented samples with\nunpaired speech feature segments and text data for model pre-training, which\nhas the advantage of low cost without using additional speech data. When mixing\n20,000 hours augmented speech data generated by our method with 12,500 hours\noriginal transcribed speech data for Italian Transformer transducer model\npre-training, we achieve 8.7% relative word error rate reduction. The\npre-trained model achieves similar performance as the model pre-trained with\nmultilingual transcribed 75,000 hours raw speech data. When merging the\naugmented speech data with the multilingual data to pre-train a new model, we\nachieve even more relative word error rate reduction of 12.2% over the\nbaseline, which further verifies the effectiveness of our method for speech\ndata augmentation.", "published": "2023-07-30 22:36:22", "link": "http://arxiv.org/abs/2307.16332v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer", "abstract": "Despite rapid progress in the voice style transfer (VST) field, recent\nzero-shot VST systems still lack the ability to transfer the voice style of a\nnovel speaker. In this paper, we present HierVST, a hierarchical adaptive\nend-to-end zero-shot VST model. Without any text transcripts, we only use the\nspeech dataset to train the model by utilizing hierarchical variational\ninference and self-supervised representation. In addition, we adopt a\nhierarchical adaptive generator that generates the pitch representation and\nwaveform audio sequentially. Moreover, we utilize unconditional generation to\nimprove the speaker-relative acoustic capacity in the acoustic representation.\nWith a hierarchical adaptive structure, the model can adapt to a novel voice\nstyle and convert speech progressively. The experimental results demonstrate\nthat our method outperforms other VST models in zero-shot VST scenarios. Audio\nsamples are available at \\url{https://hiervst.github.io/}.", "published": "2023-07-30 08:49:55", "link": "http://arxiv.org/abs/2307.16171v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UnIVAL: Unified Model for Image, Video, Audio and Language Tasks", "abstract": "Large Language Models (LLMs) have made the ambitious quest for generalist\nagents significantly far from being a fantasy. A key hurdle for building such\ngeneral models is the diversity and heterogeneity of tasks and modalities. A\npromising solution is unification, allowing the support of a myriad of tasks\nand modalities within one unified framework. While few large models (e.g.,\nFlamingo (Alayrac et al., 2022), trained on massive datasets, can support more\nthan two modalities, current small to mid-scale unified models are still\nlimited to 2 modalities, usually image-text or video-text. The question that we\nask is: is it possible to build efficiently a unified model that can support\nall modalities? To answer this, we propose UnIVAL, a step further towards this\nambitious goal. Without relying on fancy datasets sizes or models with billions\nof parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities\nand unifies text, images, video, and audio into a single model. Our model is\nefficiently pretrained on many tasks, based on task balancing and multimodal\ncurriculum learning. UnIVAL shows competitive performance to existing\nstate-of-the-art approaches, across image and video-text tasks. The feature\nrepresentations learned from image and video-text modalities, allows the model\nto achieve competitive performance when finetuned on audio-text tasks, despite\nnot being pretrained on audio. Thanks to the unified model, we propose a novel\nstudy on multimodal model merging via weight interpolation of models trained on\ndifferent multimodal tasks, showing their benefits in particular for\nout-of-distribution generalization. Finally, we motivate unification by showing\nthe synergy between tasks. The model weights and code are released here:\nhttps://github.com/mshukor/UnIVAL.", "published": "2023-07-30 09:48:36", "link": "http://arxiv.org/abs/2307.16184v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
