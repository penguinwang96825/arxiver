{"title": "Investigating LLM Variability in Personalized Conversational Information Retrieval", "abstract": "Personalized Conversational Information Retrieval (CIR) has seen rapid\nprogress in recent years, driven by the development of Large Language Models\n(LLMs). Personalized CIR aims to enhance document retrieval by leveraging\nuser-specific information, such as preferences, knowledge, or constraints, to\ntailor responses to individual needs. A key resource for this task is the TREC\niKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.\nBuilding on this resource, Mo et al. explored several strategies for\nincorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query\nreformulation. Their findings suggested that personalization from PTKBs could\nbe detrimental and that human annotations were often noisy. However, these\nconclusions were based on single-run experiments using the GPT-3.5 Turbo model,\nraising concerns about output variability and repeatability. In this\nreproducibility study, we rigorously reproduce and extend their work, focusing\non LLM output variability and model generalization. We apply the original\nmethods to the new TREC iKAT 2024 dataset and evaluate a diverse range of\nmodels, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that\nhuman-selected PTKBs consistently enhance retrieval performance, while\nLLM-based selection methods do not reliably outperform manual choices. We\nfurther compare variance across datasets and observe higher variability on iKAT\nthan on CAsT, highlighting the challenges of evaluating personalized CIR.\nNotably, recall-oriented metrics exhibit lower variance than precision-oriented\nones, a critical insight for first-stage retrievers. Finally, we underscore the\nneed for multi-run evaluations and variance reporting when assessing LLM-based\nCIR systems. By broadening evaluation across models, datasets, and metrics, our\nstudy contributes to more robust and generalizable practices for personalized\nCIR.", "published": "2025-10-04 12:13:19", "link": "http://arxiv.org/abs/2510.03795v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics", "abstract": "Evaluation for continuous piano pedal depth estimation tasks remains\nincomplete when relying only on conventional frame-level metrics, which\noverlook musically important features such as direction-change boundaries and\npedal curve contours. To provide more interpretable and musically meaningful\ninsights, we propose an evaluation framework that augments standard frame-level\nmetrics with an action-level assessment measuring direction and timing using\nsegments of press/hold/release states and a gesture-level analysis that\nevaluates contour similarity of each press-release cycle. We apply this\nframework to compare an audio-only baseline with two variants: one\nincorporating symbolic information from MIDI, and another trained in a\nbinary-valued setting, all within a unified architecture. Results show that the\nMIDI-informed model significantly outperforms the others at action and gesture\nlevels, despite modest frame-level gains. These findings demonstrate that our\nframework captures musically relevant improvements indiscernible by traditional\nmetrics, offering a more practical and effective approach to evaluating pedal\ndepth estimation models.", "published": "2025-10-04 09:29:52", "link": "http://arxiv.org/abs/2510.03750v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Privacy Enhancement in Over-the-Air Federated Learning via Adaptive Receive Scaling", "abstract": "In Federated Learning (FL) with over-the-air aggregation, the quality of the\nsignal received at the server critically depends on the receive scaling\nfactors. While a larger scaling factor can reduce the effective noise power and\nimprove training performance, it also compromises the privacy of devices by\nreducing uncertainty. In this work, we aim to adaptively design the receive\nscaling factors across training rounds to balance the trade-off between\ntraining convergence and privacy in an FL system under dynamic channel\nconditions. We formulate a stochastic optimization problem that minimizes the\noverall R\\'enyi differential privacy (RDP) leakage over the entire training\nprocess, subject to a long-term constraint that ensures convergence of the\nglobal loss function. Our problem depends on unknown future information, and we\nobserve that standard Lyapunov optimization is not applicable. Thus, we develop\na new online algorithm, termed AdaScale, based on a sequence of novel per-round\nproblems that can be solved efficiently. We further derive upper bounds on the\ndynamic regret and constraint violation of AdaSacle, establishing that it\nachieves diminishing dynamic regret in terms of time-averaged RDP leakage while\nensuring convergence of FL training to a stationary point. Numerical\nexperiments on canonical classification tasks show that our approach\neffectively reduces RDP and DP leakages compared with state-of-the-art\nbenchmarks without compromising learning performance.", "published": "2025-10-04 16:15:19", "link": "http://arxiv.org/abs/2510.03860v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO", "abstract": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies.", "published": "2025-10-04 15:17:56", "link": "http://arxiv.org/abs/2510.03831v1", "categories": ["cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "primary_category": "cs.CR"}
{"title": "Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime", "abstract": "Source polar coding is a potential solution for short blocklength-based\nlow-latency key generation with limited sources, which is a critical aspect of\nsix generation (6G) Internet of things. However, existing source coding schemes\nstill suffer from significant degradation in key generation rate and\nreconciliation reliability in short blocklength regime. To address this issue,\nwe introduce a multilevel source polarization-adjusted convolutional (PAC)\ncoding framework. Furthermore, we propose a novel code construction algorithm\nthat jointly leverages polarization effects and the maximum likelihood (ML)\ndecoding error coefficient. Simulations demonstrate that the multilevel source\nPAC scheme with the proposed code construction achieves superior key generation\nrate under key disagreement constraints compared to conventional and multilevel\nsource polar coding methods even in short blocklength regimes.", "published": "2025-10-04 14:22:53", "link": "http://arxiv.org/abs/2510.03818v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Sensing Performance Analysis in Cooperative Air-Ground ISAC Networks for LAE", "abstract": "To support the development of low altitude economy, the air-ground integrated\nsensing and communication (ISAC) networks need to be constructed to provide\nreliable and robust communication and sensing services. In this paper, the\nsensing capabilities in the cooperative air-ground ISAC networks are evaluated\nin terms of area radar detection coverage probability under a constant false\nalarm rate, where the distribution of aggregated sensing interferences is\nanalyzed as a key intermediate result. Compared with the analysis based on the\nstrongest interferer approximation, taking the aggregated sensing interference\ninto consideration is better suited for pico-cell scenarios with high base\nstation density. Simulations are conducted to validate the analysis.", "published": "2025-10-04 03:07:01", "link": "http://arxiv.org/abs/2510.03642v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Strategy Logic, Imperfect Information, and Hyperproperties", "abstract": "Strategy logic (SL) is a powerful temporal logic that enables first-class\nreasoning over strategic behavior in multi-agent systems (MAS). In many MASs,\nthe agents (and their strategies) cannot observe the global state of the\nsystem, leading to many extensions of SL centered around imperfect information,\nsuch as strategy logic with imperfect information (SL$_\\mathit{ii}$). Along\northogonal lines, researchers have studied the combination of strategic\nbehavior and hyperproperties. Hyperproperties are system properties that relate\nmultiple executions in a system and commonly arise when specifying security\npolicies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines\nquantification over strategies with the ability to express hyperproperties on\nthe executions of different strategy profiles. In this paper, we study the\nrelation between SL$_\\mathit{ii}$ and HyperSL. Our main result is that both\nlogics (restricted to formulas where no state formulas are nested within path\nformulas) are equivalent in the sense that we can encode SL$_\\mathit{ii}$\ninstances into HyperSL instances and vice versa. For the former direction, we\nbuild on the well-known observation that imperfect information is a\nhyperproperty. For the latter direction, we construct a self-composition of\nMASs and show how we can simulate hyperproperties using imperfect information.", "published": "2025-10-04 21:37:14", "link": "http://arxiv.org/abs/2510.03952v1", "categories": ["cs.LO", "cs.AI", "cs.MA"], "primary_category": "cs.LO"}
{"title": "Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning", "abstract": "High Altitude Balloons (HABs) can leverage stratospheric wind layers for\nlimited horizontal control, enabling applications in reconnaissance,\nenvironmental monitoring, and communications networks. Existing multi-agent HAB\ncoordination approaches use deterministic methods like Voronoi partitioning and\nextremum seeking control for large global constellations, which perform poorly\nfor smaller teams and localized missions. While single-agent HAB control using\nreinforcement learning has been demonstrated on HABs, coordinated multi-agent\nreinforcement learning (MARL) has not yet been investigated. This work presents\nthe first systematic application of multi-agent reinforcement learning (MARL)\nto HAB coordination for distributed area coverage. We extend our previously\ndeveloped reinforcement learning simulation environment (RLHAB) to support\ncooperative multi-agent learning, enabling multiple agents to operate\nsimultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area\ncoverage coordination, leveraging Centralized Training with Decentralized\nExecution to address atmospheric vehicle coordination challenges. Our approach\nemploys specialized observation spaces providing individual state,\nenvironmental context, and teammate data, with hierarchical rewards\nprioritizing coverage while encouraging spatial distribution. We demonstrate\nthat QMIX achieves similar performance to the theoretically optimal geometric\ndeterministic method for distributed area coverage, validating the MARL\napproach and providing a foundation for more complex autonomous multi-HAB\nmissions where deterministic methods become intractable.", "published": "2025-10-04 14:39:45", "link": "http://arxiv.org/abs/2510.03823v1", "categories": ["cs.LG", "cs.MA", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Cooperation in public goods game on regular lattices with agents changing interaction groups", "abstract": "The emergence of cooperation in the groups of interacting agents is one of\nthe most fascinating phenomena observed in many complex systems studied in\nsocial science and ecology, even in the situations where one would expect the\nagent to use a free-rider policy. This is especially surprising in the\nsituation where no external mechanisms based on reputation or punishment are\npresent. One of the possible explanations of this effect is the inhomogeneity\nof the various aspects of interactions, which can be used to clarify the\nseemingly paradoxical behavior. In this report we demonstrate that the\ndiversity of interaction networks helps to some degree to explain the emergence\nof cooperation. We extend the model of spatial interaction diversity introduced\nin [L. Shang et al., Physica A, 593:126999 (2022)] by enabling the evaluation\nof the interaction groups. We show that the process of the reevaluation of the\ninteraction group facilitates the emergence of cooperation. Furthermore, we\nalso observe that a significant participation of agents switching their\ninteraction neighborhoods has a negative impact on the formation of\ncooperation. The introduced scenario can help to understand the formation of\ncooperation in the systems where no additional mechanisms for controlling\nagents are included.", "published": "2025-10-04 10:42:10", "link": "http://arxiv.org/abs/2510.03772v1", "categories": ["physics.soc-ph", "cs.MA", "nlin.AO"], "primary_category": "physics.soc-ph"}
{"title": "Deep Reinforcement Learning for Multi-Agent Coordination", "abstract": "We address the challenge of coordinating multiple robots in narrow and\nconfined environments, where congestion and interference often hinder\ncollective task performance. Drawing inspiration from insect colonies, which\nachieve robust coordination through stigmergy -- modifying and interpreting\nenvironmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement\nLearning (S-MADRL) framework that leverages virtual pheromones to model local\nand social interactions, enabling decentralized emergent coordination without\nexplicit communication. To overcome the convergence and scalability limitations\nof existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum\nlearning, which decomposes complex tasks into progressively harder\nsub-problems. Simulation results show that our framework achieves the most\neffective coordination of up to eight agents, where robots self-organize into\nasymmetric workload distributions that reduce congestion and modulate group\nperformance. This emergent behavior, analogous to strategies observed in\nnature, demonstrates a scalable solution for decentralized multi-agent\ncoordination in crowded environments with communication constraints.", "published": "2025-10-04 00:47:20", "link": "http://arxiv.org/abs/2510.03592v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.LG"}
{"title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "abstract": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "published": "2025-10-04 00:34:32", "link": "http://arxiv.org/abs/2510.03588v1", "categories": ["cs.SE", "cs.MA"], "primary_category": "cs.SE"}
{"title": "A discrete data assimilation algorithm for the reconstruction of Gray--Scott dynamics", "abstract": "The Gray--Scott model governs the interaction of two chemical species via a\nsystem of reaction-diffusion equations. Despite its simple form, it produces\nextremely rich patterns such as spots, stripes, waves, and labyrinths. That\nmakes it ideal for studying emergent behavior, self-organization, and\ninstability-driven pattern formation. It is also known for its sensitivity to\npoorly observed initial conditions. Using such initial conditions alone quickly\nleads simulations to deviate from the true dynamics. The present paper\naddresses this challenge with a nudging-based data assimilation algorithm:\ncoarse, cell-averaged measurements are injected into the model through a\nfeedback (nudging) term, implemented as a finite-volume interpolant. We prove\ntwo main results. (i) For the continuous problem, the nudged solution\nsynchronizes with the true dynamics, and the $L^2$-error decays exponentially\nunder conditions that tie observation resolution, nudging gains, and diffusion.\n(ii) For the fully discrete semi-implicit finite-volume scheme, the same\nsynchronization holds, up to a mild time-step restriction. Numerical tests on\nlabyrinthine patterns support the theory. They show recovery of fine structure\nfrom sparse data and clarify how the observation resolution, the nudging gain,\nand the frequency of updates affect the decay rate.", "published": "2025-10-04 23:14:37", "link": "http://arxiv.org/abs/2510.03972v1", "categories": ["math.NA", "cs.NA", "math.AP", "35K57, 35Q92, 65M08, 65M12, 65M15, 65M20, 93B52"], "primary_category": "math.NA"}
{"title": "Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped", "abstract": "Simulating the kinetic Langevin dynamics is a popular approach for sampling\nfrom distributions, where only their unnormalized densities are available.\nVarious discretizations of the kinetic Langevin dynamics have been considered,\nwhere the resulting algorithm is collectively referred to as the kinetic\nLangevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,\nthe stochastic exponential Euler discretization, or exponential integrator for\nshort, has previously been studied under strongly log-concave and log-Lipschitz\nsmooth potentials via the synchronous Wasserstein coupling strategy. Existing\nanalyses, however, impose restrictions on the parameters that do not explain\nthe behavior of KLMC under various choices of parameters. In particular, all\nknown results fail to hold in the overdamped regime, suggesting that the\nexponential integrator degenerates in the overdamped limit. In this work, we\nrevisit the synchronous Wasserstein coupling analysis of KLMC with the\nexponential integrator. Our refined analysis results in Wasserstein\ncontractions and bounds on the asymptotic bias that hold under weaker\nrestrictions on the parameters, which assert that the exponential integrator is\ncapable of stably simulating the kinetic Langevin dynamics in the overdamped\nregime, as long as proper time acceleration is applied.", "published": "2025-10-04 21:30:44", "link": "http://arxiv.org/abs/2510.03949v1", "categories": ["stat.CO", "cs.NA", "math.NA", "math.PR", "stat.ML"], "primary_category": "stat.CO"}
{"title": "High-order, Compact, and Symmetric Finite Difference Methods for a $d$-Dimensional Hypercube", "abstract": "This paper presents compact, symmetric, and high-order finite difference\nmethods (FDMs) for the variable Poisson equation on a $d$-dimensional\nhypercube. Our scheme produces a symmetric linear system: an important property\nthat does not immediately hold for a high-order FDM. Since the model problem is\ncoercive, the linear system is in fact symmetric positive definite, and\nconsequently many fast solvers are applicable. Furthermore, the symmetry\ncombined with the minimum support of the stencil keeps the storage requirement\nminimal. Theoretically speaking, we prove that a compact, symmetric 1D FDM on a\nuniform grid can achieve arbitrary consistency order. On the other hand, in the\n$d$-dimensional setting, where $d \\ge 2$, the maximum consistency order that a\ncompact, symmetric FDM on a uniform grid can achieve is 4. If $d=2$ and the\ndiffusion coefficient satisfies a certain derivative condition, the maximum\nconsistency order is 6. Moreover, the finite compact, symmetric, 4th-order FDMs\nfor $d\\ge 3$, can be conveniently expressed as a linear combination of two\ntypes of FDMs: one that depends on partial derivatives along one axis, and the\nother along two axes. All finite difference stencils are explicitly provided\nfor ease of reproducibility.", "published": "2025-10-04 20:14:20", "link": "http://arxiv.org/abs/2510.03927v1", "categories": ["math.NA", "cs.NA", "65N06, 35J25"], "primary_category": "math.NA"}
{"title": "Fourier-Galerkin method for scattering poles of sound soft obstacles", "abstract": "The computation of scattering poles for a sound-soft obstacle is\ninvestigated. These poles correspond to the eigenvalues of two boundary\nintegral operators. We construct novel decompositions of these operators and\nshow that they are Fredholm. Then a Fourier-Galerkin method is proposed for\ndiscretization. By establishing the regular convergence of the discrete\noperators, an error estimate is established using the abstract approximation\ntheory for eigenvalue problems of holomorphic Fredholm operator functions. We\ngive details of the numerical implementation. Several examples are presented to\nvalidate the theory and demonstrate the effectiveness of the proposed method.", "published": "2025-10-04 14:52:44", "link": "http://arxiv.org/abs/2510.03826v1", "categories": ["math.NA", "cs.NA", "65N30, 45C05"], "primary_category": "math.NA"}
{"title": "Well-Posedness and Efficient Algorithms for Inverse Optimal Transport with Bregman Regularization", "abstract": "This work analyzes the inverse optimal transport (IOT) problem under Bregman\nregularization. We establish well-posedness results, including existence,\nuniqueness (up to equivalence classes of solutions), and stability, under\nseveral structural assumptions on the cost matrix. On the computational side,\nwe investigate the existence of solutions to the optimization problem with\ngeneral constraints on the cost matrix and provide a sufficient condition\nguaranteeing existence. In addition, we propose an inexact block coordinate\ndescent (BCD) method for the problem with a strongly convex penalty term. In\nparticular, when the penalty is quadratic, the subproblems admit a diagonal\nHessian structure, which enables highly efficient element-wise Newton updates.\nWe establish a linear convergence rate for the algorithm and demonstrate its\npractical performance through numerical experiments, including the validation\nof stability bounds, the investigation of regularization effects, and the\napplication to a marriage matching dataset.", "published": "2025-10-04 13:02:44", "link": "http://arxiv.org/abs/2510.03803v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "A Variational Method for Conformable Fractional Equations Using Rank-One Updates", "abstract": "We make a complete variational treatment of rank-one Proper Generalised\nDecomposition for separable fractional partial differential equations with\nconformable derivatives. The setting is Hilbertian, the energy is induced by a\nsymmetric coercive bilinear form, and the residual is placed in the dual space.\nA greedy rank-one update is obtained by maximizing an energy Rayleigh quotient\nover the rank-one manifold, followed by an exact line search. An exact one step\nenergy decrease identity is proved, together with geometric decay of the energy\nerror under a weak greedy condition that measures how well the search captures\nthe Riesz representer of the residual. The alternating least squares\nrealization is analyzed at the level of operators, including well posedness of\nthe alternating subproblems, a characterization of stationary points, and\nmonotonicity of the Rayleigh quotient along the inner iteration.\nDiscretizations based on weighted finite elements and on Gr\\\"unwald type\nschemes are described in detail, including assembly, boundary conditions,\ncomplexity, and memory. Two model problems, a stationary fractional Poisson\nproblem and a space time fractional diffusion problem, are treated from the\ncontinuous level down to matrices.", "published": "2025-10-04 11:06:55", "link": "http://arxiv.org/abs/2510.03778v1", "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "math.NA"}
{"title": "Neural Low-Discrepancy Sequences", "abstract": "Low-discrepancy points are designed to efficiently fill the space in a\nuniform manner. This uniformity is highly advantageous in many problems in\nscience and engineering, including in numerical integration, computer vision,\nmachine perception, computer graphics, machine learning, and simulation.\nWhereas most previous low-discrepancy constructions rely on abstract algebra\nand number theory, Message-Passing Monte Carlo (MPMC) was recently introduced\nto exploit machine learning methods for generating point sets with lower\ndiscrepancy than previously possible. However, MPMC is limited to generating\npoint sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,\nsequences of points in which every prefix has low discrepancy, a property\nessential for many applications. To address this limitation, we introduce\nNeural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based\nframework for generating LDS. Drawing inspiration from classical LDS, we train\na neural network to map indices to points such that the resulting sequences\nexhibit minimal discrepancy across all prefixes. To this end, we deploy a\ntwo-stage learning process: supervised approximation of classical constructions\nfollowed by unsupervised fine-tuning to minimize prefix discrepancies. We\ndemonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a\nsignificant margin with respect to discrepancy measures. Moreover, we\ndemonstrate the effectiveness of $NeuroLDS$ across diverse applications,\nincluding numerical integration, robot motion planning, and scientific machine\nlearning. These results highlight the promise and broad significance of Neural\nLow-Discrepancy Sequences. Our code can be found at\nhttps://github.com/camail-official/neuro-lds.", "published": "2025-10-04 09:10:37", "link": "http://arxiv.org/abs/2510.03745v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Fully discrete finite element methods for the stochastic Kuramoto-Sivashinsky equation with multiplicative noise", "abstract": "We investigate a fully discrete finite element approximation for the\nstochastic Kuramoto-Sivashinsky equation, combining the standard finite element\nmethods in spatial discretization with the implicit Euler-Maruyama scheme in\ntime. Rigorous error estimates are established for two distinct noise regimes.\nIn the case of bounded multiplicative noise, we prove optimal strong\nconvergence rates in full expectation. The analysis relies crucially on a\nstochastic Gronwall inequality and an exponential stability estimate for the\nPDE solution, which together control the interplay between the nonlinear drift\nand the multiplicative stochastic forcing. For general multiplicative noise,\nwhere boundedness no longer holds, we derive sub-optimal convergence rates in\nprobability by introducing a localization technique based on carefully\nconstructed subsets of the sample space. This dual framework demonstrates that\nthe proposed fully discrete scheme achieves strong convergence under bounded\nnoise and probabilistic convergence under general multiplicative noise, thus\nproviding the first comprehensive error analysis for numerical approximations\nof the stochastic Kuramoto-Sivashinsky equation.", "published": "2025-10-04 04:52:04", "link": "http://arxiv.org/abs/2510.03670v1", "categories": ["math.NA", "cs.NA", "math.PR"], "primary_category": "math.NA"}
{"title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design", "abstract": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo\n(QMC) methods for high-dimensional integration. We cast two long-standing QMC\ndesign problems as program synthesis and solve them with an LLM-guided\nevolutionary loop that mutates and selects code under task-specific fitness:\n(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)\nchoosing Sobol' direction numbers that minimize randomized QMC error on\ndownstream integrands. Our two-phase procedure combines constructive code\nproposals with iterative numerical refinement. On finite sets, we rediscover\nknown optima in small 2D cases and set new best-known 2D benchmarks for N >=\n40, while matching most known 3D optima up to the proven frontier (N <= 8) and\nreporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'\nparameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)\nmean-squared error for several 32-dimensional option-pricing tasks relative to\nwidely used Joe--Kuo parameters, while preserving extensibility to any sample\nsize and compatibility with standard randomizations. Taken together, the\nresults demonstrate that LLM-driven evolutionary program synthesis can automate\nthe discovery of high-quality QMC constructions, recovering classical designs\nwhere they are optimal and improving them where finite-N structure matters.\nData and code are available at\nhttps://github.com/hockeyguy123/openevolve-star-discrepancy.git.", "published": "2025-10-04 03:32:41", "link": "http://arxiv.org/abs/2510.03650v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "cs.NE", "math.NA"], "primary_category": "cs.LG"}
{"title": "Self-Speculative Masked Diffusions", "abstract": "We present self-speculative masked diffusions, a new class of masked\ndiffusion generative models for discrete data that require significantly fewer\nfunction evaluations to generate samples. Standard masked diffusion models\npredict factorized logits over currently masked positions. A number of masked\npositions are then sampled, however, the factorization approximation means that\nsampling too many positions in one go leads to poor sample quality. As a\nresult, many simulation steps and therefore neural network function evaluations\nare required to generate high-quality data. We reduce the computational burden\nby generating non-factorized predictions over masked positions. This is\nachieved by modifying the final transformer attention mask from non-causal to\ncausal, enabling draft token generation and parallel validation via a novel,\nmodel-integrated speculative sampling mechanism. This results in a\nnon-factorized predictive distribution over masked positions in a single\nforward pass. We apply our method to GPT2 scale text modelling and protein\nsequences generation, finding that we can achieve a ~2x reduction in the\nrequired number of network forward passes relative to standard masked diffusion\nmodels.", "published": "2025-10-04 20:16:38", "link": "http://arxiv.org/abs/2510.03929v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Optimal Scaling Needs Optimal Norm", "abstract": "Despite recent progress in optimal hyperparameter transfer under model and\ndataset scaling, no unifying explanatory principle has been established. Using\nthe Scion optimizer, we discover that joint optimal scaling across model and\ndataset sizes is governed by a single invariant: the operator norm of the\noutput layer. Across models with up to 1.3B parameters trained on up to 138B\ntokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$\nconsistently has the same operator norm value - a phenomenon we term norm\ntransfer. This constant norm condition is necessary but not sufficient: while\nfor each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a\nunique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient\ncondition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$\nscaling with dataset size for Scion, and find that the scaling rules are\nconsistent with those of the Adam optimizer. Tuning per-layer-group learning\nrates also improves model performance, with the output layer being the most\nsensitive and hidden layers benefiting from lower learning rates. We provide\npractical insights on norm-guided optimal scaling and release our Distributed\nScion (Disco) implementation with logs from over two thousand runs to support\nresearch on LLM training dynamics at scale.", "published": "2025-10-04 16:48:36", "link": "http://arxiv.org/abs/2510.03871v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On Provable Benefits of Muon in Federated Learning", "abstract": "The recently introduced optimizer, Muon, has gained increasing attention due\nto its superior performance across a wide range of applications. However, its\neffectiveness in federated learning remains unexplored. To address this gap,\nthis paper investigates the performance of Muon in the federated learning\nsetting. Specifically, we propose a new algorithm, FedMuon, and establish its\nconvergence rate for nonconvex problems. Our theoretical analysis reveals\nmultiple favorable properties of FedMuon. In particular, due to its\northonormalized update direction, the learning rate of FedMuon is independent\nof problem-specific parameters, and, importantly, it can naturally accommodate\nheavy-tailed noise. The extensive experiments on a variety of neural network\narchitectures validate the effectiveness of the proposed algorithm.", "published": "2025-10-04 16:27:09", "link": "http://arxiv.org/abs/2510.03866v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Hidden Game Problem", "abstract": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency.", "published": "2025-10-04 15:46:04", "link": "http://arxiv.org/abs/2510.03845v1", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting", "abstract": "We present a theoretical framework for M-FISHER, a method for sequential\ndistribution shift detection and stable adaptation in streaming data. For\ndetection, we construct an exponential martingale from non-conformity scores\nand apply Ville's inequality to obtain time-uniform guarantees on false alarm\ncontrol, ensuring statistical validity at any stopping time. Under sustained\nshifts, we further bound the expected detection delay as\n$\\mathcal{O}(\\log(1/\\delta)/\\Gamma)$, where $\\Gamma$ reflects the post-shift\ninformation gain, thereby linking detection efficiency to distributional\ndivergence. For adaptation, we show that Fisher-preconditioned updates of\nprompt parameters implement natural gradient descent on the distributional\nmanifold, yielding locally optimal updates that minimize KL divergence while\npreserving stability and parameterization invariance. Together, these results\nestablish M-FISHER as a principled approach for robust, anytime-valid detection\nand geometrically stable adaptation in sequential decision-making under\ncovariate shift.", "published": "2025-10-04 15:31:26", "link": "http://arxiv.org/abs/2510.03839v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Technical note on Fisher Information for Robust Federated Cross-Validation", "abstract": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets.", "published": "2025-10-04 15:30:04", "link": "http://arxiv.org/abs/2510.03838v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control", "abstract": "Start-ups and product grade-changes are critical steps in continuous-process\nplant operation, because any misstep immediately affects product quality and\ndrives operational losses. These transitions have long relied on manual\noperation by a handful of expert operators, but the progressive retirement of\nthat workforce is leaving plant owners without the tacit know-how needed to\nexecute them consistently. In the absence of a process model, offline\nreinforcement learning (RL) promises to capture and even surpass human\nexpertise by mining historical start-up and grade-change logs, yet standard\noffline RL struggles with distribution shift and value-overestimation whenever\na learned policy ventures outside the data envelope. We introduce HOFLON\n(Hybrid Offline Learning + Online Optimization) to overcome those limitations.\nOffline, HOFLON learns (i) a latent data manifold that represents the feasible\nregion spanned by past transitions and (ii) a long-horizon Q-critic that\npredicts the cumulative reward from state-action pairs. Online, it solves a\none-step optimization problem that maximizes the Q-critic while penalizing\ndeviations from the learned manifold and excessive rates of change in the\nmanipulated variables. We test HOFLON on two industrial case studies: a\npolymerization reactor start-up and a paper-machine grade-change problem, and\nbenchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.\nIn both plants HOFLON not only surpasses IQL but also delivers, on average,\nbetter cumulative rewards than the best start-up or grade-change observed in\nthe historical data, demonstrating its potential to automate transition\noperations beyond current expert capability.", "published": "2025-10-04 15:04:17", "link": "http://arxiv.org/abs/2510.03830v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Proximal Diffusion Neural Sampler", "abstract": "The task of learning a diffusion-based neural sampler for drawing samples\nfrom an unnormalized target distribution can be viewed as a stochastic optimal\ncontrol problem on path measures. However, the training of neural samplers can\nbe challenging when the target distribution is multimodal with significant\nbarriers separating the modes, potentially leading to mode collapse. We propose\na framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that\naddresses these challenges by tackling the stochastic optimal control problem\nvia proximal point method on the space of path measures. PDNS decomposes the\nlearning process into a series of simpler subproblems that create a path\ngradually approaching the desired distribution. This staged procedure traces a\nprogressively refined path to the desired distribution and promotes thorough\nexploration across modes. For a practical and efficient realization, we\ninstantiate each proximal step with a proximal weighted denoising cross-entropy\n(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS\nthrough extensive experiments on both continuous and discrete sampling tasks,\nincluding challenging scenarios in molecular dynamics and statistical physics.", "published": "2025-10-04 14:44:47", "link": "http://arxiv.org/abs/2510.03824v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models", "abstract": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has\nbecome the standard choice for reward-based fine-tuning of large language\nmodels (LLMs). Although recent work has explored improved estimators of\nadvantages and normalization, the clipping mechanism itself has remained\nuntouched. Originally introduced as a proxy for principled KL-based trust\nregions, clipping is a crude approximation that often causes unstable updates\nand suboptimal performance. We replace the clip objective with a novel discrete\ndifferentiable trust region projection, which provides principled token-level\nKL constraints. The projection operates on a sparse subset of the model's most\nimportant token logits to balance computational cost and projection\neffectiveness. Our approach, Trust Region Optimization for Large Language\nModels (TROLL), serves as a direct replacement for PPO-like clipping during\ntraining and does not alter the model's inference behavior. Across datasets,\nmodel families, and advantage-estimation methods, TROLL consistently\noutperforms PPO-like clipping in terms of training speed, stability, and final\nsuccess rates.", "published": "2025-10-04 14:14:20", "link": "http://arxiv.org/abs/2510.03817v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning", "abstract": "In high-dimensional learning, models remain stable until they collapse\nabruptly once the sample size falls below a critical level. This instability is\nnot algorithm-specific but a geometric mechanism: when the weakest Fisher\neigendirection falls beneath sample-level fluctuations, identifiability fails.\nOur Fisher Threshold Theorem formalizes this by proving that stability requires\nthe minimal Fisher eigenvalue to exceed an explicit $O(\\sqrt{d/n})$ bound.\nUnlike prior asymptotic or model-specific criteria, this threshold is\nfinite-sample and necessary, marking a sharp phase transition between reliable\nconcentration and inevitable failure. To make the principle constructive, we\nintroduce the Fisher floor, a verifiable spectral regularization robust to\nsmoothing and preconditioning. Synthetic experiments on Gaussian mixtures and\nlogistic models confirm the predicted transition, consistent with $d/n$\nscaling. Statistically, the threshold sharpens classical eigenvalue conditions\ninto a non-asymptotic law; learning-theoretically, it defines a spectral\nsample-complexity frontier, bridging theory with diagnostics for robust\nhigh-dimensional inference.", "published": "2025-10-04 13:33:48", "link": "http://arxiv.org/abs/2510.03809v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Robust Batched Bandits", "abstract": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness.", "published": "2025-10-04 12:26:32", "link": "http://arxiv.org/abs/2510.03798v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Allocation of Parameters in Transformers", "abstract": "Transformers have achieved remarkable successes across a wide range of\napplications, yet the theoretical foundation of their model efficiency remains\nunderexplored. In this work, we investigate how the model parameters -- mainly\nattention heads and head dimensions -- should be allocated across layers to\nbalance expressivity and efficiency. We first provide mathematical analysis on\nthe role of early layers in information extraction from an approximation\nperspective, with a theoretical characterization on the trade-off between the\nnumber of heads and head dimension under a fixed parameter budget. In addition,\nwe uncover and prove the \\emph{saturation} behavior of softmax activations:\nContinuously increasing head dimensions can lead to diminishing returns in\nlearning errors, particularly for long sequences. Supported by both theory and\nexperiments, this saturation pattern suggests that later layers can operate\nmore efficiently with reduced parameters. Combining these insights, we propose\nprincipled strategies for allocating attention heads and dimensions across\nTransformers' layers, shedding light on theoretically-grounded model efficiency\nof Transformer-based architectures.", "published": "2025-10-04 11:22:16", "link": "http://arxiv.org/abs/2510.03784v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cost Efficient Fairness Audit Under Partial Feedback", "abstract": "We study the problem of auditing the fairness of a given classifier under\npartial feedback, where true labels are available only for positively\nclassified individuals, (e.g., loan repayment outcomes are observed only for\napproved applicants). We introduce a novel cost model for acquiring additional\nlabeled data, designed to more accurately reflect real-world costs such as\ncredit assessment, loan processing, and potential defaults. Our goal is to find\noptimal fairness audit algorithms that are more cost-effective than random\nexploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no\nassumptions on the data distribution, and a mixture model, where features and\ntrue labels follow a mixture of exponential family distributions. In the\nblack-box setting, we propose a near-optimal auditing algorithm under mild\nassumptions and show that a natural baseline can be strictly suboptimal. In the\nmixture model setting, we design a novel algorithm that achieves significantly\nlower audit cost than the black-box case. Our approach leverages prior work on\nlearning from truncated samples and maximum-a-posteriori oracles, and extends\nknown results on spherical Gaussian mixtures to handle exponential family\nmixtures, which may be of independent interest. Moreover, our algorithms apply\nto popular fairness metrics including demographic parity, equal opportunity,\nand equalized odds. Empirically, we demonstrate strong performance of our\nalgorithms on real-world fair classification datasets like Adult Income and Law\nSchool, consistently outperforming natural baselines by around 50% in terms of\naudit cost.", "published": "2025-10-04 08:38:03", "link": "http://arxiv.org/abs/2510.03734v1", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach", "abstract": "Reinforcement learning (RL) has been widely applied to sequential decision\nmaking, where interpretability and performance are both critical for practical\nadoption. Current approaches typically focus on performance and rely on post\nhoc explanations to account for interpretability. Different from these\napproaches, we focus on designing an interpretability-oriented yet\nperformance-enhanced RL approach. Specifically, we propose a spectral based\nlinear RL method that extends the ridge regression-based approach through a\nspectral filter function. The proposed method clarifies the role of\nregularization in controlling estimation error and further enables the design\nof an adaptive regularization parameter selection strategy guided by the\nbias-variance trade-off principle. Theoretical analysis establishes\nnear-optimal bounds for both parameter estimation and generalization error.\nExtensive experiments on simulated environments and real-world datasets from\nKuaishou and Taobao demonstrate that our method either outperforms or matches\nexisting baselines in decision quality. We also conduct interpretability\nanalyses to illustrate how the learned policies make decisions, thereby\nenhancing user trust. These results highlight the potential of our approach to\nbridge the gap between RL theory and practical decision making, providing\ninterpretability, accuracy, and adaptability in management contexts.", "published": "2025-10-04 07:53:43", "link": "http://arxiv.org/abs/2510.03722v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning", "abstract": "Real-world graph datasets often consist of mixtures of populations, where\ngraphs are generated from multiple distinct underlying distributions. However,\nmodern representation learning approaches, such as graph contrastive learning\n(GCL) and augmentation methods like Mixup, typically overlook this mixture\nstructure. In this work, we propose a unified framework that explicitly models\ndata as a mixture of underlying probabilistic graph generative models\nrepresented by graphons. To characterize these graphons, we leverage graph\nmoments (motif densities) to cluster graphs arising from the same model. This\nenables us to disentangle the mixture components and identify their distinct\ngenerative mechanisms. This model-aware partitioning benefits two key graph\nlearning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data\naugmentation technique that interpolates in a semantically valid space guided\nby the estimated graphons, instead of assuming a single graphon per class. 2)\nFor GCL, it enables model-adaptive and principled augmentations. Additionally,\nby introducing a new model-aware objective, our proposed approach (termed MGCL)\nimproves negative sampling by restricting negatives to graphs from other\nmodels. We establish a key theoretical guarantee: a novel, tighter bound\nshowing that graphs sampled from graphons with small cut distance will have\nsimilar motif densities with high probability. Extensive experiments on\nbenchmark datasets demonstrate strong empirical performance. In unsupervised\nlearning, MGCL achieves state-of-the-art results, obtaining the top average\nrank across eight datasets. In supervised learning, GMAM consistently\noutperforms existing strategies, achieving new state-of-the-art accuracy in 6\nout of 7 datasets.", "published": "2025-10-04 06:03:04", "link": "http://arxiv.org/abs/2510.03690v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The analogy theorem in Hoare logic", "abstract": "The introduction of machine learning methods has led to significant advances\nin automation, optimization, and discoveries in various fields of science and\ntechnology. However, their widespread application faces a fundamental\nlimitation: the transfer of models between data domains generally lacks a\nrigorous mathematical justification. The key problem is the lack of formal\ncriteria to guarantee that a model trained on one type of data will retain its\nproperties on another.This paper proposes a solution to this problem by\nformalizing the concept of analogy between data sets and models using\nfirst-order logic and Hoare logic.We formulate and rigorously prove a theorem\nthat sets out the necessary and sufficient conditions for analogy in the task\nof knowledge transfer between machine learning models. Practical verification\nof the analogy theorem on model data obtained using the Monte Carlo method, as\nwell as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and\n0.88 for convolutional neural networks and random forests, respectively.The\nproposed approach not only allows us to justify the correctness of transfer\nbetween domains but also provides tools for comparing the applicability of\nmodels to different types of data.The main contribution of the work is a\nrigorous formalization of analogy at the level of program logic, providing\nverifiable guarantees of the correctness of knowledge transfer, which opens new\nopportunities for both theoretical research and the practical use of machine\nlearning models in previously inaccessible areas.", "published": "2025-10-04 05:59:43", "link": "http://arxiv.org/abs/2510.03685v1", "categories": ["stat.ML", "cs.LG", "math.LO", "stat.CO", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Group Policy Gradient", "abstract": "We introduce Group Policy Gradient (GPG), a family of critic-free\npolicy-gradient estimators for general MDPs. Inspired by the success of GRPO's\napproach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a\nlearned value function with a group-based Monte Carlo advantage estimator,\nremoving the memory, compute, and hyperparameter costs of training a critic\nwhile preserving PPO's clipped-objective structure. We prove the consistency of\nthe GPG estimator, analyze the bias-variance tradeoffs, and demonstrate\nempirically that GPG matches or outperforms PPO on standard benchmarks. GPG\nmakes better use of parallel simulations, which, together with its critic-free\ndesign, results in more efficient use of computational resources than PPO.", "published": "2025-10-04 05:20:44", "link": "http://arxiv.org/abs/2510.03679v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Sampling Data Structures for Tensor Products in Turnstile Streams", "abstract": "This paper studies the computational challenges of large-scale\nattention-based models in artificial intelligence by utilizing importance\nsampling methods in the streaming setting. Inspired by the classical definition\nof the $\\ell_2$ sampler and the recent progress of the attention scheme in\nLarge Language Models (LLMs), we propose the definition of the attention\nsampler. Our approach significantly reduces the computational burden of\ntraditional attention mechanisms. We analyze the effectiveness of the attention\nsampler from a theoretical perspective, including space and update time.\nAdditionally, our framework exhibits scalability and broad applicability across\nvarious model architectures and domains.", "published": "2025-10-04 05:16:52", "link": "http://arxiv.org/abs/2510.03678v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) are widely used to steer large language models\n(LLMs), based on the assumption that their interpretable features naturally\nenable effective model behavior steering. Yet, a fundamental question remains\nunanswered: does higher interpretability indeed imply better steering utility?\nTo answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,\nQwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,\nand evaluate their interpretability and steering utility based on SAEBench\n(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a\nrank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis\nreveals only a relatively weak positive association (tau b approx 0.298),\nindicating that interpretability is an insufficient proxy for steering\nperformance. We conjecture the interpretability utility gap may stem from the\nselection of SAE features, as not all of them are equally effective for\nsteering. To further find features that truly steer the behavior of LLMs, we\npropose a novel selection criterion called Delta Token Confidence, which\nmeasures how much amplifying a feature changes the next token distribution. We\nshow that our method improves the steering performance of three LLMs by 52.52\npercent compared to the current best output score based criterion\n(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token\nConfidence, the correlation between interpretability and utility vanishes (tau\nb approx 0), and can even become negative. This further highlights the\ndivergence between interpretability and utility for the most effective steering\nfeatures.", "published": "2025-10-04 04:14:50", "link": "http://arxiv.org/abs/2510.03659v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Implicit Models: Expressive Power Scales with Test-Time Compute", "abstract": "Implicit models, an emerging model class, compute outputs by iterating a\nsingle parameter block to a fixed point. This architecture realizes an\ninfinite-depth, weight-tied network that trains with constant memory,\nsignificantly reducing memory needs for the same level of performance compared\nto explicit models. While it is empirically known that these compact models can\noften match or even exceed larger explicit networks by allocating more\ntest-time compute, the underlying mechanism remains poorly understood.\n  We study this gap through a nonparametric analysis of expressive power. We\nprovide a strict mathematical characterization, showing that a simple and\nregular implicit operator can, through iteration, progressively express more\ncomplex mappings. We prove that for a broad class of implicit models, this\nprocess lets the model's expressive power scale with test-time compute,\nultimately matching a much richer function class. The theory is validated\nacross three domains: image reconstruction, scientific computing, and\noperations research, demonstrating that as test-time iterations increase, the\ncomplexity of the learned mapping rises, while the solution quality\nsimultaneously improves and stabilizes.", "published": "2025-10-04 02:49:22", "link": "http://arxiv.org/abs/2510.03638v1", "categories": ["cs.LG", "cs.AI", "math.RT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Handling Missing Data in Probabilistic Regression Trees: Methods and Implementation in R", "abstract": "Probabilistic Regression Trees (PRTrees) generalize traditional decision\ntrees by incorporating probability functions that associate each data point\nwith different regions of the tree, providing smooth decisions and continuous\nresponses. This paper introduces an adaptation of PRTrees capable of handling\nmissing values in covariates through three distinct approaches: (i) a uniform\nprobability method, (ii) a partial observation approach, and (iii) a\ndimension-reduced smoothing technique. The proposed methods preserve the\ninterpretability properties of PRTrees while extending their applicability to\nincomplete datasets. Simulation studies under MCAR conditions demonstrate the\nrelative performance of each approach, including comparisons with traditional\nregression trees on smooth function estimation tasks. The proposed methods,\ntogether with the original version, have been developed in R with highly\noptimized routines and are distributed in the PRTree package, publicly\navailable on CRAN. In this paper we also present and discuss the main\nfunctionalities of the PRTree package, providing researchers and practitioners\nwith new tools for incomplete data analysis.", "published": "2025-10-04 02:39:09", "link": "http://arxiv.org/abs/2510.03634v1", "categories": ["stat.ME", "stat.ML", "62G08, 62D10, 62H30, 62J02, 65C60"], "primary_category": "stat.ME"}
{"title": "Transformed $\\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding", "abstract": "Robust Principal Component Analysis (RPCA) aims to recover a low-rank\nstructure from noisy, partially observed data that is also corrupted by sparse,\npotentially large-magnitude outliers. Traditional RPCA models rely on convex\nrelaxations, such as nuclear norm and $\\ell_1$ norm, to approximate the rank of\na matrix and the $\\ell_0$ functional (the number of non-zero elements) of\nanother. In this work, we advocate a nonconvex regularization method, referred\nto as transformed $\\ell_1$ (TL1), to improve both approximations. The rationale\nis that by varying the internal parameter of TL1, its behavior asymptotically\napproaches either $\\ell_0$ or $\\ell_1$. Since the rank is equal to the number\nof non-zero singular values and the nuclear norm is defined as their sum,\napplying TL1 to the singular values can approximate either the rank or the\nnuclear norm, depending on its internal parameter. We conduct a fine-grained\ntheoretical analysis of statistical convergence rates, measured in the\nFrobenius norm, for both the low-rank and sparse components under general\nsampling schemes. These rates are comparable to those of the classical RPCA\nmodel based on the nuclear norm and $\\ell_1$ norm. Moreover, we establish\nconstant-order upper bounds on the estimated rank of the low-rank component and\nthe cardinality of the sparse component in the regime where TL1 behaves like\n$\\ell_0$, assuming that the respective matrices are exactly low-rank and\nexactly sparse. Extensive numerical experiments on synthetic data and\nreal-world applications demonstrate that the proposed approach achieves higher\naccuracy than the classic convex model, especially under non-uniform sampling\nschemes.", "published": "2025-10-04 02:09:55", "link": "http://arxiv.org/abs/2510.03624v1", "categories": ["stat.ML", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Neural Bayesian Filtering", "abstract": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining\ndistributions over hidden states, called beliefs, in partially observable\nsystems. NBF is trained to find a good latent representation of the beliefs\ninduced by a task. It maps beliefs to fixed-length embedding vectors, which\ncondition generative models for sampling. During filtering, particle-style\nupdates compute posteriors in this embedding space using incoming observations\nand the environment's dynamics. NBF combines the computational efficiency of\nclassical filters with the expressiveness of deep generative models - tracking\nrapidly shifting, multimodal beliefs while mitigating the risk of particle\nimpoverishment. We validate NBF in state estimation tasks in three partially\nobservable environments.", "published": "2025-10-04 01:58:55", "link": "http://arxiv.org/abs/2510.03614v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explore the Loss space with Hill-ADAM", "abstract": "This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus\ntowards escaping local minima in prescribed loss landscapes to find the global\nminimum. Hill-ADAM escapes minima by deterministically exploring the state\nspace. This eliminates uncertainty from random gradient updates in stochastic\nalgorithms while seldom converging at the first minimum that visits. In the\npaper we first derive an analytical approximation of the ADAM Optimizer step\nsize at a particular model state. From there define the primary condition\ndetermining ADAM limitations in escaping local minima. The proposed optimizer\nalgorithm Hill-ADAM alternates between error minimization and maximization. It\nmaximizes to escape the local minimum and minimizes again afterward. This\nalternation provides an overall exploration throughout the loss space. This\nallows the deduction of the global minimum's state. Hill-ADAM was tested with 5\nloss functions and 12 amber-saturated to cooler-shade image color correction\ninstances.", "published": "2025-10-04 01:57:46", "link": "http://arxiv.org/abs/2510.03613v1", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.m"], "primary_category": "cs.LG"}
{"title": "Understanding the Role of Training Data in Test-Time Scaling", "abstract": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures.", "published": "2025-10-04 01:38:48", "link": "http://arxiv.org/abs/2510.03605v1", "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model", "abstract": "Bayesian inference for doubly-intractable probabilistic graphical models\ntypically involves variations of the exchange algorithm or approximate Markov\nchain Monte Carlo (MCMC) samplers. However, existing methods for both classes\nof algorithms require either perfect samplers or sequential samplers for\ncomplex models, which are often either not available, or suffer from poor\nmixing, especially in high dimensions. We develop a method that does not\nrequire perfect or sequential sampling, and can be applied to both classes of\nmethods: exact and approximate MCMC. The key to our approach is to utilize the\ntractable independence model underlying an intractable probabilistic graphical\nmodel for the purpose of constructing a finite sample unbiased Monte Carlo (and\nnot MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out\nto be crucial for scalability in high dimensions. The method is demonstrated on\nthe Ising model. Gradient-based alternatives to construct a proposal, such as\nLangevin and Hamiltonian Monte Carlo approaches, also arise as a natural\ncorollary to our general procedure, and are demonstrated as well.", "published": "2025-10-04 00:34:25", "link": "http://arxiv.org/abs/2510.03587v1", "categories": ["stat.CO", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning", "abstract": "Learning dynamics is essential for model-based control and Reinforcement\nLearning in engineering systems, such as robotics and power systems. However,\nlimited system measurements, such as those from low-resolution sensors, demand\nsample-efficient learning. Symmetry provides a powerful inductive bias by\ncharacterizing equivariant relations in system states to improve sample\nefficiency. While recent methods attempt to discover symmetries from data, they\ntypically assume a single global symmetry group and treat symmetry discovery\nand dynamic learning as separate tasks, leading to limited expressiveness and\nerror accumulation. In this paper, we propose the Latent Mixture of Symmetries\n(Latent MoS), an expressive model that captures a mixture of symmetry-governed\nlatent factors from complex dynamical measurements. Latent MoS focuses on\ndynamic learning while locally and provably preserving the underlying symmetric\ntransformations. To further capture long-term equivariance, we introduce a\nhierarchical architecture that stacks MoS blocks. Numerical experiments in\ndiverse physical systems demonstrate that Latent MoS outperforms\nstate-of-the-art baselines in interpolation and extrapolation tasks while\noffering interpretable latent representations suitable for future geometric and\nsafety-critical analyses.", "published": "2025-10-04 00:06:31", "link": "http://arxiv.org/abs/2510.03578v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "From Qubits to Rhythm: Exploring Quantum Random Walks in Rhythmspaces", "abstract": "A quantum computing algorithm for rhythm generation is presented, which aims\nto expand and explore quantum computing applications in the arts, particularly\nin music. The algorithm maps quantum random walk trajectories onto a\nrhythmspace -- a 2D interface that interpolates rhythmic patterns. The\nmethodology consists of three stages. The first stage involves designing\nquantum computing algorithms and establishing a mapping between the qubit space\nand the rhythmspace. To minimize circuit depth, a decomposition of a 2D quantum\nrandom walk into two 1D quantum random walks is applied. The second stage\nfocuses on biasing the directionality of quantum random walks by introducing\nclassical potential fields, adjusting the probability distribution of the wave\nfunction based on the position gradient within these fields. Four potential\nfields are implemented: a null potential, a linear field, a Gaussian potential,\nand a Gaussian potential under inertial dynamics. The third stage addresses the\nsonification of these paths by generating MIDI drum pattern messages and\ntransmitting them to a Digital Audio Workstation (DAW). This work builds upon\nexisting literature that applies quantum computing to simpler qubit spaces with\na few positions, extending the formalism to a 2D x-y plane. It serves as a\nproof of concept for scalable quantum computing-based generative random walk\nalgorithms in music and audio applications. Furthermore, the approach is\napplicable to generic multidimensional sound spaces, as the algorithms are not\nstrictly constrained to rhythm generation and can be adapted to different\nmusical structures.", "published": "2025-10-04 15:29:15", "link": "http://arxiv.org/abs/2510.03836v1", "categories": ["quant-ph", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "quant-ph"}
{"title": "A MATLAB toolbox for Computation of Speech Transmission Index (STI)", "abstract": "The speech transmission index (STI) is a popular simple metric for the\nprediction of speech intelligibility when speech is passed through a\ntransmission channel. Computation of STI from acoustic measurements is\ndescribed in the IEC 60268-16:2020 standard. Though, reliable implementations\nof STI are not publicly accessible and are frequently limited to the use with a\nproprietary measurement hardware. We present a Matlab STI implementation of\nboth the direct and indirect approaches according to the standard, including\nthe shortened STIPA protocol. The suggested implementation meets prescribed\nrequirements, as evidenced by tests on reference signals. Additionally, we\nconducted a verification measurement in comparison to a commercial measurement\ndevice. Our software comes with open source code.", "published": "2025-10-04 14:51:46", "link": "http://arxiv.org/abs/2510.03825v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech", "abstract": "Parkinson's Disease (PD) affects over 10 million people worldwide, with\nspeech impairments in up to 89% of patients. Current speech-based detection\nsystems analyze entire utterances, potentially overlooking the diagnostic value\nof specific phonetic elements. We developed a granularity-aware approach for\nmultilingual PD detection using an automated pipeline that extracts\ntime-aligned phonemes, syllables, and words from recordings. Using Italian,\nSpanish, and English datasets, we implemented a bidirectional LSTM with\nmulti-head attention to compare diagnostic performance across the different\ngranularity levels. Phoneme-level analysis achieved superior performance with\nAUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates\nenhanced diagnostic capability for cross-linguistic PD detection. Importantly,\nattention analysis revealed that the most informative speech features align\nwith those used in established clinical protocols: sustained vowels (/a/, /e/,\n/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)\nat syllable level, and /pataka/ sequences at word level. Source code will be\navailable at https://github.com/jetliqs/clearpd.", "published": "2025-10-04 09:51:00", "link": "http://arxiv.org/abs/2510.03758v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "D\u00e9sentrelacement Fr\u00e9quentiel Doux pour les Codecs Audio Neuronaux", "abstract": "While neural-based models have led to significant advancements in audio\nfeature extraction, the interpretability of the learned representations remains\na critical challenge. To address this, disentanglement techniques have been\nintegrated into discrete neural audio codecs to impose structure on the\nextracted tokens. However, these approaches often exhibit strong dependencies\non specific datasets or task formulations. In this work, we propose a\ndisentangled neural audio codec that leverages spectral decomposition of\ntime-domain signals to enhance representation interpretability. Experimental\nevaluations demonstrate that our method surpasses a state-of-the-art baseline\nin both reconstruction fidelity and perceptual quality.", "published": "2025-10-04 08:55:27", "link": "http://arxiv.org/abs/2510.03741v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation", "abstract": "Acoustic scene classification (ASC) models on edge devices typically operate\nunder fixed class assumptions, lacking the transferability needed for\nreal-world applications that require adaptation to new or refined acoustic\ncategories. We propose ContrastASC, which learns generalizable acoustic scene\nrepresentations by structuring the embedding space to preserve semantic\nrelationships between scenes, enabling adaptation to unseen categories without\nretraining. Our approach combines supervised contrastive fine-tuning of\npre-trained models with contrastive representation distillation to transfer\nthis structured knowledge to compact student models. Our evaluation shows that\nContrastASC demonstrates improved few-shot adaptation to unseen categories\nwhile maintaining strong closed-set performance.", "published": "2025-10-04 08:20:50", "link": "http://arxiv.org/abs/2510.03728v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition", "abstract": "We propose a speaker-attributed (SA) Whisper-based model for multi-talker\nspeech recognition that combines target-speaker modeling with serialized output\ntraining (SOT). Our approach leverages a Diarization-Conditioned Whisper\n(DiCoW) encoder to extract target-speaker embeddings, which are concatenated\ninto a single representation and passed to a shared decoder. This enables the\nmodel to transcribe overlapping speech as a serialized output stream with\nspeaker tags and timestamps. In contrast to target-speaker ASR systems such as\nDiCoW, which decode each speaker separately, our approach performs joint\ndecoding, allowing the decoder to condition on the context of all speakers\nsimultaneously. Experiments show that the model outperforms existing SOT-based\napproaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).", "published": "2025-10-04 08:02:23", "link": "http://arxiv.org/abs/2510.03723v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling Multi-Talker ASR with Speaker-Agnostic Activity Streams", "abstract": "An increasingly common training paradigm for multi-talker automatic speech\nrecognition (ASR) is to use speaker activity signals to adapt single-speaker\nASR models for overlapping speech. Although effective, these systems require\nrunning the ASR model once per speaker, resulting in inference costs that scale\nwith the number of speakers and limiting their practicality. In this work, we\npropose a method that decouples the inference cost of activity-conditioned ASR\nsystems from the number of speakers by converting speaker-specific activity\noutputs into two speaker-agnostic streams. A central challenge is that\nna\\\"ively merging speaker activities into streams significantly degrades\nrecognition, since pretrained ASR models assume contiguous, single-speaker\ninputs. To address this, we design new heuristics aimed at preserving\nconversational continuity and maintaining compatibility with existing systems.\nWe show that our approach is compatible with Diarization-Conditioned Whisper\n(DiCoW) to greatly reduce runtimes on the AMI and ICSI meeting datasets while\nretaining competitive performance.", "published": "2025-10-04 02:28:40", "link": "http://arxiv.org/abs/2510.03630v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications", "abstract": "This paper investigates the robustness of affine frequency division\nmultiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation\nschemes against non-white Gaussian noise, which can model various sources of\nadditive disturbances to the received signal. The proposed approach\ndemonstrates that the performance of these waveforms depends on the ability of\nthe demodulation matrix to whiten the noise-a property that is, in turn,\nrelated to the sparsity of the matrix. AFDM is shown to outperform OTFS and\northogonal frequency division multiplexing (OFDM), as its demodulation matrix\nis generally less sparse than those of the other waveforms. Based on this\nanalysis, several application examples and use cases are presented, such as the\nuse of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.\nFinally, simulation results confirm that AFDM achieves better performance than\nOTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in\nmost application scenarios.", "published": "2025-10-04 18:53:13", "link": "http://arxiv.org/abs/2510.03901v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Robust Beamforming for Magnetic Induction Based Underground Emergency Communications", "abstract": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. Based on channel state information (CSI), magnetic beamforming can\nsignificantly improve the performance of MI communication. However, in\npost-disaster underground communication, channel estimation may suffer from\nerrors due to factors such as complex environmental interferences. Taking\nchannel estimation error into account, we formulate a beamforming optimization\nproblem for multi-user MI underground emergency communications, which aims to\nminimize the power consumption under the constraints of sum rate and signal to\ninterference plus noise ratio (SINR) of each user. Based on the worst-case\noptimization criterion and the S-procedure, the non-convex optimization problem\nis transformed into convex and solved. Numerical results show that the proposed\nrobust beamforming scheme can effectively enhance communication reliability and\neffective throughput in the presence of channel estimation errors.", "published": "2025-10-04 15:54:16", "link": "http://arxiv.org/abs/2510.03852v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On the Exact Sum PDF and CDF of \u03b1-\u03bc Variates", "abstract": "The sum of random variables (RVs) appears extensively in wireless\ncommunications, at large, both conventional and advanced, and has been subject\nof longstanding research. The statistical characterization of the referred sum\nis crucial to determine the performance of such communications systems.\nAlthough efforts have been undertaken to unveil these sum statistics, e.g.,\nprobability density function (PDF) and cumulative distribution function (CDF),\nno general efficient nor manageable solutions capable of evaluating the exact\nsum PDF and CDF are available to date. The only formulations are given in terms\nof either the multi-fold Brennan's integral or the multivariate Fox H-function.\nUnfortunately, these methods are only feasible up to a certain number of RVs,\nmeaning that when the number of RVs in the sum increases, the computation of\nthe sum PDF and CDF is subject to stability problems, convergence issues, or\ninaccurate results. In this paper, we derive new, simple, exact formulations\nfor the PDF and CDF of the sum of L independent and identically distributed\n{\\alpha}-{\\mu} RVs. Unlike the available solutions, the computational\ncomplexity of our analytical expressions is independent of the number of\nsummands. Capitalizing on our unprecedented findings, we analyze, in exact and\nasymptotic manners, the performance of L-branch pre-detection equal-gain\ncombining and maximal-ratio combining receivers over {\\alpha}-{\\mu} fading\nenvironments. The coding and diversity gains of the system for both receivers\nare analyzed and quantified. Moreover, numerical simulations show that the\ncomputation time reduces drastically when using our expressions, which are\narguably the most efficient and manageable formulations derived so far.", "published": "2025-10-04 15:51:56", "link": "http://arxiv.org/abs/2510.03850v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums", "abstract": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. However, the propagation mediums in practical MI based underground\nemergency communications are usually diverse and composed randomly due to the\nimpact of disasters, which poses a challenge for MI communication in practical\napplications. In this paper, we formulate a statistical fading channel model,\nwhich reflects the random composition of diverse mediums and is shown to follow\na lognormal distribution. To mitigate the impact of diverse medium fading,\nMulti-frequency Resonating Compensation (MuReC) based coils are used to achieve\nmultiband transmission. Then, we analyze the performance of MuReC based\nmulti-band MI communication with diverse medium fading and derive the\nexpressions of signal-to-noise ratio (SNR) probability density functions,\nergodic capacities, average bit error rates (BERs), and outage probabilities\nfor both multiplexing and diversity cases. Numerical results show that MuReC\nbased multiband transmission schemes can effectively reduce the impact of\ndiverse medium fading and enhance the performance.", "published": "2025-10-04 15:48:28", "link": "http://arxiv.org/abs/2510.03848v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models", "abstract": "There are limitations of traditional methods and deep learning methods in\nterms of interpretability, generalization, and quantification of uncertainty in\nindustrial fault diagnosis, and there are core problems of insufficient\ncredibility in industrial fault diagnosis. The architecture performs\npreliminary analysis through a Bayesian network-based diagnostic engine and\nfeatures an LLM-driven cognitive quorum module with multimodal input\ncapabilities. The module conducts expert-level arbitration of initial diagnoses\nby analyzing structured features and diagnostic charts, prioritizing final\ndecisions after conflicts are identified. To ensure the reliability of the\nsystem output, the architecture integrates a confidence calibration module\nbased on temperature calibration and a risk assessment module, which\nobjectively quantifies the reliability of the system using metrics such as\nexpected calibration error (ECE). Experimental results on a dataset containing\nmultiple fault types showed that the proposed framework improved diagnostic\naccuracy by more than 28 percentage points compared to the baseline model,\nwhile the calibrated ECE was reduced by more than 75%. Case studies have\nconfirmed that HCAA effectively corrects misjudgments caused by complex feature\npatterns or knowledge gaps in traditional models, providing novel and practical\nengineering solutions for building high-trust, explainable AI diagnostic\nsystems for industrial applications.", "published": "2025-10-04 14:11:13", "link": "http://arxiv.org/abs/2510.03815v1", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms", "abstract": "Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G\nwireless networks. The bandwidth availability and diversity of FR3 offer\nunprecedented opportunities for coherent multiband Integrated Sensing and\nCommunications (ISAC), which aggregates the carrier phase information from\nmultiple frequency bands to increase the sensing resolution to the cm-level.\nHowever, the frequency anisotropy of sensing targets over GHz-wide bands and\nthe non-contiguity of the 6G spectrum, pose critical challenges to the\napplication of existing multiband ISAC techniques. We present the first study\non coherent multiband sensing in FR3. We experimentally characterize the\nfrequency anisotropy of targets and propose new phase coherence metrics for\nmultiband processing. Then, we analyze the impact of non-contiguous FR3 bands\nconsidered by 3GPP, and design a new algorithm to mitigate the resulting\nsensing artifacts, outperforming existing techniques. Our results represent a\nfirst step toward fully developing multiband ISAC for FR3.", "published": "2025-10-04 11:39:10", "link": "http://arxiv.org/abs/2510.03787v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification", "abstract": "Cardiovascular disease (CVD) is a major pediatric health burden, and early\nscreening is of critical importance. Electrocardiography (ECG), as a\nnoninvasive and accessible tool, is well suited for this purpose. This paper\npresents the first benchmark study of deep learning for multi-label pediatric\nCVD classification on the recently released ZZU-pECG dataset, comprising 3716\nrecordings with 19 CVD categories. We systematically evaluate four\nrepresentative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under\nboth 9-lead and 12-lead configurations. All models achieved strong results,\nwith Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.\nResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and\nTransformer also showed competitive performance. Per-class analysis indicated\nchallenges for rare conditions such as hypertrophic cardiomyopathy in the\n9-lead subset, reflecting the effect of limited positive samples. This\nbenchmark establishes reusable baselines and highlights complementary strengths\nacross paradigms. It further points to the need for larger-scale, multi-center\nvalidation, age-stratified analysis, and broader disease coverage to support\nreal-world pediatric ECG applications.", "published": "2025-10-04 11:08:46", "link": "http://arxiv.org/abs/2510.03780v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "abstract": "The increasing size and complexity of medical imaging datasets, particularly\nin 3D formats, present significant barriers to collaborative research and\ntransferability. This study investigates whether the ZFP compression technique\ncan mitigate these challenges without compromising the performance of automated\ncerebrovascular segmentation, a critical first step in intracranial aneurysm\ndetection. We apply ZFP in both its error tolerance and fixed-rate modes to a\nlarge scale, and one of the most recent, datasets in the literature, 3D medical\ndataset containing ground-truth vascular segmentations. The segmentation\nquality on the compressed volumes is rigorously compared to the uncompressed\nbaseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can\nachieve substantial data reduction--up to a 22.89:1 ratio in error tolerance\nmode--while maintaining a high degree of fidelity, with the mean Dice\ncoefficient remaining high at 0.87656. These results demonstrate that ZFP is a\nviable and powerful tool for enabling more efficient and accessible research on\nlarge-scale medical datasets, fostering broader collaboration across the\ncommunity.", "published": "2025-10-04 10:37:34", "link": "http://arxiv.org/abs/2510.03769v1", "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?", "abstract": "In this paper, sensing-assisted secure communication in a multi-user\nmulti-eavesdropper integrated sensing and communication (ISAC) system is\ninvestigated. Confidential communication signals and dedicated sensing signals\nare jointly transmitted by a base station (BS) to simultaneously serve users\nand sense aerial eavesdroppers (AEs). A sum rate maximization problem is\nformulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and\nsensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A\nfractional-programming-based alternating optimization algorithm is developed to\nsolve this problem for fully digital arrays, where successive convex\napproximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle\nnon-convex constraints. Furthermore, the minimum number of dedicated sensing\nbeams is analyzed via a worst-case rank bound, upon which the proposed\nbeamforming design is further extended to the hybrid analog-digital (HAD) array\narchitecture, where the unit-modulus constraint is addressed by manifold\noptimization. Simulation results demonstrate that only a small number of\nsensing beams are sufficient for both sensing and jamming AEs, and the proposed\ndesigns consistently outperform strong baselines while also revealing the\ncommunication-sensing trade-off.", "published": "2025-10-04 09:28:47", "link": "http://arxiv.org/abs/2510.03749v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Pinching Antenna Systems (PASS) for Cell-Free Communications", "abstract": "A pinching antenna system (PASS) assisted cell-free communication system is\nproposed. A sum rate maximization problem under the BS power budget constraint\nand PA deployment constraint is formulated. To tackle the proposed non-convex\noptimization problem, an alternating optimization (AO) algorithm is developed.\nIn particular, the digital beamforming sub-problem is solved using the weighted\nminimum mean square error (WMMSE) method, whereas the pinching beamforming\nsub-problem is handled via a penalty based approach combined with element-wise\noptimization. Simulation results demonstrate that: 1) the PASS assisted\ncell-free systems achieve superior performance over benchmark schemes; 2)\nincreasing the number of PAs per waveguides can improve the advantage of PASS\nassisted cell-free systems; and 3) the cell-free architecture mitigates the\naverage user rate degradation as the number of users increases.", "published": "2025-10-04 02:23:28", "link": "http://arxiv.org/abs/2510.03628v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models", "abstract": "Significant studies on communications over doubly selective channels have\nutilized on-grid DD channel models, which are previously investigated in\nBello's seminar paper in 1963. The DD grid is typically specified by the\nbandwidth and time duration of the transmission frames. However, the physical\nchannels are determined by the propagation environments and they are typically\noff-grid. Hence, there is often a gap between an actual physical channel and\nthe on-grid model. This paper revisits the on-grid modeling of practical\nphysical channels. We study the associated on-grid DD-domain representations\nfor continuous-time, doubly selective channels with off-grid delay and Doppler\nshifts, accounting for practical time/frequency-domain windowing at the\ntransceivers. The universal models obtained are applicable under the mild\nassumption that the windows have finite supports, and they extend Bello's\nclassical results to account for more general windows. We also discuss the\nfeatures and implications of the equivalent on-grid models.", "published": "2025-10-04 02:20:29", "link": "http://arxiv.org/abs/2510.03626v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation", "abstract": "The rising aging population has increased the importance of fall detection\n(FD) systems as an assistive technology, where deep learning techniques are\nwidely applied to enhance accuracy. FD systems typically use edge devices (EDs)\nworn by individuals to collect real-time data, which are transmitted to a cloud\ncenter (CC) or processed locally. However, this architecture faces challenges\nsuch as a limited ED model size and data transmission latency to the CC. Mobile\nedge computing (MEC), which allows computations at MEC servers deployed between\nEDs and CC, has been explored to address these challenges. We propose a\nmultilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC\nsplits the architecture into stations, each with a neural network model. If\nfront-end equipment cannot detect falls reliably, data are transmitted to a\nstation with more robust back-end computing. The knowledge distillation (KD)\napproach was employed to improve front-end detection accuracy by allowing\nhigh-power back-end stations to provide additional learning experiences,\nenhancing precision while reducing latency and processing loads. Simulation\nresults demonstrate that the KD approach improved accuracy by 11.65% on the\nSisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also\nreduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on\nthe SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD\nsystem exhibits improved accuracy and reduced latency.", "published": "2025-10-04 01:31:33", "link": "http://arxiv.org/abs/2510.03601v1", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SP", "I.2.6; C.2.4"], "primary_category": "cs.LG"}
{"title": "Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems", "abstract": "Fluid antenna systems (FAS) are emerging as a transformative enabler for\nsixth-generation (6G) wireless communications, providing unprecedented spatial\ndiversity through dynamic reconfiguration of antenna ports. However, the\ninherent spatial correlation among ports poses significant challenges for\naccurate analysis. Conventional models such as Jakes are analytically\nintractable, while oversimplified constant-correlation models fail to capture\nthe true behavior. In this work, we address these challenges by applying the\nvariable block-correlation model (VBCM) -- originally proposed by\nRam\\'{i}rez-Espinosa \\textit{et al.} in 2024 -- to FAS security analysis, and\nby developing comprehensive optimization methods to enhance analytical\naccuracy. We derive new closed-form expressions for average secrecy capacity\n(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM\nframework achieves simulation-aligned accuracy, with relative errors\nconsistently below $5\\%$ (compared to $10$--$15\\%$ for constant-correlation\nmodels). To maximize ASC, we further design two algorithms: a grid search (GS)\nmethod and a gradient descent (GD) method. Numerical results reveal that the\nVBCM-based approach not only provides reliable insights into FAS security\nperformance, but also yields substantial gains -- ASC improvements exceeding\n$120\\%$ in high-threat scenarios and $18$--$19\\%$ performance enhancements for\ncompact antenna configurations. These findings underscore the practical value\nof integrating VBCM into FAS security analysis and optimization, establishing\nit as a powerful tool for advancing 6G communication systems.", "published": "2025-10-04 00:50:16", "link": "http://arxiv.org/abs/2510.03594v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
