{"title": "SHAPE: Shifted Absolute Position Embedding for Transformers", "abstract": "Position representation is crucial for building position-aware\nrepresentations in Transformers. Existing position representations suffer from\na lack of generalization to test data with unseen lengths or high computational\ncost. We investigate shifted absolute position embedding (SHAPE) to address\nboth issues. The basic idea of SHAPE is to achieve shift invariance, which is a\nkey property of recent successful position representations, by randomly\nshifting absolute positions during training. We demonstrate that SHAPE is\nempirically comparable to its counterpart while being simpler and faster.", "published": "2021-09-13 00:10:02", "link": "http://arxiv.org/abs/2109.05644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Select One Among All? An Extensive Empirical Study Towards the\n  Robustness of Knowledge Distillation in Natural Language Understanding", "abstract": "Knowledge Distillation (KD) is a model compression algorithm that helps\ntransfer the knowledge of a large neural network into a smaller one. Even\nthough KD has shown promise on a wide range of Natural Language Processing\n(NLP) applications, little is understood about how one KD algorithm compares to\nanother and whether these approaches can be complimentary to each other. In\nthis work, we evaluate various KD algorithms on in-domain, out-of-domain and\nadversarial testing. We propose a framework to assess the adversarial\nrobustness of multiple KD algorithms. Moreover, we introduce a new KD\nalgorithm, Combined-KD, which takes advantage of two promising approaches\n(better training scheme and more efficient data augmentation). Our extensive\nexperimental results show that Combined-KD achieves state-of-the-art results on\nthe GLUE benchmark, out-of-domain generalization, and adversarial robustness\ncompared to competitive methods.", "published": "2021-09-13 04:08:36", "link": "http://arxiv.org/abs/2109.05696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for Context-aware Neural Machine TranslationUsing\n  Coreference Information", "abstract": "Context-aware neural machine translation (NMT) incorporates contextual\ninformation of surrounding texts, that can improve the translation quality of\ndocument-level machine translation. Many existing works on context-aware NMT\nhave focused on developing new model architectures for incorporating additional\ncontexts and have shown some promising results. However, most existing works\nrely on cross-entropy loss, resulting in limited use of contextual information.\nIn this paper, we propose CorefCL, a novel data augmentation and contrastive\nlearning scheme based on coreference between the source and contextual\nsentences. By corrupting automatically detected coreference mentions in the\ncontextual sentence, CorefCL can train the model to be sensitive to coreference\ninconsistency. We experimented with our method on common context-aware NMT\nmodels and two document-level translation tasks. In the experiments, our method\nconsistently improved BLEU of compared models on English-German and\nEnglish-Korean tasks. We also show that our method significantly improves\ncoreference resolution in the English-German contrastive test suite.", "published": "2021-09-13 05:18:47", "link": "http://arxiv.org/abs/2109.05712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuVER: Improving First-Stage Entity Retrieval with Multi-View Entity\n  Representations", "abstract": "Entity retrieval, which aims at disambiguating mentions to canonical entities\nfrom massive KBs, is essential for many tasks in natural language processing.\nRecent progress in entity retrieval shows that the dual-encoder structure is a\npowerful and efficient framework to nominate candidates if entities are only\nidentified by descriptions. However, they ignore the property that meanings of\nentity mentions diverge in different contexts and are related to various\nportions of descriptions, which are treated equally in previous works. In this\nwork, we propose Multi-View Entity Representations (MuVER), a novel approach\nfor entity retrieval that constructs multi-view representations for entity\ndescriptions and approximates the optimal view for mentions via a heuristic\nsearching method. Our method achieves the state-of-the-art performance on\nZESHEL and improves the quality of candidates on three standard Entity Linking\ndatasets", "published": "2021-09-13 05:51:45", "link": "http://arxiv.org/abs/2109.05716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language\n  Understanding and Generation", "abstract": "In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed to utilize the shared knowledge\nbetween natural language understanding (NLU) and natural language generation\n(NLG) to boost the performance. CPT consists of three parts: a shared encoder,\nan understanding decoder, and a generation decoder. Two specific decoders with\na shared encoder are pre-trained with masked language modeling (MLM) and\ndenoising auto-encoding (DAE) tasks, respectively. With the partially shared\narchitecture and multi-task pre-training, CPT can (1) learn specific knowledge\nof both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that\nfully exploits the potential of the model. Moreover, the unbalanced Transformer\nsaves the computational and storage cost, which makes CPT competitive and\ngreatly accelerates the inference of text generation. Experimental results on a\nwide range of Chinese NLU and NLG tasks show the effectiveness of CPT.", "published": "2021-09-13 06:25:45", "link": "http://arxiv.org/abs/2109.05729v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Entity Typing via Label Reasoning", "abstract": "Conventional entity typing approaches are based on independent classification\nparadigms, which make them difficult to recognize inter-dependent, long-tailed\nand fine-grained entity types. In this paper, we argue that the implicitly\nentailed extrinsic and intrinsic dependencies between labels can provide\ncritical knowledge to tackle the above challenges. To this end, we propose\n\\emph{Label Reasoning Network(LRN)}, which sequentially reasons fine-grained\nentity labels by discovering and exploiting label dependencies knowledge\nentailed in the data. Specifically, LRN utilizes an auto-regressive network to\nconduct deductive reasoning and a bipartite attribute graph to conduct\ninductive reasoning between labels, which can effectively model, learn and\nreason complex label dependencies in a sequence-to-set, end-to-end manner.\nExperiments show that LRN achieves the state-of-the-art performance on standard\nultra fine-grained entity typing benchmarks, and can also resolve the long tail\nlabel problem effectively.", "published": "2021-09-13 07:08:47", "link": "http://arxiv.org/abs/2109.05744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection\n  via Causal Intervention", "abstract": "Event detection has long been troubled by the \\emph{trigger curse}:\noverfitting the trigger will harm the generalization ability while underfitting\nit will hurt the detection performance. This problem is even more severe in\nfew-shot scenario. In this paper, we identify and solve the trigger curse\nproblem in few-shot event detection (FSED) from a causal view. By formulating\nFSED with a structural causal model (SCM), we found that the trigger is a\nconfounder of the context and the result, which makes previous FSED methods\nmuch easier to overfit triggers. To resolve this problem, we propose to\nintervene on the context via backdoor adjustment during training. Experiments\nshow that our method significantly improves the FSED on ACE05, MAVEN and KBP17\ndatasets.", "published": "2021-09-13 07:11:30", "link": "http://arxiv.org/abs/2109.05747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perturbation CheckLists for Evaluating NLG Evaluation Metrics", "abstract": "Natural Language Generation (NLG) evaluation is a multifaceted task requiring\nassessment of multiple desirable criteria, e.g., fluency, coherency, coverage,\nrelevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG\ntasks, we observe that the human evaluation scores on these multiple criteria\nare often not correlated. For example, there is a very low correlation between\nhuman scores on fluency and data coverage for the task of structured data to\ntext generation. This suggests that the current recipe of proposing new\nautomatic evaluation metrics for NLG by showing that they correlate well with\nscores assigned by humans for a single criteria (overall quality) alone is\ninadequate. Indeed, our extensive study involving 25 automatic evaluation\nmetrics across 6 different tasks and 18 different evaluation criteria shows\nthat there is no single metric which correlates well with human scores on all\ndesirable criteria, for most NLG tasks. Given this situation, we propose\nCheckLists for better design and evaluation of automatic metrics. We design\ntemplates which target a specific criteria (e.g., coverage) and perturb the\noutput such that the quality gets affected only along this specific criteria\n(e.g., the coverage drops). We show that existing evaluation metrics are not\nrobust against even such simple perturbations and disagree with scores assigned\nby humans to the perturbed output. The proposed templates thus allow for a\nfine-grained assessment of automatic evaluation metrics exposing their\nlimitations and will facilitate better design, analysis and evaluation of such\nmetrics.", "published": "2021-09-13 08:26:26", "link": "http://arxiv.org/abs/2109.05771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wine is Not v i n. -- On the Compatibility of Tokenizations Across\n  Languages", "abstract": "The size of the vocabulary is a central design choice in large pretrained\nlanguage models, with respect to both performance and memory requirements.\nTypically, subword tokenization algorithms such as byte pair encoding and\nWordPiece are used. In this work, we investigate the compatibility of\ntokenizations for multilingual static and contextualized embedding spaces and\npropose a measure that reflects the compatibility of tokenizations across\nlanguages. Our goal is to prevent incompatible tokenizations, e.g., \"wine\"\n(word-level) in English vs.\\ \"v i n\" (character-level) in French, which make it\nhard to learn good multilingual semantic representations. We show that our\ncompatibility measure allows the system designer to create vocabularies across\nlanguages that are compatible -- a desideratum that so far has been neglected\nin multilingual models.", "published": "2021-09-13 08:31:01", "link": "http://arxiv.org/abs/2109.05772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text is NOT Enough: Integrating Visual Impressions into Open-domain\n  Dialogue Generation", "abstract": "Open-domain dialogue generation in natural language processing (NLP) is by\ndefault a pure-language task, which aims to satisfy human need for daily\ncommunication on open-ended topics by producing related and informative\nresponses. In this paper, we point out that hidden images, named as visual\nimpressions (VIs), can be explored from the text-only data to enhance dialogue\nunderstanding and help generate better responses. Besides, the semantic\ndependency between an dialogue post and its response is complicated, e.g., few\nword alignments and some topic transitions. Therefore, the visual impressions\nof them are not shared, and it is more reasonable to integrate the response\nvisual impressions (RVIs) into the decoder, rather than the post visual\nimpressions (PVIs). However, both the response and its RVIs are not given\ndirectly in the test process. To handle the above issues, we propose a\nframework to explicitly construct VIs based on pure-language dialogue datasets\nand utilize them for better dialogue understanding and generation.\nSpecifically, we obtain a group of images (PVIs) for each post based on a\npre-trained word-image mapping model. These PVIs are used in a co-attention\nencoder to get a post representation with both visual and textual information.\nSince the RVIs are not provided directly during testing, we design a cascade\ndecoder that consists of two sub-decoders. The first sub-decoder predicts the\ncontent words in response, and applies the word-image mapping model to get\nthose RVIs. Then, the second sub-decoder generates the response based on the\npost and RVIs. Experimental results on two open-domain dialogue datasets show\nthat our proposed approach achieves superior performance over competitive\nbaselines.", "published": "2021-09-13 08:57:13", "link": "http://arxiv.org/abs/2109.05778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effectiveness of Pre-training for Few-shot Intent Classification", "abstract": "This paper investigates the effectiveness of pre-training for few-shot intent\nclassification. While existing paradigms commonly further pre-train language\nmodels such as BERT on a vast amount of unlabeled corpus, we find it highly\neffective and efficient to simply fine-tune BERT with a small set of labeled\nutterances from public datasets. Specifically, fine-tuning BERT with roughly\n1,000 labeled data yields a pre-trained model -- IntentBERT, which can easily\nsurpass the performance of existing pre-trained models for few-shot intent\nclassification on novel domains with very different semantics. The high\neffectiveness of IntentBERT confirms the feasibility and practicality of\nfew-shot intent detection, and its high generalization ability across different\ndomains suggests that intent classification tasks may share a similar\nunderlying structure, which can be efficiently learned from a small set of\nlabeled data. The source code can be found at\nhttps://github.com/hdzhang-code/IntentBERT.", "published": "2021-09-13 09:00:09", "link": "http://arxiv.org/abs/2109.05782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Virtual Data Augmentation: A Robust and General Framework for\n  Fine-tuning Pre-trained Models", "abstract": "Recent works have shown that powerful pre-trained language models (PLM) can\nbe fooled by small perturbations or intentional attacks. To solve this issue,\nvarious data augmentation techniques are proposed to improve the robustness of\nPLMs. However, it is still challenging to augment semantically relevant\nexamples with sufficient diversity. In this work, we present Virtual Data\nAugmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on\nthe original token embeddings, we construct a multinomial mixture for\naugmenting virtual data embeddings, where a masked language model guarantees\nthe semantic relevance and the Gaussian noise provides the augmentation\ndiversity. Furthermore, a regularized training strategy is proposed to balance\nthe two aspects. Extensive experiments on six datasets show that our approach\nis able to improve the robustness of PLMs and alleviate the performance\ndegradation under adversarial attacks. Our codes and data are publicly\navailable at \\textcolor{blue}{\\url{https://github.com/RUCAIBox/VDA}}.", "published": "2021-09-13 09:15:28", "link": "http://arxiv.org/abs/2109.05793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Show Me How To Revise: Improving Lexically Constrained Sentence\n  Generation with XLNet", "abstract": "Lexically constrained sentence generation allows the incorporation of prior\nknowledge such as lexical constraints into the output. This technique has been\napplied to machine translation, and dialog response generation. Previous work\nusually used Markov Chain Monte Carlo (MCMC) sampling to generate lexically\nconstrained sentences, but they randomly determined the position to be edited\nand the action to be taken, resulting in many invalid refinements. To overcome\nthis challenge, we used a classifier to instruct the MCMC-based models where\nand how to refine the candidate sentences. First, we developed two methods to\ncreate synthetic data on which the pre-trained model is fine-tuned to obtain a\nreliable classifier. Next, we proposed a two-step approach, \"Predict and\nRevise\", for constrained sentence generation. During the predict step, we\nleveraged the classifier to compute the learned prior for the candidate\nsentence. During the revise step, we resorted to MCMC sampling to revise the\ncandidate sentence by conducting a sampled action at a sampled position drawn\nfrom the learned prior. We compared our proposed models with many strong\nbaselines on two tasks, generating sentences with lexical constraints and text\ninfilling. Experimental results have demonstrated that our proposed model\nperforms much better than the previous work in terms of sentence fluency and\ndiversity. Our code and pre-trained models are available at\nhttps://github.com/NLPCode/MCMCXLNet.", "published": "2021-09-13 09:21:07", "link": "http://arxiv.org/abs/2109.05797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding End-to-End Question Answering on Differentiable Knowledge\n  Graphs with Intersection", "abstract": "End-to-end question answering using a differentiable knowledge graph is a\npromising technique that requires only weak supervision, produces interpretable\nresults, and is fully differentiable. Previous implementations of this\ntechnique (Cohen et al., 2020) have focused on single-entity questions using a\nrelation following operation. In this paper, we propose a model that explicitly\nhandles multiple-entity questions by implementing a new intersection operation,\nwhich identifies the shared elements between two sets of entities. We find that\nintroducing intersection improves performance over a baseline model on two\ndatasets, WebQuestionsSP (69.6% to 73.3% Hits@1) and ComplexWebQuestions (39.8%\nto 48.7% Hits@1), and in particular, improves performance on questions with\nmultiple entities by over 14% on WebQuestionsSP and by 19% on\nComplexWebQuestions.", "published": "2021-09-13 09:33:50", "link": "http://arxiv.org/abs/2109.05808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge\n  Distillation", "abstract": "With the rapid increase of multimedia data, a large body of literature has\nemerged to work on multimodal summarization, the majority of which target at\nrefining salient information from textual and visual modalities to output a\npictorial summary with the most relevant images. Existing methods mostly focus\non either extractive or abstractive summarization and rely on qualified image\ncaptions to build image references. We are the first to propose a Unified\nframework for Multimodal Summarization grounding on BART, UniMS, that\nintegrates extractive and abstractive objectives, as well as selecting the\nimage output. Specially, we adopt knowledge distillation from a vision-language\npretrained model to improve image selection, which avoids any requirement on\nthe existence and quality of image captions. Besides, we introduce a visual\nguided decoder to better integrate textual and visual modalities in guiding\nabstractive text generation. Results show that our best model achieves a new\nstate-of-the-art result on a large-scale benchmark dataset. The newly involved\nextractive objective as well as the knowledge distillation technique are proven\nto bring a noticeable improvement to the multimodal summarization task.", "published": "2021-09-13 09:36:04", "link": "http://arxiv.org/abs/2109.05812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring a Unified Sequence-To-Sequence Transformer for Medical Product\n  Safety Monitoring in Social Media", "abstract": "Adverse Events (AE) are harmful events resulting from the use of medical\nproducts. Although social media may be crucial for early AE detection, the\nsheer scale of this data makes it logistically intractable to analyze using\nhuman agents, with NLP representing the only low-cost and scalable alternative.\nIn this paper, we frame AE Detection and Extraction as a sequence-to-sequence\nproblem using the T5 model architecture and achieve strong performance\nimprovements over competitive baselines on several English benchmarks (F1 =\n0.71, 12.7% relative improvement for AE Detection; Strict F1 = 0.713, 12.4%\nrelative improvement for AE Extraction). Motivated by the strong commonalities\nbetween AE-related tasks, the class imbalance in AE benchmarks and the\nlinguistic and structural variety typical of social media posts, we propose a\nnew strategy for multi-task training that accounts, at the same time, for task\nand dataset characteristics. Our multi-task approach increases model\nrobustness, leading to further performance gains. Finally, our framework shows\nsome language transfer capabilities, obtaining higher performance than\nMultilingual BERT in zero-shot learning on French data.", "published": "2021-09-13 09:37:39", "link": "http://arxiv.org/abs/2109.05815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Entity Resolution and Question Answering Using Differentiable\n  Knowledge Graphs", "abstract": "Recently, end-to-end (E2E) trained models for question answering over\nknowledge graphs (KGQA) have delivered promising results using only a weakly\nsupervised dataset. However, these models are trained and evaluated in a\nsetting where hand-annotated question entities are supplied to the model,\nleaving the important and non-trivial task of entity resolution (ER) outside\nthe scope of E2E learning. In this work, we extend the boundaries of E2E\nlearning for KGQA to include the training of an ER component. Our model only\nneeds the question text and the answer entities to train, and delivers a\nstand-alone QA model that does not require an additional ER component to be\nsupplied during runtime. Our approach is fully differentiable, thanks to its\nreliance on a recent method for building differentiable KGs (Cohen et al.,\n2020). We evaluate our E2E trained model on two public datasets and show that\nit comes close to baseline models that use hand-annotated entities.", "published": "2021-09-13 09:40:16", "link": "http://arxiv.org/abs/2109.05817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Examples for Evaluating Math Word Problem Solvers", "abstract": "Standard accuracy metrics have shown that Math Word Problem (MWP) solvers\nhave achieved high performance on benchmark datasets. However, the extent to\nwhich existing MWP solvers truly understand language and its relation with\nnumbers is still unclear. In this paper, we generate adversarial attacks to\nevaluate the robustness of state-of-the-art MWP solvers. We propose two methods\nQuestion Reordering and Sentence Paraphrasing to generate adversarial attacks.\nWe conduct experiments across three neural MWP solvers over two benchmark\ndatasets. On average, our attack method is able to reduce the accuracy of MWP\nsolvers by over 40 percentage points on these datasets. Our results demonstrate\nthat existing MWP solvers are sensitive to linguistic variations in the problem\ntext. We verify the validity and quality of generated adversarial examples\nthrough human evaluation.", "published": "2021-09-13 12:47:40", "link": "http://arxiv.org/abs/2109.05925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting the Tesseract Open-Source OCR Engine for Tamil and Sinhala\n  Legacy Fonts and Creating a Parallel Corpus for Tamil-Sinhala-English", "abstract": "Most low-resource languages do not have the necessary resources to create\neven a substantial monolingual corpus. These languages may often be found in\ngovernment proceedings but mainly in Portable Document Format (PDF) that\ncontains legacy fonts. Extracting text from these documents to create a\nmonolingual corpus is challenging due to legacy font usage and printer-friendly\nencoding, which are not optimized for text extraction. Therefore, we propose a\nsimple, automatic, and novel idea that can scale for Tamil, Sinhala, English\nlanguages, and many documents along with parallel corpora. Since Tamil and\nSinhala are Low-Resource Languages, we improved the performance of Tesseract by\nemploying LSTM-based training on more than 20 legacy fonts to recognize printed\ncharacters in these languages. Especially, our model detects code-mixed text,\nnumbers, and special characters from the printed document. It is shown that\nthis approach can reduce the character-level error rate of Tesseract from 6.03\nto 2.61 for Tamil (-3.42% relative change) and 7.61 to 4.74 for Sinhala (-2.87%\nrelative change), as well as the word-level error rate from 39.68 to 20.61 for\nTamil (-19.07% relative change) and 35.04 to 26.58 for Sinhala (-8.46% relative\nchange) on the test set. Also, our newly created parallel corpus consists of\n185.4k, 168.9k, and 181.04k sentences and 2.11M, 2.22M, and 2.33M Words in\nTamil, Sinhala, and English respectively. This study shows that fine-tuning\nTesseract models on multiple new fonts help to understand the texts and\nenhances the performance of the OCR. We made newly trained models and the\nsource code for fine-tuning Tesseract, freely available.", "published": "2021-09-13 13:26:30", "link": "http://arxiv.org/abs/2109.05952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When is Wall a Pared and when a Muro? -- Extracting Rules Governing\n  Lexical Selection", "abstract": "Learning fine-grained distinctions between vocabulary items is a key\nchallenge in learning a new language. For example, the noun \"wall\" has\ndifferent lexical manifestations in Spanish -- \"pared\" refers to an indoor wall\nwhile \"muro\" refers to an outside wall. However, this variety of lexical\ndistinction may not be obvious to non-native learners unless the distinction is\nexplained in such a way. In this work, we present a method for automatically\nidentifying fine-grained lexical distinctions, and extracting concise\ndescriptions explaining these distinctions in a human- and machine-readable\nformat. We confirm the quality of these extracted descriptions in a language\nlearning setup for two languages, Spanish and Greek, where we use them to teach\nnon-native speakers when to translate a given ambiguous word into its different\npossible translations. Code and data are publicly released here\n(https://github.com/Aditi138/LexSelection)", "published": "2021-09-13 14:49:00", "link": "http://arxiv.org/abs/2109.06014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Traffic Event Detection as a Slot Filling Problem", "abstract": "In this paper, we introduce the new problem of extracting fine-grained\ntraffic information from Twitter streams by also making publicly available the\ntwo (constructed) traffic-related datasets from Belgium and the Brussels\ncapital region. In particular, we experiment with several models to identify\n(i) whether a tweet is traffic-related or not, and (ii) in the case that the\ntweet is traffic-related to identify more fine-grained information regarding\nthe event (e.g., the type of the event, where the event happened). To do so, we\nframe (i) the problem of identifying whether a tweet is a traffic-related event\nor not as a text classification subtask, and (ii) the problem of identifying\nmore fine-grained traffic-related information as a slot filling subtask, where\nfine-grained information (e.g., where an event has happened) is represented as\na slot/entity of a particular type. We propose the use of several methods that\nprocess the two subtasks either separately or in a joint setting, and we\nevaluate the effectiveness of the proposed methods for solving the traffic\nevent detection problem. Experimental results indicate that the proposed\narchitectures achieve high performance scores (i.e., more than 95% in terms of\nF$_{1}$ score) on the constructed datasets for both of the subtasks (i.e., text\nclassification and slot filling) even in a transfer learning scenario. In\naddition, by incorporating tweet-level information in each of the tokens\ncomprising the tweet (for the BERT-based model) can lead to a performance\nimprovement for the joint setting.", "published": "2021-09-13 15:02:40", "link": "http://arxiv.org/abs/2109.06035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmented Abstractive Summarization With Document-LevelSemantic Graph", "abstract": "Previous abstractive methods apply sequence-to-sequence structures to\ngenerate summary without a module to assist the system to detect vital mentions\nand relationships within a document. To address this problem, we utilize\nsemantic graph to boost the generation performance. Firstly, we extract\nimportant entities from each document and then establish a graph inspired by\nthe idea of distant supervision \\citep{mintz-etal-2009-distant}. Then, we\ncombine a Bi-LSTM with a graph encoder to obtain the representation of each\ngraph node. A novel neural decoder is presented to leverage the information of\nsuch entity graphs. Automatic and human evaluations show the effectiveness of\nour technique.", "published": "2021-09-13 15:12:34", "link": "http://arxiv.org/abs/2109.06046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Packed Levitated Marker for Entity and Relation Extraction", "abstract": "Recent entity and relation extraction works focus on investigating how to\nobtain a better span representation from the pre-trained encoder. However, a\nmajor limitation of existing works is that they ignore the interrelation\nbetween spans (pairs). In this work, we propose a novel span representation\napproach, named Packed Levitated Markers (PL-Marker), to consider the\ninterrelation between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a neighborhood-oriented packing\nstrategy, which considers the neighbor spans integrally to better model the\nentity boundary information. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects to model the interrelation between the\nsame-subject span pairs. The experimental results show that, with the enhanced\nmarker feature, our model advances baselines on six NER benchmarks, and obtains\na 4.1%-4.3% strict relation F1 improvement with higher speed over previous\nstate-of-the-art models on ACE04 and ACE05.", "published": "2021-09-13 15:38:13", "link": "http://arxiv.org/abs/2109.06067v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Language Models for Creoles", "abstract": "Creole languages such as Nigerian Pidgin English and Haitian Creole are\nunder-resourced and largely ignored in the NLP literature. Creoles typically\nresult from the fusion of a foreign language with multiple local languages, and\nwhat grammatical and lexical features are transferred to the creole is a\ncomplex process. While creoles are generally stable, the prominence of some\nfeatures may be much stronger with certain demographics or in some linguistic\nsituations. This paper makes several contributions: We collect existing corpora\nand release models for Haitian Creole, Nigerian Pidgin English, and Singaporean\nColloquial English. We evaluate these models on intrinsic and extrinsic tasks.\nMotivated by the above literature, we compare standard language models with\ndistributionally robust ones and find that, somewhat surprisingly, the standard\nlanguage models are superior to the distributionally robust ones. We\ninvestigate whether this is an effect of over-parameterization or relative\ndistributional stability, and find that the difference persists in the absence\nof over-parameterization, and that drift is limited, confirming the relative\nstability of creole languages.", "published": "2021-09-13 15:51:15", "link": "http://arxiv.org/abs/2109.06074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xGQA: Cross-Lingual Visual Question Answering", "abstract": "Recent advances in multimodal vision and language modeling have predominantly\nfocused on the English language, mostly due to the lack of multilingual\nmultimodal datasets to steer modeling efforts. In this work, we address this\ngap and provide xGQA, a new multilingual evaluation benchmark for the visual\nquestion answering task. We extend the established English GQA dataset to 7\ntypologically diverse languages, enabling us to detect and explore crucial\nchallenges in cross-lingual visual question answering. We further propose new\nadapter-based approaches to adapt multimodal transformer-based models to become\nmultilingual, and -- vice versa -- multilingual models to become multimodal.\nOur proposed methods outperform current state-of-the-art multilingual\nmultimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the\naccuracy remains low across the board; a performance drop of around 38 accuracy\npoints in target languages showcases the difficulty of zero-shot cross-lingual\ntransfer for this task. Our results suggest that simple cross-lingual transfer\nof multimodal models yields latent multilingual multimodal misalignment,\ncalling for more sophisticated methods for vision and multilingual language\nmodeling.", "published": "2021-09-13 15:58:21", "link": "http://arxiv.org/abs/2109.06082v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SituatedQA: Incorporating Extra-Linguistic Contexts into QA", "abstract": "Answers to the same question may change depending on the extra-linguistic\ncontexts (when and where the question was asked). To study this challenge, we\nintroduce SituatedQA, an open-retrieval QA dataset where systems must produce\nthe correct answer to a question given the temporal or geographical context. To\nconstruct SituatedQA, we first identify such questions in existing QA datasets.\nWe find that a significant proportion of information seeking questions have\ncontext-dependent answers (e.g., roughly 16.5% of NQ-Open). For such\ncontext-dependent questions, we then crowdsource alternative contexts and their\ncorresponding answers. Our study shows that existing models struggle with\nproducing answers that are frequently updated or from uncommon locations. We\nfurther quantify how existing models, which are trained on data collected in\nthe past, fail to generalize to answering questions asked in the present, even\nwhen provided with an updated evidence corpus (a roughly 15 point drop in\naccuracy). Our analysis suggests that open-retrieval QA benchmarks should\nincorporate extra-linguistic context to stay relevant globally and in the\nfuture. Our data, code, and datasheet are available at\nhttps://situatedqa.github.io/ .", "published": "2021-09-13 17:53:21", "link": "http://arxiv.org/abs/2109.06157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPARQLing Database Queries from Intermediate Question Decompositions", "abstract": "To translate natural language questions into executable database queries,\nmost approaches rely on a fully annotated training set. Annotating a large\ndataset with queries is difficult as it requires query-language expertise. We\nreduce this burden using grounded in databases intermediate question\nrepresentations. These representations are simpler to collect and were\noriginally crowdsourced within the Break dataset (Wolfson et al., 2020). Our\npipeline consists of two parts: a neural semantic parser that converts natural\nlanguage questions into the intermediate representations and a non-trainable\ntranspiler to the SPARQL query language (a standard language for accessing\nknowledge graphs and semantic web). We chose SPARQL because its queries are\nstructurally closer to our intermediate representations (compared to SQL). We\nobserve that the execution accuracy of queries constructed by our model on the\nchallenging Spider dataset is comparable with the state-of-the-art text-to-SQL\nmethods trained with annotated SQL queries. Our code and data are publicly\navailable (see https://github.com/yandex-research/sparqling-queries).", "published": "2021-09-13 17:57:12", "link": "http://arxiv.org/abs/2109.06162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Multiway Multilingual NMT in the Turkic Languages", "abstract": "Despite the increasing number of large and comprehensive machine translation\n(MT) systems, evaluation of these methods in various languages has been\nrestrained by the lack of high-quality parallel corpora as well as engagement\nwith the people that speak these languages. In this study, we present an\nevaluation of state-of-the-art approaches to training and evaluating MT systems\nin 22 languages from the Turkic language family, most of which being extremely\nunder-explored. First, we adopt the TIL Corpus with a few key improvements to\nthe training and the evaluation sets. Then, we train 26 bilingual baselines as\nwell as a multi-way neural MT (MNMT) model using the corpus and perform an\nextensive analysis using automatic metrics as well as human evaluations. We\nfind that the MNMT model outperforms almost all bilingual baselines in the\nout-of-domain test sets and finetuning the model on a downstream task of a\nsingle pair also results in a huge performance boost in both low- and\nhigh-resource scenarios. Our attentive analysis of evaluation criteria for MT\nmodels in Turkic languages also points to the necessity for further research in\nthis direction. We release the corpus splits, test sets as well as models to\nthe public.", "published": "2021-09-13 19:01:07", "link": "http://arxiv.org/abs/2109.06262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-OCR Document Correction with large Ensembles of Character\n  Sequence-to-Sequence Models", "abstract": "In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.", "published": "2021-09-13 19:05:02", "link": "http://arxiv.org/abs/2109.06264v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STraTA: Self-Training with Task Augmentation for Better Few-shot\n  Learning", "abstract": "Despite their recent successes in tackling many NLP tasks, large-scale\npre-trained language models do not perform as well in few-shot settings where\nonly a handful of training examples are available. To address this shortcoming,\nwe propose STraTA, which stands for Self-Training with Task Augmentation, an\napproach that builds on two key ideas for effective leverage of unlabeled data.\nFirst, STraTA uses task augmentation, a novel technique that synthesizes a\nlarge amount of data for auxiliary-task fine-tuning from target-task unlabeled\ntexts. Second, STraTA performs self-training by further fine-tuning the strong\nbase model created by task augmentation on a broad distribution of\npseudo-labeled data. Our experiments demonstrate that STraTA can substantially\nimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on the\nSST-2 sentiment dataset, STraTA, with only 8 training examples per class,\nachieves comparable results to standard fine-tuning with 67K training examples.\nOur analyses reveal that task augmentation and self-training are both\ncomplementary and independently effective.", "published": "2021-09-13 19:14:01", "link": "http://arxiv.org/abs/2109.06270v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Algorithms for Multiparallel Word Alignment", "abstract": "With the advent of end-to-end deep learning approaches in machine\ntranslation, interest in word alignments initially decreased; however, they\nhave again become a focus of research more recently. Alignments are useful for\ntypological research, transferring formatting like markup to translated texts,\nand can be used in the decoding of machine translation systems. At the same\ntime, massively multilingual processing is becoming an important NLP scenario,\nand pretrained language and machine translation models that are truly\nmultilingual are proposed. However, most alignment algorithms rely on bitexts\nonly and do not leverage the fact that many parallel corpora are multiparallel.\nIn this work, we exploit the multiparallelity of corpora by representing an\ninitial set of bilingual alignments as a graph and then predicting additional\nedges in the graph. We present two graph algorithms for edge prediction: one\ninspired by recommender systems and one based on network link prediction. Our\nexperimental results show absolute improvements in $F_1$ of up to 28% over the\nbaseline bilingual word aligner in different datasets.", "published": "2021-09-13 19:40:29", "link": "http://arxiv.org/abs/2109.06283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to\n  Corpus Exploration", "abstract": "Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.", "published": "2021-09-13 20:31:57", "link": "http://arxiv.org/abs/2109.06304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Constraints and Descriptive Segmentation for Subevent Detection", "abstract": "Event mentions in text correspond to real-world events of varying degrees of\ngranularity. The task of subevent detection aims to resolve this granularity\nissue, recognizing the membership of multi-granular events in event complexes.\nSince knowing the span of descriptive contexts of event complexes helps infer\nthe membership of events, we propose the task of event-based text segmentation\n(EventSeg) as an auxiliary task to improve the learning for subevent detection.\nTo bridge the two tasks together, we propose an approach to learning and\nenforcing constraints that capture dependencies between subevent detection and\nEventSeg prediction, as well as guiding the model to make globally consistent\ninference. Specifically, we adopt Rectifier Networks for constraint learning\nand then convert the learned constraints to a regularization term in the loss\nfunction of the neural model. Experimental results show that the proposed\nmethod outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for\nsubevent detection, HiEve and IC, respectively, while achieving a decent\nperformance on EventSeg prediction.", "published": "2021-09-13 20:50:37", "link": "http://arxiv.org/abs/2109.06316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Transferability of BERT Models on Uralic Languages", "abstract": "Transformer-based language models such as BERT have outperformed previous\nmodels on a large number of English benchmarks, but their evaluation is often\nlimited to English or a small number of well-resourced languages. In this work,\nwe evaluate monolingual, multilingual, and randomly initialized language models\nfrom the BERT family on a variety of Uralic languages including Estonian,\nFinnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,\nNorthern S\\'ami, and Skolt S\\'ami. When monolingual models are available\n(currently only et, fi, hu), these perform better on their native language, but\nin general they transfer worse than multilingual models or models of\ngenetically unrelated languages that share the same character set. Remarkably,\nstraightforward transfer of high-resource models, even without special efforts\ntoward hyperparameter optimization, yields what appear to be state of the art\nPOS and NER tools for the minority Uralic languages where there is sufficient\ndata for finetuning.", "published": "2021-09-13 21:10:29", "link": "http://arxiv.org/abs/2109.06327v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting degree and polarity: An artificial language learning study", "abstract": "We investigate a new linguistic generalization in pre-trained language models\n(taking BERT (Devlin et al., 2019) as a case study). We focus on degree\nmodifiers (expressions like slightly, very, rather, extremely) and test the\nhypothesis that the degree expressed by a modifier (low, medium or high degree)\nis related to the modifier's sensitivity to sentence polarity (whether it shows\npreference for affirmative or negative sentences or neither). To probe this\nconnection, we apply the Artificial Language Learning experimental paradigm\nfrom psycholinguistics to a neural language model. Our experimental results\nsuggest that BERT generalizes in line with existing linguistic observations\nthat relate degree semantics to polarity sensitivity, including the main one:\nlow degree semantics is associated with preference towards positive polarity.", "published": "2021-09-13 21:36:01", "link": "http://arxiv.org/abs/2109.06333v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine Grained Human Evaluation for English-to-Chinese Machine\n  Translation: A Case Study on Scientific Text", "abstract": "Recent research suggests that neural machine translation (MT) in the news\ndomain has reached human-level performance, but for other professional domains,\nit is far below the level. In this paper, we conduct a fine-grained systematic\nhuman evaluation for four widely used Chinese-English NMT systems on scientific\nabstracts which are collected from published journals and books. Our human\nevaluation results show that all the systems return with more than 10\\% error\nrates on average, which requires much post editing effort for real academic\nuse. Furthermore, we categorize six main error types and and provide some real\nexamples. Our findings emphasise the needs that research attention in the MT\ncommunity should be shifted from short text generic translation to professional\nmachine translation and build large scale bilingual corpus for these specific\ndomains.", "published": "2021-09-13 23:51:25", "link": "http://arxiv.org/abs/2110.14766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEXTOIR: An Integrated and Visualized Platform for Text Open Intent\n  Recognition", "abstract": "TEXTOIR is the first integrated and visualized platform for text open intent\nrecognition. It is composed of two main modules: open intent detection and open\nintent discovery. Each module integrates most of the state-of-the-art\nalgorithms and benchmark intent datasets. It also contains an overall framework\nconnecting the two modules in a pipeline scheme. In addition, this platform has\nvisualized tools for data and model management, training, evaluation and\nanalysis of the performance from different aspects. TEXTOIR provides useful\ntoolkits and convenient visualized interfaces for each sub-module (Toolkit\ncode: https://github.com/thuiar/TEXTOIR), and designs a framework to implement\na complete process to both identify known intents and discover open intents\n(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).", "published": "2021-09-13 02:08:18", "link": "http://arxiv.org/abs/2110.15063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstract, Rationale, Stance: A Joint Model for Scientific Claim\n  Verification", "abstract": "Scientific claim verification can help the researchers to easily find the\ntarget scientific papers with the sentence evidence from a large corpus for the\ngiven claim. Some existing works propose pipeline models on the three tasks of\nabstract retrieval, rationale selection and stance prediction. Such works have\nthe problems of error propagation among the modules in the pipeline and lack of\nsharing valuable information among modules. We thus propose an approach, named\nas ARSJoint, that jointly learns the modules for the three tasks with a machine\nreading comprehension framework by including claim information. In addition, we\nenhance the information exchanges and constraints among tasks by proposing a\nregularization term between the sentence attention scores of abstract retrieval\nand the estimated outputs of rational selection. The experimental results on\nthe benchmark dataset SciFact show that our approach outperforms the existing\nworks.", "published": "2021-09-13 10:07:26", "link": "http://arxiv.org/abs/2110.15116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Raise a Child in Large Language Model: Towards Effective and\n  Generalizable Fine-tuning", "abstract": "Recent pretrained language models extend from millions to billions of\nparameters. Thus the need to fine-tune an extremely large pretrained model with\na limited training corpus arises in various downstream tasks. In this paper, we\npropose a straightforward yet effective fine-tuning technique, Child-Tuning,\nwhich updates a subset of parameters (called child network) of large pretrained\nmodels via strategically masking out the gradients of the non-child network\nduring the backward process. Experiments on various downstream tasks in GLUE\nbenchmark show that Child-Tuning consistently outperforms the vanilla\nfine-tuning by 1.5~8.6 average score among four different pretrained models,\nand surpasses the prior fine-tuning techniques by 0.6~1.3 points. Furthermore,\nempirical results on domain transfer and task transfer show that Child-Tuning\ncan obtain better generalization performance by large margins.", "published": "2021-09-13 03:39:52", "link": "http://arxiv.org/abs/2109.05687v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCORE-IT: A Machine Learning-based Tool for Automatic Standardization of\n  EEG Reports", "abstract": "Machine learning (ML)-based analysis of electroencephalograms (EEGs) is\nplaying an important role in advancing neurological care. However, the\ndifficulties in automatically extracting useful metadata from clinical records\nhinder the development of large-scale EEG-based ML models. EEG reports, which\nare the primary sources of metadata for EEG studies, suffer from lack of\nstandardization. Here we propose a machine learning-based system that\nautomatically extracts components from the SCORE specification from\nunstructured, natural-language EEG reports. Specifically, our system identifies\n(1) the type of seizure that was observed in the recording, per physician\nimpression; (2) whether the session recording was normal or abnormal according\nto physician impression; (3) whether the patient was diagnosed with epilepsy or\nnot. We performed an evaluation of our system using the publicly available TUH\nEEG corpus and report F1 scores of 0.92, 0.82, and 0.97 for the respective\ntasks.", "published": "2021-09-13 04:05:37", "link": "http://arxiv.org/abs/2109.05694v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Textual Adversarial Examples through Randomized Substitution\n  and Vote", "abstract": "A line of work has shown that natural text processing models are vulnerable\nto adversarial examples. Correspondingly, various defense methods are proposed\nto mitigate the threat of textual adversarial examples, eg, adversarial\ntraining, input transformations, detection, etc. In this work, we treat the\noptimization process for synonym substitution based textual adversarial attacks\nas a specific sequence of word replacement, in which each word mutually\ninfluences other words. We identify that we could destroy such mutual\ninteraction and eliminate the adversarial perturbation by randomly substituting\na word with its synonyms. Based on this observation, we propose a novel textual\nadversarial example detection method, termed Randomized Substitution and Vote\n(RS&V), which votes the prediction label by accumulating the logits of k\nsamples generated by randomly substituting the words in the input text with\nsynonyms. The proposed RS&V is generally applicable to any existing neural\nnetworks without modification on the architecture or extra training, and it is\northogonal to prior work on making the classification network itself more\nrobust. Empirical evaluations on three benchmark datasets demonstrate that our\nRS&V could detect the textual adversarial examples more successfully than the\nexisting detection methods while maintaining the high classification accuracy\non benign samples.", "published": "2021-09-13 04:17:58", "link": "http://arxiv.org/abs/2109.05698v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Language-Dependent Ethnic Bias in BERT", "abstract": "BERT and other large-scale language models (LMs) contain gender and racial\nbias. They also exhibit other dimensions of social bias, most of which have not\nbeen studied in depth, and some of which vary depending on the language. In\nthis paper, we study ethnic bias and how it varies across languages by\nanalyzing and mitigating ethnic bias in monolingual BERT for English, German,\nSpanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we\ndevelop a novel metric called Categorical Bias score. Then we propose two\nmethods for mitigation; first using a multilingual model, and second using\ncontextual word alignment of two monolingual models. We compare our proposed\nmethods with monolingual BERT and show that these methods effectively alleviate\nthe ethnic bias. Which of the two methods works better depends on the amount of\nNLP resources available for that language. We additionally experiment with\nArabic and Greek to verify that our proposed methods work for a wider variety\nof languages.", "published": "2021-09-13 04:52:41", "link": "http://arxiv.org/abs/2109.05704v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CEM: Commonsense-aware Empathetic Response Generation", "abstract": "A key trait of daily conversations between individuals is the ability to\nexpress empathy towards others, and exploring ways to implement empathy is a\ncrucial step towards human-like dialogue systems. Previous approaches on this\ntopic mainly focus on detecting and utilizing the user's emotion for generating\nempathetic responses. However, since empathy includes both aspects of affection\nand cognition, we argue that in addition to identifying the user's emotion,\ncognitive understanding of the user's situation should also be considered. To\nthis end, we propose a novel approach for empathetic response generation, which\nleverages commonsense to draw more information about the user's situation and\nuses this additional information to further enhance the empathy expression in\ngenerated responses. We evaluate our approach on EmpatheticDialogues, which is\na widely-used benchmark dataset for empathetic response generation. Empirical\nresults demonstrate that our approach outperforms the baseline models in both\nautomatic and human evaluations and can generate more informative and\nempathetic responses.", "published": "2021-09-13 06:55:14", "link": "http://arxiv.org/abs/2109.05739v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based\n  on Transformer Networks", "abstract": "A key problem in multi-task learning (MTL) research is how to select\nhigh-quality auxiliary tasks automatically. This paper presents GradTS, an\nautomatic auxiliary task selection method based on gradient calculation in\nTransformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS\nimproves the performance of MT-DNN with a bert-base-cased backend model, from\n0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE\nbenchmarks. GradTS is also time-saving since (1) its gradient calculations are\nbased on single-task experiments and (2) the gradients are re-used without\nadditional experiments when the candidate task set changes. On the 8 GLUE\nclassification tasks, for example, GradTS costs on average 21.32% less time\nthan AUTOSEM with comparable GPU consumption. Further, we show the robustness\nof GradTS across various task settings and model selections, e.g. mixed\nobjectives among candidate tasks. The efficiency and efficacy of GradTS in\nthese case studies illustrate its general applicability in MTL research without\nrequiring manual task filtering or costly parameter tuning.", "published": "2021-09-13 07:15:38", "link": "http://arxiv.org/abs/2109.05748v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Categorical Semantics of Reversible Pattern-Matching", "abstract": "This paper is concerned with categorical structures for reversible\ncomputation. In particular, we focus on a typed, functional reversible language\nbased on Theseus. We discuss how join inverse rig categories do not in general\ncapture pattern-matching, the core construct Theseus uses to enforce\nreversibility. We then derive a categorical structure to add to join inverse\nrig categories in order to capture pattern-matching. We show how such a\nstructure makes an adequate model for reversible pattern-matching.", "published": "2021-09-13 10:05:11", "link": "http://arxiv.org/abs/2109.05837v3", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Attention Weights in Transformer NMT Fail Aligning Words Between\n  Sequences but Largely Explain Model Predictions", "abstract": "This work proposes an extensive analysis of the Transformer architecture in\nthe Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder\nattention mechanism, we prove that attention weights systematically make\nalignment errors by relying mainly on uninformative tokens from the source\nsequence. However, we observe that NMT models assign attention to these tokens\nto regulate the contribution in the prediction of the two contexts, the source\nand the prefix of the target sequence. We provide evidence about the influence\nof wrong alignments on the model behavior, demonstrating that the\nencoder-decoder attention mechanism is well suited as an interpretability\nmethod for NMT. Finally, based on our analysis, we propose methods that largely\nreduce the word alignment error rate compared to standard induced alignments\nfrom attention weights.", "published": "2021-09-13 10:44:02", "link": "http://arxiv.org/abs/2109.05853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not All Models Localize Linguistic Knowledge in the Same Place: A\n  Layer-wise Probing on BERToids' Representations", "abstract": "Most of the recent works on probing representations have focused on BERT,\nwith the presumption that the findings might be similar to the other models. In\nthis work, we extend the probing studies to two other models in the family,\nnamely ELECTRA and XLNet, showing that variations in the pre-training\nobjectives or architectural choices can result in different behaviors in\nencoding linguistic information in the representations. Most notably, we\nobserve that ELECTRA tends to encode linguistic knowledge in the deeper layers,\nwhereas XLNet instead concentrates that in the earlier layers. Also, the former\nmodel undergoes a slight change during fine-tuning, whereas the latter\nexperiences significant adjustments. Moreover, we show that drawing conclusions\nbased on the weight mixing evaluation strategy -- which is widely used in the\ncontext of layer-wise probing -- can be misleading given the norm disparity of\nthe representations across different layers. Instead, we adopt an alternative\ninformation-theoretic probing with minimum description length, which has\nrecently been proven to provide more reliable and informative results.", "published": "2021-09-13 13:37:21", "link": "http://arxiv.org/abs/2109.05958v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keyword Extraction for Improved Document Retrieval in Conversational\n  Search", "abstract": "Recent research has shown that mixed-initiative conversational search, based\non the interaction between users and computers to clarify and improve a query,\nprovides enormous advantages. Nonetheless, incorporating additional information\nprovided by the user from the conversation poses some challenges. In fact,\nfurther interactions could confuse the system as a user might use words\nirrelevant to the information need but crucial for correct sentence\nconstruction in the context of multi-turn conversations. To this aim, in this\npaper, we have collected two conversational keyword extraction datasets and\npropose an end-to-end document retrieval pipeline incorporating them.\nFurthermore, we study the performance of two neural keyword extraction models,\nnamely, BERT and sequence to sequence, in terms of extraction accuracy and\nhuman annotation. Finally, we study the effect of keyword extraction on the\nend-to-end neural IR performance and show that our approach beats\nstate-of-the-art IR models. We make the two datasets publicly available to\nfoster research in this area.", "published": "2021-09-13 13:55:37", "link": "http://arxiv.org/abs/2109.05979v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning to Ground Visual Objects for Visual Dialog", "abstract": "Visual dialog is challenging since it needs to answer a series of coherent\nquestions based on understanding the visual environment. How to ground related\nvisual objects is one of the key problems. Previous studies utilize the\nquestion and history to attend to the image and achieve satisfactory\nperformance, however these methods are not sufficient to locate related visual\nobjects without any guidance. The inappropriate grounding of visual objects\nprohibits the performance of visual dialog models. In this paper, we propose a\nnovel approach to Learn to Ground visual objects for visual dialog, which\nemploys a novel visual objects grounding mechanism where both prior and\nposterior distributions over visual objects are used to facilitate visual\nobjects grounding. Specifically, a posterior distribution over visual objects\nis inferred from both context (history and questions) and answers, and it\nensures the appropriate grounding of visual objects during the training\nprocess. Meanwhile, a prior distribution, which is inferred from context only,\nis used to approximate the posterior distribution so that appropriate visual\nobjects can be grounded even without answers during the inference process.\nExperimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our\napproach improves the previous strong models in both generative and\ndiscriminative settings by a significant margin.", "published": "2021-09-13 14:48:44", "link": "http://arxiv.org/abs/2109.06013v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Few-Shot Cross-Lingual Stance Detection with Sentiment-Based\n  Pre-Training", "abstract": "The goal of stance detection is to determine the viewpoint expressed in a\npiece of text towards a target. These viewpoints or contexts are often\nexpressed in many different languages depending on the user and the platform,\nwhich can be a local news outlet, a social media platform, a news forum, etc.\nMost research in stance detection, however, has been limited to working with a\nsingle language and on a few limited targets, with little work on cross-lingual\nstance detection. Moreover, non-English sources of labelled data are often\nscarce and present additional challenges. Recently, large multilingual language\nmodels have substantially improved the performance on many non-English tasks,\nespecially such with limited numbers of examples. This highlights the\nimportance of model pre-training and its ability to learn from few examples. In\nthis paper, we present the most comprehensive study of cross-lingual stance\ndetection to date: we experiment with 15 diverse datasets in 12 languages from\n6 language families, and with 6 low-resource evaluation settings each. For our\nexperiments, we build on pattern-exploiting training, proposing the addition of\na novel label encoder to simplify the verbalisation procedure. We further\npropose sentiment-based generation of stance data for pre-training, which shows\nsizeable improvement of more than 6% F1 absolute in low-shot settings compared\nto several strong baselines.", "published": "2021-09-13 15:20:06", "link": "http://arxiv.org/abs/2109.06050v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Pursuit of Designing Multi-modal Transformer for Video Grounding", "abstract": "Video grounding aims to localize the temporal segment corresponding to a\nsentence query from an untrimmed video. Almost all existing video grounding\nmethods fall into two frameworks: 1) Top-down model: It predefines a set of\nsegment candidates and then conducts segment classification and regression. 2)\nBottom-up model: It directly predicts frame-wise probabilities of the\nreferential segment boundaries. However, all these methods are not end-to-end,\ni.e., they always rely on some time-consuming post-processing steps to refine\npredictions. To this end, we reformulate video grounding as a set prediction\ntask and propose a novel end-to-end multi-modal Transformer model, dubbed as\nGTR. Specifically, GTR has two encoders for video and language encoding, and a\ncross-modal decoder for grounding prediction. To facilitate the end-to-end\ntraining, we use a Cubic Embedding layer to transform the raw videos into a set\nof visual tokens. To better fuse these two modalities in the decoder, we design\na new Multi-head Cross-Modal Attention. The whole GTR is optimized via a\nMany-to-One matching loss. Furthermore, we conduct comprehensive studies to\ninvestigate different model design choices. Extensive results on three\nbenchmarks have validated the superiority of GTR. All three typical GTR\nvariants achieve record-breaking performance on all datasets and metrics, with\nseveral times faster inference speed.", "published": "2021-09-13 16:01:19", "link": "http://arxiv.org/abs/2109.06085v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Joint prediction of truecasing and punctuation for conversational speech\n  in low-resource scenarios", "abstract": "Capitalization and punctuation are important cues for comprehending written\ntexts and conversational transcripts. Yet, many ASR systems do not produce\npunctuated and case-formatted speech transcripts. We propose to use a\nmulti-task system that can exploit the relations between casing and punctuation\nto improve their prediction performance. Whereas text data for predicting\npunctuation and truecasing is seemingly abundant, we argue that written text\nresources are inadequate as training data for conversational models. We\nquantify the mismatch between written and conversational text domains by\ncomparing the joint distributions of punctuation and word cases, and by testing\nour model cross-domain. Further, we show that by training the model in the\nwritten text domain and then transfer learning to conversations, we can achieve\nreasonable performance with less data.", "published": "2021-09-13 16:25:37", "link": "http://arxiv.org/abs/2109.06103v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic\n  Rewriting into Gender-Neutral Alternatives", "abstract": "Recent years have seen an increasing need for gender-neutral and inclusive\nlanguage. Within the field of NLP, there are various mono- and bilingual use\ncases where gender inclusive language is appropriate, if not preferred due to\nambiguity or uncertainty in terms of the gender of referents. In this work, we\npresent a rule-based and a neural approach to gender-neutral rewriting for\nEnglish along with manually curated synthetic data (WinoBias+) and natural data\n(OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic\nevaluation highlights how our NeuTral Rewriter, trained on data generated by\nthe rule-based approach, obtains word error rates (WER) below 0.18% on\nsynthetic, in-domain and out-domain test sets.", "published": "2021-09-13 16:26:12", "link": "http://arxiv.org/abs/2109.06105v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering the Unknown Knowns: Turning Implicit Knowledge in the\n  Dataset into Explicit Training Examples for Visual Question Answering", "abstract": "Visual question answering (VQA) is challenging not only because the model has\nto handle multi-modal information, but also because it is just so hard to\ncollect sufficient training examples -- there are too many questions one can\nask about an image. As a result, a VQA model trained solely on human-annotated\nexamples could easily over-fit specific question styles or image contents that\nare being asked, leaving the model largely ignorant about the sheer diversity\nof questions. Existing methods address this issue primarily by introducing an\nauxiliary task such as visual grounding, cycle consistency, or debiasing. In\nthis paper, we take a drastically different approach. We found that many of the\n\"unknowns\" to the learned VQA model are indeed \"known\" in the dataset\nimplicitly. For instance, questions asking about the same object in different\nimages are likely paraphrases; the number of detected or annotated objects in\nan image already provides the answer to the \"how many\" question, even if the\nquestion has not been annotated for that image. Building upon these insights,\nwe present a simple data augmentation pipeline SimpleAug to turn this \"known\"\nknowledge into training examples for VQA. We show that these augmented examples\ncan notably improve the learned VQA models' performance, not only on the VQA-CP\ndataset with language prior shifts but also on the VQA v2 dataset without such\nshifts. Our method further opens up the door to leverage weakly-labeled or\nunlabeled images in a principled way to enhance VQA models. Our code and data\nare publicly available at https://github.com/heendung/simpleAUG.", "published": "2021-09-13 16:56:43", "link": "http://arxiv.org/abs/2109.06122v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can Language Models Encode Perceptual Structure Without Grounding? A\n  Case Study in Color", "abstract": "Pretrained language models have been shown to encode relational information,\nsuch as the relations between entities or concepts in knowledge-bases --\n(Paris, Capital, France). However, simple relations of this type can often be\nrecovered heuristically and the extent to which models implicitly reflect\ntopological structure that is grounded in world, such as perceptual structure,\nis unknown. To explore this question, we conduct a thorough case study on\ncolor. Namely, we employ a dataset of monolexemic color terms and color chips\nrepresented in CIELAB, a color space with a perceptually meaningful distance\nmetric.\n  Using two methods of evaluating the structural alignment of colors in this\nspace with text-derived color term representations, we find significant\ncorrespondence. Analyzing the differences in alignment across the color\nspectrum, we find that warmer colors are, on average, better aligned to the\nperceptual color space than cooler ones, suggesting an intriguing connection to\nfindings from recent work on efficient communication in color naming. Further\nanalysis suggests that differences in alignment are, in part, mediated by\ncollocationality and differences in syntactic usage, posing questions as to the\nrelationship between color perception and usage and context.", "published": "2021-09-13 17:09:40", "link": "http://arxiv.org/abs/2109.06129v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language\n  Models via Knowledge Distillation", "abstract": "The development of over-parameterized pre-trained language models has made a\nsignificant contribution toward the success of natural language processing.\nWhile over-parameterization of these models is the key to their generalization\npower, it makes them unsuitable for deployment on low-capacity devices. We push\nthe limits of state-of-the-art Transformer-based pre-trained language model\ncompression using Kronecker decomposition. We use this decomposition for\ncompression of the embedding layer, all linear mappings in the multi-head\nattention, and the feed-forward network modules in the Transformer layer. We\nperform intermediate-layer knowledge distillation using the uncompressed model\nas the teacher to improve the performance of the compressed model. We present\nour KroneckerBERT, a compressed version of the BERT_BASE model obtained using\nthis framework. We evaluate the performance of KroneckerBERT on well-known NLP\nbenchmarks and show that for a high compression factor of 19 (5% of the size of\nthe BERT_BASE model), our KroneckerBERT outperforms state-of-the-art\ncompression methods on the GLUE. Our experiments indicate that the proposed\nmodel has promising out-of-distribution robustness and is superior to the\nstate-of-the-art compression methods on SQuAD.", "published": "2021-09-13 18:19:30", "link": "http://arxiv.org/abs/2109.06243v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Scheduled Sampling with Elastic Weight Consolidation for\n  Neural Machine Translation", "abstract": "Despite strong performance in many sequence-to-sequence tasks, autoregressive\nmodels trained with maximum likelihood estimation suffer from exposure bias,\ni.e. the discrepancy between the ground-truth prefixes used during training and\nthe model-generated prefixes used at inference time. Scheduled sampling is a\nsimple and empirically successful approach which addresses this issue by\nincorporating model-generated prefixes into training. However, it has been\nargued that it is an inconsistent training objective leading to models ignoring\nthe prefixes altogether. In this paper, we conduct systematic experiments and\nfind that scheduled sampling, while it ameliorates exposure bias by increasing\nmodel reliance on the input sequence, worsens performance when the prefix at\ninference time is correct, a form of catastrophic forgetting. We propose to use\nElastic Weight Consolidation to better balance mitigating exposure bias with\nretaining performance. Experiments on four IWSLT'14 and WMT'14 translation\ndatasets demonstrate that our approach alleviates catastrophic forgetting and\nsignificantly outperforms maximum likelihood estimation and scheduled sampling\nbaselines.", "published": "2021-09-13 20:37:58", "link": "http://arxiv.org/abs/2109.06308v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning", "abstract": "In this work, we focus on a more challenging few-shot intent detection\nscenario where many intents are fine-grained and semantically similar. We\npresent a simple yet effective few-shot intent detection schema via contrastive\npre-training and fine-tuning. Specifically, we first conduct self-supervised\ncontrastive pre-training on collected intent datasets, which implicitly learns\nto discriminate semantically similar utterances without using any labels. We\nthen perform few-shot intent detection together with supervised contrastive\nlearning, which explicitly pulls utterances from the same intent closer and\npushes utterances across different intents farther. Experimental results show\nthat our proposed method achieves state-of-the-art performance on three\nchallenging intent detection datasets under 5-shot and 10-shot settings.", "published": "2021-09-13 22:28:58", "link": "http://arxiv.org/abs/2109.06349v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Machine Translation Evaluation", "abstract": "Several neural-based metrics have been recently proposed to evaluate machine\ntranslation quality. However, all of them resort to point estimates, which\nprovide limited information at segment level. This is made worse as they are\ntrained on noisy, biased and scarce human judgements, often resulting in\nunreliable quality predictions. In this paper, we introduce uncertainty-aware\nMT evaluation and analyze the trustworthiness of the predicted quality. We\ncombine the COMET framework with two uncertainty estimation methods, Monte\nCarlo dropout and deep ensembles, to obtain quality scores along with\nconfidence intervals. We compare the performance of our uncertainty-aware MT\nevaluation methods across multiple language pairs from the QT21 dataset and the\nWMT20 metrics task, augmented with MQM annotations. We experiment with varying\nnumbers of references and further discuss the usefulness of uncertainty-aware\nquality estimation (without references) to flag possibly critical translation\nmistakes.", "published": "2021-09-13 22:46:03", "link": "http://arxiv.org/abs/2109.06352v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Concept of Semantic Value in Social Network Analysis: an Application\n  to Comparative Mythology", "abstract": "Human sciences have traditionally relied on human reasoning and intelligence\nto infer knowledge from a wide range of sources, such as oral and written\nnarrations, reports, and traditions. Here we develop an extension of classical\nsocial network analysis approaches to incorporate the concept of meaning in\neach actor, as a mean to quantify and infer further knowledge from the original\nsource of the network. This extension is based on a new affinity function, the\nsemantic affinity, that establishes fuzzy-like relationships between the\ndifferent actors in the network, using combinations of affinity functions. We\nalso propose a new heuristic algorithm based on the shortest capacity problem\nto compute this affinity function. We use these concept of meaning and semantic\naffinity to analyze and compare the gods and heroes from three different\nclassical mythologies: Greek, Celtic and Nordic. We study the relationships of\neach individual mythology and those of common structure that is formed when we\nfuse the three of them. We show a strong connection between the Celtic and\nNordic gods and that Greeks put more emphasis on heroic characters rather than\ndeities. Our approach provides a technique to highlight and quantify important\nrelationships in the original domain of the network not deducible from its\nstructural properties.", "published": "2021-09-13 15:50:54", "link": "http://arxiv.org/abs/2109.08023v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Towards Fine-Grained Reasoning for Fake News Detection", "abstract": "The detection of fake news often requires sophisticated reasoning skills,\nsuch as logically combining information by considering word-level subtle clues.\nIn this paper, we move towards fine-grained reasoning for fake news detection\nby better reflecting the logical processes of human thinking and enabling the\nmodeling of subtle clues. In particular, we propose a fine-grained reasoning\nframework by following the human information-processing model, introduce a\nmutual-reinforcement-based method for incorporating human knowledge about which\nevidence is more important, and design a prior-aware bi-channel kernel graph\nnetwork to model subtle differences between pieces of evidence. Extensive\nexperiments show that our model outperforms the state-of-the-art methods and\ndemonstrate the explainability of our approach.", "published": "2021-09-13 15:45:36", "link": "http://arxiv.org/abs/2110.15064v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explain Me the Painting: Multi-Topic Knowledgeable Art Description\n  Generation", "abstract": "Have you ever looked at a painting and wondered what is the story behind it?\nThis work presents a framework to bring art closer to people by generating\ncomprehensive descriptions of fine-art paintings. Generating informative\ndescriptions for artworks, however, is extremely challenging, as it requires to\n1) describe multiple aspects of the image such as its style, content, or\ncomposition, and 2) provide background and contextual knowledge about the\nartist, their influences, or the historical period. To address these\nchallenges, we introduce a multi-topic and knowledgeable art description\nframework, which modules the generated sentences according to three artistic\ntopics and, additionally, enhances each description with external knowledge.\nThe framework is validated through an exhaustive analysis, both quantitative\nand qualitative, as well as a comparative human evaluation, demonstrating\noutstanding results in terms of both topic diversity and information veracity.", "published": "2021-09-13 07:08:46", "link": "http://arxiv.org/abs/2109.05743v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Building and Evaluating Open-Domain Dialogue Corpora with Clarifying\n  Questions", "abstract": "Enabling open-domain dialogue systems to ask clarifying questions when\nappropriate is an important direction for improving the quality of the system\nresponse. Namely, for cases when a user request is not specific enough for a\nconversation system to provide an answer right away, it is desirable to ask a\nclarifying question to increase the chances of retrieving a satisfying answer.\nTo address the problem of 'asking clarifying questions in open-domain\ndialogues': (1) we collect and release a new dataset focused on open-domain\nsingle- and multi-turn conversations, (2) we benchmark several state-of-the-art\nneural baselines, and (3) we propose a pipeline consisting of offline and\nonline steps for evaluating the quality of clarifying questions in various\ndialogues. These contributions are suitable as a foundation for further\nresearch.", "published": "2021-09-13 09:16:14", "link": "http://arxiv.org/abs/2109.05794v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Process Discovery Using Graph Neural Networks", "abstract": "Automatically discovering a process model from an event log is the prime\nproblem in process mining. This task is so far approached as an unsupervised\nlearning problem through graph synthesis algorithms. Algorithmic design\ndecisions and heuristics allow for efficiently finding models in a reduced\nsearch space. However, design decisions and heuristics are derived from\nassumptions about how a given behavioral description - an event log -\ntranslates into a process model and were not learned from actual models which\nintroduce biases in the solutions. In this paper, we explore the problem of\nsupervised learning of a process discovery technique D. We introduce a\ntechnique for training an ML-based model D using graph convolutional neural\nnetworks; D translates a given input event log into a sound Petri net. We show\nthat training D on synthetically generated pairs of input logs and output\nmodels allows D to translate previously unseen synthetic and several real-life\nevent logs into sound, arbitrarily structured models of comparable accuracy and\nsimplicity as existing state of the art techniques for discovering imperative\nprocess models. We analyze the limitations of the proposed technique and\noutline alleys for future work.", "published": "2021-09-13 10:04:34", "link": "http://arxiv.org/abs/2109.05835v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "Question Answering over Electronic Devices: A New Benchmark Dataset and\n  a Multi-Task Learning based QA Framework", "abstract": "Answering questions asked from instructional corpora such as E-manuals,\nrecipe books, etc., has been far less studied than open-domain factoid\ncontext-based question answering. This can be primarily attributed to the\nabsence of standard benchmark datasets. In this paper we meticulously create a\nlarge amount of data connected with E-manuals and develop suitable algorithm to\nexploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals and\npretrain RoBERTa on this large corpus. We create various benchmark QA datasets\nwhich include question answer pairs curated by experts based upon two\nE-manuals, real user questions from Community Question Answering Forum\npertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering\nPipeline) that answers questions pertaining to electronics devices. Built upon\nthe pretrained RoBERTa, it harbors a supervised multi-task learning framework\nwhich efficiently performs the dual tasks of identifying the section in the\nE-manual where the answer can be found and the exact answer span within that\nsection. For E-Manual annotated question-answer pairs, we show an improvement\nof about 40% in ROUGE-L F1 scores over the most competitive baseline. We\nperform a detailed ablation study and establish the versatility of EMQAP across\ndifferent circumstances. The code and datasets are shared at\nhttps://github.com/abhi1nandy2/EMNLP-2021-Findings, and the corresponding\nproject website is https://sites.google.com/view/emanualqa/home.", "published": "2021-09-13 12:11:39", "link": "http://arxiv.org/abs/2109.05897v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "r-GAT: Relational Graph Attention Network for Multi-Relational Graphs", "abstract": "Graph Attention Network (GAT) focuses on modelling simple undirected and\nsingle relational graph data only. This limits its ability to deal with more\ngeneral and complex multi-relational graphs that contain entities with directed\nlinks of different labels (e.g., knowledge graphs). Therefore, directly\napplying GAT on multi-relational graphs leads to sub-optimal solutions. To\ntackle this issue, we propose r-GAT, a relational graph attention network to\nlearn multi-channel entity representations. Specifically, each channel\ncorresponds to a latent semantic aspect of an entity. This enables us to\naggregate neighborhood information for the current aspect using relation\nfeatures. We further propose a query-aware attention mechanism for subsequent\ntasks to select useful aspects. Extensive experiments on link prediction and\nentity classification tasks show that our r-GAT can model multi-relational\ngraphs effectively. Also, we show the interpretability of our approach by case\nstudy.", "published": "2021-09-13 12:43:00", "link": "http://arxiv.org/abs/2109.05922v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Analysing Mixed Initiatives and Search Strategies during Conversational\n  Search", "abstract": "Information seeking conversations between users and Conversational Search\nAgents (CSAs) consist of multiple turns of interaction. While users initiate a\nsearch session, ideally a CSA should sometimes take the lead in the\nconversation by obtaining feedback from the user by offering query suggestions\nor asking for query clarifications i.e. mixed initiative. This creates the\npotential for more engaging conversational searches, but substantially\nincreases the complexity of modelling and evaluating such scenarios due to the\nlarge interaction space coupled with the trade-offs between the costs and\nbenefits of the different interactions. In this paper, we present a model for\nconversational search -- from which we instantiate different observed\nconversational search strategies, where the agent elicits: (i) Feedback-First,\nor (ii) Feedback-After. Using 49 TREC WebTrack Topics, we performed an analysis\ncomparing how well these different strategies combine with different mixed\ninitiative approaches: (i) Query Suggestions vs. (ii) Query Clarifications. Our\nanalysis reveals that there is no superior or dominant combination, instead it\nshows that query clarifications are better when asked first, while query\nsuggestions are better when asked after presenting results. We also show that\nthe best strategy and approach depends on the trade-offs between the relative\ncosts between querying and giving feedback, the performance of the initial\nquery, the number of assessments per query, and the total amount of gain\nrequired. While this work highlights the complexities and challenges involved\nin analyzing CSAs, it provides the foundations for evaluating conversational\nstrategies and conversational search agents in batch/offline settings.", "published": "2021-09-13 13:30:10", "link": "http://arxiv.org/abs/2109.05955v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Graph-based Retrieval for Claim Verification over Cross-Document\n  Evidence", "abstract": "Verifying the veracity of claims requires reasoning over a large knowledge\nbase, often in the form of corpora of trustworthy sources. A common approach\nconsists in retrieving short portions of relevant text from the reference\ndocuments and giving them as input to a natural language inference module that\ndetermines whether the claim can be inferred or contradicted from them. This\napproach, however, struggles when multiple pieces of evidence need to be\ncollected and combined from different documents, since the single documents are\noften barely related to the target claim and hence they are left out by the\nretrieval module. We conjecture that a graph-based approach can be beneficial\nto identify fragmented evidence. We tested this hypothesis by building, over\nthe whole corpus, a large graph that interconnects text portions by means of\nmentioned entities and exploiting such a graph for identifying candidate sets\nof evidence from multiple sources. Our experiments show that leveraging on a\ngraph structure is beneficial in identifying a reasonably small portion of\npassages related to a claim.", "published": "2021-09-13 14:54:26", "link": "http://arxiv.org/abs/2109.06022v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The Grammar-Learning Trajectories of Neural Language Models", "abstract": "The learning trajectories of linguistic phenomena in humans provide insight\ninto linguistic representation, beyond what can be gleaned from inspecting the\nbehavior of an adult speaker. To apply a similar approach to analyze neural\nlanguage models (NLM), it is first necessary to establish that different models\nare similar enough in the generalizations they make. In this paper, we show\nthat NLMs with different initialization, architecture, and training data\nacquire linguistic phenomena in a similar order, despite their different end\nperformance. These findings suggest that there is some mutual inductive bias\nthat underlies these models' learning of linguistic phenomena. Taking\ninspiration from psycholinguistics, we argue that studying this inductive bias\nis an opportunity to study the linguistic representation implicit in NLMs.\n  Leveraging these findings, we compare the relative performance on different\nphenomena at varying learning stages with simpler reference models. Results\nsuggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find\nthe learning trajectory to be approximately one-dimensional: given an NLM with\na certain overall performance, it is possible to predict what linguistic\ngeneralizations it has already acquired. Initial analysis of these stages\npresents phenomena clusters (notably morphological ones), whose performance\nprogresses in unison, suggesting a potential link between the generalizations\nbehind them.", "published": "2021-09-13 16:17:23", "link": "http://arxiv.org/abs/2109.06096v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Isolated Utterances: Conversational Emotion Recognition", "abstract": "Speech emotion recognition is the task of recognizing the speaker's emotional\nstate given a recording of their utterance. While most of the current\napproaches focus on inferring emotion from isolated utterances, we argue that\nthis is not sufficient to achieve conversational emotion recognition (CER)\nwhich deals with recognizing emotions in conversations. In this work, we\npropose several approaches for CER by treating it as a sequence labeling task.\nWe investigated transformer architecture for CER and, compared it with\nResNet-34 and BiLSTM architectures in both contextual and context-less\nscenarios using IEMOCAP corpus. Based on the inner workings of the\nself-attention mechanism, we proposed DiverseCatAugment (DCA), an augmentation\nscheme, which improved the transformer model performance by an absolute 3.3%\nmicro-f1 on conversations and 3.6% on isolated utterances. We further enhanced\nthe performance by introducing an interlocutor-aware transformer model where we\nlearn a dictionary of interlocutor index embeddings to exploit diarized\nconversations.", "published": "2021-09-13 16:40:35", "link": "http://arxiv.org/abs/2109.06112v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Emergence of the Shape Bias Results from Communicative Efficiency", "abstract": "By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.", "published": "2021-09-13 18:05:59", "link": "http://arxiv.org/abs/2109.06232v2", "categories": ["cs.CL", "cs.IT", "cs.NE", "math.IT"], "primary_category": "cs.CL"}
{"title": "Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length\n  Bias and Beam-Search Degradation", "abstract": "Neural Machine Translation (NMT) is known to suffer from a beam-search\nproblem: after a certain point, increasing beam size causes an overall drop in\ntranslation quality. This effect is especially pronounced for long sentences.\nWhile much work was done analyzing this phenomenon, primarily for\nautoregressive NMT models, there is still no consensus on its underlying cause.\nIn this work, we analyze errors that cause major quality degradation with large\nbeams in NMT and Automatic Speech Recognition (ASR). We show that a factor that\nstrongly contributes to the quality degradation with large beams is\n\\textit{dataset length-bias} - \\textit{NMT datasets are strongly biased towards\nshort sentences}. To mitigate this issue, we propose a new data augmentation\ntechnique -- \\textit{Multi-Sentence Resampling (MSR)}. This technique extends\nthe training examples by concatenating several sentences from the original\ndataset to make a long training example. We demonstrate that MSR significantly\nreduces degradation with growing beam size and improves final translation\nquality on the IWSTL$15$ En-Vi, IWSTL$17$ En-Fr, and WMT$14$ En-De datasets.", "published": "2021-09-13 18:40:53", "link": "http://arxiv.org/abs/2109.06253v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MindCraft: Theory of Mind Modeling for Situated Dialogue in\n  Collaborative Tasks", "abstract": "An ideal integration of autonomous agents in a human world implies that they\nare able to collaborate on human terms. In particular, theory of mind plays an\nimportant role in maintaining common ground during human collaboration and\ncommunication. To enable theory of mind modeling in situated interactions, we\nintroduce a fine-grained dataset of collaborative tasks performed by pairs of\nhuman subjects in the 3D virtual blocks world of Minecraft. It provides\ninformation that captures partners' beliefs of the world and of each other as\nan interaction unfolds, bringing abundant opportunities to study human\ncollaborative behaviors in situated language communication. As a first step\ntowards our goal of developing embodied AI agents able to infer belief states\nof collaborative partners in situ, we build and present results on\ncomputational models for several theory of mind tasks.", "published": "2021-09-13 19:26:19", "link": "http://arxiv.org/abs/2109.06275v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Massively Multilingual Analysis of Cross-linguality in Shared\n  Embedding Space", "abstract": "In cross-lingual language models, representations for many different\nlanguages live in the same space. Here, we investigate the linguistic and\nnon-linguistic factors affecting sentence-level alignment in cross-lingual\npretrained language models for 101 languages and 5,050 language pairs. Using\nBERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our\ncorpus, we compute a task-based measure of cross-lingual alignment in the form\nof bitext retrieval performance, as well as four intrinsic measures of vector\nspace alignment and isomorphism. We then examine a range of linguistic,\nquasi-linguistic, and training-related features as potential predictors of\nthese alignment metrics. The results of our analyses show that word order\nagreement and agreement in morphological complexity are two of the strongest\nlinguistic predictors of cross-linguality. We also note in-family training data\nas a stronger predictor than language-specific training data across the board.\nWe verify some of our linguistic findings by looking at the effect of\nmorphological segmentation on English-Inuktitut alignment, in addition to\nexamining the effect of word order agreement on isomorphism for 66 zero-shot\nlanguage pairs from a different corpus. We make the data and code for our\nexperiments publicly available.", "published": "2021-09-13 21:05:37", "link": "http://arxiv.org/abs/2109.06324v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "InceptionXML: A Lightweight Framework with Synchronized Negative\n  Sampling for Short Text Extreme Classification", "abstract": "Automatic annotation of short-text data to a large number of target labels,\nreferred to as Short Text Extreme Classification, has found numerous\napplications including prediction of related searches and product\nrecommendation. In this paper, we propose a convolutional architecture\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\nlack of word-order in short-text queries encountered in search and\nrecommendation. We demonstrate the efficacy of applying convolutions by\nrecasting the operation along the embedding dimension instead of the word\ndimension as applied in conventional CNNs for text classification. Towards\nscaling our model to datasets with millions of labels, we also propose SyncXML\npipeline which improves upon the shortcomings of the recently proposed dynamic\nhard-negative mining technique for label short-listing by synchronizing the\nlabel-shortlister and extreme classifier. SyncXML not only reduces the\ninference time to half but is also an order of magnitude smaller than\nstate-of-the-art Astec in terms of model size. Through a comprehensive\nempirical comparison, we show that not only can InceptionXML outperform\nexisting approaches on benchmark datasets but also the transformer baselines\nrequiring only 2% FLOPs. The code for InceptionXML is available at\nhttps://github.com/xmc-aalto/inceptionxml.", "published": "2021-09-13 18:55:37", "link": "http://arxiv.org/abs/2109.07319v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Studying Fake News Spreading, Polarisation Dynamics, and Manipulation by\n  Bots: a Tale of Networks and Language", "abstract": "With the explosive growth of online social media, the ancient problem of\ninformation disorders interfering with news diffusion has surfaced with a\nrenewed intensity threatening our democracies, public health, and news outlets'\ncredibility. Therefore, thousands of scientific papers have been published in a\nrelatively short period, making researchers of different disciplines struggle\nwith an information overload problem. The aim of this survey is threefold: (1)\nwe present the results of a network-based analysis of the existing\nmultidisciplinary literature to support the search for relevant trends and\ncentral publications; (2) we describe the main results and necessary background\nto attack the problem under a computational perspective; (3) we review selected\ncontributions using network science as a unifying framework and computational\nlinguistics as the tool to make sense of the shared content. Despite scholars\nworking on computational linguistics and networks traditionally belong to\ndifferent scientific communities, we expect that those interested in the area\nof fake news should be aware of crucial aspects of both disciplines.", "published": "2021-09-13 14:10:44", "link": "http://arxiv.org/abs/2109.07909v2", "categories": ["cs.CY", "cs.CL", "cs.SI", "A.1; J.4; G.2; K.4; I.2.7"], "primary_category": "cs.CY"}
{"title": "Studying squeeze-and-excitation used in CNN for speaker verification", "abstract": "In speaker verification, the extraction of voice representations is mainly\nbased on the Residual Neural Network (ResNet) architecture. ResNet is built\nupon convolution layers which learn filters to capture local spatial patterns\nalong all the input, then generate feature maps that jointly encode the spatial\nand channel information. Unfortunately, all feature maps in a convolution layer\nare learnt independently (the convolution layer does not exploit the\ndependencies between feature maps) and locally. This problem has first been\ntackled in image processing. A channel attention mechanism, called\nsqueeze-and-excitation (SE), has recently been proposed in convolution layers\nand applied to speaker verification. This mechanism re-weights the information\nextracted across features maps. In this paper, we first propose an original\nqualitative study about the influence and the role of the SE mechanism applied\nto the speaker verification task at different stages of the ResNet, and then\nevaluate several SE architectures. We finally propose to improve the SE\napproach with a new pool- ing variant based on the concatenation of mean- and\nstandard- deviation-pooling. Results showed that applying SE only on the first\nstages of the ResNet allows to better capture speaker information for the\nverification task, and that significant discrimination gains on Voxceleb1-E,\nVoxceleb1-H and SITW evaluation tasks have been noted using the proposed\npooling variant.", "published": "2021-09-13 13:54:02", "link": "http://arxiv.org/abs/2109.05977v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
