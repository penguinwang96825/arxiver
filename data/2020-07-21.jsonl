{"title": "Neural Machine Translation with Error Correction", "abstract": "Neural machine translation (NMT) generates the next target token given as\ninput the previous ground truth target tokens during training while the\nprevious generated target tokens during inference, which causes discrepancy\nbetween training and inference as well as error propagation, and affects the\ntranslation accuracy. In this paper, we introduce an error correction mechanism\ninto NMT, which corrects the error information in the previous generated tokens\nto better predict the next token. Specifically, we introduce two-stream\nself-attention from XLNet into NMT decoder, where the query stream is used to\npredict the next token, and meanwhile the content stream is used to correct the\nerror information from the previous predicted tokens. We leverage scheduled\nsampling to simulate the prediction errors during training. Experiments on\nthree IWSLT translation datasets and two WMT translation datasets demonstrate\nthat our method achieves improvements over Transformer baseline and scheduled\nsampling. Further experimental analyses also verify the effectiveness of our\nproposed error correction mechanism to improve the translation quality.", "published": "2020-07-21 09:41:07", "link": "http://arxiv.org/abs/2007.10681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XD at SemEval-2020 Task 12: Ensemble Approach to Offensive Language\n  Identification in Social Media Using Transformer Encoders", "abstract": "This paper presents six document classification models using the latest\ntransformer encoders and a high-performing ensemble model for a task of\noffensive language identification in social media. For the individual models,\ndeep transformer layers are applied to perform multi-head attentions. For the\nensemble model, the utterance representations taken from those individual\nmodels are concatenated and fed into a linear decoder to make the final\ndecisions. Our ensemble model outperforms the individual models and shows up to\n8.6% improvement over the individual models on the development set. On the test\nset, it achieves macro-F1 of 90.9% and becomes one of the high performing\nsystems among 85 participants in the sub-task A of this shared task. Our\nanalysis shows that although the ensemble model significantly improves the\naccuracy on the development set, the improvement is not as evident on the test\nset.", "published": "2020-07-21 17:03:00", "link": "http://arxiv.org/abs/2007.10945v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Check_square at CheckThat! 2020: Claim Detection in Social Media via\n  Fusion of Transformer and Syntactic Features", "abstract": "In this digital age of news consumption, a news reader has the ability to\nreact, express and share opinions with others in a highly interactive and fast\nmanner. As a consequence, fake news has made its way into our daily life\nbecause of very limited capacity to verify news on the Internet by large\ncompanies as well as individuals. In this paper, we focus on solving two\nproblems which are part of the fact-checking ecosystem that can help to\nautomate fact-checking of claims in an ever increasing stream of content on\nsocial media. For the first problem, claim check-worthiness prediction, we\nexplore the fusion of syntactic features and deep transformer Bidirectional\nEncoder Representations from Transformers (BERT) embeddings, to classify\ncheck-worthiness of a tweet, i.e. whether it includes a claim or not. We\nconduct a detailed feature analysis and present our best performing models for\nEnglish and Arabic tweets. For the second problem, claim retrieval, we explore\nthe pre-trained embeddings from a Siamese network transformer model\n(sentence-transformers) specifically trained for semantic textual similarity,\nand perform KD-search to retrieve verified claims with respect to a query\ntweet.", "published": "2020-07-21 00:07:17", "link": "http://arxiv.org/abs/2007.10534v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Analysis and Optimization of Service Delay for Multi-quality Videos in\n  Multi-tier Heterogeneous Network with Random Caching", "abstract": "Aiming to minimize service delay, we propose a new random caching scheme in\ndevice-to-device (D2D)-assisted heterogeneous network. To support diversified\nviewing qualities of multimedia video services, each video file is encoded into\na base layer (BL) and multiple enhancement layers (ELs) by scalable video\ncoding (SVC). A super layer, including the BL and several ELs, is transmitted\nto every user. We define and quantify the service delay of multi-quality videos\nby deriving successful transmission probabilities when a user is served by a\nD2D helper, a small-cell base station (SBS) and a macro-cell base station\n(MBS). We formulate a delay minimization problem subject to the limited cache\nsizes of D2D helpers and SBSs. The structure of the optimal solutions to the\nproblem is revealed, and then an improved standard gradient projection method\nis designed to effectively obtain the solutions. Both theoretical analysis and\nMonte-Carlo simulations validate the successful transmission probabilities.\nCompared with three benchmark caching policies, the proposed SVC-based random\ncaching scheme is superior in terms of reducing the service delay.", "published": "2020-07-21 07:28:46", "link": "http://arxiv.org/abs/2007.10633v1", "categories": ["cs.NI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic", "abstract": "The COVID-19 pandemic has developed to be more than a bio-crisis as global\nnews has reported a sharp rise in xenophobia and discrimination in both online\nand offline communities. Such toxic behaviors take a heavy toll on society,\nespecially during these daunting times. Despite the gravity of the issue, very\nfew studies have studied online antisocial behaviors amid the COVID-19\npandemic. In this paper, we fill the research gap by collecting and annotating\na large dataset of over 40 million COVID-19 related tweets. Specially, we\npropose an annotation framework to annotate the antisocial behavior tweets\nautomatically. We also conduct an empirical analysis of our annotated dataset\nand found that new abusive lexicons are introduced amid the COVID-19 pandemic.\nOur study also identified the vulnerable targets of antisocial behaviors and\nthe factors that influence the spreading of online antisocial content.", "published": "2020-07-21 11:11:35", "link": "http://arxiv.org/abs/2007.10712v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Human Abnormality Detection Based on Bengali Text", "abstract": "In the field of natural language processing and human-computer interaction,\nhuman attitudes and sentiments have attracted the researchers. However, in the\nfield of human-computer interaction, human abnormality detection has not been\ninvestigated extensively and most works depend on image-based information. In\nnatural language processing, effective meaning can potentially convey by all\nwords. Each word may bring out difficult encounters because of their semantic\nconnection with ideas or categories. In this paper, an efficient and effective\nhuman abnormality detection model is introduced, that only uses Bengali text.\nThis proposed model can recognize whether the person is in a normal or abnormal\nstate by analyzing their typed Bengali text. To the best of our knowledge, this\nis the first attempt in developing a text based human abnormality detection\nsystem. We have created our Bengali dataset (contains 2000 sentences) that is\ngenerated by voluntary conversations. We have performed the comparative\nanalysis by using Naive Bayes and Support Vector Machine as classifiers. Two\ndifferent feature extraction techniques count vector, and TF-IDF is used to\nexperiment on our constructed dataset. We have achieved a maximum 89% accuracy\nand 92% F1-score with our constructed dataset in our experiment.", "published": "2020-07-21 11:21:26", "link": "http://arxiv.org/abs/2007.10718v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention for\n  Sentiment Analysis of Code Mixed Text", "abstract": "Sentiment Analysis of code-mixed text has diversified applications in opinion\nmining ranging from tagging user reviews to identifying social or political\nsentiments of a sub-population. In this paper, we present an ensemble\narchitecture of convolutional neural net (CNN) and self-attention based LSTM\nfor sentiment analysis of code-mixed tweets. While the CNN component helps in\nthe classification of positive and negative tweets, the self-attention based\nLSTM, helps in the classification of neutral tweets, because of its ability to\nidentify correct sentiment among multiple sentiment bearing units. We achieved\nF1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English\n(Hinglish) and Spanish-English (Spanglish) datasets, respectively. The\nsubmissions for Hinglish and Spanglish tasks were made under the usernames\nayushk and harsh_6 respectively.", "published": "2020-07-21 14:05:51", "link": "http://arxiv.org/abs/2007.10819v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "newsSweeper at SemEval-2020 Task 11: Context-Aware Rich Feature\n  Representations For Propaganda Classification", "abstract": "This paper describes our submissions to SemEval 2020 Task 11: Detection of\nPropaganda Techniques in News Articles for each of the two subtasks of Span\nIdentification and Technique Classification. We make use of pre-trained BERT\nlanguage model enhanced with tagging techniques developed for the task of Named\nEntity Recognition (NER), to develop a system for identifying propaganda spans\nin the text. For the second subtask, we incorporate contextual features in a\npre-trained RoBERTa model for the classification of propaganda techniques. We\nwere ranked 5th in the propaganda technique classification subtask.", "published": "2020-07-21 14:06:59", "link": "http://arxiv.org/abs/2007.10827v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE", "abstract": "In this paper, we describe our system for Task 4 of SemEval 2020, which\ninvolves differentiating between natural language statements that confirm to\ncommon sense and those that do not. The organizers propose three subtasks -\nfirst, selecting between two sentences, the one which is against common sense.\nSecond, identifying the most crucial reason why a statement does not make\nsense. Third, generating novel reasons for explaining the against common sense\nstatement. Out of the three subtasks, this paper reports the system description\nof subtask A and subtask B. This paper proposes a model based on transformer\nneural network architecture for addressing the subtasks. The novelty in work\nlies in the architecture design, which handles the logical implication of\ncontradicting statements and simultaneous information extraction from both\nsentences. We use a parallel instance of transformers, which is responsible for\na boost in the performance. We achieved an accuracy of 94.8% in subtask A and\n89% in subtask B on the test set.", "published": "2020-07-21 14:08:02", "link": "http://arxiv.org/abs/2007.10830v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Connecting Embeddings for Knowledge Graph Entity Typing", "abstract": "Knowledge graph (KG) entity typing aims at inferring possible missing entity\ntype instances in KG, which is a very significant but still under-explored\nsubtask of knowledge graph completion. In this paper, we propose a novel\napproach for KG entity typing which is trained by jointly utilizing local\ntyping knowledge from existing entity type assertions and global triple\nknowledge from KGs. Specifically, we present two distinct knowledge-driven\neffective mechanisms of entity type inference. Accordingly, we build two novel\nembedding models to realize the mechanisms. Afterward, a joint model with them\nis used to infer missing entity type instances, which favors inferences that\nagree with both entity type instances and triple knowledge in KGs. Experimental\nresults on two real-world datasets (Freebase and YAGO) demonstrate the\neffectiveness of our proposed mechanisms and models for improving KG entity\ntyping. The source code and data of this paper can be obtained from:\nhttps://github.com/ Adam1679/ConnectE", "published": "2020-07-21 15:00:01", "link": "http://arxiv.org/abs/2007.10873v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "problemConquero at SemEval-2020 Task 12: Transformer and Soft\n  label-based approaches", "abstract": "In this paper, we present various systems submitted by our team\nproblemConquero for SemEval-2020 Shared Task 12 Multilingual Offensive Language\nIdentification in Social Media. We participated in all the three sub-tasks of\nOffensEval-2020, and our final submissions during the evaluation phase included\ntransformer-based approaches and a soft label-based approach. BERT based\nfine-tuned models were submitted for each language of sub-task A (offensive\ntweet identification). RoBERTa based fine-tuned model for sub-task B (automatic\ncategorization of offense types) was submitted. We submitted two models for\nsub-task C (offense target identification), one using soft labels and the other\nusing BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out\nof 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and\nEnglish-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our\nbest rank for sub-task C was 20 out of 39 using BERT based fine-tuned model.", "published": "2020-07-21 15:06:58", "link": "http://arxiv.org/abs/2007.10877v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Curriculum Vitae Recommendation Based on Text Mining", "abstract": "During the last years, the development in diverse areas related to computer\nscience and internet, allowed to generate new alternatives for decision making\nin the selection of personnel for state and private companies. In order to\noptimize this selection process, the recommendation systems are the most\nsuitable for working with explicit information related to the likes and\ndislikes of employers or end users, since this information allows to generate\nlists of recommendations based on collaboration or similarity of content.\nTherefore, this research takes as a basis these characteristics contained in\nthe database of curricula and job offers, which correspond to the Peruvian\nambit, which highlights the experience, knowledge and skills of each candidate,\nwhich are described in textual terms or words. This research focuses on the\nproblem: how we can take advantage from the growth of unstructured information\nabout job offers and curriculum vitae on different websites for CV\nrecommendation. So, we use the techniques from Text Mining and Natural Language\nProcessing. Then, as a relevant technique for the present study, we emphasize\nthe technique frequency of the Term - Inverse Frequency of the documents\n(TF-IDF), which allows identifying the most relevant CVs in relation to a job\noffer of website through the average values (TF-IDF). So, the weighted value\ncan be used as a qualification value of the relevant curriculum vitae for the\nrecommendation.", "published": "2020-07-21 19:29:26", "link": "http://arxiv.org/abs/2007.11053v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explainable Rumor Detection using Inter and Intra-feature Attention\n  Networks", "abstract": "With social media becoming ubiquitous, information consumption from this\nmedia has also increased. However, one of the serious problems that have\nemerged with this increase, is the propagation of rumors. Therefore, rumor\nidentification is a very critical task with significant implications to\neconomy, democracy as well as public health and safety. We tackle the problem\nof automated detection of rumors in social media in this paper by designing a\nmodular explainable architecture that uses both latent and handcrafted features\nand can be expanded to as many new classes of features as desired. This\napproach will allow the end user to not only determine whether the piece of\ninformation on the social media is real of a rumor, but also give explanations\non why the algorithm arrived at its conclusion. Using attention mechanisms, we\nare able to interpret the relative importance of each of these features as well\nas the relative importance of the feature classes themselves. The advantage of\nthis approach is that the architecture is expandable to more handcrafted\nfeatures as they become available and also to conduct extensive testing to\ndetermine the relative influences of these features in the final decision.\nExtensive experimentation on popular datasets and benchmarking against eleven\ncontemporary algorithms, show that our approach performs significantly better\nin terms of F-score and accuracy while also being interpretable.", "published": "2020-07-21 19:35:39", "link": "http://arxiv.org/abs/2007.11057v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Book Success Prediction with Pretrained Sentence Embeddings and\n  Readability Scores", "abstract": "Predicting the potential success of a book in advance is vital in many\napplications. This could help both publishers and readers in their\ndecision-making process whether or not a book is worth publishing and reading,\nrespectively. In this paper, we propose a model that leverages pretrained\nsentence embeddings along with various readability scores for book success\nprediction. Unlike previous methods, the proposed method requires no\ncount-based, lexical, or syntactic features. Instead, we use a convolutional\nneural network over pretrained sentence embeddings and leverage different\nreadability scores through a simple concatenation operation. Our proposed model\noutperforms strong baselines for this task by as large as 6.4\\% F1-score\npoints. Moreover, our experiments show that according to our model, only the\nfirst 1K sentences are good enough to predict the potential success of books.", "published": "2020-07-21 20:11:18", "link": "http://arxiv.org/abs/2007.11073v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Representation for Rhythms", "abstract": "This paper proposes a word representation strategy for rhythm patterns. Using\n1034 pieces of Nottingham Dataset, a rhythm word dictionary whose size is 450\n(without control tokens) is generated. BERT model is created to explore\nsyntactic potentials of rhythm words. Our model is able to find overall music\nstructures and cluster different meters. In a larger scheme, a think mode -\nmusic as language - is proposed for systematic considerations.", "published": "2020-07-21 05:39:12", "link": "http://arxiv.org/abs/2007.10610v4", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "IITK at SemEval-2020 Task 10: Transformers for Emphasis Selection", "abstract": "This paper describes the system proposed for addressing the research problem\nposed in Task 10 of SemEval-2020: Emphasis Selection For Written Text in Visual\nMedia. We propose an end-to-end model that takes as input the text and\ncorresponding to each word gives the probability of the word to be emphasized.\nOur results show that transformer-based models are particularly effective in\nthis task. We achieved the best Matchm score (described in section 2.2) of\n0.810 and were ranked third on the leaderboard.", "published": "2020-07-21 14:05:56", "link": "http://arxiv.org/abs/2007.10820v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of\n  Internet Memes", "abstract": "Social media is abundant in visual and textual information presented together\nor in isolation. Memes are the most popular form, belonging to the former\nclass. In this paper, we present our approaches for the Memotion Analysis\nproblem as posed in SemEval-2020 Task 8. The goal of this task is to classify\nmemes based on their emotional content and sentiment. We leverage techniques\nfrom Natural Language Processing (NLP) and Computer Vision (CV) towards the\nsentiment classification of internet memes (Subtask A). We consider Bimodal\n(text and image) as well as Unimodal (text-only) techniques in our study\nranging from the Na\\\"ive Bayes classifier to Transformer-based approaches. Our\nresults show that a text-only approach, a simple Feed Forward Neural Network\n(FFNN) with Word2vec embeddings as input, performs superior to all the others.\nWe stand first in the Sentiment analysis task with a relative improvement of\n63% over the baseline macro-F1 score. Our work is relevant to any task\nconcerned with the combination of different modalities.", "published": "2020-07-21 14:06:26", "link": "http://arxiv.org/abs/2007.10822v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITK-RSA at SemEval-2020 Task 5: Detecting Counterfactuals", "abstract": "This paper describes our efforts in tackling Task 5 of SemEval-2020. The task\ninvolved detecting a class of textual expressions known as counterfactuals and\nseparating them into their constituent elements. Counterfactual statements\ndescribe events that have not or could not have occurred and the possible\nimplications of such events. While counterfactual reasoning is natural for\nhumans, understanding these expressions is difficult for artificial agents due\nto a variety of linguistic subtleties. Our final submitted approaches were an\nensemble of various fine-tuned transformer-based and CNN-based models for the\nfirst subtask and a transformer model with dependency tree information for the\nsecond subtask. We ranked 4-th and 9-th in the overall leaderboard. We also\nexplored various other approaches that involved the use of classical methods,\nother neural architectures and the incorporation of different linguistic\nfeatures.", "published": "2020-07-21 14:45:53", "link": "http://arxiv.org/abs/2007.10866v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guided multi-branch learning systems for sound event detection with\n  sound separation", "abstract": "In this paper, we describe in detail our systems for DCASE 2020 Task 4. The\nsystems are based on the 1st-place system of DCASE 2019 Task 4, which adopts\nweakly-supervised framework with an attention-based embedding-level pooling\nmodule and a semi-supervised learning approach named guided learning. This\nyear, we incorporate multi-branch learning (MBL) into the original system to\nfurther improve its performance. MBL uses different branches with different\npooling strategies (including instance-level and embedding-level strategies)\nand different pooling modules (including attention pooling, global max pooling\nor global average pooling modules), which share the same feature encoder of the\nmodel. Therefore, multiple branches pursuing different purposes and focusing on\ndifferent characteristics of the data can help the feature encoder model the\nfeature space better and avoid over-fitting. To better exploit the\nstrongly-labeled synthetic data, inspired by multi-task learning, we also\nemploy a sound event detection branch. To combine sound separation (SS) with\nsound event detection (SED), we fuse the results of SED systems with SS-SED\nsystems which are trained using separated sound output by an SS system. The\nexperimental results prove that MBL can improve the model performance and using\nSS has great potential to improve the performance of SED ensemble system.", "published": "2020-07-21 07:35:16", "link": "http://arxiv.org/abs/2007.10638v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Very Fast Keyword Spotting System with Real Time Factor below 0.01", "abstract": "In the paper we present an architecture of a keyword spotting (KWS) system\nthat is based on modern neural networks, yields good performance on various\ntypes of speech data and can run very fast. We focus mainly on the last aspect\nand propose optimizations for all the steps required in a KWS design: signal\nprocessing and likelihood computation, Viterbi decoding, spot candidate\ndetection and confidence calculation. We present time and memory efficient\nmodelling by bidirectional feedforward sequential memory networks (an\nalternative to recurrent nets) either by standard triphones or so called\nquasi-monophones, and an entirely forward decoding of speech frames (with\nminimal need for look back). Several variants of the proposed scheme are\nevaluated on 3 large Czech datasets (broadcast, internet and telephone, 17\nhours in total) and their performance is compared by Detection Error Tradeoff\n(DET) diagrams and real-time (RT) factors. We demonstrate that the complete\nsystem can run in a single pass with a RT factor close to 0.001 if all\noptimizations (including a GPU for likelihood computation) are applied.", "published": "2020-07-21 10:55:07", "link": "http://arxiv.org/abs/2007.10706v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech\n  Recognition", "abstract": "Recent advances in Automatic Speech Recognition (ASR) demonstrated how\nend-to-end systems are able to achieve state-of-the-art performance. There is a\ntrend towards deeper neural networks, however those ASR models are also more\ncomplex and prone against specially crafted noisy data. Those Audio Adversarial\nExamples (AAE) were previously demonstrated on ASR systems that use\nConnectionist Temporal Classification (CTC), as well as attention-based\nencoder-decoder architectures. Following the idea of the hybrid CTC/attention\nASR system, this work proposes algorithms to generate AAEs to combine both\napproaches into a joint CTC-attention gradient method. Evaluation is performed\nusing a hybrid CTC/attention end-to-end ASR model on two reference sentences as\ncase study, as well as the TEDlium v2 speech recognition task. We then\ndemonstrate the application of this algorithm for adversarial training to\nobtain a more robust ASR model.", "published": "2020-07-21 11:30:24", "link": "http://arxiv.org/abs/2007.10723v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "3D Localization of a Sound Source Using Mobile Microphone Arrays\n  Referenced by SLAM", "abstract": "A microphone array can provide a mobile robot with the capability of\nlocalizing, tracking and separating distant sound sources in 2D, i.e.,\nestimating their relative elevation and azimuth. To combine acoustic data with\nvisual information in real world settings, spatial correlation must be\nestablished. The approach explored in this paper consists of having two robots,\neach equipped with a microphone array, localizing themselves in a shared\nreference map using SLAM. Based on their locations, data from the microphone\narrays are used to triangulate in 3D the location of a sound source in relation\nto the same map. This strategy results in a novel cooperative sound mapping\napproach using mobile microphone arrays. Trials are conducted using two mobile\nrobots localizing a static or a moving sound source to examine in which\nconditions this is possible. Results suggest that errors under 0.3 m are\nobserved when the relative angle between the two robots are above 30 degrees\nfor a static sound source, while errors under 0.3 m for angles between 40\ndegrees and 140 degrees are observed with a moving sound source.", "published": "2020-07-21 20:22:00", "link": "http://arxiv.org/abs/2007.11079v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video\n  Parsing", "abstract": "In this paper, we introduce a new problem, named audio-visual video parsing,\nwhich aims to parse a video into temporal event segments and label them as\neither audible, visible, or both. Such a problem is essential for a complete\nunderstanding of the scene depicted inside a video. To facilitate exploration,\nwe collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual\nvideo parsing in a weakly-supervised manner. This task can be naturally\nformulated as a Multimodal Multiple Instance Learning (MMIL) problem.\nConcretely, we propose a novel hybrid attention network to explore unimodal and\ncross-modal temporal contexts simultaneously. We develop an attentive MMIL\npooling method to adaptively explore useful audio and visual content from\ndifferent temporal extent and modalities. Furthermore, we discover and mitigate\nmodality bias and noisy label issues with an individual-guided learning\nmechanism and label smoothing technique, respectively. Experimental results\nshow that the challenging audio-visual video parsing can be achieved even with\nonly video-level weak labels. Our proposed framework can effectively leverage\nunimodal and cross-modal temporal contexts and alleviate modality bias and\nnoisy labels problems.", "published": "2020-07-21 01:53:31", "link": "http://arxiv.org/abs/2007.10558v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "CSLNSpeech: solving extended speech separation problem with the help of\n  Chinese sign language", "abstract": "Previous audio-visual speech separation methods use the synchronization of\nthe speaker's facial movement and speech in the video to supervise the speech\nseparation in a self-supervised way. In this paper, we propose a model to solve\nthe speech separation problem assisted by both face and sign language, which we\ncall the extended speech separation problem. We design a general deep learning\nnetwork for learning the combination of three modalities, audio, face, and sign\nlanguage information, for better solving the speech separation problem. To\ntrain the model, we introduce a large-scale dataset named the Chinese Sign\nLanguage News Speech (CSLNSpeech) dataset, in which three modalities of audio,\nface, and sign language coexist. Experiment results show that the proposed\nmodel has better performance and robustness than the usual audio-visual system.\nBesides, sign language modality can also be used alone to supervise speech\nseparation tasks, and the introduction of sign language is helpful for\nhearing-impaired people to learn and communicate. Last, our model is a general\nspeech separation framework and can achieve very competitive separation\nperformance on two open-source audio-visual datasets. The code is available at\nhttps://github.com/iveveive/SLNSpeech", "published": "2020-07-21 07:22:33", "link": "http://arxiv.org/abs/2007.10629v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Optimization of data-driven filterbank for automatic speaker\n  verification", "abstract": "Most of the speech processing applications use triangular filters spaced in\nmel-scale for feature extraction. In this paper, we propose a new data-driven\nfilter design method which optimizes filter parameters from a given speech\ndata. First, we introduce a frame-selection based approach for developing\nspeech-signal-based frequency warping scale. Then, we propose a new method for\ncomputing the filter frequency responses by using principal component analysis\n(PCA). The main advantage of the proposed method over the recently introduced\ndeep learning based methods is that it requires very limited amount of\nunlabeled speech-data. We demonstrate that the proposed filterbank has more\nspeaker discriminative power than commonly used mel filterbank as well as\nexisting data-driven filterbank. We conduct automatic speaker verification\n(ASV) experiments with different corpora using various classifier back-ends. We\nshow that the acoustic features created with proposed filterbank are better\nthan existing mel-frequency cepstral coefficients (MFCCs) and\nspeech-signal-based frequency cepstral coefficients (SFCCs) in most cases. In\nthe experiments with VoxCeleb1 and popular i-vector back-end, we observe 9.75%\nrelative improvement in equal error rate (EER) over MFCCs. Similarly, the\nrelative improvement is 4.43% with recently introduced x-vector system. We\nobtain further improvement using fusion of the proposed method with standard\nMFCC-based approach.", "published": "2020-07-21 11:42:20", "link": "http://arxiv.org/abs/2007.10729v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Read and Follow Music in Complete Score Sheet Images", "abstract": "This paper addresses the task of score following in sheet music given as\nunprocessed images. While existing work either relies on OMR software to obtain\na computer-readable score representation, or crucially relies on prepared sheet\nimage excerpts, we propose the first system that directly performs score\nfollowing in full-page, completely unprocessed sheet images. Based on incoming\naudio and a given image of the score, our system directly predicts the most\nlikely position within the page that matches the audio, outperforming current\nstate-of-the-art image-based score followers in terms of alignment precision.\nWe also compare our method to an OMR-based approach and empirically show that\nit can be a viable alternative to such a system.", "published": "2020-07-21 11:53:22", "link": "http://arxiv.org/abs/2007.10736v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Time-Frequency Scattering Accurately Models Auditory Similarities\n  Between Instrumental Playing Techniques", "abstract": "Instrumental playing techniques such as vibratos, glissandos, and trills\noften denote musical expressivity, both in classical and folk contexts.\nHowever, most existing approaches to music similarity retrieval fail to\ndescribe timbre beyond the so-called \"ordinary\" technique, use instrument\nidentity as a proxy for timbre quality, and do not allow for customization to\nthe perceptual idiosyncrasies of a new subject. In this article, we ask 31\nhuman subjects to organize 78 isolated notes into a set of timbre clusters.\nAnalyzing their responses suggests that timbre perception operates within a\nmore flexible taxonomy than those provided by instruments or playing techniques\nalone. In addition, we propose a machine listening model to recover the cluster\ngraph of auditory similarities across instruments, mutes, and techniques. Our\nmodel relies on joint time--frequency scattering features to extract\nspectrotemporal modulations as acoustic features. Furthermore, it minimizes\ntriplet loss in the cluster graph by means of the large-margin nearest neighbor\n(LMNN) metric learning algorithm. Over a dataset of 9346 isolated notes, we\nreport a state-of-the-art average precision at rank five (AP@5) of\n$99.0\\%\\pm1$. An ablation study demonstrates that removing either the joint\ntime--frequency scattering transform or the metric learning algorithm\nnoticeably degrades performance.", "published": "2020-07-21 16:37:15", "link": "http://arxiv.org/abs/2007.10926v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Foley Music: Learning to Generate Music from Videos", "abstract": "In this paper, we introduce Foley Music, a system that can synthesize\nplausible music for a silent video clip about people playing musical\ninstruments. We first identify two key intermediate representations for a\nsuccessful video to music generator: body keypoints from videos and MIDI events\nfrom audio recordings. We then formulate music generation from videos as a\nmotion-to-MIDI translation problem. We present a Graph$-$Transformer framework\nthat can accurately predict MIDI event sequences in accordance with the body\nmovements. The MIDI event can then be converted to realistic music using an\noff-the-shelf music synthesizer tool. We demonstrate the effectiveness of our\nmodels on videos containing a variety of music performances. Experimental\nresults show that our model outperforms several existing systems in generating\nmusic that is pleasant to listen to. More importantly, the MIDI representations\nare fully interpretable and transparent, thus enabling us to perform music\nediting flexibly. We encourage the readers to watch the demo video with audio\nturned on to experience the results.", "published": "2020-07-21 17:59:06", "link": "http://arxiv.org/abs/2007.10984v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
