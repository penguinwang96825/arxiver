{"title": "TransAug: Translate as Augmentation for Sentence Embeddings", "abstract": "While contrastive learning greatly advances the representation of sentence\nembeddings, it is still limited by the size of the existing sentence datasets.\nIn this paper, we present TransAug (Translate as Augmentation), which provide\nthe first exploration of utilizing translated sentence pairs as data\naugmentation for text, and introduce a two-stage paradigm to advances the\nstate-of-the-art sentence embeddings. Instead of adopting an encoder trained in\nother languages setting, we first distill a Chinese encoder from a SimCSE\nencoder (pretrained in English), so that their embeddings are close in semantic\nspace, which can be regraded as implicit data augmentation. Then, we only\nupdate the English encoder via cross-lingual contrastive learning and frozen\nthe distilled Chinese encoder. Our approach achieves a new state-of-art on\nstandard semantic textual similarity (STS), outperforming both SimCSE and\nSentence-T5, and the best performance in corresponding tracks on transfer tasks\nevaluated by SentEval.", "published": "2021-10-30 03:13:28", "link": "http://arxiv.org/abs/2111.00157v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Knowledge Augmentation for Generative Commonsense Reasoning", "abstract": "Generative commonsense reasoning is the capability of a language model to\ngenerate a sentence with a given concept-set that is based on commonsense\nknowledge. However, generative language models still struggle to provide\noutputs, and the training set does not contain patterns that are sufficient for\ngenerative commonsense reasoning. In this paper, we propose a data-centric\nmethod that uses automatic knowledge augmentation to extend commonsense\nknowledge using a machine knowledge generator. This method can generate\nsemi-golden sentences that improve the generative commonsense reasoning of a\nlanguage model without architecture modifications. Furthermore, this approach\nis a model-agnostic method and does not require human effort for data\nconstruction.", "published": "2021-10-30 06:53:48", "link": "http://arxiv.org/abs/2111.00192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Magic Pyramid: Accelerating Inference with Early Exiting and Token\n  Pruning", "abstract": "Pre-training and then fine-tuning large language models is commonly used to\nachieve state-of-the-art performance in natural language processing (NLP)\ntasks. However, most pre-trained models suffer from low inference speed.\nDeploying such large models to applications with latency constraints is\nchallenging. In this work, we focus on accelerating the inference via\nconditional computations. To achieve this, we propose a novel idea, Magic\nPyramid (MP), to reduce both width-wise and depth-wise computation via token\npruning and early exiting for Transformer-based models, particularly BERT. The\nformer manages to save the computation via removing non-salient tokens, while\nthe latter can fulfill the computation reduction by terminating the inference\nearly before reaching the final layer, if the exiting condition is met. Our\nempirical studies demonstrate that compared to previous state of arts, MP is\nnot only able to achieve a speed-adjustable inference but also to surpass token\npruning and early exiting by reducing up to 70% giga floating point operations\n(GFLOPs) with less than 0.5% accuracy drop. Token pruning and early exiting\nexpress distinctive preferences to sequences with different lengths. However,\nMP is capable of achieving an average of 8.06x speedup on two popular text\nclassification tasks, regardless of the sizes of the inputs.", "published": "2021-10-30 11:07:43", "link": "http://arxiv.org/abs/2111.00230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventNarrative: A large-scale Event-centric Dataset for Knowledge\n  Graph-to-Text Generation", "abstract": "We introduce EventNarrative, a knowledge graph-to-text dataset from publicly\navailable open-world knowledge graphs. Given the recent advances in\nevent-driven Information Extraction (IE), and that prior research on\ngraph-to-text only focused on entity-driven KGs, this paper focuses on\nevent-centric data. However, our data generation system can still be adapted to\nother other types of KG data. Existing large-scale datasets in the\ngraph-to-text area are non-parallel, meaning there is a large disconnect\nbetween the KGs and text. The datasets that have a paired KG and text, are\nsmall scale and manually generated or generated without a rich ontology, making\nthe corresponding graphs sparse. Furthermore, these datasets contain many\nunlinked entities between their KG and text pairs. EventNarrative consists of\napproximately 230,000 graphs and their corresponding natural language text, 6\ntimes larger than the current largest parallel dataset. It makes use of a rich\nontology, all of the KGs entities are linked to the text, and our manual\nannotations confirm a high data quality. Our aim is two-fold: help break new\nground in event-centric research where data is lacking, and to give researchers\na well-defined, large-scale dataset in order to better evaluate existing and\nfuture knowledge graph-to-text models. We also evaluate two types of baseline\non EventNarrative: a graph-to-text specific model and two state-of-the-art\nlanguage models, which previous work has shown to be adaptable to the knowledge\ngraph-to-text domain.", "published": "2021-10-30 15:39:20", "link": "http://arxiv.org/abs/2111.00276v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language\n  Models", "abstract": "Gigantic pre-trained models have become central to natural language\nprocessing (NLP), serving as the starting point for fine-tuning towards a range\nof downstream tasks. However, two pain points persist for this paradigm: (a) as\nthe pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the\nfine-tuning process can be time-consuming and computationally expensive; (b)\nthe fine-tuned model has the same size as its starting point by default, which\nis neither sensible due to its more specialized functionality, nor practical\nsince many fine-tuned models will be deployed in resource-constrained\nenvironments. To address these pain points, we propose a framework for\nresource- and parameter-efficient fine-tuning by leveraging the sparsity prior\nin both weight updates and the final model weights. Our proposed framework,\ndubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two\nkey objectives: (i) parameter efficient fine-tuning - by enforcing\nsparsity-aware low-rank updates on top of the pre-trained weights; and (ii)\nresource-efficient inference - by encouraging a sparse weight structure towards\nthe final fine-tuned model. We leverage sparsity in these two directions by\nexploiting both unstructured and structured sparse patterns in pre-trained\nlanguage models via a unified approach. Extensive experiments and in-depth\ninvestigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2)\non dozens of datasets, consistently demonstrate impressive\nparameter-/inference-efficiency, while maintaining competitive downstream\nperformance. For instance, DSEE saves about 25% inference FLOPs while achieving\ncomparable performance, with 0.5% trainable parameters on BERT. Codes are\navailable in https://github.com/VITA-Group/DSEE.", "published": "2021-10-30 03:29:47", "link": "http://arxiv.org/abs/2111.00160v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hierarchical Heterogeneous Graph Representation Learning for Short Text\n  Classification", "abstract": "Short text classification is a fundamental task in natural language\nprocessing. It is hard due to the lack of context information and labeled data\nin practice. In this paper, we propose a new method called SHINE, which is\nbased on graph neural network (GNN), for short text classification. First, we\nmodel the short text dataset as a hierarchical heterogeneous graph consisting\nof word-level component graphs which introduce more semantic and syntactic\ninformation. Then, we dynamically learn a short document graph that facilitates\neffective label propagation among similar short texts. Thus, compared with\nexisting GNN-based methods, SHINE can better exploit interactions between nodes\nof the same types and capture similarities between short texts. Extensive\nexperiments on various benchmark short text datasets show that SHINE\nconsistently outperforms state-of-the-art methods, especially with fewer\nlabels.", "published": "2021-10-30 05:33:05", "link": "http://arxiv.org/abs/2111.00180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How should human translation coexist with NMT? Efficient tool for\n  building high quality parallel corpus", "abstract": "This paper proposes a tool for efficiently constructing high-quality parallel\ncorpora with minimizing human labor and making this tool publicly available.\nOur proposed construction process is based on neural machine translation (NMT)\nto allow for it to not only coexist with human translation, but also improve\nits efficiency by combining data quality control with human translation in a\ndata-centric approach.", "published": "2021-10-30 06:53:41", "link": "http://arxiv.org/abs/2111.00191v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments", "abstract": "In this paper, we introduce EmpBot: an end-to-end empathetic chatbot.\nEmpathetic conversational agents should not only understand what is being\ndiscussed, but also acknowledge the implied feelings of the conversation\npartner and respond appropriately. To this end, we propose a method based on a\ntransformer pretrained language model (T5). Specifically, during finetuning we\npropose to use three objectives: response language modeling, sentiment\nunderstanding, and empathy forcing. The first objective is crucial for\ngenerating relevant and coherent responses, while the next ones are significant\nfor acknowledging the sentimental state of the conversational partner and for\nfavoring empathetic responses. We evaluate our model on the EmpatheticDialogues\ndataset using both automated metrics and human evaluation. The inclusion of the\nsentiment understanding and empathy forcing auxiliary losses favor empathetic\nresponses, as human evaluation results indicate, comparing with the current\nstate-of-the-art.", "published": "2021-10-30 19:04:48", "link": "http://arxiv.org/abs/2111.00310v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdvCodeMix: Adversarial Attack on Code-Mixed Data", "abstract": "Research on adversarial attacks are becoming widely popular in the recent\nyears. One of the unexplored areas where prior research is lacking is the\neffect of adversarial attacks on code-mixed data. Therefore, in the present\nwork, we have explained the first generalized framework on text perturbation to\nattack code-mixed classification models in a black-box setting. We rely on\nvarious perturbation techniques that preserve the semantic structures of the\nsentences and also obscure the attacks from the perception of a human user. The\npresent methodology leverages the importance of a token to decide where to\nattack by employing various perturbation strategies. We test our strategies on\nvarious sentiment classification models trained on Bengali-English and\nHindi-English code-mixed datasets, and reduce their F1-scores by nearly 51 %\nand 53 % respectively, which can be further reduced if a larger number of\ntokens are perturbed in a given sentence.", "published": "2021-10-30 22:02:22", "link": "http://arxiv.org/abs/2111.00350v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pseudo-Labeling for Massively Multilingual Speech Recognition", "abstract": "Semi-supervised learning through pseudo-labeling has become a staple of\nstate-of-the-art monolingual speech recognition systems. In this work, we\nextend pseudo-labeling to massively multilingual speech recognition with 60\nlanguages. We propose a simple pseudo-labeling recipe that works well even with\nlow-resource languages: train a supervised multilingual model, fine-tune it\nwith semi-supervised learning on a target language, generate pseudo-labels for\nthat language, and train a final model using pseudo-labels for all languages,\neither from scratch or by fine-tuning. Experiments on the labeled Common Voice\nand unlabeled VoxPopuli datasets show that our recipe can yield a model with\nbetter performance for many languages that also transfers well to LibriSpeech.", "published": "2021-10-30 03:30:17", "link": "http://arxiv.org/abs/2111.00161v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Backdoor Pre-trained Models Can Transfer to All", "abstract": "Pre-trained general-purpose language models have been a dominating component\nin enabling real-world natural language processing (NLP) applications. However,\na pre-trained model with backdoor can be a severe threat to the applications.\nMost existing backdoor attacks in NLP are conducted in the fine-tuning phase by\nintroducing malicious triggers in the targeted class, thus relying greatly on\nthe prior knowledge of the fine-tuning task. In this paper, we propose a new\napproach to map the inputs containing triggers directly to a predefined output\nrepresentation of the pre-trained NLP models, e.g., a predefined output\nrepresentation for the classification token in BERT, instead of a target label.\nIt can thus introduce backdoor to a wide range of downstream tasks without any\nprior knowledge. Additionally, in light of the unique properties of triggers in\nNLP, we propose two new metrics to measure the performance of backdoor attacks\nin terms of both effectiveness and stealthiness. Our experiments with various\ntypes of triggers show that our method is widely applicable to different\nfine-tuning tasks (classification and named entity recognition) and to\ndifferent models (such as BERT, XLNet, BART), which poses a severe threat.\nFurthermore, by collaborating with the popular online model repository Hugging\nFace, the threat brought by our method has been confirmed. Finally, we analyze\nthe factors that may affect the attack performance and share insights on the\ncauses of the success of our backdoor attack.", "published": "2021-10-30 07:11:24", "link": "http://arxiv.org/abs/2111.00197v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-attention conformer for context modeling in speech enhancement for\n  ASR", "abstract": "This work introduces \\emph{cross-attention conformer}, an attention-based\narchitecture for context modeling in speech enhancement. Given that the context\ninformation can often be sequential, and of different length as the audio that\nis to be enhanced, we make use of cross-attention to summarize and merge\ncontextual information with input features. Building upon the recently proposed\nconformer model that uses self attention layers as building blocks, the\nproposed cross-attention conformer can be used to build deep contextual models.\nAs a concrete example, we show how noise context, i.e., short noise-only audio\nsegment preceding an utterance, can be used to build a speech enhancement\nfeature frontend using cross-attention conformer layers for improving noise\nrobustness of automatic speech recognition.", "published": "2021-10-30 00:15:19", "link": "http://arxiv.org/abs/2111.00127v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Speech Denoising Using Only Noisy Audio Signals", "abstract": "In traditional speech denoising tasks, clean audio signals are often used as\nthe training target, but absolutely clean signals are collected from expensive\nrecording equipment or in studios with the strict environments. To overcome\nthis drawback, we propose an end-to-end self-supervised speech denoising\ntraining scheme using only noisy audio signals, named Only-Noisy Training\n(ONT), without extra training conditions. The proposed ONT strategy constructs\ntraining pairs only from each single noisy audio, and it contains two modules:\ntraining audio pairs generated module and speech denoising module. The first\nmodule adopts a random audio sub-sampler on each noisy audio to generate\ntraining pairs. The sub-sampled pairs are then fed into a novel complex-valued\nspeech denoising module. Experimental results show that the proposed method not\nonly eliminates the high dependence on clean targets of traditional audio\ndenoising tasks, but also achieves on-par or better performance than other\ntraining strategies. Availability-ONT is available at\nhttps://github.com/liqingchunnnn/Only-Noisy-Training", "published": "2021-10-30 13:00:23", "link": "http://arxiv.org/abs/2111.00242v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Continuous Representation of Audio for Arbitrary Scale Super\n  Resolution", "abstract": "Audio super resolution aims to predict the missing high resolution components\nof the low resolution audio signals. While audio in nature is a continuous\nsignal, current approaches treat it as discrete data (i.e., input is defined on\ndiscrete time domain), and consider the super resolution over a fixed scale\nfactor (i.e., it is required to train a new neural network to change output\nresolution). To obtain a continuous representation of audio and enable super\nresolution for arbitrary scale factor, we propose a method of implicit neural\nrepresentation, coined Local Implicit representation for Super resolution of\nArbitrary scale (LISA). Our method locally parameterizes a chunk of audio as a\nfunction of continuous time, and represents each chunk with the local latent\ncodes of neighboring chunks so that the function can extrapolate the signal at\nany time coordinate, i.e., infinite resolution. To learn a continuous\nrepresentation for audio, we design a self-supervised learning strategy to\npractice super resolution tasks up to the original resolution by stochastic\nselection. Our numerical evaluation shows that LISA outperforms the previous\nfixed-scale methods with a fraction of parameters, but also is capable of\narbitrary scale super resolution even beyond the resolution of training data.", "published": "2021-10-30 07:09:18", "link": "http://arxiv.org/abs/2111.00195v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time Speaker counting in a cocktail party scenario using\n  Attention-guided Convolutional Neural Network", "abstract": "Most current speech technology systems are designed to operate well even in\nthe presence of multiple active speakers. However, most solutions assume that\nthe number of co-current speakers is known. Unfortunately, this information\nmight not always be available in real-world applications. In this study, we\npropose a real-time, single-channel attention-guided Convolutional Neural\nNetwork (CNN) to estimate the number of active speakers in overlapping speech.\nThe proposed system extracts higher-level information from the speech spectral\ncontent using a CNN model. Next, the attention mechanism summarizes the\nextracted information into a compact feature vector without losing critical\ninformation. Finally, the active speakers are classified using a fully\nconnected network. Experiments on simulated overlapping speech using WSJ corpus\nshow that the attention solution is shown to improve the performance by almost\n3% absolute over conventional temporal average pooling. The proposed\nAttention-guided CNN achieves 76.15% for both Weighted Accuracy and average\nRecall, and 75.80% Precision on speech segments as short as 20 frames (i.e.,\n200 ms). All the classification metrics exceed 92% for the attention-guided\nmodel in offline scenarios where the input signal is more than 100 frames long\n(i.e., 1s).", "published": "2021-10-30 19:24:57", "link": "http://arxiv.org/abs/2111.00316v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker conditioning of acoustic models using affine transformation for\n  multi-speaker speech recognition", "abstract": "This study addresses the problem of single-channel Automatic Speech\nRecognition of a target speaker within an overlap speech scenario. In the\nproposed method, the hidden representations in the acoustic model are modulated\nby speaker auxiliary information to recognize only the desired speaker. Affine\ntransformation layers are inserted into the acoustic model network to integrate\nspeaker information with the acoustic features. The speaker conditioning\nprocess allows the acoustic model to perform computation in the context of\ntarget-speaker auxiliary information. The proposed speaker conditioning method\nis a general approach and can be applied to any acoustic model architecture.\nHere, we employ speaker conditioning on a ResNet acoustic model. Experiments on\nthe WSJ corpus show that the proposed speaker conditioning method is an\neffective solution to fuse speaker auxiliary information with acoustic features\nfor multi-speaker speech recognition, achieving +9% and +20% relative WER\nreduction for clean and overlap speech scenarios, respectively, compared to the\noriginal ResNet acoustic model baseline.", "published": "2021-10-30 19:49:52", "link": "http://arxiv.org/abs/2111.00320v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
