{"title": "T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics", "abstract": "Modern embedding-based metrics for evaluation of generated text generally\nfall into one of two paradigms: discriminative metrics that are trained to\ndirectly predict which outputs are of higher quality according to supervised\nhuman annotations, and generative metrics that are trained to evaluate text\nbased on the probabilities of a generative model. Both have their advantages;\ndiscriminative metrics are able to directly optimize for the problem of\ndistinguishing between good and bad outputs, while generative metrics can be\ntrained using abundant raw text. In this paper, we present a framework that\ncombines the best of both worlds, using both supervised and unsupervised\nsignals from whatever data we have available. We operationalize this idea by\ntraining T5Score, a metric that uses these training signals with mT5 as the\nbackbone. We perform an extensive empirical comparison with other existing\nmetrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility\nof our method. Experimental results show that: T5Score achieves the best\nperformance on all datasets against existing top-scoring metrics at the segment\nlevel. We release our code and models at https://github.com/qinyiwei/T5Score.", "published": "2022-12-12 06:29:04", "link": "http://arxiv.org/abs/2212.05726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Effective Multilingual Fine-Tuning Methods: A Case Study\n  in Summarization", "abstract": "Recently, a large number of tuning strategies have been proposed to adapt\npre-trained language models to downstream tasks. In this paper, we perform an\nextensive empirical evaluation of various tuning strategies for multilingual\nlearning, particularly in the context of text summarization. Specifically, we\nexplore the relative advantages of three families of multilingual tuning\nstrategies (a total of five models) and empirically evaluate them for\nsummarization over 45 languages. Experimentally, we not only established a new\nstate-of-the-art on the XL-Sum dataset but also derive a series of observations\nthat hopefully can provide hints for future research on the design of\nmultilingual tuning strategies.", "published": "2022-12-12 07:37:45", "link": "http://arxiv.org/abs/2212.05740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Natural Language Processing for Programming", "abstract": "Natural language processing for programming aims to use NLP techniques to\nassist programming. It is increasingly prevalent for its effectiveness in\nimproving productivity. Distinct from natural language, a programming language\nis highly structured and functional. Constructing a structure-based\nrepresentation and a functionality-oriented algorithm is at the heart of\nprogram understanding and generation. In this paper, we conduct a systematic\nreview covering tasks, datasets, evaluation methods, techniques, and models\nfrom the perspective of the structure-based and functionality-oriented\nproperty, aiming to understand the role of the two properties in each\ncomponent. Based on the analysis, we illustrate unexplored areas and suggest\npotential directions for future work.", "published": "2022-12-12 08:51:30", "link": "http://arxiv.org/abs/2212.05773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborating Heterogeneous Natural Language Processing Tasks via\n  Federated Learning", "abstract": "The increasing privacy concerns on personal private text data promote the\ndevelopment of federated learning (FL) in recent years. However, the existing\nstudies on applying FL in NLP are not suitable to coordinate participants with\nheterogeneous or private learning objectives. In this study, we further broaden\nthe application scope of FL in NLP by proposing an Assign-Then-Contrast\n(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks\nto construct an FL course and learn useful knowledge from each other.\nSpecifically, the clients are suggested to first perform local training with\nthe unified tasks assigned by the server rather than using their own learning\nobjectives, which is called the Assign training stage. After that, in the\nContrast training stage, clients train with different local learning objectives\nand exchange knowledge with other clients who contribute consistent and useful\nmodel updates. We conduct extensive experiments on six widely-used datasets\ncovering both Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) tasks, and the proposed ATC framework achieves significant\nimprovements compared with various baseline methods. The source code is\navailable at\n\\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.", "published": "2022-12-12 09:27:50", "link": "http://arxiv.org/abs/2212.05789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Transformer: Towards Better Document-to-Document Neural Machine\n  Translation", "abstract": "Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.", "published": "2022-12-12 11:19:05", "link": "http://arxiv.org/abs/2212.05830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"I think this is the most disruptive technology\": Exploring Sentiments\n  of ChatGPT Early Adopters using Twitter Data", "abstract": "Large language models have recently attracted significant attention due to\ntheir impressive performance on a variety of tasks. ChatGPT developed by OpenAI\nis one such implementation of a large, pre-trained language model that has\ngained immense popularity among early adopters, where certain users go to the\nextent of characterizing it as a disruptive technology in many domains.\nUnderstanding such early adopters' sentiments is important because it can\nprovide insights into the potential success or failure of the technology, as\nwell as its strengths and weaknesses. In this paper, we conduct a mixed-method\nstudy using 10,732 tweets from early ChatGPT users. We first use topic\nmodelling to identify the main topics and then perform an in-depth qualitative\nsentiment analysis of each topic. Our results show that the majority of the\nearly adopters have expressed overwhelmingly positive sentiments related to\ntopics such as Disruptions to software development, Entertainment and\nexercising creativity. Only a limited percentage of users expressed concerns\nabout issues such as the potential for misuse of ChatGPT, especially regarding\ntopics such as Impact on educational aspects. We discuss these findings by\nproviding specific examples for each topic and then detail implications related\nto addressing these concerns for both researchers and users.", "published": "2022-12-12 12:41:24", "link": "http://arxiv.org/abs/2212.05856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated ICD Coding using Extreme Multi-label Long Text\n  Transformer-based Models", "abstract": "Background: Encouraged by the success of pretrained Transformer models in\nmany natural language processing tasks, their use for International\nClassification of Diseases (ICD) coding tasks is now actively being explored.\nIn this study, we investigate three types of Transformer-based models, aiming\nto address the extreme label set and long text classification challenges that\nare posed by automated ICD coding tasks. Methods: The Transformer-based model\nPLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD\ncoding benchmark dataset MIMIC-III. It was chosen as our baseline model to be\nfurther optimised. XR-Transformer, the new SOTA model in the general extreme\nmulti-label text classification domain, and XR-LAT, a novel adaptation of the\nXR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a\nrecursively trained model chain on a predefined hierarchical code tree with\nlabel-wise attention, knowledge transferring and dynamic negative sampling\nmechanisms. Results: Our optimised PLM-ICD model, which was trained with longer\ntotal and chunk sequence lengths, significantly outperformed the current SOTA\nPLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The\nXR-Transformer model, although SOTA in the general domain, did not perform well\nacross all metrics. The best XR-LAT based model obtained results that were\ncompetitive with the current SOTA PLM-ICD model, including improving the\nmacro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA\nmodel for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT\nmodel performs competitively with the previous SOTA PLM-ICD model.", "published": "2022-12-12 12:48:33", "link": "http://arxiv.org/abs/2212.05857v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaNLP@SMM4H22: BERT for Classification of Twitter Posts", "abstract": "The reported work is our straightforward approach for the shared task\nClassification of tweets self-reporting age organized by the Social Media\nMining for Health Applications (SMM4H) workshop. This literature describes the\napproach that was used to build a binary classification system, that classifies\nthe tweets related to birthday posts into two classes namely, exact\nage(positive class) and non-exact age(negative class). We made two submissions\nwith variations in the preprocessing of text which yielded F1 scores of 0.80\nand 0.81 when evaluated by the organizers.", "published": "2022-12-12 14:43:46", "link": "http://arxiv.org/abs/2301.05395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensembling Transformers for Cross-domain Automatic Term Extraction", "abstract": "Automatic term extraction plays an essential role in domain language\nunderstanding and several natural language processing downstream tasks. In this\npaper, we propose a comparative study on the predictive power of\nTransformers-based pretrained language models toward term extraction in a\nmulti-language cross-domain setting. Besides evaluating the ability of\nmonolingual models to extract single- and multi-word terms, we also experiment\nwith ensembles of mono- and multilingual models by conducting the intersection\nor union on the term output sets of different language models. Our experiments\nhave been conducted on the ACTER corpus covering four specialized domains\n(Corruption, Wind energy, Equitation, and Heart failure) and three languages\n(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four\nadditional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The\nresults show that the strategy of employing monolingual models outperforms the\nstate-of-the-art approaches from the related work leveraging multilingual\nmodels, regarding all the languages except Dutch and French if the term\nextraction task excludes the extraction of named entity terms. Furthermore, by\ncombining the outputs of the two best performing models, we achieve significant\nimprovements.", "published": "2022-12-12 04:20:11", "link": "http://arxiv.org/abs/2212.05696v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Implementing Deep Learning-Based Approaches for Article Summarization in\n  Indian Languages", "abstract": "The research on text summarization for low-resource Indian languages has been\nlimited due to the availability of relevant datasets. This paper presents a\nsummary of various deep-learning approaches used for the ILSUM 2022 Indic\nlanguage summarization datasets. The ISUM 2022 dataset consists of news\narticles written in Indian English, Hindi, and Gujarati respectively, and their\nground-truth summarizations. In our work, we explore different pre-trained\nseq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,\nthe fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned\nIndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS\nmodel along with a translation mapping-based approach for Gujarati. Our scores\non the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4\nas the evaluation metrics.", "published": "2022-12-12 04:50:43", "link": "http://arxiv.org/abs/2212.05702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Momentum Contrastive Pre-training for Question Answering", "abstract": "Existing pre-training methods for extractive Question Answering (QA) generate\ncloze-like queries different from natural questions in syntax structure, which\ncould overfit pre-trained models to simple keyword matching. In order to\naddress this problem, we propose a novel Momentum Contrastive pRe-training fOr\nqueStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS\nintroduces a momentum contrastive learning framework to align the answer\nprobability between cloze-like and natural query-passage sample pairs. Hence,\nthe pre-trained models can better transfer the knowledge learned in cloze-like\nsamples to answering natural questions. Experimental results on three\nbenchmarking QA datasets show that our method achieves noticeable improvement\ncompared with all baselines in both supervised and zero-shot scenarios.", "published": "2022-12-12 08:28:22", "link": "http://arxiv.org/abs/2212.05762v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation of Transformer-Based Models using Unlabeled Data for\n  Relevance and Polarity Classification of German Customer Feedback", "abstract": "Understanding customer feedback is becoming a necessity for companies to\nidentify problems and improve their products and services. Text classification\nand sentiment analysis can play a major role in analyzing this data by using a\nvariety of machine and deep learning approaches. In this work, different\ntransformer-based models are utilized to explore how efficient these models are\nwhen working with a German customer feedback dataset. In addition, these\npre-trained models are further analyzed to determine if adapting them to a\nspecific domain using unlabeled data can yield better results than\noff-the-shelf pre-trained models. To evaluate the models, two downstream tasks\nfrom the GermEval 2017 are considered. The experimental results show that\ntransformer-based models can reach significant improvements compared to a\nfastText baseline and outperform the published scores and previous models. For\nthe subtask Relevance Classification, the best models achieve a micro-averaged\n$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a\nscore of 85.1 % and 85.3 % for the subtask Polarity Classification.", "published": "2022-12-12 08:32:28", "link": "http://arxiv.org/abs/2212.05764v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information-Theoretic Text Hallucination Reduction for Video-grounded\n  Dialogue", "abstract": "Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question\nregarding a given video and dialogue context. Despite the recent success of\nmulti-modal reasoning to generate answer sentences, existing dialogue systems\nstill suffer from a text hallucination problem, which denotes indiscriminate\ntext-copying from input texts without an understanding of the question. This is\ndue to learning spurious correlations from the fact that answer sentences in\nthe dataset usually include the words of input texts, thus the VGD system\nexcessively relies on copying words from input texts by hoping those words to\noverlap with ground-truth texts. Hence, we design Text Hallucination Mitigating\n(THAM) framework, which incorporates Text Hallucination Regularization (THR)\nloss derived from the proposed information-theoretic text hallucination\nmeasurement approach. Applying THAM with current dialogue systems validates the\neffectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows\nenhanced interpretability.", "published": "2022-12-12 08:38:28", "link": "http://arxiv.org/abs/2212.05765v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph", "abstract": "Answering complex questions over textual resources remains a challenge,\nparticularly when dealing with nuanced relationships between multiple entities\nexpressed within natural-language sentences. To this end, curated knowledge\nbases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used\nand gained great acceptance for question-answering (QA) applications in the\npast decade. While these KBs offer a structured knowledge representation, they\nlack the contextual diversity found in natural-language sources. To address\nthis limitation, BigText-QA introduces an integrated QA approach, which is able\nto answer questions based on a more redundant form of a knowledge graph (KG)\nthat organizes both structured and unstructured (i.e., \"hybrid\") knowledge in a\nunified graphical representation. Thereby, BigText-QA is able to combine the\nbest of both worlds$\\unicode{x2013}$a canonical set of named entities, mapped\nto a structured background KB (such as YAGO or Wikidata), as well as an open\nset of textual clauses providing highly diversified relational paraphrases with\nrich context information. Our experimental results demonstrate that BigText-QA\noutperforms DrQA, a neural-network-based QA system, and achieves competitive\nresults to QUEST, a graph-based unsupervised QA system.", "published": "2022-12-12 09:49:02", "link": "http://arxiv.org/abs/2212.05798v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Generalization of Pre-trained Language Models via Stochastic\n  Weight Averaging", "abstract": "Knowledge Distillation (KD) is a commonly used technique for improving the\ngeneralization of compact Pre-trained Language Models (PLMs) on downstream\ntasks. However, such methods impose the additional burden of training a\nseparate teacher model for every new dataset. Alternatively, one may directly\nwork on the improvement of the optimization procedure of the compact model\ntoward better generalization. Recent works observe that the flatness of the\nlocal minimum correlates well with better generalization. In this work, we\nadapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a\nflatter minimum, to fine-tuning PLMs. We conduct extensive experiments on\nvarious NLP tasks (text classification, question answering, and generation) and\ndifferent model architectures and demonstrate that our adaptation improves the\ngeneralization without extra computation cost. Moreover, we observe that this\nsimple optimization technique is able to outperform the state-of-the-art KD\nmethods for compact models.", "published": "2022-12-12 15:09:56", "link": "http://arxiv.org/abs/2212.05956v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning\n  for Language Understanding", "abstract": "Data augmentation is a widely used technique in machine learning to improve\nmodel performance. However, existing data augmentation techniques in natural\nlanguage understanding (NLU) may not fully capture the complexity of natural\nlanguage variations, and they can be challenging to apply to large datasets.\nThis paper proposes the Random Position Noise (RPN) algorithm, a novel data\naugmentation technique that operates at the word vector level. RPN modifies the\nword embeddings of the original text by introducing noise based on the existing\nvalues of selected word vectors, allowing for more fine-grained modifications\nand better capturing natural language variations. Unlike traditional data\naugmentation methods, RPN does not require gradients in the computational graph\nduring virtual sample updates, making it simpler to apply to large datasets.\nExperimental results demonstrate that RPN consistently outperforms existing\ndata augmentation techniques across various NLU tasks, including sentiment\nanalysis, natural language inference, and paraphrase detection. Moreover, RPN\nperforms well in low-resource settings and is applicable to any model featuring\na word embeddings layer. The proposed RPN algorithm is a promising approach for\nenhancing NLU performance and addressing the challenges associated with\ntraditional data augmentation techniques in large-scale NLU tasks. Our\nexperimental results demonstrated that the RPN algorithm achieved\nstate-of-the-art performance in all seven NLU tasks, thereby highlighting its\neffectiveness and potential for real-world NLU applications.", "published": "2022-12-12 15:16:12", "link": "http://arxiv.org/abs/2212.05961v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Federated Few-Shot Learning for Mobile NLP", "abstract": "Natural language processing (NLP) sees rich mobile applications. To support\nvarious language understanding tasks, a foundation NLP model is often\nfine-tuned in a federated, privacy-preserving setting (FL). This process\ncurrently relies on at least hundreds of thousands of labeled training samples\nfrom mobile clients; yet mobile users often lack willingness or knowledge to\nlabel their data. Such an inadequacy of data labels is known as a few-shot\nscenario; it becomes the key blocker for mobile NLP applications.\n  For the first time, this work investigates federated NLP in the few-shot\nscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and\nprompt learning, we first establish a training pipeline that delivers\ncompetitive accuracy when only 0.05% (fewer than 100) of the training data is\nlabeled and the remaining is unlabeled. To instantiate the workflow, we further\npresent a system FeS, addressing the high execution cost with novel designs.\n(1) Curriculum pacing, which injects pseudo labels to the training workflow at\na rate commensurate to the learning progress; (2) Representational diversity, a\nmechanism for selecting the most learnable data, only for which pseudo labels\nwill be generated; (3) Co-planning of a model's training depth and layer\ncapacity. Together, these designs reduce the training delay, client energy, and\nnetwork traffic by up to 46.0$\\times$, 41.2$\\times$ and 3000.0$\\times$,\nrespectively. Through algorithm/system co-design, FFNLP demonstrates that FL\ncan apply to challenging settings where most training samples are unlabeled.", "published": "2022-12-12 15:29:48", "link": "http://arxiv.org/abs/2212.05974v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Real-World Compositional Generalization with Disentangled\n  Sequence-to-Sequence Learning", "abstract": "Compositional generalization is a basic mechanism in human language learning,\nwhich current neural networks struggle with. A recently proposed Disentangled\nsequence-to-sequence model (Dangle) shows promising generalization capability\nby learning specialized encodings for each decoding step. We introduce two key\nmodifications to this model which encourage more disentangled representations\nand improve its compute and memory efficiency, allowing us to tackle\ncompositional generalization in a more realistic setting. Specifically, instead\nof adaptively re-encoding source keys and values at each time step, we\ndisentangle their representations and only re-encode keys periodically, at some\ninterval. Our new architecture leads to better generalization performance\nacross existing tasks and datasets, and a new machine translation benchmark\nwhich we create by detecting naturally occurring compositional patterns in\nrelation to a training set. We show this methodology better emulates real-world\nrequirements than artificial challenges.", "published": "2022-12-12 15:40:30", "link": "http://arxiv.org/abs/2212.05982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continuation KD: Improved Knowledge Distillation through the Lens of\n  Continuation Optimization", "abstract": "Knowledge Distillation (KD) has been extensively used for natural language\nunderstanding (NLU) tasks to improve a small model's (a student) generalization\nby transferring the knowledge from a larger model (a teacher). Although KD\nmethods achieve state-of-the-art performance in numerous settings, they suffer\nfrom several problems limiting their performance. It is shown in the literature\nthat the capacity gap between the teacher and the student networks can make KD\nineffective. Additionally, existing KD techniques do not mitigate the noise in\nthe teacher's output: modeling the noisy behaviour of the teacher can distract\nthe student from learning more useful features. We propose a new KD method that\naddresses these problems and facilitates the training compared to previous\ntechniques. Inspired by continuation optimization, we design a training\nprocedure that optimizes the highly non-convex KD objective by starting with\nthe smoothed version of this objective and making it more complex as the\ntraining proceeds. Our method (Continuation-KD) achieves state-of-the-art\nperformance across various compact architectures on NLU (GLUE benchmark) and\ncomputer vision tasks (CIFAR-10 and CIFAR-100).", "published": "2022-12-12 16:00:20", "link": "http://arxiv.org/abs/2212.05998v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Effective Seed-Guided Topic Discovery by Integrating Multiple Types of\n  Contexts", "abstract": "Instead of mining coherent topics from a given text corpus in a completely\nunsupervised manner, seed-guided topic discovery methods leverage user-provided\nseed words to extract distinctive and coherent topics so that the mined topics\ncan better cater to the user's interest. To model the semantic correlation\nbetween words and seeds for discovering topic-indicative terms, existing\nseed-guided approaches utilize different types of context signals, such as\ndocument-level word co-occurrences, sliding window-based local contexts, and\ngeneric linguistic knowledge brought by pre-trained language models. In this\nwork, we analyze and show empirically that each type of context information has\nits value and limitation in modeling word semantics under seed guidance, but\ncombining three types of contexts (i.e., word embeddings learned from local\ncontexts, pre-trained language model representations obtained from\ngeneral-domain training, and topic-indicative sentences retrieved based on seed\ninformation) allows them to complement each other for discovering quality\ntopics. We propose an iterative framework, SeedTopicMine, which jointly learns\nfrom the three types of contexts and gradually fuses their context signals via\nan ensemble ranking process. Under various sets of seeds and on multiple\ndatasets, SeedTopicMine consistently yields more coherent and accurate topics\nthan existing seed-guided topic discovery approaches.", "published": "2022-12-12 16:03:38", "link": "http://arxiv.org/abs/2212.06002v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Prompting Is Programming: A Query Language for Large Language Models", "abstract": "Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n  Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks while abstracting language model internals and providing\nhigh-level semantics.\n  To enable LMP, we implement LMQL(short for Language Model Query Language),\nwhich leverages the constraints and control flow from an LMP prompt to generate\nan efficient inference procedure that minimizes the number of expensive calls\nto the underlying language model.\n  We show that LMQL can capture a wide range of state-of-the-art prompting\nmethods in an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (26-85% cost savings).", "published": "2022-12-12 18:09:09", "link": "http://arxiv.org/abs/2212.06094v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In Defense of Cross-Encoders for Zero-Shot Retrieval", "abstract": "Bi-encoders and cross-encoders are widely used in many state-of-the-art\nretrieval pipelines. In this work we study the generalization ability of these\ntwo types of architectures on a wide range of parameter count on both in-domain\nand out-of-domain scenarios. We find that the number of parameters and early\nquery-document interactions of cross-encoders play a significant role in the\ngeneralization ability of retrieval models. Our experiments show that\nincreasing model size results in marginal gains on in-domain test sets, but\nmuch larger gains in new domains never seen during fine-tuning. Furthermore, we\nshow that cross-encoders largely outperform bi-encoders of similar size in\nseveral tasks. In the BEIR benchmark, our largest cross-encoder surpasses a\nstate-of-the-art bi-encoder by more than 4 average points. Finally, we show\nthat using bi-encoders as first-stage retrievers provides no gains in\ncomparison to a simpler retriever such as BM25 on out-of-domain tasks. The code\nis available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git", "published": "2022-12-12 18:50:03", "link": "http://arxiv.org/abs/2212.06121v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Mortality Prediction Models with Clinical Notes Using Sparse Attention\n  at the Word and Sentence Levels", "abstract": "Intensive Care in-hospital mortality prediction has various clinical\napplications. Neural prediction models, especially when capitalising on\nclinical notes, have been put forward as improvement on currently existing\nmodels. However, to be acceptable these models should be performant and\ntransparent. This work studies different attention mechanisms for clinical\nneural prediction models in terms of their discrimination and calibration.\nSpecifically, we investigate sparse attention as an alternative to dense\nattention weights in the task of in-hospital mortality prediction from clinical\nnotes. We evaluate the attention mechanisms based on: i) local self-attention\nover words in a sentence, and ii) global self-attention with a transformer\narchitecture across sentences. We demonstrate that the sparse mechanism\napproach outperforms the dense one for the local self-attention in terms of\npredictive performance with a publicly available dataset, and puts higher\nattention to prespecified relevant directive words. The performance at the\nsentence level, however, deteriorates as sentences including the influential\ndirective words tend to be dropped all together.", "published": "2022-12-12 22:08:45", "link": "http://arxiv.org/abs/2212.06267v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust and Explainable Identification of Logical Fallacies in Natural\n  Language Arguments", "abstract": "The spread of misinformation, propaganda, and flawed argumentation has been\namplified in the Internet era. Given the volume of data and the subtlety of\nidentifying violations of argumentation norms, supporting information analytics\ntasks, like content moderation, with trustworthy methods that can identify\nlogical fallacies is essential. In this paper, we formalize prior theoretical\nwork on logical fallacies into a comprehensive three-stage evaluation framework\nof detection, coarse-grained, and fine-grained classification. We adapt\nexisting evaluation datasets for each stage of the evaluation. We employ three\nfamilies of robust and explainable methods based on prototype reasoning,\ninstance-based reasoning, and knowledge injection. The methods combine language\nmodels with background knowledge and explainable mechanisms. Moreover, we\naddress data sparsity with strategies for data augmentation and curriculum\nlearning. Our three-stage framework natively consolidates prior datasets and\nmethods from existing tasks, like propaganda detection, serving as an\noverarching evaluation testbed. We extensively evaluate these methods on our\ndatasets, focusing on their robustness and explainability. Our results provide\ninsight into the strengths and weaknesses of the methods on different\ncomponents and fallacy classes, indicating that fallacy identification is a\nchallenging task that may require specialized forms of reasoning to capture\nvarious classes. We share our open-source code and data on GitHub to support\nfurther work on logical fallacy identification.", "published": "2022-12-12 20:27:17", "link": "http://arxiv.org/abs/2212.07425v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic,\n  and Multimodal", "abstract": "Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering,\nrecommendation systems, and etc. According to the graph types, existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. Early works in this domain mainly\nfocus on static KGR, and recent works try to leverage the temporal and\nmulti-modal information, which are more practical and closer to real-world.\nHowever, no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a first survey for knowledge graph reasoning tracing from static to\ntemporal and then to multi-modal KGs. Concretely, the models are reviewed based\non bi-level taxonomy, i.e., top-level (graph types) and base-level (techniques\nand scenarios). Besides, the performances, as well as datasets, are summarized\nand presented. Moreover, we point out the challenges and potential\nopportunities to enlighten the readers. The corresponding open-source\nrepository is shared on GitHub\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.", "published": "2022-12-12 08:40:04", "link": "http://arxiv.org/abs/2212.05767v7", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Direct Speech-to-speech Translation without Textual Annotation using\n  Bottleneck Features", "abstract": "Speech-to-speech translation directly translates a speech utterance to\nanother between different languages, and has great potential in tasks such as\nsimultaneous interpretation. State-of-art models usually contains an auxiliary\nmodule for phoneme sequences prediction, and this requires textual annotation\nof the training dataset. We propose a direct speech-to-speech translation model\nwhich can be trained without any textual annotation or content information.\nInstead of introducing an auxiliary phoneme prediction task in the model, we\npropose to use bottleneck features as intermediate training objectives for our\nmodel to ensure the translation performance of the system. Experiments on\nMandarin-Cantonese speech translation demonstrate the feasibility of the\nproposed approach and the performance can match a cascaded system with respect\nof translation and synthesis qualities.", "published": "2022-12-12 10:03:10", "link": "http://arxiv.org/abs/2212.05805v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Text Mining-Based Patent Analysis for Automated Rule Checking in AEC", "abstract": "Automated rule checking (ARC), which is expected to promote the efficiency of\nthe compliance checking process in the architecture, engineering, and\nconstruction (AEC) industry, is gaining increasing attention. Throwing light on\nthe ARC application hotspots and forecasting its trends are useful to the\nrelated research and drive innovations. Therefore, this study takes the patents\nfrom the database of the Derwent Innovations Index database (DII) and China\nnational knowledge infrastructure (CNKI) as data sources and then carried out a\nthree-step analysis including (1) quantitative characteristics (i.e., annual\ndistribution analysis) of patents, (2) identification of ARC topics using a\nlatent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of\nARC topics. The results show that the research hotspots and trends of Chinese\nand English patents are different. The contributions of this study have three\naspects: (1) an approach to a comprehensive analysis of patents by integrating\nmultiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the\napplication hotspots and development trends of ARC are reviewed based on patent\nanalysis; and (3) a signpost for technological development and innovation of\nARC is provided.", "published": "2022-12-12 13:48:38", "link": "http://arxiv.org/abs/2212.05891v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Parameter-Efficient Finetuning of Transformers for Source Code", "abstract": "Pretrained Transformers achieve state-of-the-art performance in various\ncode-processing tasks but may be too large to be deployed. As software\ndevelopment tools often incorporate modules for various purposes which may\npotentially use a single instance of the pretrained model, it appears relevant\nto utilize parameter-efficient fine-tuning for the pretrained models of code.\nIn this work, we test two widely used approaches, adapters and LoRA, which were\ninitially tested on NLP tasks, on four code-processing tasks. We find that\nthough the efficient fine-tuning approaches may achieve comparable or higher\nperformance than the standard, full, fine-tuning in code understanding tasks,\nthey underperform full fine-tuning in code-generative tasks. These results\nunderline the importance of testing efficient fine-tuning approaches on other\ndomains than NLP and motivate future research in efficient fine-tuning for\nsource code.", "published": "2022-12-12 14:00:57", "link": "http://arxiv.org/abs/2212.05901v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Earthquake Impact Analysis Based on Text Mining and Social Media\n  Analytics", "abstract": "Earthquakes have a deep impact on wide areas, and emergency rescue operations\nmay benefit from social media information about the scope and extent of the\ndisaster. Therefore, this work presents a text miningbased approach to collect\nand analyze social media data for early earthquake impact analysis. First,\ndisasterrelated microblogs are collected from the Sina microblog based on\ncrawler technology. Then, after data cleaning a series of analyses are\nconducted including (1) the hot words analysis, (2) the trend of the number of\nmicroblogs, (3) the trend of public opinion sentiment, and (4) a keyword and\nrule-based text classification for earthquake impact analysis. Finally, two\nrecent earthquakes with the same magnitude and focal depth in China are\nanalyzed to compare their impacts. The results show that the public opinion\ntrend analysis and the trend of public opinion sentiment can estimate the\nearthquake's social impact at an early stage, which will be helpful to\ndecision-making and rescue management.", "published": "2022-12-12 13:51:07", "link": "http://arxiv.org/abs/2212.06765v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Synthetic Datasets for Conversational Recommender Systems", "abstract": "For researchers leveraging Large-Language Models (LLMs) in the generation of\ntraining datasets, especially for conversational recommender systems - the\nabsence of robust evaluation frameworks has been a long-standing problem. The\nefficiency brought about by LLMs in the data generation phase is impeded during\nthe process of evaluation of the generated data, since it generally requires\nhuman-raters to ensure that the data generated is of high quality and has\nsufficient diversity. Since the quality of training data is critical for\ndownstream applications, it is important to develop metrics that evaluate the\nquality holistically and identify biases. In this paper, we present a framework\nthat takes a multi-faceted approach towards evaluating datasets produced by\ngenerative models and discuss the advantages and limitations of various\nevaluation methods.", "published": "2022-12-12 18:53:10", "link": "http://arxiv.org/abs/2212.08167v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Page Layout Analysis of Text-heavy Historical Documents: a Comparison of\n  Textual and Visual Approaches", "abstract": "Page layout analysis is a fundamental step in document processing which\nenables to segment a page into regions of interest. With highly complex layouts\nand mixed scripts, scholarly commentaries are text-heavy documents which remain\nchallenging for state-of-the-art models. Their layout considerably varies\nacross editions and their most important regions are mainly defined by semantic\nrather than graphical characteristics such as position or appearance. This\nsetting calls for a comparison between textual, visual and hybrid approaches.\nWe therefore assess the performances of two transformers (LayoutLMv3 and\nRoBERTa) and an objection-detection network (YOLOv5). If results show a clear\nadvantage in favor of the latter, we also list several caveats to this finding.\nIn addition to our experiments, we release a dataset of ca. 300 annotated pages\nsampled from 19th century commentaries.", "published": "2022-12-12 10:10:29", "link": "http://arxiv.org/abs/2212.13924v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "TriNet: stabilizing self-supervised learning from complete or slow\n  collapse on ASR", "abstract": "Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.", "published": "2022-12-12 05:55:07", "link": "http://arxiv.org/abs/2301.00656v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network", "abstract": "The goal of accent conversion (AC) is to convert the accent of speech into\nthe target accent while preserving the content and speaker identity. AC enables\na variety of applications, such as language learning, speech content creation,\nand data augmentation. Previous methods rely on reference utterances in the\ninference phase or are unable to preserve speaker identity. To address these\nissues, we propose a zero-shot reference-free accent conversion method, which\nis able to convert unseen speakers' utterances into a target accent. Pseudo\nSiamese Disentanglement Network (PSDN) is proposed to disentangle the accent\nfrom the content representation. Experimental results show that our model\ngenerates speech samples with much higher accentedness than the input and\ncomparable naturalness, on two-way conversion including foreign-to-native and\nnative-to-foreign.", "published": "2022-12-12 08:02:02", "link": "http://arxiv.org/abs/2212.05751v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
