{"title": "Unsupervised Cross-Lingual Transfer of Structured Predictors without\n  Source Data", "abstract": "Providing technologies to communities or domains where training data is\nscarce or protected e.g., for privacy reasons, is becoming increasingly\nimportant. To that end, we generalise methods for unsupervised transfer from\nmultiple input models for structured prediction. We show that the means of\naggregating over the input models is critical, and that multiplying marginal\nprobabilities of substructures to obtain high-probability structures for\ndistant supervision is substantially better than taking the union of such\nstructures over the input models, as done in prior work. Testing on 18\nlanguages, we demonstrate that the method works in a cross-lingual setting,\nconsidering both dependency parsing and part-of-speech structured prediction\nproblems. Our analyses show that the proposed method produces less noisy labels\nfor the distant supervision.", "published": "2021-10-08 02:46:34", "link": "http://arxiv.org/abs/2110.03866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CheerBots: Chatbots toward Empathy and Emotionusing Reinforcement\n  Learning", "abstract": "Apart from the coherence and fluency of responses, an empathetic chatbot\nemphasizes more on people's feelings. By considering altruistic behaviors\nbetween human interaction, empathetic chatbots enable people to get a better\ninteractive and supportive experience. This study presents a framework whereby\nseveral empathetic chatbots are based on understanding users' implied feelings\nand replying empathetically for multiple dialogue turns. We call these chatbots\nCheerBots. CheerBots can be retrieval-based or generative-based and were\nfinetuned by deep reinforcement learning. To respond in an empathetic way, we\ndevelop a simulating agent, a Conceptual Human Model, as aids for CheerBots in\ntraining with considerations on changes in user's emotional states in the\nfuture to arouse sympathy. Finally, automatic metrics and human rating results\ndemonstrate that CheerBots outperform other baseline chatbots and achieves\nreciprocal altruism. The code and the pre-trained models will be made\navailable.", "published": "2021-10-08 07:44:47", "link": "http://arxiv.org/abs/2110.03949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perceived and Intended Sarcasm Detection with Graph Attention Networks", "abstract": "Existing sarcasm detection systems focus on exploiting linguistic markers,\ncontext, or user-level priors. However, social studies suggest that the\nrelationship between the author and the audience can be equally relevant for\nthe sarcasm usage and interpretation. In this work, we propose a framework\njointly leveraging (1) a user context from their historical tweets together\nwith (2) the social information from a user's conversational neighborhood in an\ninteraction graph, to contextualize the interpretation of the post. We use\ngraph attention networks (GAT) over users and tweets in a conversation thread,\ncombined with dense user history representations. Apart from achieving\nstate-of-the-art results on the recently published dataset of 19k Twitter users\nwith 30K labeled tweets, adding 10M unlabeled tweets as context, our results\nindicate that the model contributes to interpreting the sarcastic intentions of\nan author more than to predicting the sarcasm perception by others.", "published": "2021-10-08 09:52:42", "link": "http://arxiv.org/abs/2110.04001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Do Things without Words: Modeling Semantic Drift of Emoji", "abstract": "Emoji have become a significant part of our informal textual communication.\nPrevious work addressing the societal and linguistic functions of emoji\noverlook the evolving meaning of the symbol. This evolution could be addressed\nthrough the framework of semantic drifts. In this paper we model and analyze\nthe semantic drift of emoji and discuss the features that may be contributing\nto the drift, some are unique to emoji and some are more general.", "published": "2021-10-08 12:45:26", "link": "http://arxiv.org/abs/2110.04093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Do Not Understand What I Cannot Define: Automatic Question Generation\n  With Pedagogically-Driven Content Selection", "abstract": "Most learners fail to develop deep text comprehension when reading textbooks\npassively. Posing questions about what learners have read is a well-established\nway of fostering their text comprehension. However, many textbooks lack\nself-assessment questions because authoring them is timeconsuming and\nexpensive. Automatic question generators may alleviate this scarcity by\ngenerating sound pedagogical questions. However, generating questions\nautomatically poses linguistic and pedagogical challenges. What should we ask?\nAnd, how do we phrase the question automatically? We address those challenges\nwith an automatic question generator grounded in learning theory. The paper\nintroduces a novel pedagogically meaningful content selection mechanism to find\nquestion-worthy sentences and answers in arbitrary textbook contents. We\nconducted an empirical evaluation study with educational experts, annotating\n150 generated questions in six different domains. Results indicate a high\nlinguistic quality of the generated questions. Furthermore, the evaluation\nresults imply that the majority of the generated questions inquire central\ninformation related to the given text and may foster text comprehension in\nspecific learning scenarios.", "published": "2021-10-08 13:29:13", "link": "http://arxiv.org/abs/2110.04123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of an Extractive Title Generation System Using Titles of\n  Papers of Top Conferences for Intermediate English Students", "abstract": "The formulation of good academic paper titles in English is challenging for\nintermediate English authors (particularly students). This is because such\nauthors are not aware of the type of titles that are generally in use. We aim\nto realize a support system for formulating more effective English titles for\nintermediate English and beginner authors. This study develops an extractive\ntitle generation system that formulates titles from keywords extracted from an\nabstract. Moreover, we realize a title evaluation model that can evaluate the\nappropriateness of paper titles. We train the model with titles of\ntop-conference papers by using BERT. This paper describes the training data,\nimplementation, and experimental results. The results show that our evaluation\nmodel can identify top-conference titles more effectively than intermediate\nEnglish and beginner students.", "published": "2021-10-08 15:41:27", "link": "http://arxiv.org/abs/2110.04204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VieSum: How Robust Are Transformer-based Models on Vietnamese\n  Summarization?", "abstract": "Text summarization is a challenging task within natural language processing\nthat involves text generation from lengthy input sequences. While this task has\nbeen widely studied in English, there is very limited research on summarization\nfor Vietnamese text. In this paper, we investigate the robustness of\ntransformer-based encoder-decoder architectures for Vietnamese abstractive\nsummarization. Leveraging transfer learning and self-supervised learning, we\nvalidate the performance of the methods on two Vietnamese datasets.", "published": "2021-10-08 17:10:31", "link": "http://arxiv.org/abs/2110.04257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DPUV3INT8: A Compiler View to programmable FPGA Inference Engines", "abstract": "We have a FPGA design, we make it fast, efficient, and tested for a few\nimportant examples. Now we must infer a general solution to deploy in the data\ncenter. Here, we describe the FPGA DPUV3INT8 design and our compiler effort.\nThe hand-tuned SW-HW solution for Resnet50\\_v1 has (close to) 2 times better\nimages per second (throughput) than our best FPGA implementation; the compiler\ngeneralizes the hand written techniques achieving about 1.5 times better\nperformance for the same example, the compiler generalizes the optimizations to\na model zoo of networks, and it achieves 80+\\% HW efficiency.", "published": "2021-10-08 18:33:12", "link": "http://arxiv.org/abs/2110.04327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Few More Examples May Be Worth Billions of Parameters", "abstract": "We investigate the dynamics of increasing the number of model parameters\nversus the number of labeled examples across a wide variety of tasks. Our\nexploration reveals that while scaling parameters consistently yields\nperformance improvements, the contribution of additional examples highly\ndepends on the task's format. Specifically, in open question answering tasks,\nenlarging the training set does not improve performance. In contrast,\nclassification, extractive question answering, and multiple choice tasks\nbenefit so much from additional examples that collecting a few hundred examples\nis often \"worth\" billions of parameters. We hypothesize that unlike open\nquestion answering, which involves recalling specific information, solving\nstrategies for tasks with a more restricted output space transfer across\nexamples, and can therefore be learned with small amounts of labeled data.", "published": "2021-10-08 20:51:52", "link": "http://arxiv.org/abs/2110.04374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Summarization Systems across Gender, Age, and Race", "abstract": "Summarization systems are ultimately evaluated by human annotators and\nraters. Usually, annotators and raters do not reflect the demographics of end\nusers, but are recruited through student populations or crowdsourcing platforms\nwith skewed demographics. For two different evaluation scenarios -- evaluation\nagainst gold summaries and system output ratings -- we show that summary\nevaluation is sensitive to protected attributes. This can severely bias system\ndevelopment and evaluation, leading us to build models that cater for some\ngroups rather than others.", "published": "2021-10-08 21:30:20", "link": "http://arxiv.org/abs/2110.04384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and\n  Results", "abstract": "In this paper, we introduce the Eval4NLP-2021shared task on explainable\nquality estimation. Given a source-translation pair, this shared task requires\nnot only to provide a sentence-level score indicating the overall quality of\nthe translation, but also to explain this score by identifying the words that\nnegatively impact translation quality. We present the data, annotation\nguidelines and evaluation setup of the shared task, describe the six\nparticipating systems, and analyze the results. To the best of our knowledge,\nthis is the first shared task on explainable NLP evaluation metrics. Datasets\nand results are available at https://github.com/eval4nlp/SharedTask2021.", "published": "2021-10-08 21:57:08", "link": "http://arxiv.org/abs/2110.04392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Explainability of BERT-Based Evaluation Metrics by Disentangling\n  along Linguistic Factors", "abstract": "Evaluation metrics are a key ingredient for progress of text generation\nsystems. In recent years, several BERT-based evaluation metrics have been\nproposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much\nbetter with human assessment of text generation quality than BLEU or ROUGE,\ninvented two decades ago. However, little is known what these metrics, which\nare based on black-box language model representations, actually capture (it is\ntypically assumed they model semantic similarity). In this work, we use a\nsimple regression based global explainability technique to disentangle metric\nscores along linguistic factors, including semantics, syntax, morphology, and\nlexical overlap. We show that the different metrics capture all aspects to some\ndegree, but that they are all substantially sensitive to lexical overlap, just\nlike BLEU and ROUGE. This exposes limitations of these novelly proposed\nmetrics, which we also highlight in an adversarial test scenario.", "published": "2021-10-08 22:40:33", "link": "http://arxiv.org/abs/2110.04399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HydraSum: Disentangling Stylistic Features in Text Summarization using\n  Multi-Decoder Models", "abstract": "Summarization systems make numerous \"decisions\" about summary properties\nduring inference, e.g. degree of copying, specificity and length of outputs,\netc. However, these are implicitly encoded within model parameters and specific\nstyles cannot be enforced. To address this, we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels to a mixture-of-experts version with multiple decoders. We show that\nHydraSum's multiple decoders automatically learn contrasting summary styles\nwhen trained under the standard training objective without any extra\nsupervision. Through experiments on three summarization datasets (CNN, Newsroom\nand XSum), we show that HydraSum provides a simple mechanism to obtain\nstylistically-diverse summaries by sampling from either individual decoders or\ntheir mixtures, outperforming baseline models. Finally, we demonstrate that a\nsmall modification to the gating strategy during training can enforce an even\nstricter style partitioning, e.g. high- vs low-abstractiveness or high- vs\nlow-specificity, allowing users to sample from a larger area in the generation\nspace and vary summary styles along multiple dimensions.", "published": "2021-10-08 22:49:49", "link": "http://arxiv.org/abs/2110.04400v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Input Length Matters: Improving RNN-T and MWER Training for Long-form\n  Telephony Speech Recognition", "abstract": "End-to-end models have achieved state-of-the-art results on several automatic\nspeech recognition tasks. However, they perform poorly when evaluated on\nlong-form data, e.g., minutes long conversational telephony audio. One reason\nthe model fails on long-form speech is that it has only seen short utterances\nduring training. In this paper we study the effect of training utterance length\non the word error rate (WER) for RNN-transducer (RNN-T) model. We compare two\nwidely used training objectives, log loss (or RNN-T loss) and minimum word\nerror rate (MWER) loss. We conduct experiments on telephony datasets in four\nlanguages. Our experiments show that for both losses, the WER on long-form\nspeech reduces substantially as the training utterance length increases. The\naverage relative WER gain is 15.7% for log loss and 8.8% for MWER loss. When\ntraining on short utterances, MWER loss leads to a lower WER than the log loss.\nSuch difference between the two losses diminishes when the input length\nincreases.", "published": "2021-10-08 00:50:46", "link": "http://arxiv.org/abs/2110.03841v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Speeding up Deep Model Training by Sharing Weights and Then Unsharing", "abstract": "We propose a simple and efficient approach for training the BERT model. Our\napproach exploits the special structure of BERT that contains a stack of\nrepeated modules (i.e., transformer encoders). Our proposed approach first\ntrains BERT with the weights shared across all the repeated modules till some\npoint. This is for learning the commonly shared component of weights across all\nrepeated layers. We then stop weight sharing and continue training until\nconvergence. We present theoretic insights for training by sharing weights then\nunsharing with analysis for simplified models. Empirical experiments on the\nBERT model show that our method yields better performance of trained models,\nand significantly reduces the number of training iterations.", "published": "2021-10-08 01:23:34", "link": "http://arxiv.org/abs/2110.03848v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion\n  Parameter Pretraining", "abstract": "Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.", "published": "2021-10-08 04:24:51", "link": "http://arxiv.org/abs/2110.03888v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer\n  Assessments", "abstract": "Peer assessment has been widely applied across diverse academic fields over\nthe last few decades and has demonstrated its effectiveness. However, the\nadvantages of peer assessment can only be achieved with high-quality peer\nreviews. Previous studies have found that high-quality review comments usually\ncomprise several features (e.g., contain suggestions, mention problems, use a\npositive tone). Thus, researchers have attempted to evaluate peer-review\ncomments by detecting different features using various machine learning and\ndeep learning models. However, there is no single study that investigates using\na multi-task learning (MTL) model to detect multiple features simultaneously.\nThis paper presents two MTL models for evaluating peer-review comments by\nleveraging the state-of-the-art pre-trained language representation models BERT\nand DistilBERT. Our results demonstrate that BERT-based models significantly\noutperform previous GloVe-based methods by around 6% in F1-score on tasks of\ndetecting a single feature, and MTL further improves performance while reducing\nmodel size.", "published": "2021-10-08 05:13:41", "link": "http://arxiv.org/abs/2110.03895v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular\n  Subword Units", "abstract": "In end-to-end automatic speech recognition (ASR), a model is expected to\nimplicitly learn representations suitable for recognizing a word-level\nsequence. However, the huge abstraction gap between input acoustic signals and\noutput linguistic tokens makes it challenging for a model to learn the\nrepresentations. In this work, to promote the word-level representation\nlearning in end-to-end ASR, we propose a hierarchical conditional model that is\nbased on connectionist temporal classification (CTC). Our model is trained by\nauxiliary CTC losses applied to intermediate layers, where the vocabulary size\nof each target subword sequence is gradually increased as the layer becomes\nclose to the word-level output. Here, we make each level of sequence prediction\nexplicitly conditioned on the previous sequences predicted at lower levels.\nWith the proposed approach, we expect the proposed model to learn the\nword-level representations effectively by exploiting a hierarchy of linguistic\nstructures. Experimental results on LibriSpeech-{100h, 960h} and TEDLIUM2\ndemonstrate that the proposed model improves over a standard CTC-based model\nand other competitive models from prior work. We further analyze the results to\nconfirm the effectiveness of the intended representation learning with our\nmodel.", "published": "2021-10-08 13:15:58", "link": "http://arxiv.org/abs/2110.04109v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Text analysis and deep learning: A network approach", "abstract": "Much information available to applied researchers is contained within written\nlanguage or spoken text. Deep language models such as BERT have achieved\nunprecedented success in many applications of computational linguistics.\nHowever, much less is known about how these models can be used to analyze\nexisting text. We propose a novel method that combines transformer models with\nnetwork analysis to form a self-referential representation of language use\nwithin a corpus of interest. Our approach produces linguistic relations\nstrongly consistent with the underlying model as well as mathematically\nwell-defined operations on them, while reducing the amount of discretionary\nchoices of representation and distance measures. It represents, to the best of\nour knowledge, the first unsupervised method to extract semantic networks\ndirectly from deep language models. We illustrate our approach in a semantic\nanalysis of the term \"founder\". Using the entire corpus of Harvard Business\nReview from 1980 to 2020, we find that ties in our network track the semantics\nof discourse over time, and across contexts, identifying and relating clusters\nof semantic and syntactic relations. Finally, we discuss how this method can\nalso complement and inform analyses of the behavior of deep learning models.", "published": "2021-10-08 14:18:36", "link": "http://arxiv.org/abs/2110.04151v2", "categories": ["cs.CL", "cs.SI", "I.2.7; I.5.4; J.4"], "primary_category": "cs.CL"}
{"title": "Iterative Decoding for Compositional Generalization in Transformers", "abstract": "Deep learning models generalize well to in-distribution data but struggle to\ngeneralize compositionally, i.e., to combine a set of learned primitives to\nsolve more complex tasks. In sequence-to-sequence (seq2seq) learning,\ntransformers are often unable to predict correct outputs for longer examples\nthan those seen at training. This paper introduces iterative decoding, an\nalternative to seq2seq that (i) improves transformer compositional\ngeneralization in the PCFG and Cartesian product datasets and (ii) evidences\nthat, in these datasets, seq2seq transformers do not learn iterations that are\nnot unrolled. In iterative decoding, training examples are broken down into a\nsequence of intermediate steps that the transformer learns iteratively. At\ninference time, the intermediate outputs are fed back to the transformer as\nintermediate inputs until an end-of-iteration token is predicted. We conclude\nby illustrating some limitations of iterative decoding in the CFQ dataset.", "published": "2021-10-08 14:52:25", "link": "http://arxiv.org/abs/2110.04169v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Contrastive String Representation Learning using Synthetic Data", "abstract": "String representation Learning (SRL) is an important task in the field of\nNatural Language Processing, but it remains under-explored. The goal of SRL is\nto learn dense and low-dimensional vectors (or embeddings) for encoding\ncharacter sequences. The learned representation from this task can be used in\nmany downstream application tasks such as string similarity matching or lexical\nnormalization. In this paper, we propose a new method for to train a SRL model\nby only using synthetic data. Our approach makes use of Contrastive Learning in\norder to maximize similarity between related strings while minimizing it for\nunrelated strings. We demonstrate the effectiveness of our approach by\nevaluating the learned representation on the task of string similarity\nmatching. Codes, data and pretrained models will be made publicly available.", "published": "2021-10-08 16:06:54", "link": "http://arxiv.org/abs/2110.04217v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Taming Sparsely Activated Transformer with Stochastic Experts", "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at:\nhttps://github.com/microsoft/Stochastic-Mixture-of-Experts.", "published": "2021-10-08 17:15:47", "link": "http://arxiv.org/abs/2110.04260v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain\n  Question Answering", "abstract": "Current Open-Domain Question Answering (ODQA) model paradigm often contains a\nretrieving module and a reading module. Given an input question, the reading\nmodule predicts the answer from the relevant passages which are retrieved by\nthe retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on\ntop of the pretrained generative model T5, achieves the state-of-the-art\nperformance in the reading module. Although being effective, it remains\nconstrained by inefficient attention on all retrieved passages which contain a\nlot of noise. In this work, we propose a novel method KG-FiD, which filters\nnoisy passages by leveraging the structural relationship among the retrieved\npassages with a knowledge graph. We initiate the passage node embedding from\nthe FiD encoder and then use graph neural network (GNN) to update the\nrepresentation for reranking. To improve the efficiency, we build the GNN on\ntop of the intermediate layer output of the FiD encoder and only pass a few top\nreranked passages into the higher layers of encoder and decoder for answer\ngeneration. We also apply the proposed GNN based reranking method to enhance\nthe passage retrieval results in the retrieving module. Extensive experiments\non common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate\nthat KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score\nand achieve comparable performance with FiD with only 40% of computation cost.", "published": "2021-10-08 18:39:59", "link": "http://arxiv.org/abs/2110.04330v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Describe Solutions for Bug Reports Based on Developer\n  Discussions", "abstract": "When a software bug is reported, developers engage in a discussion to\ncollaboratively resolve it. While the solution is likely formulated within the\ndiscussion, it is often buried in a large amount of text, making it difficult\nto comprehend and delaying its implementation. To expedite bug resolution, we\npropose generating a concise natural language description of the solution by\nsynthesizing relevant content within the discussion, which encompasses both\nnatural language and source code. We build a corpus for this task using a novel\ntechnique for obtaining noisy supervision from repository changes linked to bug\nreports, with which we establish benchmarks. We also design two systems for\ngenerating a description during an ongoing discussion by classifying when\nsufficient context for performing the task emerges in real-time. With automated\nand human evaluation, we find this task to form an ideal testbed for complex\nreasoning in long, bimodal dialogue context.", "published": "2021-10-08 19:39:55", "link": "http://arxiv.org/abs/2110.04353v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.", "published": "2021-10-08 20:22:26", "link": "http://arxiv.org/abs/2110.04366v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accessible Visualization via Natural Language Descriptions: A Four-Level\n  Model of Semantic Content", "abstract": "Natural language descriptions sometimes accompany visualizations to better\ncommunicate and contextualize their insights, and to improve their\naccessibility for readers with disabilities. However, it is difficult to\nevaluate the usefulness of these descriptions, and how effectively they improve\naccess to meaningful information, because we have little understanding of the\nsemantic content they convey, and how different readers receive this content.\nIn response, we introduce a conceptual model for the semantic content conveyed\nby natural language descriptions of visualizations. Developed through a\ngrounded theory analysis of 2,147 sentences, our model spans four levels of\nsemantic content: enumerating visualization construction properties (e.g.,\nmarks and encodings); reporting statistical concepts and relations (e.g.,\nextrema and correlations); identifying perceptual and cognitive phenomena\n(e.g., complex trends and patterns); and elucidating domain-specific insights\n(e.g., social and political context). To demonstrate how our model can be\napplied to evaluate the effectiveness of visualization descriptions, we conduct\na mixed-methods evaluation with 30 blind and 90 sighted readers, and find that\nthese reader groups differ significantly on which semantic content they rank as\nmost useful. Together, our model and findings suggest that access to meaningful\ninformation is strongly reader-specific, and that research in automatic\nvisualization captioning should orient toward descriptions that more richly\ncommunicate overall trends and statistics, sensitive to reader preferences. Our\nwork further opens a space of research on natural language as a data interface\ncoequal with visualization.", "published": "2021-10-08 23:37:25", "link": "http://arxiv.org/abs/2110.04406v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A guided journey through non-interactive automatic story generation", "abstract": "We present a literature survey on non-interactive computational story\ngeneration. The article starts with the presentation of requirements for\ncreative systems, three types of models of creativity (computational,\nsocio-cultural, and individual), and models of human creative writing. Then it\nreviews each class of story generation approach depending on the used\ntechnology: story-schemas, analogy, rules, planning, evolutionary algorithms,\nimplicit knowledge learning, and explicit knowledge learning. Before the\nconcluding section, the article analyses the contributions of the reviewed work\nto improve the quality of the generated stories. This analysis addresses the\ndescription of the story characters, the use of narrative knowledge including\nabout character believability, and the possible lack of more comprehensive or\nmore detailed knowledge or creativity models. Finally, the article presents\nconcluding remarks in the form of suggestions of research topics that might\nhave a significant impact on the advancement of the state of the art on\nautonomous non-interactive story generation systems. The article concludes that\nthe autonomous generation and adoption of the main idea to be conveyed and the\nautonomous design of the creativity ensuring criteria are possibly two of most\nimportant topics for future research.", "published": "2021-10-08 10:01:36", "link": "http://arxiv.org/abs/2110.11167v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Concept Map Generation through Task-Guided Graph\n  Translation", "abstract": "Recent years have witnessed the rapid development of concept map generation\ntechniques due to their advantages in providing well-structured summarization\nof knowledge from free texts. Traditional unsupervised methods do not generate\ntask-oriented concept maps, whereas deep generative models require large\namounts of training data. In this work, we present GT-D2G (Graph\nTranslation-based Document To Graph), an automatic concept map generation\nframework that leverages generalized NLP pipelines to derive semantic-rich\ninitial graphs, and translates them into more concise structures under the weak\nsupervision of downstream task labels. The concept maps generated by GT-D2G can\nprovide interpretable summarization of structured knowledge for the input\ntexts, which are demonstrated through human evaluation and case studies on\nthree real-world corpora. Further experiments on the downstream task of\ndocument classification show that GT-D2G beats other concept map generation\nmethods. Moreover, we specifically validate the labeling efficiency of GT-D2G\nin the label-efficient learning setting and the flexibility of generated graph\nsizes in controlled hyper-parameter studies.", "published": "2021-10-08 20:17:10", "link": "http://arxiv.org/abs/2110.15720v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis and Topic Modeling for COVID-19 Vaccine Discussions", "abstract": "The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has lasted for\nnearly two years and caused unprecedented impacts on people's daily life around\nthe world. Even worse, the emergence of the COVID-19 Delta variant once again\nputs the world in danger. Fortunately, many countries and companies have\nstarted to develop coronavirus vaccines since the beginning of this disaster.\nTill now, more than 20 vaccines have been approved by the World Health\nOrganization (WHO), bringing light to people besieged by the pandemic. The\npromotion of COVID-19 vaccination around the world also brings a lot of\ndiscussions on social media about different aspects of vaccines, such as\nefficacy and security. However, there does not exist much research work to\nsystematically analyze public opinion towards COVID-19 vaccines. In this study,\nwe conduct an in-depth analysis of tweets related to the coronavirus vaccine on\nTwitter to understand the trending topics and their corresponding sentimental\npolarities regarding the country and vaccine levels. The results show that a\nmajority of people are confident in the effectiveness of vaccines and are\nwilling to get vaccinated. In contrast, the negative tweets are often\nassociated with the complaints of vaccine shortages, side effects after\ninjections and possible death after being vaccinated. Overall, this study\nexploits popular NLP and topic modeling methods to mine people's opinions on\nthe COVID-19 vaccines on social media and to analyse and visualise them\nobjectively. Our findings can improve the readability of the noisy information\non social media and provide effective data support for the government and\npolicy makers.", "published": "2021-10-08 23:30:17", "link": "http://arxiv.org/abs/2111.04415v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Machine Translation Verbosity Control for Automatic Dubbing", "abstract": "Automatic dubbing aims at seamlessly replacing the speech in a video document\nwith synthetic speech in a different language. The task implies many\nchallenges, one of which is generating translations that not only convey the\noriginal content, but also match the duration of the corresponding utterances.\nIn this paper, we focus on the problem of controlling the verbosity of machine\ntranslation output, so that subsequent steps of our automatic dubbing pipeline\ncan generate dubs of better quality. We propose new methods to control the\nverbosity of MT output and compare them against the state of the art with both\nintrinsic and extrinsic evaluations. For our experiments we use a public data\nset to dub English speeches into French, Italian, German and Spanish. Finally,\nwe report extensive subjective tests that measure the impact of MT verbosity\ncontrol on the final quality of dubbed video clips.", "published": "2021-10-08 01:19:10", "link": "http://arxiv.org/abs/2110.03847v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A study on the efficacy of model pre-training in developing neural\n  text-to-speech system", "abstract": "In the development of neural text-to-speech systems, model pre-training with\na large amount of non-target speakers' data is a common approach. However, in\nterms of ultimately achieved system performance for target speaker(s), the\nactual benefits of model pre-training are uncertain and unstable, depending\nvery much on the quantity and text content of training data. This study aims to\nunderstand better why and how model pre-training can positively contribute to\nTTS system performance. It is postulated that the pre-training process plays a\ncritical role in learning text-related variation in speech, while further\ntraining with the target speaker's data aims to capture the speaker-related\nvariation. Different test sets are created with varying degrees of similarity\nto target speaker data in terms of text content. Experiments show that\nleveraging a speaker-independent TTS trained on speech data with diverse text\ncontent can improve the target speaker TTS on domain-mismatched text. We also\nattempt to reduce the amount of pre-training data for a new text domain and\nimprove the data and computational efficiency. It is found that the TTS system\ncould achieve comparable performance when the pre-training data is reduced to\n1/8 of its original size.", "published": "2021-10-08 02:09:28", "link": "http://arxiv.org/abs/2110.03857v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Representation of professions in entertainment media: Insights into\n  frequency and sentiment trends through computational text analysis", "abstract": "Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.", "published": "2021-10-08 03:05:08", "link": "http://arxiv.org/abs/2110.03873v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phone-to-audio alignment without text: A Semi-supervised Approach", "abstract": "The task of phone-to-audio alignment has many applications in speech\nresearch. Here we introduce two Wav2Vec2-based models for both text-dependent\nand text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a\nsemi-supervised model, directly learns phone-to-audio alignment through\ncontrastive learning and a forward sum loss, and can be coupled with a\npretrained phone recognizer to achieve text-independent alignment. The other\nmodel, Wav2Vec2-FC, is a frame classification model trained on forced aligned\nlabels that can both perform forced alignment and text-independent\nsegmentation. Evaluation results suggest that both proposed methods, even when\ntranscriptions are not available, generate highly close results to existing\nforced alignment tools. Our work presents a neural pipeline of fully automated\nphone-to-audio alignment. Code and pretrained models are available at\nhttps://github.com/lingjzhu/charsiu.", "published": "2021-10-08 03:30:24", "link": "http://arxiv.org/abs/2110.03876v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explaining the Attention Mechanism of End-to-End Speech Recognition\n  Using Decision Trees", "abstract": "The attention mechanism has largely improved the performance of end-to-end\nspeech recognition systems. However, the underlying behaviours of attention is\nnot yet clearer. In this study, we use decision trees to explain how the\nattention mechanism impact itself in speech recognition. The results indicate\nthat attention levels are largely impacted by their previous states rather than\nthe encoder and decoder patterns. Additionally, the default attention mechanism\nseems to put more weights on closer states, but behaves poorly on modelling\nlong-term dependencies of attention states.", "published": "2021-10-08 03:36:09", "link": "http://arxiv.org/abs/2110.03879v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Math-Aware Automated Classification and Similarity Search of\n  Scientific Publications: Methods of Mathematical Content Representations", "abstract": "In this paper, we investigate mathematical content representations suitable\nfor the automated classification of and the similarity search in STEM documents\nusing standard machine learning algorithms: the Latent Dirichlet Allocation\n(LDA) and the Latent Semantic Indexing (LSI). The methods are evaluated on a\nsubset of arXiv.org papers with the Mathematics Subject Classification (MSC) as\na reference classification and using the standard precision/recall/F1-measure\nmetrics. The results give insight into how different math representations may\ninfluence the performance of the classification and similarity search tasks in\nSTEM repositories. Non-surprisingly, machine learning methods are able to grab\ndistributional semantics from textual tokens. A proper selection of weighted\ntokens representing math may improve the quality of the results slightly. A\nstructured math representation that imitates successful text-processing\ntechniques with math is shown to yield better results than flat TeX tokens.", "published": "2021-10-08 11:27:40", "link": "http://arxiv.org/abs/2110.04040v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "97E40 (Primary) 00Axx, 68T50, 97-XX (Secondary)", "H.3; H.4; I.2; I.7; I.1"], "primary_category": "cs.IR"}
{"title": "lambeq: An Efficient High-Level Python Library for Quantum NLP", "abstract": "We present lambeq, the first high-level Python library for Quantum Natural\nLanguage Processing (QNLP). The open-source toolkit offers a detailed hierarchy\nof modules and classes implementing all stages of a pipeline for converting\nsentences to string diagrams, tensor networks, and quantum circuits ready to be\nused on a quantum computer. lambeq supports syntactic parsing, rewriting and\nsimplification of string diagrams, ansatz creation and manipulation, as well as\na number of compositional models for preparing quantum-friendly representations\nof sentences, employing various degrees of syntax sensitivity. We present the\ngeneric architecture and describe the most important modules in detail,\ndemonstrating the usage with illustrative examples. Further, we test the\ntoolkit in practice by using it to perform a number of experiments on simple\nNLP tasks, implementing both classical and quantum pipelines.", "published": "2021-10-08 16:40:56", "link": "http://arxiv.org/abs/2110.04236v1", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Exploring Heterogeneous Characteristics of Layers in ASR Models for More\n  Efficient Training", "abstract": "Transformer-based architectures have been the subject of research aimed at\nunderstanding their overparameterization and the non-uniform importance of\ntheir layers. Applying these approaches to Automatic Speech Recognition, we\ndemonstrate that the state-of-the-art Conformer models generally have multiple\nambient layers. We study the stability of these layers across runs and model\nsizes, propose that group normalization may be used without disrupting their\nformation, and examine their correlation with model weight updates in each\nlayer. Finally, we apply these findings to Federated Learning in order to\nimprove the training procedure, by targeting Federated Dropout to layers by\nimportance. This allows us to reduce the model size optimized by clients\nwithout quality degradation, and shows potential for future exploration.", "published": "2021-10-08 17:25:19", "link": "http://arxiv.org/abs/2110.04267v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Local and Global Context-Based Pairwise Models for Sentence Ordering", "abstract": "Sentence Ordering refers to the task of rearranging a set of sentences into\nthe appropriate coherent order. For this task, most previous approaches have\nexplored global context-based end-to-end methods using Sequence Generation\ntechniques. In this paper, we put forward a set of robust local and global\ncontext-based pairwise ordering strategies, leveraging which our prediction\nstrategies outperform all previous works in this domain. Our proposed encoding\nmethod utilizes the paragraph's rich global contextual information to predict\nthe pairwise order using novel transformer architectures. Analysis of the two\nproposed decoding strategies helps better explain error propagation in pairwise\nmodels. This approach is the most accurate pure pairwise model and our encoding\nstrategy also significantly improves the performance of other recent approaches\nthat use pairwise models, including the previous state-of-the-art,\ndemonstrating the research novelty and generalizability of this work.\nAdditionally, we show how the pre-training task for ALBERT helps it to\nsignificantly outperform BERT, despite having considerably lesser parameters.\nThe extensive experimental results, architectural analysis and ablation studies\ndemonstrate the effectiveness and superiority of the proposed models compared\nto the previous state-of-the-art, besides providing a much better understanding\nof the functioning of pairwise models.", "published": "2021-10-08 17:57:59", "link": "http://arxiv.org/abs/2110.04291v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.LO", "I.2.7; H.3.3; H.3.1"], "primary_category": "cs.CL"}
{"title": "Self-supervised Speaker Recognition with Loss-gated Learning", "abstract": "In self-supervised learning for speaker recognition, pseudo labels are useful\nas the supervision signals. It is a known fact that a speaker recognition model\ndoesn't always benefit from pseudo labels due to their unreliability. In this\nwork, we observe that a speaker recognition network tends to model the data\nwith reliable labels faster than those with unreliable labels. This motivates\nus to study a loss-gated learning (LGL) strategy, which extracts the reliable\nlabels through the fitting ability of the neural network during training. With\nthe proposed LGL, our speaker recognition model obtains a $46.3\\%$ performance\ngain over the system without it. Further, the proposed self-supervised speaker\nrecognition with LGL trained on the VoxCeleb2 dataset without any labels\nachieves an equal error rate of $1.66\\%$ on the VoxCeleb1 original test set.\nCode has been made available at:\nhttps://github.com/TaoRuijie/Loss-Gated-Learning.", "published": "2021-10-08 02:57:19", "link": "http://arxiv.org/abs/2110.03869v3", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Environment Aware Text-to-Speech Synthesis", "abstract": "This study aims at designing an environment-aware text-to-speech (TTS) system\nthat can generate speech to suit specific acoustic environments. It is also\nmotivated by the desire to leverage massive data of speech audio from\nheterogeneous sources in TTS system development. The key idea is to model the\nacoustic environment in speech audio as a factor of data variability and\nincorporate it as a condition in the process of neural network based speech\nsynthesis. Two embedding extractors are trained with two purposely constructed\ndatasets for characterization and disentanglement of speaker and environment\nfactors in speech. A neural network model is trained to generate speech from\nextracted speaker and environment embeddings. Objective and subjective\nevaluation results demonstrate that the proposed TTS system is able to\neffectively disentangle speaker and environment factors and synthesize speech\naudio that carries designated speaker characteristics and environment\nattribute. Audio samples are available online for demonstration\nhttps://daxintan-cuhk.github.io/Environment-Aware-TTS/ .", "published": "2021-10-08 04:19:19", "link": "http://arxiv.org/abs/2110.03887v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Scattering for Automatic Chick Call Recognition", "abstract": "Animal vocalisations contain important information about health, emotional\nstate, and behaviour, thus can be potentially used for animal welfare\nmonitoring. Motivated by the spectro-temporal patterns of chick calls in the\ntime$-$frequency domain, in this paper we propose an automatic system for chick\ncall recognition using the joint time$-$frequency scattering transform (JTFS).\nTaking full-length recordings as input, the system first extracts chick call\ncandidates by an onset detector and silence removal. After computing their JTFS\nfeatures, a support vector machine classifier groups each candidate into\ndifferent chick call types. Evaluating on a dataset comprising 3013 chick calls\ncollected in laboratory conditions, the proposed recognition system using the\nJTFS features improves the frame- and event-based macro F-measures by 9.5% and\n11.7%, respectively, than that of a mel-frequency cepstral coefficients\nbaseline.", "published": "2021-10-08 08:31:17", "link": "http://arxiv.org/abs/2110.03965v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-speaker Emotion Transfer Based on Speaker Condition Layer\n  Normalization and Semi-Supervised Training in Text-To-Speech", "abstract": "In expressive speech synthesis, there are high requirements for emotion\ninterpretation. However, it is time-consuming to acquire emotional audio corpus\nfor arbitrary speakers due to their deduction ability. In response to this\nproblem, this paper proposes a cross-speaker emotion transfer method that can\nrealize the transfer of emotions from source speaker to target speaker. A set\nof emotion tokens is firstly defined to represent various categories of\nemotions. They are trained to be highly correlated with corresponding emotions\nfor controllable synthesis by cross-entropy loss and semi-supervised training\nstrategy. Meanwhile, to eliminate the down-gradation to the timbre similarity\nfrom cross-speaker emotion transfer, speaker condition layer normalization is\nimplemented to model speaker characteristics. Experimental results show that\nthe proposed method outperforms the multi-reference based baseline in terms of\ntimbre similarity, stability and emotion perceive evaluations.", "published": "2021-10-08 14:19:59", "link": "http://arxiv.org/abs/2110.04153v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cognitive Coding of Speech", "abstract": "We propose an approach for cognitive coding of speech by unsupervised\nextraction of contextual representations in two hierarchical levels of\nabstraction. Speech attributes such as phoneme identity that last one hundred\nmilliseconds or less are captured in the lower level of abstraction, while\nspeech attributes such as speaker identity and emotion that persist up to one\nsecond are captured in the higher level of abstraction. This decomposition is\nachieved by a two-stage neural network, with a lower and an upper stage\noperating at different time scales. Both stages are trained to predict the\ncontent of the signal in their respective latent spaces. A top-down pathway\nbetween stages further improves the predictive capability of the network. With\nan application in speech compression in mind, we investigate the effect of\ndimensionality reduction and low bitrate quantization on the extracted\nrepresentations. The performance measured on the LibriSpeech and EmoV-DB\ndatasets reaches, and for some speech attributes even exceeds, that of\nstate-of-the-art approaches.", "published": "2021-10-08 16:49:16", "link": "http://arxiv.org/abs/2110.04241v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A study of the robustness of raw waveform based speaker embeddings under\n  mismatched conditions", "abstract": "In this paper, we conduct a cross-dataset study on parametric and\nnon-parametric raw-waveform based speaker embeddings through speaker\nverification experiments. In general, we observe a more significant performance\ndegradation of these raw-waveform systems compared to spectral based systems.\nWe then propose two strategies to improve the performance of raw-waveform based\nsystems on cross-dataset tests. The first strategy is to change the real-valued\nfilters into analytic filters to ensure shift-invariance. The second strategy\nis to apply variational dropout to non-parametric filters to prevent them from\noverfitting irrelevant nuance features.", "published": "2021-10-08 17:21:21", "link": "http://arxiv.org/abs/2110.04265v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Auto-DSP: Learning to Optimize Acoustic Echo Cancellers", "abstract": "Adaptive filtering algorithms are commonplace in signal processing and have\nwide-ranging applications from single-channel denoising to multi-channel\nacoustic echo cancellation and adaptive beamforming. Such algorithms typically\noperate via specialized online, iterative optimization methods and have\nachieved tremendous success, but require expert knowledge, are slow to develop,\nand are difficult to customize. In our work, we present a new method to\nautomatically learn adaptive filtering update rules directly from data. To do\nso, we frame adaptive filtering as a differentiable operator and train a\nlearned optimizer to output a gradient descent-based update rule from data via\nbackpropagation through time. We demonstrate our general approach on an\nacoustic echo cancellation task (single-talk with noise) and show that we can\nlearn high-performing adaptive filters for a variety of common linear and\nnon-linear multidelayed block frequency domain filter architectures. We also\nfind that our learned update rules exhibit fast convergence, can optimize in\nthe presence of nonlinearities, and are robust to acoustic scene changes\ndespite never encountering any during training.", "published": "2021-10-08 17:52:24", "link": "http://arxiv.org/abs/2110.04284v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Location-based training for multi-channel talker-independent speaker\n  separation", "abstract": "Permutation-invariant training (PIT) is a dominant approach for addressing\nthe permutation ambiguity problem in talker-independent speaker separation.\nLeveraging spatial information afforded by microphone arrays, we propose a new\ntraining approach to resolving permutation ambiguities for multi-channel\nspeaker separation. The proposed approach, named location-based training (LBT),\nassigns speakers on the basis of their spatial locations. This training\nstrategy is easy to apply, and organizes speakers according to their positions\nin physical space. Specifically, this study investigates azimuth angles and\nsource distances for location-based training. Evaluation results on separating\ntwo- and three-speaker mixtures show that azimuth-based training consistently\noutperforms PIT, and distance-based training further improves the separation\nperformance when speaker azimuths are close. Furthermore, we dynamically select\nazimuth-based or distance-based training by estimating the azimuths of\nseparated speakers, which further improves separation performance. LBT has a\nlinear training complexity with respect to the number of speakers, as opposed\nto the factorial complexity of PIT. We further demonstrate the effectiveness of\nLBT for the separation of four and five concurrent speakers.", "published": "2021-10-08 17:56:39", "link": "http://arxiv.org/abs/2110.04289v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusicNet: Compact Convolutional Neural Network for Real-time Background\n  Music Detection", "abstract": "With the recent growth of remote work, online meetings often encounter\nchallenging audio contexts such as background noise, music, and echo. Accurate\nreal-time detection of music events can help to improve the user experience. In\nthis paper, we present MusicNet, a compact neural model for detecting\nbackground music in the real-time communications pipeline. In video meetings,\nmusic frequently co-occurs with speech and background noises, making the\naccurate classification quite challenging. We propose a compact convolutional\nneural network core preceded by an in-model featurization layer. MusicNet takes\n9 seconds of raw audio as input and does not require any model-specific\nfeaturization in the product stack. We train our model on the balanced subset\nof the Audio Set~\\cite{gemmeke2017audio} data and validate it on 1000\ncrowd-sourced real test clips. Finally, we compare MusicNet performance with 20\nstate-of-the-art models. MusicNet has a true positive rate (TPR) of 81.3% at a\n0.1% false positive rate (FPR), which is significantly better than\nstate-of-the-art models included in our study. MusicNet is also 10x smaller and\nhas 4x faster inference than the best performing models we benchmarked.", "published": "2021-10-08 18:42:58", "link": "http://arxiv.org/abs/2110.04331v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Individualized Hear-through For Acoustic Transparency Using PCA-Based\n  Sound Pressure Estimation At The Eardrum", "abstract": "The hear-through functionality on hearing devices, which allows hearing\nequivalent to the open-ear while providing the possibility to modify the sound\npressure at the eardrum in a desired manner, has drawn great attention from\nresearchers in recent years. To this end, the output of the device is processed\nby means of an equalization filter, such that the transfer function between\nexternal sound sources and the eardrum is equivalent for the open-ear and the\naided condition with the device in the ear. To achieve an ideal performance,\nthe equalization filter design assumes the exact knowledge of all the relevant\nacoustic transfer functions. A particular challenge is the transfer function\nbetween the hearing device receiver and the eardrum, which is difficult to\nobtain in practice as it requires additional probe-tube measurements. In this\nwork, we address this issue by proposing an individualized hear-through\nequalization filter design that leverages the measurement of the so-called\nsecondary path to predict the sound pressure at the eardrum. Experimental\nresults using real-ear measured transfer functions confirm that the proposed\nmethod achieves a good sound quality compared to the open-ear while\noutperforming filter designs that do not leverage the proposed estimator.", "published": "2021-10-08 21:35:28", "link": "http://arxiv.org/abs/2110.04385v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TitaNet: Neural Model for speaker representation with 1D Depth-wise\n  separable convolutions and global context", "abstract": "In this paper, we propose TitaNet, a novel neural network architecture for\nextracting speaker representations. We employ 1D depth-wise separable\nconvolutions with Squeeze-and-Excitation (SE) layers with global context\nfollowed by channel attention based statistics pooling layer to map\nvariable-length utterances to a fixed-length embedding (t-vector). TitaNet is a\nscalable architecture and achieves state-of-the-art performance on speaker\nverification task with an equal error rate (EER) of 0.68% on the VoxCeleb1\ntrial file and also on speaker diarization tasks with diarization error rate\n(DER) of 1.73% on AMI-MixHeadset, 1.99% on AMI-Lapel and 1.11% on CH109.\nFurthermore, we investigate various sizes of TitaNet and present a light\nTitaNet-S model with only 6M parameters that achieve near state-of-the-art\nresults in diarization tasks.", "published": "2021-10-08 23:49:42", "link": "http://arxiv.org/abs/2110.04410v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE using\n  Mel-spectrograms", "abstract": "In this paper, we propose a novel neural network model called KaraSinger for\na less-studied singing voice synthesis (SVS) task named score-free SVS, in\nwhich the prosody and melody are spontaneously decided by machine. KaraSinger\ncomprises a vector-quantized variational autoencoder (VQ-VAE) that compresses\nthe Mel-spectrograms of singing audio to sequences of discrete codes, and a\nlanguage model (LM) that learns to predict the discrete codes given the\ncorresponding lyrics. For the VQ-VAE part, we employ a Connectionist Temporal\nClassification (CTC) loss to encourage the discrete codes to carry\nphoneme-related information. For the LM part, we use location-sensitive\nattention for learning a robust alignment between the input phoneme sequence\nand the output discrete code. We keep the architecture of both the VQ-VAE and\nLM light-weight for fast training and inference speed. We validate the\neffectiveness of the proposed design choices using a proprietary collection of\n550 English pop songs sung by multiple amateur singers. The result of a\nlistening test shows that KaraSinger achieves high scores in intelligibility,\nmusicality, and the overall quality.", "published": "2021-10-08 10:00:23", "link": "http://arxiv.org/abs/2110.04005v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant\n  Sound Source Separation", "abstract": "In recent years, many deep learning techniques for single-channel sound\nsource separation have been proposed using recurrent, convolutional and\ntransformer networks. When multiple microphones are available, spatial\ndiversity between speakers and background noise in addition to spectro-temporal\ndiversity can be exploited by using multi-channel filters for sound source\nseparation. Aiming at end-to-end multi-channel source separation, in this paper\nwe propose a transformer-recurrent-U network (TRUNet), which directly estimates\nmulti-channel filters from multi-channel input spectra. TRUNet consists of a\nspatial processing network with an attention mechanism across microphone\nchannels aiming at capturing the spatial diversity, and a spectro-temporal\nprocessing network aiming at capturing spectral and temporal diversities. In\naddition to multi-channel filters, we also consider estimating single-channel\nfilters from multi-channel input spectra using TRUNet. We train the network on\na large reverberant dataset using a combined compressed mean-squared error loss\nfunction, which further improves the sound separation performance. We evaluate\nthe network on a realistic and challenging reverberant dataset, generated from\nmeasured room impulse responses of an actual microphone array. The experimental\nresults on realistic reverberant sound source separation show that the proposed\nTRUNet outperforms state-of-the-art single-channel and multi-channel source\nseparation methods.", "published": "2021-10-08 11:45:56", "link": "http://arxiv.org/abs/2110.04047v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Improving Pseudo-label Training For End-to-end Speech Recognition Using\n  Gradient Mask", "abstract": "In the recent trend of semi-supervised speech recognition, both\nself-supervised representation learning and pseudo-labeling have shown\npromising results. In this paper, we propose a novel approach to combine their\nideas for end-to-end speech recognition model. Without any extra loss function,\nwe utilize the Gradient Mask to optimize the model when training on\npseudo-label. This method forces the speech recognition model to predict from\nthe masked input to learn strong acoustic representation and make training\nrobust to label noise. In our semi-supervised experiments, the method can\nimprove the model performance when training on pseudo-label and our method\nachieved competitive results comparing with other semi-supervised approaches on\nthe Librispeech 100 hours experiments.", "published": "2021-10-08 12:05:25", "link": "http://arxiv.org/abs/2110.04056v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Method for Capturing and Reproducing Directional Reverberation in Six\n  Degrees of Freedom", "abstract": "The reproduction of acoustics is an important aspect of the preservation of\ncultural heritage. A common approach is to capture an impulse response in a\nhall and auralize it by convolving an input signal with the measured\nreverberant response. For immersive applications, it is typical to acquire\nspatial impulse responses using a spherical microphone array to capture the\nreverberant sound field. While this allows a listener to freely rotate their\nhead from the captured location during reproduction, delicate considerations\nmust be made to allow a full six degrees of freedom auralization. Furthermore,\nthe computational cost of convolution with a high-order Ambisonics impulse\nresponse remains prohibitively expensive for current real-time applications,\nwhere most of the resources are dedicated towards rendering graphics. For this\nreason, simplifications are often made in the reproduction of reverberation,\nsuch as using a uniform decay around the listener. However, recent work has\nhighlighted the importance of directional characteristics in the late\nreverberant sound field and more efficient reproduction methods have been\ndeveloped. In this article, we propose a framework that extracts directional\ndecay properties from a set of captured spatial impulse responses to\ncharacterize a directional feedback delay network. For this purpose, a data set\nwas acquired in the main auditorium of the Finnish National Opera and Ballet in\nHelsinki from multiple source-listener positions, in order to analyze the\nanisotropic characteristics of this auditorium and illustrate the proposed\nreproduction framework.", "published": "2021-10-08 12:18:17", "link": "http://arxiv.org/abs/2110.04082v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Affective Burst Detection from Speech using Kernel-fusion Dilated\n  Convolutional Neural Networks", "abstract": "As speech-interfaces are getting richer and widespread, speech emotion\nrecognition promises more attractive applications. In the continuous emotion\nrecognition (CER) problem, tracking changes across affective states is an\nimportant and desired capability. Although CER studies widely use correlation\nmetrics in evaluations, these metrics do not always capture all the\nhigh-intensity changes in the affective domain. In this paper, we define a\nnovel affective burst detection problem to accurately capture high-intensity\nchanges of the affective attributes. For this problem, we formulate a two-class\nclassification approach to isolate affective burst regions over the affective\nstate contour. The proposed classifier is a kernel-fusion dilated convolutional\nneural network (KFDCNN) architecture driven by speech spectral features to\nsegment the affective attribute contour into idle and burst sections.\nExperimental evaluations are performed on the RECOLA and CreativeIT datasets.\nThe proposed KFDCNN is observed to outperform baseline feedforward neural\nnetworks on both datasets.", "published": "2021-10-08 12:40:43", "link": "http://arxiv.org/abs/2110.04091v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition", "abstract": "End-to-end Automatic Speech Recognition (ASR) models are usually trained to\noptimize the loss of the whole token sequence, while neglecting explicit\nphonemic-granularity supervision. This could result in recognition errors due\nto similar-phoneme confusion or phoneme reduction. To alleviate this problem,\nwe propose a novel framework based on Supervised Contrastive Learning (SCaLa)\nto enhance phonemic representation learning for end-to-end ASR systems.\nSpecifically, we extend the self-supervised Masked Contrastive Predictive\nCoding (MCPC) to a fully-supervised setting, where the supervision is applied\nin the following way. First, SCaLa masks variable-length encoder features\naccording to phoneme boundaries given phoneme forced-alignment extracted from a\npre-trained acoustic model; it then predicts the masked features via\ncontrastive learning. The forced-alignment can provide phoneme labels to\nmitigate the noise introduced by positive-negative pairs in self-supervised\nMCPC. Experiments on reading and spontaneous speech datasets show that our\nproposed approach achieves 2.8 and 1.4 points Character Error Rate (CER)\nabsolute reductions compared to the baseline, respectively.", "published": "2021-10-08 15:15:38", "link": "http://arxiv.org/abs/2110.04187v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance optimizations on deep noise suppression models", "abstract": "We study the role of magnitude structured pruning as an architecture search\nto speed up the inference time of a deep noise suppression (DNS) model. While\ndeep learning approaches have been remarkably successful in enhancing audio\nquality, their increased complexity inhibits their deployment in real-time\napplications. We achieve up to a 7.25X inference speedup over the baseline,\nwith a smooth model performance degradation. Ablation studies indicate that our\nproposed network re-parameterization (i.e., size per layer) is the major driver\nof the speedup, and that magnitude structured pruning does comparably to\ndirectly training a model in the smaller size. We report inference speed\nbecause a parameter reduction does not necessitate speedup, and we measure\nmodel quality using an accurate non-intrusive objective speech quality metric.", "published": "2021-10-08 21:00:01", "link": "http://arxiv.org/abs/2110.04378v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Aura: Privacy-preserving Augmentation to Improve Test Set Diversity in\n  Speech Enhancement", "abstract": "Noise suppression models running in production environments are commonly\ntrained on publicly available datasets. However, this approach leads to\nregressions due to the lack of training/testing on representative customer\ndata. Moreover, due to privacy reasons, developers cannot listen to customer\ncontent. This `ears-off' situation motivates augmenting existing datasets in a\nprivacy-preserving manner. In this paper, we present Aura, a solution to make\nexisting noise suppression test sets more challenging and diverse while being\nsample efficient. Aura is `ears-off' because it relies on a feature extractor\nand a metric of speech quality, DNSMOS P.835, both pre-trained on data obtained\nfrom public sources. As an application of Aura, we augment the INTERSPEECH 2021\nDNS challenge by sampling audio files from a new batch of data of 20K clean\nspeech clips from Librivox mixed with noise clips obtained from AudioSet. Aura\nmakes the existing benchmark test set harder by 0.27 in DNSMOS P.835 OVLR (7%),\n0.64 harder in DNSMOS P.835 SIG (16%), increases diversity by 31%, and achieves\na 26% improvement in Spearman's rank correlation coefficient (SRCC) compared to\nrandom sampling. Finally, we open-source Aura to stimulate research of test set\ndevelopment.", "published": "2021-10-08 21:56:54", "link": "http://arxiv.org/abs/2110.04391v3", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the invertibility of a voice privacy system using embedding\n  alignement", "abstract": "This paper explores various attack scenarios on a voice anonymization system\nusing embeddings alignment techniques. We use Wasserstein-Procrustes (an\nalgorithm initially designed for unsupervised translation) or Procrustes\nanalysis to match two sets of x-vectors, before and after voice anonymization,\nto mimic this transformation as a rotation function. We compute the optimal\nrotation and compare the results of this approximation to the official Voice\nPrivacy Challenge results. We show that a complex system like the baseline of\nthe Voice Privacy Challenge can be approximated by a rotation, estimated using\na limited set of x-vectors. This paper studies the space of solutions for voice\nanonymization within the specific scope of rotations. Rotations being\nreversible, the proposed method can recover up to 62% of the speaker identities\nfrom anonymized embeddings.", "published": "2021-10-08 14:43:47", "link": "http://arxiv.org/abs/2110.05431v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Model Reprogramming with Similarity Based Mapping for\n  Low-Resource Spoken Command Recognition", "abstract": "In this study, we propose a novel adversarial reprogramming (AR) approach for\nlow-resource spoken command recognition (SCR), and build an AR-SCR system. The\nAR procedure aims to modify the acoustic signals (from the target domain) to\nrepurpose a pretrained SCR model (from the source domain). To solve the label\nmismatches between source and target domains, and further improve the stability\nof AR, we propose a novel similarity-based label mapping technique to align\nclasses. In addition, the transfer learning (TL) technique is combined with the\noriginal AR process to improve the model adaptation capability. We evaluate the\nproposed AR-SCR system on three low-resource SCR datasets, including Arabic,\nLithuanian, and dysarthric Mandarin speech. Experimental results show that with\na pretrained AM trained on a large-scale English dataset, the proposed AR-SCR\nsystem outperforms the current state-of-the-art results on Arabic and\nLithuanian speech commands datasets, with only a limited amount of training\ndata.", "published": "2021-10-08 05:07:35", "link": "http://arxiv.org/abs/2110.03894v5", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
