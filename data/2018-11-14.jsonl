{"title": "Translating a Math Word Problem to an Expression Tree", "abstract": "Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to\nautomatic math word problem solving. Despite its simplicity, a drawback still\nremains: a math word problem can be correctly solved by more than one\nequations. This non-deterministic transduction harms the performance of maximum\nlikelihood estimation. In this paper, by considering the uniqueness of\nexpression tree, we propose an equation normalization method to normalize the\nduplicated equations. Moreover, we analyze the performance of three popular\nSEQ2SEQ models on the math word problem solving. We find that each model has\nits own specialty in solving problems, consequently an ensemble model is then\nproposed to combine their advantages. Experiments on dataset Math23K show that\nthe ensemble model with equation normalization significantly outperforms the\nprevious state-of-the-art methods.", "published": "2018-11-14 04:18:00", "link": "http://arxiv.org/abs/1811.05632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Coherence for Discourse Neural Machine Translation", "abstract": "Discourse coherence plays an important role in the translation of one text.\nHowever, the previous reported models most focus on improving performance over\nindividual sentence while ignoring cross-sentence links and dependencies, which\naffects the coherence of the text. In this paper, we propose to use discourse\ncontext and reward to refine the translation quality from the discourse\nperspective. In particular, we generate the translation of individual sentences\nat first. Next, we deliberate the preliminary produced translations, and train\nthe model to learn the policy that produces discourse coherent text by a reward\nteacher. Practical results on multiple discourse test datasets indicate that\nour model significantly improves the translation quality over the\nstate-of-the-art baseline system by +1.23 BLEU score. Moreover, our model\ngenerates more discourse coherent text and obtains +2.2 BLEU improvements when\nevaluated by discourse metrics.", "published": "2018-11-14 08:22:27", "link": "http://arxiv.org/abs/1811.05683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Aspect Phrase Embeddings for Cross-Domain Review Rating\n  Prediction", "abstract": "Online review platforms are a popular way for users to post reviews by\nexpressing their opinions towards a product or service, as well as they are\nvaluable for other users and companies to find out the overall opinions of\ncustomers. These reviews tend to be accompanied by a rating, where the star\nrating has become the most common approach for users to give their feedback in\na quantitative way, generally as a likert scale of 1-5 stars. In other social\nmedia platforms like Facebook or Twitter, an automated review rating prediction\nsystem can be useful to determine the rating that a user would have given to\nthe product or service. Existing work on review rating prediction focuses on\nspecific domains, such as restaurants or hotels. This, however, ignores the\nfact that some review domains which are less frequently rated, such as\ndentists, lack sufficient data to build a reliable prediction model. In this\npaper, we experiment on 12 datasets pertaining to 12 different review domains\nof varying level of popularity to assess the performance of predictions across\ndifferent domains. We introduce a model that leverages aspect phrase embeddings\nextracted from the reviews, which enables the development of both in-domain and\ncross-domain review rating prediction systems. Our experiments show that both\nof our review rating prediction systems outperform all other baselines. The\ncross-domain review rating prediction system is particularly significant for\nthe least popular review domains, where leveraging training data from other\ndomains leads to remarkable improvements in performance. The in-domain review\nrating prediction system is instead more suitable for popular review domains,\nprovided that a model built from training data pertaining to the target domain\nis more suitable when this data is abundant.", "published": "2018-11-14 08:43:07", "link": "http://arxiv.org/abs/1811.05689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Multiple Diverse Responses for Short-Text Conversation", "abstract": "Neural generative models have become popular and achieved promising\nperformance on short-text conversation tasks. They are generally trained to\nbuild a 1-to-1 mapping from the input post to its output response. However, a\ngiven post is often associated with multiple replies simultaneously in real\napplications. Previous research on this task mainly focuses on improving the\nrelevance and informativeness of the top one generated response for each post.\nVery few works study generating multiple accurate and diverse responses for the\nsame post. In this paper, we propose a novel response generation model, which\nconsiders a set of responses jointly and generates multiple diverse responses\nsimultaneously. A reinforcement learning algorithm is designed to solve our\nmodel. Experiments on two short-text conversation tasks validate that the\nmultiple responses generated by our model obtain higher quality and larger\ndiversity compared with various state-of-the-art generative models.", "published": "2018-11-14 09:20:46", "link": "http://arxiv.org/abs/1811.05696v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan-And-Write: Towards Better Automatic Storytelling", "abstract": "Automatic storytelling is challenging since it requires generating long,\ncoherent natural language to describes a sensible sequence of events. Despite\nconsiderable efforts on automatic story generation in the past, prior work\neither is restricted in plot planning, or can only generate stories in a narrow\ndomain. In this paper, we explore open-domain story generation that writes\nstories given a title (topic) as input. We propose a plan-and-write\nhierarchical generation framework that first plans a storyline, and then\ngenerates a story based on the storyline. We compare two planning strategies.\nThe dynamic schema interweaves story planning and its surface realization in\ntext, while the static schema plans out the entire storyline before generating\nstories. Experiments show that with explicit storyline planning, the generated\nstories are more diverse, coherent, and on topic than those generated without\ncreating a full plan, according to both automatic and human evaluations.", "published": "2018-11-14 09:37:40", "link": "http://arxiv.org/abs/1811.05701v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deterministic Algorithm for Bridging Anaphora Resolution", "abstract": "Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et\nal., 2013b) use syntactic preposition patterns to calculate word relatedness.\nHowever, such patterns only consider NPs' head nouns and hence do not fully\ncapture the semantics of NPs. Recently, Hou (2018) created word embeddings\n(embeddings_PP) to capture associative similarity (ie, relatedness) between\nnouns by exploring the syntactic structure of noun phrases. But embeddings_PP\nonly contains word representations for nouns. In this paper, we create new word\nvectors by combining embeddings_PP with GloVe. This new word embeddings\n(embeddings_bridging) are a more general lexical knowledge resource for\nbridging and allow us to represent the meaning of an NP beyond its head easily.\nWe therefore develop a deterministic approach for bridging anaphora resolution,\nwhich represents the semantics of an NP based on its head noun and\nmodifications. We show that this simple approach achieves the competitive\nresults compared to the best system in Hou et al.(2013b) which explores Markov\nLogic Networks to model the problem. Additionally, we further improve the\nresults for bridging anaphora resolution reported in Hou (2018) by combining\nour simple deterministic approach with Hou et al.(2013b)'s best system MLN II.", "published": "2018-11-14 10:49:39", "link": "http://arxiv.org/abs/1811.05721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Based Statement Classification for Biased Language", "abstract": "Biased language commonly occurs around topics which are of controversial\nnature, thus, stirring disagreement between the different involved parties of a\ndiscussion. This is due to the fact that for language and its use,\nspecifically, the understanding and use of phrases, the stances are cohesive\nwithin the particular groups. However, such cohesiveness does not hold across\ngroups.\n  In collaborative environments or environments where impartial language is\ndesired (e.g. Wikipedia, news media), statements and the language therein\nshould represent equally the involved parties and be neutrally phrased. Biased\nlanguage is introduced through the presence of inflammatory words or phrases,\nor statements that may be incorrect or one-sided, thus violating such\nconsensus.\n  In this work, we focus on the specific case of phrasing bias, which may be\nintroduced through specific inflammatory words or phrases in a statement. For\nthis purpose, we propose an approach that relies on a recurrent neural networks\nin order to capture the inter-dependencies between words in a phrase that\nintroduced bias.\n  We perform a thorough experimental evaluation, where we show the advantages\nof a neural based approach over competitors that rely on word lexicons and\nother hand-crafted features in detecting biased language. We are able to\ndistinguish biased statements with a precision of P=0.92, thus significantly\noutperforming baseline models with an improvement of over 30%. Finally, we\nrelease the largest corpus of statements annotated for biased language.", "published": "2018-11-14 11:55:51", "link": "http://arxiv.org/abs/1811.05740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Grammar Induction with a Neural Variational Transition-based\n  Parser", "abstract": "Dependency grammar induction is the task of learning dependency syntax\nwithout annotated training data. Traditional graph-based models with global\ninference achieve state-of-the-art results on this task but they require\n$O(n^3)$ run time. Transition-based models enable faster inference with $O(n)$\ntime complexity, but their performance still lags behind. In this work, we\npropose a neural transition-based parser for dependency grammar induction,\nwhose inference procedure utilizes rich neural features with $O(n)$ time\ncomplexity. We train the parser with an integration of variational inference,\nposterior regularization and variance reduction techniques. The resulting\nframework outperforms previous unsupervised transition-based dependency parsers\nand achieves performance comparable to graph-based models, both on the English\nPenn Treebank and on the Universal Dependency Treebank. In an empirical\ncomparison, we show that our approach substantially increases parsing speed\nover graph-based models.", "published": "2018-11-14 16:30:22", "link": "http://arxiv.org/abs/1811.05889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ADAPT System Description for the IWSLT 2018 Basque to English\n  Translation Task", "abstract": "In this paper we present the ADAPT system built for the Basque to English Low\nResource MT Evaluation Campaign. Basque is a low-resourced,\nmorphologically-rich language. This poses a challenge for Neural Machine\nTranslation models which usually achieve better performance when trained with\nlarge sets of data.\n  Accordingly, we used synthetic data to improve the translation quality\nproduced by a model built using only authentic data. Our proposal uses\nback-translated data to: (a) create new sentences, so the system can be trained\nwith more data; and (b) translate sentences that are close to the test set, so\nthe model can be fine-tuned to the document to be translated.", "published": "2018-11-14 17:06:33", "link": "http://arxiv.org/abs/1811.05909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic\n  Tasks", "abstract": "Much effort has been devoted to evaluate whether multi-task learning can be\nleveraged to learn rich representations that can be used in various Natural\nLanguage Processing (NLP) down-stream applications. However, there is still a\nlack of understanding of the settings in which multi-task learning has a\nsignificant effect. In this work, we introduce a hierarchical model trained in\na multi-task learning setup on a set of carefully selected semantic tasks. The\nmodel is trained in a hierarchical fashion to introduce an inductive bias by\nsupervising a set of low level tasks at the bottom layers of the model and more\ncomplex tasks at the top layers of the model. This model achieves\nstate-of-the-art results on a number of tasks, namely Named Entity Recognition,\nEntity Mention Detection and Relation Extraction without hand-engineered\nfeatures or external NLP tools like syntactic parsers. The hierarchical\ntraining supervision induces a set of shared semantic representations at lower\nlayers of the model. We show that as we move from the bottom to the top layers\nof the model, the hidden states of the layers tend to represent more complex\nsemantic information.", "published": "2018-11-14 19:42:03", "link": "http://arxiv.org/abs/1811.06031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distantly Supervised Relation Extraction with Neural Noise\n  Converter and Conditional Optimal Selector", "abstract": "Distant supervised relation extraction has been successfully applied to large\ncorpus with thousands of relations. However, the inevitable wrong labeling\nproblem by distant supervision will hurt the performance of relation\nextraction. In this paper, we propose a method with neural noise converter to\nalleviate the impact of noisy data, and a conditional optimal selector to make\nproper prediction. Our noise converter learns the structured transition matrix\non logit level and captures the property of distant supervised relation\nextraction dataset. The conditional optimal selector on the other hand helps to\nmake proper prediction decision of an entity pair even if the group of\nsentences is overwhelmed by no-relation sentences. We conduct experiments on a\nwidely used dataset and the results show significant improvement over\ncompetitive baseline methods.", "published": "2018-11-14 03:02:12", "link": "http://arxiv.org/abs/1811.05616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extractive Summary as Discrete Latent Variables", "abstract": "In this paper, we compare various methods to compress a text using a neural\nmodel. We find that extracting tokens as latent variables significantly\noutperforms the state-of-the-art discrete latent variable models such as\nVQ-VAE. Furthermore, we compare various extractive compression schemes. There\nare two best-performing methods that perform equally. One method is to simply\nchoose the tokens with the highest tf-idf scores. Another is to train a\nbidirectional language model similar to ELMo and choose the tokens with the\nhighest loss. If we consider any subsequence of a text to be a text in a\nbroader sense, we conclude that language is a strong compression code of\nitself. Our finding justifies the high quality of generation achieved with\nhierarchical method, as their latent variables are nothing but natural language\nsummary. We also conclude that there is a hierarchy in language such that an\nentire text can be predicted much more easily based on a sequence of a small\nnumber of keywords, which can be easily found by classical methods as tf-idf.\nWe speculate that this extraction process may be useful for unsupervised\nhierarchical text generation.", "published": "2018-11-14 02:02:18", "link": "http://arxiv.org/abs/1811.05542v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Jointly Learning to Label Sentences and Tokens", "abstract": "Learning to construct text representations in end-to-end systems can be\ndifficult, as natural languages are highly compositional and task-specific\nannotated datasets are often limited in size. Methods for directly supervising\nlanguage composition can allow us to guide the models based on existing\nknowledge, regularizing them towards more robust and interpretable\nrepresentations. In this paper, we investigate how objectives at different\ngranularities can be used to learn better language representations and we\npropose an architecture for jointly learning to label sentences and tokens. The\npredictions at each level are combined together using an attention mechanism,\nwith token-level labels also acting as explicit supervision for composing\nsentence-level representations. Our experiments show that by learning to\nperform these tasks jointly on multiple levels, the model achieves substantial\nimprovements for both sentence classification and sequence labeling.", "published": "2018-11-14 18:32:18", "link": "http://arxiv.org/abs/1811.05949v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "Automatic Grammar Augmentation for Robust Voice Command Recognition", "abstract": "This paper proposes a novel pipeline for automatic grammar augmentation that\nprovides a significant improvement in the voice command recognition accuracy\nfor systems with small footprint acoustic model (AM). The improvement is\nachieved by augmenting the user-defined voice command set, also called grammar\nset, with alternate grammar expressions. For a given grammar set, a set of\npotential grammar expressions (candidate set) for augmentation is constructed\nfrom an AM-specific statistical pronunciation dictionary that captures the\nconsistent patterns and errors in the decoding of AM induced by variations in\npronunciation, pitch, tempo, accent, ambiguous spellings, and noise conditions.\nUsing this candidate set, greedy optimization based and cross-entropy-method\n(CEM) based algorithms are considered to search for an augmented grammar set\nwith improved recognition accuracy utilizing a command-specific dataset. Our\nexperiments show that the proposed pipeline along with algorithms considered in\nthis paper significantly reduce the mis-detection and mis-classification rate\nwithout increasing the false-alarm rate. Experiments also demonstrate the\nconsistent superior performance of CEM method over greedy-based algorithms.", "published": "2018-11-14 22:19:03", "link": "http://arxiv.org/abs/1811.06096v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Study of Language and Classifier-independent Feature Analysis for\n  Vocal Emotion Recognition", "abstract": "Every speech signal carries implicit information about the emotions, which\ncan be extracted by speech processing methods. In this paper, we propose an\nalgorithm for extracting features that are independent from the spoken language\nand the classification method to have comparatively good recognition\nperformance on different languages independent from the employed classification\nmethods. The proposed algorithm is composed of three stages. In the first\nstage, we propose a feature ranking method analyzing the state-of-the-art voice\nquality features. In the second stage, we propose a method for finding the\nsubset of the common features for each language and classifier. In the third\nstage, we compare our approach with the recognition rate of the\nstate-of-the-art filter methods. We use three databases with different\nlanguages, namely, Polish, Serbian and English. Also three different\nclassifiers, namely, nearest neighbour, support vector machine and gradient\ndescent neural network, are employed. It is shown that our method for selecting\nthe most significant language-independent and method-independent features in\nmany cases outperforms state-of-the-art filter methods.", "published": "2018-11-14 08:54:24", "link": "http://arxiv.org/abs/1811.08935v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Free Text to Clusters of Content in Health Records: An Unsupervised\n  Graph Partitioning Approach", "abstract": "Electronic Healthcare records contain large volumes of unstructured data in\ndifferent forms. Free text constitutes a large portion of such data, yet this\nsource of richly detailed information often remains under-used in practice\nbecause of a lack of suitable methodologies to extract interpretable content in\na timely manner. Here we apply network-theoretical tools to the analysis of\nfree text in Hospital Patient Incident reports in the English National Health\nService, to find clusters of reports in an unsupervised manner and at different\nlevels of resolution based directly on the free text descriptions contained\nwithin them. To do so, we combine recently developed deep neural network\ntext-embedding methodologies based on paragraph vectors with multi-scale Markov\nStability community detection applied to a similarity graph of documents\nobtained from sparsified text vector similarities. We showcase the approach\nwith the analysis of incident reports submitted in Imperial College Healthcare\nNHS Trust, London. The multiscale community structure reveals levels of meaning\nwith different resolution in the topics of the dataset, as shown by relevant\ndescriptive terms extracted from the groups of records, as well as by comparing\na posteriori against hand-coded categories assigned by healthcare personnel.\nOur content communities exhibit good correspondence with well-defined\nhand-coded categories, yet our results also provide further medical detail in\ncertain areas as well as revealing complementary descriptors of incidents\nbeyond the external classification. We also discuss how the method can be used\nto monitor reports over time and across different healthcare providers, and to\ndetect emerging trends that fall outside of pre-existing categories.", "published": "2018-11-14 10:08:19", "link": "http://arxiv.org/abs/1811.05711v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "math.SP"], "primary_category": "cs.CL"}
{"title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling", "abstract": "In real-world applications of natural language generation, there are often\nconstraints on the target sentences in addition to fluency and naturalness\nrequirements. Existing language generation techniques are usually based on\nrecurrent neural networks (RNNs). However, it is non-trivial to impose\nconstraints on RNNs while maintaining generation quality, since RNNs generate\nsentences sequentially (or with beam search) from the first word to the last.\nIn this paper, we propose CGMH, a novel approach using Metropolis-Hastings\nsampling for constrained sentence generation. CGMH allows complicated\nconstraints such as the occurrence of multiple keywords in the target\nsentences, which cannot be handled in traditional RNN-based approaches.\nMoreover, CGMH works in the inference stage, and does not require parallel\ncorpora for training. We evaluate our method on a variety of tasks, including\nkeywords-to-sentence generation, unsupervised sentence paraphrasing, and\nunsupervised sentence error correction. CGMH achieves high performance compared\nwith previous supervised methods for sentence generation. Our code is released\nat https://github.com/NingMiao/CGMH", "published": "2018-11-14 15:46:57", "link": "http://arxiv.org/abs/1811.10996v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.CL"}
{"title": "Speech Coding, Speech Interfaces and IoT - Opportunities and Challenges", "abstract": "Recent speech and audio coding standards such as 3GPP Enhanced Voice Services\nmatch the foreseeable needs and requirements in transmission of speech and\naudio, when using current transmission infrastructure and applications. Trends\nin Internet-of-Things technology and development in personal digital assistants\n(PDAs) however begs us to consider future requirements for speech and audio\ncodecs. The opportunities and challenges are here summarized in three concepts:\ncollaboration, unification and privacy. First, an increasing number of devices\nwill in the future be speech-operated, whereby the ability to focus voice\ncommands to a specific devices becomes essential. We therefore need methods\nwhich allows collaboration between devices, such that ambiguities can be\nresolved. Second, such collaboration can be achieved with a unified and\nstandardized communication protocol between voice-operated devices. To achieve\nsuch collaboration protocols, we need to develop distributed speech coding\ntechnology for ad-hoc IoT networks. Finally however, collaboration will\nincrease the demand for privacy protection in speech interfaces and it is\ntherefore likely that technologies for supporting privacy and generating trust\nwill be in high demand.", "published": "2018-11-14 10:42:34", "link": "http://arxiv.org/abs/1811.05720v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "To bee or not to bee: Investigating machine learning approaches for\n  beehive sound recognition", "abstract": "In this work, we aim to explore the potential of machine learning methods to\nthe problem of beehive sound recognition. A major contribution of this work is\nthe creation and release of annotations for a selection of beehive recordings.\nBy experimenting with both support vector machines and convolutional neural\nnetworks, we explore important aspects to be considered in the development of\nbeehive sound recognition systems using machine learning approaches.", "published": "2018-11-14 19:16:27", "link": "http://arxiv.org/abs/1811.06016v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feature exploration for almost zero-resource ASR-free keyword spotting\n  using a multilingual bottleneck extractor and correspondence autoencoders", "abstract": "We compare features for dynamic time warping (DTW) when used to bootstrap\nkeyword spotting (KWS) in an almost zero-resource setting. Such\nquickly-deployable systems aim to support United Nations (UN) humanitarian\nrelief efforts in parts of Africa with severely under-resourced languages. Our\nobjective is to identify acoustic features that provide acceptable KWS\nperformance in such environments. As supervised resource, we restrict ourselves\nto a small, easily acquired and independently compiled set of isolated\nkeywords. For feature extraction, a multilingual bottleneck feature (BNF)\nextractor, trained on well-resourced out-of-domain languages, is integrated\nwith a correspondence autoencoder (CAE) trained on extremely sparse in-domain\ndata. On their own, BNFs and CAE features are shown to achieve a more than 2%\nabsolute performance improvement over baseline MFCCs. However, by using BNFs as\ninput to the CAE, even better performance is achieved, with a more than 11%\nabsolute improvement in ROC AUC over MFCCs and more than twice as many top-10\nretrievals for two evaluated languages, English and Luganda. We conclude that\nintegrating BNFs with the CAE allows both large out-of-domain and sparse\nin-domain resources to be exploited for improved ASR-free keyword spotting.", "published": "2018-11-14 09:29:11", "link": "http://arxiv.org/abs/1811.08284v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
