{"title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing", "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.", "published": "2024-06-24 01:30:22", "link": "http://arxiv.org/abs/2406.16253v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Influence of Prompt-Specific Shortcuts in AI Generated\n  Text Detection", "abstract": "AI Generated Text (AIGT) detectors are developed with texts from humans and\nLLMs of common tasks. Despite the diversity of plausible prompt choices, these\ndatasets are generally constructed with a limited number of prompts. The lack\nof prompt variation can introduce prompt-specific shortcut features that exist\nin data collected with the chosen prompt, but do not generalize to others. In\nthis paper, we analyze the impact of such shortcuts in AIGT detection. We\npropose Feedback-based Adversarial Instruction List Optimization (FAILOpt), an\nattack that searches for instructions deceptive to AIGT detectors exploiting\nprompt-specific shortcuts. FAILOpt effectively drops the detection performance\nof the target detector, comparable to other attacks based on adversarial\nin-context examples. We also utilize our method to enhance the robustness of\nthe detector by mitigating the shortcuts. Based on the findings, we further\ntrain the classifier with the dataset augmented by FAILOpt prompt. The\naugmented classifier exhibits improvements across generation models, tasks, and\nattacks. Our code will be available at https://github.com/zxcvvxcz/FAILOpt.", "published": "2024-06-24 02:50:09", "link": "http://arxiv.org/abs/2406.16275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PlagBench: Exploring the Duality of Large Language Models in Plagiarism\n  Generation and Detection", "abstract": "Recent studies have raised concerns about the potential threats large\nlanguage models (LLMs) pose to academic integrity and copyright protection.\nYet, their investigation is predominantly focused on literal copies of original\ntexts. Also, how LLMs can facilitate the detection of LLM-generated plagiarism\nremains largely unexplored. To address these gaps, we introduce \\textbf{{\\sf\nPlagBench}}, a dataset of 46.5K synthetic text pairs that represent three major\ntypes of plagiarism: verbatim copying, paraphrasing, and summarization. These\nsamples are generated by three advanced LLMs. We rigorously validate the\nquality of PlagBench through a combination of fine-grained automatic evaluation\nand human annotation. We then utilize this dataset for two purposes: (1) to\nexamine LLMs' ability to transform original content into accurate paraphrases\nand summaries, and (2) to evaluate the plagiarism detection performance of five\nmodern LLMs alongside three specialized plagiarism checkers. Our results show\nthat GPT-3.5 Turbo can produce high-quality paraphrases and summaries without\nsignificantly increasing text complexity compared to GPT-4 Turbo. However, in\nterms of detection, GPT-4 outperforms other LLMs and commercial detection tools\nby 20%, highlights the evolving capabilities of LLMs not only in content\ngeneration but also in plagiarism detection. Data and source code are available\nat https://github.com/Brit7777/plagbench.", "published": "2024-06-24 03:29:53", "link": "http://arxiv.org/abs/2406.16288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelled Multivariate Overlap: A method for measuring vowel merger", "abstract": "This paper introduces a novel method for quantifying vowel overlap. There is\na tension in previous work between using multivariate measures, such as those\nderived from empirical distributions, and the ability to control for unbalanced\ndata and extraneous factors, as is possible when using fitted model parameters.\nThe method presented here resolves this tension by jointly modelling all\nacoustic dimensions of interest and by simulating distributions from the model\nto compute a measure of vowel overlap. An additional benefit of this method is\nthat computation of uncertainty becomes straightforward. We evaluate this\nmethod on corpus speech data targeting the PIN-PEN merger in four dialects of\nEnglish and find that using modelled distributions to calculate Bhattacharyya\naffinity substantially improves results compared to empirical distributions,\nwhile the difference between multivariate and univariate modelling is subtle.", "published": "2024-06-24 04:56:26", "link": "http://arxiv.org/abs/2406.16319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for\n  Gaussian-Noise-free Text-Image Corruption and Evaluation", "abstract": "Vision-Language Models (VLMs) have gained community-spanning prominence due\nto their ability to integrate visual and textual inputs to perform complex\ntasks. Despite their success, the internal decision-making processes of these\nmodels remain opaque, posing challenges in high-stakes applications. To address\nthis, we introduce NOTICE, the first Noise-free Text-Image Corruption and\nEvaluation pipeline for mechanistic interpretability in VLMs. NOTICE\nincorporates a Semantic Minimal Pairs (SMP) framework for image corruption and\nSymmetric Token Replacement (STR) for text. This approach enables semantically\nmeaningful causal mediation analysis for both modalities, providing a robust\nmethod for analyzing multimodal integration within models like BLIP. Our\nexperiments on the SVO-Probes, MIT-States, and Facial Expression Recognition\ndatasets reveal crucial insights into VLM decision-making, identifying the\nsignificant role of middle-layer cross-attention heads. Further, we uncover a\nset of ``universal cross-attention heads'' that consistently contribute across\ntasks and modalities, each performing distinct functions such as implicit image\nsegmentation, object inhibition, and outlier inhibition. This work paves the\nway for more transparent and interpretable multimodal systems.", "published": "2024-06-24 05:13:19", "link": "http://arxiv.org/abs/2406.16320v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EHRCon: Dataset for Checking Consistency between Unstructured Notes and\n  Structured Tables in Electronic Health Records", "abstract": "Electronic Health Records (EHRs) are integral for storing comprehensive\npatient medical records, combining structured data (e.g., medications) with\ndetailed clinical notes (e.g., physician notes). These elements are essential\nfor straightforward data retrieval and provide deep, contextual insights into\npatient care. However, they often suffer from discrepancies due to unintuitive\nEHR system designs and human errors, posing serious risks to patient safety. To\naddress this, we developed EHRCon, a new dataset and task specifically designed\nto ensure data consistency between structured tables and unstructured notes in\nEHRs. EHRCon was crafted in collaboration with healthcare professionals using\nthe MIMIC-III EHR dataset, and includes manual annotations of 4,101 entities\nacross 105 clinical notes checked against database entries for consistency.\nEHRCon has two versions, one using the original MIMIC-III schema, and another\nusing the OMOP CDM schema, in order to increase its applicability and\ngeneralizability. Furthermore, leveraging the capabilities of large language\nmodels, we introduce CheckEHR, a novel framework for verifying the consistency\nbetween clinical notes and database tables. CheckEHR utilizes an eight-stage\nprocess and shows promising results in both few-shot and zero-shot settings.\nThe code is available at https://github.com/dustn1259/EHRCon.", "published": "2024-06-24 06:26:50", "link": "http://arxiv.org/abs/2406.16341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded\n  Adversarialness", "abstract": "Adversarial datasets should validate AI robustness by providing samples on\nwhich humans perform well, but models do not. However, as models evolve,\ndatasets can become obsolete. Measuring whether a dataset remains adversarial\nis hindered by the lack of a standardized metric for measuring adversarialness.\nWe propose AdvScore, a human-grounded evaluation metric that assesses a\ndataset's adversarialness by capturing models' and humans' varying abilities\nwhile also identifying poor examples. We then use AdvScore to motivate a new\ndataset creation pipeline for realistic and high-quality adversarial samples,\nenabling us to collect an adversarial question answering (QA) dataset, AdvQA.\nWe apply AdvScore using 9,347 human responses and ten language models'\npredictions to track model improvement over five years, from 2020 to 2024.\nAdvScore thus provides guidance for achieving robustness comparable with human\ncapabilities. Furthermore, it helps determine to what extent adversarial\ndatasets continue to pose challenges, ensuring that, rather than reflecting\noutdated or overly artificial difficulties, they effectively test model\ncapabilities.", "published": "2024-06-24 06:27:47", "link": "http://arxiv.org/abs/2406.16342v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Instruction-Following Ability for Large Language Models on\n  Story-Ending Generation", "abstract": "Instruction-tuned Large Language Models (LLMs) have achieved remarkable\nperformance across various benchmark tasks. While providing instructions to\nLLMs for guiding their generations is user-friendly, assessing their\ninstruction-following capabilities is still unclarified due to a lack of\nevaluation metrics. In this paper, we focus on evaluating the\ninstruction-following ability of LLMs in the context of story-ending\ngeneration, which requires diverse and context-specific instructions. We\npropose an automatic evaluation pipeline that utilizes a machine reading\ncomprehension (MRC) model to determine whether the generated story-ending\nreflects instruction. Our findings demonstrate that our proposed metric aligns\nwith human evaluation. Furthermore, our experiments confirm that recent\nopen-source LLMs can achieve instruction-following performance close to\nGPT-3.5, as assessed through automatic evaluation.", "published": "2024-06-24 06:53:36", "link": "http://arxiv.org/abs/2406.16356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot\n  Cross-Lingual Natural Language Understanding", "abstract": "Cross-lingual representation learning transfers knowledge from resource-rich\ndata to resource-scarce ones to improve the semantic understanding abilities of\ndifferent languages. However, previous works rely on shallow unsupervised data\ngenerated by token surface matching, regardless of the global context-aware\nsemantics of the surrounding text tokens. In this paper, we propose an\nUnsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for\ncross-lingual natural language understanding to enrich the training data\nwithout human interventions. Specifically, to retrieve the tokens with similar\nmeanings for the semantic data augmentation across different languages, we\npropose a sequential clustering process in 3 stages: within a single language,\nacross multiple languages of a language family, and across languages from\nmultiple language families. Meanwhile, considering the multi-lingual knowledge\ninfusion with context-aware semantics while alleviating computation burden, we\ndirectly replace the key constituents of the sentences with the above-learned\nmulti-lingual family knowledge, viewed as pseudo-semantic. The infusion process\nis further optimized via three de-biasing techniques without introducing any\nneural parameters. Extensive experiments demonstrate that our model\nconsistently improves the performance on general zero-shot cross-lingual\nnatural language understanding tasks, including sequence classification,\ninformation extraction, and question answering.", "published": "2024-06-24 07:27:01", "link": "http://arxiv.org/abs/2406.16372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KEHRL: Learning Knowledge-Enhanced Language Representations with\n  Hierarchical Reinforcement Learning", "abstract": "Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation\ntriples from knowledge graphs (KGs) and integrate these external data sources\ninto language models via self-supervised learning. Previous works treat\nknowledge enhancement as two independent operations, i.e., knowledge injection\nand knowledge integration. In this paper, we propose to learn\nKnowledge-Enhanced language representations with Hierarchical Reinforcement\nLearning (KEHRL), which jointly addresses the problems of detecting positions\nfor knowledge injection and integrating external knowledge into the model in\norder to avoid injecting inaccurate or irrelevant knowledge. Specifically, a\nhigh-level reinforcement learning (RL) agent utilizes both internal and prior\nknowledge to iteratively detect essential positions in texts for knowledge\ninjection, which filters out less meaningful entities to avoid diverting the\nknowledge learning direction. Once the entity positions are selected, a\nrelevant triple filtration module is triggered to perform low-level RL to\ndynamically refine the triples associated with polysemic entities through\nbinary-valued actions. Experiments validate KEHRL's effectiveness in probing\nfactual knowledge and enhancing the model's performance on various natural\nlanguage understanding tasks.", "published": "2024-06-24 07:32:35", "link": "http://arxiv.org/abs/2406.16374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large\n  Language Models", "abstract": "Sequential decision-making refers to algorithms that take into account the\ndynamics of the environment, where early decisions affect subsequent decisions.\nWith large language models (LLMs) demonstrating powerful capabilities between\ntasks, we can't help but ask: Can Current LLMs Effectively Make Sequential\nDecisions? In order to answer this question, we propose the UNO Arena based on\nthe card game UNO to evaluate the sequential decision-making capability of LLMs\nand explain in detail why we choose UNO. In UNO Arena, We evaluate the\nsequential decision-making capability of LLMs dynamically with novel metrics\nbased Monte Carlo methods. We set up random players, DQN-based reinforcement\nlearning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison\ntesting. Furthermore, in order to improve the sequential decision-making\ncapability of LLMs, we propose the TUTRI player, which can involves having LLMs\nreflect their own actions wtih the summary of game history and the game\nstrategy. Numerous experiments demonstrate that the TUTRI player achieves a\nnotable breakthrough in the performance of sequential decision-making compared\nto the vanilla LLM player.", "published": "2024-06-24 07:47:34", "link": "http://arxiv.org/abs/2406.16382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons", "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously update factual\nknowledge across multiple languages within large language models (LLMs).\nPrevious research indicates that the same knowledge across different languages\nwithin LLMs exhibits a degree of shareability. However, most existing MKE\nmethods overlook the connections of the same knowledge between different\nlanguages, resulting in knowledge conflicts and limited edit performance. To\naddress this issue, we first investigate how LLMs process multilingual factual\nknowledge and discover that the same factual knowledge in different languages\ngenerally activates a shared set of neurons, which we call language-agnostic\nfactual neurons (LAFNs). These neurons represent the same factual knowledge\nshared across languages and imply the semantic connections among multilingual\nknowledge. Inspired by this finding, we propose a new MKE method by Locating\nand Updating Language-Agnostic Factual Neurons (LU-LAFNs) to edit multilingual\nknowledge simultaneously, which avoids knowledge conflicts and thus improves\nedit performance. Experimental results on Bi-ZsRE and MzsRE benchmarks\ndemonstrate that our method achieves the best edit performance, indicating the\neffectiveness and importance of modeling the semantic connections among\nmultilingual knowledge.", "published": "2024-06-24 08:06:56", "link": "http://arxiv.org/abs/2406.16416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniCoder: Scaling Code Large Language Model via Universal Code", "abstract": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.", "published": "2024-06-24 08:32:48", "link": "http://arxiv.org/abs/2406.16441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building on Efficient Foundations: Effectively Training LLMs with\n  Structured Feedforward Layers", "abstract": "State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter counts and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFNs), which are less studied than attention blocks. We\nconsider three structured linear parameterizations of the FFN using efficient\nlow-rank and block-diagonal matrices. In contrast to many previous works that\nexamined these approximations, our study i) explores these structures from a\ntraining-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)\nis conducted within recent Transformer-based LLMs rather than convolutional\narchitectures. We demonstrate that these structures can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Interestingly,\nthe scaling performance of structured matrices is explored, revealing steeper\ncurves in scaling training FLOPs, along with a favorable scaling trend in the\novertraining regime. Specifically, we show that wide and structured networks\ncan utilize training FLOPs more efficiently, with fewer parameters and lower\nloss than dense models at their optimal trade-off. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.", "published": "2024-06-24 08:43:21", "link": "http://arxiv.org/abs/2406.16450v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses\n  and Annotations", "abstract": "The study of multimodal interaction in therapy can yield a comprehensive\nunderstanding of therapist and patient behavior that can be used to develop a\nmultimodal virtual agent supporting therapy. This investigation aims to uncover\nhow therapists skillfully blend therapy's task goal (employing classical steps\nof Motivational Interviewing) with the social goal (building a trusting\nrelationship and expressing empathy). Furthermore, we seek to categorize\npatients into various ``types'' requiring tailored therapeutic approaches. To\nthis intent, we present multimodal annotations of a corpus consisting of\nsimulated motivational interviewing conversations, wherein actors portray the\nroles of patients and therapists. We introduce EMMI, composed of two publicly\navailable MI corpora, AnnoMI and the Motivational Interviewing Dataset, for\nwhich we add multimodal annotations. We analyze these annotations to\ncharacterize functional behavior for developing a virtual agent performing\nmotivational interviews emphasizing social and empathic behaviors. Our analysis\nfound three clusters of patients expressing significant differences in behavior\nand adaptation of the therapist's behavior to those types. This shows the\nimportance of a therapist being able to adapt their behavior depending on the\ncurrent situation within the dialog and the type of user.", "published": "2024-06-24 09:32:28", "link": "http://arxiv.org/abs/2406.16478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deepfake tweets automatic detection", "abstract": "This study addresses the critical challenge of detecting DeepFake tweets by\nleveraging advanced natural language processing (NLP) techniques to distinguish\nbetween genuine and AI-generated texts. Given the increasing prevalence of\nmisinformation, our research utilizes the TweepFake dataset to train and\nevaluate various machine learning models. The objective is to identify\neffective strategies for recognizing DeepFake content, thereby enhancing the\nintegrity of digital communications. By developing reliable methods for\ndetecting AI-generated misinformation, this work contributes to a more\ntrustworthy online information environment.", "published": "2024-06-24 09:55:31", "link": "http://arxiv.org/abs/2406.16489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task\n  in Civil Procedure", "abstract": "This study investigates the performance of the zero-shot method in\nclassifying data using three large language models, alongside two models with\nlarge input token sizes and the two pre-trained models on legal data. Our main\ndataset comes from the domain of U.S. civil procedure. It includes summaries of\nlegal cases, specific questions, potential answers, and detailed explanations\nfor why each solution is relevant, all sourced from a book aimed at law\nstudents. By comparing different methods, we aimed to understand how\neffectively they handle the complexities found in legal datasets. Our findings\nshow how well the zero-shot method of large language models can understand\ncomplicated data. We achieved our highest F1 score of 64% in these experiments.", "published": "2024-06-24 09:57:44", "link": "http://arxiv.org/abs/2406.16490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Vocabulary Size Improves Large Language Models", "abstract": "This paper empirically investigates the relationship between subword\nvocabulary size and the performance of large language models (LLMs) to provide\ninsights on how to define the vocabulary size. Experimental results show that\nlarger vocabulary sizes lead to better performance in LLMs. Moreover, we\nconsider a continual training scenario where a pre-trained language model is\ntrained on a different target language. We introduce a simple method to use a\nnew vocabulary instead of the pre-defined one. We show that using the new\nvocabulary outperforms the model with the vocabulary used in pre-training.", "published": "2024-06-24 10:27:07", "link": "http://arxiv.org/abs/2406.16508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Privileged Students: On the Value of Initialization in Multilingual\n  Knowledge Distillation", "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of smaller models in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model\ninitialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our findings show that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation\nprocess itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.", "published": "2024-06-24 10:59:26", "link": "http://arxiv.org/abs/2406.16524v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions", "abstract": "We investigate the abilities of a representative set of Large language Models\n(LLMs) to reason about cardinal directions (CDs). To do so, we create two\ndatasets: the first, co-created with ChatGPT, focuses largely on recall of\nworld knowledge about CDs; the second is generated from a set of templates,\ncomprehensively testing an LLM's ability to determine the correct CD given a\nparticular scenario. The templates allow for a number of degrees of variation\nsuch as means of locomotion of the agent involved, and whether set in the first\n, second or third person. Even with a temperature setting of zero, Our\nexperiments show that although LLMs are able to perform well in the simpler\ndataset, in the second more complex dataset no LLM is able to reliably\ndetermine the correct CD, even with a temperature setting of zero.", "published": "2024-06-24 11:07:01", "link": "http://arxiv.org/abs/2406.16528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Graph-based Cross-document Relation Extraction via\n  Non-bridge Entity Enhancement and Prediction Debiasing", "abstract": "Cross-document Relation Extraction aims to predict the relation between\ntarget entities located in different documents. In this regard, the dominant\nmodels commonly retain useful information for relation prediction via bridge\nentities, which allows the model to elaborately capture the intrinsic\ninterdependence between target entities. However, these studies ignore the\nnon-bridge entities, each of which co-occurs with only one target entity and\noffers the semantic association between target entities for relation\nprediction. Besides, the commonly-used dataset--CodRED contains substantial NA\ninstances, leading to the prediction bias during inference. To address these\nissues, in this paper, we propose a novel graph-based cross-document RE model\nwith non-bridge entity enhancement and prediction debiasing. Specifically, we\nuse a unified entity graph to integrate numerous non-bridge entities with\ntarget entities and bridge entities, modeling various associations between\nthem, and then use a graph recurrent network to encode this graph. Finally, we\nintroduce a novel debiasing strategy to calibrate the original prediction\ndistribution. Experimental results on the closed and open settings show that\nour model significantly outperforms all baselines, including the GPT-3.5-turbo\nand InstructUIE, achieving state-of-the-art performance. Particularly, our\nmodel obtains 66.23% and 55.87% AUC points in the official\nleaderboard\\footnote{\\url{https://codalab.lisn.upsaclay.fr/competitions/3770#results}}\nunder the two settings, respectively, ranking the first place in all\nsubmissions since December 2023. Our code is available at\nhttps://github.com/DeepLearnXMU/CoRE-NEPD.", "published": "2024-06-24 11:08:28", "link": "http://arxiv.org/abs/2406.16529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-LLM: Learn to Check Chinese Spelling Errors Character by Character", "abstract": "Chinese Spell Checking (CSC) aims to detect and correct spelling errors in\nsentences. Despite Large Language Models (LLMs) exhibit robust capabilities and\nare widely applied in various tasks, their performance on CSC is often\nunsatisfactory. We find that LLMs fail to meet the Chinese character-level\nconstraints of the CSC task, namely equal length and phonetic similarity,\nleading to a performance bottleneck. Further analysis reveal that this issue\nstems from the granularity of tokenization, as current mixed character-word\ntokenization struggles to satisfy these character-level constraints. To address\nthis issue, we propose C-LLM, a Large Language Model-based Chinese Spell\nChecking method that learns to check errors Character by Character.\nCharacter-level tokenization enables the model to learn character-level\nalignment, effectively mitigating issues related to character-level\nconstraints. Furthermore, CSC is simplified to replication-dominated and\nsubstitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate\nthat C-LLM achieves an average improvement of 10% over existing methods.\nSpecifically, it shows a 2.1% improvement in general scenarios and a\nsignificant 12% improvement in vertical domain scenarios, establishing\nstate-of-the-art performance. The source code can be accessed at\nhttps://github.com/ktlKTL/C-LLM.", "published": "2024-06-24 11:16:31", "link": "http://arxiv.org/abs/2406.16536v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual\n  Pre-training", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising\nframework for scaling up large language models (LLMs). However, training MoE\nfrom scratch in a large-scale setting still suffers from data-hungry and\ninstability problems. Motivated by this limit, we investigate building MoE\nmodels from existing dense large language models. Specifically, based on the\nwell-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert\nConstruction, which partitions the parameters of original Feed-Forward Networks\n(FFNs) into multiple experts; (2) Continual Pre-training, which further trains\nthe transformed MoE model and additional gate networks. In this paper, we\ncomprehensively explore different methods for expert construction and various\ndata sampling strategies for continual pre-training. After these stages, our\nLLaMA-MoE models could maintain language abilities and route the input tokens\nto specific experts with part of the parameters activated. Empirically, by\ntraining 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense\nmodels that contain similar activation parameters. The source codes and models\nare available at https://github.com/pjlab-sys4nlp/llama-moe .", "published": "2024-06-24 11:43:07", "link": "http://arxiv.org/abs/2406.16554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are there identifiable structural parts in the sentence embedding whole?", "abstract": "Sentence embeddings from transformer models encode in a fixed length vector\nmuch linguistic information. We explore the hypothesis that these embeddings\nconsist of overlapping layers of information that can be separated, and on\nwhich specific types of information -- such as information about chunks and\ntheir structural and semantic properties -- can be detected. We show that this\nis the case using a dataset consisting of sentences with known chunk structure,\nand two linguistic intelligence datasets, solving which relies on detecting\nchunks and their grammatical number, and respectively, their semantic roles,\nand through analyses of the performance on the tasks and of the internal\nrepresentations built during learning.", "published": "2024-06-24 11:58:33", "link": "http://arxiv.org/abs/2406.16563v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Data Augmentation of Multi-turn Psychological Dialogue via\n  Knowledge-driven Progressive Thought Prompting", "abstract": "Existing dialogue data augmentation (DA) techniques predominantly focus on\naugmenting utterance-level dialogues, which makes it difficult to take dialogue\ncontextual information into account. The advent of large language models (LLMs)\nhas simplified the implementation of multi-turn dialogues. Due to absence of\nprofessional understanding and knowledge, it remains challenging to deliver\nsatisfactory performance in low-resource domain, like psychological dialogue\ndialogue. DA involves creating new training or prompting data based on the\nexisting data, which help the model better understand and generate\npsychology-related responses. In this paper, we aim to address the issue of\nmulti-turn dialogue data augmentation for boosted performance in the psychology\ndomain. We propose a knowledge-driven progressive thought prompting method to\nguide LLM to generate multi-turn psychology-related dialogue. This method\nintegrates a progressive thought generator, a psychology knowledge generator,\nand a multi-turn dialogue generator. The thought generated by the progressive\nthought generator serves as a prompt to prevent the generated dialogue from\nhaving significant semantic deviations, while the psychology knowledge\ngenerator produces psychological knowledge to serve as the dialogue history for\nthe LLM, guiding the dialogue generator to create multi-turn psychological\ndialogue. To ensure the precision of multi-turn psychological dialogue\ngeneration by LLM, a meticulous professional evaluation is required. Extensive\nexperiments conducted on three datasets related to psychological dialogue\nverify the effectiveness of the proposed method.", "published": "2024-06-24 12:02:56", "link": "http://arxiv.org/abs/2406.16567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Cross-Lingual Knowledge-Free Reasoners", "abstract": "Large Language Models have demonstrated impressive reasoning capabilities\nacross multiple languages. However, the relationship between capabilities in\ndifferent languages is less explored. In this work, we decompose the process of\nreasoning tasks into two separated components: knowledge retrieval and\nknowledge-free reasoning, and analyze the relationship between cross-lingual\ntransferability and these two components. With adapted commonsense reasoning\ndatasets and constructed knowledge-free reasoning datasets, we show that the\nknowledge-free reasoning capability can be nearly perfectly transferred across\nvarious source-target language directions despite the secondary impact of\nresource in some specific target languages, while cross-lingual knowledge\nretrieval significantly hinders the transfer. Moreover, by analyzing the hidden\nstates and feed-forward network neuron activation during the reasoning, we show\nthat higher similarity of hidden representations and larger overlap of\nactivated neurons could explain the better cross-lingual transferability of\nknowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that\nknowledge-free reasoning shares similar neurons in different languages for\nreasoning, while knowledge is stored separately in different languages. Our\ncode and data is available at:\nhttps://github.com/NJUNLP/Knowledge-Free-Reasoning.", "published": "2024-06-24 14:03:04", "link": "http://arxiv.org/abs/2406.16655v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Approaches to the Detection of Lesser-Known Rhetorical\n  Figures: A Systematic Survey and Research Challenges", "abstract": "Rhetorical figures play a major role in our everyday communication as they\nmake text more interesting, more memorable, or more persuasive. Therefore, it\nis important to computationally detect rhetorical figures to fully understand\nthe meaning of a text. We provide a comprehensive overview of computational\napproaches to lesser-known rhetorical figures. We explore the linguistic and\ncomputational perspectives on rhetorical figures, emphasizing their\nsignificance for the domain of Natural Language Processing. We present\ndifferent figures in detail, delving into datasets, definitions, rhetorical\nfunctions, and detection approaches. We identified challenges such as dataset\nscarcity, language limitations, and reliance on rule-based methods.", "published": "2024-06-24 14:31:34", "link": "http://arxiv.org/abs/2406.16674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Linear Complexity Language Models", "abstract": "The interest in linear complexity models for large language models is on the\nrise, although their scaling capacity remains uncertain. In this study, we\npresent the scaling laws for linear complexity language models to establish a\nfoundation for their scalability. Specifically, we examine the scaling\nbehaviors of three efficient linear architectures. These include TNL, a linear\nattention model with data-independent decay; HGRN2, a linear RNN with\ndata-dependent decay; and cosFormer2, a linear attention model without decay.\nWe also include LLaMA as a baseline architecture for softmax attention for\ncomparison. These models were trained with six variants, ranging from 70M to 7B\nparameters on a 300B-token corpus, and evaluated with a total of 1,376\nintermediate checkpoints on various downstream tasks. These tasks include\nvalidation loss, commonsense reasoning, and information retrieval and\ngeneration. The study reveals that existing linear complexity language models\nexhibit similar scaling capabilities as conventional transformer-based models\nwhile also demonstrating superior linguistic proficiency and knowledge\nretention.", "published": "2024-06-24 14:51:31", "link": "http://arxiv.org/abs/2406.16690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task Oriented In-Domain Data Augmentation", "abstract": "Large Language Models (LLMs) have shown superior performance in various\napplications and fields. To achieve better performance on specialized domains\nsuch as law and advertisement, LLMs are often continue pre-trained on in-domain\ndata. However, existing approaches suffer from two major issues. First,\nin-domain data are scarce compared with general domain-agnostic data. Second,\ndata used for continual pre-training are not task-aware, such that they may not\nbe helpful to downstream applications. We propose TRAIT, a task-oriented\nin-domain data augmentation framework. Our framework is divided into two parts:\nin-domain data selection and task-oriented synthetic passage generation. The\ndata selection strategy identifies and selects a large amount of in-domain data\nfrom general corpora, and thus significantly enriches domain knowledge in the\ncontinual pre-training data. The synthetic passages contain guidance on how to\nuse domain knowledge to answer questions about downstream tasks. By training on\nsuch passages, the model aligns with the need of downstream applications. We\nadapt LLMs to two domains: advertisement and math. On average, TRAIT improves\nLLM performance by 8% in the advertisement domain and 7.5% in the math domain.", "published": "2024-06-24 14:58:11", "link": "http://arxiv.org/abs/2406.16694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Venturing into Uncharted Waters: The Navigation Compass from Transformer\n  to Mamba", "abstract": "Transformer, a deep neural network architecture, has long dominated the field\nof natural language processing and beyond. Nevertheless, the recent\nintroduction of Mamba challenges its supremacy, sparks considerable interest\namong researchers, and gives rise to a series of Mamba-based models that have\nexhibited notable potential. This survey paper orchestrates a comprehensive\ndiscussion, diving into essential research dimensions, covering: (i) the\nfunctioning of the Mamba mechanism and its foundation on the principles of\nstructured state space models; (ii) the proposed improvements and the\nintegration of Mamba with various networks, exploring its potential as a\nsubstitute for Transformers; (iii) the combination of Transformers and Mamba to\ncompensate for each other's shortcomings. We have also made efforts to\ninterpret Mamba and Transformer in the framework of kernel functions, allowing\nfor a comparison of their mathematical nature within a unified context. Our\npaper encompasses the vast majority of improvements related to Mamba to date.", "published": "2024-06-24 15:27:21", "link": "http://arxiv.org/abs/2406.16722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLIMATELI: Evaluating Entity Linking on Climate Change Data", "abstract": "Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.", "published": "2024-06-24 15:36:00", "link": "http://arxiv.org/abs/2406.16732v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large\n  Language Models via Opposite Prompt Optimization", "abstract": "With the widespread application of Large Language Models (LLMs), it has\nbecome a significant concern to ensure their safety and prevent harmful\nresponses. While current safe-alignment methods based on instruction\nfine-tuning and Reinforcement Learning from Human Feedback (RLHF) can\neffectively reduce harmful responses from LLMs, they often require high-quality\ndatasets and heavy computational overhead during model training. Another way to\nalign language models is to modify the logit of tokens in model outputs without\nheavy training. Recent studies have shown that contrastive decoding can enhance\nthe performance of language models by reducing the likelihood of confused\ntokens. However, these methods require the manual selection of contrastive\nmodels or instruction templates. To this end, we propose Adversarial\nContrastive Decoding (ACD), an optimization-based framework to generate two\nopposite system prompts for prompt-based contrastive decoding. ACD only needs\nto apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min\nfor each model) without training the target model. Experiments conducted on\nextensive models and benchmarks demonstrate that the proposed method achieves\nmuch better safety performance than previous model training-free decoding\nmethods without sacrificing its original generation ability.", "published": "2024-06-24 15:51:30", "link": "http://arxiv.org/abs/2406.16743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters", "abstract": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which is\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup in inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation.", "published": "2024-06-24 16:06:50", "link": "http://arxiv.org/abs/2406.16758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The GPT-WritingPrompts Dataset: A Comparative Analysis of Character\n  Portrayal in Short Stories", "abstract": "The improved generative capabilities of large language models have made them\na powerful tool for creative writing and storytelling. It is therefore\nimportant to quantitatively understand the nature of generated stories, and how\nthey differ from human storytelling. We augment the Reddit WritingPrompts\ndataset with short stories generated by GPT-3.5, given the same prompts. We\nquantify and compare the emotional and descriptive features of storytelling\nfrom both generative processes, human and machine, along a set of six\ndimensions. We find that generated stories differ significantly from human\nstories along all six dimensions, and that human and machine generations\ndisplay similar biases when grouped according to the narrative point-of-view\nand gender of the main protagonist. We release our dataset and code at\nhttps://github.com/KristinHuangg/gpt-writing-prompts.", "published": "2024-06-24 16:24:18", "link": "http://arxiv.org/abs/2406.16767v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Transformer Circuits with Edge Pruning", "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.", "published": "2024-06-24 16:40:54", "link": "http://arxiv.org/abs/2406.16778v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It Is Not About What You Say, It Is About How You Say It: A Surprisingly\n  Simple Approach for Improving Reading Comprehension", "abstract": "Natural language processing has seen rapid progress over the past decade. Due\nto the speed of developments, some practices get established without proper\nevaluation. Considering one such case and focusing on reading comprehension, we\nask our first research question: 1) How does the order of inputs -- i.e.,\nquestion and context -- affect model performance? Additionally, given recent\nadvancements in input emphasis, we ask a second research question: 2) Does\nemphasizing either the question, the context, or both enhance performance?\nExperimenting with 9 large language models across 3 datasets, we find that\npresenting the context before the question improves model performance, with an\naccuracy increase of up to $31\\%$. Furthermore, emphasizing the context yields\nsuperior results compared to question emphasis, and in general, emphasizing\nparts of the input is particularly effective for addressing questions that\nmodels lack the parametric knowledge to answer. Experimenting with both\nprompt-based and attention-based emphasis methods, we additionally find that\nthe best method is surprisingly simple: it only requires concatenating a few\ntokens to the input and results in an accuracy improvement of up to $36\\%$,\nallowing smaller models to outperform their significantly larger counterparts.", "published": "2024-06-24 16:43:11", "link": "http://arxiv.org/abs/2406.16779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RES-Q: Evaluating Code-Editing Large Language Model Systems at the\n  Repository Scale", "abstract": "The instruction-following ability of Large Language Models (LLMs) has\ncultivated a class of LLM-based systems capable of approaching complex tasks\nsuch as making edits to large code repositories. Due to the high sensitivity\nand unpredictability of LLM behavior in response to changes in prompting,\nrobust evaluation tools are needed to drive future iteration of these systems.\nWe propose RES-Q, a natural language instruction-based benchmark for evaluating\n$\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of\n100 handcrafted repository editing tasks derived from real GitHub commits.\nGiven an edit instruction and a code repository, RES-Q evaluates an LLM\nsystem's ability to interpret the instruction, navigate the repository to\ngather relevant information, and construct an appropriate edit that satisfies\nthe specified criteria. We argue that evaluating LLMs in this way addresses\nissues with traditional benchmarks and provides a more holistic assessment of a\nmodel's abilities. We evaluate various state-of-the-art LLMs as language agents\nin a repository-editing system built on Qurrent OS, our language agent\ndevelopment software. Despite their 1% pass@1 performance difference on\nHumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q,\nindicating RES-Q's capacity to differentiate model capability as traditional\nbenchmarks approach saturation. We further investigate token efficiency,\nperformance relationships with existing benchmarks, and interesting disparities\nbetween closed and open-source LLMs. Code and dataset are available at\nhttps://github.com/Qurrent-AI/RES-Q.", "published": "2024-06-24 17:08:17", "link": "http://arxiv.org/abs/2406.16801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Factual Entailment with NLI: A News Media Study", "abstract": "We explore the relationship between factuality and Natural Language Inference\n(NLI) by introducing FactRel -- a novel annotation scheme that models\n\\textit{factual} rather than \\textit{textual} entailment, and use it to\nannotate a dataset of naturally occurring sentences from news articles. Our\nanalysis shows that 84\\% of factually supporting pairs and 63\\% of factually\nundermining pairs do not amount to NLI entailment or contradiction,\nrespectively, suggesting that factual relationships are more apt for analyzing\nmedia discourse. We experiment with models for pairwise classification on the\nnew dataset, and find that in some cases, generating synthetic data with GPT-4\non the basis of the annotated dataset can improve performance. Surprisingly,\nfew-shot learning with GPT-4 yields strong results on par with medium LMs\n(DeBERTa) trained on the labelled dataset. We hypothesize that these results\nindicate the fundamental dependence of this task on both world knowledge and\nadvanced reasoning abilities.", "published": "2024-06-24 17:47:55", "link": "http://arxiv.org/abs/2406.16842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RaTEScore: A Metric for Radiology Report Generation", "abstract": "This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.", "published": "2024-06-24 17:49:28", "link": "http://arxiv.org/abs/2406.16845v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "modeLing: A Novel Dataset for Testing Linguistic Reasoning in Language\n  Models", "abstract": "We introduce modeLing, a novel benchmark of Linguistics Olympiad-style\npuzzles which tests few-shot reasoning in AI systems. Solving these puzzles\nnecessitates inferring aspects of a language's grammatical structure from a\nsmall number of examples. Such puzzles provide a natural testbed for language\nmodels, as they require compositional generalization and few-shot inductive\nreasoning. Consisting solely of new puzzles written specifically for this work,\nmodeLing has no risk of appearing in the training data of existing AI systems:\nthis ameliorates the risk of data leakage, a potential confounder for many\nprior evaluations of reasoning. Evaluating several large open source language\nmodels and GPT on our benchmark, we observe non-negligible accuracy,\ndemonstrating few-shot emergent reasoning ability which cannot merely be\nattributed to shallow memorization. However, imperfect model performance\nsuggests that modeLing can be used to measure further progress in linguistic\nreasoning.", "published": "2024-06-24 18:00:59", "link": "http://arxiv.org/abs/2406.17038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Instruction: Amplifying Attention in the Middle via Prompting", "abstract": "The context window of large language models has been extended to 128k tokens\nor more. However, language models still suffer from position bias and have\ndifficulty in accessing and using the middle part of the context due to the\nlack of attention. We examine the relative position awareness of LLMs and the\nfeasibility of mitigating disproportional attention through prompting. We\naugment the original task instruction with $\\texttt{attention instructions}$\nthat direct language models to allocate more attention towards a selected\nsegment of the context. We conduct a comprehensive investigation on\nmulti-document question answering task with both position-based and index-based\ninstructions. We find that language models do not have relative position\nawareness of the context. Nevertheless, they demonstrate the capacity to adapt\nattention to a specific segment using matching indexes. Our analysis\ncontributes to a deeper understanding of position bias in LLMs and provides a\npathway to mitigate this bias by instruction, thus benefiting LLMs in locating\nand utilizing relevant information from retrieved documents in RAG\napplications.", "published": "2024-06-24 19:35:11", "link": "http://arxiv.org/abs/2406.17095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Adversarial Discovery for Safety Classifiers", "abstract": "Safety classifiers are critical in mitigating toxicity on online forums such\nas social media and in chatbots. Still, they continue to be vulnerable to\nemergent, and often innumerable, adversarial attacks. Traditional automated\nadversarial data generation methods, however, tend to produce attacks that are\nnot diverse, but variations of previously observed harm types. We formalize the\ntask of automated adversarial discovery for safety classifiers - to find new\nattacks along previously unseen harm dimensions that expose new weaknesses in\nthe classifier. We measure progress on this task along two key axes (1)\nadversarial success: does the attack fool the classifier? and (2) dimensional\ndiversity: does the attack represent a previously unseen harm type? Our\nevaluation of existing attack generation methods on the CivilComments toxicity\ntask reveals their limitations: Word perturbation attacks fail to fool\nclassifiers, while prompt-based LLM attacks have more adversarial success, but\nlack dimensional diversity. Even our best-performing prompt-based method finds\nnew successful attacks on unseen harm dimensions of attacks only 5\\% of the\ntime. Automatically finding new harmful dimensions of attack is crucial and\nthere is substantial headroom for future research on our new task.", "published": "2024-06-24 19:45:12", "link": "http://arxiv.org/abs/2406.17104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vaporetto: Efficient Japanese Tokenization Based on Improved Pointwise\n  Linear Classification", "abstract": "This paper proposes an approach to improve the runtime efficiency of Japanese\ntokenization based on the pointwise linear classification (PLC) framework,\nwhich formulates the whole tokenization process as a sequence of linear\nclassification problems. Our approach optimizes tokenization by leveraging the\ncharacteristics of the PLC framework and the task definition. Our approach\ninvolves (1) composing multiple classifications into array-based operations,\n(2) efficient feature lookup with memory-optimized automata, and (3) three\northogonal pre-processing methods for reducing actual score calculation. Thus,\nour approach makes the tokenization speed 5.7 times faster than the current\napproach based on the same model without decreasing tokenization accuracy. Our\nimplementation is available at https://github.com/daac-tools/vaporetto under\nthe MIT or Apache-2.0 license.", "published": "2024-06-24 23:47:20", "link": "http://arxiv.org/abs/2406.17185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models", "abstract": "Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.", "published": "2024-06-24 02:03:57", "link": "http://arxiv.org/abs/2406.16264v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Supervised Learning and Reinforcement Learning for Multi-Label\n  Classification Tasks with Partial Labels", "abstract": "Traditional supervised learning heavily relies on human-annotated datasets,\nespecially in data-hungry neural approaches. However, various tasks, especially\nmulti-label tasks like document-level relation extraction, pose challenges in\nfully manual annotation due to the specific domain knowledge and large class\nsets. Therefore, we address the multi-label positive-unlabelled learning\n(MLPUL) problem, where only a subset of positive classes is annotated. We\npropose Mixture Learner for Partially Annotated Classification (MLPAC), an\nRL-based framework combining the exploration ability of reinforcement learning\nand the exploitation ability of supervised learning. Experimental results\nacross various tasks, including document-level relation extraction, multi-label\nimage classification, and binary PU learning, demonstrate the generalization\nand effectiveness of our framework.", "published": "2024-06-24 03:36:19", "link": "http://arxiv.org/abs/2406.16293v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LangSuitE: Planning, Controlling and Interacting with Large Language\n  Models in Embodied Text Environments", "abstract": "Recent advances in Large Language Models (LLMs) have shown inspiring\nachievements in constructing autonomous agents that rely on language\ndescriptions as inputs. However, it remains unclear how well LLMs can function\nas few-shot or zero-shot embodied agents in dynamic interactive environments.\nTo address this gap, we introduce LangSuitE, a versatile and simulation-free\ntestbed featuring 6 representative embodied tasks in textual embodied worlds.\nCompared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to\ndiverse environments without multiple simulation engines, (ii) evaluates\nagents' capacity to develop ``internalized world knowledge'' with embodied\nobservations, and (iii) allows easy customization of communication and action\nstrategies. To address the embodiment challenge, we devise a novel\nchain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.\nhistory information. Comprehensive benchmark results illustrate challenges and\ninsights of embodied planning. LangSuitE represents a significant step toward\nbuilding embodied generalists in the context of language models.", "published": "2024-06-24 03:36:29", "link": "http://arxiv.org/abs/2406.16294v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compensate Quantization Errors: Make Weights Hierarchical to Compensate\n  Each Other", "abstract": "Emergent Large Language Models (LLMs) use their extraordinary performance and\npowerful deduction capacity to discern from traditional language models.\nHowever, the expenses of computational resources and storage for these LLMs are\nstunning, quantization then arises as a trending conversation. To address\naccuracy decay caused by quantization, two streams of works in post-training\nquantization methods stand out. One uses other weights to compensate existing\nquantization error, while the other transfers the quantization difficulty to\nother parts in the model. Combining both merits, we introduce Learnable\nSingular value Increment (LSI) as an advanced solution. LSI uses Singular Value\nDecomposition to extract singular values of the weights and make them learnable\nto help weights compensate each other conditioned on activation. Incorporating\nLSI with existing techniques, we achieve state-of-the-art performance in\ndiverse quantization settings, no matter in weight-only, weight-activation or\nextremely low bit scenarios. By unleashing the potential of LSI, efficient\nfinetuning on quantized model is no longer a prohibitive problem.", "published": "2024-06-24 03:52:52", "link": "http://arxiv.org/abs/2406.16299v1", "categories": ["cs.CL", "cs.AI", "F.2.3"], "primary_category": "cs.CL"}
{"title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer\n  Merging", "abstract": "While large language models (LLMs) excel in many domains, their complexity\nand scale challenge deployment in resource-limited environments. Current\ncompression techniques, such as parameter pruning, often fail to effectively\nutilize the knowledge from pruned parameters. To address these challenges, we\npropose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA),\na novel approach that uses manifold learning and the Normalized Pairwise\nInformation Bottleneck (NPIB) measure to merge similar layers, reducing model\nsize while preserving essential performance. We evaluate MKA on multiple\nbenchmark datasets and various LLMs. Our findings show that MKA not only\npreserves model performance but also achieves substantial compression ratios,\noutperforming traditional pruning methods. Moreover, when coupled with\nquantization, MKA delivers even greater compression. Specifically, on the MMLU\ndataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75%\nwith a minimal performance decrease of only 2.82\\%. The proposed MKA method\noffers a resource-efficient and performance-preserving model compression\ntechnique for LLMs.", "published": "2024-06-24 05:57:55", "link": "http://arxiv.org/abs/2406.16330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task", "abstract": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability.", "published": "2024-06-24 06:10:13", "link": "http://arxiv.org/abs/2406.16332v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On the Transformations across Reward Model, Parameter Update, and\n  In-Context Prompt", "abstract": "Despite the general capabilities of pre-trained large language models (LLMs),\nthey still need further adaptation to better serve practical applications. In\nthis paper, we demonstrate the interchangeability of three popular and distinct\nadaptation tools: parameter updating, reward modeling, and in-context\nprompting. This interchangeability establishes a triangular framework with six\ntransformation directions, each of which facilitates a variety of applications.\nOur work offers a holistic view that unifies numerous existing studies and\nsuggests potential research directions. We envision our work as a useful\nroadmap for future research on LLMs.", "published": "2024-06-24 07:42:32", "link": "http://arxiv.org/abs/2406.16377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Symmetry Property of Christoffel Words", "abstract": "Motivated by the theory of trapezoidal words, whose sequences of cardinality\nof factors by length are symmetric, we introduce a bivariate variant of this\nsymmetry. We show that this symmetry characterizes Christoffel words, and\nestablish other related results.", "published": "2024-06-24 08:05:21", "link": "http://arxiv.org/abs/2406.16408v1", "categories": ["math.CO", "cs.CL"], "primary_category": "math.CO"}
{"title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration", "abstract": "To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\nK-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset\nreveal that open-source models lag behind proprietary ones in understanding\nKorean culture, highlighting key areas for improvement. We also present a\nseries of further analyses, including human evaluation, augmenting VLMs with\nexternal knowledge, and the evaluation beyond multiple-choice QA. Our dataset\nis available at https://huggingface.co/datasets/ddehun/k-viscuit.", "published": "2024-06-24 09:18:15", "link": "http://arxiv.org/abs/2406.16469v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World\n  Image Super-Resolution", "abstract": "Image super-resolution pursuits reconstructing high-fidelity high-resolution\ncounterpart for low-resolution image. In recent years, diffusion-based models\nhave garnered significant attention due to their capabilities with rich prior\nknowledge. The success of diffusion models based on general text prompts has\nvalidated the effectiveness of textual control in the field of text2image.\nHowever, given the severe degradation commonly presented in low-resolution\nimages, coupled with the randomness characteristics of diffusion models,\ncurrent models struggle to adequately discern semantic and degradation\ninformation within severely degraded images. This often leads to obstacles such\nas semantic loss, visual artifacts, and visual hallucinations, which pose\nsubstantial challenges for practical use. To address these challenges, this\npaper proposes to leverage degradation-aligned language prompt for accurate,\nfine-grained, and high-fidelity image restoration. Complementary priors\nincluding semantic content descriptions and degradation prompts are explored.\nSpecifically, on one hand, image-restoration prompt alignment decoder is\nproposed to automatically discern the degradation degree of LR images, thereby\ngenerating beneficial degradation priors for image restoration. On the other\nhand, much richly tailored descriptions from pretrained multimodal large\nlanguage model elicit high-level semantic priors closely aligned with human\nperception, ensuring fidelity control for image restoration. Comprehensive\ncomparisons with state-of-the-art methods have been done on several popular\nsynthetic and real-world benchmark datasets. The quantitative and qualitative\nanalysis have demonstrated that the proposed method achieves a new\nstate-of-the-art perceptual quality level. Related source codes and pre-trained\nparameters were public in https://github.com/puppy210/DaLPSR.", "published": "2024-06-24 09:30:36", "link": "http://arxiv.org/abs/2406.16477v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to\n  construct Observer-Thinker-Conceiver-Expresser", "abstract": "Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.", "published": "2024-06-24 10:05:23", "link": "http://arxiv.org/abs/2406.16495v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Carrot and Stick: Inducing Self-Motivation with Positive & Negative\n  Feedback", "abstract": "Positive thinking is thought to be an important component of self-motivation\nin various practical fields such as education and the workplace. Previous work,\nincluding sentiment transfer and positive reframing, has focused on the\npositive side of language. However, self-motivation that drives people to reach\ntheir goals has not yet been studied from a computational perspective.\nMoreover, negative feedback has not yet been explored, even though positive and\nnegative feedback are both necessary to grow self-motivation. To facilitate\nself-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of\n12,590 sentences with 5 different strategies for enhancing self-motivation. Our\ndata and code are publicly available at here.", "published": "2024-06-24 10:55:31", "link": "http://arxiv.org/abs/2406.16521v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models", "abstract": "The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment.", "published": "2024-06-24 11:56:15", "link": "http://arxiv.org/abs/2406.16562v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluation of Language Models in the Medical Context Under\n  Resource-Constrained Settings", "abstract": "Since the Transformer architecture emerged, language model development has\ngrown, driven by their promising potential. Releasing these models into\nproduction requires properly understanding their behavior, particularly in\nsensitive domains like medicine. Despite this need, the medical literature\nstill lacks practical assessment of pre-trained language models, which are\nespecially valuable in settings where only consumer-grade computational\nresources are available. To address this gap, we have conducted a comprehensive\nsurvey of language models in the medical field and evaluated a subset of these\nfor medical text classification and conditional text generation. The subset\nincludes 53 models with 110 million to 13 billion parameters, spanning the\nTransformer-based model families and knowledge domains. Different approaches\nare employed for text classification, including zero-shot learning, enabling\ntuning without the need to train the model. These approaches are helpful in our\ntarget settings, where many users of language models find themselves. The\nresults reveal remarkable performance across the tasks and datasets evaluated,\nunderscoring the potential of certain models to contain medical knowledge, even\nwithout domain specialization. This study thus advocates for further\nexploration of model applications in medical contexts, particularly in\ncomputational resource-constrained settings, to benefit a wide range of users.\nThe code is available on https://github.com/anpoc/Language-models-in-medicine.", "published": "2024-06-24 12:52:02", "link": "http://arxiv.org/abs/2406.16611v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding\n  with Task Divide-and-Conquer", "abstract": "Recent advancements in Large Language Models (LLMs) have expanded their\ncapabilities to multimodal contexts, including comprehensive video\nunderstanding. However, processing extensive videos such as 24-hour CCTV\nfootage or full-length films presents significant challenges due to the vast\ndata and processing demands. Traditional methods, like extracting key frames or\nconverting frames to text, often result in substantial information loss. To\naddress these shortcomings, we develop OmAgent, efficiently stores and\nretrieves relevant video frames for specific queries, preserving the detailed\ncontent of videos. Additionally, it features an Divide-and-Conquer Loop capable\nof autonomous reasoning, dynamically invoking APIs and tools to enhance query\nprocessing and accuracy. This approach ensures robust video understanding,\nsignificantly reducing information loss. Experimental results affirm OmAgent's\nefficacy in handling various types of videos and complex tasks. Moreover, we\nhave endowed it with greater autonomy and a robust tool-calling system,\nenabling it to accomplish even more intricate tasks.", "published": "2024-06-24 13:05:39", "link": "http://arxiv.org/abs/2406.16620v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CAVE: Controllable Authorship Verification Explanations", "abstract": "Authorship Verification (AV) (do two documents have the same author?) is\nessential in many real-life applications. AV is often used in privacy-sensitive\ndomains that require an offline proprietary model that is deployed on premises,\nmaking publicly served online models (APIs) a suboptimal choice. Current\noffline AV models however have lower downstream utility due to limited accuracy\n(eg: traditional stylometry AV systems) and lack of accessible post-hoc\nexplanations. In this work, we address the above challenges by developing a\ntrained, offline model CAVE (Controllable Authorship Verification\nExplanations). CAVE generates free-text AV explanations that are controlled to\nbe (1) accessible (uniform structure that can be decomposed into\nsub-explanations grounded to relevant linguistic features), and (2) easily\nverified for explanation-label consistency. We generate silver-standard\ntraining data grounded to the desirable linguistic features by a prompt-based\nmethod Prompt-CAVE. We then filter the data based on rationale-label\nconsistency using a novel metric Cons-R-L. Finally, we fine-tune a small,\noffline model (Llama-3-8B) with this data to create our model CAVE. Results on\nthree difficult AV datasets show that CAVE generates high quality explanations\n(as measured by automatic and human evaluation) as well as competitive task\naccuracy.", "published": "2024-06-24 14:27:54", "link": "http://arxiv.org/abs/2406.16672v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers", "abstract": "Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.", "published": "2024-06-24 15:55:59", "link": "http://arxiv.org/abs/2406.16747v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OCALM: Object-Centric Assessment with Language Models", "abstract": "Properly defining a reward signal to efficiently train a reinforcement\nlearning (RL) agent is a challenging task. Designing balanced objective\nfunctions from which a desired behavior can emerge requires expert knowledge,\nespecially for complex environments. Learning rewards from human feedback or\nusing large language models (LLMs) to directly provide rewards are promising\nalternatives, allowing non-experts to specify goals for the agent. However,\nblack-box reward models make it difficult to debug the reward. In this work, we\npropose Object-Centric Assessment with Language Models (OCALM) to derive\ninherently interpretable reward functions for RL agents from natural language\ntask descriptions. OCALM uses the extensive world-knowledge of LLMs while\nleveraging the object-centric nature common to many environments to derive\nreward functions focused on relational concepts, providing RL agents with the\nability to derive policies from task descriptions.", "published": "2024-06-24 15:57:48", "link": "http://arxiv.org/abs/2406.16748v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?", "abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).", "published": "2024-06-24 16:31:12", "link": "http://arxiv.org/abs/2406.16772v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech\n  Translation System for IWSLT 2024", "abstract": "Large Language Models (LLMs) are currently under exploration for various\ntasks, including Automatic Speech Recognition (ASR), Machine Translation (MT),\nand even End-to-End Speech Translation (ST). In this paper, we present KIT's\noffline submission in the constrained + LLM track by incorporating recently\nproposed techniques that can be added to any cascaded speech translation.\nSpecifically, we integrate\nMistral-7B\\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to\nenhance it in two ways. Firstly, we refine the ASR outputs by utilizing the\nN-best lists generated by our system and fine-tuning the LLM to predict the\ntranscript accurately. Secondly, we refine the MT outputs at the document level\nby fine-tuning the LLM, leveraging both ASR and MT predictions to improve\ntranslation quality. We find that integrating the LLM into the ASR and MT\nsystems results in an absolute improvement of $0.3\\%$ in Word Error Rate and\n$0.65\\%$ in COMET for tst2019 test set. In challenging test sets with\noverlapping speakers and background noise, we find that integrating LLM is not\nbeneficial due to poor ASR performance. Here, we use ASR with chunked long-form\ndecoding to improve context usage that may be unavailable when transcribing\nwith Voice Activity Detection segmentation alone.", "published": "2024-06-24 16:38:17", "link": "http://arxiv.org/abs/2406.16777v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs", "abstract": "Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over lottery tickets (or sparse task vectors), LoTA also enables\nmodel merging over highly dissimilar tasks. Our code is made publicly available\nat https://github.com/kiddyboots216/lottery-ticket-adaptation.", "published": "2024-06-24 16:58:23", "link": "http://arxiv.org/abs/2406.16797v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models", "abstract": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.", "published": "2024-06-24 17:45:59", "link": "http://arxiv.org/abs/2406.16838v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "abstract": "Inference with modern Large Language Models (LLMs) is expensive and\ntime-consuming, and speculative sampling has proven to be an effective\nsolution. Most speculative sampling methods such as EAGLE use a static draft\ntree, implicitly assuming that the acceptance rate of draft tokens depends only\non their position. Interestingly, we found that the acceptance rate of draft\ntokens is also context-dependent. In this paper, building upon EAGLE, we\npropose EAGLE-2, which introduces a new technique of context-aware dynamic\ndraft tree into drafting modeling. This improvement leverages the fact that the\ndraft model of EAGLE is well-calibrated: the confidence scores from the draft\nmodel approximate acceptance rates with small errors. We conducted extensive\nevaluations on three series of LLMs and six tasks, with EAGLE-2 achieving\nspeedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also\nensures that the distribution of the generated text remains unchanged, making\nit a lossless acceleration algorithm.", "published": "2024-06-24 17:59:11", "link": "http://arxiv.org/abs/2406.16858v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DEXTER: A Benchmark for open-domain Complex Question Answering using\n  LLMs", "abstract": "Open-domain complex Question Answering (QA) is a difficult task with\nchallenges in evidence retrieval and reasoning. The complexity of such\nquestions could stem from questions being compositional, hybrid evidence, or\nambiguity in questions. While retrieval performance for classical QA tasks is\nwell explored, their capabilities for heterogeneous complex retrieval tasks,\nespecially in an open-domain setting, and the impact on downstream QA\nperformance, are relatively unexplored. To address this, in this work, we\npropose a benchmark composing diverse complex QA tasks and provide a toolkit to\nevaluate state-of-the-art pre-trained dense and sparse retrieval models in an\nopen-domain setting. We observe that late interaction models and surprisingly\nlexical models like BM25 perform well compared to other pre-trained dense\nretrieval models. In addition, since context-based reasoning is critical for\nsolving complex QA tasks, we also evaluate the reasoning capabilities of LLMs\nand the impact of retrieval performance on their reasoning capabilities.\nThrough experiments, we observe that much progress is to be made in retrieval\nfor complex QA to improve downstream QA performance. Our software and related\ndata can be accessed at https://github.com/VenkteshV/DEXTER", "published": "2024-06-24 22:09:50", "link": "http://arxiv.org/abs/2406.17158v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models", "abstract": "As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.", "published": "2024-06-24 23:02:56", "link": "http://arxiv.org/abs/2406.17169v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n  Analysis Generation", "abstract": "Legal professionals need to write analyses that rely on citations to relevant\nprecedents, i.e., previous case decisions. Intelligent systems assisting legal\nprofessionals in writing such documents provide great benefits but are\nchallenging to design. Such systems need to help locate, summarize, and reason\nover salient precedents in order to be useful. To enable systems for such\ntasks, we work with legal professionals to transform a large open-source legal\ncorpus into a dataset supporting two important backbone tasks: information\nretrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC\n(Case Law Evaluation Retrieval Corpus), is constructed for training and\nevaluating models on their ability to (1) find corresponding citations for a\ngiven piece of legal analysis and to (2) compile the text of these citations\n(as well as previous context) into a cogent analysis that supports a reasoning\ngoal. We benchmark state-of-the-art models on CLERC, showing that current\napproaches still struggle: GPT-4o generates analyses with the highest ROUGE\nF-scores but hallucinates the most, while zero-shot IR models only achieve\n48.3% recall@1000.", "published": "2024-06-24 23:57:57", "link": "http://arxiv.org/abs/2406.17186v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MedBench: A Comprehensive, Standardized, and Reliable Benchmarking\n  System for Evaluating Chinese Medical Large Language Models", "abstract": "Ensuring the general efficacy and goodness for human beings from medical\nlarge language models (LLM) before real-world deployment is crucial. However, a\nwidely accepted and accessible evaluation process for medical LLM, especially\nin the Chinese context, remains to be established. In this work, we introduce\n\"MedBench\", a comprehensive, standardized, and reliable benchmarking system for\nChinese medical LLM. First, MedBench assembles the currently largest evaluation\ndataset (300,901 questions) to cover 43 clinical specialties and performs\nmulti-facet evaluation on medical LLM. Second, MedBench provides a standardized\nand fully automatic cloud-based evaluation infrastructure, with physical\nseparations for question and ground truth. Third, MedBench implements dynamic\nevaluation mechanisms to prevent shortcut learning and answer remembering.\nApplying MedBench to popular general and medical LLMs, we observe unbiased,\nreproducible evaluation results largely aligning with medical professionals'\nperspectives. This study establishes a significant foundation for preparing the\npractical applications of Chinese medical LLMs. MedBench is publicly accessible\nat https://medbench.opencompass.org.cn.", "published": "2024-06-24 02:25:48", "link": "http://arxiv.org/abs/2407.10990v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AlleNoise: large-scale text classification benchmark dataset with\n  real-world label noise", "abstract": "Label noise remains a challenge for training robust classification models.\nMost methods for mitigating label noise have been benchmarked using primarily\ndatasets with synthetic noise. While the need for datasets with realistic noise\ndistribution has partially been addressed by web-scraped benchmarks such as\nWebVision and Clothing1M, those benchmarks are restricted to the computer\nvision domain. With the growing importance of Transformer-based models, it is\ncrucial to establish text classification benchmarks for learning with noisy\nlabels. In this paper, we present AlleNoise, a new curated text classification\nbenchmark dataset with real-world instance-dependent label noise, containing\nover 500,000 examples across approximately 5,600 classes, complemented with a\nmeaningful, hierarchical taxonomy of categories. The noise distribution comes\nfrom actual users of a major e-commerce marketplace, so it realistically\nreflects the semantics of human mistakes. In addition to the noisy labels, we\nprovide human-verified clean labels, which help to get a deeper insight into\nthe noise distribution, unlike web-scraped datasets typically used in the\nfield. We demonstrate that a representative selection of established methods\nfor learning with noisy labels is inadequate to handle such real-world noise.\nIn addition, we show evidence that these algorithms do not alleviate excessive\nmemorization. As such, with AlleNoise, we set the bar high for the development\nof label noise methods that can handle real-world label noise in text\nclassification tasks. The code and dataset are available for download at\nhttps://github.com/allegro/AlleNoise.", "published": "2024-06-24 09:29:14", "link": "http://arxiv.org/abs/2407.10992v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Effects of Embodiment and Personality Expression on Learning in\n  LLM-based Educational Agents", "abstract": "This work investigates how personality expression and embodiment affect\npersonality perception and learning in educational conversational agents. We\nextend an existing personality-driven conversational agent framework by\nintegrating LLM-based conversation support tailored to an educational\napplication. We describe a user study built on this system to evaluate two\ndistinct personality styles: high extroversion and agreeableness and low\nextroversion and agreeableness. For each personality style, we assess three\nmodels: (1) a dialogue-only model that conveys personality through dialogue,\n(2) an animated human model that expresses personality solely through dialogue,\nand (3) an animated human model that expresses personality through both\ndialogue and body and facial animations. The results indicate that all models\nare positively perceived regarding both personality and learning outcomes.\nModels with high personality traits are perceived as more engaging than those\nwith low personality traits. We provide a comprehensive quantitative and\nqualitative analysis of perceived personality traits, learning parameters, and\nuser experiences based on participant ratings of the model types and\npersonality styles, as well as users' responses to open-ended questions.", "published": "2024-06-24 09:38:26", "link": "http://arxiv.org/abs/2407.10993v1", "categories": ["cs.CL", "cs.GR"], "primary_category": "cs.CL"}
{"title": "LionGuard: Building a Contextualized Moderation Classifier to Tackle\n  Localized Unsafe Content", "abstract": "As large language models (LLMs) become increasingly prevalent in a wide\nvariety of applications, concerns about the safety of their outputs have become\nmore significant. Most efforts at safety-tuning or moderation today take on a\npredominantly Western-centric view of safety, especially for toxic, hateful, or\nviolent speech. In this paper, we describe LionGuard, a\nSingapore-contextualized moderation classifier that can serve as guardrails\nagainst unsafe LLM outputs. When assessed on Singlish data, LionGuard\noutperforms existing widely-used moderation APIs, which are not finetuned for\nthe Singapore context, by 14% (binary) and up to 51% (multi-label). Our work\nhighlights the benefits of localization for moderation classifiers and presents\na practical and scalable approach for low-resource languages.", "published": "2024-06-24 14:05:56", "link": "http://arxiv.org/abs/2407.10995v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence Regulation Neurons in Language Models", "abstract": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.", "published": "2024-06-24 01:31:03", "link": "http://arxiv.org/abs/2406.16254v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment", "abstract": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks.", "published": "2024-06-24 04:08:35", "link": "http://arxiv.org/abs/2406.16306v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Anomaly Detection of Tabular Data Using LLMs", "abstract": "Large language models (LLMs) have shown their potential in long-context\nunderstanding and mathematical reasoning. In this paper, we study the problem\nof using LLMs to detect tabular anomalies and show that pre-trained LLMs are\nzero-shot batch-level anomaly detectors. That is, without extra\ndistribution-specific model fitting, they can discover hidden outliers in a\nbatch of data, demonstrating their ability to identify low-density data\nregions. For LLMs that are not well aligned with anomaly detection and\nfrequently output factual errors, we apply simple yet effective data-generating\nprocesses to simulate synthetic batch-level anomaly detection datasets and\npropose an end-to-end fine-tuning strategy to bring out the potential of LLMs\nin detecting real anomalies. Experiments on a large anomaly detection benchmark\n(ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art\ntransductive learning-based anomaly detection methods and ii) the efficacy of\nour synthetic dataset and fine-tuning strategy in aligning LLMs to this task.", "published": "2024-06-24 04:17:03", "link": "http://arxiv.org/abs/2406.16308v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Song Data Cleansing for End-to-End Neural Singer Diarization Using\n  Neural Analysis and Synthesis Framework", "abstract": "We propose a data cleansing method that utilizes a neural analysis and\nsynthesis (NANSY++) framework to train an end-to-end neural diarization model\n(EEND) for singer diarization. Our proposed model converts song data with\nchoral singing which is commonly contained in popular music and unsuitable for\ngenerating a simulated dataset to the solo singing data. This cleansing is\nbased on NANSY++, which is a framework trained to reconstruct an input\nnon-overlapped audio signal. We exploit the pre-trained NANSY++ to convert\nchoral singing into clean, non-overlapped audio. This cleansing process\nmitigates the mislabeling of choral singing to solo singing and helps the\neffective training of EEND models even when the majority of available song data\ncontains choral singing sections. We experimentally evaluated the EEND model\ntrained with a dataset using our proposed method using annotated popular duet\nsongs. As a result, our proposed method improved 14.8 points in diarization\nerror rate.", "published": "2024-06-24 04:48:29", "link": "http://arxiv.org/abs/2406.16315v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of\n  Language Models?", "abstract": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.", "published": "2024-06-24 04:50:12", "link": "http://arxiv.org/abs/2406.16316v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection", "abstract": "Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.", "published": "2024-06-24 09:13:42", "link": "http://arxiv.org/abs/2406.16464v5", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SyROCCo: Enhancing Systematic Reviews using Machine Learning", "abstract": "The sheer number of research outputs published every year makes systematic\nreviewing increasingly time- and resource-intensive. This paper explores the\nuse of machine learning techniques to help navigate the systematic review\nprocess. ML has previously been used to reliably 'screen' articles for review -\nthat is, identify relevant articles based on reviewers' inclusion criteria. The\napplication of ML techniques to subsequent stages of a review, however, such as\ndata extraction and evidence mapping, is in its infancy. We therefore set out\nto develop a series of tools that would assist in the profiling and analysis of\n1,952 publications on the theme of 'outcomes-based contracting'. Tools were\ndeveloped for the following tasks: assign publications into 'policy area'\ncategories; identify and extract key information for evidence mapping, such as\norganisations, laws, and geographical information; connect the evidence base to\nan existing dataset on the same topic; and identify subgroups of articles that\nmay share thematic content. An interactive tool using these techniques and a\npublic dataset with their outputs have been released. Our results demonstrate\nthe utility of ML techniques to enhance evidence accessibility and analysis\nwithin the systematic review processes. These efforts show promise in\npotentially yielding substantial efficiencies for future systematic reviewing\nand for broadening their analytical scope. Our work suggests that there may be\nimplications for the ease with which policymakers and practitioners can access\nevidence. While ML techniques seem poised to play a significant role in\nbridging the gap between research and policy by offering innovative ways of\ngathering, accessing, and analysing data from systematic reviews, we also\nhighlight their current limitations and the need to exercise caution in their\napplication, particularly given the potential for errors and biases.", "published": "2024-06-24 11:04:43", "link": "http://arxiv.org/abs/2406.16527v1", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Token-based Decision Criteria Are Suboptimal in In-context Learning", "abstract": "In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL. Our official code implementation\ncan be found at https://github.com/hc495/Hidden_Calibration.", "published": "2024-06-24 11:16:26", "link": "http://arxiv.org/abs/2406.16535v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLEAR: Can Language Models Really Understand Causal Graphs?", "abstract": "Causal reasoning is a cornerstone of how humans interpret the world. To model\nand reason about causality, causal graphs offer a concise yet effective\nsolution. Given the impressive advancements in language models, a crucial\nquestion arises: can they really understand causal graphs? To this end, we\npioneer an investigation into language models' understanding of causal graphs.\nSpecifically, we develop a framework to define causal graph understanding, by\nassessing language models' behaviors through four practical criteria derived\nfrom diverse disciplines (e.g., philosophy and psychology). We then develop\nCLEAR, a novel benchmark that defines three complexity levels and encompasses\n20 causal graph-based tasks across these levels. Finally, based on our\nframework and benchmark, we conduct extensive experiments on six leading\nlanguage models and summarize five empirical findings. Our results indicate\nthat while language models demonstrate a preliminary understanding of causal\ngraphs, significant potential for improvement remains. Our project website is\nat https://github.com/OpenCausaLab/CLEAR.", "published": "2024-06-24 12:46:15", "link": "http://arxiv.org/abs/2406.16605v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "abstract": "The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated efficiency techniques like quantization\nand sparsity. Contextual sparsity, where the sparsity pattern is\ninput-dependent, is crucial in LLMs because the permanent removal of attention\nheads or neurons from LLMs can significantly degrade accuracy. Prior work has\nattempted to model contextual sparsity using neural networks trained to predict\nactivation magnitudes, which can be used to dynamically prune structures with\nlow predicted activation magnitude. In this paper, we look beyond\nmagnitude-based pruning criteria to assess attention head and neuron importance\nin LLMs. We develop a novel predictor called ShadowLLM, which can shadow the\nLLM behavior and enforce better sparsity patterns, resulting in over 15%\nimprovement in end-to-end accuracy compared to prior methods. In addition,\nShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on Llama-2 and OPT models with up\nto 30 billion parameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.", "published": "2024-06-24 13:41:08", "link": "http://arxiv.org/abs/2406.16635v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Segment Any Text: A Universal Approach for Robust, Efficient and\n  Adaptable Sentence Segmentation", "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://github.com/segment-any-text/wtpsplit under the MIT license.", "published": "2024-06-24 14:36:11", "link": "http://arxiv.org/abs/2406.16678v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models", "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.", "published": "2024-06-24 15:16:45", "link": "http://arxiv.org/abs/2406.16714v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Responsible Foundation Model Development Cheatsheet: A Review of\n  Tools & Resources", "abstract": "Foundation model development attracts a rapidly expanding body of\ncontributors, scientists, and applications. To help shape responsible\ndevelopment practices, we introduce the Foundation Model Development\nCheatsheet: a growing collection of 250+ tools and resources spanning text,\nvision, and speech modalities. We draw on a large body of prior work to survey\nresources (e.g. software, documentation, frameworks, guides, and practical\ntools) that support informed data selection, processing, and understanding,\nprecise and limitation-aware artifact documentation, efficient model training,\nadvance awareness of the environmental impact from training, careful model\nevaluation of capabilities, risks, and claims, as well as responsible model\nrelease, licensing and deployment practices. We hope this curated collection of\nresources helps guide more responsible development. The process of curating\nthis list, enabled us to review the AI development ecosystem, revealing what\ntools are critically missing, misused, or over-used in existing practices. We\nfind that (i) tools for data sourcing, model evaluation, and monitoring are\ncritically under-serving ethical and real-world needs, (ii) evaluations for\nmodel safety, capabilities, and environmental impact all lack reproducibility\nand transparency, (iii) text and particularly English-centric analyses continue\nto dominate over multilingual and multi-modal analyses, and (iv) evaluation of\nsystems, rather than just models, is needed so that capabilities and impact are\nassessed in context.", "published": "2024-06-24 15:55:49", "link": "http://arxiv.org/abs/2406.16746v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Zero-Shot Text-To-Speech for Arabic Dialects", "abstract": "Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for\nEnglish, however, it still lags behind due to insufficient resources. We\naddress this gap for Arabic, a language of more than 450 million native\nspeakers, by first adapting a sizeable existing dataset to suit the needs of\nspeech synthesis. Additionally, we employ a set of Arabic dialect\nidentification models to explore the impact of pre-defined dialect labels on\nimproving the ZS-TTS model in a multi-dialect setting. Subsequently, we\nfine-tune the\nXTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}\nmodel, an open-source architecture. We then evaluate our models on a dataset\ncomprising 31 unseen speakers and an in-house dialectal dataset. Our automated\nand human evaluation results show convincing performance while capable of\ngenerating dialectal speech. Our study highlights significant potential for\nimprovements in this emerging area of research in Arabic.", "published": "2024-06-24 15:58:15", "link": "http://arxiv.org/abs/2406.16751v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models", "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.", "published": "2024-06-24 16:45:13", "link": "http://arxiv.org/abs/2406.16783v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation", "abstract": "Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.", "published": "2024-06-24 17:19:34", "link": "http://arxiv.org/abs/2406.16807v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural\n  Unlearning Perspective", "abstract": "While unlearning knowledge from large language models (LLMs) is receiving\nincreasing attention, one important aspect remains unexplored. Existing\napproaches and benchmarks assume data points to-be-forgotten are independent,\nignoring their inter-connectivity - a fundamental characteristic of real-world\ndata structures. In this paper, we propose PISTOL, a method for compiling\nstructural datasets. PISTOL leverages the inherently structured nature of\ncontractual relationships, offering several key benefits. First, it enables\ninsights into the impact of structural data on unlearning effectiveness.\nSecond, it provides precise and concise ground truths for clearer evaluation.\nThird, its attribute generation does not require input from pre-trained LLMs,\nmitigating confounding risks. Leveraging datasets synthesized using PISTOL, we\ndemonstrate how data inter-connectivity impacts LLM unlearning. Specifically,\n(a) in both the pre-trained and fine-tuned models, unlearning difficulty\nincreases as data inter-connectivity grows, (b) there is a positive correlation\nbetween the density of the knowledge graph and unlearning difficulty, and (c)\nwhen the to-be-forgotten data is skewed towards one domain, balancing retaining\nperformance across all domains is challenging.", "published": "2024-06-24 17:22:36", "link": "http://arxiv.org/abs/2406.16810v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024\n  Retrieval-Augmented Generation Track", "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google\nAI~Overviews? These might sound familiar because the modern-day search stack\nhas recently evolved to include retrieval-augmented generation (RAG) systems.\nThey allow searching and incorporating real-time data into large language\nmodels (LLMs) to provide a well-informed, attributed, concise summary in\ncontrast to the traditional search paradigm that relies on displaying a ranked\nlist of documents. Therefore, given these recent advancements, it is crucial to\nhave an arena to build, test, visualize, and systematically evaluate RAG-based\nsearch systems. With this in mind, we propose the TREC 2024 RAG Track to foster\ninnovation in evaluating RAG systems. In our work, we lay out the steps we've\nmade towards making this track a reality -- we describe the details of our\nreusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1\ncollection choice, release the development topics for the track, and\nstandardize the I/O definitions which assist the end user. Next, using\nRagnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's\nGPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface\nfor an interactive arena allowing benchmarking pairwise RAG systems by\ncrowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve\na unified standard for future RAG systems.", "published": "2024-06-24 17:37:52", "link": "http://arxiv.org/abs/2406.16828v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understanding and Mitigating Tokenization Bias in Language Models", "abstract": "State-of-the-art language models are autoregressive and operate on subword\nunits known as tokens. Specifically, one must encode the conditioning string\ninto a list of tokens before passing to the language models for next-token\nprediction. We show that popular encoding schemes, such as maximum prefix\nencoding (MPE) and byte-pair-encoding (BPE), induce a sampling bias that cannot\nbe mitigated with more training or data. To counter this universal problem, for\neach encoding scheme above, we propose a novel algorithm to obtain unbiased\nestimates from any language model trained on tokenized data. Our methods do not\nrequire finetuning the model, and the complexity, defined as the number of\nmodel runs, scales linearly with the sequence length in the case of MPE. As a\nresult, we show that one can simulate token-free behavior from a tokenized\nlanguage model. We empirically verify the correctness of our method through a\nMarkov-chain setup, where it accurately recovers the transition probabilities,\nas opposed to the conventional method of directly prompting tokens into the\nlanguage model.", "published": "2024-06-24 17:38:02", "link": "http://arxiv.org/abs/2406.16829v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and\n  $\\underline{D}$ogmatism in Long $\\underline{C}$onversations", "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].", "published": "2024-06-24 17:41:53", "link": "http://arxiv.org/abs/2406.16833v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Losing Visual Needles in Image Haystacks: Vision Language Models are\n  Easily Distracted in Short and Long Contexts", "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking logarithmic decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.", "published": "2024-06-24 17:58:03", "link": "http://arxiv.org/abs/2406.16851v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Large Language Models Assume People are More Rational than We Really are", "abstract": "In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.", "published": "2024-06-24 18:15:27", "link": "http://arxiv.org/abs/2406.17055v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Testing network clustering algorithms with Natural Language Processing", "abstract": "The advent of online social networks has led to the development of an\nabundant literature on the study of online social groups and their relationship\nto individuals' personalities as revealed by their textual productions. Social\nstructures are inferred from a wide range of social interactions. Those\ninteractions form complex -- sometimes multi-layered -- networks, on which\ncommunity detection algorithms are applied to extract higher order structures.\nThe choice of the community detection algorithm is however hardily questioned\nin relation with the cultural production of the individual they classify. In\nthis work, we assume the entangled nature of social networks and their cultural\nproduction to propose a definition of cultural based online social groups as\nsets of individuals whose online production can be categorized as social\ngroup-related. We take advantage of this apparently self-referential\ndescription of online social groups with a hybrid methodology that combines a\ncommunity detection algorithm and a natural language processing classification\nalgorithm. A key result of this analysis is the possibility to score community\ndetection algorithms using their agreement with the natural language processing\nclassification. A second result is that we can assign the opinion of a random\nuser at >85% accuracy.", "published": "2024-06-24 20:54:32", "link": "http://arxiv.org/abs/2406.17135v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Paraphrase and Aggregate with Large Language Models for Minimizing\n  Intent Classification Errors", "abstract": "Large language models (LLM) have achieved remarkable success in natural\nlanguage generation but lesser focus has been given to their applicability in\ndecision making tasks such as classification. We show that LLMs like LLaMa can\nachieve high performance on large multi-class classification tasks but still\nmake classification errors and worse, generate out-of-vocabulary class labels.\nTo address these critical issues, we introduce Paraphrase and AGgregate\n(PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input\nquery (parallel queries), performs multi-class classification for the original\nquery and each paraphrase, and at the end aggregate all the classification\nlabels based on their confidence scores. We evaluate PAG-LLM on two large\nmulti-class classication datasets: CLINC, and Banking and show 22.7% and 15.1%\nerror reduction. We show that PAG-LLM is especially effective for hard examples\nwhere LLM is uncertain, and reduces the critical misclassification and\nhallucinated label generation errors", "published": "2024-06-24 22:30:26", "link": "http://arxiv.org/abs/2406.17163v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training-Free Exponential Context Extension via Cascading KV Cache", "abstract": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.", "published": "2024-06-24 03:59:17", "link": "http://arxiv.org/abs/2406.17808v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BitNet b1.58 Reloaded: State-of-the-art Performance Also on Smaller\n  Networks", "abstract": "Recently proposed methods for 1-bit and 1.58-bit quantization aware training\ninvestigate the performance and behavior of these methods in the context of\nlarge language models, finding state-of-the-art performance for models with\nmore than 3B parameters. In this work, we investigate 1.58-bit quantization for\nsmall language and vision models ranging from 100K to 48M parameters. We\nintroduce a variant of BitNet b1.58, which allows to rely on the median rather\nthan the mean in the quantization process.\n  Through extensive experiments we investigate the performance of 1.58-bit\nmodels obtained through quantization aware training. We further investigate the\nrobustness of 1.58-bit quantization-aware training to changes in the learning\nrate and regularization through weight decay, finding different patterns for\nsmall language and vision models than previously reported for large language\nmodels.\n  Our results showcase that 1.58-bit quantization-aware training provides\nstate-of-the-art performance for small language models when doubling hidden\nlayer sizes and reaches or even surpasses state-of-the-art performance for\nsmall vision models of identical size. Ultimately, we demonstrate that 1.58-bit\nquantization-aware training is a viable and promising approach also for\ntraining smaller deep learning networks, facilitating deployment of such models\nin low-resource use-cases and encouraging future research.", "published": "2024-06-24 20:55:36", "link": "http://arxiv.org/abs/2407.09527v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Classification of Geological Borehole Descriptions Using a Domain\n  Adapted Large Language Model", "abstract": "Geological borehole descriptions contain detailed textual information about\nthe composition of the subsurface. However, their unstructured format presents\nsignificant challenges for extracting relevant features into a structured\nformat. This paper introduces GEOBERTje: a domain adapted large language model\ntrained on geological borehole descriptions from Flanders (Belgium) in the\nDutch language. This model effectively extracts relevant information from the\nborehole descriptions and represents it into a numeric vector space. Showcasing\njust one potential application of GEOBERTje, we finetune a classifier model on\na limited number of manually labeled observations. This classifier categorizes\nborehole descriptions into a main, second and third lithology class. We show\nthat our classifier outperforms both a rule-based approach and GPT-4 of OpenAI.\nThis study exemplifies how domain adapted large language models enhance the\nefficiency and accuracy of extracting information from complex, unstructured\ngeological descriptions. This offers new opportunities for geological analysis\nand modeling using vast amounts of data.", "published": "2024-06-24 07:29:43", "link": "http://arxiv.org/abs/2407.10991v1", "categories": ["cs.CL", "cs.LG", "physics.geo-ph"], "primary_category": "cs.CL"}
{"title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant", "abstract": "The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.", "published": "2024-06-24 12:09:34", "link": "http://arxiv.org/abs/2407.10994v4", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visualization Literacy of Multimodal Large Language Models: A\n  Comparative Study", "abstract": "The recent introduction of multimodal large language models (MLLMs) combine\nthe inherent power of large language models (LLMs) with the renewed\ncapabilities to reason about the multimodal context. The potential usage\nscenarios for MLLMs significantly outpace their text-only counterparts. Many\nrecent works in visualization have demonstrated MLLMs' capability to understand\nand interpret visualization results and explain the content of the\nvisualization to users in natural language. In the machine learning community,\nthe general vision capabilities of MLLMs have been evaluated and tested through\nvarious visual understanding benchmarks. However, the ability of MLLMs to\naccomplish specific visualization tasks based on visual perception has not been\nproperly explored and evaluated, particularly, from a visualization-centric\nperspective.\n  In this work, we aim to fill the gap by utilizing the concept of\nvisualization literacy to evaluate MLLMs. We assess MLLMs' performance over two\npopular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under\nthe framework of visualization literacy, we develop a general setup to compare\ndifferent multimodal large language models (e.g., GPT4-o, Claude 3 Opus, Gemini\n1.5 Pro) as well as against existing human baselines. Our study demonstrates\nMLLMs' competitive performance in visualization literacy, where they outperform\nhumans in certain tasks such as identifying correlations, clusters, and\nhierarchical structures.", "published": "2024-06-24 17:52:16", "link": "http://arxiv.org/abs/2407.10996v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DreamVoice: Text-Guided Voice Conversion", "abstract": "Generative voice technologies are rapidly evolving, offering opportunities\nfor more personalized and inclusive experiences. Traditional one-shot voice\nconversion (VC) requires a target recording during inference, limiting ease of\nusage in generating desired voice timbres. Text-guided generation offers an\nintuitive solution to convert voices to desired \"DreamVoices\" according to the\nusers' needs. Our paper presents two major contributions to VC technology: (1)\nDreamVoiceDB, a robust dataset of voice timbre annotations for 900 speakers\nfrom VCTK and LibriTTS. (2) Two text-guided VC methods: DreamVC, an end-to-end\ndiffusion-based text-guided VC model; and DreamVG, a versatile text-to-voice\ngeneration plugin that can be combined with any one-shot VC models. The\nexperimental results demonstrate that our proposed methods trained on the\nDreamVoiceDB dataset generate voice timbres accurately aligned with the text\nprompt and achieve high-quality VC.", "published": "2024-06-24 04:46:50", "link": "http://arxiv.org/abs/2406.16314v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "RefXVC: Cross-Lingual Voice Conversion with Enhanced Reference\n  Leveraging", "abstract": "This paper proposes RefXVC, a method for cross-lingual voice conversion (XVC)\nthat leverages reference information to improve conversion performance.\nPrevious XVC works generally take an average speaker embedding to condition the\nspeaker identity, which does not account for the changing timbre of speech that\noccurs with different pronunciations. To address this, our method uses both\nglobal and local speaker embeddings to capture the timbre changes during speech\nconversion. Additionally, we observed a connection between timbre and\npronunciation in different languages and utilized this by incorporating a\ntimbre encoder and a pronunciation matching network into our model.\nFurthermore, we found that the variation in tones is not adequately reflected\nin a sentence, and therefore, we used multiple references to better capture the\nrange of a speaker's voice. The proposed method outperformed existing systems\nin terms of both speech quality and speaker similarity, highlighting the\neffectiveness of leveraging reference information in cross-lingual voice\nconversion. The converted speech samples can be found on the website:\n\\url{http://refxvc.dn3point.com}", "published": "2024-06-24 05:40:09", "link": "http://arxiv.org/abs/2406.16326v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SNR-Progressive Model with Harmonic Compensation for Low-SNR Speech\n  Enhancement", "abstract": "Despite significant progress made in the last decade, deep neural network\n(DNN) based speech enhancement (SE) still faces the challenge of notable\ndegradation in the quality of recovered speech under low signal-to-noise ratio\n(SNR) conditions. In this letter, we propose an SNR-progressive speech\nenhancement model with harmonic compensation for low-SNR SE. Reliable pitch\nestimation is obtained from the intermediate output, which has the benefit of\nretaining more speech components than the coarse estimate while possessing a\nsignificant higher SNR than the input noisy speech. An effective harmonic\ncompensation mechanism is introduced for better harmonic recovery. Extensive\nex-periments demonstrate the advantage of our proposed model. A multi-modal\nspeech extraction system based on the proposed backbone model ranks first in\nthe ICASSP 2024 MISP Challenge:\nhttps://mispchallenge.github.io/mispchallenge2023/index.html.", "published": "2024-06-24 04:52:28", "link": "http://arxiv.org/abs/2406.16317v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the Capability of Mamba in Speech Applications", "abstract": "This paper explores the capability of Mamba, a recently proposed architecture\nbased on state space models (SSMs), as a competitive alternative to\nTransformer-based models. In the speech domain, well-designed Transformer-based\nmodels, such as the Conformer and E-Branchformer, have become the de facto\nstandards. Extensive evaluations have demonstrated the effectiveness of these\nTransformer-based models across a wide range of speech tasks. In contrast, the\nevaluation of SSMs has been limited to a few tasks, such as automatic speech\nrecognition (ASR) and speech synthesis. In this paper, we compared Mamba with\nstate-of-the-art Transformer variants for various speech applications,\nincluding ASR, text-to-speech, spoken language understanding, and speech\nsummarization. Experimental evaluations revealed that Mamba achieves comparable\nor better performance than Transformer-based models, and demonstrated its\nefficiency in long-form speech processing.", "published": "2024-06-24 17:20:58", "link": "http://arxiv.org/abs/2406.16808v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Field Synthesis with Acoustic Waves", "abstract": "We propose a practical framework to synthesize the broadband sound-field on a\nsmall rigid surface based on the physics of sound propagation. The sound-field\nis generated as a composite map of two components: the room component and the\ndevice component, with acoustic plane waves as the core tool for the\ngeneration. This decoupling of room and device components significantly reduces\nthe problem complexity and provides accurate rendering of the sound-field.\n  We describe in detail the theoretical foundations, and efficient procedures\nof the implementation. The effectiveness of the proposed framework is\nestablished through rigorous validation under different environment setups.", "published": "2024-06-24 19:52:48", "link": "http://arxiv.org/abs/2406.17111v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "One-Class Learning with Adaptive Centroid Shift for Audio Deepfake\n  Detection", "abstract": "As speech synthesis systems continue to make remarkable advances in recent\nyears, the importance of robust deepfake detection systems that perform well in\nunseen systems has grown. In this paper, we propose a novel adaptive centroid\nshift (ACS) method that updates the centroid representation by continually\nshifting as the weighted average of bonafide representations. Our approach uses\nonly bonafide samples to define their centroid, which can yield a specialized\ncentroid for one-class learning. Integrating our ACS with one-class learning\ngathers bonafide representations into a single cluster, forming well-separated\nembeddings robust to unseen spoofing attacks. Our proposed method achieves an\nequal error rate (EER) of 2.19% on the ASVspoof 2021 deepfake dataset,\noutperforming all existing systems. Furthermore, the t-SNE visualization\nillustrates that our method effectively maps the bonafide embeddings into a\nsingle cluster and successfully disentangles the bonafide and spoof classes.", "published": "2024-06-24 15:21:50", "link": "http://arxiv.org/abs/2406.16716v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AND: Audio Network Dissection for Interpreting Deep Acoustic Models", "abstract": "Neuron-level interpretations aim to explain network behaviors and properties\nby investigating neurons responsive to specific perceptual or structural input\npatterns. Although there is emerging work in the vision and language domains,\nnone is explored for acoustic models. To bridge the gap, we introduce\n$\\textit{AND}$, the first $\\textbf{A}$udio $\\textbf{N}$etwork\n$\\textbf{D}$issection framework that automatically establishes natural language\nexplanations of acoustic neurons based on highly-responsive audio.\n$\\textit{AND}$ features the use of LLMs to summarize mutual acoustic features\nand identities among audio. Extensive experiments are conducted to verify\n$\\textit{AND}$'s precise and informative descriptions. In addition, we\ndemonstrate a potential use of $\\textit{AND}$ for audio machine unlearning by\nconducting concept-specific pruning based on the generated descriptions.\nFinally, we highlight two acoustic model behaviors with analysis by\n$\\textit{AND}$: (i) models discriminate audio with a combination of basic\nacoustic features rather than high-level abstract concepts; (ii) training\nstrategies affect model behaviors and neuron interpretability -- supervised\ntraining guides neurons to gradually narrow their attention, while\nself-supervised learning encourages neurons to be polysemantic for exploring\nhigh-level features.", "published": "2024-06-24 06:02:07", "link": "http://arxiv.org/abs/2406.16990v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Maximum Likelihood Estimation of the Direction of Sound In A Reverberant\n  Noisy Environment", "abstract": "We describe a new method for estimating the direction of sound in a\nreverberant environment from basic principles of sound propagation. The method\nutilizes SNR-adaptive features from time-delay and energy of the directional\ncomponents after acoustic wave decomposition of the observed sound field to\nestimate the line-of-sight direction under noisy and reverberant conditions.\nThe effectiveness of the approach is established with measured data of\ndifferent microphone array configurations under various usage scenarios.", "published": "2024-06-24 19:42:22", "link": "http://arxiv.org/abs/2406.17103v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating Confidence Estimation Measures for Speaker Diarization", "abstract": "Speaker diarization systems segment a conversation recording based on the\nspeakers' identity. Such systems can misclassify the speaker of a portion of\naudio due to a variety of factors, such as speech pattern variation, background\nnoise, and overlapping speech. These errors propagate to, and can adversely\naffect, downstream systems that rely on the speaker's identity, such as\nspeaker-adapted speech recognition. One of the ways to mitigate these errors is\nto provide segment-level diarization confidence scores to downstream systems.\nIn this work, we investigate multiple methods for generating diarization\nconfidence scores, including those derived from the original diarization system\nand those derived from an external model. Our experiments across multiple\ndatasets and diarization systems demonstrate that the most competitive\nconfidence score methods can isolate ~30% of the diarization errors within\nsegments with the lowest ~10% of confidence scores.", "published": "2024-06-24 20:21:38", "link": "http://arxiv.org/abs/2406.17124v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring compressibility of transformer based text-to-music (TTM)\n  models", "abstract": "State-of-the art Text-To-Music (TTM) generative AI models are large and\nrequire desktop or server class compute, making them infeasible for deployment\non mobile phones. This paper presents an analysis of trade-offs between model\ncompression and generation performance of TTM models. We study compression\nthrough knowledge distillation and specific modifications that enable\napplicability over the various components of the TTM model (encoder, generative\nmodel and the decoder). Leveraging these methods we create TinyTTM (89.2M\nparams) that achieves a FAD of 3.66 and KL of 1.32 on MusicBench dataset,\nbetter than MusicGen-Small (557.6M params) but not lower than MusicGen-small\nfine-tuned on MusicBench.", "published": "2024-06-24 22:17:32", "link": "http://arxiv.org/abs/2406.17159v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
