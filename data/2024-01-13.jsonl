{"title": "Knowledge-Centric Templatic Views of Documents", "abstract": "Authors seeking to communicate with broader audiences often share their ideas\nin various document formats, such as slide decks, newsletters, reports, and\nposters. Prior work on document generation has generally tackled the creation\nof each separate format to be a different task, leading to fragmented learning\nprocesses, redundancy in models and methods, and disjointed evaluation. We\nconsider each of these documents as templatic views of the same underlying\nknowledge/content, and we aim to unify the generation and evaluation of these\ntemplatic views. We begin by showing that current LLMs are capable of\ngenerating various document formats with little to no supervision. Further, a\nsimple augmentation involving a structured intermediate representation can\nimprove performance, especially for smaller models. We then introduce a novel\nunified evaluation framework that can be adapted to measuring the quality of\ndocument generators for heterogeneous downstream applications. This evaluation\nis adaptable to a range of user defined criteria and application scenarios,\nobviating the need for task specific evaluation metrics. Finally, we conduct a\nhuman evaluation, which shows that people prefer 82% of the documents generated\nwith our method, while correlating more highly with our unified evaluation\nframework than prior metrics in the literature.", "published": "2024-01-13 01:22:15", "link": "http://arxiv.org/abs/2401.06945v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Preference Gap between Retrievers and LLMs", "abstract": "Large Language Models (LLMs) have demonstrated superior results across a wide\nrange of tasks, and Retrieval-augmented Generation (RAG) is an effective way to\nenhance the performance by locating relevant information and placing it into\nthe context window of the LLM. However, the relationship between retrievers and\nLLMs in a RAG is still under-investigated. Most existing work treats the\nretriever and the LLM as independent components and leaves a gap between\nretrieving human-\"friendly\" information and assembling a LLM-\"friendly\"\ncontext. In this work, we examine a novel bridge mechanism. We validate the\nranking and selection assumptions of retrievers in the context of RAG and\npropose a framework that chains together supervised and reinforcement learning\nto train a bridge model that optimizes the connection between the retriever and\nthe LLM. Empirical results demonstrate the effectiveness of our method in both\nquestion-answering and personalized generation tasks.", "published": "2024-01-13 02:20:17", "link": "http://arxiv.org/abs/2401.06954v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending LLMs' Context Window with 100 Samples", "abstract": "Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.", "published": "2024-01-13 07:57:01", "link": "http://arxiv.org/abs/2401.07004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation of Black-Box Large Language Models", "abstract": "Given the exceptional performance of proprietary large language models (LLMs)\nlike GPT-4, recent research has increasingly focused on boosting the\ncapabilities of smaller models through knowledge distillation (KD) from these\npowerful yet black-box teachers. While leveraging the high-quality outputs of\nthese teachers is advantageous, the inaccessibility of their internal states\noften limits effective knowledge transfer. To overcome this limitation, we\nintroduce Proxy-KD, a novel method that uses a proxy model to facilitate the\nefficient transfer of knowledge from black-box LLMs to smaller models. Our\nexperiments show that Proxy-KD not only enhances the performance of KD from\nblack-box teacher models but also surpasses traditional white-box KD\ntechniques.~This approach presents a compelling new avenue for distilling\nknowledge from advanced LLMs.", "published": "2024-01-13 08:43:32", "link": "http://arxiv.org/abs/2401.07013v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics\n  Capabilities", "abstract": "LLMs have demonstrated remarkable capability for understanding semantics, but\nthey often struggle with understanding pragmatics. To demonstrate this fact, we\nrelease a Pragmatics Understanding Benchmark (PUB) dataset consisting of\nfourteen tasks in four pragmatics phenomena, namely, Implicature,\nPresupposition, Reference, and Deixis. We curated high-quality test sets for\neach task, consisting of Multiple Choice Question Answers (MCQA). PUB includes\na total of 28k data points, 6.1k of which have been created by us, and the rest\nare adapted from existing datasets. We evaluated nine models varying in the\nnumber of parameters and type of training. Our study indicates that fine-tuning\nfor instruction-following and chat significantly enhances the pragmatics\ncapabilities of smaller language models. However, for larger models, the base\nversions perform comparably with their chat-adapted counterparts. Additionally,\nthere is a noticeable performance gap between human capabilities and model\ncapabilities. Furthermore, unlike the consistent performance of humans across\nvarious tasks, the models demonstrate variability in their proficiency, with\nperformance levels fluctuating due to different hints and the complexities of\ntasks within the same dataset. Overall, the benchmark aims to provide a\ncomprehensive evaluation of LLM's ability to handle real-world language tasks\nthat require pragmatic reasoning.", "published": "2024-01-13 13:46:14", "link": "http://arxiv.org/abs/2401.07078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ\n  Generation using GPT", "abstract": "We introduce a multi-stage prompting approach (MSP) for the generation of\nmultiple choice questions (MCQs), harnessing the capabilities of GPT models\nsuch as text-davinci-003 and GPT-4, renowned for their excellence across\nvarious NLP tasks. Our approach incorporates the innovative concept of\nchain-of-thought prompting, a progressive technique in which the GPT model is\nprovided with a series of interconnected cues to guide the MCQ generation\nprocess. Automated evaluations consistently demonstrate the superiority of our\nproposed MSP method over the traditional single-stage prompting (SSP) baseline,\nresulting in the production of high-quality distractors. Furthermore, the\none-shot MSP technique enhances automatic evaluation results, contributing to\nimproved distractor generation in multiple languages, including English,\nGerman, Bengali, and Hindi. In human evaluations, questions generated using our\napproach exhibit superior levels of grammaticality, answerability, and\ndifficulty, highlighting its efficacy in various languages.", "published": "2024-01-13 15:18:44", "link": "http://arxiv.org/abs/2401.07098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for NLG Evaluation: Advances and\n  Challenges", "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG)\nevaluation, introducing Large Language Models (LLMs) has opened new avenues for\nassessing generated content quality, e.g., coherence, creativity, and context\nrelevance. This paper aims to provide a thorough overview of leveraging LLMs\nfor NLG evaluation, a burgeoning area that lacks a systematic analysis. We\npropose a coherent taxonomy for organizing existing LLM-based evaluation\nmetrics, offering a structured framework to understand and compare these\nmethods. Our detailed exploration includes critically assessing various\nLLM-based methodologies, as well as comparing their strengths and limitations\nin evaluating NLG outputs. By discussing unresolved challenges, including bias,\nrobustness, domain-specificity, and unified evaluation, this paper seeks to\noffer insights to researchers and advocate for fairer and more advanced NLG\nevaluation techniques.", "published": "2024-01-13 15:59:09", "link": "http://arxiv.org/abs/2401.07103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "abstract": "Translation systems, including foundation models capable of translation, can\nproduce errors that result in gender mistranslation, and such errors can be\nespecially harmful. To measure the extent of such potential harms when\ntranslating into and out of English, we introduce a dataset, MiTTenS, covering\n26 languages from a variety of language families and scripts, including several\ntraditionally under-represented in digital resources. The dataset is\nconstructed with handcrafted passages that target known failure patterns,\nlonger synthetically generated passages, and natural passages sourced from\nmultiple domains. We demonstrate the usefulness of the dataset by evaluating\nboth neural machine translation systems and foundation models, and show that\nall systems exhibit gender mistranslation and potential harm, even in high\nresource languages.", "published": "2024-01-13 00:08:23", "link": "http://arxiv.org/abs/2401.06935v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Detoxification with Contrastive Decoding", "abstract": "The field of natural language generation has witnessed significant\nadvancements in recent years, including the development of controllable text\ngeneration techniques. However, controlling the attributes of the generated\ntext remains a challenge, especially when aiming to avoid undesirable behavior\nsuch as toxicity. In this work, we introduce Detoxification Generator\n(DETOXIGEN), an inference-time algorithm that steers the generation away from\nunwanted styles. DETOXIGEN is an ensemble of a pre-trained language model\n(generator) and a detoxifier. The detoxifier is trained intentionally on the\ntoxic data representative of the undesirable attribute, encouraging it to\ngenerate text in that style exclusively. During the actual generation, we use\nthe trained detoxifier to produce undesirable tokens for the generator to\ncontrast against at each decoding step. This approach directly informs the\ngenerator to avoid generating tokens that the detoxifier considers highly\nlikely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS\nbenchmark (Gehman et al., 2020) with various language models as generators. We\nfind that it significantly outperforms previous approaches in detoxification\nmetrics while not compromising on the generation quality. Moreover, the\ndetoxifier is obtained by soft prompt-tuning using the same backbone language\nmodel as the generator. Hence, DETOXIGEN requires only a tiny amount of extra\nweights from the virtual tokens of the detoxifier to be loaded into GPU memory\nwhile decoding, making it a promising lightweight, practical, and\nparameter-efficient detoxification strategy.", "published": "2024-01-13 01:46:20", "link": "http://arxiv.org/abs/2401.06947v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models", "abstract": "Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.", "published": "2024-01-13 02:11:20", "link": "http://arxiv.org/abs/2401.06951v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Extraction of Uyghur Medicine Knowledge with Edge Computing", "abstract": "Medical knowledge extraction methods based on edge computing deploy deep\nlearning models on edge devices to achieve localized entity and relation\nextraction. This approach avoids transferring substantial sensitive data to\ncloud data centers, effectively safeguarding the privacy of healthcare\nservices. However, existing relation extraction methods mainly employ a\nsequential pipeline approach, which classifies relations between determined\nentities after entity recognition. This mode faces challenges such as error\npropagation between tasks, insufficient consideration of dependencies between\nthe two subtasks, and the neglect of interrelations between different relations\nwithin a sentence. To address these challenges, a joint extraction model with\nparameter sharing in edge computing is proposed, named CoEx-Bert. This model\nleverages shared parameterization between two models to jointly extract\nentities and relations. Specifically, CoEx-Bert employs two models, each\nseparately sharing hidden layer parameters, and combines these two loss\nfunctions for joint backpropagation to optimize the model parameters.\nAdditionally, it effectively resolves the issue of entity overlapping when\nextracting knowledge from unstructured Uyghur medical texts by considering\ncontextual relations. Finally, this model is deployed on edge devices for\nreal-time extraction and inference of Uyghur medical knowledge. Experimental\nresults demonstrate that CoEx-Bert outperforms existing state-of-the-art\nmethods, achieving accuracy, recall, and F1 scores of 90.65\\%, 92.45\\%, and\n91.54\\%, respectively, in the Uyghur traditional medical literature dataset.\nThese improvements represent a 6.45\\% increase in accuracy, a 9.45\\% increase\nin recall, and a 7.95\\% increase in F1 score compared to the baseline.", "published": "2024-01-13 08:27:24", "link": "http://arxiv.org/abs/2401.07009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual\n  Chain-of-Thought Reasoning", "abstract": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit\nreasoning in large language models and improve a variety of downstream tasks.\nCoT mainly demonstrates excellent performance in English, but its usage in\nlow-resource languages is constrained due to poor language generalization. To\nbridge the gap among different languages, we propose a cross-lingual\ninstruction fine-tuning framework (xCOT) to transfer knowledge from\nhigh-resource languages to low-resource languages. Specifically, the\nmultilingual instruction training data (xCOT-INSTRUCT) is created to encourage\nthe semantic alignment of multiple languages. We introduce cross-lingual\nin-context few-shot learning (xICL)) to accelerate multilingual agreement in\ninstruction tuning, where some fragments of source languages in examples are\nrandomly substituted by their counterpart translations of target languages.\nDuring multilingual instruction tuning, we adopt the randomly online CoT\nstrategy to enhance the multilingual reasoning ability of the large language\nmodel by first translating the query to another language and then answering in\nEnglish. To further facilitate the language transfer, we leverage the\nhigh-resource CoT to supervise the training of low-resource languages with\ncross-lingual distillation. Experimental results on previous benchmarks\ndemonstrate the superior performance of xCoT in reducing the gap among\ndifferent languages, highlighting its potential to reduce the cross-lingual\ngap.", "published": "2024-01-13 10:53:53", "link": "http://arxiv.org/abs/2401.07037v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Directed Regular and Context-Free Languages", "abstract": "We study the problem of deciding whether a given language is directed. A\nlanguage $L$ is \\emph{directed} if every pair of words in $L$ have a common\n(scattered) superword in $L$. Deciding directedness is a fundamental problem in\nconnection with ideal decompositions of downward closed sets. Another\nmotivation is that deciding whether two \\emph{directed} context-free languages\nhave the same downward closures can be decided in polynomial time, whereas for\ngeneral context-free languages, this problem is known to be coNEXP-complete.\n  We show that the directedness problem for regular languages, given as NFAs,\nbelongs to $AC^1$, and thus polynomial time. Moreover, it is NL-complete for\nfixed alphabet sizes. Furthermore, we show that for context-free languages, the\ndirectedness problem is PSPACE-complete.", "published": "2024-01-13 16:13:45", "link": "http://arxiv.org/abs/2401.07106v2", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "One Agent Too Many: User Perspectives on Approaches to Multi-agent\n  Conversational AI", "abstract": "Conversational agents have been gaining increasing popularity in recent\nyears. Influenced by the widespread adoption of task-oriented agents such as\nApple Siri and Amazon Alexa, these agents are being deployed into various\napplications to enhance user experience. Although these agents promote \"ask me\nanything\" functionality, they are typically built to focus on a single or\nfinite set of expertise. Given that complex tasks often require more than one\nexpertise, this results in the users needing to learn and adopt multiple\nagents. One approach to alleviate this is to abstract the orchestration of\nagents in the background. However, this removes the option of choice and\nflexibility, potentially harming the ability to complete tasks. In this paper,\nwe explore these different interaction experiences (one agent for all) vs (user\nchoice of agents) for conversational AI. We design prototypes for each,\nsystematically evaluating their ability to facilitate task completion. Through\na series of conducted user studies, we show that users have a significant\npreference for abstracting agent orchestration in both system usability and\nsystem performance. Additionally, we demonstrate that this mode of interaction\nis able to provide quality responses that are rated within 1% of human-selected\nanswers.", "published": "2024-01-13 17:30:57", "link": "http://arxiv.org/abs/2401.07123v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex\n  Tabular Reasoning on Electronic Health Records", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in\nplanning and tool utilization as autonomous agents, but few have been developed\nfor medical problem-solving. We propose EHRAgent, an LLM agent empowered with a\ncode interface, to autonomously generate and execute code for multi-tabular\nreasoning within electronic health records (EHRs). First, we formulate an EHR\nquestion-answering task into a tool-use planning process, efficiently\ndecomposing a complicated task into a sequence of manageable actions. By\nintegrating interactive coding and execution feedback, EHRAgent learns from\nerror messages and improves the originally generated code through iterations.\nFurthermore, we enhance the LLM agent by incorporating long-term memory, which\nallows EHRAgent to effectively select and build upon the most relevant\nsuccessful cases from past experiences. Experiments on three real-world\nmulti-tabular EHR datasets show that EHRAgent outperforms the strongest\nbaseline by up to 29.6% in success rate. EHRAgent leverages the emerging\nfew-shot learning capabilities of LLMs, enabling autonomous code generation and\nexecution to tackle complex clinical tasks with minimal demonstrations.", "published": "2024-01-13 18:09:05", "link": "http://arxiv.org/abs/2401.07128v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Answer Validation using Text Similarity", "abstract": "Automated answer validation can help improve learning outcomes by providing\nappropriate feedback to learners, and by making question answering systems and\nonline learning solutions more widely available. There have been some works in\nscience question answering which show that information retrieval methods\noutperform neural methods, especially in the multiple choice version of this\nproblem. We implement Siamese neural network models and produce a generalised\nsolution to this problem. We compare our supervised model with other text\nsimilarity based solutions.", "published": "2024-01-13 07:13:08", "link": "http://arxiv.org/abs/2401.08688v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Combining Confidence Elicitation and Sample-based Methods for\n  Uncertainty Quantification in Misinformation Mitigation", "abstract": "Large Language Models have emerged as prime candidates to tackle\nmisinformation mitigation. However, existing approaches struggle with\nhallucinations and overconfident predictions. We propose an uncertainty\nquantification framework that leverages both direct confidence elicitation and\nsampled-based consistency methods to provide better calibration for NLP\nmisinformation mitigation solutions. We first investigate the calibration of\nsample-based consistency methods that exploit distinct features of consistency\nacross sample sizes and stochastic levels. Next, we evaluate the performance\nand distributional shift of a robust numeric verbalization prompt across single\nvs. two-step confidence elicitation procedure. We also compare the performance\nof the same prompt with different versions of GPT and different numerical\nscales. Finally, we combine the sample-based consistency and verbalized methods\nto propose a hybrid framework that yields a better uncertainty estimation for\nGPT models. Overall, our work proposes novel uncertainty quantification methods\nthat will improve the reliability of Large Language Models in misinformation\nmitigation applications.", "published": "2024-01-13 16:36:58", "link": "http://arxiv.org/abs/2401.08694v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tracing the Genealogies of Ideas with Large Language Model Embeddings", "abstract": "In this paper, I present a novel method to detect intellectual influence\nacross a large corpus. Taking advantage of the unique affordances of large\nlanguage models in encoding semantic and structural meaning while remaining\nrobust to paraphrasing, we can search for substantively similar ideas and hints\nof intellectual influence in a computationally efficient manner. Such a method\nallows us to operationalize different levels of confidence: we can allow for\ndirect quotation, paraphrase, or speculative similarity while remaining open\nabout the limitations of each threshold. I apply an ensemble method combining\nGeneral Text Embeddings, a state-of-the-art sentence embedding method optimized\nto capture semantic content and an Abstract Meaning Representation graph\nrepresentation designed to capture structural similarities in argumentation\nstyle and the use of metaphor. I apply this method to vectorize sentences from\na corpus of roughly 400,000 nonfiction books and academic publications from the\n19th century for instances of ideas and arguments appearing in Darwin's\npublications. This functions as an initial evaluation and proof of concept; the\nmethod is not limited to detecting Darwinian ideas but is capable of detecting\nsimilarities on a large scale in a wide range of corpora and contexts.", "published": "2024-01-13 18:42:27", "link": "http://arxiv.org/abs/2402.01661v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'\n  Mathematical Reasoning Capabilities", "abstract": "Recent large language models (LLMs) have shown indications of mathematical\nreasoning ability on challenging competition-level problems, especially with\nself-generated verbalizations of intermediate reasoning steps (i.e.,\nchain-of-thought prompting). However, current evaluations mainly focus on the\nend-to-end final answer correctness, and it is unclear whether LLMs can make\nuse of helpful side information such as problem-specific hints. In this paper,\nwe propose a challenging benchmark dataset for enabling such analyses. The\nConcept and Hint-Annotated Math Problems (CHAMP) consists of high school math\ncompetition problems, annotated with concepts, or general math facts, and\nhints, or problem-specific tricks. These annotations allow us to explore the\neffects of additional information, such as relevant hints, misleading concepts,\nor related problems. This benchmark is difficult, with the best model only\nscoring 58.1% in standard settings. With concepts and hints, performance\nsometimes improves, indicating that some models can make use of such side\ninformation. Furthermore, we annotate model-generated solutions for their\ncorrectness. Using this corpus, we find that models often arrive at the correct\nfinal answer through wrong reasoning steps. In addition, we test whether models\nare able to verify these solutions, and find that most models struggle.", "published": "2024-01-13 03:18:16", "link": "http://arxiv.org/abs/2401.06961v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Unsupervised and Supervised Training for Automatic Speech\n  Recognition via Bilevel Optimization", "abstract": "In this paper, we present a novel bilevel optimization-based training\napproach to training acoustic models for automatic speech recognition (ASR)\ntasks that we term {bi-level joint unsupervised and supervised training\n(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an\nunsupervised loss and a supervised loss respectively, leveraging recent\nadvances in penalty-based bilevel optimization to solve this challenging ASR\nproblem with affordable complexity and rigorous convergence guarantees.} To\nevaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2\ndatasets have been conducted. BL-JUST achieves superior performance over the\ncommonly used pre-training followed by fine-tuning strategy.", "published": "2024-01-13 05:01:47", "link": "http://arxiv.org/abs/2401.06980v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Edge-Enabled Anomaly Detection and Information Completion for Social\n  Network Knowledge Graphs", "abstract": "In the rapidly advancing information era, various human behaviors are being\nprecisely recorded in the form of data, including identity information,\ncriminal records, and communication data. Law enforcement agencies can\neffectively maintain social security and precisely combat criminal activities\nby analyzing the aforementioned data. In comparison to traditional data\nanalysis methods, deep learning models, relying on the robust computational\npower in cloud centers, exhibit higher accuracy in extracting data features and\ninferring data. However, within the architecture of cloud centers, the\ntransmission of data from end devices introduces significant latency, hindering\nreal-time inference of data. Furthermore, low-latency edge computing\narchitectures face limitations in direct deployment due to relatively weak\ncomputing and storage capacities of nodes. To address these challenges, a\nlightweight distributed knowledge graph completion architecture is proposed.\nFirstly, we introduce a lightweight distributed knowledge graph completion\narchitecture that utilizes knowledge graph embedding for data analysis.\nSubsequently, to filter out substandard data, a personnel data quality\nassessment method named PDQA is proposed. Lastly, we present a model pruning\nalgorithm that significantly reduces the model size while maximizing\nperformance, enabling lightweight deployment. In experiments, we compare the\neffects of 11 advanced models on completing the knowledge graph of public\nsecurity personnel information. The results indicate that the RotatE model\noutperforms other models significantly in knowledge graph completion, with the\npruned model size reduced by 70\\%, and hits@10 reaching 86.97\\%.}", "published": "2024-01-13 09:27:37", "link": "http://arxiv.org/abs/2401.07022v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Graph Language Models", "abstract": "While Language Models (LMs) are the workhorses of NLP, their interplay with\nstructured knowledge graphs (KGs) is still actively researched. Current methods\nfor encoding such graphs typically either (i) linearize them for embedding with\nLMs -- which underutilize structural information, or (ii) use Graph Neural\nNetworks (GNNs) to preserve the graph structure -- but GNNs cannot represent\ntext features as well as pretrained LMs. In our work we introduce a novel LM\ntype, the Graph Language Model (GLM), that integrates the strengths of both\napproaches and mitigates their weaknesses. The GLM parameters are initialized\nfrom a pretrained LM to enhance understanding of individual graph concepts and\ntriplets. Simultaneously, we design the GLM's architecture to incorporate graph\nbiases, thereby promoting effective knowledge distribution within the graph.\nThis enables GLMs to process graphs, texts, and interleaved inputs of both.\nEmpirical evaluations on relation classification tasks show that GLM embeddings\nsurpass both LM- and GNN-based baselines in supervised and zero-shot setting,\ndemonstrating their versatility.", "published": "2024-01-13 16:09:49", "link": "http://arxiv.org/abs/2401.07105v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.0; I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Assessing Large Language Models in Mechanical Engineering Education: A\n  Study on Mechanics-Focused Conceptual Understanding", "abstract": "This study is a pioneering endeavor to investigate the capabilities of Large\nLanguage Models (LLMs) in addressing conceptual questions within the domain of\nmechanical engineering with a focus on mechanics. Our examination involves a\nmanually crafted exam encompassing 126 multiple-choice questions, spanning\nvarious aspects of mechanics courses, including Fluid Mechanics, Mechanical\nVibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of\nElasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),\nChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against\nengineering faculties and students with or without mechanical engineering\nbackground. The findings reveal GPT-4's superior performance over the other two\nLLMs and human cohorts in answering questions across various mechanics topics,\nexcept for Continuum Mechanics. This signals the potential future improvements\nfor GPT models in handling symbolic calculations and tensor analyses. The\nperformances of LLMs were all significantly improved with explanations prompted\nprior to direct responses, underscoring the crucial role of prompt engineering.\nInterestingly, GPT-3.5 demonstrates improved performance with prompts covering\na broader domain, while GPT-4 excels with prompts focusing on specific\nsubjects. Finally, GPT-4 exhibits notable advancements in mitigating input\nbias, as evidenced by guessing preferences for humans. This study unveils the\nsubstantial potential of LLMs as highly knowledgeable assistants in both\nmechanical pedagogy and scientific research.", "published": "2024-01-13 19:19:04", "link": "http://arxiv.org/abs/2401.12983v1", "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "primary_category": "cs.CL"}
{"title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human\n  Personalities through Open Large Language Models", "abstract": "The emergence of unveiling human-like behaviors in Large Language Models\n(LLMs) has led to a closer connection between NLP and human psychology.\nScholars have been studying the inherent personalities exhibited by LLMs and\nattempting to incorporate human traits and behaviors into them. However, these\nefforts have primarily focused on commercially-licensed LLMs, neglecting the\nwidespread use and notable advancements seen in Open LLMs. This work aims to\naddress this gap by employing a set of 12 LLM Agents based on the most\nrepresentative Open models and subject them to a series of assessments\nconcerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five\nInventory (BFI) test. Our approach involves evaluating the intrinsic\npersonality traits of Open LLM agents and determining the extent to which these\nagents can mimic human personalities when conditioned by specific personalities\nand roles. Our findings unveil that $(i)$ each Open LLM agent showcases\ndistinct human personalities; $(ii)$ personality-conditioned prompting produces\nvarying effects on the agents, with only few successfully mirroring the imposed\npersonality, while most of them being ``closed-minded'' (i.e., they retain\ntheir intrinsic traits); and $(iii)$ combining role and personality\nconditioning can enhance the agents' ability to mimic human personalities. Our\nwork represents a step up in understanding the dense relationship between NLP\nand human psychology through the lens of Open LLMs.", "published": "2024-01-13 16:41:40", "link": "http://arxiv.org/abs/2401.07115v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.AI"}
{"title": "ScripTONES: Sentiment-Conditioned Music Generation for Movie Scripts", "abstract": "Film scores are considered an essential part of the film cinematic\nexperience, but the process of film score generation is often expensive and\ninfeasible for small-scale creators. Automating the process of film score\ncomposition would provide useful starting points for music in small projects.\nIn this paper, we propose a two-stage pipeline for generating music from a\nmovie script. The first phase is the Sentiment Analysis phase where the\nsentiment of a scene from the film script is encoded into the valence-arousal\ncontinuous space. The second phase is the Conditional Music Generation phase\nwhich takes as input the valence-arousal vector and conditionally generates\npiano MIDI music to match the sentiment. We study the efficacy of various music\ngeneration architectures by performing a qualitative user survey and propose\nmethods to improve sentiment-conditioning in VAE architectures.", "published": "2024-01-13 14:19:28", "link": "http://arxiv.org/abs/2401.07084v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
