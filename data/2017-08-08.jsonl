{"title": "Mining fine-grained opinions on closed captions of YouTube videos with\n  an attention-RNN", "abstract": "Video reviews are the natural evolution of written product reviews. In this\npaper we target this phenomenon and introduce the first dataset created from\nclosed captions of YouTube product review videos as well as a new attention-RNN\nmodel for aspect extraction and joint aspect extraction and sentiment\nclassification. Our model provides state-of-the-art performance on aspect\nextraction without requiring the usage of hand-crafted features on the SemEval\nABSA corpus, while it outperforms the baseline on the joint task. In our\ndataset, the attention-RNN model outperforms the baseline for both tasks, but\nwe observe important performance drops for all models in comparison to SemEval.\nThese results, as well as further experiments on domain adaptation for aspect\nextraction, suggest that differences between speech and written text, which\nhave been discussed extensively in the literature, also extend to the domain of\nproduct reviews, where they are relevant for fine-grained opinion mining.", "published": "2017-08-08 09:27:55", "link": "http://arxiv.org/abs/1708.02420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural-based Context Representation Learning for Dialog Act\n  Classification", "abstract": "We explore context representation learning methods in neural-based models for\ndialog act classification. We propose and compare extensively different methods\nwhich combine recurrent neural network architectures and attention mechanisms\n(AMs) at different context levels. Our experimental results on two benchmark\ndatasets show consistent improvements compared to the models without contextual\ninformation and reveal that the most suitable AM in the architecture depends on\nthe nature of the dataset.", "published": "2017-08-08 17:03:25", "link": "http://arxiv.org/abs/1708.02561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Encoding is the Best for Text Classification in Chinese, English,\n  Japanese and Korean?", "abstract": "This article offers an empirical study on the different ways of encoding\nChinese, Japanese, Korean (CJK) and English languages for text classification.\nDifferent encoding levels are studied, including UTF-8 bytes, characters,\nwords, romanized characters and romanized words. For all encoding levels,\nwhenever applicable, we provide comparisons with linear models, fastText and\nconvolutional networks. For convolutional networks, we compare between encoding\nmechanisms using character glyph images, one-hot (or one-of-n) encoding, and\nembedding. In total there are 473 models, using 14 large-scale text\nclassification datasets in 4 languages including Chinese, English, Japanese and\nKorean. Some conclusions from these results include that byte-level one-hot\nencoding based on UTF-8 consistently produces competitive results for\nconvolutional networks, that word-level n-grams linear models are competitive\neven without perfect word segmentation, and that fastText provides the best\nresult using character-level n-gram encoding but can overfit when the features\nare overly rich.", "published": "2017-08-08 21:24:44", "link": "http://arxiv.org/abs/1708.02657v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "abstract": "Active learning aims to select a small subset of data for annotation such\nthat a classifier learned on the data is highly accurate. This is usually done\nusing heuristic selection methods, however the effectiveness of such methods is\nlimited and moreover, the performance of heuristics varies between datasets. To\naddress these shortcomings, we introduce a novel formulation by reframing the\nactive learning as a reinforcement learning problem and explicitly learning a\ndata selection policy, where the policy takes the role of the active learning\nheuristic. Importantly, our method allows the selection policy learned using\nsimulation on one language to be transferred to other languages. We demonstrate\nour method using cross-lingual named entity recognition, observing uniform\nimprovements over traditional active learning.", "published": "2017-08-08 07:06:48", "link": "http://arxiv.org/abs/1708.02383v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
