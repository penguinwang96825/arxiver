{"title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", "abstract": "This paper is concerned with nearest neighbor search in distributional\nsemantic models. A normal nearest neighbor search only returns a ranked list of\nneighbors, with no information about the structure or topology of the local\nneighborhood. This is a potentially serious shortcoming of the mode of querying\na distributional semantic model, since a ranked list of neighbors may conflate\nseveral different senses. We argue that the topology of neighborhoods in\nsemantic space provides important information about the different senses of\nterms, and that such topological structures can be used for word-sense\ninduction. We also argue that the topology of the neighborhoods in semantic\nspace can be used to determine the semantic horizon of a point, which we define\nas the set of neighbors that have a direct connection to the point. We\nintroduce relative neighborhood graphs as method to uncover the topological\nproperties of neighborhoods in semantic models. We also provide examples of\nrelative neighborhood graphs for three well-known semantic models; the PMI\nmodel, the GloVe model, and the skipgram model.", "published": "2015-01-12 14:48:54", "link": "http://arxiv.org/abs/1501.02670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Visual Attributes to Adjectives through Decompositional\n  Distributional Semantics", "abstract": "As automated image analysis progresses, there is increasing interest in\nricher linguistic annotation of pictures, with attributes of objects (e.g.,\nfurry, brown...) attracting most attention. By building on the recent\n\"zero-shot learning\" approach, and paying attention to the linguistic nature of\nattributes as noun modifiers, and specifically adjectives, we show that it is\npossible to tag images with attribute-denoting adjectives even when no training\ndata containing the relevant annotation are available. Our approach relies on\ntwo key observations. First, objects can be seen as bundles of attributes,\ntypically expressed as adjectival modifiers (a dog is something furry, brown,\netc.), and thus a function trained to map visual representations of objects to\nnominal labels can implicitly learn to map attributes to adjectives. Second,\nobjects and attributes come together in pictures (the same thing is a dog and\nit is brown). We can thus achieve better attribute (and object) label retrieval\nby treating images as \"visual phrases\", and decomposing their linguistic\nrepresentation into an attribute-denoting adjective and an object-denoting\nnoun. Our approach performs comparably to a method exploiting manual attribute\nannotation, it outperforms various competitive alternatives in both attribute\nand object annotation, and it automatically constructs attribute-centric\nrepresentations that significantly improve performance in supervised object\nrecognition.", "published": "2015-01-12 16:48:19", "link": "http://arxiv.org/abs/1501.02714v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Autodetection and Classification of Hidden Cultural City Districts from\n  Yelp Reviews", "abstract": "Topic models are a way to discover underlying themes in an otherwise\nunstructured collection of documents. In this study, we specifically used the\nLatent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to\nclassify restaurants based off of their reviews. Furthermore, we hypothesize\nthat within a city, restaurants can be grouped into similar \"clusters\" based on\nboth location and similarity. We used several different clustering methods,\nincluding K-means Clustering and a Probabilistic Mixture Model, in order to\nuncover and classify districts, both well-known and hidden (i.e. cultural areas\nlike Chinatown or hearsay like \"the best street for Italian restaurants\")\nwithin a city. We use these models to display and label different clusters on a\nmap. We also introduce a topic similarity heatmap that displays the similarity\ndistribution in a city to a new restaurant.", "published": "2015-01-12 03:10:01", "link": "http://arxiv.org/abs/1501.02527v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Dataset for Movie Description", "abstract": "Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production.", "published": "2015-01-12 03:31:33", "link": "http://arxiv.org/abs/1501.02530v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Combining Language and Vision with a Multimodal Skip-gram Model", "abstract": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual\ninformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)\nbuild vector-based word representations by learning to predict linguistic\ncontexts in text corpora. However, for a restricted set of words, the models\nare also exposed to visual representations of the objects they denote\n(extracted from natural images), and must predict linguistic and visual\nfeatures jointly. The MMSKIP-GRAM models achieve good performance on a variety\nof semantic benchmarks. Moreover, since they propagate visual information to\nall words, we use them to improve image labeling and retrieval in the zero-shot\nsetup, where the test concepts are never seen during model training. Finally,\nthe MMSKIP-GRAM models discover intriguing visual properties of abstract words,\npaving the way to realistic implementations of embodied theories of meaning.", "published": "2015-01-12 10:48:32", "link": "http://arxiv.org/abs/1501.02598v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
