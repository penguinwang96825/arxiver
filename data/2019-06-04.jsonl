{"title": "Simultaneous Translation with Flexible Policy via Restricted Imitation\n  Learning", "abstract": "Simultaneous translation is widely useful but remains one of the most\ndifficult tasks in NLP. Previous work either uses fixed-latency policies, or\ntrain a complicated two-staged model using reinforcement learning. We propose a\nmuch simpler single model that adds a `delay' token to the target vocabulary,\nand design a restricted dynamic oracle to greatly simplify training.\nExperiments on Chinese<->English simultaneous translation show that our work\nleads to flexible policies that achieve better BLEU scores and lower latencies\ncompared to both fixed and RL-learned policies.", "published": "2019-06-04 00:11:52", "link": "http://arxiv.org/abs/1906.01135v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System Demo for Transfer Learning across Vision and Text using Domain\n  Specific CNN Accelerator for On-Device NLP Applications", "abstract": "Power-efficient CNN Domain Specific Accelerator (CNN-DSA) chips are currently\navailable for wide use in mobile devices. These chips are mainly used in\ncomputer vision applications. However, the recent work of Super Characters\nmethod for text classification and sentiment analysis tasks using\ntwo-dimensional CNN models has also achieved state-of-the-art results through\nthe method of transfer learning from vision to text. In this paper, we\nimplemented the text classification and sentiment analysis applications on\nmobile devices using CNN-DSA chips. Compact network representations using\none-bit and three-bits precision for coefficients and five-bits for activations\nare used in the CNN-DSA chip with power consumption less than 300mW. For edge\ndevices under memory and compute constraints, the network is further compressed\nby approximating the external Fully Connected (FC) layers within the CNN-DSA\nchip. At the workshop, we have two system demonstrations for NLP tasks. The\nfirst demo classifies the input English Wikipedia sentence into one of the 14\nontologies. The second demo classifies the Chinese online-shopping review into\npositive or negative.", "published": "2019-06-04 00:51:23", "link": "http://arxiv.org/abs/1906.01145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Long Distance Slot Carryover in Spoken Dialogue Systems", "abstract": "Tracking the state of the conversation is a central component in\ntask-oriented spoken dialogue systems. One such approach for tracking the\ndialogue state is slot carryover, where a model makes a binary decision if a\nslot from the context is relevant to the current turn. Previous work on the\nslot carryover task used models that made independent decisions for each slot.\nA close analysis of the results show that this approach results in poor\nperformance over longer context dialogues. In this paper, we propose to jointly\nmodel the slots. We propose two neural network architectures, one based on\npointer networks that incorporate slot ordering information, and the other\nbased on transformer networks that uses self attention mechanism to model the\nslot interdependencies. Our experiments on an internal dialogue benchmark\ndataset and on the public DSTC2 dataset demonstrate that our proposed models\nare able to resolve longer distance slot references and are able to achieve\ncompetitive performance.", "published": "2019-06-04 01:13:20", "link": "http://arxiv.org/abs/1906.01149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Local Insights from Global Labels: Supervised & Zero-Shot\n  Sequence Labeling via a Convolutional Decomposition", "abstract": "We propose a new, more actionable view of neural network interpretability and\ndata analysis by leveraging the remarkable matching effectiveness of\nrepresentations derived from deep networks, guided by an approach for\nclass-conditional feature detection. The decomposition of the filter-ngram\ninteractions of a convolutional neural network and a linear layer over a\npre-trained deep network yields a strong binary sequence labeler, with\nflexibility in producing predictions at -- and defining loss functions for --\nvarying label granularities, from the fully-supervised sequence labeling\nsetting to the challenging zero-shot sequence labeling setting, in which we\nseek token-level predictions but only have document-level labels for training.\nFrom this sequence-labeling layer we derive dense representations of the input\nthat can then be matched to instances from training, or a support set with\nknown labels. Such introspection with inference-time decision rules provides a\nmeans, in some settings, of making local updates to the model by altering the\nlabels or instances in the support set without re-training the full model.\nFinally, we construct a particular K-nearest neighbors (K-NN) model from\nmatched exemplar representations that approximates the original model's\npredictions and is at least as effective a predictor with respect to the\nground-truth labels. This additionally yields interpretable heuristics at the\ntoken level for determining when predictions are less likely to be reliable,\nand for screening input dissimilar to the support set. In effect, we show that\nwe can transform the deep network into a simple weighting over exemplars and\nassociated labels, yielding an introspectable -- and modestly updatable --\nversion of the original model.", "published": "2019-06-04 01:54:42", "link": "http://arxiv.org/abs/1906.01154v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Zero-shot Neural Machine Translation via Ignoring Spurious\n  Correlations", "abstract": "Zero-shot translation, translating between language pairs on which a Neural\nMachine Translation (NMT) system has never been trained, is an emergent\nproperty when training the system in multilingual settings. However, naive\ntraining for zero-shot NMT easily fails, and is sensitive to hyper-parameter\nsetting. The performance typically lags far behind the more conventional\npivot-based approach which translates twice using a third language as a pivot.\nIn this work, we address the degeneracy problem due to capturing spurious\ncorrelations by quantitatively analyzing the mutual information between\nlanguage IDs of the source and decoded sentences. Inspired by this analysis, we\npropose to use two simple but effective approaches: (1) decoder pre-training;\n(2) back-translation. These methods show significant improvement (4~22 BLEU\npoints) over the vanilla zero-shot translation on three challenging\nmultilingual datasets, and achieve similar or better results than the\npivot-based approach.", "published": "2019-06-04 03:30:22", "link": "http://arxiv.org/abs/1906.01181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Converse Attention Knowledge Transfer for Low-Resource Named Entity\n  Recognition", "abstract": "In recent years, great success has been achieved in many tasks of natural\nlanguage processing (NLP), e.g., named entity recognition (NER), especially in\nthe high-resource language, i.e., English, thanks in part to the considerable\namount of labeled resources. However, most low-resource languages do not have\nsuch an abundance of labeled data as high-resource English, leading to poor\nperformance of NER in these low-resource languages. Inspired by knowledge\ntransfer, we propose Converse Attention Network, or CAN in short, to improve\nthe performance of NER in low-resource languages by leveraging the knowledge\nlearned in pretrained high-resource English models. CAN first translates\nlow-resource languages into high-resource English using an attention based\ntranslation module. In the process of translation, CAN obtain the attention\nmatrices that align the two languages. Furthermore, CAN use the attention\nmatrices to align the high-resource semantic features from a pretrained\nhigh-resource English model with the low-resource semantic features. As a\nresult, CAN obtains aligned high-resource semantic features to enrich the\nrepresentations of low-resource languages. Experiments on four low-resource NER\ndatasets show that CAN achieves consistent and significant performance\nimprovements, which indicates the effectiveness of CAN.", "published": "2019-06-04 03:33:51", "link": "http://arxiv.org/abs/1906.01183v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Effects of Context and User History for Predicting Online\n  Conversation Re-entries", "abstract": "As the online world continues its exponential growth, interpersonal\ncommunication has come to play an increasingly central role in opinion\nformation and change. In order to help users better engage with each other\nonline, we study a challenging problem of re-entry prediction foreseeing\nwhether a user will come back to a conversation they once participated in. We\nhypothesize that both the context of the ongoing conversations and the users'\nprevious chatting history will affect their continued interests in future\nengagement. Specifically, we propose a neural framework with three main layers,\neach modeling context, user history, and interactions between them, to explore\nhow the conversation context and user chatting history jointly result in their\nre-entry behavior. We experiment with two large-scale datasets collected from\nTwitter and Reddit. Results show that our proposed framework with bi-attention\nachieves an F1 score of 61.1 on Twitter conversations, outperforming the\nstate-of-the-art methods from previous work.", "published": "2019-06-04 03:43:37", "link": "http://arxiv.org/abs/1906.01185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Self-Supervised Attention Learning for Aspect-Level\n  Sentiment Analysis", "abstract": "In aspect-level sentiment classification (ASC), it is prevalent to equip\ndominant neural models with attention mechanisms, for the sake of acquiring the\nimportance of each context word on the given aspect. However, such a mechanism\ntends to excessively focus on a few frequent words with sentiment polarities,\nwhile ignoring infrequent ones. In this paper, we propose a progressive\nself-supervised attention learning approach for neural ASC models, which\nautomatically mines useful attention supervision information from a training\ncorpus to refine attention mechanisms. Specifically, we iteratively conduct\nsentiment predictions on all training instances. Particularly, at each\niteration, the context word with the maximum attention weight is extracted as\nthe one with active/misleading influence on the correct/incorrect prediction of\nevery instance, and then the word itself is masked for subsequent iterations.\nFinally, we augment the conventional training objective with a regularization\nterm, which enables ASC models to continue equally focusing on the extracted\nactive context words while decreasing weights of those misleading ones.\nExperimental results on multiple datasets show that our proposed approach\nyields better attention mechanisms, leading to substantial improvements over\nthe two state-of-the-art neural ASC models. Source code and trained models are\navailable at https://github.com/DeepLearnXMU/PSSAttention.", "published": "2019-06-04 06:07:56", "link": "http://arxiv.org/abs/1906.01213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Independent Prediction to Re-ordered Prediction: Integrating\n  Relative Position and Global Label Information to Emotion Cause\n  Identification", "abstract": "Emotion cause identification aims at identifying the potential causes that\nlead to a certain emotion expression in text. Several techniques including rule\nbased methods and traditional machine learning methods have been proposed to\naddress this problem based on manually designed rules and features. More\nrecently, some deep learning methods have also been applied to this task, with\nthe attempt to automatically capture the causal relationship of emotion and its\ncauses embodied in the text. In this work, we find that in addition to the\ncontent of the text, there are another two kinds of information, namely\nrelative position and global labels, that are also very important for emotion\ncause identification. To integrate such information, we propose a model based\non the neural network architecture to encode the three elements ($i.e.$, text\ncontent, relative position and global label), in an unified and end-to-end\nfashion. We introduce a relative position augmented embedding learning\nalgorithm, and transform the task from an independent prediction problem to a\nreordered prediction problem, where the dynamic global label information is\nincorporated. Experimental results on a benchmark emotion cause dataset show\nthat our model achieves new state-of-the-art performance and performs\nsignificantly better than a number of competitive baselines. Further analysis\nshows the effectiveness of the relative position augmented embedding learning\nalgorithm and the reordered prediction mechanism with dynamic global labels.", "published": "2019-06-04 07:02:22", "link": "http://arxiv.org/abs/1906.01230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause\n  Extraction", "abstract": "The emotion cause extraction (ECE) task aims at discovering the potential\ncauses behind a certain emotion expression in a document. Techniques including\nrule-based methods, traditional machine learning methods and deep neural\nnetworks have been proposed to solve this task. However, most of the previous\nwork considered ECE as a set of independent clause classification problems and\nignored the relations between multiple clauses in a document. In this work, we\npropose a joint emotion cause extraction framework, named RNN-Transformer\nHierarchical Network (RTHN), to encode and classify multiple clauses\nsynchronously. RTHN is composed of a lower word-level encoder based on RNNs to\nencode multiple words in each clause, and an upper clause-level encoder based\non Transformer to learn the correlation between multiple clauses in a document.\nWe furthermore propose ways to encode the relative position and global\npredication information into Transformer that can capture the causality between\nclauses and make RTHN more efficient. We finally achieve the best performance\namong 12 compared systems and improve the F1 score of the state-of-the-art from\n72.69\\% to 76.77\\%.", "published": "2019-06-04 07:10:16", "link": "http://arxiv.org/abs/1906.01236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning\n  Easy-First Strategies", "abstract": "In Semantic Dependency Parsing (SDP), semantic relations form directed\nacyclic graphs, rather than trees. We propose a new iterative predicate\nselection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based\nand transition-based parsing approaches in order to handle multiple semantic\nhead words. We train the IPS model using a combination of multi-task learning\nand task-specific policy gradient training. Trained this way, IPS achieves a\nnew state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we\nobserve that policy gradient training learns an easy-first strategy.", "published": "2019-06-04 07:13:49", "link": "http://arxiv.org/abs/1906.01239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Explain: Answering Why-Questions via Rephrasing", "abstract": "Providing plausible responses to why questions is a challenging but critical\ngoal for language based human-machine interaction. Explanations are challenging\nin that they require many different forms of abstract knowledge and reasoning.\nPrevious work has either relied on human-curated structured knowledge bases or\ndetailed domain representation to generate satisfactory explanations. They are\nalso often limited to ranking pre-existing explanation choices. In our work, we\ncontribute to the under-explored area of generating natural language\nexplanations for general phenomena. We automatically collect large datasets of\nexplanation-phenomenon pairs which allow us to train sequence-to-sequence\nmodels to generate natural language explanations. We compare different training\nstrategies and evaluate their performance using both automatic scores and human\nratings. We demonstrate that our strategy is sufficient to generate highly\nplausible explanations for general open-domain phenomena compared to other\nmodels trained on different datasets.", "published": "2019-06-04 07:27:35", "link": "http://arxiv.org/abs/1906.01243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test", "abstract": "Cloze-style reading comprehension in Chinese is still limited due to the lack\nof various corpora. In this paper we propose a large-scale Chinese cloze test\ndataset ChID, which studies the comprehension of idiom, a unique language\nphenomenon in Chinese. In this corpus, the idioms in a passage are replaced by\nblank symbols and the correct answer needs to be chosen from well-designed\ncandidate idioms. We carefully study how the design of candidate idioms and the\nrepresentation of idioms affect the performance of state-of-the-art models.\nResults show that the machine accuracy is substantially worse than that of\nhuman, indicating a large space for further research.", "published": "2019-06-04 08:29:00", "link": "http://arxiv.org/abs/1906.01265v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts", "abstract": "Emotion cause extraction (ECE), the task aimed at extracting the potential\ncauses behind certain emotions in text, has gained much attention in recent\nyears due to its wide applications. However, it suffers from two shortcomings:\n1) the emotion must be annotated before cause extraction in ECE, which greatly\nlimits its applications in real-world scenarios; 2) the way to first annotate\nemotion and then extract the cause ignores the fact that they are mutually\nindicative. In this work, we propose a new task: emotion-cause pair extraction\n(ECPE), which aims to extract the potential pairs of emotions and corresponding\ncauses in a document. We propose a 2-step approach to address this new ECPE\ntask, which first performs individual emotion extraction and cause extraction\nvia multi-task learning, and then conduct emotion-cause pairing and filtering.\nThe experimental results on a benchmark emotion cause corpus prove the\nfeasibility of the ECPE task as well as the effectiveness of our approach.", "published": "2019-06-04 08:29:26", "link": "http://arxiv.org/abs/1906.01267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Sentential Context for Neural Machine Translation", "abstract": "In this work, we present novel approaches to exploit sentential context for\nneural machine translation (NMT). Specifically, we first show that a shallow\nsentential context extracted from the top encoder layer only, can improve\ntranslation performance via contextualizing the encoding representations of\nindividual words. Next, we introduce a deep sentential context, which\naggregates the sentential context representations from all the internal layers\nof the encoder to form a more comprehensive context representation.\nExperimental results on the WMT14 English-to-German and English-to-French\nbenchmarks show that our model consistently improves performance over the\nstrong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity\nand effectiveness of exploiting sentential context for NMT.", "published": "2019-06-04 08:29:33", "link": "http://arxiv.org/abs/1906.01268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are we there yet? Encoder-decoder neural networks as cognitive models of\n  English past tense inflection", "abstract": "The cognitive mechanisms needed to account for the English past tense have\nlong been a subject of debate in linguistics and cognitive science. Neural\nnetwork models were proposed early on, but were shown to have clear flaws.\nRecently, however, Kirov and Cotterell (2018) showed that modern\nencoder-decoder (ED) models overcome many of these flaws. They also presented\nevidence that ED models demonstrate humanlike performance in a nonce-word task.\nHere, we look more closely at the behaviour of their model in this task. We\nfind that (1) the model exhibits instability across multiple simulations in\nterms of its correlation with human data, and (2) even when results are\naggregated across simulations (treating each simulation as an individual human\nparticipant), the fit to the human data is not strong---worse than an older\nrule-based model. These findings hold up through several alternative training\nregimes and evaluation measures. Although other neural architectures might do\nbetter, we conclude that there is still insufficient evidence to claim that\nneural nets are a good cognitive model for this task.", "published": "2019-06-04 08:56:56", "link": "http://arxiv.org/abs/1906.01280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lattice-Based Transformer Encoder for Neural Machine Translation", "abstract": "Neural machine translation (NMT) takes deterministic sequences for source\nrepresentations. However, either word-level or subword-level segmentations have\nmultiple choices to split a source sequence with different word segmentors or\ndifferent subword vocabulary sizes. We hypothesize that the diversity in\nsegmentations may affect the NMT performance. To integrate different\nsegmentations with the state-of-the-art NMT model, Transformer, we propose\nlattice-based encoders to explore effective word or subword representation in\nan automatic way during training. We propose two methods: 1) lattice positional\nencoding and 2) lattice-aware self-attention. These two methods can be used\ntogether and show complementary to each other to further improve translation\nperformance. Experiment results show superiorities of lattice-based encoders in\nword-level and subword-level representations over conventional Transformer\nencoder.", "published": "2019-06-04 08:58:14", "link": "http://arxiv.org/abs/1906.01282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Large Are Lions? Inducing Distributions over Quantitative Attributes", "abstract": "Most current NLP systems have little knowledge about quantitative attributes\nof objects and events. We propose an unsupervised method for collecting\nquantitative information from large amounts of web data, and use it to create a\nnew, very large resource consisting of distributions over physical quantities\nassociated with objects, adjectives, and verbs which we call Distributions over\nQuantitative (DoQ). This contrasts with recent work in this area which has\nfocused on making only relative comparisons such as \"Is a lion bigger than a\nwolf?\". Our evaluation shows that DoQ compares favorably with state of the art\nresults on existing datasets for relative comparisons of nouns and adjectives,\nand on a new dataset we introduce.", "published": "2019-06-04 10:34:33", "link": "http://arxiv.org/abs/1906.01327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curate and Generate: A Corpus and Method for Joint Control of Semantics\n  and Style in Neural NLG", "abstract": "Neural natural language generation (NNLG) from structured meaning\nrepresentations has become increasingly popular in recent years. While we have\nseen progress with generating syntactically correct utterances that preserve\nsemantics, various shortcomings of NNLG systems are clear: new tasks require\nnew training data which is not available or straightforward to acquire, and\nmodel outputs are simple and may be dull and repetitive. This paper addresses\nthese two critical challenges in NNLG by: (1) scalably (and at no cost)\ncreating training datasets of parallel meaning representations and reference\ntexts with rich style markup by using data from freely available and naturally\ndescriptive user reviews, and (2) systematically exploring how the style markup\nenables joint control of semantic and stylistic aspects of neural model output.\nWe present YelpNLG, a corpus of 300,000 rich, parallel meaning representations\nand highly stylistically varied reference texts spanning different restaurant\nattributes, and describe a novel methodology that can be scalably reused to\ngenerate NLG datasets for other domains. The experiments show that the models\ncontrol important aspects, including lexical choice of adjectives, output\nlength, and sentiment, allowing the models to successfully hit multiple style\ntargets without sacrificing semantics.", "published": "2019-06-04 10:51:32", "link": "http://arxiv.org/abs/1906.01334v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence\n  Matching", "abstract": "We present a latent variable model for predicting the relationship between a\npair of text sequences. Unlike previous auto-encoding--based approaches that\nconsider each sequence separately, our proposed framework utilizes both\nsequences within a single model by generating a sequence that has a given\nrelationship with a source sequence. We further extend the cross-sentence\ngenerating framework to facilitate semi-supervised training. We also define\nnovel semantic constraints that lead the decoder network to generate\nsemantically plausible and diverse sequences. We demonstrate the effectiveness\nof the proposed model from quantitative and qualitative experiments, while\nachieving state-of-the-art results on semi-supervised natural language\ninference and paraphrase identification.", "published": "2019-06-04 11:03:49", "link": "http://arxiv.org/abs/1906.01343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper\n  Summarization Based on Conference Talks", "abstract": "Currently, no large-scale training data is available for the task of\nscientific paper summarization. In this paper, we propose a novel method that\nautomatically generates summaries for scientific papers, by utilizing videos of\ntalks at scientific conferences. We hypothesize that such talks constitute a\ncoherent and concise description of the papers' content, and can form the basis\nfor good summaries. We collected 1716 papers and their corresponding videos,\nand created a dataset of paper summaries. A model trained on this dataset\nachieves similar performance as models trained on a dataset of summaries\ncreated manually. In addition, we validated the quality of our summaries by\nhuman experts.", "published": "2019-06-04 11:25:14", "link": "http://arxiv.org/abs/1906.01351v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire", "abstract": "Named entity recognition (NER) is widely used in natural language processing\napplications and downstream tasks. However, most NER tools target flat\nannotation from popular datasets, eschewing the semantic information available\nin nested entity mentions. We describe NNE---a fine-grained, nested named\nentity dataset over the full Wall Street Journal portion of the Penn Treebank\n(PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to\n6 layers of nesting. We hope the public release of this large dataset for\nEnglish newswire will encourage development of new techniques for nested NER.", "published": "2019-06-04 11:46:37", "link": "http://arxiv.org/abs/1906.01359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HighRES: Highlight-based Reference-less Evaluation of Summarization", "abstract": "There has been substantial progress in summarization research enabled by the\navailability of novel, often large-scale, datasets and recent advances on\nneural network-based approaches. However, manual evaluation of the system\ngenerated summaries is inconsistent due to the difficulty the task poses to\nhuman non-expert readers. To address this issue, we propose a novel approach\nfor manual evaluation, Highlight-based Reference-less Evaluation of\nSummarization (HighRES), in which summaries are assessed by multiple annotators\nagainst the source document via manually highlighted salient content in the\nlatter. Thus summary assessment on the source document by human judges is\nfacilitated, while the highlights can be used for evaluating multiple systems.\nTo validate our approach we employ crowd-workers to augment with highlights a\nrecently proposed dataset and compare two state-of-the-art systems. We\ndemonstrate that HighRES improves inter-annotator agreement in comparison to\nusing the source document directly, while they help emphasize differences among\nsystems that would be ignored under other evaluation approaches.", "published": "2019-06-04 11:47:23", "link": "http://arxiv.org/abs/1906.01361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relational Word Embeddings", "abstract": "While word embeddings have been shown to implicitly encode various forms of\nattributional knowledge, the extent to which they capture relational\ninformation is far more limited. In previous work, this limitation has been\naddressed by incorporating relational knowledge from external knowledge bases\nwhen learning the word embedding. Such strategies may not be optimal, however,\nas they are limited by the coverage of available resources and conflate\nsimilarity with other forms of relatedness. As an alternative, in this paper we\npropose to encode relational knowledge in a separate word embedding, which is\naimed to be complementary to a given standard word embedding. This relational\nword embedding is still learned from co-occurrence statistics, and can thus be\nused even when no external knowledge base is available. Our analysis shows that\nrelational word vectors do indeed capture information that is complementary to\nwhat is encoded in standard word embeddings.", "published": "2019-06-04 12:30:02", "link": "http://arxiv.org/abs/1906.01373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distantly Supervised Named Entity Recognition using Positive-Unlabeled\n  Learning", "abstract": "In this work, we explore the way to perform named entity recognition (NER)\nusing only unlabeled data and named entity dictionaries. To this end, we\nformulate the task as a positive-unlabeled (PU) learning problem and\naccordingly propose a novel PU learning algorithm to perform the task. We prove\nthat the proposed algorithm can unbiasedly and consistently estimate the task\nloss as if there is fully labeled data. A key feature of the proposed method is\nthat it does not require the dictionaries to label every entity within a\nsentence, and it even does not require the dictionaries to label all of the\nwords constituting an entity. This greatly reduces the requirement on the\nquality of the dictionaries and makes our method generalize well with quite\nsimple dictionaries. Empirical studies on four public NER datasets demonstrate\nthe effectiveness of our proposed method. We have published the source code at\n\\url{https://github.com/v-mipeng/LexiconNER}.", "published": "2019-06-04 12:39:10", "link": "http://arxiv.org/abs/1906.01378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognising Agreement and Disagreement between Stances with Reason\n  Comparing Networks", "abstract": "We identify agreement and disagreement between utterances that express\nstances towards a topic of discussion. Existing methods focus mainly on\nconversational settings, where dialogic features are used for (dis)agreement\ninference. We extend this scope and seek to detect stance (dis)agreement in a\nbroader setting, where independent stance-bearing utterances, which prevail in\nmany stance corpora and real-world scenarios, are compared. To cope with such\nnon-dialogic utterances, we find that the reasons uttered to back up a specific\nstance can help predict stance (dis)agreements. We propose a reason comparing\nnetwork (RCN) to leverage reason information for stance comparison. Empirical\nresults on a well-known stance corpus show that our method can discover useful\nreason information, enabling it to outperform several baselines in stance\n(dis)agreement detection.", "published": "2019-06-04 13:04:38", "link": "http://arxiv.org/abs/1906.01392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Discourse in Structured Text Representations", "abstract": "Discourse structure is integral to understanding a text and is helpful in\nmany NLP tasks. Learning latent representations of discourse is an attractive\nalternative to acquiring expensive labeled discourse data. Liu and Lapata\n(2018) propose a structured attention mechanism for text classification that\nderives a tree over a text, akin to an RST discourse tree. We examine this\nmodel in detail, and evaluate on additional discourse-relevant tasks and\ndatasets, in order to assess whether the structured attention improves\nperformance on the end task and whether it captures a text's discourse\nstructure. We find the learned latent trees have little to no structure and\ninstead focus on lexical cues; even after obtaining more structured trees with\nproposed model modifications, the trees are still far from capturing discourse\nstructure when compared to discourse dependency trees from an existing\ndiscourse parser. Finally, ablation studies show the structured attention\nprovides little benefit, sometimes even hurting performance.", "published": "2019-06-04 14:21:22", "link": "http://arxiv.org/abs/1906.01472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Neural Response Selection for Task-Oriented Dialogue Systems", "abstract": "Despite their popularity in the chatbot literature, retrieval-based models\nhave had modest impact on task-oriented dialogue systems, with the main\nobstacle to their application being the low-data regime of most task-oriented\ndialogue tasks. Inspired by the recent success of pretraining in language\nmodelling, we propose an effective method for deploying response selection in\ntask-oriented dialogue. To train response selection models for task-oriented\ndialogue tasks, we propose a novel method which: 1) pretrains the response\nselection model on large general-domain conversational corpora; and then 2)\nfine-tunes the pretrained model for the target dialogue domain, relying only on\nthe small in-domain dataset to capture the nuances of the given dialogue\ndomain. Our evaluation on six diverse application domains, ranging from\ne-commerce to banking, demonstrates the effectiveness of the proposed training\nmethod.", "published": "2019-06-04 15:58:54", "link": "http://arxiv.org/abs/1906.01543v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence Tagging with Contextual and Non-Contextual Subword\n  Representations: A Multilingual Evaluation", "abstract": "Pretrained contextual and non-contextual subword embeddings have become\navailable in over 250 languages, allowing massively multilingual NLP. However,\nwhile there is no dearth of pretrained embeddings, the distinct lack of\nsystematic evaluations makes it difficult for practitioners to choose between\nthem. In this work, we conduct an extensive evaluation comparing non-contextual\nsubword embeddings, namely FastText and BPEmb, and a contextual representation\nmethod, namely BERT, on multilingual named entity recognition and\npart-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and\ncharacter representations works best across languages and tasks. A more\ndetailed analysis reveals different strengths and weaknesses: Multilingual BERT\nperforms well in medium- to high-resource languages, but is outperformed by\nnon-contextual subword embeddings in a low-resource setting.", "published": "2019-06-04 16:36:53", "link": "http://arxiv.org/abs/1906.01569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Feature Extraction techniques for Sentiment Analysis", "abstract": "Sentiment Analysis refers to the study of systematically extracting the\nmeaning of subjective text . When analysing sentiments from the subjective text\nusing Machine Learning techniques,feature extraction becomes a significant\npart. We perform a study on the performance of feature extraction techniques\nTF-IDF(Term Frequency-Inverse Document Frequency) and Doc2vec (Document to\nVector) using Cornell movie review datasets, UCI sentiment labeled datasets,\nstanford movie review datasets,effectively classifying the text into positive\nand negative polarities by using various pre-processing methods like\neliminating StopWords and Tokenization which increases the performance of\nsentiment analysis in terms of accuracy and time taken by the classifier.The\nfeatures obtained after applying feature extraction techniques on the text\nsentences are trained and tested using the classifiers Logistic\nRegression,Support Vector Machines,K-Nearest Neighbours , Decision Tree and\nBernoulli Nave Bayes", "published": "2019-06-04 16:37:44", "link": "http://arxiv.org/abs/1906.01573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pitfalls in the Evaluation of Sentence Embeddings", "abstract": "Deep learning models continuously break new records across different NLP\ntasks. At the same time, their success exposes weaknesses of model evaluation.\nHere, we compile several key pitfalls of evaluation of sentence embeddings, a\ncurrently very popular NLP paradigm. These pitfalls include the comparison of\nembeddings of different sizes, normalization of embeddings, and the low (and\ndiverging) correlations between transfer and probing tasks. Our motivation is\nto challenge the current evaluation of sentence embeddings and to provide an\neasy-to-access reference for future research. Based on our insights, we also\nrecommend better practices for better future evaluations of sentence\nembeddings.", "published": "2019-06-04 16:41:15", "link": "http://arxiv.org/abs/1906.01575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attentional Models for Lattice Inputs", "abstract": "Lattices are an efficient and effective method to encode ambiguity of\nupstream systems in natural language processing tasks, for example to compactly\ncapture multiple speech recognition hypotheses, or to represent multiple\nlinguistic analyses. Previous work has extended recurrent neural networks to\nmodel lattice inputs and achieved improvements in various tasks, but these\nmodels suffer from very slow computation speeds. This paper extends the\nrecently proposed paradigm of self-attention to handle lattice inputs.\nSelf-attention is a sequence modeling technique that relates inputs to one\nanother by computing pairwise similarities and has gained popularity for both\nits strong results and its computational efficiency. To extend such models to\nhandle lattices, we introduce probabilistic reachability masks that incorporate\nlattice structure into the model and support lattice scores if available. We\nalso propose a method for adapting positional embeddings to lattice structures.\nWe apply the proposed model to a speech translation task and find that it\noutperforms all examined baselines while being much faster to compute than\nprevious neural lattice models during both training and inference.", "published": "2019-06-04 17:51:03", "link": "http://arxiv.org/abs/1906.01617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Lossless Encoding of Sentences", "abstract": "A lot of work has been done in the field of image compression via machine\nlearning, but not much attention has been given to the compression of natural\nlanguage. Compressing text into lossless representations while making features\neasily retrievable is not a trivial task, yet has huge benefits. Most methods\ndesigned to produce feature rich sentence embeddings focus solely on performing\nwell on downstream tasks and are unable to properly reconstruct the original\nsequence from the learned embedding. In this work, we propose a near lossless\nmethod for encoding long sequences of texts as well as all of their\nsub-sequences into feature rich representations. We test our method on\nsentiment analysis and show good performance across all sub-sentence and\nsentence embeddings.", "published": "2019-06-04 18:03:13", "link": "http://arxiv.org/abs/1906.01659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Syntactic Change Using a Neural Part-of-Speech Tagger", "abstract": "We train a diachronic long short-term memory (LSTM) part-of-speech tagger on\na large corpus of American English from the 19th, 20th, and 21st centuries. We\nanalyze the tagger's ability to implicitly learn temporal structure between\nyears, and the extent to which this knowledge can be transferred to date new\nsentences. The learned year embeddings show a strong linear correlation between\ntheir first principal component and time. We show that temporal information\nencoded in the model can be used to predict novel sentences' years of\ncomposition relatively well. Comparisons to a feedforward baseline suggest that\nthe temporal change learned by the LSTM is syntactic rather than purely\nlexical. Thus, our results suggest that our tagger is implicitly learning to\nmodel syntactic change in American English over the course of the 19th, 20th,\nand early 21st centuries.", "published": "2019-06-04 18:04:14", "link": "http://arxiv.org/abs/1906.01661v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-editing Productivity with Neural Machine Translation: An Empirical\n  Assessment of Speed and Quality in the Banking and Finance Domain", "abstract": "Neural machine translation (NMT) has set new quality standards in automatic\ntranslation, yet its effect on post-editing productivity is still pending\nthorough investigation. We empirically test how the inclusion of NMT, in\naddition to domain-specific translation memories and termbases, impacts speed\nand quality in professional translation of financial texts. We find that even\nwith language pairs that have received little attention in research settings\nand small amounts of in-domain data for system adaptation, NMT post-editing\nallows for substantial time savings and leads to equal or slightly better\nquality.", "published": "2019-06-04 19:05:11", "link": "http://arxiv.org/abs/1906.01685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic\n  Change", "abstract": "State-of-the-art models of lexical semantic change detection suffer from\nnoise stemming from vector space alignment. We have empirically tested the\nTemporal Referencing method for lexical semantic change and show that, by\navoiding alignment, it is less affected by this noise. We show that, trained on\na diachronic corpus, the skip-gram with negative sampling architecture with\ntemporal referencing outperforms alignment models on a synthetic task as well\nas a manual testset. We introduce a principled way to simulate lexical semantic\nchange and systematically control for possible biases.", "published": "2019-06-04 19:19:46", "link": "http://arxiv.org/abs/1906.01688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Sesame: Getting Inside BERT's Linguistic Knowledge", "abstract": "How and to what extent does BERT encode syntactically-sensitive hierarchical\ninformation or positionally-sensitive linear information? Recent work has shown\nthat contextual representations like BERT perform well on tasks that require\nsensitivity to linguistic structure. We present here two studies which aim to\nprovide a better understanding of the nature of BERT's representations. The\nfirst of these focuses on the identification of structurally-defined elements\nusing diagnostic classifiers, while the second explores BERT's representation\nof subject-verb agreement and anaphor-antecedent dependencies through a\nquantitative assessment of self-attention vectors. In both cases, we find that\nBERT encodes positional information about word tokens well on its lower layers,\nbut switches to a hierarchically-oriented encoding on higher layers. We\nconclude then that BERT's representations do indeed model linguistically\nrelevant aspects of hierarchical structure, though they do not appear to show\nthe sharp sensitivity to hierarchical structure that is found in human\nprocessing of reflexive anaphora.", "published": "2019-06-04 19:41:10", "link": "http://arxiv.org/abs/1906.01698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and\n  Abstractive Hierarchical Model", "abstract": "Automatic generation of summaries from multiple news articles is a valuable\ntool as the number of online publications grows rapidly. Single document\nsummarization (SDS) systems have benefited from advances in neural\nencoder-decoder model thanks to the availability of large datasets. However,\nmulti-document summarization (MDS) of news articles has been limited to\ndatasets of a couple of hundred examples. In this paper, we introduce\nMulti-News, the first large-scale MDS news dataset. Additionally, we propose an\nend-to-end model which incorporates a traditional extractive summarization\nmodel with a standard SDS model and achieves competitive results on MDS\ndatasets. We benchmark several methods on Multi-News and release our data and\ncode in hope that this work will promote advances in summarization in the\nmulti-document setting.", "published": "2019-06-04 23:00:43", "link": "http://arxiv.org/abs/1906.01749v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Joint Modeling of Cross-document Entity and Event Coreference\n  Resolution", "abstract": "Recognizing coreferring events and entities across multiple texts is crucial\nfor many NLP applications. Despite the task's importance, research focus was\ngiven mostly to within-document entity coreference, with rather little\nattention to the other variants. We propose a neural architecture for\ncross-document coreference resolution. Inspired by Lee et al (2012), we jointly\nmodel entity and event coreference. We represent an event (entity) mention\nusing its lexical span, surrounding context, and relation to entity (event)\nmentions via predicate-arguments structures. Our model outperforms the previous\nstate-of-the-art event coreference model on ECB+, while providing the first\nentity coreference results on this corpus. Our analysis confirms that all our\nrepresentation elements, including the mention span itself, its context, and\nthe relation to other mentions contribute to the model's success.", "published": "2019-06-04 23:36:50", "link": "http://arxiv.org/abs/1906.01753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coherent Comment Generation for Chinese Articles with a\n  Graph-to-Sequence Model", "abstract": "Automatic article commenting is helpful in encouraging user engagement and\ninteraction on online news platforms. However, the news documents are usually\ntoo long for traditional encoder-decoder based models, which often results in\ngeneral and irrelevant comments. In this paper, we propose to generate comments\nwith a graph-to-sequence model that models the input news as a topic\ninteraction graph. By organizing the article into graph structure, our model\ncan better understand the internal structure of the article and the connection\nbetween topics, which makes it better able to understand the story. We collect\nand release a large scale news-comment corpus from a popular Chinese online\nnews platform Tencent Kuaibao. Extensive experiment results show that our model\ncan generate much more coherent and informative comments compared with several\nstrong baseline models.", "published": "2019-06-04 07:03:04", "link": "http://arxiv.org/abs/1906.01231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcoding compositionally: using attention to find more generalizable\n  solutions", "abstract": "While sequence-to-sequence models have shown remarkable generalization power\nacross several natural language tasks, their construct of solutions are argued\nto be less compositional than human-like generalization. In this paper, we\npresent seq2attn, a new architecture that is specifically designed to exploit\nattention to find compositional patterns in the input. In seq2attn, the two\nstandard components of an encoder-decoder model are connected via a transcoder,\nthat modulates the information flow between them. We show that seq2attn can\nsuccessfully generalize, without requiring any additional supervision, on two\ntasks which are specifically constructed to challenge the compositional skills\nof neural networks. The solutions found by the model are highly interpretable,\nallowing easy analysis of both the types of solutions that are found and\npotential causes for mistakes. We exploit this opportunity to introduce a new\nparadigm to test compositionality that studies the extent to which a model\novergeneralizes when confronted with exceptions. We show that seq2attn exhibits\nsuch overgeneralization to a larger degree than a standard sequence-to-sequence\nmodel.", "published": "2019-06-04 07:07:56", "link": "http://arxiv.org/abs/1906.01234v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for\n  Evaluating Natural Language Inference", "abstract": "We present SherLIiC, a testbed for lexical inference in context (LIiC),\nconsisting of 3985 manually annotated inference rule candidates (InfCands),\naccompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual\nrelations between Freebase entities extracted from the large entity-linked\ncorpus ClueWeb09. Each InfCand consists of one of these relations, expressed as\na lemmatized dependency path, and two argument placeholders, each linked to one\nor more Freebase types. Due to our candidate selection process based on strong\ndistributional evidence, SherLIiC is much harder than existing testbeds because\ndistributional evidence is of little utility in the classification of InfCands.\nWe also show that, due to its construction, many of SherLIiC's correct InfCands\nare novel and missing from existing rule bases. We evaluate a number of strong\nbaselines on SherLIiC, ranging from semantic vector space models to state of\nthe art neural models of natural language inference (NLI). We show that\nSherLIiC poses a tough challenge to existing NLI systems.", "published": "2019-06-04 13:07:35", "link": "http://arxiv.org/abs/1906.01393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "4-D Scene Alignment in Surveillance Video", "abstract": "Designing robust activity detectors for fixed camera surveillance video\nrequires knowledge of the 3-D scene. This paper presents an automatic camera\ncalibration process that provides a mechanism to reason about the spatial\nproximity between objects at different times. It combines a CNN-based camera\npose estimator with a vertical scale provided by pedestrian observations to\nestablish the 4-D scene geometry. Unlike some previous methods, the people do\nnot need to be tracked nor do the head and feet need to be explicitly detected.\nIt is robust to individual height variations and camera parameter estimation\nerrors.", "published": "2019-06-04 18:39:20", "link": "http://arxiv.org/abs/1906.01675v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Neural Language Models by Segmenting, Attending, and\n  Predicting the Future", "abstract": "Common language models typically predict the next word given the context. In\nthis work, we propose a method that improves language modeling by learning to\nalign the given context and the following phrase. The model does not require\nany linguistic annotation of phrase segmentation. Instead, we define syntactic\nheights and phrase segmentation rules, enabling the model to automatically\ninduce phrases, recognize their task-specific heads, and generate phrase\nembeddings in an unsupervised learning manner. Our method can easily be applied\nto language models with different network architectures since an independent\nmodule is used for phrase induction and context-phrase alignment, and no change\nis required in the underlying language modeling network. Experiments have shown\nthat our model outperformed several strong baseline models on different data\nsets. We achieved a new state-of-the-art performance of 17.4 perplexity on the\nWikitext-103 dataset. Additionally, visualizing the outputs of the phrase\ninduction module showed that our model is able to learn approximate\nphrase-level structural knowledge without any annotation.", "published": "2019-06-04 19:58:05", "link": "http://arxiv.org/abs/1906.01702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ShEMO -- A Large-Scale Validated Database for Persian Speech Emotion\n  Detection", "abstract": "This paper introduces a large-scale, validated database for Persian called\nSharif Emotional Speech Database (ShEMO). The database includes 3000\nsemi-natural utterances, equivalent to 3 hours and 25 minutes of speech data\nextracted from online radio plays. The ShEMO covers speech samples of 87\nnative-Persian speakers for five basic emotions including anger, fear,\nhappiness, sadness and surprise, as well as neutral state. Twelve annotators\nlabel the underlying emotional state of utterances and majority voting is used\nto decide on the final labels. According to the kappa measure, the\ninter-annotator agreement is 64% which is interpreted as \"substantial\nagreement\". We also present benchmark results based on common classification\nmethods in speech emotion detection task. According to the experiments, support\nvector machine achieves the best results for both gender-independent (58.2%)\nand gender-dependent models (female=59.4%, male=57.6%). The ShEMO is available\nfor academic purposes free of charge to provide a baseline for further research\non Persian emotional speech.", "published": "2019-06-04 01:58:58", "link": "http://arxiv.org/abs/1906.01155v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Review of Automated Speech and Language Features for Assessment of\n  Cognitive and Thought Disorders", "abstract": "It is widely accepted that information derived from analyzing speech (the\nacoustic signal) and language production (words and sentences) serves as a\nuseful window into the health of an individual's cognitive ability. In fact,\nmost neuropsychological testing batteries have a component related to speech\nand language where clinicians elicit speech from patients for subjective\nevaluation across a broad set of dimensions. With advances in speech signal\nprocessing and natural language processing, there has been recent interest in\ndeveloping tools to detect more subtle changes in cognitive-linguistic\nfunction. This work relies on extracting a set of features from recorded and\ntranscribed speech for objective assessments of speech and language, early\ndiagnosis of neurological disease, and tracking of disease after diagnosis.\nWith an emphasis on cognitive and thought disorders, in this paper we provide a\nreview of existing speech and language features used in this domain, discuss\ntheir clinical application, and highlight their advantages and disadvantages.\nBroadly speaking, the review is split into two categories: language features\nbased on natural language processing and speech features based on speech signal\nprocessing. Within each category, we consider features that aim to measure\ncomplementary dimensions of cognitive-linguistics, including language\ndiversity, syntactic complexity, semantic coherence, and timing. We conclude\nthe review with a proposal of new research directions to further advance the\nfield.", "published": "2019-06-04 02:17:18", "link": "http://arxiv.org/abs/1906.01157v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Learning Attention-based Embeddings for Relation Prediction in Knowledge\n  Graphs", "abstract": "The recent proliferation of knowledge graphs (KGs) coupled with incomplete or\npartial information, in the form of missing relations (links) between entities,\nhas fueled a lot of research on knowledge base completion (also known as\nrelation prediction). Several recent works suggest that convolutional neural\nnetwork (CNN) based models generate richer and more expressive feature\nembeddings and hence also perform well on relation prediction. However, we\nobserve that these KG embeddings treat triples independently and thus fail to\ncover the complex and hidden information that is inherently implicit in the\nlocal neighborhood surrounding a triple. To this effect, our paper proposes a\nnovel attention based feature embedding that captures both entity and relation\nfeatures in any given entity's neighborhood. Additionally, we also encapsulate\nrelation clusters and multihop relations in our model. Our empirical study\noffers insights into the efficacy of our attention based model and we show\nmarked performance gains in comparison to state of the art methods on all\ndatasets.", "published": "2019-06-04 04:59:08", "link": "http://arxiv.org/abs/1906.01195v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring Phoneme-Level Speech Representations for End-to-End Speech\n  Translation", "abstract": "Previous work on end-to-end translation from speech has primarily used\nframe-level features as speech representations, which creates longer, sparser\nsequences than text. We show that a naive method to create compressed\nphoneme-like speech representations is far more effective and efficient for\ntranslation than traditional frame-level speech features. Specifically, we\ngenerate phoneme labels for speech frames and average consecutive frames with\nthe same label to create shorter, higher-level source sequences for\ntranslation. We see improvements of up to 5 BLEU on both our high and low\nresource language pairs, with a reduction in training time of 60%. Our\nimprovements hold across multiple data sizes and two language pairs.", "published": "2019-06-04 05:12:31", "link": "http://arxiv.org/abs/1906.01199v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Strong and Robust Baseline for Text-Image Matching", "abstract": "We review the current schemes of text-image matching models and propose\nimprovements for both training and inference. First, we empirically show\nlimitations of two popular loss (sum and max-margin loss) widely used in\ntraining text-image embeddings and propose a trade-off: a kNN-margin loss which\n1) utilizes information from hard negatives and 2) is robust to noise as all\n$K$-most hardest samples are taken into account, tolerating \\emph{pseudo}\nnegatives and outliers. Second, we advocate the use of Inverted Softmax\n(\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to\nmitigate the so-called hubness problem in high-dimensional embedding space,\nenhancing scores of all metrics by a large margin.", "published": "2019-06-04 05:42:58", "link": "http://arxiv.org/abs/1906.01205v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Boosting Entity Linking Performance by Leveraging Unlabeled Documents", "abstract": "Modern entity linking systems rely on large collections of documents\nspecifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose\nan approach which exploits only naturally occurring information: unlabeled\ndocuments and Wikipedia. Our approach consists of two stages. First, we\nconstruct a high recall list of candidate entities for each mention in an\nunlabeled document. Second, we use the candidate lists as weak supervision to\nconstrain our document-level entity linking model. The model treats entities as\nlatent variables and, when estimated on a collection of unlabelled texts,\nlearns to choose entities relying both on local context of each mention and on\ncoherence with other entities in the document. The resulting approach rivals\nfully-supervised state-of-the-art systems on standard test sets. It also\napproaches their performance in the very challenging setting: when tested on a\ntest set sampled from the data used to estimate the supervised systems. By\ncomparing to Wikipedia-only training of our model, we demonstrate that modeling\nunlabeled documents is beneficial.", "published": "2019-06-04 07:49:46", "link": "http://arxiv.org/abs/1906.01250v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tracing Antisemitic Language Through Diachronic Embedding Projections:\n  France 1789-1914", "abstract": "We investigate some aspects of the history of antisemitism in France, one of\nthe cradles of modern antisemitism, using diachronic word embeddings. We\nconstructed a large corpus of French books and periodicals issues that contain\na keyword related to Jews and performed a diachronic word embedding over the\n1789-1914 period. We studied the changes over time in the semantic spaces of 4\ntarget words and performed embedding projections over 6 streams of antisemitic\ndiscourse. This allowed us to track the evolution of antisemitic bias in the\nreligious, economic, socio-politic, racial, ethic and conspiratorial domains.\nProjections show a trend of growing antisemitism, especially in the years\nstarting in the mid-80s and culminating in the Dreyfus affair. Our analysis\nalso allows us to highlight the peculiar adverse bias towards Judaism in the\nbroader context of other religions.", "published": "2019-06-04 13:54:47", "link": "http://arxiv.org/abs/1906.01440v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "How multilingual is Multilingual BERT?", "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et\nal. (2018) as a single language model pre-trained from monolingual corpora in\n104 languages, is surprisingly good at zero-shot cross-lingual model transfer,\nin which task-specific annotations in one language are used to fine-tune the\nmodel for evaluation in another language. To understand why, we present a large\nnumber of probing experiments, showing that transfer is possible even to\nlanguages in different scripts, that transfer works best between typologically\nsimilar languages, that monolingual corpora can train models for\ncode-switching, and that the model can find translation pairs. From these\nresults, we can conclude that M-BERT does create multilingual representations,\nbut that these representations exhibit systematic deficiencies affecting\ncertain language pairs.", "published": "2019-06-04 15:12:47", "link": "http://arxiv.org/abs/1906.01502v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The PhotoBook Dataset: Building Common Ground through Visually-Grounded\n  Dialogue", "abstract": "This paper introduces the PhotoBook dataset, a large-scale collection of\nvisually-grounded, task-oriented dialogues in English designed to investigate\nshared dialogue history accumulating during conversation. Taking inspiration\nfrom seminal work on dialogue analysis, we propose a data-collection task\nformulated as a collaborative game prompting two online participants to refer\nto images utilising both their visual context as well as previously established\nreferring expressions. We provide a detailed description of the task setup and\na thorough analysis of the 2,500 dialogues collected. To further illustrate the\nnovel features of the dataset, we propose a baseline model for reference\nresolution which uses a simple method to take into account shared information\naccumulated in a reference chain. Our results show that this information is\nparticularly important to resolve later descriptions and underline the need to\ndevelop more sophisticated models of common ground in dialogue interaction.", "published": "2019-06-04 15:41:32", "link": "http://arxiv.org/abs/1906.01530v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Blackbox meets blackbox: Representational Similarity and Stability\n  Analysis of Neural Language Models and Brains", "abstract": "In this paper, we define and apply representational stability analysis\n(ReStA), an intuitive way of analyzing neural language models. ReStA is a\nvariant of the popular representational similarity analysis (RSA) in cognitive\nneuroscience. While RSA can be used to compare representations in models, model\ncomponents, and human brains, ReStA compares instances of the same model, while\nsystematically varying single model parameter. Using ReStA, we study four\nrecent and successful neural language models, and evaluate how sensitive their\ninternal representations are to the amount of prior context. Using RSA, we\nperform a systematic study of how similar the representational spaces in the\nfirst and second (or higher) layers of these models are to each other and to\npatterns of activation in the human brain. Our results reveal surprisingly\nstrong differences between language models, and give insights into where the\ndeep linguistic processing, that integrates information over multiple\nsentences, is happening in these models. The combination of ReStA and RSA on\nmodels and brains allows us to start addressing the important question of what\nkind of linguistic processes we can hope to observe in fMRI brain imaging data.\nIn particular, our results suggest that the data on story reading from Wehbe et\nal. (2014) contains a signal of shallow linguistic processing, but show no\nevidence on the more interesting deep linguistic processing.", "published": "2019-06-04 15:52:46", "link": "http://arxiv.org/abs/1906.01539v2", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "Optimal coding and the origins of Zipfian laws", "abstract": "The problem of compression in standard information theory consists of\nassigning codes as short as possible to numbers. Here we consider the problem\nof optimal coding -- under an arbitrary coding scheme -- and show that it\npredicts Zipf's law of abbreviation, namely a tendency in natural languages for\nmore frequent words to be shorter. We apply this result to investigate optimal\ncoding also under so-called non-singular coding, a scheme where unique\nsegmentation is not warranted but codes stand for a distinct number. Optimal\nnon-singular coding predicts that the length of a word should grow\napproximately as the logarithm of its frequency rank, which is again consistent\nwith Zipf's law of abbreviation. Optimal non-singular coding in combination\nwith the maximum entropy principle also predicts Zipf's rank-frequency\ndistribution. Furthermore, our findings on optimal non-singular coding\nchallenge common beliefs about random typing. It turns out that random typing\nis in fact an optimal coding process, in stark contrast with the common\nassumption that it is detached from cost cutting considerations. Finally, we\ndiscuss the implications of optimal coding for the construction of a compact\ntheory of Zipfian laws and other linguistic laws.", "published": "2019-06-04 16:03:18", "link": "http://arxiv.org/abs/1906.01545v4", "categories": ["cs.CL", "cs.IT", "math.IT", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Task-Guided Pair Embedding in Heterogeneous Network", "abstract": "Many real-world tasks solved by heterogeneous network embedding methods can\nbe cast as modeling the likelihood of pairwise relationship between two nodes.\nFor example, the goal of author identification task is to model the likelihood\nof a paper being written by an author (paper-author pairwise relationship).\nExisting task-guided embedding methods are node-centric in that they simply\nmeasure the similarity between the node embeddings to compute the likelihood of\na pairwise relationship between two nodes. However, we claim that for\ntask-guided embeddings, it is crucial to focus on directly modeling the\npairwise relationship. In this paper, we propose a novel task-guided pair\nembedding framework in heterogeneous network, called TaPEm, that directly\nmodels the relationship between a pair of nodes that are related to a specific\ntask (e.g., paper-author relationship in author identification). To this end,\nwe 1) propose to learn a pair embedding under the guidance of its associated\ncontext path, i.e., a sequence of nodes between the pair, and 2) devise the\npair validity classifier to distinguish whether the pair is valid with respect\nto the specific task at hand. By introducing pair embeddings that capture the\nsemantics behind the pairwise relationships, we are able to learn the\nfine-grained pairwise relationship between two nodes, which is paramount for\ntask-guided embedding methods. Extensive experiments on author identification\ntask demonstrate that TaPEm outperforms the state-of-the-art methods,\nespecially for authors with few publication records.", "published": "2019-06-04 16:04:04", "link": "http://arxiv.org/abs/1906.01546v3", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Finding Syntactic Representations in Neural Stacks", "abstract": "Neural network architectures have been augmented with differentiable stacks\nin order to introduce a bias toward learning hierarchy-sensitive regularities.\nIt has, however, proven difficult to assess the degree to which such a bias is\neffective, as the operation of the differentiable stack is not always\ninterpretable. In this paper, we attempt to detect the presence of latent\nrepresentations of hierarchical structure through an exploration of the\nunsupervised learning of constituency structure. Using a technique due to Shen\net al. (2018a,b), we extract syntactic trees from the pushing behavior of stack\nRNNs trained on language modeling and classification objectives. We find that\nour models produce parses that reflect natural language syntactic\nconstituencies, demonstrating that stack RNNs do indeed infer linguistically\nrelevant hierarchical structure.", "published": "2019-06-04 17:14:26", "link": "http://arxiv.org/abs/1906.01594v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Do Neural Dialog Systems Use the Conversation History Effectively? An\n  Empirical Study", "abstract": "Neural generative models have been become increasingly popular when building\nconversational agents. They offer flexibility, can be easily adapted to new\ndomains, and require minimal domain engineering. A common criticism of these\nsystems is that they seldom understand or use the available dialog history\neffectively. In this paper, we take an empirical approach to understanding how\nthese models use the available dialog history by studying the sensitivity of\nthe models to artificially introduced unnatural changes or perturbations to\ntheir context at test time. We experiment with 10 different types of\nperturbations on 4 multi-turn dialog datasets and find that commonly used\nneural dialog architectures like recurrent and transformer-based seq2seq models\nare rarely sensitive to most perturbations such as missing or reordering\nutterances, shuffling words, etc. Also, by open-sourcing our code, we believe\nthat it will serve as a useful diagnostic tool for evaluating dialog systems in\nthe future.", "published": "2019-06-04 17:32:35", "link": "http://arxiv.org/abs/1906.01603v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KERMIT: Generative Insertion-Based Modeling for Sequences", "abstract": "We present KERMIT, a simple insertion-based approach to generative modeling\nfor sequences and sequence pairs. KERMIT models the joint distribution and its\ndecompositions (i.e., marginals and conditionals) using a single neural network\nand, unlike much prior work, does not rely on a prespecified factorization of\nthe data distribution. During training, one can feed KERMIT paired data $(x,\ny)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired\ndata $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we\nhave access to the conditionals $p(x \\mid y)$ and $p(y \\mid x)$ in both\ndirections. We can also sample from the joint distribution or the marginals.\nThe model supports both serial fully autoregressive decoding and parallel\npartially autoregressive decoding, with the latter exhibiting an empirically\nlogarithmic runtime. We demonstrate through experiments in machine translation,\nrepresentation learning, and zero-shot cloze question answering that our\nunified approach is capable of matching or exceeding the performance of\ndedicated state-of-the-art systems across a wide range of tasks without the\nneed for problem-specific architectural adaptation.", "published": "2019-06-04 17:35:35", "link": "http://arxiv.org/abs/1906.01604v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transferable Neural Projection Representations", "abstract": "Neural word representations are at the core of many state-of-the-art natural\nlanguage processing models. A widely used approach is to pre-train, store and\nlook up word or character embedding matrices. While useful, such\nrepresentations occupy huge memory making it hard to deploy on-device and often\ndo not generalize to unknown words due to vocabulary pruning.\n  In this paper, we propose a skip-gram based architecture coupled with\nLocality-Sensitive Hashing (LSH) projections to learn efficient dynamically\ncomputable representations. Our model does not need to store lookup tables as\nrepresentations are computed on-the-fly and require low memory footprint. The\nrepresentations can be trained in an unsupervised fashion and can be easily\ntransferred to other NLP tasks. For qualitative evaluation, we analyze the\nnearest neighbors of the word representations and discover semantically similar\nwords even with misspellings. For quantitative evaluation, we plug our\ntransferable projections into a simple LSTM and run it on multiple NLP tasks\nand show how our transferable projections achieve better performance compared\nto prior work.", "published": "2019-06-04 17:39:52", "link": "http://arxiv.org/abs/1906.01605v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential Neural Networks as Automata", "abstract": "This work attempts to explain the types of computation that neural networks\ncan perform by relating them to automata. We first define what it means for a\nreal-time network with bounded precision to accept a language. A measure of\nnetwork memory follows from this definition. We then characterize the classes\nof languages acceptable by various recurrent networks, attention, and\nconvolutional networks. We find that LSTMs function like counter machines and\nrelate convolutional networks to the subregular hierarchy. Overall, this work\nattempts to increase our understanding and ability to interpret neural networks\nthrough the lens of theory. These theoretical insights help explain neural\ncomputation, as well as the relationship between neural networks and natural\nlanguage grammar.", "published": "2019-06-04 17:47:34", "link": "http://arxiv.org/abs/1906.01615v3", "categories": ["cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Girls Neko or Sh\u014djo? Cross-Lingual Alignment of Non-Isomorphic\n  Embeddings with Iterative Normalization", "abstract": "Cross-lingual word embeddings (CLWE) underlie many multilingual natural\nlanguage processing systems, often through orthogonal transformations of\npre-trained monolingual embeddings. However, orthogonal mapping only works on\nlanguage pairs whose embeddings are naturally isomorphic. For non-isomorphic\npairs, our method (Iterative Normalization) transforms monolingual embeddings\nto make orthogonal alignment easier by simultaneously enforcing that (1)\nindividual word vectors are unit length, and (2) each language's average vector\nis zero. Iterative Normalization consistently improves word translation\naccuracy of three CLWE methods, with the largest improvement observed on\nEnglish-Japanese (from 2% to 44% test accuracy).", "published": "2019-06-04 17:56:22", "link": "http://arxiv.org/abs/1906.01622v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Realization of Compositionality in Neural Networks", "abstract": "We present a detailed comparison of two types of sequence to sequence models\ntrained to conduct a compositional task. The models are architecturally\nidentical at inference time, but differ in the way that they are trained: our\nbaseline model is trained with a task-success signal only, while the other\nmodel receives additional supervision on its attention mechanism (Attentive\nGuidance), which has shown to be an effective method for encouraging more\ncompositional solutions (Hupkes et al.,2019). We first confirm that the models\nwith attentive guidance indeed infer more compositional solutions than the\nbaseline, by training them on the lookup table task presented by Li\\v{s}ka et\nal. (2019). We then do an in-depth analysis of the structural differences\nbetween the two model types, focusing in particular on the organisation of the\nparameter space and the hidden layer activations and find noticeable\ndifferences in both these aspects. Guided networks focus more on the components\nof the input rather than the sequence as a whole and develop small functional\ngroups of neurons with specific purposes that use their gates more selectively.\nResults from parameter heat maps, component swapping and graph analysis also\nindicate that guided networks exhibit a more modular structure with a small\nnumber of specialized, strongly connected neurons.", "published": "2019-06-04 07:30:48", "link": "http://arxiv.org/abs/1906.01634v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Ghostwriters in High Schools", "abstract": "Students hiring ghostwriters to write their assignments is an increasing\nproblem in educational institutions all over the world, with companies selling\nthese services as a product. In this work, we develop automatic techniques with\nspecial focus on detecting such ghostwriting in high school assignments. This\nis done by training deep neural networks on an unprecedented large amount of\ndata supplied by the Danish company MaCom, which covers 90% of Danish high\nschools. We achieve an accuracy of 0.875 and a AUC score of 0.947 on an evenly\nsplit data set.", "published": "2019-06-04 13:03:38", "link": "http://arxiv.org/abs/1906.01635v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Effectiveness of Transformer Language Models in\n  Grammatical Error Correction", "abstract": "Recent work on Grammatical Error Correction (GEC) has highlighted the\nimportance of language modeling in that it is certainly possible to achieve\ngood performance by comparing the probabilities of the proposed edits. At the\nsame time, advancements in language modeling have managed to generate\nlinguistic output, which is almost indistinguishable from that of\nhuman-generated text. In this paper, we up the ante by exploring the potential\nof more sophisticated language models in GEC and offer some key insights on\ntheir strengths and weaknesses. We show that, in line with recent results in\nother NLP tasks, Transformer architectures achieve consistently high\nperformance and provide a competitive baseline for future machine learning\nmodels.", "published": "2019-06-04 21:28:31", "link": "http://arxiv.org/abs/1906.01733v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Just and Comprehensive Strategy for Using NLP to Address Online Abuse", "abstract": "Online abusive behavior affects millions and the NLP community has attempted\nto mitigate this problem by developing technologies to detect abuse. However,\ncurrent methods have largely focused on a narrow definition of abuse to\ndetriment of victims who seek both validation and solutions. In this position\npaper, we argue that the community needs to make three substantive changes: (1)\nexpanding our scope of problems to tackle both more subtle and more serious\nforms of abuse, (2) developing proactive technologies that counter or inhibit\nabuse before it harms, and (3) reframing our effort within a framework of\njustice to promote healthy communities.", "published": "2019-06-04 21:54:08", "link": "http://arxiv.org/abs/1906.01738v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Investigating Writing Style Development in High School", "abstract": "In this paper we do the first large scale analysis of writing style\ndevelopment among Danish high school students. More than 10K students with more\nthan 100K essays are analyzed. Writing style itself is often studied in the\nnatural language processing community, but usually with the goal of verifying\nauthorship, assessing quality or popularity, or other kinds of predictions.\n  In this work, we analyze writing style changes over time, with the goal of\ndetecting global development trends among students, and identifying at-risk\nstudents. We train a Siamese neural network to compute the similarity between\ntwo texts. Using this similarity measure, a student's newer essays are compared\nto their first essays, and a writing style development profile is constructed\nfor the student. We cluster these student profiles and analyze the resulting\nclusters in order to detect general development patterns. We evaluate clusters\nwith respect to writing style quality indicators, and identify optimal\nclusters, showing significant improvement in writing style, while also\nobserving suboptimal clusters, exhibiting periods of limited development and\neven setbacks.\n  Furthermore, we identify general development trends between high school\nstudents, showing that as students progress through high school, their writing\nstyle deviates, leaving students less similar when they finish high school,\nthan when they start.", "published": "2019-06-04 13:20:40", "link": "http://arxiv.org/abs/1906.03072v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CY"}
{"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "abstract": "Capturing high-level structure in audio waveforms is challenging because a\nsingle second of audio spans tens of thousands of timesteps. While long-range\ndependencies are difficult to model directly in the time domain, we show that\nthey can be more tractably modelled in two-dimensional time-frequency\nrepresentations such as spectrograms. By leveraging this representational\nadvantage, in conjunction with a highly expressive probabilistic model and a\nmultiscale generation procedure, we design a model capable of generating\nhigh-fidelity audio samples which capture structure at timescales that\ntime-domain models have yet to achieve. We apply our model to a variety of\naudio generation tasks, including unconditional speech generation, music\ngeneration, and text-to-speech synthesis---showing improvements over previous\napproaches in both density estimates and human judgments.", "published": "2019-06-04 04:58:19", "link": "http://arxiv.org/abs/1906.01083v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Dilated Convolution with Dilated GRU for Music Source Separation", "abstract": "Stacked dilated convolutions used in Wavenet have been shown effective for\ngenerating high-quality audios. By replacing pooling/striding with dilation in\nconvolution layers, they can preserve high-resolution information and still\nreach distant locations. Producing high-resolution predictions is also crucial\nin music source separation, whose goal is to separate different sound sources\nwhile maintaining the quality of the separated sounds. Therefore, this paper\ninvestigates using stacked dilated convolutions as the backbone for music\nsource separation. However, while stacked dilated convolutions can reach wider\ncontext than standard convolutions, their effective receptive fields are still\nfixed and may not be wide enough for complex music audio signals. To reach\ninformation at remote locations, we propose to combine dilated convolution with\na modified version of gated recurrent units (GRU) called the `Dilated GRU' to\nform a block. A Dilated GRU unit receives information from k steps before\ninstead of the previous step for a fixed k. This modification allows a GRU unit\nto reach a location with fewer recurrent steps and run faster because it can\nexecute partially in parallel. We show that the proposed model with a stack of\nsuch blocks performs equally well or better than the state-of-the-art models\nfor separating vocals and accompaniments.", "published": "2019-06-04 05:39:43", "link": "http://arxiv.org/abs/1906.01203v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
