{"title": "A Question-Focused Multi-Factor Attention Network for Question Answering", "abstract": "Neural network models recently proposed for question answering (QA) primarily\nfocus on capturing the passage-question relation. However, they have minimal\ncapability to link relevant facts distributed across multiple sentences which\nis crucial in achieving deeper understanding, such as performing multi-sentence\nreasoning, co-reference resolution, etc. They also do not explicitly focus on\nthe question and answer type which often plays a critical role in QA. In this\npaper, we propose a novel end-to-end question-focused multi-factor attention\nnetwork for answer extraction. Multi-factor attentive encoding using\ntensor-based transformation aggregates meaningful facts even when they are\nlocated in multiple sentences. To implicitly infer the answer type, we also\npropose a max-attentional question aggregation mechanism to encode a question\nvector based on the important words in a question. During prediction, we\nincorporate sequence-level encoding of the first wh-word and its immediately\nfollowing word as an additional source of question type information. Our\nproposed model achieves significant improvements over the best prior\nstate-of-the-art results on three large-scale challenging QA datasets, namely\nNewsQA, TriviaQA, and SearchQA.", "published": "2018-01-25 07:08:04", "link": "http://arxiv.org/abs/1801.08290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Space Reordering Models for Phrase-based MT", "abstract": "Bilingual sequence models improve phrase-based translation and reordering by\novercoming phrasal independence assumption and handling long range reordering.\nHowever, due to data sparsity, these models often fall back to very small\ncontext sizes. This problem has been previously addressed by learning sequences\nover generalized representations such as POS tags or word clusters. In this\npaper, we explore an alternative based on neural network models. More\nconcretely we train neuralized versions of lexicalized reordering and the\noperation sequence models using feed-forward neural network. Our results show\nimprovements of up to 0.6 and 0.5 BLEU points on top of the baseline\nGerman->English and English->German systems. We also observed improvements\ncompared to the systems that used POS tags and word clusters to train these\nmodels. Because we modify the bilingual corpus to integrate reordering\noperations, this allows us to also train a sequence-to-sequence neural MT model\nhaving explicit reordering triggers. Our motivation was to directly enable\nreordering information in the encoder-decoder framework, which otherwise relies\nsolely on the attention model to handle long range reordering. We tried both\ncoarser and fine-grained reordering operations. However, these experiments did\nnot yield any improvements over the baseline Neural MT systems.", "published": "2018-01-25 10:17:32", "link": "http://arxiv.org/abs/1801.08337v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
