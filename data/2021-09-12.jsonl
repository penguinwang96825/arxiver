{"title": "Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning", "abstract": "Single-table text-to-SQL aims to transform a natural language question into a\nSQL query according to one single table. Recent work has made promising\nprogress on this task by pre-trained language models and a multi-submodule\nframework. However, zero-shot table, that is, the invisible table in the\ntraining set, is currently the most critical bottleneck restricting the\napplication of existing approaches to real-world scenarios. Although some work\nhas utilized auxiliary tasks to help handle zero-shot tables, expensive extra\nmanual annotation limits their practicality. In this paper, we propose a new\napproach for the zero-shot text-to-SQL task which does not rely on any\nadditional manual annotations. Our approach consists of two parts. First, we\npropose a new model that leverages the abundant information of table content to\nhelp establish the mapping between questions and zero-shot tables. Further, we\npropose a simple but efficient meta-learning strategy to train our model. The\nstrategy utilizes the two-step gradient update to force the model to learn a\ngeneralization ability towards zero-shot tables. We conduct extensive\nexperiments on a public open-domain text-to-SQL dataset WikiSQL and a\ndomain-specific dataset ESQL. Compared to existing approaches using the same\npre-trained model, our approach achieves significant improvements on both\ndatasets. Compared to the larger pre-trained model and the tabular-specific\npre-trained model, our approach is still competitive. More importantly, on the\nzero-shot subsets of both the datasets, our approach further increases the\nimprovements.", "published": "2021-09-12 01:01:28", "link": "http://arxiv.org/abs/2109.05395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Let Your Characters Tell Their Story\": A Dataset for Character-Centric\n  Narrative Understanding", "abstract": "When reading a literary piece, readers often make inferences about various\ncharacters' roles, personalities, relationships, intents, actions, etc. While\nhumans can readily draw upon their past experiences to build such a\ncharacter-centric view of the narrative, understanding characters in narratives\ncan be a challenging task for machines. To encourage research in this field of\ncharacter-centric narrative understanding, we present LiSCU -- a new dataset of\nliterary pieces and their summaries paired with descriptions of characters that\nappear in them. We also introduce two new tasks on LiSCU: Character\nIdentification and Character Description Generation. Our experiments with\nseveral pre-trained language models adapted for these tasks demonstrate that\nthere is a need for better models of narrative comprehension.", "published": "2021-09-12 06:12:55", "link": "http://arxiv.org/abs/2109.05438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Task Difficulty for Few-Shot Relation Extraction", "abstract": "Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.", "published": "2021-09-12 09:40:33", "link": "http://arxiv.org/abs/2109.05473v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylistic Retrieval-based Dialogue System with Unparallel Training Data", "abstract": "The ability of a dialog system to express consistent language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. Although previous studies have demonstrated that style transfer\nis feasible with a large amount of parallel data, it is often impossible to\ncollect such data for different styles. In this paper, instead of manually\nconstructing conversation data with a certain style, we propose a flexible\nframework that adapts a generic retrieval-based dialogue system to mimic the\nlanguage style of a specified persona without any parallel data. Our approach\nis based on automatic generation of stylized data by learning the usage of\njargon, and then rewriting the generic conversations to a stylized one by\nincorporating the jargon. In experiments we implemented dialogue systems with\nfive distinct language styles, and the result shows our framework significantly\noutperforms baselines in terms of the average score of responses' relevance and\nstyle degree, and content diversity. A/B testing on a commercial chatbot shows\nthat users are more satisfied with our system. This study demonstrates the\nfeasibility of building stylistic dialogue systems by simple data augmentation.", "published": "2021-09-12 09:56:24", "link": "http://arxiv.org/abs/2109.05477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in\n  Dialogue Generation", "abstract": "Although pre-training models have achieved great success in dialogue\ngeneration, their performance drops dramatically when the input contains an\nentity that does not appear in pre-training and fine-tuning datasets (unseen\nentity). To address this issue, existing methods leverage an external knowledge\nbase to generate appropriate responses. In real-world scenario, the entity may\nnot be included by the knowledge base or suffer from the precision of knowledge\nretrieval. To deal with this problem, instead of introducing knowledge base as\nthe input, we force the model to learn a better semantic representation by\npredicting the information in the knowledge base, only based on the input\ncontext. Specifically, with the help of a knowledge base, we introduce two\nauxiliary training objectives: 1) Interpret Masked Word, which conjectures the\nmeaning of the masked entity given the context; 2) Hypernym Generation, which\npredicts the hypernym of the entity based on the context. Experiment results on\ntwo dialogue corpus verify the effectiveness of our methods under both\nknowledge available and unavailable settings.", "published": "2021-09-12 11:13:19", "link": "http://arxiv.org/abs/2109.05487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Event Temporal Relations via Hyperbolic Geometry", "abstract": "Detecting events and their evolution through time is a crucial task in\nnatural language understanding. Recent neural approaches to event temporal\nrelation extraction typically map events to embeddings in the Euclidean space\nand train a classifier to detect temporal relations between event pairs.\nHowever, embeddings in the Euclidean space cannot capture richer asymmetric\nrelations such as event temporal relations. We thus propose to embed events\ninto hyperbolic spaces, which are intrinsically oriented at modeling\nhierarchical structures. We introduce two approaches to encode events and their\ntemporal relations in hyperbolic spaces. One approach leverages hyperbolic\nembeddings to directly infer event relations through simple geometrical\noperations. In the second one, we devise an end-to-end architecture composed of\nhyperbolic neural units tailored for the temporal relation extraction task.\nThorough experimental assessments on widely used datasets have shown the\nbenefits of revisiting the tasks on a different geometrical space, resulting in\nstate-of-the-art performance on several standard metrics. Finally, the ablation\nstudy and several qualitative analyses highlighted the rich event semantics\nimplicitly encoded into hyperbolic spaces.", "published": "2021-09-12 14:40:13", "link": "http://arxiv.org/abs/2109.05527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good-Enough Example Extrapolation", "abstract": "This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.", "published": "2021-09-12 20:08:42", "link": "http://arxiv.org/abs/2109.05602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Levenshtein Training for Word-level Quality Estimation", "abstract": "We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.", "published": "2021-09-12 20:45:48", "link": "http://arxiv.org/abs/2109.05611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RockNER: A Simple Method to Create Adversarial Examples for Evaluating\n  the Robustness of Named Entity Recognition Models", "abstract": "To audit the robustness of named entity recognition (NER) models, we propose\nRockNER, a simple yet effective method to create natural adversarial examples.\nSpecifically, at the entity level, we replace target entities with other\nentities of the same semantic class in Wikidata; at the context level, we use\npre-trained language models (e.g., BERT) to generate word substitutions.\nTogether, the two levels of attack produce natural adversarial examples that\nresult in a shifted distribution from the training data on which our target\nmodels have been trained. We apply the proposed method to the OntoNotes dataset\nand create a new benchmark named OntoRock for evaluating the robustness of\nexisting NER models via a systematic evaluation protocol. Our experiments and\nanalysis reveal that even the best model has a significant performance drop,\nand these models seem to memorize in-domain entity patterns instead of\nreasoning from the context. Our work also studies the effects of a few simple\ndata augmentation methods to improve the robustness of NER models.", "published": "2021-09-12 21:30:21", "link": "http://arxiv.org/abs/2109.05620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Topic Flows in the Generative Chatbot by Enhancing the\n  ConceptNet with the Conversation Corpora", "abstract": "Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.", "published": "2021-09-12 02:24:35", "link": "http://arxiv.org/abs/2109.05406v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Pairwise Supervised Contrastive Learning of Sentence Representations", "abstract": "Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.", "published": "2021-09-12 04:12:16", "link": "http://arxiv.org/abs/2109.05424v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Not All Negatives are Equal: Label-Aware Contrastive Loss for\n  Fine-grained Text Classification", "abstract": "Fine-grained classification involves dealing with datasets with larger number\nof classes with subtle differences between them. Guiding the model to focus on\ndifferentiating dimensions between these commonly confusable classes is key to\nimproving performance on fine-grained tasks. In this work, we analyse the\ncontrastive fine-tuning of pre-trained language models on two fine-grained text\nclassification tasks, emotion classification and sentiment analysis. We\nadaptively embed class relationships into a contrastive objective function to\nhelp differently weigh the positives and negatives, and in particular,\nweighting closely confusable negatives more than less similar negative\nexamples. We find that Label-aware Contrastive Loss outperforms previous\ncontrastive methods, in the presence of larger number and/or more confusable\nclasses, and helps models to produce output distributions that are more\ndifferentiated.", "published": "2021-09-12 04:19:17", "link": "http://arxiv.org/abs/2109.05427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias\n  in Image Search", "abstract": "Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.", "published": "2021-09-12 04:47:33", "link": "http://arxiv.org/abs/2109.05433v1", "categories": ["cs.CV", "cs.CL", "I.2.7"], "primary_category": "cs.CV"}
{"title": "End-to-End Conversational Search for Online Shopping with Utterance\n  Transfer", "abstract": "Successful conversational search systems can present natural, adaptive and\ninteractive shopping experience for online shopping customers. However,\nbuilding such systems from scratch faces real word challenges from both\nimperfect product schema/knowledge and lack of training dialog data.In this\nwork we first propose ConvSearch, an end-to-end conversational search system\nthat deeply combines the dialog system with search. It leverages the text\nprofile to retrieve products, which is more robust against imperfect product\nschema/knowledge compared with using product attributes alone. We then address\nthe lack of data challenges by proposing an utterance transfer approach that\ngenerates dialogue utterances by using existing dialog from other domains, and\nleveraging the search behavior data from e-commerce retailer. With utterance\ntransfer, we introduce a new conversational search dataset for online shopping.\nExperiments show that our utterance transfer method can significantly improve\nthe availability of training dialogue data without crowd-sourcing, and the\nconversational search system significantly outperformed the best tested\nbaseline.", "published": "2021-09-12 08:33:44", "link": "http://arxiv.org/abs/2109.05460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constructing Phrase-level Semantic Labels to Form Multi-Grained\n  Supervision for Image-Text Retrieval", "abstract": "Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.", "published": "2021-09-12 14:21:15", "link": "http://arxiv.org/abs/2109.05523v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FLiText: A Faster and Lighter Semi-Supervised Text Classification with\n  Convolution Networks", "abstract": "In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised\nlearning (SSL) frameworks have shown great performance on deep pre-trained\nlanguage models such as BERT, and are expected to significantly reduce the\ndemand for manual labeling. However, our empirical studies indicate that these\nframeworks are not suitable for lightweight models such as TextCNN, LSTM and\netc. In this work, we develop a new SSL framework called FLiText, which stands\nfor Faster and Lighter semi-supervised Text classification. FLiText introduces\nan inspirer network together with the consistency regularization framework,\nwhich leverages a generalized regular constraint on the lightweight models for\nefficient SSL. As a result, FLiText obtains new SOTA performance for\nlightweight models across multiple SSL benchmarks on text classification.\nCompared with existing SOTA SSL methods on TextCNN, FLiText improves the\naccuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to\n58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo. In addition, compared with\nthe fully supervised method on the full dataset, FLiText just uses less than 1%\nof labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the\ndatasets of IMDb, Yelp-5, and Yahoo respectively.", "published": "2021-09-12 09:05:35", "link": "http://arxiv.org/abs/2110.11869v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logic Traps in Evaluating Attribution Scores", "abstract": "Modern deep learning models are notoriously opaque, which has motivated the\ndevelopment of methods for interpreting how deep models predict. This goal is\nusually approached with attribution method, which assesses the influence of\nfeatures on model predictions. As an explanation method, the evaluation\ncriteria of attribution methods is how accurately it re-reflects the actual\nreasoning process of the model (faithfulness). Meanwhile, since the reasoning\nprocess of deep models is inaccessible, researchers design various evaluation\nmethods to demonstrate their arguments. However, some crucial logic traps in\nthese evaluation methods are ignored in most works, causing inaccurate\nevaluation and unfair comparison. This paper systematically reviews existing\nmethods for evaluating attribution scores and summarizes the logic traps in\nthese methods. We further conduct experiments to demonstrate the existence of\neach logic trap. Through both the theoretical and experimental analysis, we\nhope to increase attention on the inaccurate evaluation of attribution scores.\nMoreover, with this paper, we suggest stopping focusing on improving\nperformance under unreliable evaluation systems and starting efforts on\nreducing the impact of proposed logic traps", "published": "2021-09-12 08:50:17", "link": "http://arxiv.org/abs/2109.05463v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource\n  Languages", "abstract": "Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.", "published": "2021-09-12 11:45:26", "link": "http://arxiv.org/abs/2109.05494v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TEASEL: A Transformer-Based Speech-Prefixed Language Model", "abstract": "Multimodal language analysis is a burgeoning field of NLP that aims to\nsimultaneously model a speaker's words, acoustical annotations, and facial\nexpressions. In this area, lexicon features usually outperform other modalities\nbecause they are pre-trained on large corpora via Transformer-based models.\nDespite their strong performance, training a new self-supervised learning (SSL)\nTransformer on any modality is not usually attainable due to insufficient data,\nwhich is the case in multimodal language learning. This work proposes a\nTransformer-Based Speech-Prefixed Language Model called TEASEL to approach the\nmentioned constraints without training a complete Transformer model. TEASEL\nmodel includes speech modality as a dynamic prefix besides the textual modality\ncompared to a conventional language model. This method exploits a conventional\npre-trained language model as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.\nExtensive experiments show that our model outperforms unimodal baseline\nlanguage models by 4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%\nsmaller than the SoTA model.", "published": "2021-09-12 14:08:57", "link": "http://arxiv.org/abs/2109.05522v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music\n  Source Separation", "abstract": "Deep neural network based methods have been successfully applied to music\nsource separation. They typically learn a mapping from a mixture spectrogram to\na set of source spectrograms, all with magnitudes only. This approach has\nseveral limitations: 1) its incorrect phase reconstruction degrades the\nperformance, 2) it limits the magnitude of masks between 0 and 1 while we\nobserve that 22% of time-frequency bins have ideal ratio mask values of over~1\nin a popular dataset, MUSDB18, 3) its potential on very deep architectures is\nunder-explored. Our proposed system is designed to overcome these. First, we\npropose to estimate phases by estimating complex ideal ratio masks (cIRMs)\nwhere we decouple the estimation of cIRMs into magnitude and phase estimations.\nSecond, we extend the separation method to effectively allow the magnitude of\nthe mask to be larger than 1. Finally, we propose a residual UNet architecture\nwith up to 143 layers. Our proposed system achieves a state-of-the-art MSS\nresult on the MUSDB18 dataset, especially, a SDR of 8.98~dB on vocals,\noutperforming the previous best performance of 7.24~dB. The source code is\navailable at: https://github.com/bytedance/music_source_separation", "published": "2021-09-12 03:42:48", "link": "http://arxiv.org/abs/2109.05418v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration", "abstract": "Given a piece of speech and its transcript text, text-based speech editing\naims to generate speech that can be seamlessly inserted into the given speech\nby editing the transcript. Existing methods adopt a two-stage approach:\nsynthesize the input text using a generic text-to-speech (TTS) engine and then\ntransform the voice to the desired voice using voice conversion (VC). A major\nproblem of this framework is that VC is a challenging problem which usually\nneeds a moderate amount of parallel training data to work satisfactorily. In\nthis paper, we propose a one-stage context-aware framework to generate natural\nand coherent target speech without any training data of the target speaker. In\nparticular, we manage to perform accurate zero-shot duration prediction for the\ninserted text. The predicted duration is used to regulate both text embedding\nand speech embedding. Then, based on the aligned cross-modality input, we\ndirectly generate the mel-spectrogram of the edited speech with a\ntransformer-based decoder. Subjective listening tests show that despite the\nlack of training data for the speaker, our method has achieved satisfactory\nresults. It outperforms a recent zero-shot TTS engine by a large margin.", "published": "2021-09-12 04:17:53", "link": "http://arxiv.org/abs/2109.05426v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
