{"title": "Towards a Holistic Understanding of Mathematical Questions with\n  Contrastive Pre-training", "abstract": "Understanding mathematical questions effectively is a crucial task, which can\nbenefit many applications, such as difficulty estimation. Researchers have\ndrawn much attention to designing pre-training models for question\nrepresentations due to the scarcity of human annotations (e.g., labeling\ndifficulty). However, unlike general free-format texts (e.g., user comments),\nmathematical questions are generally designed with explicit purposes and\nmathematical logic, and usually consist of more complex content, such as\nformulas, and related mathematical knowledge (e.g., Function). Therefore, the\nproblem of holistically representing mathematical questions remains\nunderexplored. To this end, in this paper, we propose a novel contrastive\npre-training approach for mathematical question representations, namely QuesCo,\nwhich attempts to bring questions with more similar purposes closer.\nSpecifically, we first design two-level question augmentations, including\ncontent-level and structure-level, which generate literally diverse question\npairs with similar purposes. Then, to fully exploit hierarchical information of\nknowledge concepts, we propose a knowledge hierarchy-aware rank strategy\n(KHAR), which ranks the similarities between questions in a fine-grained\nmanner. Next, we adopt a ranking contrastive learning task to optimize our\nmodel based on the augmented and ranked questions. We conduct extensive\nexperiments on two real-world mathematical datasets. The experimental results\ndemonstrate the effectiveness of our model.", "published": "2023-01-18 14:23:29", "link": "http://arxiv.org/abs/2301.07558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,\n  and Detection", "abstract": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.", "published": "2023-01-18 15:23:25", "link": "http://arxiv.org/abs/2301.07597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Detecting Hallucinations in Neural Machine Translation\n  via Model Introspection", "abstract": "Neural sequence generation models are known to \"hallucinate\", by producing\noutputs that are unrelated to the source text. These hallucinations are\npotentially harmful, yet it remains unclear in what conditions they arise and\nhow to mitigate their impact. In this work, we first identify internal model\nsymptoms of hallucinations by analyzing the relative token contributions to the\ngeneration in contrastive hallucinated vs. non-hallucinated outputs generated\nvia source perturbations. We then show that these symptoms are reliable\nindicators of natural hallucinations, by using them to design a lightweight\nhallucination detector which outperforms both model-free baselines and strong\nclassifiers based on quality estimation or large pre-trained models on manually\nannotated English-Chinese and German-English translation test beds.", "published": "2023-01-18 20:43:13", "link": "http://arxiv.org/abs/2301.07779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KILDST: Effective Knowledge-Integrated Learning for Dialogue State\n  Tracking using Gazetteer and Speaker Information", "abstract": "Dialogue State Tracking (DST) is core research in dialogue systems and has\nreceived much attention. In addition, it is necessary to define a new problem\nthat can deal with dialogue between users as a step toward the conversational\nAI that extracts and recommends information from the dialogue between users.\nSo, we introduce a new task - DST from dialogue between users about scheduling\nan event (DST-USERS). The DST-USERS task is much more challenging since it\nrequires the model to understand and track dialogue states in the dialogue\nbetween users and to understand who suggested the schedule and who agreed to\nthe proposed schedule. To facilitate DST-USERS research, we develop dialogue\ndatasets between users that plan a schedule. The annotated slot values which\nneed to be extracted in the dialogue are date, time, and location. Previous\napproaches, such as Machine Reading Comprehension (MRC) and traditional DST\ntechniques, have not achieved good results in our extensive evaluations. By\nadopting the knowledge-integrated learning method, we achieve exceptional\nresults. The proposed model architecture combines gazetteer features and\nspeaker information efficiently. Our evaluations of the dialogue datasets\nbetween users that plan a schedule show that our model outperforms the baseline\nmodel.", "published": "2023-01-18 07:11:56", "link": "http://arxiv.org/abs/2301.07341v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for\n  Text-to-SQL Parsing", "abstract": "The task of text-to-SQL parsing, which aims at converting natural language\nquestions into executable SQL queries, has garnered increasing attention in\nrecent years, as it can assist end users in efficiently extracting vital\ninformation from databases without the need for technical background. One of\nthe major challenges in text-to-SQL parsing is domain generalization, i.e., how\nto generalize well to unseen databases. Recently, the pre-trained text-to-text\ntransformer model, namely T5, though not specialized for text-to-SQL parsing,\nhas achieved state-of-the-art performance on standard benchmarks targeting\ndomain generalization. In this work, we explore ways to further augment the\npre-trained T5 model with specialized components for text-to-SQL parsing. Such\ncomponents are expected to introduce structural inductive bias into text-to-SQL\nparsers thus improving model's capacity on (potentially multi-hop) reasoning,\nwhich is critical for generating structure-rich SQLs. To this end, we propose a\nnew architecture GRAPHIX-T5, a mixed model with the standard pre-trained\ntransformer model augmented by some specially-designed graph-aware layers.\nExtensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5\nacross four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5\nsurpass all other T5-based parsers with a significant margin, achieving new\nstate-of-the-art performance. Notably, GRAPHIX-T5-large reach performance\nsuperior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6%\non execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and\n1.5% on EX.", "published": "2023-01-18 13:29:05", "link": "http://arxiv.org/abs/2301.07507v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Adapting Multilingual Speech Representation Model for a New,\n  Underresourced Language through Multilingual Fine-tuning and Continued\n  Pretraining", "abstract": "In recent years, neural models learned through self-supervised pretraining on\nlarge scale multilingual text or speech data have exhibited promising results\nfor underresourced languages, especially when a relatively large amount of data\nfrom related language(s) is available. While the technology has a potential for\nfacilitating tasks carried out in language documentation projects, such as\nspeech transcription, pretraining a multilingual model from scratch for every\nnew language would be highly impractical. We investigate the possibility for\nadapting an existing multilingual wav2vec 2.0 model for a new language,\nfocusing on actual fieldwork data from a critically endangered tongue: Ainu.\nSpecifically, we (i) examine the feasibility of leveraging data from similar\nlanguages also in fine-tuning; (ii) verify whether the model's performance can\nbe improved by further pretraining on target language data. Our results show\nthat continued pretraining is the most effective method to adapt a wav2vec 2.0\nmodel for a new language and leads to considerable reduction in error rates.\nFurthermore, we find that if a model pretrained on a related speech variety or\nan unrelated language with similar phonological characteristics is available,\nmultilingual fine-tuning using additional data from that language can have\npositive impact on speech recognition performance when there is very little\nlabeled data in the target language.", "published": "2023-01-18 03:57:53", "link": "http://arxiv.org/abs/2301.07295v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "News and Load: A Quantitative Exploration of Natural Language Processing\n  Applications for Forecasting Day-ahead Electricity System Demand", "abstract": "The relationship between electricity demand and weather is well established\nin power systems, along with the importance of behavioral and social aspects\nsuch as holidays and significant events. This study explores the link between\nelectricity demand and more nuanced information about social events. This is\ndone using mature Natural Language Processing (NLP) and demand forecasting\ntechniques. The results indicate that day-ahead forecasts are improved by\ntextual features such as word frequencies, public sentiments, topic\ndistributions, and word embeddings. The social events contained in these\nfeatures include global pandemics, politics, international conflicts,\ntransportation, etc. Causality effects and correlations are discussed to\npropose explanations for the mechanisms behind the links highlighted. This\nstudy is believed to bring a new perspective to traditional electricity demand\nanalysis. It confirms the feasibility of improving forecasts from unstructured\ntext, with potential consequences for sociology and economics.", "published": "2023-01-18 13:55:08", "link": "http://arxiv.org/abs/2301.07535v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Dereverberation in Acoustic Sensor Networks Using Weighted Prediction\n  Error With Microphone-dependent Prediction Delays", "abstract": "In the last decades several multi-microphone speech dereverberation\nalgorithms have been proposed, among which the weighted prediction error (WPE)\nalgorithm. In the WPE algorithm, a prediction delay is required to reduce the\ncorrelation between the prediction signals and the direct component in the\nreference microphone signal. In compact arrays with closely-spaced microphones,\nthe prediction delay is often chosen microphone-independent. In acoustic sensor\nnetworks with spatially distributed microphones, large\ntime-differences-of-arrival (TDOAs) of the speech source between the reference\nmicrophone and other microphones typically occur. Hence, when using a\nmicrophone-independent prediction delay the reference and prediction signals\nmay still be significantly correlated, leading to distortion in the\ndereverberated output signal. In order to decorrelate the signals, in this\npaper we propose to apply TDOA compensation with respect to the reference\nmicrophone, resulting in microphone-dependent prediction delays for the WPE\nalgorithm. We consider both optimal TDOA compensation using crossband filtering\nin the short-time Fourier transform domain as well as band-to-band and integer\ndelay approximations. Simulation results for different reverberation times\nusing oracle as well as estimated TDOAs clearly show the benefit of using\nmicrophone-dependent prediction delays.", "published": "2023-01-18 16:52:07", "link": "http://arxiv.org/abs/2301.07649v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An investigation of the reconstruction capacity of stacked convolutional\n  autoencoders for log-mel-spectrograms", "abstract": "In audio processing applications, the generation of expressive sounds based\non high-level representations demonstrates a high demand. These representations\ncan be used to manipulate the timbre and influence the synthesis of creative\ninstrumental notes. Modern algorithms, such as neural networks, have inspired\nthe development of expressive synthesizers based on musical instrument timbre\ncompression. Unsupervised deep learning methods can achieve audio compression\nby training the network to learn a mapping from waveforms or spectrograms to\nlow-dimensional representations. This study investigates the use of stacked\nconvolutional autoencoders for the compression of time-frequency audio\nrepresentations for a variety of instruments for a single pitch. Further\nexploration of hyper-parameters and regularization techniques is demonstrated\nto enhance the performance of the initial design. In an unsupervised manner,\nthe network is able to reconstruct a monophonic and harmonic sound based on\nlatent representations. In addition, we introduce an evaluation metric to\nmeasure the similarity between the original and reconstructed samples.\nEvaluating a deep generative model for the synthesis of sound is a challenging\ntask. Our approach is based on the accuracy of the generated frequencies as it\npresents a significant metric for the perception of harmonic sounds. This work\nis expected to accelerate future experiments on audio compression using neural\nautoencoders.", "published": "2023-01-18 17:19:04", "link": "http://arxiv.org/abs/2301.07665v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
