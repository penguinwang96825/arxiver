{"title": "Guiding Attention for Self-Supervised Learning with Transformers", "abstract": "In this paper, we propose a simple and effective technique to allow for\nefficient self-supervised learning with bi-directional Transformers. Our\napproach is motivated by recent studies demonstrating that self-attention\npatterns in trained models contain a majority of non-linguistic regularities.\nWe propose a computationally efficient auxiliary loss function to guide\nattention heads to conform to such patterns. Our method is agnostic to the\nactual pre-training objective and results in faster convergence of models as\nwell as better performance on downstream tasks compared to the baselines,\nachieving state of the art results in low-resource settings. Surprisingly, we\nalso find that linguistic properties of attention heads are not necessarily\ncorrelated with language modeling performance.", "published": "2020-10-06 00:04:08", "link": "http://arxiv.org/abs/2010.02399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Few-Shot Named Entity Recognition with Structured\n  Nearest Neighbor Learning", "abstract": "We present a simple few-shot named entity recognition (NER) system based on\nnearest neighbor learning and structured inference. Our system uses a\nsupervised NER model trained on the source domain, as a feature extractor.\nAcross several test domains, we show that a nearest neighbor classifier in this\nfeature-space is far more effective than the standard meta-learning approaches.\nWe further propose a cheap but effective method to capture the label\ndependencies between entity tags without expensive CRF training. We show that\nour method of combining structured decoding with nearest neighbor learning\nachieves state-of-the-art performance on standard few-shot NER evaluation\ntasks, improving F1 scores by $6\\%$ to $16\\%$ absolute points over prior\nmeta-learning based systems.", "published": "2020-10-06 00:25:50", "link": "http://arxiv.org/abs/2010.02405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Supervision in Unsupervised Constituency Parsing", "abstract": "We analyze several recent unsupervised constituency parsing models, which are\ntuned with respect to the parsing $F_1$ score on the Wall Street Journal (WSJ)\ndevelopment set (1,700 sentences). We introduce strong baselines for them, by\ntraining an existing supervised parsing model (Kitaev and Klein, 2018) on the\nsame labeled examples they access. When training on the 1,700 examples, or even\nwhen using only 50 examples for training and 5 for development, such a few-shot\nparsing approach can outperform all the unsupervised parsing methods by a\nsignificant margin. Few-shot parsing can be further improved by a simple data\naugmentation method and self-training. This suggests that, in order to arrive\nat fair conclusions, we should carefully consider the amount of labeled data\nused for model development. We propose two protocols for future work on\nunsupervised parsing: (i) use fully unsupervised criteria for hyperparameter\ntuning and model selection; (ii) use as few labeled examples as possible for\nmodel development, and compare to few-shot parsing trained on the same labeled\nexamples.", "published": "2020-10-06 01:34:58", "link": "http://arxiv.org/abs/2010.02423v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UnQovering Stereotyping Biases via Underspecified Questions", "abstract": "While language embeddings have been shown to have stereotyping biases, how\nthese biases affect downstream question answering (QA) models remains\nunexplored. We present UNQOVER, a general framework to probe and quantify\nbiases through underspecified questions. We show that a naive use of model\nscores can lead to incorrect bias estimates due to two forms of reasoning\nerrors: positional dependence and question independence. We design a formalism\nthat isolates the aforementioned errors. As case studies, we use this metric to\nanalyze four important classes of stereotypes: gender, nationality, ethnicity,\nand religion. We probe five transformer-based QA models trained on two QA\ndatasets, along with their underlying language models. Our broad study reveals\nthat (1) all these models, with and without fine-tuning, have notable\nstereotyping biases in these classes; (2) larger models often have higher bias;\nand (3) the effect of fine-tuning on bias varies strongly with the dataset and\nthe model size.", "published": "2020-10-06 01:49:52", "link": "http://arxiv.org/abs/2010.02428v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Preconditions in Text with a Crowd-sourced Dataset", "abstract": "Preconditions provide a form of logical connection between events that\nexplains why some events occur together and information that is complementary\nto the more widely studied relations such as causation, temporal ordering,\nentailment, and discourse relations. Modeling preconditions in text has been\nhampered in part due to the lack of large scale labeled data grounded in text.\nThis paper introduces PeKo, a crowd-sourced annotation of preconditions between\nevent pairs in newswire, an order of magnitude larger than prior text\nannotations. To complement this new corpus, we also introduce two challenge\ntasks aimed at modeling preconditions: (i) Precondition Identification -- a\nstandard classification task defined over pairs of event mentions, and (ii)\nPrecondition Generation -- a generative task aimed at testing a more general\nability to reason about a given event. Evaluation on both tasks shows that\nmodeling preconditions is challenging even for today's large language models\n(LM). This suggests that precondition knowledge is not easily accessible in\nLM-derived representations alone. Our generation results show that fine-tuning\nan LM on PeKo yields better conditional relations than when trained on raw text\nor temporally-ordered corpora.", "published": "2020-10-06 01:52:34", "link": "http://arxiv.org/abs/2010.02429v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Fact Correction in Abstractive Text Summarization", "abstract": "Pre-trained neural abstractive summarization systems have dominated\nextractive strategies on news summarization performance, at least in terms of\nROUGE. However, system-generated abstractive summaries often face the pitfall\nof factual inconsistency: generating incorrect facts with respect to the source\ntext. To address this challenge, we propose Span-Fact, a suite of two factual\ncorrection models that leverages knowledge learned from question answering\nmodels to make corrections in system-generated summaries via span selection.\nOur models employ single or multi-masking strategies to either iteratively or\nauto-regressively replace entities in order to ensure semantic consistency\nw.r.t. the source text, while retaining the syntactic structure of summaries\ngenerated by abstractive summarization models. Experiments show that our models\nsignificantly boost the factual consistency of system-generated summaries\nwithout sacrificing summary quality in terms of both automatic metrics and\nhuman evaluation.", "published": "2020-10-06 02:51:02", "link": "http://arxiv.org/abs/2010.02443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Branching Bias of Syntax Extracted from Pre-trained Language\n  Models", "abstract": "Many efforts have been devoted to extracting constituency trees from\npre-trained language models, often proceeding in two stages: feature definition\nand parsing. However, this kind of methods may suffer from the branching bias\nissue, which will inflate the performances on languages with the same branch it\nbiases to. In this work, we propose quantitatively measuring the branching bias\nby comparing the performance gap on a language and its reversed language, which\nis agnostic to both language models and extracting methods. Furthermore, we\nanalyze the impacts of three factors on the branching bias, namely parsing\nalgorithms, feature definitions, and language models. Experiments show that\nseveral existing works exhibit branching biases, and some implementations of\nthese three factors can introduce the branching bias.", "published": "2020-10-06 03:09:14", "link": "http://arxiv.org/abs/2010.02448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Words Commensurate with Actions? Quantifying Commitment to a Cause\n  from Online Public Messaging", "abstract": "Public entities such as companies and politicians increasingly use online\nsocial networks to communicate directly with their constituencies. Often, this\npublic messaging is aimed at aligning the entity with a particular cause or\nissue, such as the environment or public health. However, as a consumer or\nvoter, it can be difficult to assess an entity's true commitment to a cause\nbased on public messaging. In this paper, we present a text classification\napproach to categorize a message according to its commitment level toward a\ncause. We then compare the volume of such messages with external ratings based\non entities' actions (e.g., a politician's voting record with respect to the\nenvironment or a company's rating from environmental non-profits). We find that\nby distinguishing between low- and high- level commitment messages, we can more\nreliably identify truly committed entities. Furthermore, by measuring the\ndiscrepancy between classified messages and external ratings, we can identify\nentities whose public messaging does not align with their actions, thereby\nproviding a methodology to identify potentially \"inauthentic\" messaging\ncampaigns.", "published": "2020-10-06 04:12:28", "link": "http://arxiv.org/abs/2010.02466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Domain-Repaired Back-Translation", "abstract": "In this paper, we focus on the domain-specific translation with low\nresources, where in-domain parallel corpora are scarce or nonexistent. One\ncommon and effective strategy for this case is exploiting in-domain monolingual\ndata with the back-translation method. However, the synthetic parallel data is\nvery noisy because they are generated by imperfect out-of-domain systems,\nresulting in the poor performance of domain adaptation. To address this issue,\nwe propose a novel iterative domain-repaired back-translation framework, which\nintroduces the Domain-Repair (DR) model to refine translations in synthetic\nbilingual data. To this end, we construct corresponding data for the DR model\ntraining by round-trip translating the monolingual sentences, and then design\nthe unified training framework to optimize paired DR and NMT models jointly.\nExperiments on adapting NMT models between specific domains and from the\ngeneral domain to specific domains demonstrate the effectiveness of our\nproposed approach, achieving 15.79 and 4.47 BLEU improvements on average over\nunadapted models and back-translation.", "published": "2020-10-06 04:38:09", "link": "http://arxiv.org/abs/2010.02473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretrained Language Model Embryology: The Birth of ALBERT", "abstract": "While behaviors of pretrained language models (LMs) have been thoroughly\nexamined, what happened during pretraining is rarely studied. We thus\ninvestigate the developmental process from a set of randomly initialized\nparameters to a totipotent language model, which we refer to as the embryology\nof a pretrained language model. Our results show that ALBERT learns to\nreconstruct and predict tokens of different parts of speech (POS) in different\nlearning speeds during pretraining. We also find that linguistic knowledge and\nworld knowledge do not generally improve as pretraining proceeds, nor do\ndownstream tasks' performance. These findings suggest that knowledge of a\npretrained model varies during pretraining, and having more pretrain steps does\nnot necessarily provide a model with more comprehensive knowledge. We will\nprovide source codes and pretrained models to reproduce our results at\nhttps://github.com/d223302/albert-embryology.", "published": "2020-10-06 05:15:39", "link": "http://arxiv.org/abs/2010.02480v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Help! Need Advice on Identifying Advice", "abstract": "Humans use language to accomplish a wide variety of tasks - asking for and\ngiving advice being one of them. In online advice forums, advice is mixed in\nwith non-advice, like emotional support, and is sometimes stated explicitly,\nsometimes implicitly. Understanding the language of advice would equip systems\nwith a better grasp of language pragmatics; practically, the ability to\nidentify advice would drastically increase the efficiency of advice-seeking\nonline, as well as advice-giving in natural language generation systems.\n  We present a dataset in English from two Reddit advice forums - r/AskParents\nand r/needadvice - annotated for whether sentences in posts contain advice or\nnot. Our analysis reveals rich linguistic phenomena in advice discourse. We\npresent preliminary models showing that while pre-trained language models are\nable to capture advice better than rule-based systems, advice identification is\nchallenging, and we identify directions for future research.\n  Comments: To be presented at EMNLP 2020.", "published": "2020-10-06 05:49:03", "link": "http://arxiv.org/abs/2010.02494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRUEN for Evaluating Linguistic Quality of Generated Text", "abstract": "Automatic evaluation metrics are indispensable for evaluating generated text.\nTo date, these metrics have focused almost exclusively on the content selection\naspect of the system output, ignoring the linguistic quality aspect altogether.\nWe bridge this gap by proposing GRUEN for evaluating Grammaticality,\nnon-Redundancy, focUs, structure and coherENce of generated text. GRUEN\nutilizes a BERT-based model and a class of syntactic, semantic, and contextual\nfeatures to examine the system output. Unlike most existing evaluation metrics\nwhich require human references as an input, GRUEN is reference-less and\nrequires only the system output. Besides, it has the advantage of being\nunsupervised, deterministic, and adaptable to various tasks. Experiments on\nseven datasets over four language generation tasks show that the proposed\nmetric correlates highly with human judgments.", "published": "2020-10-06 05:59:25", "link": "http://arxiv.org/abs/2010.02498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Tokenization Strategies for Various Korean NLP\n  Tasks", "abstract": "Typically, tokenization is the very first step in most text processing works.\nAs a token serves as an atomic unit that embeds the contextual information of\ntext, how to define a token plays a decisive role in the performance of a\nmodel.Even though Byte Pair Encoding (BPE) has been considered the de facto\nstandard tokenization method due to its simplicity and universality, it still\nremains unclear whether BPE works best across all languages and tasks. In this\npaper, we test several tokenization strategies in order to answer our primary\nresearch question, that is, \"What is the best tokenization strategy for Korean\nNLP tasks?\" Experimental results demonstrate that a hybrid approach of\nmorphological segmentation followed by BPE works best in Korean to/from English\nmachine translation and natural language understanding tasks such as KorNLI,\nKorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of\nSQuAD, BPE segmentation turns out to be the most effective.", "published": "2020-10-06 07:20:41", "link": "http://arxiv.org/abs/2010.02534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Explicit Alignments Robustly Improve Multilingual Encoders?", "abstract": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised\nmultilingual encoders can effectively learn cross-lingual representation.\nExplicit alignment objectives based on bitexts like Europarl or MultiUN have\nbeen shown to further improve these representations. However, word-level\nalignments are often suboptimal and such bitexts are unavailable for many\nlanguages. In this paper, we propose a new contrastive alignment objective that\ncan better utilize such signal, and examine whether these previous alignment\nmethods can be adapted to noisier sources of aligned data: a randomly sampled 1\nmillion pair subset of the OPUS collection. Additionally, rather than report\nresults on a single dataset with a single model run, we report the mean and\nstandard derivation of multiple runs with different seeds, on four datasets and\ntasks. Our more extensive analysis finds that, while our new objective\noutperforms previous work, overall these methods do not improve performance\nwith a more robust evaluation framework. Furthermore, the gains from using a\nbetter underlying model eclipse any benefits from alignment training. These\nnegative results dictate more care in evaluating these methods and suggest\nlimitations in applying explicit alignment objectives.", "published": "2020-10-06 07:43:17", "link": "http://arxiv.org/abs/2010.02537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Please Mind the Root: Decoding Arborescences for Dependency Parsing", "abstract": "The connection between dependency trees and spanning trees is exploited by\nthe NLP community to train and to decode graph-based dependency parsers.\nHowever, the NLP literature has missed an important difference between the two\nstructures: only one edge may emanate from the root in a dependency tree. We\nanalyzed the output of state-of-the-art parsers on many languages from the\nUniversal Dependency Treebank: although these parsers are often able to learn\nthat trees which violate the constraint should be assigned lower probabilities,\ntheir ability to do so unsurprisingly de-grades as the size of the training set\ndecreases. In fact, the worst constraint-violation rate we observe is 24%.\nPrior work has proposed an inefficient algorithm to enforce the constraint,\nwhich adds a factor of n to the decoding runtime. We adapt an algorithm due to\nGabow and Tarjan (1984) to dependency parsing, which satisfies the constraint\nwithout compromising the original runtime.", "published": "2020-10-06 08:31:14", "link": "http://arxiv.org/abs/2010.02550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Rejuvenation: Exploiting Inactive Training Examples for Neural\n  Machine Translation", "abstract": "Large-scale training datasets lie at the core of the recent success of neural\nmachine translation (NMT) models. However, the complex patterns and potential\nnoises in the large-scale data make training NMT models difficult. In this\nwork, we explore to identify the inactive training examples which contribute\nless to the model performance, and show that the existence of inactive examples\ndepends on the data distribution. We further introduce data rejuvenation to\nimprove the training of NMT models on large-scale datasets by exploiting\ninactive examples. The proposed framework consists of three phases. First, we\ntrain an identification model on the original training data, and use it to\ndistinguish inactive examples and active examples by their sentence-level\noutput probabilities. Then, we train a rejuvenation model on the active\nexamples, which is used to re-label the inactive examples with\nforward-translation. Finally, the rejuvenated examples and the active examples\nare combined to train the final NMT model. Experimental results on WMT14\nEnglish-German and English-French datasets show that the proposed data\nrejuvenation consistently and significantly improves performance for several\nstrong NMT models. Extensive analyses reveal that our approach stabilizes and\naccelerates the training process of NMT models, resulting in final models with\nbetter generalization capability.", "published": "2020-10-06 08:57:31", "link": "http://arxiv.org/abs/2010.02552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PolicyQA: A Reading Comprehension Dataset for Privacy Policies", "abstract": "Privacy policy documents are long and verbose. A question answering (QA)\nsystem can assist users in finding the information that is relevant and\nimportant to them. Prior studies in this domain frame the QA task as retrieving\nthe most relevant text segment or a list of sentences from the policy document\ngiven a question. On the contrary, we argue that providing users with a short\ntext span from policy documents reduces the burden of searching the target\ninformation from a lengthy text segment. In this paper, we present PolicyQA, a\ndataset that contains 25,017 reading comprehension style examples curated from\nan existing corpus of 115 website privacy policies. PolicyQA provides 714\nhuman-annotated questions written for a wide range of privacy practices. We\nevaluate two existing neural QA models and perform rigorous analysis to reveal\nthe advantages and challenges offered by PolicyQA.", "published": "2020-10-06 09:04:58", "link": "http://arxiv.org/abs/2010.02557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEGAL-BERT: The Muppets straight out of Law School", "abstract": "BERT has achieved impressive performance in several NLP tasks. However, there\nhas been limited investigation on its adaptation guidelines in specialised\ndomains. Here we focus on the legal domain, where we explore several approaches\nfor applying BERT models to downstream legal tasks, evaluating on multiple\ndatasets. Our findings indicate that the previous guidelines for pre-training\nand fine-tuning, often blindly followed, do not always generalize well in the\nlegal domain. Thus we propose a systematic investigation of the available\nstrategies when applying BERT in specialised domains. These are: (a) use the\noriginal BERT out of the box, (b) adapt BERT by additional pre-training on\ndomain-specific corpora, and (c) pre-train BERT from scratch on domain-specific\ncorpora. We also propose a broader hyper-parameter search space when\nfine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT\nmodels intended to assist legal NLP research, computational law, and legal\ntechnology applications.", "published": "2020-10-06 09:06:07", "link": "http://arxiv.org/abs/2010.02559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Text Classification with Minimal Resources by Transferring\n  a Sparse Teacher", "abstract": "Cross-lingual text classification alleviates the need for manually labeled\ndocuments in a target language by leveraging labeled documents from other\nlanguages. Existing approaches for transferring supervision across languages\nrequire expensive cross-lingual resources, such as parallel corpora, while less\nexpensive cross-lingual representation learning approaches train classifiers\nwithout target labeled documents. In this work, we propose a cross-lingual\nteacher-student method, CLTS, that generates \"weak\" supervision in the target\nlanguage using minimal cross-lingual resources, in the form of a small number\nof word translations. Given a limited translation budget, CLTS extracts and\ntransfers only the most important task-specific seed words across languages and\ninitializes a teacher classifier based on the translated seed words. Then, CLTS\niteratively trains a more powerful student that also exploits the context of\nthe seed words in unlabeled target documents and outperforms the teacher. CLTS\nis simple and surprisingly effective in 18 diverse languages: by transferring\njust 20 seed words, even a bag-of-words logistic regression student outperforms\nstate-of-the-art cross-lingual methods (e.g., based on multilingual BERT).\nMoreover, CLTS can accommodate any type of student classifier: leveraging a\nmonolingual BERT student leads to further improvements and outperforms even\nmore expensive approaches by up to 12% in accuracy. Finally, CLTS addresses\nemerging tasks in low-resource languages using just a small number of word\ntranslations.", "published": "2020-10-06 09:11:02", "link": "http://arxiv.org/abs/2010.02562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SupMMD: A Sentence Importance Model for Extractive Summarization using\n  Maximum Mean Discrepancy", "abstract": "Most work on multi-document summarization has focused on generic\nsummarization of information present in each individual document set. However,\nthe under-explored setting of update summarization, where the goal is to\nidentify the new information present in each set, is of equal practical\ninterest (e.g., presenting readers with updates on an evolving news topic). In\nthis work, we present SupMMD, a novel technique for generic and update\nsummarization based on the maximum mean discrepancy from kernel two-sample\ntesting. SupMMD combines both supervised learning for salience and unsupervised\nlearning for coverage and diversity. Further, we adapt multiple kernel learning\nto make use of similarity across multiple information sources (e.g., text\nfeatures and knowledge based concepts). We show the efficacy of SupMMD in both\ngeneric and update summarization tasks by meeting or exceeding the current\nstate-of-the-art on the DUC-2004 and TAC-2009 datasets.", "published": "2020-10-06 09:26:55", "link": "http://arxiv.org/abs/2010.02568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyleDGPT: Stylized Response Generation with Pre-trained Language Models", "abstract": "Generating responses following a desired style has great potentials to extend\napplications of open-domain dialogue systems, yet is refrained by lacking of\nparallel data for training. In this work, we explore the challenging task with\npre-trained language models that have brought breakthrough to various natural\nlanguage tasks. To this end, we introduce a KL loss and a style classifier to\nthe fine-tuning step in order to steer response generation towards the target\nstyle in both a word-level and a sentence-level. Comprehensive empirical\nstudies with two public datasets indicate that our model can significantly\noutperform state-of-the-art methods in terms of both style consistency and\ncontextual coherence.", "published": "2020-10-06 09:29:50", "link": "http://arxiv.org/abs/2010.02569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does the Objective Matter? Comparing Training Objectives for Pronoun\n  Resolution", "abstract": "Hard cases of pronoun resolution have been used as a long-standing benchmark\nfor commonsense reasoning. In the recent literature, pre-trained language\nmodels have been used to obtain state-of-the-art results on pronoun resolution.\nOverall, four categories of training and evaluation objectives have been\nintroduced. The variety of training datasets and pre-trained language models\nused in these works makes it unclear whether the choice of training objective\nis critical. In this work, we make a fair comparison of the performance and\nseed-wise stability of four models that represent the four categories of\nobjectives. Our experiments show that the objective of sequence ranking\nperforms the best in-domain, while the objective of semantic similarity between\ncandidates and pronoun performs the best out-of-domain. We also observe a\nseed-wise instability of the model using sequence ranking, which is not the\ncase when the other objectives are used.", "published": "2020-10-06 09:29:51", "link": "http://arxiv.org/abs/2010.02570v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Universal Natural Language Processing with Limited Annotations: Try\n  Few-shot Textual Entailment as a Start", "abstract": "A standard way to address different NLP problems is by first constructing a\nproblem-specific dataset, then building a model to fit this dataset. To build\nthe ultimate artificial intelligence, we desire a single machine that can\nhandle diverse new problems, for which task-specific annotations are limited.\nWe bring up textual entailment as a unified solver for such NLP problems.\nHowever, current research of textual entailment has not spilled much ink on the\nfollowing questions: (i) How well does a pretrained textual entailment system\ngeneralize across domains with only a handful of domain-specific examples? and\n(ii) When is it worth transforming an NLP task into textual entailment? We\nargue that the transforming is unnecessary if we can obtain rich annotations\nfor this task. Textual entailment really matters particularly when the target\nNLP task has insufficient annotations.\n  Universal NLP can be probably achieved through different routines. In this\nwork, we introduce Universal Few-shot textual Entailment (UFO-Entail). We\ndemonstrate that this framework enables a pretrained entailment model to work\nwell on new entailment domains in a few-shot setting, and show its\neffectiveness as a unified solver for several downstream NLP tasks such as\nquestion answering and coreference resolution when the end-task annotations are\nlimited. Code: https://github.com/salesforce/UniversalFewShotNLP", "published": "2020-10-06 09:50:25", "link": "http://arxiv.org/abs/2010.02584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Span Identification Tasks with Performance Prediction", "abstract": "Span identification (in short, span ID) tasks such as chunking, NER, or\ncode-switching detection, ask models to identify and classify relevant spans in\na text. Despite being a staple of NLP, and sharing a common structure, there is\nlittle insight on how these tasks' properties influence their difficulty, and\nthus little guidance on what model families work well on span ID tasks, and\nwhy. We analyze span ID tasks via performance prediction, estimating how well\nneural architectures do on different tasks. Our contributions are: (a) we\nidentify key properties of span ID tasks that can inform performance\nprediction; (b) we carry out a large-scale experiment on English data, building\na model to predict performance for unseen span ID tasks that can support\narchitecture choices; (c), we investigate the parameters of the meta model,\nyielding new insights on how model and task properties interact to affect span\nID performance. We find, e.g., that span frequency is especially important for\nLSTMs, and that CRFs help when spans are infrequent and boundaries\nnon-distinctive.", "published": "2020-10-06 09:55:00", "link": "http://arxiv.org/abs/2010.02587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRefi: A Crowd Sourcing Suite for Coreference Annotation", "abstract": "Coreference annotation is an important, yet expensive and time consuming,\ntask, which often involved expert annotators trained on complex decision\nguidelines. To enable cheaper and more efficient annotation, we present CoRefi,\na web-based coreference annotation suite, oriented for crowdsourcing. Beyond\nthe core coreference annotation tool, CoRefi provides guided onboarding for the\ntask as well as a novel algorithm for a reviewing phase. CoRefi is open source\nand directly embeds into any website, including popular crowdsourcing\nplatforms.\n  CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo:\nhttps://github.com/aribornstein/corefi", "published": "2020-10-06 09:55:36", "link": "http://arxiv.org/abs/2010.02588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scene Graph Modification Based on Natural Language Commands", "abstract": "Structured representations like graphs and parse trees play a crucial role in\nmany Natural Language Processing systems. In recent years, the advancements in\nmulti-turn user interfaces necessitate the need for controlling and updating\nthese structured representations given new sources of information. Although\nthere have been many efforts focusing on improving the performance of the\nparsers that map text to graphs or parse trees, very few have explored the\nproblem of directly manipulating these representations. In this paper, we\nexplore the novel problem of graph modification, where the systems need to\nlearn how to update an existing scene graph given a new user's command. Our\nnovel models based on graph-based sparse transformer and cross attention\ninformation fusion outperform previous systems adapted from the machine\ntranslation and graph generation literature. We further contribute our large\ngraph modification datasets to the research community to encourage future\nresearch for this new problem.", "published": "2020-10-06 10:01:19", "link": "http://arxiv.org/abs/2010.02591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Position-Aware Tagging for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the\ntriplets of target entities, their associated sentiment, and opinion spans\nexplaining the reason for the sentiment. Existing research efforts mostly solve\nthis problem using pipeline approaches, which break the triplet extraction\nprocess into several stages. Our observation is that the three elements within\na triplet are highly related to each other, and this motivates us to build a\njoint model to extract such triplets using a sequence tagging approach.\nHowever, how to effectively design a tagging approach to extract the triplets\nthat can capture the rich interactions among the elements is a challenging\nresearch question. In this work, we propose the first end-to-end model with a\nnovel position-aware tagging scheme that is capable of jointly extracting the\ntriplets. Our experimental results on several existing datasets show that\njointly capturing elements in the triplet using our approach leads to improved\nperformance over the existing approaches. We also conducted extensive\nexperiments to investigate the model effectiveness and robustness.", "published": "2020-10-06 10:40:34", "link": "http://arxiv.org/abs/2010.02609v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Speech Synthesis for Estonian", "abstract": "This technical report describes the results of a collaboration between the\nNLP research group at the University of Tartu and the Institute of Estonian\nLanguage on improving neural speech synthesis for Estonian. The report (written\nin Estonian) describes the project results, the summary of which is: (1) Speech\nsynthesis data from 6 speakers for a total of 92.4 hours is collected and\nopenly released (CC-BY-4.0). Data available at https://konekorpus.tartunlp.ai\nand https://www.eki.ee/litsents/. (2) software and models for neural speech\nsynthesis is released open-source (MIT license). Available at\nhttps://koodivaramu.eesti.ee/tartunlp/text-to-speech . (3) We ran evaluations\nof the new models and compared them to other existing solutions (HMM-based HTS\nmodels from EKI, http://www.eki.ee/heli/, and Google's speech synthesis for\nEstonian, accessed via https://translate.google.com). Evaluation includes voice\nacceptability MOS scores for sentence-level and longer excerpts, detailed error\nanalysis and evaluation of the pre-processing module.", "published": "2020-10-06 11:37:46", "link": "http://arxiv.org/abs/2010.02636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Sparsity of Neural Machine Translation Models", "abstract": "Modern neural machine translation (NMT) models employ a large number of\nparameters, which leads to serious over-parameterization and typically causes\nthe underutilization of computational resources. In response to this problem,\nwe empirically investigate whether the redundant parameters can be reused to\nachieve better performance. Experiments and analyses are systematically\nconducted on different datasets and NMT architectures. We show that: 1) the\npruned parameters can be rejuvenated to improve the baseline model by up to\n+0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the\nability of modeling low-level lexical information.", "published": "2020-10-06 11:47:20", "link": "http://arxiv.org/abs/2010.02646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Modeling with Evidence Filter for Multiple Choice Question\n  Answering", "abstract": "Multiple-Choice Question Answering (MCQA) is a challenging task in machine\nreading comprehension. The main challenge in MCQA is to extract \"evidence\" from\nthe given context that supports the correct answer. In the OpenbookQA dataset,\nthe requirement of extracting \"evidence\" is particularly important due to the\nmutual independence of sentences in the context. Existing work tackles this\nproblem by annotated evidence or distant supervision with rules which overly\nrely on human efforts. To address the challenge, we propose a simple yet\neffective approach termed evidence filtering to model the relationships between\nthe encoded contexts with respect to different options collectively and to\npotentially highlight the evidence sentences and filter out unrelated\nsentences. In addition to the effective reduction of human efforts of our\napproach compared, through extensive experiments on OpenbookQA, we show that\nthe proposed approach outperforms the models that use the same backbone and\nmore training data; and our parameter analysis also demonstrates the\ninterpretability of our approach.", "published": "2020-10-06 11:53:23", "link": "http://arxiv.org/abs/2010.02649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "If beam search is the answer, what was the question?", "abstract": "Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural\nlanguage generators frequently leads to low-quality results. Rather, most\nstate-of-the-art results on language generation tasks are attained using beam\nsearch despite its overwhelmingly high search error rate. This implies that the\nMAP objective alone does not express the properties we desire in text, which\nmerits the question: if beam search is the answer, what was the question? We\nframe beam search as the exact solution to a different decoding objective in\norder to gain insights into why high probability under a model alone may not\nindicate adequacy. We find that beam search enforces uniform information\ndensity in text, a property motivated by cognitive science. We suggest a set of\ndecoding objectives that explicitly enforce this property and find that exact\ndecoding with these objectives alleviates the problems encountered when\ndecoding poorly calibrated language generation models. Additionally, we analyze\nthe text produced using various decoding strategies and see that, in our neural\nmachine translation experiments, the extent to which this property is adhered\nto strongly correlates with BLEU.", "published": "2020-10-06 11:57:03", "link": "http://arxiv.org/abs/2010.02650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Implicitly Asserted Propositions in Argumentation", "abstract": "Argumentation accommodates various rhetorical devices, such as questions,\nreported speech, and imperatives. These rhetorical tools usually assert\nargumentatively relevant propositions rather implicitly, so understanding their\ntrue meaning is key to understanding certain arguments properly. However, most\nargument mining systems and computational linguistics research have paid little\nattention to implicitly asserted propositions in argumentation. In this paper,\nwe examine a wide range of computational methods for extracting propositions\nthat are implicitly asserted in questions, reported speech, and imperatives in\nargumentation. By evaluating the models on a corpus of 2016 U.S. presidential\ndebates and online commentary, we demonstrate the effectiveness and limitations\nof the computational models. Our study may inform future research on argument\nmining and the semantics of these rhetorical devices in argumentation.", "published": "2020-10-06 12:03:47", "link": "http://arxiv.org/abs/2010.02654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Instance Multi-Label Learning Networks for Aspect-Category\n  Sentiment Analysis", "abstract": "Aspect-category sentiment analysis (ACSA) aims to predict sentiment\npolarities of sentences with respect to given aspect categories. To detect the\nsentiment toward a particular aspect category in a sentence, most previous\nmethods first generate an aspect category-specific sentence representation for\nthe aspect category, then predict the sentiment polarity based on the\nrepresentation. These methods ignore the fact that the sentiment of an aspect\ncategory mentioned in a sentence is an aggregation of the sentiments of the\nwords indicating the aspect category in the sentence, which leads to suboptimal\nperformance. In this paper, we propose a Multi-Instance Multi-Label Learning\nNetwork for Aspect-Category sentiment analysis (AC-MIMLLN), which treats\nsentences as bags, words as instances, and the words indicating an aspect\ncategory as the key instances of the aspect category. Given a sentence and the\naspect categories mentioned in the sentence, AC-MIMLLN first predicts the\nsentiments of the instances, then finds the key instances for the aspect\ncategories, finally obtains the sentiments of the sentence toward the aspect\ncategories by aggregating the key instance sentiments. Experimental results on\nthree public datasets demonstrate the effectiveness of AC-MIMLLN.", "published": "2020-10-06 12:07:54", "link": "http://arxiv.org/abs/2010.02656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Attackable Sentences in Arguments", "abstract": "Finding attackable sentences in an argument is the first step toward\nsuccessful refutation in argumentation. We present a first large-scale analysis\nof sentence attackability in online arguments. We analyze driving reasons for\nattacks in argumentation and identify relevant characteristics of sentences. We\ndemonstrate that a sentence's attackability is associated with many of these\ncharacteristics regarding the sentence's content, proposition types, and tone,\nand that an external knowledge source can provide useful information about\nattackability. Building on these findings, we demonstrate that machine learning\nmodels can automatically detect attackable sentences in arguments,\nsignificantly better than several baselines and comparably well to laypeople.", "published": "2020-10-06 12:13:00", "link": "http://arxiv.org/abs/2010.02660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metaphor Interpretation Using Word Embeddings", "abstract": "We suggest a model for metaphor interpretation using word embeddings trained\nover a relatively large corpus. Our system handles nominal metaphors, like\n\"time is money\". It generates a ranked list of potential interpretations of\ngiven metaphors. Candidate meanings are drawn from collocations of the topic\n(\"time\") and vehicle (\"money\") components, automatically extracted from a\ndependency-parsed corpus. We explore adding candidates derived from word\nassociation norms (common human responses to cues). Our ranking procedure\nconsiders similarity between candidate interpretations and metaphor components,\nmeasured in a semantic vector space. Lastly, a clustering algorithm removes\nsemantically related duplicates, thereby allowing other candidate\ninterpretations to attain higher rank. We evaluate using different sets of\nannotated metaphors, with encouraging preliminary results.", "published": "2020-10-06 12:35:13", "link": "http://arxiv.org/abs/2010.02665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking\n  Scalar Adjectives with Contextualised Representations", "abstract": "Adjectives like pretty, beautiful and gorgeous describe positive properties\nof the nouns they modify but with different intensity. These differences are\nimportant for natural language understanding and reasoning. We propose a novel\nBERT-based approach to intensity detection for scalar adjectives. We model\nintensity by vectors directly derived from contextualised representations and\nshow they can successfully rank scalar adjectives. We evaluate our models both\nintrinsically, on gold standard datasets, and on an Indirect Question Answering\ntask. Our results demonstrate that BERT encodes rich knowledge about the\nsemantics of scalar adjectives, and is able to provide better quality intensity\nrankings than static embeddings and previous models with access to dedicated\nresources.", "published": "2020-10-06 13:05:47", "link": "http://arxiv.org/abs/2010.02686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Individual Neurons in Pre-trained Language Models", "abstract": "While a lot of analysis has been carried to demonstrate linguistic knowledge\ncaptured by the representations learned within deep NLP models, very little\nattention has been paid towards individual neurons.We carry outa neuron-level\nanalysis using core linguistic tasks of predicting morphology, syntax and\nsemantics, on pre-trained language models, with questions like: i) do\nindividual neurons in pre-trained models capture linguistic information? ii)\nwhich parts of the network learn more about certain linguistic phenomena? iii)\nhow distributed or focused is the information? and iv) how do various\narchitectures differ in learning these properties? We found small subsets of\nneurons to predict linguistic tasks, with lower level tasks (such as\nmorphology) localized in fewer neurons, compared to higher level task of\npredicting syntax. Our study also reveals interesting cross architectural\ncomparisons. For example, we found neurons in XLNet to be more localized and\ndisjoint when predicting properties compared to BERT and others, where they are\nmore distributed and coupled.", "published": "2020-10-06 13:17:38", "link": "http://arxiv.org/abs/2010.02695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Based Sentiment Analysis with Aspect-Specific Opinion Spans", "abstract": "Aspect based sentiment analysis, predicting sentiment polarity of given\naspects, has drawn extensive attention. Previous attention-based models\nemphasize using aspect semantics to help extract opinion features for\nclassification. However, these works are either not able to capture opinion\nspans as a whole, or not able to capture variable-length opinion spans. In this\npaper, we present a neat and effective structured attention model by\naggregating multiple linear-chain CRFs. Such a design allows the model to\nextract aspect-specific opinion spans and then evaluate sentiment polarity by\nexploiting the extracted opinion features. The experimental results on four\ndatasets demonstrate the effectiveness of the proposed model, and our analysis\ndemonstrates that our model can capture aspect-specific opinion spans.", "published": "2020-10-06 13:18:35", "link": "http://arxiv.org/abs/2010.02696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Notes on Coalgebras in Stylometry", "abstract": "The syntactic behaviour of texts can highly vary depending on their contexts\n(e.g. author, genre, etc.). From the standpoint of stylometry, it can be\nhelpful to objectively measure this behaviour. In this paper, we discuss how\ncoalgebras are used to formalise the notion of behaviour by embedding syntactic\nfeatures of a given text into probabilistic transition systems. By introducing\nthe behavioural distance, we are then able to quantitatively measure\ndifferences between points in these systems and thus, comparing features of\ndifferent texts. Furthermore, the behavioural distance of points can be\napproximated by a polynomial-time algorithm.", "published": "2020-10-06 13:55:11", "link": "http://arxiv.org/abs/2010.02733v2", "categories": ["cs.CL", "G.3; J.5"], "primary_category": "cs.CL"}
{"title": "Stepwise Extractive Summarization and Planning with Structured\n  Transformers", "abstract": "We propose encoder-centric stepwise models for extractive summarization using\nstructured transformers -- HiBERT and Extended Transformers. We enable stepwise\nsummarization by injecting the previously generated summary into the structured\ntransformer as an auxiliary sub-structure. Our models are not only efficient in\nmodeling the structure of long inputs, but they also do not rely on\ntask-specific redundancy-aware modeling, making them a general purpose\nextractive content planner for different tasks. When evaluated on CNN/DailyMail\nextractive summarization, stepwise models achieve state-of-the-art performance\nin terms of Rouge without any redundancy aware modeling or sentence filtering.\nThis also holds true for Rotowire table-to-text generation, where our models\nsurpass previously reported metrics for content selection, planning and\nordering, highlighting the strength of stepwise modeling. Amongst the two\nstructured transformers we test, stepwise Extended Transformers provides the\nbest performance across both datasets and sets a new standard for these\nchallenges.", "published": "2020-10-06 14:12:58", "link": "http://arxiv.org/abs/2010.02744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based\n  Inference Networks", "abstract": "Many tasks in natural language processing involve predicting structured\noutputs, e.g., sequence labeling, semantic role labeling, parsing, and machine\ntranslation. Researchers are increasingly applying deep representation learning\nto these problems, but the structured component of these approaches is usually\nquite simplistic. In this work, we propose several high-order energy terms to\ncapture complex dependencies among labels in sequence labeling, including\nseveral that consider the entire label sequence. We use neural\nparameterizations for these energy terms, drawing from convolutional,\nrecurrent, and self-attention networks. We use the framework of learning\nenergy-based inference networks (Tu and Gimpel, 2018) for dealing with the\ndifficulties of training and inference with such models. We empirically\ndemonstrate that this approach achieves substantial improvement using a variety\nof high-order energy terms on four sequence labeling tasks, while having the\nsame decoding speed as simple, local classifiers. We also find high-order\nenergies to help in noisy data conditions.", "published": "2020-10-06 14:59:16", "link": "http://arxiv.org/abs/2010.02789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COSMIC: COmmonSense knowledge for eMotion Identification in\n  Conversations", "abstract": "In this paper, we address the task of utterance level emotion recognition in\nconversations using commonsense knowledge. We propose COSMIC, a new framework\nthat incorporates different elements of commonsense such as mental states,\nevents, and causal relations, and build upon them to learn interactions between\ninterlocutors participating in a conversation. Current state-of-the-art methods\noften encounter difficulties in context propagation, emotion shift detection,\nand differentiating between related emotion classes. By learning distinct\ncommonsense representations, COSMIC addresses these challenges and achieves new\nstate-of-the-art results for emotion recognition on four different benchmark\nconversational datasets. Our code is available at\nhttps://github.com/declare-lab/conv-emotion.", "published": "2020-10-06 15:09:38", "link": "http://arxiv.org/abs/2010.02795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Probing through Dimension Selection", "abstract": "Most modern NLP systems make use of pre-trained contextual representations\nthat attain astonishingly high performance on a variety of tasks. Such high\nperformance should not be possible unless some form of linguistic structure\ninheres in these representations, and a wealth of research has sprung up on\nprobing for it. In this paper, we draw a distinction between intrinsic probing,\nwhich examines how linguistic information is structured within a\nrepresentation, and the extrinsic probing popular in prior work, which only\nargues for the presence of such information by showing that it can be\nsuccessfully extracted. To enable intrinsic probing, we propose a novel\nframework based on a decomposable multivariate Gaussian probe that allows us to\ndetermine whether the linguistic information in word embeddings is dispersed or\nfocal. We then probe fastText and BERT for various morphosyntactic attributes\nacross 36 languages. We find that most attributes are reliably encoded by only\na few neurons, with fastText concentrating its linguistic structure more than\nBERT.", "published": "2020-10-06 15:21:08", "link": "http://arxiv.org/abs/2010.02812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QADiscourse -- Discourse Relations as QA Pairs: Representation,\n  Crowdsourcing and Baselines", "abstract": "Discourse relations describe how two propositions relate to one another, and\nidentifying them automatically is an integral part of natural language\nunderstanding. However, annotating discourse relations typically requires\nexpert annotators. Recently, different semantic aspects of a sentence have been\nrepresented and crowd-sourced via question-and-answer (QA) pairs. This paper\nproposes a novel representation of discourse relations as QA pairs, which in\nturn allows us to crowd-source wide-coverage data annotated with discourse\nrelations, via an intuitively appealing interface for composing such questions\nand answers. Based on our proposed representation, we collect a novel and\nwide-coverage QADiscourse dataset, and present baseline algorithms for\npredicting QADiscourse relations.", "published": "2020-10-06 15:25:15", "link": "http://arxiv.org/abs/2010.02815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Challenge Set for Hebrew Morphological Disambiguation and\n  Diacritics Restoration", "abstract": "One of the primary tasks of morphological parsers is the disambiguation of\nhomographs. Particularly difficult are cases of unbalanced ambiguity, where one\nof the possible analyses is far more frequent than the others. In such cases,\nthere may not exist sufficient examples of the minority analyses in order to\nproperly evaluate performance, nor to train effective classifiers. In this\npaper we address the issue of unbalanced morphological ambiguities in Hebrew.\nWe offer a challenge set for Hebrew homographs -- the first of its kind --\ncontaining substantial attestation of each analysis of 21 Hebrew homographs. We\nshow that the current SOTA of Hebrew disambiguation performs poorly on cases of\nunbalanced ambiguity. Leveraging our new dataset, we achieve a new\nstate-of-the-art for all 21 words, improving the overall average F1 score from\n0.67 to 0.95. Our resulting annotated datasets are made publicly available for\nfurther research.", "published": "2020-10-06 16:34:03", "link": "http://arxiv.org/abs/2010.02864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COD3S: Diverse Generation with Discrete Semantic Signatures", "abstract": "We present COD3S, a novel method for generating semantically diverse\nsentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an\ninput, seq2seq models typically produce semantically and syntactically\nhomogeneous sets of sentences and thus perform poorly on one-to-many sequence\ngeneration tasks. Our two-stage approach improves output diversity by\nconditioning generation on locality-sensitive hash (LSH)-based semantic\nsentence codes whose Hamming distances highly correlate with human judgments of\nsemantic textual similarity. Though it is generally applicable, we apply COD3S\nto causal generation, the task of predicting a proposition's plausible causes\nor effects. We demonstrate through automatic and human evaluation that\nresponses produced using our method exhibit improved diversity without\ndegrading task performance.", "published": "2020-10-06 17:06:50", "link": "http://arxiv.org/abs/2010.02882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keep CALM and Explore: Language Models for Action Generation in\n  Text-based Games", "abstract": "Text-based games present a unique challenge for autonomous agents to operate\nin natural language and handle enormous action spaces. In this paper, we\npropose the Contextual Action Language Model (CALM) to generate a compact set\nof action candidates at each game state. Our key insight is to train language\nmodels on human gameplay, where people demonstrate linguistic priors and a\ngeneral game sense for promising actions conditioned on game history. We\ncombine CALM with a reinforcement learning agent which re-ranks the generated\naction candidates to maximize in-game rewards. We evaluate our approach using\nthe Jericho benchmark, on games unseen by CALM during training. Our method\nobtains a 69% relative improvement in average game score over the previous\nstate-of-the-art model. Surprisingly, on half of these games, CALM is\ncompetitive with or better than other models that have access to ground truth\nadmissible actions. Code and data are available at\nhttps://github.com/princeton-nlp/calm-textgame.", "published": "2020-10-06 17:36:29", "link": "http://arxiv.org/abs/2010.02903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Seeded Iterated Learning for Interactive Language Learning", "abstract": "Language drift has been one of the major obstacles to train language models\nthrough interaction. When word-based conversational agents are trained towards\ncompleting a task, they tend to invent their language rather than leveraging\nnatural language. In recent literature, two general methods partially counter\nthis phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).\nWhile S2P jointly trains interactive and supervised losses to counter the\ndrift, SIL changes the training dynamics to prevent language drift from\noccurring. In this paper, we first highlight their respective weaknesses, i.e.,\nlate-stage training collapses and higher negative likelihood when evaluated on\nhuman corpus. Given these observations, we introduce Supervised Seeded Iterated\nLearning to combine both methods to minimize their respective weaknesses. We\nthen show the effectiveness of \\algo in the language-drift translation game.", "published": "2020-10-06 19:09:02", "link": "http://arxiv.org/abs/2010.02975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are \"Undocumented Workers\" the Same as \"Illegal Aliens\"? Disentangling\n  Denotation and Connotation in Vector Spaces", "abstract": "In politics, neologisms are frequently invented for partisan objectives. For\nexample, \"undocumented workers\" and \"illegal aliens\" refer to the same group of\npeople (i.e., they have the same denotation), but they carry clearly different\nconnotations. Examples like these have traditionally posed a challenge to\nreference-based semantic theories and led to increasing acceptance of\nalternative theories (e.g., Two-Factor Semantics) among philosophers and\ncognitive scientists. In NLP, however, popular pretrained models encode both\ndenotation and connotation as one entangled representation. In this study, we\npropose an adversarial neural network that decomposes a pretrained\nrepresentation as independent denotation and connotation representations. For\nintrinsic interpretability, we show that words with the same denotation but\ndifferent connotations (e.g., \"immigrants\" vs. \"aliens\", \"estate tax\" vs.\n\"death tax\") move closer to each other in denotation space while moving further\napart in connotation space. For extrinsic application, we train an information\nretrieval system with our disentangled representations and show that the\ndenotation vectors improve the viewpoint diversity of document rankings.", "published": "2020-10-06 19:09:03", "link": "http://arxiv.org/abs/2010.02976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review on Fact Extraction and Verification", "abstract": "We study the fact checking problem, which aims to identify the veracity of a\ngiven claim. Specifically, we focus on the task of Fact Extraction and\nVERification (FEVER) and its accompanied dataset. The task consists of the\nsubtasks of retrieving the relevant documents (and sentences) from Wikipedia\nand validating whether the information in the documents supports or refutes a\ngiven claim. This task is essential and can be the building block of\napplications such as fake news detection and medical claim verification. In\nthis paper, we aim at a better understanding of the challenges of the task by\npresenting the literature in a structured and comprehensive way. We describe\nthe proposed methods by analyzing the technical perspectives of the different\napproaches and discussing the performance results on the FEVER dataset, which\nis the most well-studied and formally structured dataset on the fact extraction\nand verification task. We also conduct the largest experimental study to date\non identifying beneficial loss functions for the sentence retrieval component.\nOur analysis indicates that sampling negative sentences is important for\nimproving the performance and decreasing the computational complexity. Finally,\nwe describe open issues and future challenges, and we motivate future research\nin the task.", "published": "2020-10-06 20:05:43", "link": "http://arxiv.org/abs/2010.03001v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and\n  Event Extraction", "abstract": "Recent progress in cross-lingual relation and event extraction use graph\nconvolutional networks (GCNs) with universal dependency parses to learn\nlanguage-agnostic sentence representations such that models trained on one\nlanguage can be applied to other languages. However, GCNs struggle to model\nwords with long-range dependencies or are not directly connected in the\ndependency tree. To address these challenges, we propose to utilize the\nself-attention mechanism where we explicitly fuse structural information to\nlearn the dependencies between words with different syntactic distances. We\nintroduce GATE, a {\\bf G}raph {\\bf A}ttention {\\bf T}ransformer {\\bf E}ncoder,\nand test its cross-lingual transferability on relation and event extraction\ntasks. We perform experiments on the ACE05 dataset that includes three\ntypologically different languages: English, Chinese, and Arabic. The evaluation\nresults show that GATE outperforms three recently proposed methods by a large\nmargin. Our detailed analysis reveals that due to the reliance on syntactic\ndependencies, GATE produces robust representations that facilitate transfer\nacross languages.", "published": "2020-10-06 20:30:35", "link": "http://arxiv.org/abs/2010.03009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic\n  Priming", "abstract": "Models trained to estimate word probabilities in context have become\nubiquitous in natural language processing. How do these models use lexical cues\nin context to inform their word probabilities? To answer this question, we\npresent a case study analyzing the pre-trained BERT model with tests informed\nby semantic priming. Using English lexical stimuli that show priming in humans,\nwe find that BERT too shows \"priming,\" predicting a word with greater\nprobability when the context includes a related word versus an unrelated one.\nThis effect decreases as the amount of information provided by the context\nincreases. Follow-up analysis shows BERT to be increasingly distracted by\nrelated prime words as context becomes more informative, assigning lower\nprobabilities to related words. Our findings highlight the importance of\nconsidering contextual constraint effects when studying word prediction in\nthese models, and highlight possible parallels with human processing.", "published": "2020-10-06 20:30:59", "link": "http://arxiv.org/abs/2010.03010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resource-Enhanced Neural Model for Event Argument Extraction", "abstract": "Event argument extraction (EAE) aims to identify the arguments of an event\nand classify the roles that those arguments play. Despite great efforts made in\nprior work, there remain many challenges: (1) Data scarcity. (2) Capturing the\nlong-range dependency, specifically, the connection between an event trigger\nand a distant event argument. (3) Integrating event trigger information into\ncandidate argument representation. For (1), we explore using unlabeled data in\ndifferent ways. For (2), we propose to use a syntax-attending Transformer that\ncan utilize dependency parses to guide the attention mechanism. For (3), we\npropose a trigger-aware sequence encoder with several types of\ntrigger-dependent sequence representations. We also support argument extraction\neither from text annotated with gold entities or from plain text. Experiments\non the English ACE2005 benchmark show that our approach achieves a new\nstate-of-the-art.", "published": "2020-10-06 21:00:54", "link": "http://arxiv.org/abs/2010.03022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Skip If You Can Combine: A Simple Knowledge Distillation Technique\n  for Intermediate Layers", "abstract": "With the growth of computing power neural machine translation (NMT) models\nalso grow accordingly and become better. However, they also become harder to\ndeploy on edge devices due to memory constraints. To cope with this problem, a\ncommon practice is to distill knowledge from a large and accurately-trained\nteacher network (T) into a compact student network (S). Although knowledge\ndistillation (KD) is useful in most cases, our study shows that existing KD\ntechniques might not be suitable enough for deep NMT engines, so we propose a\nnovel alternative. In our model, besides matching T and S predictions we have a\ncombinatorial mechanism to inject layer-level supervision from T to S. In this\npaper, we target low-resource settings and evaluate our translation engines for\nPortuguese--English, Turkish--English, and English--German directions. Students\ntrained using our technique have 50% fewer parameters and can still deliver\ncomparable results to those of 12-layer teachers.", "published": "2020-10-06 21:08:16", "link": "http://arxiv.org/abs/2010.03034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Recognizing Textual Entailment as an NLP Evaluation", "abstract": "Recognizing Textual Entailment (RTE) was proposed as a unified evaluation\nframework to compare semantic understanding of different NLP systems. In this\nsurvey paper, we provide an overview of different approaches for evaluating and\nunderstanding the reasoning capabilities of NLP systems. We then focus our\ndiscussion on RTE by highlighting prominent RTE datasets as well as advances in\nRTE dataset that focus on specific linguistic phenomena that can be used to\nevaluate NLP systems on a fine-grained level. We conclude by arguing that when\nevaluating NLP systems, the community should utilize newly introduced RTE\ndatasets that focus on specific linguistic phenomena.", "published": "2020-10-06 22:23:00", "link": "http://arxiv.org/abs/2010.03061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories", "abstract": "Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature.", "published": "2020-10-06 22:33:58", "link": "http://arxiv.org/abs/2010.03065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient One-Pass End-to-End Entity Linking for Questions", "abstract": "We present ELQ, a fast end-to-end entity linking model for questions, which\nuses a biencoder to jointly perform mention detection and linking in one pass.\nEvaluated on WebQSP and GraphQuestions with extended annotations that cover\nmultiple entities per question, ELQ outperforms the previous state of the art\nby a large margin of +12.7% and +19.6% F1, respectively. With a very fast\ninference time (1.57 examples/s on a single CPU), ELQ can be useful for\ndownstream question answering systems. In a proof-of-concept experiment, we\ndemonstrate that using ELQ significantly improves the downstream QA performance\nof GraphRetriever (arXiv:1911.03868). Code and data available at\nhttps://github.com/facebookresearch/BLINK/tree/master/elq", "published": "2020-10-06 01:14:10", "link": "http://arxiv.org/abs/2010.02413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Inference For Neural Machine Translation", "abstract": "Large Transformer models have achieved state-of-the-art results in neural\nmachine translation and have become standard in the field. In this work, we\nlook for the optimal combination of known techniques to optimize inference\nspeed without sacrificing translation quality. We conduct an empirical study\nthat stacks various approaches and demonstrates that combination of replacing\ndecoder self-attention with simplified recurrent units, adopting a deep encoder\nand a shallow decoder architecture and multi-head attention pruning can achieve\nup to 109% and 84% speedup on CPU and GPU respectively and reduce the number of\nparameters by 25% while maintaining the same translation quality in terms of\nBLEU.", "published": "2020-10-06 01:21:11", "link": "http://arxiv.org/abs/2010.02416v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on\n  Chest X-rays", "abstract": "Automatic medical image report generation has drawn growing attention due to\nits potential to alleviate radiologists' workload. Existing work on report\ngeneration often trains encoder-decoder networks to generate complete reports.\nHowever, such models are affected by data bias (e.g.~label imbalance) and face\ncommon issues inherent in text generation models (e.g.~repetition). In this\nwork, we focus on reporting abnormal findings on radiology images; instead of\ntraining on complete radiology reports, we propose a method to identify\nabnormal findings from the reports in addition to grouping them with\nunsupervised clustering and minimal rules. We formulate the task as cross-modal\nretrieval and propose Conditional Visual-Semantic Embeddings to align images\nand fine-grained abnormal findings in a joint embedding space. We demonstrate\nthat our method is able to retrieve abnormal findings and outperforms existing\ngeneration models on both clinical correctness and text generation metrics.", "published": "2020-10-06 04:18:18", "link": "http://arxiv.org/abs/2010.02467v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dynamic Semantic Matching and Aggregation Network for Few-shot Intent\n  Detection", "abstract": "Few-shot Intent Detection is challenging due to the scarcity of available\nannotated utterances. Although recent works demonstrate that multi-level\nmatching plays an important role in transferring learned knowledge from seen\ntraining classes to novel testing classes, they rely on a static similarity\nmeasure and overly fine-grained matching components. These limitations inhibit\ngeneralizing capability towards Generalized Few-shot Learning settings where\nboth seen and novel classes are co-existent. In this paper, we propose a novel\nSemantic Matching and Aggregation Network where semantic components are\ndistilled from utterances via multi-head self-attention with additional dynamic\nregularization constraints. These semantic components capture high-level\ninformation, resulting in more effective matching between instances. Our\nmulti-perspective matching method provides a comprehensive matching measure to\nenhance representations of both labeled and unlabeled instances. We also\npropose a more challenging evaluation setting that considers classification on\nthe joint all-class label space. Extensive experimental results demonstrate the\neffectiveness of our method. Our code and data are publicly available.", "published": "2020-10-06 05:16:38", "link": "http://arxiv.org/abs/2010.02481v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Meta Lifelong-Learning with Limited Memory", "abstract": "Current natural language processing models work well on a single task, yet\nthey often fail to continuously learn new tasks without forgetting previous\nones as they are re-trained throughout their lifetime, a challenge known as\nlifelong learning. State-of-the-art lifelong language learning methods store\npast examples in episodic memory and replay them at both training and inference\ntime. However, as we show later in our experiments, there are three significant\nimpediments: (1) needing unrealistically large memory module to achieve good\nperformance, (2) suffering from negative transfer, (3) requiring multiple local\nadaptation steps for each test example that significantly slows down the\ninference speed. In this paper, we identify three common principles of lifelong\nlearning methods and propose an efficient meta-lifelong framework that combines\nthem in a synergistic fashion. To achieve sample efficiency, our method trains\nthe model in a manner that it learns a better initialization for local\nadaptation. Extensive experiments on text classification and question answering\nbenchmarks demonstrate the effectiveness of our framework by achieving\nstate-of-the-art performance using merely 1% memory size and narrowing the gap\nwith multi-task learning. We further show that our method alleviates both\ncatastrophic forgetting and negative transfer at the same time.", "published": "2020-10-06 06:08:07", "link": "http://arxiv.org/abs/2010.02500v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating African-American Vernacular English in Transformer-Based\n  Text Generation", "abstract": "The growth of social media has encouraged the written use of African American\nVernacular English (AAVE), which has traditionally been used only in oral\ncontexts. However, NLP models have historically been developed using dominant\nEnglish varieties, such as Standard American English (SAE), due to text corpora\navailability. We investigate the performance of GPT-2 on AAVE text by creating\na dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating\nsyntactic structure and AAVE- or SAE-specific language for each pair. We\nevaluate each sample and its GPT-2 generated text with pretrained sentiment\nclassifiers and find that while AAVE text results in more classifications of\nnegative sentiment than SAE, the use of GPT-2 generally increases occurrences\nof positive sentiment for both. Additionally, we conduct human evaluation of\nAAVE and SAE text generated with GPT-2 to compare contextual rigor and overall\nquality.", "published": "2020-10-06 06:27:02", "link": "http://arxiv.org/abs/2010.02510v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Multilingual Neural Machine Translation", "abstract": "While monolingual data has been shown to be useful in improving bilingual\nneural machine translation (NMT), effectively and efficiently leveraging\nmonolingual data for Multilingual NMT (MNMT) systems is a less explored area.\nIn this work, we propose a multi-task learning (MTL) framework that jointly\ntrains the model with the translation task on bitext data and two denoising\ntasks on the monolingual data. We conduct extensive empirical studies on MNMT\nsystems with 10 language pairs from WMT datasets. We show that the proposed\napproach can effectively improve the translation quality for both high-resource\nand low-resource languages with large margin, achieving significantly better\nresults than the individual bilingual models. We also demonstrate the efficacy\nof the proposed approach in the zero-shot setup for language pairs without\nbitext training data. Furthermore, we show the effectiveness of MTL over\npre-training approaches for both NMT and cross-lingual transfer learning NLU\ntasks; the proposed approach outperforms massive scale models trained on single\ntask.", "published": "2020-10-06 06:54:12", "link": "http://arxiv.org/abs/2010.02523v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding the Evidence: Localization-aware Answer Prediction for Text\n  Visual Question Answering", "abstract": "Image text carries essential information to understand the scene and perform\nreasoning. Text-based visual question answering (text VQA) task focuses on\nvisual questions that require reading text in images. Existing text VQA systems\ngenerate an answer by selecting from optical character recognition (OCR) texts\nor a fixed vocabulary. Positional information of text is underused and there is\na lack of evidence for the generated answer. As such, this paper proposes a\nlocalization-aware answer prediction network (LaAP-Net) to address this\nchallenge. Our LaAP-Net not only generates the answer to the question but also\npredicts a bounding box as evidence of the generated answer. Moreover, a\ncontext-enriched OCR representation (COR) for multimodal fusion is proposed to\nfacilitate the localization task. Our proposed LaAP-Net outperforms existing\napproaches on three benchmark datasets for the text VQA task by a noticeable\nmargin.", "published": "2020-10-06 09:46:20", "link": "http://arxiv.org/abs/2010.02582v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Knowing What You Know: Calibrating Dialogue Belief State Distributions\n  via Ensembles", "abstract": "The ability to accurately track what happens during a conversation is\nessential for the performance of a dialogue system. Current state-of-the-art\nmulti-domain dialogue state trackers achieve just over 55% accuracy on the\ncurrent go-to benchmark, which means that in almost every second dialogue turn\nthey place full confidence in an incorrect dialogue state. Belief trackers, on\nthe other hand, maintain a distribution over possible dialogue states. However,\nthey lack in performance compared to dialogue state trackers, and do not\nproduce well calibrated distributions. In this work we present state-of-the-art\nperformance in calibration for multi-domain dialogue belief trackers using a\ncalibrated ensemble of models. Our resulting dialogue belief tracker also\noutperforms previous dialogue belief tracking models in terms of accuracy.", "published": "2020-10-06 09:51:04", "link": "http://arxiv.org/abs/2010.02586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Embedding Words in Non-Vector Space with Unsupervised Graph Learning", "abstract": "It has become a de-facto standard to represent words as elements of a vector\nspace (word2vec, GloVe). While this approach is convenient, it is unnatural for\nlanguage: words form a graph with a latent hierarchical structure, and this\nstructure has to be revealed and encoded by word embeddings. We introduce\nGraphGlove: unsupervised graph word representations which are learned\nend-to-end. In our setting, each word is a node in a weighted graph and the\ndistance between words is the shortest path distance between the corresponding\nnodes. We adopt a recent method learning a representation of data in the form\nof a differentiable weighted graph and use it to modify the GloVe training\nalgorithm. We show that our graph-based representations substantially\noutperform vector-based methods on word similarity and analogy tasks. Our\nanalysis reveals that the structure of the learned graphs is hierarchical and\nsimilar to that of WordNet, the geometry is highly non-trivial and contains\nsubgraphs with different local topology.", "published": "2020-10-06 10:17:49", "link": "http://arxiv.org/abs/2010.02598v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Converting the Point of View of Messages Spoken to Virtual Assistants", "abstract": "Virtual Assistants can be quite literal at times. If the user says \"tell Bob\nI love him,\" most virtual assistants will extract the message \"I love him\" and\nsend it to the user's contact named Bob, rather than properly converting the\nmessage to \"I love you.\" We designed a system to allow virtual assistants to\ntake a voice message from one user, convert the point of view of the message,\nand then deliver the result to its target user. We developed a rule-based\nmodel, which integrates a linear text classification model, part-of-speech\ntagging, and constituency parsing with rule-based transformation methods. We\nalso investigated Neural Machine Translation (NMT) approaches, including LSTMs,\nCopyNet, and T5. We explored 5 metrics to gauge both naturalness and\nfaithfulness automatically, and we chose to use BLEU plus METEOR for\nfaithfulness and relative perplexity using a separately trained language model\n(GPT) for naturalness. Transformer-Copynet and T5 performed similarly on\nfaithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a\nMETEOR score of 83.0. CopyNet was the most natural, with a relative perplexity\nof 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly\nreleased our dataset, which is composed of 46,565 crowd-sourced samples.", "published": "2020-10-06 10:19:39", "link": "http://arxiv.org/abs/2010.02600v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Semantics and Data-Driven Path Representation for Knowledge Graph\n  Inference", "abstract": "Inference on a large-scale knowledge graph (KG) is of great importance for KG\napplications like question answering. The path-based reasoning models can\nleverage much information over paths other than pure triples in the KG, which\nface several challenges: all the existing path-based methods are data-driven,\nlacking explainability for path representation. Besides, some methods either\nconsider only relational paths or ignore the heterogeneity between entities and\nrelations both contained in paths, which cannot capture the rich semantics of\npaths well. To address the above challenges, in this work, we propose a novel\njoint semantics and data-driven path representation that balances\nexplainability and generalization in the framework of KG embedding. More\nspecifically, we inject horn rules to obtain the condensed paths by the\ntransparent and explainable path composition procedure. The entity converter is\ndesigned to transform the entities along paths into the representations in the\nsemantic level similar to relations for reducing the heterogeneity between\nentities and relations, in which the KGs both with and without type information\nare considered. Our proposed model is evaluated on two classes of tasks: link\nprediction and path query answering task. The experimental results show that it\nhas a significant performance gain over several different state-of-the-art\nbaselines.", "published": "2020-10-06 10:24:45", "link": "http://arxiv.org/abs/2010.02602v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DaNetQA: a yes/no Question Answering Dataset for the Russian Language", "abstract": "DaNetQA, a new question-answering corpus, follows (Clark et. al, 2019)\ndesign: it comprises natural yes/no questions. Each question is paired with a\nparagraph from Wikipedia and an answer, derived from the paragraph. The task is\nto take both the question and a paragraph as input and come up with a yes/no\nanswer, i.e. to produce a binary output. In this paper, we present a\nreproducible approach to DaNetQA creation and investigate transfer learning\nmethods for task and language transferring. For task transferring we leverage\nthree similar sentence modelling tasks: 1) a corpus of paraphrases,\nParaphraser, 2) an NLI task, for which we use the Russian part of XNLI, 3)\nanother question answering task, SberQUAD. For language transferring we use\nEnglish to Russian translation together with multilingual language fine-tuning.", "published": "2020-10-06 10:30:48", "link": "http://arxiv.org/abs/2010.02605v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Interplay Between Fine-tuning and Sentence-level Probing for\n  Linguistic Knowledge in Pre-trained Transformers", "abstract": "Fine-tuning pre-trained contextualized embedding models has become an\nintegral part of the NLP pipeline. At the same time, probing has emerged as a\nway to investigate the linguistic knowledge captured by pre-trained models.\nVery little is, however, understood about how fine-tuning affects the\nrepresentations of pre-trained models and thereby the linguistic knowledge they\nencode. This paper contributes towards closing this gap. We study three\ndifferent pre-trained models: BERT, RoBERTa, and ALBERT, and investigate\nthrough sentence-level probing how fine-tuning affects their representations.\nWe find that for some probing tasks fine-tuning leads to substantial changes in\naccuracy, possibly suggesting that fine-tuning introduces or even removes\nlinguistic knowledge from a pre-trained model. These changes, however, vary\ngreatly across different models, fine-tuning and probing tasks. Our analysis\nreveals that while fine-tuning indeed changes the representations of a\npre-trained model and these changes are typically larger for higher layers,\nonly in very few cases, fine-tuning has a positive effect on probing accuracy\nthat is larger than just using the pre-trained model with a strong pooling\nmethod. Based on our findings, we argue that both positive and negative effects\nof fine-tuning on probing require a careful interpretation.", "published": "2020-10-06 10:54:00", "link": "http://arxiv.org/abs/2010.02616v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Sub-Layer Functionalities of Transformer Decoder", "abstract": "There have been significant efforts to interpret the encoder of\nTransformer-based encoder-decoder architectures for neural machine translation\n(NMT); meanwhile, the decoder remains largely unexamined despite its critical\nrole. During translation, the decoder must predict output tokens by considering\nboth the source-language text from the encoder and the target-language prefix\nproduced in previous steps. In this work, we study how Transformer-based\ndecoders leverage information from the source and target languages --\ndeveloping a universal probe task to assess how information is propagated\nthrough each module of each decoder layer. We perform extensive experiments on\nthree major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis\nprovides insight on when and where decoders leverage different sources. Based\non these insights, we demonstrate that the residual feed-forward module in each\nTransformer decoder layer can be dropped with minimal loss of performance -- a\nsignificant reduction in computation and number of parameters, and consequently\na significant boost to both training and inference speed.", "published": "2020-10-06 11:50:54", "link": "http://arxiv.org/abs/2010.02648v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Behavioral Hypotheses for Query Generation", "abstract": "Generative neural networks have been shown effective on query suggestion.\nCommonly posed as a conditional generation problem, the task aims to leverage\nearlier inputs from users in a search session to predict queries that they will\nlikely issue at a later time. User inputs come in various forms such as\nquerying and clicking, each of which can imply different semantic signals\nchanneled through the corresponding behavioral patterns. This paper induces\nthese behavioral biases as hypotheses for query generation, where a generic\nencoder-decoder Transformer framework is presented to aggregate arbitrary\nhypotheses of choice. Our experimental results show that the proposed approach\nleads to significant improvements on top-$k$ word error rate and Bert F1 Score\ncompared to a recent BART model.", "published": "2020-10-06 12:38:02", "link": "http://arxiv.org/abs/2010.02667v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Multi-Task Incremental Learning Framework with Category Name Embedding\n  for Aspect-Category Sentiment Analysis", "abstract": "(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and\ntargeted aspect-category sentiment analysis (TACSA), aims at identifying\nsentiment polarity on predefined categories. Incremental learning on new\ncategories is necessary for (T)ACSA real applications. Though current\nmulti-task learning models achieve good performance in (T)ACSA tasks, they\nsuffer from catastrophic forgetting problems in (T)ACSA incremental learning\ntasks. In this paper, to make multi-task learning feasible for incremental\nlearning, we proposed Category Name Embedding network (CNE-net). We set both\nencoder and decoder shared among all categories to weaken the catastrophic\nforgetting problem. Besides the origin input sentence, we applied another input\nfeature, i.e., category name, for task discrimination. Our model achieved\nstate-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a\ndataset for (T)ACSA incremental learning and achieved the best performance\ncompared with other strong baselines.", "published": "2020-10-06 14:52:54", "link": "http://arxiv.org/abs/2010.02784v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning to Ignore: Long Document Coreference with Bounded Memory Neural\n  Networks", "abstract": "Long document coreference resolution remains a challenging task due to the\nlarge memory and runtime requirements of current models. Recent work doing\nincremental coreference resolution using just the global representation of\nentities shows practical benefits but requires keeping all entities in memory,\nwhich can be impractical for long documents. We argue that keeping all entities\nin memory is unnecessary, and we propose a memory-augmented neural network that\ntracks only a small bounded number of entities at a time, thus guaranteeing a\nlinear runtime in length of document. We show that (a) the model remains\ncompetitive with models with high memory and computational requirements on\nOntoNotes and LitBank, and (b) the model learns an efficient memory management\nstrategy easily outperforming a rule-based strategy.", "published": "2020-10-06 15:16:31", "link": "http://arxiv.org/abs/2010.02807v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech\n  to Standard German Text Corpus", "abstract": "We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss\nGerman speech to Standard German text corpus. This first version of the corpus\nis based on publicly available data of the Bernese cantonal parliament and\nconsists of 293 hours of data. It was created using a novel forced sentence\nalignment procedure and an alignment quality estimator, which can be used to\ntrade off corpus size and quality. We trained Automatic Speech Recognition\n(ASR) models as baselines on different subsets of the data and achieved a Word\nError Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The\ncorpus is freely available for download.", "published": "2020-10-06 15:18:21", "link": "http://arxiv.org/abs/2010.02810v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Evaluation for Text-to-SQL with Distilled Test Suites", "abstract": "We propose test suite accuracy to approximate semantic accuracy for\nText-to-SQL models. Our method distills a small test suite of databases that\nachieves high code coverage for the gold query from a large number of randomly\ngenerated databases. At evaluation time, it computes the denotation accuracy of\nthe predicted queries on the distilled test suite, hence calculating a tight\nupper-bound for semantic accuracy efficiently. We use our proposed method to\nevaluate 21 models submitted to the Spider leader board and manually verify\nthat our method is always correct on 100 examples. In contrast, the current\nSpider metric leads to a 2.5% false negative rate on average and 8.1% in the\nworst case, indicating that test suite accuracy is needed. Our implementation,\nalong with distilled test suites for eleven Text-to-SQL datasets, is publicly\navailable.", "published": "2020-10-06 16:04:12", "link": "http://arxiv.org/abs/2010.02840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plug and Play Autoencoders for Conditional Text Generation", "abstract": "Text autoencoders are commonly used for conditional generation tasks such as\nstyle transfer. We propose methods which are plug and play, where any\npretrained autoencoder can be used, and only require learning a mapping within\nthe autoencoder's embedding space, training embedding-to-embedding (Emb2Emb).\nThis reduces the need for labeled training data for the task and makes the\ntraining procedure more efficient. Crucial to the success of this method is a\nloss term for keeping the mapped embedding on the manifold of the autoencoder\nand a mapping which is trained to navigate the manifold by learning offset\nvectors. Evaluations on style transfer tasks both with and without\nsequence-to-sequence supervision show that our method performs better than or\ncomparable to strong baselines while being up to four times faster.", "published": "2020-10-06 19:18:06", "link": "http://arxiv.org/abs/2010.02983v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Negative Interference in Multilingual Models: Findings and A\n  Meta-Learning Treatment", "abstract": "Modern multilingual models are trained on concatenated text from multiple\nlanguages in hopes of conferring benefits to each (positive transfer), with the\nmost pronounced benefits accruing to low-resource languages. However, recent\nwork has shown that this approach can degrade performance on high-resource\nlanguages, a phenomenon known as negative interference. In this paper, we\npresent the first systematic study of negative interference. We show that,\ncontrary to previous belief, negative interference also impacts low-resource\nlanguages. While parameters are maximally shared to learn language-universal\nstructures, we demonstrate that language-specific parameters do exist in\nmultilingual models and they are a potential cause of negative interference.\nMotivated by these observations, we also present a meta-learning algorithm that\nobtains better cross-lingual transferability and alleviates negative\ninterference, by adding language-specific layers as meta-parameters and\ntraining them in a manner that explicitly improves shared layers'\ngeneralization on all languages. Overall, our results show that negative\ninterference is more common than previously known, suggesting new directions\nfor improving multilingual representations.", "published": "2020-10-06 20:48:58", "link": "http://arxiv.org/abs/2010.03017v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond [CLS] through Ranking by Generation", "abstract": "Generative models for Information Retrieval, where ranking of documents is\nviewed as the task of generating a query from a document's language model, were\nvery successful in various IR tasks in the past. However, with the advent of\nmodern deep neural networks, attention has shifted to discriminative ranking\nfunctions that model the semantic similarity of documents and queries instead.\nRecently, deep generative models such as GPT2 and BART have been shown to be\nexcellent text generators, but their effectiveness as rankers have not been\ndemonstrated yet. In this work, we revisit the generative framework for\ninformation retrieval and show that our generative approaches are as effective\nas state-of-the-art semantic similarity-based discriminative models for the\nanswer selection task. Additionally, we demonstrate the effectiveness of\nunlikelihood losses for IR.", "published": "2020-10-06 22:56:31", "link": "http://arxiv.org/abs/2010.03073v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LSTMs Compose (and Learn) Bottom-Up", "abstract": "Recent work in NLP shows that LSTM language models capture hierarchical\nstructure in language data. In contrast to existing work, we consider the\n\\textit{learning} process that leads to their compositional behavior. For a\ncloser look at how an LSTM's sequential representations are composed\nhierarchically, we present a related measure of Decompositional Interdependence\n(DI) between word meanings in an LSTM, based on their gate interactions. We\nconnect this measure to syntax with experiments on English language data, where\nDI is higher on pairs of words with lower syntactic distance. To explore the\ninductive biases that cause these compositional representations to arise during\ntraining, we conduct simple experiments on synthetic data. These synthetic\nexperiments support a specific hypothesis about how hierarchical structures are\ndiscovered over the course of training: that LSTM constituent representations\nare learned bottom-up, relying on effective representations of their shorter\nchildren, rather than learning the longer-range relations independently from\nchildren.", "published": "2020-10-06 13:00:32", "link": "http://arxiv.org/abs/2010.04650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for\n  Low-Latency Inference in NLP Applications", "abstract": "Deep neural networks have become the standard approach to building reliable\nNatural Language Processing (NLP) applications, ranging from Neural Machine\nTranslation (NMT) to dialogue systems. However, improving accuracy by\nincreasing the model size requires a large number of hardware computations,\nwhich can slow down NLP applications significantly at inference time. To\naddress this issue, we propose a novel vector-vector-matrix architecture\n(VVMA), which greatly reduces the latency at inference time for NMT. This\narchitecture takes advantage of specialized hardware that has low-latency\nvector-vector operations and higher-latency vector-matrix operations. It also\nreduces the number of parameters and FLOPs for virtually all models that rely\non efficient matrix multipliers without significantly impacting accuracy. We\npresent empirical results suggesting that our framework can reduce the latency\nof sequence-to-sequence and Transformer models used for NMT by a factor of\nfour. Finally, we show evidence suggesting that our VVMA extends to other\ndomains, and we discuss novel hardware for its efficient use.", "published": "2020-10-06 16:54:08", "link": "http://arxiv.org/abs/2010.08412v1", "categories": ["cs.CL", "cs.AR"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training", "abstract": "To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates quality assessment on pseudo labels by (meta) learning from the\nsuccessful and failed attempts on Relation Classification Network as an\nadditional meta-objective. To reduce the influence of noisy pseudo labels,\nMetaSRE adopts a pseudo label selection and exploitation scheme which assesses\npseudo label quality on unlabeled samples and only exploits high-quality pseudo\nlabels in a self-training fashion to incrementally augment labeled samples for\nboth robustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.", "published": "2020-10-06 03:54:11", "link": "http://arxiv.org/abs/2010.16410v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Grammatical Error Correction", "abstract": "Recent works in Grammatical Error Correction (GEC) have leveraged the\nprogress in Neural Machine Translation (NMT), to learn rewrites from parallel\ncorpora of grammatically incorrect and corrected sentences, achieving\nstate-of-the-art results. At the same time, Generative Adversarial Networks\n(GANs) have been successful in generating realistic texts across many different\ntasks by learning to directly minimize the difference between human-generated\nand synthetic text. In this work, we present an adversarial learning approach\nto GEC, using the generator-discriminator framework. The generator is a\nTransformer model, trained to produce grammatically correct sentences given\ngrammatically incorrect ones. The discriminator is a sentence-pair\nclassification model, trained to judge a given pair of grammatically\nincorrect-correct sentences on the quality of grammatical correction. We\npre-train both the discriminator and the generator on parallel texts and then\nfine-tune them further using a policy gradient method that assigns high rewards\nto sentences which could be true corrections of the grammatically incorrect\ntext. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that\nAdversarial-GEC can achieve competitive GEC quality compared to NMT-based\nbaselines.", "published": "2020-10-06 00:31:33", "link": "http://arxiv.org/abs/2010.02407v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ERFit: Entropic Regression Fit Matlab Package, for Data-Driven System\n  Identification of Underlying Dynamic Equations", "abstract": "Data-driven sparse system identification becomes the general framework for a\nwide range of problems in science and engineering. It is a problem of growing\nimportance in applied machine learning and artificial intelligence algorithms.\nIn this work, we developed the Entropic Regression Software Package (ERFit), a\nMATLAB package for sparse system identification using the entropic regression\nmethod. The code requires minimal supervision, with a wide range of options\nthat make it adapt easily to different problems in science and engineering. The\nERFit is available at https://github.com/almomaa/ERFit-Package", "published": "2020-10-06 01:07:15", "link": "http://arxiv.org/abs/2010.02411v1", "categories": ["math.DS", "cs.CL", "stat.CO", "stat.ML"], "primary_category": "math.DS"}
{"title": "The Sequence-to-Sequence Baseline for the Voice Conversion Challenge\n  2020: Cascading ASR and TTS", "abstract": "This paper presents the sequence-to-sequence (seq2seq) baseline system for\nthe voice conversion challenge (VCC) 2020. We consider a naive approach for\nvoice conversion (VC), which is to first transcribe the input speech with an\nautomatic speech recognition (ASR) model, followed using the transcriptions to\ngenerate the voice of the target with a text-to-speech (TTS) model. We revisit\nthis method under a sequence-to-sequence (seq2seq) framework by utilizing\nESPnet, an open-source end-to-end speech processing toolkit, and the many\nwell-configured pretrained models provided by the community. Official\nevaluation results show that our system comes out top among the participating\nsystems in terms of conversion similarity, demonstrating the promising ability\nof seq2seq models to convert speaker identity. The implementation is made\nopen-source at: https://github.com/espnet/espnet/tree/master/egs/vcc20.", "published": "2020-10-06 02:27:38", "link": "http://arxiv.org/abs/2010.02434v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Identifying Spurious Correlations for Robust Text Classification", "abstract": "The predictions of text classifiers are often driven by spurious correlations\n-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\nthough the term itself does not semantically convey a positive sentiment. In\nthis paper, we propose a method to distinguish spurious and genuine\ncorrelations in text classification. We treat this as a supervised\nclassification problem, using features derived from treatment effect estimators\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\nnature of these features and their small dimensionality, we find that the\napproach works well even with limited training examples, and that it is\npossible to transport the word classifier to new domains. Experiments on four\ndatasets (sentiment classification and toxicity detection) suggest that using\nthis approach to inform feature selection also leads to more robust\nclassification, as measured by improved worst-case accuracy on the samples\naffected by spurious correlations.", "published": "2020-10-06 03:49:22", "link": "http://arxiv.org/abs/2010.02458v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Joint Turn and Dialogue level User Satisfaction Estimation on\n  Multi-Domain Conversations", "abstract": "Dialogue level quality estimation is vital for optimizing data driven\ndialogue management. Current automated methods to estimate turn and dialogue\nlevel user satisfaction employ hand-crafted features and rely on complex\nannotation schemes, which reduce the generalizability of the trained models. We\npropose a novel user satisfaction estimation approach which minimizes an\nadaptive multi-task loss function in order to jointly predict turn-level\nResponse Quality labels provided by experts and explicit dialogue-level ratings\nprovided by end users. The proposed BiLSTM based deep neural net model\nautomatically weighs each turn's contribution towards the estimated\ndialogue-level rating, implicitly encodes temporal dependencies, and removes\nthe need to hand-craft features.\n  On dialogues sampled from 28 Alexa domains, two dialogue systems and three\nuser groups, the joint dialogue-level satisfaction estimation model achieved up\nto an absolute 27% (0.43->0.70) and 7% (0.63->0.70) improvement in linear\ncorrelation performance over baseline deep neural net and benchmark Gradient\nboosting regression models, respectively.", "published": "2020-10-06 05:53:13", "link": "http://arxiv.org/abs/2010.02495v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SHERLock: Self-Supervised Hierarchical Event Representation Learning", "abstract": "Temporal event representations are an essential aspect of learning among\nhumans. They allow for succinct encoding of the experiences we have through a\nvariety of sensory inputs. Also, they are believed to be arranged\nhierarchically, allowing for an efficient representation of complex\nlong-horizon experiences. Additionally, these representations are acquired in a\nself-supervised manner. Analogously, here we propose a model that learns\ntemporal representations from long-horizon visual demonstration data and\nassociated textual descriptions, without explicit temporal supervision. Our\nmethod produces a hierarchy of representations that align more closely with\nground-truth human-annotated events (+15.3) than state-of-the-art unsupervised\nbaselines.\n  Our results are comparable to heavily-supervised baselines in complex visual\ndomains such as Chess Openings, YouCook2 and TutorialVQA datasets. Finally, we\nperform ablation studies illustrating the robustness of our approach. We\nrelease our code and demo visualizations in the Supplementary Material.", "published": "2020-10-06 09:04:01", "link": "http://arxiv.org/abs/2010.02556v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Multilingual Amazon Reviews Corpus", "abstract": "We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale\ncollection of Amazon reviews for multilingual text classification. The corpus\ncontains reviews in English, Japanese, German, French, Spanish, and Chinese,\nwhich were collected between 2015 and 2019. Each record in the dataset contains\nthe review text, the review title, the star rating, an anonymized reviewer ID,\nan anonymized product ID, and the coarse-grained product category (e.g.,\n'books', 'appliances', etc.) The corpus is balanced across the 5 possible star\nratings, so each rating constitutes 20% of the reviews in each language. For\neach language, there are 200,000, 5,000, and 5,000 reviews in the training,\ndevelopment, and test sets, respectively. We report baseline results for\nsupervised text classification and zero-shot cross-lingual transfer learning by\nfine-tuning a multilingual BERT model on reviews data. We propose the use of\nmean absolute error (MAE) instead of classification accuracy for this task,\nsince MAE accounts for the ordinal nature of the ratings.", "published": "2020-10-06 09:34:01", "link": "http://arxiv.org/abs/2010.02573v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantically Driven Sentence Fusion: Modeling and Evaluation", "abstract": "Sentence fusion is the task of joining related sentences into coherent text.\nCurrent training and evaluation schemes for this task are based on single\nreference ground-truths and do not account for valid fusion variants. We show\nthat this hinders models from robustly capturing the semantic relationship\nbetween input sentences. To alleviate this, we present an approach in which\nground-truth solutions are automatically expanded into multiple references via\ncurated equivalence classes of connective phrases. We apply this method to a\nlarge-scale dataset and use the augmented dataset for both model training and\nevaluation. To improve the learning of semantic representation using multiple\nreferences, we enrich the model with auxiliary discourse classification tasks\nunder a multi-tasking framework. Our experiments highlight the improvements of\nour approach over state-of-the-art models.", "published": "2020-10-06 10:06:01", "link": "http://arxiv.org/abs/2010.02592v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Poison Attacks against Text Datasets with Conditional Adversarially\n  Regularized Autoencoder", "abstract": "This paper demonstrates a fatal vulnerability in natural language inference\n(NLI) and text classification systems. More concretely, we present a 'backdoor\npoisoning' attack on NLP models. Our poisoning attack utilizes conditional\nadversarially regularized autoencoder (CARA) to generate poisoned training\nsamples by poison injection in latent space. Just by adding 1% poisoned data,\nour experiments show that a victim BERT finetuned classifier's predictions can\nbe steered to the poison target class with success rates of >80% when the input\nhypothesis is injected with the poison signature, demonstrating that NLI and\ntext classification systems face a huge security risk.", "published": "2020-10-06 13:03:49", "link": "http://arxiv.org/abs/2010.02684v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection\n  and Slot Filling", "abstract": "Slot filling and intent detection are two main tasks in spoken language\nunderstanding (SLU) system. In this paper, we propose a novel\nnon-autoregressive model named SlotRefine for joint intent detection and slot\nfilling. Besides, we design a novel two-pass iteration mechanism to handle the\nuncoordinated slots problem caused by conditional independence of\nnon-autoregressive model. Experiments demonstrate that our model significantly\noutperforms previous models in slot filling task, while considerably speeding\nup the decoding (up to X 10.77). In-depth analyses show that 1) pretraining\nschemes could further enhance our model; 2) two-pass mechanism indeed remedy\nthe uncoordinated slots.", "published": "2020-10-06 13:16:53", "link": "http://arxiv.org/abs/2010.02693v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Mask Generator: Learning to Generate Adaptive Word Maskings for\n  Language Model Adaptation", "abstract": "We propose a method to automatically generate a domain- and task-adaptive\nmaskings of the given text for self-supervised pre-training, such that we can\neffectively adapt the language model to a particular target task (e.g. question\nanswering). Specifically, we present a novel reinforcement learning-based\nframework which learns the masking policy, such that using the generated masks\nfor further pre-training of the target language model helps improve task\nperformance on unseen texts. We use off-policy actor-critic with entropy\nregularization and experience replay for reinforcement learning, and propose a\nTransformer-based policy network that can consider the relative importance of\nwords in a given text. We validate our Neural Mask Generator (NMG) on several\nquestion answering and text classification datasets using BERT and DistilBERT\nas the language models, on which it outperforms rule-based masking strategies,\nby automatically learning optimal adaptive maskings.", "published": "2020-10-06 13:27:01", "link": "http://arxiv.org/abs/2010.02705v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tackling the Low-resource Challenge for Canonical Segmentation", "abstract": "Canonical morphological segmentation consists of dividing words into their\nstandardized morphemes. Here, we are interested in approaches for the task when\ntraining data is limited. We compare model performance in a simulated\nlow-resource setting for the high-resource languages German, English, and\nIndonesian to experiments on new datasets for the truly low-resource languages\nPopoluca and Tepehua. We explore two new models for the task, borrowing from\nthe closely related area of morphological generation: an LSTM pointer-generator\nand a sequence-to-sequence model with hard monotonic attention trained with\nimitation learning. We find that, in the low-resource setting, the novel\napproaches outperform existing ones on all languages by up to 11.4% accuracy.\nHowever, while accuracy in emulated low-resource scenarios is over 50% for all\nlanguages, for the truly low-resource languages Popoluca and Tepehua, our best\nmodel only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude\nthat canonical segmentation is still a challenging task for low-resource\nlanguages.", "published": "2020-10-06 15:15:05", "link": "http://arxiv.org/abs/2010.02804v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Textual Supervision for Visually Grounded Spoken Language Understanding", "abstract": "Visually-grounded models of spoken language understanding extract semantic\ninformation directly from speech, without relying on transcriptions. This is\nuseful for low-resource languages, where transcriptions can be expensive or\nimpossible to obtain. Recent work showed that these models can be improved if\ntranscriptions are available at training time. However, it is not clear how an\nend-to-end approach compares to a traditional pipeline-based approach when one\nhas access to transcriptions. Comparing different strategies, we find that the\npipeline approach works better when enough text is available. With low-resource\nlanguages in mind, we also show that translations can be effectively used in\nplace of transcriptions but more data is needed to obtain similar results.", "published": "2020-10-06 15:16:23", "link": "http://arxiv.org/abs/2010.02806v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PRover: Proof Generation for Interpretable Reasoning over Rules", "abstract": "Recent work by Clark et al. (2020) shows that transformers can act as 'soft\ntheorem provers' by answering questions over explicitly provided knowledge in\nnatural language. In our work, we take a step closer to emulating formal\ntheorem provers, by proposing PROVER, an interpretable transformer-based model\nthat jointly answers binary questions over rule-bases and generates the\ncorresponding proofs. Our model learns to predict nodes and edges corresponding\nto proof graphs in an efficient constrained training paradigm. During\ninference, a valid proof, satisfying a set of global constraints is generated.\nWe conduct experiments on synthetic, hand-authored, and human-paraphrased\nrule-bases to show promising results for QA and proof generation, with strong\ngeneralization performance. First, PROVER generates proofs with an accuracy of\n87%, while retaining or improving performance on the QA task, compared to\nRuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained\non questions requiring lower depths of reasoning, it generalizes significantly\nbetter to higher depths (up to 15% improvement). Third, PROVER obtains near\nperfect QA accuracy of 98% using only 40% of the training data. However,\ngenerating proofs for questions requiring higher depths of reasoning becomes\nchallenging, and the accuracy drops to 65% for 'depth 5', indicating\nsignificant scope for future work. Our code and models are publicly available\nat https://github.com/swarnaHub/PRover", "published": "2020-10-06 15:47:53", "link": "http://arxiv.org/abs/2010.02830v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustness and Reliability of Gender Bias Assessment in Word Embeddings:\n  The Role of Base Pairs", "abstract": "It has been shown that word embeddings can exhibit gender bias, and various\nmethods have been proposed to quantify this. However, the extent to which the\nmethods are capturing social stereotypes inherited from the data has been\ndebated. Bias is a complex concept and there exist multiple ways to define it.\nPrevious work has leveraged gender word pairs to measure bias and extract\nbiased analogies. We show that the reliance on these gendered pairs has strong\nlimitations: bias measures based off of them are not robust and cannot identify\ncommon types of real-world bias, whilst analogies utilising them are unsuitable\nindicators of bias. In particular, the well-known analogy \"man is to\ncomputer-programmer as woman is to homemaker\" is due to word similarity rather\nthan societal bias. This has important implications for work on measuring bias\nin embeddings and related work debiasing embeddings.", "published": "2020-10-06 16:09:05", "link": "http://arxiv.org/abs/2010.02847v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LOGAN: Local Group Bias Detection by Clustering", "abstract": "Machine learning techniques have been widely used in natural language\nprocessing (NLP). However, as revealed by many recent studies, machine learning\nmodels often inherit and amplify the societal biases in data. Various metrics\nhave been proposed to quantify biases in model predictions. In particular,\nseveral of them evaluate disparity in model performance between protected\ngroups and advantaged groups in the test corpus. However, we argue that\nevaluating bias at the corpus level is not enough for understanding how biases\nare embedded in a model. In fact, a model with similar aggregated performance\nbetween different groups on the entire data may behave differently on instances\nin a local region. To analyze and detect such local bias, we propose LOGAN, a\nnew bias detection technique based on clustering. Experiments on toxicity\nclassification and object classification tasks show that LOGAN identifies bias\nin a local region and allows us to better analyze the biases in model\npredictions.", "published": "2020-10-06 16:42:51", "link": "http://arxiv.org/abs/2010.02867v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Represent Image and Text with Denotation Graph", "abstract": "Learning to fuse vision and language information and representing them is an\nimportant research problem with many applications. Recent progresses have\nleveraged the ideas of pre-training (from language modeling) and attention\nlayers in Transformers to learn representation from datasets containing images\naligned with linguistic expressions that describe the images. In this paper, we\npropose learning representations from a set of implied, visually grounded\nexpressions between image and text, automatically mined from those datasets. In\nparticular, we use denotation graphs to represent how specific concepts (such\nas sentences describing images) can be linked to abstract and generic concepts\n(such as short phrases) that are also visually grounded. This type of\ngeneric-to-specific relations can be discovered using linguistic analysis\ntools. We propose methods to incorporate such relations into learning\nrepresentation. We show that state-of-the-art multimodal learning models can be\nfurther improved by leveraging automatically harvested structural relations.\nThe representations lead to stronger empirical results on downstream tasks of\ncross-modal image retrieval, referring expression, and compositional\nattribute-object recognition. Both our codes and the extracted denotation\ngraphs on the Flickr30K and the COCO datasets are publically available on\nhttps://sha-lab.github.io/DG.", "published": "2020-10-06 18:00:58", "link": "http://arxiv.org/abs/2010.02949v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Digital Voicing of Silent Speech", "abstract": "In this paper, we consider the task of digitally voicing silent speech, where\nsilently mouthed words are converted to audible speech based on\nelectromyography (EMG) sensor measurements that capture muscle impulses. While\nprior work has focused on training speech synthesis models from EMG collected\nduring vocalized speech, we are the first to train from EMG collected during\nsilently articulated speech. We introduce a method of training on silent EMG by\ntransferring audio targets from vocalized to silent signals. Our method greatly\nimproves intelligibility of audio generated from silent EMG compared to a\nbaseline that only trains with vocalized data, decreasing transcription word\nerror rate from 64% to 4% in one data condition and 88% to 68% in another. To\nspur further development on this task, we share our new dataset of silent and\nvocalized facial EMG measurements.", "published": "2020-10-06 18:23:35", "link": "http://arxiv.org/abs/2010.02960v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Compositional Demographic Word Embeddings", "abstract": "Word embeddings are usually derived from corpora containing text from many\nindividuals, thus leading to general purpose representations rather than\nindividually personalized representations. While personalized embeddings can be\nuseful to improve language model performance and other language processing\ntasks, they can only be computed for people with a large amount of longitudinal\ndata, which is not the case for new users. We propose a new form of\npersonalized word embeddings that use demographic-specific word representations\nderived compositionally from full or partial demographic information for a user\n(i.e., gender, age, location, religion). We show that the resulting\ndemographic-aware word representations outperform generic word representations\non two tasks for English: language modeling and word associations. We further\nexplore the trade-off between the number of available attributes and their\nrelative effectiveness and discuss the ethical implications of using them.", "published": "2020-10-06 19:23:46", "link": "http://arxiv.org/abs/2010.02986v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small\n  Sample Medical Imaging", "abstract": "A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.", "published": "2020-10-06 22:20:29", "link": "http://arxiv.org/abs/2010.03060v5", "categories": ["cs.LG", "cs.CL", "cs.CV", "eess.IV"], "primary_category": "cs.LG"}
{"title": "RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text", "abstract": "In recent years, large neural networks for natural language generation (NLG)\nhave made leaps and bounds in their ability to generate fluent text. However,\nthe tasks of evaluating quality differences between NLG systems and\nunderstanding how humans perceive the generated text remain both crucial and\ndifficult. In this system demonstration, we present Real or Fake Text (RoFT), a\nwebsite that tackles both of these challenges by inviting users to try their\nhand at detecting machine-generated text in a variety of domains. We introduce\na novel evaluation task based on detecting the boundary at which a text passage\nthat starts off human-written transitions to being machine-generated. We show\npreliminary results of using RoFT to evaluate detection of machine-generated\nnews articles.", "published": "2020-10-06 22:47:43", "link": "http://arxiv.org/abs/2010.03070v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Program Enhanced Fact Verification with Verbalization and Graph\n  Attention Network", "abstract": "Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.", "published": "2020-10-06 23:29:08", "link": "http://arxiv.org/abs/2010.03084v6", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Is the Best Better? Bayesian Statistical Model Comparison for Natural\n  Language Processing", "abstract": "Recent work raises concerns about the use of standard splits to compare\nnatural language processing models. We propose a Bayesian statistical model\ncomparison technique which uses k-fold cross-validation across multiple data\nsets to estimate the likelihood that one model will outperform the other, or\nthat the two will produce practically equivalent results. We use this technique\nto rank six English part-of-speech taggers across two data sets and three\nevaluation metrics.", "published": "2020-10-06 23:37:28", "link": "http://arxiv.org/abs/2010.03088v1", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Rank and run-time aware compression of NLP Applications", "abstract": "Sequence model based NLP applications can be large. Yet, many applications\nthat benefit from them run on small devices with very limited compute and\nstorage capabilities, while still having run-time constraints. As a result,\nthere is a need for a compression technique that can achieve significant\ncompression without negatively impacting inference run-time and task accuracy.\nThis paper proposes a new compression technique called Hybrid Matrix\nFactorization that achieves this dual objective. HMF improves low-rank matrix\nfactorization (LMF) techniques by doubling the rank of the matrix using an\nintelligent hybrid-structure leading to better accuracy than LMF. Further, by\npreserving dense matrices, it leads to faster inference run-time than pruning\nor structure matrix based compression technique. We evaluate the impact of this\ntechnique on 5 NLP benchmarks across multiple tasks (Translation, Intent\nDetection, Language Modeling) and show that for similar accuracy values and\ncompression factors, HMF can achieve more than 2.32x faster inference run-time\nthan pruning and 16.77% better accuracy than LMF.", "published": "2020-10-06 16:03:15", "link": "http://arxiv.org/abs/2010.03193v1", "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "A Unified Deep Learning Framework for Short-Duration Speaker\n  Verification in Adverse Environments", "abstract": "Speaker verification (SV) has recently attracted considerable research\ninterest due to the growing popularity of virtual assistants. At the same time,\nthere is an increasing requirement for an SV system: it should be robust to\nshort speech segments, especially in noisy and reverberant environments. In\nthis paper, we consider one more important requirement for practical\napplications: the system should be robust to an audio stream containing long\nnon-speech segments, where a voice activity detection (VAD) is not applied. To\nmeet these two requirements, we introduce feature pyramid module (FPM)-based\nmulti-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present\nthe FPM-based MSA to deal with short speech segments in noisy and reverberant\nenvironments. Also, we use the SAS-VAD to increase the robustness to long\nnon-speech segments. To further improve the robustness to acoustic distortions\n(i.e., noise and reverberation), we apply a masking-based speech enhancement\n(SE) method. We combine SV, VAD, and SE models in a unified deep learning\nframework and jointly train the entire network in an end-to-end manner. To the\nbest of our knowledge, this is the first work combining these three models in a\ndeep learning framework. We conduct experiments on Korean indoor (KID) and\nVoxCeleb datasets, which are corrupted by noise and reverberation. The results\nshow that the proposed method is effective for SV in the challenging conditions\nand performs better than the baseline i-vector and deep speaker embedding\nsystems.", "published": "2020-10-06 04:51:45", "link": "http://arxiv.org/abs/2010.02477v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "The Academia Sinica Systems of Voice Conversion for VCC2020", "abstract": "This paper describes the Academia Sinica systems for the two tasks of Voice\nConversion Challenge 2020, namely voice conversion within the same language\n(Task 1) and cross-lingual voice conversion (Task 2). For both tasks, we\nfollowed the cascaded ASR+TTS structure, using phonetic tokens as the TTS input\ninstead of the text or characters. For Task 1, we used the international\nphonetic alphabet (IPA) as the input of the TTS model. For Task 2, we used\nunsupervised phonetic symbols extracted by the vector-quantized variational\nautoencoder (VQVAE). In the evaluation, the listening test showed that our\nsystems performed well in the VCC2020 challenge.", "published": "2020-10-06 12:40:06", "link": "http://arxiv.org/abs/2010.02669v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceGrad: Non-Parallel Any-to-Many Voice Conversion with Annealed\n  Langevin Dynamics", "abstract": "In this paper, we propose a non-parallel any-to-many voice conversion (VC)\nmethod termed VoiceGrad. Inspired by WaveGrad, a recently introduced novel\nwaveform generation method, VoiceGrad is based upon the concepts of score\nmatching and Langevin dynamics. It uses weighted denoising score matching to\ntrain a score approximator, a fully convolutional network with a U-Net\nstructure designed to predict the gradient of the log density of the speech\nfeature sequences of multiple speakers, and performs VC by using annealed\nLangevin dynamics to iteratively update an input feature sequence towards the\nnearest stationary point of the target distribution based on the trained score\napproximator network. Thanks to the nature of this concept, VoiceGrad enables\nany-to-many VC, a VC scenario in which the speaker of input speech can be\narbitrary, and allows for non-parallel training, which requires no parallel\nutterances or transcriptions.", "published": "2020-10-06 19:09:37", "link": "http://arxiv.org/abs/2010.02977v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pay Attention to the cough: Early Diagnosis of COVID-19 using\n  Interpretable Symptoms Embeddings with Cough Sound Signal Processing", "abstract": "COVID-19 (coronavirus disease 2019) pandemic caused by SARS-CoV-2 has led to\na treacherous and devastating catastrophe for humanity. At the time of writing,\nno specific antivirus drugs or vaccines are recommended to control infection\ntransmission and spread. The current diagnosis of COVID-19 is done by\nReverse-Transcription Polymer Chain Reaction (RT-PCR) testing. However, this\nmethod is expensive, time-consuming, and not easily available in straitened\nregions. An interpretable and COVID-19 diagnosis AI framework is devised and\ndeveloped based on the cough sounds features and symptoms metadata to overcome\nthese limitations. The proposed framework's performance was evaluated using a\nmedical dataset containing Symptoms and Demographic data of 30000 audio\nsegments, 328 cough sounds from 150 patients with four cough classes (\nCOVID-19, Asthma, Bronchitis, and Healthy). Experiments' results show that the\nmodel captures the better and robust feature embedding to distinguish between\nCOVID-19 patient coughs and several types of non-COVID-19 coughs with higher\nspecificity and accuracy of 95.04 $\\pm$ 0.18% and 96.83$\\pm$ 0.18%\nrespectively, all the while maintaining interpretability.", "published": "2020-10-06 01:22:50", "link": "http://arxiv.org/abs/2010.02417v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Principles for Designing Computer Music Controllers", "abstract": "This paper will present observations on the design, artistic, and human\nfactors of creating digital music controllers. Specific projects will be\npresented, and a set of design principles will be supported from those\nexamples.", "published": "2020-10-06 17:10:13", "link": "http://arxiv.org/abs/2010.06524v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
