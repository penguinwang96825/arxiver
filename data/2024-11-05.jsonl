{"title": "Blending Ensemble for Classification with Genetic-algorithm generated Alpha factors and Sentiments (GAS)", "abstract": "With the increasing maturity and expansion of the cryptocurrency market,\nunderstanding and predicting its price fluctuations has become an important\nissue in the field of financial engineering. This article introduces an\ninnovative Genetic Algorithm-generated Alpha Sentiment (GAS) blending ensemble\nmodel specifically designed to predict Bitcoin market trends. The model\nintegrates advanced ensemble learning methods, feature selection algorithms,\nand in-depth sentiment analysis to effectively capture the complexity and\nvariability of daily Bitcoin trading data. The GAS framework combines 34 Alpha\nfactors with 8 news economic sentiment factors to provide deep insights into\nBitcoin price fluctuations by accurately analyzing market sentiment and\ntechnical indicators. The core of this study is using a stacked model\n(including LightGBM, XGBoost, and Random Forest Classifier) for trend\nprediction which demonstrates excellent performance in traditional buy-and-hold\nstrategies. In addition, this article also explores the effectiveness of using\ngenetic algorithms to automate alpha factor construction as well as enhancing\npredictive models through sentiment analysis. Experimental results show that\nthe GAS model performs competitively in daily Bitcoin trend prediction\nespecially when analyzing highly volatile financial assets with rich data.", "published": "2024-11-05 12:15:01", "link": "http://arxiv.org/abs/2411.03035v1", "categories": ["q-fin.CP", "cs.LG", "q-fin.TR", "68T07, 91G60, 62M45", "I.2.6; G.3; I.5.4"], "primary_category": "q-fin.CP"}
{"title": "Time-Causal VAE: Robust Financial Time Series Generator", "abstract": "We build a time-causal variational autoencoder (TC-VAE) for robust generation\nof financial time series data. Our approach imposes a causality constraint on\nthe encoder and decoder networks, ensuring a causal transport from the real\nmarket time series to the fake generated time series. Specifically, we prove\nthat the TC-VAE loss provides an upper bound on the causal Wasserstein distance\nbetween market distributions and generated distributions. Consequently, the\nTC-VAE loss controls the discrepancy between optimal values of various dynamic\nstochastic optimization problems under real and generated distributions. To\nfurther enhance the model's ability to approximate the latent representation of\nthe real market distribution, we integrate a RealNVP prior into the TC-VAE\nframework. Finally, extensive numerical experiments show that TC-VAE achieves\npromising results on both synthetic and real market data. This is done by\ncomparing real and generated distributions according to various statistical\ndistances, demonstrating the effectiveness of the generated data for downstream\nfinancial optimization tasks, as well as showcasing that the generated data\nreproduces stylized facts of real financial market data.", "published": "2024-11-05 09:44:15", "link": "http://arxiv.org/abs/2411.02947v1", "categories": ["cs.LG", "q-fin.CP", "37M10, 68T07"], "primary_category": "cs.LG"}
{"title": "Utilizing RNN for Real-time Cryptocurrency Price Prediction and Trading Strategy Optimization", "abstract": "This study explores the use of Recurrent Neural Networks (RNN) for real-time\ncryptocurrency price prediction and optimized trading strategies. Given the\nhigh volatility of the cryptocurrency market, traditional forecasting models\noften fall short. By leveraging RNNs' capability to capture long-term patterns\nin time-series data, this research aims to improve accuracy in price prediction\nand develop effective trading strategies. The project follows a structured\napproach involving data collection, preprocessing, and model refinement,\nfollowed by rigorous backtesting for profitability and risk assessment. This\nwork contributes to both the academic and practical fields by providing a\nrobust predictive model and optimized trading strategies that address the\nchallenges of cryptocurrency trading.", "published": "2024-11-05 22:44:52", "link": "http://arxiv.org/abs/2411.05829v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "q-fin.ST"}
{"title": "Beyond the Traditional VIX: A Novel Approach to Identifying Uncertainty Shocks in Financial Markets", "abstract": "We introduce a new identification strategy for uncertainty shocks to explain\nmacroeconomic volatility in financial markets. The Chicago Board Options\nExchange Volatility Index (VIX) measures market expectations of future\nvolatility, but traditional methods based on second-moment shocks and\ntime-varying volatility of the VIX often fail to capture the non-Gaussian,\nheavy-tailed nature of asset returns. To address this, we construct a revised\nVIX by fitting a double-subordinated Normal Inverse Gaussian Levy process to\nS&P 500 option prices, providing a more comprehensive measure of volatility\nthat reflects the extreme movements and heavy tails observed in financial data.\nUsing an axiomatic approach, we introduce a general family of risk-reward\nratios, computed with our revised VIX and fitted over a fractional time series\nto more accurately identify uncertainty shocks in financial markets.", "published": "2024-11-05 04:34:27", "link": "http://arxiv.org/abs/2411.02804v1", "categories": ["econ.EM", "q-fin.ST"], "primary_category": "econ.EM"}
{"title": "Novelty-focused R&D landscaping using transformer and local outlier\n  factor", "abstract": "While numerous studies have explored the field of research and development\n(R&D) landscaping, the preponderance of these investigations has emphasized\npredictive analysis based on R&D outcomes, specifically patents, and academic\nliterature. However, the value of research proposals and novelty analysis has\nseldom been addressed. This study proposes a systematic approach to\nconstructing and navigating the R&D landscape that can be utilized to guide\norganizations to respond in a reproducible and timely manner to the challenges\npresented by increasing number of research proposals. At the heart of the\nproposed approach is the composite use of the transformer-based language model\nand the local outlier factor (LOF). The semantic meaning of the research\nproposals is captured with our further-trained transformers, thereby\nconstructing a comprehensive R&D landscape. Subsequently, the novelty of the\nnewly selected research proposals within the annual landscape is quantified on\na numerical scale utilizing the LOF by assessing the dissimilarity of each\nproposal to others preceding and within the same year. A case study examining\nresearch proposals in the energy and resource sector in South Korea is\npresented. The systematic process and quantitative outcomes are expected to be\nuseful decision-support tools, providing future insights regarding R&D planning\nand roadmapping.", "published": "2024-11-05 02:12:22", "link": "http://arxiv.org/abs/2411.02738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Translation of Circumlocution in Arabic Short Stories into English", "abstract": "This study investigates the translation of circumlocution from Arabic to\nEnglish in a corpus of short stories by renowned Arabic authors. By analyzing\nthe source and target texts, the study aims to identify and categorize\ncircumlocution instances in Arabic and their corresponding renditions in\nEnglish. The study employs Nida's (1964) translation theory as a framework to\nassess the appropriateness of the translation strategies employed. It examines\nthe extent to which translators successfully rendered Arabic circumlocution\ninto English, identifying potential challenges and limitations in the\ntranslation process. The findings reveal significant similarities between\nArabic circumlocution categories and English metadiscourse categories,\nparticularly in terms of textual and interpersonal functions. However, the\nstudy also highlights instances where translators encountered difficulties in\naccurately conveying the nuances of circumlocution, often resorting to\nstrategies like addition, subtraction, and alteration.https://ntu.edu.iq/", "published": "2024-11-05 07:59:22", "link": "http://arxiv.org/abs/2411.02887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA\n  Dataset and Self-adaptive Planning Agent", "abstract": "Multimodal Retrieval Augmented Generation (mRAG) plays an important role in\nmitigating the \"hallucination\" issue inherent in multimodal large language\nmodels (MLLMs). Although promising, existing heuristic mRAGs typically\npredefined fixed retrieval processes, which causes two issues: (1) Non-adaptive\nRetrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws\ncannot be adequately reflected by current knowledge-seeking visual question\nanswering (VQA) datasets, since the most required knowledge can be readily\nobtained with a standard two-step retrieval. To bridge the dataset gap, we\nfirst construct Dyn-VQA dataset, consisting of three types of \"dynamic\"\nquestions, which require complex knowledge retrieval strategies variable in\nquery, tool, and time: (1) Questions with rapidly changing answers. (2)\nQuestions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments\non Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient\nand precisely relevant knowledge for dynamic questions due to their rigid\nretrieval processes. Hence, we further propose the first self-adaptive planning\nagent for multimodal retrieval, OmniSearch. The underlying idea is to emulate\nthe human behavior in question solution which dynamically decomposes complex\nmultimodal questions into sub-question chains with retrieval action. Extensive\nexperiments prove the effectiveness of our OmniSearch, also provide direction\nfor advancing mRAG. The code and dataset will be open-sourced at\nhttps://github.com/Alibaba-NLP/OmniSearch.", "published": "2024-11-05 09:27:21", "link": "http://arxiv.org/abs/2411.02937v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach", "abstract": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.", "published": "2024-11-05 09:37:23", "link": "http://arxiv.org/abs/2411.02943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictor-Corrector Enhanced Transformers with Exponential Moving\n  Average Coefficient Learning", "abstract": "Residual networks, as discrete approximations of Ordinary Differential\nEquations (ODEs), have inspired significant advancements in neural network\ndesign, including multistep methods, high-order methods, and multi-particle\ndynamical systems. The precision of the solution to ODEs significantly affects\nparameter optimization, thereby impacting model performance. In this work, we\npresent a series of advanced explorations of Transformer architecture design to\nminimize the error compared to the true ``solution.'' First, we introduce a\npredictor-corrector learning framework to minimize truncation errors, which\nconsists of a high-order predictor and a multistep corrector. Second, we\npropose an exponential moving average-based coefficient learning method to\nstrengthen our higher-order predictor. Extensive experiments on large-scale\nmachine translation, abstractive summarization, language modeling, and natural\nlanguage understanding benchmarks demonstrate the superiority of our approach.\nOn the WMT'14 English-German and English-French tasks, our model achieved BLEU\nscores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual\nmachine translation task, our model surpasses a robust 3.8B DeepNet by an\naverage of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats\nLLama models by 5.7 accuracy points on the LM Harness Evaluation.", "published": "2024-11-05 12:26:25", "link": "http://arxiv.org/abs/2411.03042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification for Clinical Outcome Predictions with (Large)\n  Language Models", "abstract": "To facilitate healthcare delivery, language models (LMs) have significant\npotential for clinical prediction tasks using electronic health records (EHRs).\nHowever, in these high-stakes applications, unreliable decisions can result in\nhigh costs due to compromised patient safety and ethical concerns, thus\nincreasing the need for good uncertainty modeling of automated clinical\npredictions. To address this, we consider the uncertainty quantification of LMs\nfor EHR tasks in white- and black-box settings. We first quantify uncertainty\nin white-box models, where we can access model parameters and output logits. We\nshow that an effective reduction of model uncertainty can be achieved by using\nthe proposed multi-tasking and ensemble methods in EHRs. Continuing with this\nidea, we extend our approach to black-box settings, including popular\nproprietary LMs such as GPT-4. We validate our framework using longitudinal\nclinical data from more than 6,000 patients in ten clinical prediction tasks.\nResults show that ensembling methods and multi-task prediction prompts reduce\nuncertainty across different scenarios. These findings increase the\ntransparency of the model in white-box and black-box settings, thus advancing\nreliable AI healthcare.", "published": "2024-11-05 20:20:15", "link": "http://arxiv.org/abs/2411.03497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Write Rationally: How Information Is Distributed in\n  Non-Native Speakers' Essays", "abstract": "People tend to distribute information evenly in language production for\nbetter and clearer communication. In this study, we compared essays written by\nsecond language learners with various native language (L1) backgrounds to\ninvestigate how they distribute information in their non-native language (L2)\nproduction. Analyses of surprisal and constancy of entropy rate indicated that\nwriters with higher L2 proficiency can reduce the expected uncertainty of\nlanguage production while still conveying informative content. However, the\nuniformity of information distribution showed less variability among different\ngroups of L2 speakers, suggesting that this feature may be universal in L2\nessay writing and less affected by L2 writers' variability in L1 background and\nL2 proficiency.", "published": "2024-11-05 23:09:37", "link": "http://arxiv.org/abs/2411.03550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Loss of Context-awareness in General Instruction Fine-tuning", "abstract": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We identify and demonstrate that the loss of context\nawareness, particularly in open-source models, occurs in instruction fine-tuned\nLLMs when the chat template is applied to input prompts. We identify that the\nperformance decline is associated with a bias toward different roles learned\nduring conversational instruction fine-tuning. We demonstrate this correlation\nby visualizing changes in attention allocation after the chat template is\napplied and manually steering the attention heads. The bias can be learned from\ntraining examples that align with the model's internal knowledge and rely less\non the user-provided context to generate correct responses. Based on these\nobservations, we propose a metric to identify context-dependent examples from\ngeneral instruction fine-tuning datasets. We then apply conditional instruction\nfine-tuning with a context-dependency indicator, enabling the model to preserve\ncontext awareness after SFT. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities.", "published": "2024-11-05 00:16:01", "link": "http://arxiv.org/abs/2411.02688v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Commonsense Knowledge Distillation for Visual Question\n  Answering", "abstract": "Existing Multimodal Large Language Models (MLLMs) and Visual Language\nPretrained Models (VLPMs) have shown remarkable performances in the general\nVisual Question Answering (VQA). However, these models struggle with VQA\nquestions that require external commonsense knowledge due to the challenges in\ngenerating high-quality prompts and the high computational costs of\nfine-tuning. In this work, we propose a novel graph-based multimodal\ncommonsense knowledge distillation framework that constructs a unified\nrelational graph over commonsense knowledge, visual objects and questions\nthrough a Graph Convolutional Network (GCN) following a teacher-student\nenvironment. This proposed framework is flexible with any type of teacher and\nstudent models without further fine-tuning, and has achieved competitive\nperformances on the ScienceQA dataset.", "published": "2024-11-05 01:37:16", "link": "http://arxiv.org/abs/2411.02722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Natural Language Processing Approach to Support Biomedical Data\n  Harmonization: Leveraging Large Language Models", "abstract": "Biomedical research requires large, diverse samples to produce unbiased\nresults. Automated methods for matching variables across datasets can\naccelerate this process. Research in this area has been limited, primarily\nfocusing on lexical matching and ontology based semantic matching. We aimed to\ndevelop new methods, leveraging large language models (LLM) and ensemble\nlearning, to automate variable matching. Methods: We utilized data from two\nGERAS cohort (European and Japan) studies to develop variable matching methods.\nWe first manually created a dataset by matching 352 EU variables with 1322\ncandidate JP variables, where matched variable pairs were positive and\nunmatched pairs were negative instances. Using this dataset, we developed and\nevaluated two types of natural language processing (NLP) methods, which matched\nvariables based on variable labels and definitions from data dictionaries: (1)\nLLM-based and (2) fuzzy matching. We then developed an ensemble-learning\nmethod, using the Random Forest model, to integrate individual NLP methods. RF\nwas trained and evaluated on 50 trials. Each trial had a random split (4:1) of\ntraining and test sets, with the model's hyperparameters optimized through\ncross-validation on the training set. For each EU variable, 1322 candidate JP\nvariables were ranked based on NLP-derived similarity scores or RF's\nprobability scores, denoting their likelihood to match the EU variable. Ranking\nperformance was measured by top-n hit ratio (HRn) and mean reciprocal rank\n(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30\nand 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less\nthan 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived\nfeatures contributed most to RF's performance. One major cause of errors in\nautomatic variable matching was ambiguous variable definitions within data\ndictionaries.", "published": "2024-11-05 01:58:31", "link": "http://arxiv.org/abs/2411.02730v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory Augmented Cross-encoders for Controllable Personalized Search", "abstract": "Personalized search represents a problem where retrieval models condition on\nhistorical user interaction data in order to improve retrieval results.\nHowever, personalization is commonly perceived as opaque and not amenable to\ncontrol by users. Further, personalization necessarily limits the space of\nitems that users are exposed to. Therefore, prior work notes a tension between\npersonalization and users' ability for discovering novel items. While discovery\nof novel items in personalization setups may be resolved through search result\ndiversification, these approaches do little to allow user control over\npersonalization. Therefore, in this paper, we introduce an approach for\ncontrollable personalized search. Our model, CtrlCE presents a novel\ncross-encoder model augmented with an editable memory constructed from users\nhistorical items. Our proposed memory augmentation allows cross-encoder models\nto condition on large amounts of historical user data and supports interaction\nfrom users permitting control over personalization. Further, controllable\npersonalization for search must account for queries which don't require\npersonalization, and in turn user control. For this, we introduce a calibrated\nmixing model which determines when personalization is necessary. This allows\nsystem designers using CtrlCE to only obtain user input for control when\nnecessary. In multiple datasets of personalized search, we show CtrlCE to\nresult in effective personalization as well as fulfill various key goals for\ncontrollable personalized search.", "published": "2024-11-05 03:55:25", "link": "http://arxiv.org/abs/2411.02790v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical\n  Representation Learning", "abstract": "Multimodal Sentiment Analysis (MSA) is an important research area that aims\nto understand and recognize human sentiment through multiple modalities. The\ncomplementary information provided by multimodal fusion promotes better\nsentiment analysis compared to utilizing only a single modality. Nevertheless,\nin real-world applications, many unavoidable factors may lead to situations of\nuncertain modality missing, thus hindering the effectiveness of multimodal\nmodeling and degrading the model's performance. To this end, we propose a\nHierarchical Representation Learning Framework (HRLF) for the MSA task under\nuncertain missing modalities. Specifically, we propose a fine-grained\nrepresentation factorization module that sufficiently extracts valuable\nsentiment information by factorizing modality into sentiment-relevant and\nmodality-specific representations through crossmodal translation and sentiment\nsemantic reconstruction. Moreover, a hierarchical mutual information\nmaximization mechanism is introduced to incrementally maximize the mutual\ninformation between multi-scale representations to align and reconstruct the\nhigh-level semantics in the representations. Ultimately, we propose a\nhierarchical adversarial learning mechanism that further aligns and adapts the\nlatent distribution of sentiment-relevant representations to produce robust\njoint multimodal representations. Comprehensive experiments on three datasets\ndemonstrate that HRLF significantly improves MSA performance under uncertain\nmodality missing cases.", "published": "2024-11-05 04:04:41", "link": "http://arxiv.org/abs/2411.02793v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Evolution of RWKV: Advancements in Efficient Language Modeling", "abstract": "This paper reviews the development of the Receptance Weighted Key Value\n(RWKV) architecture, emphasizing its advancements in efficient language\nmodeling. RWKV combines the training efficiency of Transformers with the\ninference efficiency of RNNs through a novel linear attention mechanism. We\nexamine its core innovations, adaptations across various domains, and\nperformance advantages over traditional models. The paper also discusses\nchallenges and future directions for RWKV as a versatile architecture in deep\nlearning.", "published": "2024-11-05 04:10:05", "link": "http://arxiv.org/abs/2411.02795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grounding Natural Language to SQL Translation with Data-Based\n  Self-Explanations", "abstract": "Natural Language Interfaces for Databases empower non-technical users to\ninteract with data using natural language (NL). Advanced approaches, utilizing\neither neural sequence-to-sequence or more recent sophisticated large-scale\nlanguage models, typically implement NL to SQL (NL2SQL) translation in an\nend-to-end fashion. However, like humans, these end-to-end translation models\nmay not always generate the best SQL output on their first try. In this paper,\nwe propose CycleSQL, an iterative framework designed for end-to-end translation\nmodels to autonomously generate the best output through self-evaluation. The\nmain idea of CycleSQL is to introduce data-grounded NL explanations of query\nresults as self-provided feedback, and use the feedback to validate the\ncorrectness of the translation iteratively, hence improving the overall\ntranslation accuracy. Extensive experiments, including quantitative and\nqualitative evaluations, are conducted to study CycleSQL by applying it to\nseven existing translation models on five widely used benchmarks. The results\nshow that 1) the feedback loop introduced in CycleSQL can consistently improve\nthe performance of existing models, and in particular, by applying CycleSQL to\nRESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set,\nand 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL\nexplanations can also provide insightful information for users, aiding in the\ncomprehension of translation results and consequently enhancing the\ninterpretability of NL2SQL translation.", "published": "2024-11-05 09:44:53", "link": "http://arxiv.org/abs/2411.02948v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI", "abstract": "We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study.", "published": "2024-11-05 10:18:53", "link": "http://arxiv.org/abs/2411.02973v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Growing a Tail: Increasing Output Diversity in Large Language Models", "abstract": "How diverse are the outputs of large language models when diversity is\ndesired? We examine the diversity of responses of various models to questions\nwith multiple possible answers, comparing them with human responses. Our\nfindings suggest that models' outputs are highly concentrated, reflecting a\nnarrow, mainstream 'worldview', in comparison to humans, whose responses\nexhibit a much longer-tail. We examine three ways to increase models' output\ndiversity: 1) increasing generation randomness via temperature sampling; 2)\nprompting models to answer from diverse perspectives; 3) aggregating outputs\nfrom several models. A combination of these measures significantly increases\nmodels' output diversity, reaching that of humans. We discuss implications of\nthese findings for AI policy that wishes to preserve cultural diversity, an\nessential building block of a democratic social fabric.", "published": "2024-11-05 10:52:20", "link": "http://arxiv.org/abs/2411.02989v1", "categories": ["cs.CL", "cs.CY", "I.2.7; K.5.m; K.4.1"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models in Code Question Answering: Baselines\n  and Issues", "abstract": "Question answering over source code provides software engineers and project\nmanagers with helpful information about the implemented features of a software\nproduct. This paper presents a work devoted to using large language models for\nquestion answering over source code in Python. The proposed method for\nimplementing a source code question answering system involves fine-tuning a\nlarge language model on a unified dataset of questions and answers for Python\ncode. To achieve the highest quality answers, we tested various models trained\non datasets preprocessed in different ways: a dataset without grammar\ncorrection, a dataset with grammar correction, and a dataset augmented with the\ngenerated summaries. The model answers were also analyzed for errors manually.\nWe report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along\nwith the conclusions from the manual error analysis. The obtained experimental\nresults highlight the current problems of the research area, such as poor\nquality of the public genuine question-answering datasets. In addition, the\nfindings include the positive effect of the grammar correction of the training\ndata on the testing metric values. The addressed findings and issues could be\nimportant for other researchers who attempt to improve the quality of source\ncode question answering solutions. The training and evaluation code is publicly\navailable at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.", "published": "2024-11-05 11:25:12", "link": "http://arxiv.org/abs/2411.03012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Compositional Data Augmentation for Scientific Keyphrase Generation", "abstract": "State-of-the-art models for keyphrase generation require large amounts of\ntraining data to achieve good performance. However, obtaining keyphrase-labeled\ndocuments can be challenging and costly. To address this issue, we present a\nself-compositional data augmentation method. More specifically, we measure the\nrelatedness of training documents based on their shared keyphrases, and combine\nsimilar documents to generate synthetic samples. The advantage of our method\nlies in its ability to create additional training samples that keep domain\ncoherence, without relying on external data or resources. Our results on\nmultiple datasets spanning three different domains, demonstrate that our method\nconsistently improves keyphrase generation. A qualitative analysis of the\ngenerated keyphrases for the Computer Science domain confirms this improvement\ntowards their representativity property.", "published": "2024-11-05 12:22:51", "link": "http://arxiv.org/abs/2411.03039v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "VERITAS: A Unified Approach to Reliability Evaluation", "abstract": "Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting.", "published": "2024-11-05 17:53:25", "link": "http://arxiv.org/abs/2411.03300v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs for Domain Generation Algorithm Detection", "abstract": "This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains.", "published": "2024-11-05 18:01:12", "link": "http://arxiv.org/abs/2411.03307v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "MME-Finance: A Multimodal Finance Benchmark for Expert-level\n  Understanding and Reasoning", "abstract": "In recent years, multimodal benchmarks for general domains have guided the\nrapid development of multimodal models on general tasks. However, the financial\nfield has its peculiarities. It features unique graphical images (e.g.,\ncandlestick charts, technical indicator charts) and possesses a wealth of\nspecialized financial knowledge (e.g., futures, turnover rate). Therefore,\nbenchmarks from general fields often fail to measure the performance of\nmultimodal models in the financial domain, and thus cannot effectively guide\nthe rapid development of large financial models. To promote the development of\nlarge financial multimodal models, we propose MME-Finance, an bilingual\nopen-ended and practical usage-oriented Visual Question Answering (VQA)\nbenchmark. The characteristics of our benchmark are finance and expertise,\nwhich include constructing charts that reflect the actual usage needs of users\n(e.g., computer screenshots and mobile photography), creating questions\naccording to the preferences in financial domain inquiries, and annotating\nquestions by experts with 10+ years of experience in the financial industry.\nAdditionally, we have developed a custom-designed financial evaluation system\nin which visual information is first introduced in the multi-modal evaluation\nprocess. Extensive experimental evaluations of 19 mainstream MLLMs are\nconducted to test their perception, reasoning, and cognition capabilities. The\nresults indicate that models performing well on general benchmarks cannot do\nwell on MME-Finance; for instance, the top-performing open-source and\nclosed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),\nrespectively. Their performance is particularly poor in categories most\nrelevant to finance, such as candlestick charts and technical indicator charts.\nIn addition, we propose a Chinese version, which helps compare performance of\nMLLMs under a Chinese context.", "published": "2024-11-05 18:59:51", "link": "http://arxiv.org/abs/2411.03314v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Large Language Models for Specialist-level Oncology Care", "abstract": "Large language models (LLMs) have shown remarkable progress in encoding\nclinical knowledge and responding to complex medical queries with appropriate\nclinical reasoning. However, their applicability in subspecialist or complex\nmedical settings remains underexplored. In this work, we probe the performance\nof AMIE, a research conversational diagnostic AI system, in the subspecialist\ndomain of breast oncology care without specific fine-tuning to this challenging\ndomain. To perform this evaluation, we curated a set of 50 synthetic breast\ncancer vignettes representing a range of treatment-naive and\ntreatment-refractory cases and mirroring the key information available to a\nmultidisciplinary tumor board for decision-making (openly released with this\nwork). We developed a detailed clinical rubric for evaluating management plans,\nincluding axes such as the quality of case summarization, safety of the\nproposed care plan, and recommendations for chemotherapy, radiotherapy, surgery\nand hormonal therapy. To improve performance, we enhanced AMIE with the\ninference-time ability to perform web search retrieval to gather relevant and\nup-to-date clinical knowledge and refine its responses with a multi-stage\nself-critique pipeline. We compare response quality of AMIE with internal\nmedicine trainees, oncology fellows, and general oncology attendings under both\nautomated and specialist clinician evaluations. In our evaluations, AMIE\noutperformed trainees and fellows demonstrating the potential of the system in\nthis challenging and important domain. We further demonstrate through\nqualitative examples, how systems such as AMIE might facilitate conversational\ninteractions to assist clinicians in their decision making. However, AMIE's\nperformance was overall inferior to attending oncologists suggesting that\nfurther research is needed prior to consideration of prospective uses.", "published": "2024-11-05 18:30:13", "link": "http://arxiv.org/abs/2411.03395v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SAUCE: Synchronous and Asynchronous User-Customizable Environment for\n  Multi-Agent LLM Interaction", "abstract": "Many human interactions, such as political debates, are carried out in group\nsettings, where there are arbitrarily many participants, each with different\nviews and agendas. To explore such complex social settings, we present SAUCE: a\ncustomizable Python platform, allowing researchers to plug-and-play various\nLLMs participating in discussions on any topic chosen by the user. Our platform\ntakes care of instantiating the models, scheduling their responses, managing\nthe discussion history, and producing a comprehensive output log, all\ncustomizable through configuration files, requiring little to no coding skills.\nA novel feature of SAUCE is our asynchronous communication feature, where\nmodels decide when to speak in addition to what to say, thus modeling an\nimportant facet of human communication. We show SAUCE's attractiveness in two\ninitial experiments, and invite the community to use it in simulating various\ngroup simulations.", "published": "2024-11-05 18:31:06", "link": "http://arxiv.org/abs/2411.03397v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs", "abstract": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning.", "published": "2024-11-05 19:52:58", "link": "http://arxiv.org/abs/2411.03471v2", "categories": ["cs.AR", "cs.CL"], "primary_category": "cs.AR"}
{"title": "LLM Generated Distribution-Based Prediction of US Electoral Results,\n  Part I", "abstract": "This paper introduces distribution-based prediction, a novel approach to\nusing Large Language Models (LLMs) as predictive tools by interpreting output\ntoken probabilities as distributions representing the models' learned\nrepresentation of the world. This distribution-based nature offers an\nalternative perspective for analyzing algorithmic fidelity, complementing the\napproach used in silicon sampling. We demonstrate the use of distribution-based\nprediction in the context of recent United States presidential election,\nshowing that this method can be used to determine task specific bias, prompt\nnoise, and algorithmic fidelity. This approach has significant implications for\nassessing the reliability and increasing transparency of LLM-based predictions\nacross various domains.", "published": "2024-11-05 20:10:25", "link": "http://arxiv.org/abs/2411.03486v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LASER: Attention with Exponential Transformation", "abstract": "Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER Attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average\nof ~1% improvement over standard attention on downstream evaluations. Using\nLASER gives the following relative improvements in generalization performance\nacross a variety of tasks (vision, text and speech): 4.67% accuracy in Vision\nTransformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech\nspeech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2\nbillion parameters.", "published": "2024-11-05 20:18:28", "link": "http://arxiv.org/abs/2411.03493v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automatic Generation of Question Hints for Mathematics Problems using\n  Large Language Models in Educational Technology", "abstract": "The automatic generation of hints by Large Language Models (LLMs) within\nIntelligent Tutoring Systems (ITSs) has shown potential to enhance student\nlearning. However, generating pedagogically sound hints that address student\nmisconceptions and adhere to specific educational objectives remains\nchallenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as\nteachers to generate effective hints for students simulated through LLMs\n(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math\nexercises designed for human high-school students, and designed using cognitive\nscience principles. We present here the study of several dimensions: 1)\nidentifying error patterns made by simulated students on secondary-level math\nexercises; 2) developing various prompts for GPT-4o as a teacher and evaluating\ntheir effectiveness in generating hints that enable simulated students to\nself-correct; and 3) testing the best-performing prompts, based on their\nability to produce relevant hints and facilitate error correction, with\nLlama-3-8B-Instruct as the teacher, allowing for a performance comparison with\nGPT-4o. The results show that model errors increase with higher temperature\nsettings. Notably, when hints are generated by GPT-4o, the most effective\nprompts include prompts tailored to specific errors as well as prompts\nproviding general hints based on common mathematical errors. Interestingly,\nLlama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.\nAlso the problem-solving and response revision capabilities of the LLMs as\nstudents, particularly GPT-3.5-turbo, improved significantly after receiving\nhints, especially at lower temperature settings. However, models like\nMistral-7B-Instruct demonstrated a decline in performance as the temperature\nincreased.", "published": "2024-11-05 20:18:53", "link": "http://arxiv.org/abs/2411.03495v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Metric Bias in Minimum Bayes Risk Decoding", "abstract": "While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or\nMetricX has outperformed traditional decoding methods such as greedy or beam\nsearch, it introduces a challenge we refer to as metric bias. As MBR decoding\naims to produce translations that score highly according to a specific utility\nmetric, this very process makes it impossible to use the same metric for both\ndecoding and evaluation, as improvements might simply be due to reward hacking\nrather than reflecting real quality improvements. In this work we find that\ncompared to human ratings, neural metrics not only overestimate the quality of\nMBR decoding when the same metric is used as the utility metric, but they also\noverestimate the quality of MBR/QE decoding with other neural utility metrics\nas well. We also show that the metric bias issue can be mitigated by using an\nensemble of utility metrics during MBR decoding: human evaluations show that\nMBR decoding using an ensemble of utility metrics outperforms a single utility\nmetric.", "published": "2024-11-05 22:01:27", "link": "http://arxiv.org/abs/2411.03524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Long Context RAG Performance of Large Language Models", "abstract": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research.", "published": "2024-11-05 22:37:43", "link": "http://arxiv.org/abs/2411.03538v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring the Benefits of Domain-Pretraining of Generative Large\n  Language Models for Chemistry", "abstract": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and\nmore) are driving forward novel development of multipurpose AI for a variety of\ntasks, particularly natural language processing (NLP) tasks. These models\ndemonstrate strong performance on a range of tasks; however, there has been\nevidence of brittleness when applied to more niche or narrow domains where\nhallucinations or fluent but incorrect responses reduce performance. Given the\ncomplex nature of scientific domains, it is prudent to investigate the\ntrade-offs of leveraging off-the-shelf versus more targeted foundation models\nfor scientific domains. In this work, we examine the benefits of in-domain\npre-training for a given scientific domain, chemistry, and compare these to\nopen-source, off-the-shelf models with zero-shot and few-shot prompting. Our\nresults show that not only do in-domain base models perform reasonably well on\nin-domain tasks in a zero-shot setting but that further adaptation using\ninstruction fine-tuning yields impressive performance on chemistry-specific\ntasks such as named entity recognition and molecular formula generation.", "published": "2024-11-05 22:45:10", "link": "http://arxiv.org/abs/2411.03542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on the Impact of Test-Driven Development (TDD) and\n  Behavior-Driven Development (BDD) on Enterprise Software Delivery\n  Effectiveness", "abstract": "This paper compares the impact of Test-Driven Development (TDD) and\nBehavior-Driven Development (BDD) on software delivery effectiveness within\nenterprise environments. Using a qualitative research design, data were\ncollected through in-depth interviews with developers and project managers from\nenterprises adopting TDD or BDD. Moreover, the findings reveal distinct effects\nof each model on delivery speed, software quality, and team collaboration.\nSpecifically, TDD emphasizes early testing and iterative development, leading\nto enhanced code quality and fewer defects, while BDD improves cross-functional\ncommunication by focusing on behavior specifications that involve stakeholders\ndirectly. However, TDD may create a higher initial time investment, and BDD\nmight encounter challenges in requirement clarity. These differences highlight\ngaps in understanding how each model aligns with varying project types and\nstakeholder needs, which can guide enterprises in selecting the most suitable\nmodel for their unique requirements. The study contributes to the literature by\nproviding insights into the practical application and challenges of TDD and\nBDD, suggesting future research on their long-term impacts in diverse settings.", "published": "2024-11-05 06:47:11", "link": "http://arxiv.org/abs/2411.04141v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Software Design Pattern Model and Data Structure Algorithm Abilities on\n  Microservices Architecture Design in High-tech Enterprises", "abstract": "This study investigates the impact of software design model capabilities and\ndata structure algorithm abilities on microservices architecture design within\nenterprises. Utilizing a qualitative methodology, the research involved\nin-depth interviews with software architects and developers who possess\nextensive experience in microservices implementation. The findings reveal that\norganizations emphasizing robust design models and efficient algorithms achieve\nsuperior scalability, performance, and flexibility in their microservices\narchitecture. Notably, participants highlighted that a strong foundation in\nthese areas facilitates better service decomposition, optimizes data\nprocessing, and enhances system responsiveness. Despite these insights, gaps\nremain regarding the integration of emerging technologies and the evolving\nnature of software design practices. This paper contributes to the existing\nliterature by underscoring the critical role of these competencies in fostering\neffective microservices architectures and suggests avenues for future research\nto address identified gaps", "published": "2024-11-05 07:26:53", "link": "http://arxiv.org/abs/2411.04143v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "AI Ethics by Design: Implementing Customizable Guardrails for\n  Responsible AI Development", "abstract": "This paper explores the development of an ethical guardrail framework for AI\nsystems, emphasizing the importance of customizable guardrails that align with\ndiverse user values and underlying ethics. We address the challenges of AI\nethics by proposing a structure that integrates rules, policies, and AI\nassistants to ensure responsible AI behavior, while comparing the proposed\nframework to the existing state-of-the-art guardrails. By focusing on practical\nmechanisms for implementing ethical standards, we aim to enhance transparency,\nuser autonomy, and continuous improvement in AI systems. Our approach\naccommodates ethical pluralism, offering a flexible and adaptable solution for\nthe evolving landscape of AI governance. The paper concludes with strategies\nfor resolving conflicts between ethical directives, underscoring the present\nand future need for robust, nuanced and context-aware AI systems.", "published": "2024-11-05 18:38:30", "link": "http://arxiv.org/abs/2411.14442v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase", "abstract": "Knowledge Graphs have emerged as a compelling abstraction for capturing key\nrelationship among the entities of interest to enterprises and for integrating\ndata from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by\nleveraging knowledge graphs across the organization for multiple mission\ncritical applications such as risk assessment, fraud detection, investment\nadvice, etc. A core problem in leveraging a knowledge graph is to link mentions\n(e.g., company names) that are encountered in textual sources to entities in\nthe knowledge graph. Although several techniques exist for entity linking, they\nare tuned for entities that exist in Wikipedia, and fail to generalize for the\nentities that are of interest to an enterprise. In this paper, we propose a\nnovel end-to-end neural entity linking model (JEL) that uses minimal context\ninformation and a margin loss to generate entity embeddings, and a Wide & Deep\nLearning model to match character and semantic information respectively. We\nshow that JEL achieves the state-of-the-art performance to link mentions of\ncompany names in financial news with entities in our knowledge graph. We report\non our efforts to deploy this model in the company-wide system to generate\nalerts in response to financial news. The methodology used for JEL is directly\napplicable and usable by other enterprises who need entity linking solutions\nfor data that are unique to their respective situations.", "published": "2024-11-05 00:46:25", "link": "http://arxiv.org/abs/2411.02695v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under\n  Misleading Scenarios", "abstract": "Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency\nin their responses is essential for developing trustworthy multimodal\nintelligence. However, existing benchmarks include many samples where all MLLMs\n\\textit{exhibit high response uncertainty when encountering misleading\ninformation}, requiring even 5-15 response attempts per sample to effectively\nassess uncertainty. Therefore, we propose a two-stage pipeline: first, we\ncollect MLLMs' responses without misleading information, and then gather\nmisleading ones via specific misleading instructions. By calculating the\nmisleading rate, and capturing both correct-to-incorrect and\nincorrect-to-correct shifts between the two sets of responses, we can\neffectively metric the model's response uncertainty. Eventually, we establish a\n\\textbf{\\underline{M}}ultimodal \\textbf{\\underline{U}}ncertainty\n\\textbf{\\underline{B}}enchmark (\\textbf{MUB}) that employs both explicit and\nimplicit misleading instructions to comprehensively assess the vulnerability of\nMLLMs across diverse domains. Our experiments reveal that all open-source and\nclose-source MLLMs are highly susceptible to misleading instructions, with an\naverage misleading rate exceeding 86\\%. To enhance the robustness of MLLMs, we\nfurther fine-tune all open-source MLLMs by incorporating explicit and implicit\nmisleading data, which demonstrates a significant reduction in misleading\nrates. Our code is available at:\n\\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}", "published": "2024-11-05 01:11:28", "link": "http://arxiv.org/abs/2411.02708v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Game Plot Design with an LLM-powered Assistant: An Empirical Study with\n  Game Designers", "abstract": "We introduce GamePlot, an LLM-powered assistant that supports game designers\nin crafting immersive narratives for turn-based games, and allows them to test\nthese games through a collaborative game play and refine the plot throughout\nthe process. Our user study with 14 game designers shows high levels of both\nsatisfaction with the generated game plots and sense of ownership over the\nnarratives, but also reconfirms that LLM are limited in their ability to\ngenerate complex and truly innovative content. We also show that diverse user\npopulations have different expectations from AI assistants, and encourage\nresearchers to study how tailoring assistants to diverse user groups could\npotentially lead to increased job satisfaction and greater creativity and\ninnovation over time.", "published": "2024-11-05 01:26:35", "link": "http://arxiv.org/abs/2411.02714v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving", "abstract": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.", "published": "2024-11-05 05:41:41", "link": "http://arxiv.org/abs/2411.02820v3", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Mixtures of In-Context Learners", "abstract": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory.", "published": "2024-11-05 06:02:41", "link": "http://arxiv.org/abs/2411.02830v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PersianRAG: A Retrieval-Augmented Generation System for Persian Language", "abstract": "Retrieval augmented generation (RAG) models, which integrate large-scale\npre-trained generative models with external retrieval mechanisms, have shown\nsignificant success in various natural language processing (NLP) tasks.\nHowever, applying RAG models in Persian language as a low-resource language,\nposes distinct challenges. These challenges primarily involve the\npreprocessing, embedding, retrieval, prompt construction, language modeling,\nand response evaluation of the system. In this paper, we address the challenges\ntowards implementing a real-world RAG system for Persian language called\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\nour approach using several Persian benchmark datasets. Our experimental results\ndemonstrate the capability of the PersianRAG framework to enhance question\nanswering task in Persian.", "published": "2024-11-05 06:11:17", "link": "http://arxiv.org/abs/2411.02832v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning", "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments.", "published": "2024-11-05 07:12:36", "link": "http://arxiv.org/abs/2411.02864v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection", "abstract": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.", "published": "2024-11-05 07:56:24", "link": "http://arxiv.org/abs/2411.02886v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textual Aesthetics in Large Language Models", "abstract": "Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard.", "published": "2024-11-05 09:22:08", "link": "http://arxiv.org/abs/2411.02930v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Post-Training Enhanced Optimization Approach for Small Language Models", "abstract": "This paper delves into the continuous post-training optimization methods for\nsmall language models, and proposes a continuous post-training alignment data\nconstruction method for small language models. The core of this method is based\non the data guidance of large models, optimizing the diversity and accuracy of\nalignment data. In addition, to verify the effectiveness of the methods in this\npaper, we used Qwen2-0.5B-Instruct model as the baseline model for small\nlanguage models, using the alignment dataset constructed by our proposed\nmethod, we trained and compared several groups of experiments, including SFT\n(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky\noptimization) post-training experiment, as well as SFT-KTO two-stage\npost-training experiment and model weight fusion experiment. Finally, we\nevaluated and analyzed the performance of post-training models, and confirmed\nthat the continuous post-training optimization method proposed by us can\nsignificantly improve the performance of small language models.", "published": "2024-11-05 09:32:26", "link": "http://arxiv.org/abs/2411.02939v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review.", "published": "2024-11-05 16:47:53", "link": "http://arxiv.org/abs/2411.03250v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents", "abstract": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.", "published": "2024-11-05 17:33:39", "link": "http://arxiv.org/abs/2411.03284v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific\n  Papers: NeurIPS'24 Experiment", "abstract": "Large language models (LLMs) represent a promising, but controversial, tool\nin aiding scientific peer review. This study evaluates the usefulness of LLMs\nin a conference setting as a tool for vetting paper submissions against\nsubmission standards. We conduct an experiment at the 2024 Neural Information\nProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily\nsubmitted to an \"LLM-based Checklist Assistant.\" This assistant validates\nwhether papers adhere to the author checklist used by NeurIPS, which includes\nquestions to ensure compliance with research and manuscript preparation\nstandards. Evaluation of the assistant by NeurIPS paper authors suggests that\nthe LLM-based assistant was generally helpful in verifying checklist\ncompletion. In post-usage surveys, over 70% of authors found the assistant\nuseful, and 70% indicate that they would revise their papers or checklist\nresponses based on its feedback. While causal attribution to the assistant is\nnot definitive, qualitative evidence suggests that the LLM contributed to\nimproving some submissions. Survey responses and analysis of re-submissions\nindicate that authors made substantive revisions to their submissions in\nresponse to specific feedback from the LLM. The experiment also highlights\ncommon issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)\nwere the most frequent issues flagged by authors. We also conduct experiments\nto understand potential gaming of the system, which reveal that the assistant\ncould be manipulated to enhance scores through fabricated justifications,\nhighlighting potential vulnerabilities of automated review tools.", "published": "2024-11-05 18:58:00", "link": "http://arxiv.org/abs/2411.03417v2", "categories": ["cs.CL", "cs.DL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer\n  Redundancy", "abstract": "This paper introduces a novel model compression approach through dynamic\nlayer-specific pruning in Large Language Models (LLMs), enhancing the\ntraditional methodology established by SliceGPT. By transitioning from constant\nto dynamic slicing, our method leverages the newly proposed Layer Redundancy\n(LR) score, which assesses how much change each layer changes its input by\nmeasuring the cosine similarity of the input to the output of the layer. We use\nthis score to prune parts of individual layers based on redundancy in such a\nway that the average pruned percentage for all layers is a fixed value. We\nconducted extensive experiments using models like Llama3-8B and Mistral-7B on\nmultiple datasets, evaluating different slicing bases and percentages to\ndetermine optimal configurations that balance efficiency and performance. Our\nfindings show that our dynamic slicing approach not only maintains but, in many\ncases, enhances model performance compared to the baseline established by\nconstant slicing methods. For instance, in several settings, we see performance\nimprovements of up to 5% over the SliceGPT baseline. Additionally, a perplexity\ndecrease by as much as 7% was observed across multiple benchmarks, validating\nthe effectiveness of our method. The code, model weights, and datasets are\nopen-sourced at https://github.com/RazvanDu/DynamicSlicing.", "published": "2024-11-05 21:19:49", "link": "http://arxiv.org/abs/2411.03513v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "primary_category": "cs.CL"}
{"title": "Unified Pathological Speech Analysis with Prompt Tuning", "abstract": "Pathological speech analysis has been of interest in the detection of certain\ndiseases like depression and Alzheimer's disease and attracts much interest\nfrom researchers. However, previous pathological speech analysis models are\ncommonly designed for a specific disease while overlooking the connection\nbetween diseases, which may constrain performance and lower training\nefficiency. Instead of fine-tuning deep models for different tasks, prompt\ntuning is a much more efficient training paradigm. We thus propose a unified\npathological speech analysis system for as many as three diseases with the\nprompt tuning technique. This system uses prompt tuning to adjust only a small\npart of the parameters to detect different diseases from speeches of possible\npatients. Our system leverages a pre-trained spoken language model and\ndemonstrates strong performance across multiple disorders while only\nfine-tuning a fraction of the parameters. This efficient training approach\nleads to faster convergence and improved F1 scores by allowing knowledge to be\nshared across tasks. Our experiments on Alzheimer's disease, Depression, and\nParkinson's disease show competitive results, highlighting the effectiveness of\nour method in pathological speech analysis.", "published": "2024-11-05 06:47:26", "link": "http://arxiv.org/abs/2411.04142v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RT-Affordance: Affordances are Versatile Intermediate Representations\n  for Robot Manipulation", "abstract": "We explore how intermediate policy representations can facilitate\ngeneralization by providing guidance on how to perform manipulation tasks.\nExisting representations such as language, goal images, and trajectory sketches\nhave been shown to be helpful, but these representations either do not provide\nenough context or provide over-specified context that yields less robust\npolicies. We propose conditioning policies on affordances, which capture the\npose of the robot at key stages of the task. Affordances offer expressive yet\nlightweight abstractions, are easy for users to specify, and facilitate\nefficient learning by transferring knowledge from large internet datasets. Our\nmethod, RT-Affordance, is a hierarchical model that first proposes an\naffordance plan given the task language, and then conditions the policy on this\naffordance plan to perform manipulation. Our model can flexibly bridge\nheterogeneous sources of supervision including large web datasets and robot\ntrajectories. We additionally train our model on cheap-to-collect in-domain\naffordance images, allowing us to learn new tasks without collecting any\nadditional costly robot trajectories. We show on a diverse set of novel tasks\nhow RT-Affordance exceeds the performance of existing methods by over 50%, and\nwe empirically demonstrate that affordances are robust to novel settings.\nVideos available at https://snasiriany.me/rt-affordance", "published": "2024-11-05 01:02:51", "link": "http://arxiv.org/abs/2411.02704v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual\n  Visual Answer Localization", "abstract": "The goal of Multilingual Visual Answer Localization (MVAL) is to locate a\nvideo segment that answers a given multilingual question. Existing methods\neither focus solely on visual modality or integrate visual and subtitle\nmodalities. However, these methods neglect the audio modality in videos,\nconsequently leading to incomplete input information and poor performance in\nthe MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span\nLocalization (AVTSL) method that incorporates audio modality to augment both\nvisual and textual representations for the MVAL task. Specifically, we\nintegrate features from three modalities and develop three predictors, each\ntailored to the unique contributions of the fused modalities: an audio-visual\npredictor, a visual predictor, and a textual predictor. Each predictor\ngenerates predictions based on its respective modality. To maintain consistency\nacross the predicted results, we introduce an Audio-Visual-Textual Consistency\nmodule. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing\neach modality's predictor to dynamically learn from the others. This\ncollaborative learning ensures that the model generates consistent and\ncomprehensive answers. Extensive experiments show that our proposed method\noutperforms several state-of-the-art (SOTA) methods, which demonstrates the\neffectiveness of the audio modality.", "published": "2024-11-05 06:49:14", "link": "http://arxiv.org/abs/2411.02851v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Membership Inference Attacks against Large Vision-Language Models", "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA.", "published": "2024-11-05 08:35:08", "link": "http://arxiv.org/abs/2411.02902v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Solving Trojan Detection Competitions with Linear Weight Classification", "abstract": "Neural networks can conceal malicious Trojan backdoors that allow a trigger\nto covertly change the model behavior. Detecting signs of these backdoors,\nparticularly without access to any triggered data, is the subject of ongoing\nresearch and open challenges. In one common formulation of the problem, we are\ngiven a set of clean and poisoned models and need to predict whether a given\ntest model is clean or poisoned. In this paper, we introduce a detector that\nworks remarkably well across many of the existing datasets and domains. It is\nobtained by training a binary classifier on a large number of models' weights\nafter performing a few different pre-processing steps including feature\nselection and standardization, reference model weights subtraction, and model\nalignment prior to detection. We evaluate this algorithm on a diverse set of\nTrojan detection benchmarks and domains and examine the cases where the\napproach is most and least effective.", "published": "2024-11-05 19:00:34", "link": "http://arxiv.org/abs/2411.03445v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Language Models and Cycle Consistency for Self-Reflective Machine\n  Translation", "abstract": "This paper introduces a novel framework that leverages large language models\n(LLMs) for machine translation (MT). We start with one conjecture: an ideal\ntranslation should contain complete and accurate information for a strong\nenough LLM to recover the original sentence. We generate multiple translation\ncandidates from a source language A to a target language B, and subsequently\ntranslate these candidates back to the original language A. By evaluating the\ncycle consistency between the original and back-translated sentences using\nmetrics such as token-level precision and accuracy, we implicitly estimate the\ntranslation quality in language B, without knowing its ground-truth. This also\nhelps to evaluate the LLM translation capability, only with monolingual\ncorpora. For each source sentence, we identify the translation candidate with\noptimal cycle consistency with the original sentence as the final answer. Our\nexperiments demonstrate that larger LLMs, or the same LLM with more forward\npasses during inference, exhibit increased cycle consistency, aligning with the\nLLM model size scaling law and test-time computation scaling law. This work\nprovide methods for, 1) to implicitly evaluate translation quality of a\nsentence in the target language, 2), to evaluate capability of LLM for\nany-to-any-language translation, and 3), how to generate a better translation\nfor a specific LLM.", "published": "2024-11-05 04:01:41", "link": "http://arxiv.org/abs/2411.02791v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Noise-Robust Hearing Aid Voice Control", "abstract": "Advancing the design of robust hearing aid (HA) voice control is crucial to\nincrease the HA use rate among hard of hearing people as well as to improve HA\nusers' experience. In this work, we contribute towards this goal by, first,\npresenting a novel HA speech dataset consisting of noisy own voice captured by\n2 behind-the-ear (BTE) and 1 in-ear-canal (IEC) microphones. Second, we provide\nbaseline HA voice control results from the evaluation of light,\nstate-of-the-art keyword spotting models utilizing different combinations of HA\nmicrophone signals. Experimental results show the benefits of exploiting\nbandwidth-limited bone-conducted speech (BCS) from the IEC microphone to\nachieve noise-robust HA voice control. Furthermore, results also demonstrate\nthat voice control performance can be boosted by assisting BCS by the\nbroader-bandwidth BTE microphone signals. Aiming at setting a baseline upon\nwhich the scientific community can continue to progress, the HA noisy speech\ndataset has been made publicly available.", "published": "2024-11-05 14:49:05", "link": "http://arxiv.org/abs/2411.03150v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Reference Microphone Selection for the Weighted Prediction Error\n  Algorithm using the Normalized L-p Norm", "abstract": "Reverberation may severely degrade the quality of speech signals recorded\nusing microphones in a room. For compact microphone arrays, the choice of the\nreference microphone for multi-microphone dereverberation typically does not\nhave a large influence on the dereverberation performance. In contrast, when\nthe microphones are spatially distributed, the choice of the reference\nmicrophone may significantly contribute to the dereverberation performance. In\nthis paper, we propose to perform reference microphone selection for the\nweighted prediction error (WPE) dereverberation algorithm based on the\nnormalized $\\ell_p$-norm of the dereverberated output signal. Experimental\nresults for different source positions in a reverberant laboratory show that\nthe proposed method yields a better dereverberation performance than reference\nmicrophone selection based on the early-to-late reverberation ratio or signal\npower.", "published": "2024-11-05 15:17:26", "link": "http://arxiv.org/abs/2411.03168v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Multi-View Learning for Disentangled Music Audio\n  Representations", "abstract": "Self-supervised learning (SSL) offers a powerful way to learn robust,\ngeneralizable representations without labeled data. In music, where labeled\ndata is scarce, existing SSL methods typically use generated supervision and\nmulti-view redundancy to create pretext tasks. However, these approaches often\nproduce entangled representations and lose view-specific information. We\npropose a novel self-supervised multi-view learning framework for audio\ndesigned to incentivize separation between private and shared representation\nspaces. A case study on audio disentanglement in a controlled setting\ndemonstrates the effectiveness of our method.", "published": "2024-11-05 01:21:28", "link": "http://arxiv.org/abs/2411.02711v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-Time Scream Detection and Position Estimation for Worker Safety in\n  Construction Sites", "abstract": "The construction industry faces high risks due to frequent accidents, often\nleaving workers in perilous situations where rapid response is critical.\nTraditional safety monitoring methods, including wearable sensors and GPS,\noften fail under obstructive or indoor conditions. This research introduces a\nnovel real-time scream detection and localization system tailored for\nconstruction sites, especially in low-resource environments. Integrating\nWav2Vec2 and Enhanced ConvNet models for accurate scream detection, coupled\nwith the GCC-PHAT algorithm for robust time delay estimation under reverberant\nconditions, followed by a gradient descent-based approach to achieve precise\nposition estimation in noisy environments. Our approach combines these concepts\nto achieve high detection accuracy and rapid localization, thereby minimizing\nfalse alarms and optimizing emergency response. Preliminary results demonstrate\nthat the system not only accurately detects distress calls amidst construction\nnoise but also reliably identifies the caller's location. This solution\nrepresents a substantial improvement in worker safety, with the potential for\nwidespread application across high-risk occupational environments. The scripts\nused for training, evaluation of scream detection, position estimation, and\nintegrated framework will be released at:\nhttps://github.com/Anmol2059/construction_safety.", "published": "2024-11-05 11:37:13", "link": "http://arxiv.org/abs/2411.03016v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised detection and classification of heartbeats using the\n  dissimilarity matrix in PCG signals", "abstract": "The proposed system consists of a two-stage cascade. The first stage performs\na rough heartbeat detection while the second stage refines the previous one,\nimproving the temporal localization and also classifying the heartbeats into\ntypes S1 and S2. The first contribution is a novel approach that combines the\ndissimilarity matrix with the frame-level spectral divergence to locate\nheartbeats using the repetitiveness shown by the heart sounds and the temporal\nrelationships between the intervals defined by the events S1/S2 and non-S1/S2\n(systole and diastole). The second contribution is a\nverification-correction-classification process based on a sliding window that\nallows the preservation of the temporal structure of the cardiac cycle in order\nto be applied in the heart sound classification. The proposed method has been\nassessed using the open access databases PASCAL, CirCor DigiScope\nPhonocardiogram and an additional sound mixing procedure considering both\nAdditive White Gaussian Noise (AWGN) and different kinds of clinical ambient\nnoises from a commercial database. The proposed method provides the best\ndetection/classification performance in realistic scenarios where the presence\nof cardiac anomalies as well as different types of clinical environmental\nnoises are active in the PCG signal. Of note, the promising modelling of the\ntemporal structures of the heart provided by the dissimilarity matrix together\nwith the frame-level spectral divergence, as well as the removal of a\nsignificant number of spurious heart events and recovery of missing heart\nevents, both corrected by the proposed verification-correction-classification\nalgorithm, suggest that our proposal is a successful tool to be applied in\nheart segmentation.", "published": "2024-11-05 12:48:16", "link": "http://arxiv.org/abs/2411.03061v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DEMONet: Underwater Acoustic Target Recognition based on Multi-Expert\n  Network and Cross-Temporal Variational Autoencoder", "abstract": "Building a robust underwater acoustic recognition system in real-world\nscenarios is challenging due to the complex underwater environment and the\ndynamic motion states of targets. A promising optimization approach is to\nleverage the intrinsic physical characteristics of targets, which remain\ninvariable regardless of environmental conditions, to provide robust insights.\nHowever, our study reveals that while physical characteristics exhibit robust\nproperties, they may lack class-specific discriminative patterns. Consequently,\ndirectly incorporating physical characteristics into model training can\npotentially introduce unintended inductive biases, leading to performance\ndegradation. To utilize the benefits of physical characteristics while\nmitigating possible detrimental effects, we propose DEMONet in this study,\nwhich utilizes the detection of envelope modulation on noise (DEMON) to provide\nrobust insights into the shaft frequency or blade counts of targets. DEMONet is\na multi-expert network that allocates various underwater signals to their\nbest-matched expert layer based on DEMON spectra for fine-grained signal\nprocessing. Thereinto, DEMON spectra are solely responsible for providing\nimplicit physical characteristics without establishing a mapping relationship\nwith the target category. Furthermore, to mitigate noise and spurious\nmodulation spectra in DEMON features, we introduce a cross-temporal alignment\nstrategy and employ a variational autoencoder (VAE) to reconstruct\nnoise-resistant DEMON spectra to replace the raw DEMON features. The\neffectiveness of the proposed DEMONet with cross-temporal VAE was primarily\nevaluated on the DeepShip dataset and our proprietary datasets. Experimental\nresults demonstrated that our approach could achieve state-of-the-art\nperformance on both datasets.", "published": "2024-11-05 03:04:51", "link": "http://arxiv.org/abs/2411.02758v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing Robust Underwater Acoustic Target Recognition through\n  Multi-task Learning and Multi-Gate Mixture-of-Experts", "abstract": "Underwater acoustic target recognition has emerged as a prominent research\narea within the field of underwater acoustics. However, the current\navailability of authentic underwater acoustic signal recordings remains\nlimited, which hinders data-driven acoustic recognition models from learning\nrobust patterns of targets from a limited set of intricate underwater signals,\nthereby compromising their stability in practical applications. To overcome\nthese limitations, this study proposes a recognition framework called M3\n(Multi-task, Multi-gate, Multi-expert) to enhance the model's ability to\ncapture robust patterns by making it aware of the inherent properties of\ntargets. In this framework, an auxiliary task that focuses on target\nproperties, such as estimating target size, is designed. The auxiliary task\nthen shares parameters with the recognition task to realize multi-task\nlearning. This paradigm allows the model to concentrate on shared information\nacross tasks and identify robust patterns of targets in a regularized manner,\nthereby enhancing the model's generalization ability. Moreover, M3 incorporates\nmulti-expert and multi-gate mechanisms, allowing for the allocation of distinct\nparameter spaces to various underwater signals. This enables the model to\nprocess intricate signal patterns in a fine-grained and differentiated manner.\nTo evaluate the effectiveness of M3, extensive experiments were implemented on\nthe ShipsEar underwater ship-radiated noise dataset. The results substantiate\nthat M3 has the ability to outperform the most advanced single-task recognition\nmodels, thereby achieving the state-of-the-art performance.", "published": "2024-11-05 03:52:36", "link": "http://arxiv.org/abs/2411.02787v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for\n  Feature Extraction Using Wav2Vec2 and HuBERT", "abstract": "Speech is the most natural way of expressing ourselves as humans. Identifying\nemotion from speech is a nontrivial task due to the ambiguous definition of\nemotion itself. Speaker Emotion Recognition (SER) is essential for\nunderstanding human emotional behavior. The SER task is challenging due to the\nvariety of speakers, background noise, complexity of emotions, and speaking\nstyles. It has many applications in education, healthcare, customer service,\nand Human-Computer Interaction (HCI). Previously, conventional machine learning\nmethods such as SVM, HMM, and KNN have been used for the SER task. In recent\nyears, deep learning methods have become popular, with convolutional neural\nnetworks and recurrent neural networks being used for SER tasks. The input of\nthese methods is mostly spectrograms and hand-crafted features. In this work,\nwe study the use of self-supervised transformer-based models, Wav2Vec2 and\nHuBERT, to determine the emotion of speakers from their voice. The models\nautomatically extract features from raw audio signals, which are then used for\nthe classification task. The proposed solution is evaluated on reputable\ndatasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show\nthe effectiveness of the proposed method on different datasets. Moreover, the\nmodel has been used for real-world applications like call center conversations,\nand the results demonstrate that the model accurately predicts emotions.", "published": "2024-11-05 10:06:40", "link": "http://arxiv.org/abs/2411.02964v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Separation with Pretrained Frontend to Minimize Domain Mismatch", "abstract": "Speech separation seeks to separate individual speech signals from a speech\nmixture. Typically, most separation models are trained on synthetic data due to\nthe unavailability of target reference in real-world cocktail party scenarios.\nAs a result, there exists a domain gap between real and synthetic data when\ndeploying speech separation models in real-world applications. In this paper,\nwe propose a self-supervised domain-invariant pretrained (DIP) frontend that is\nexposed to mixture data without the need for target reference speech. The DIP\nfrontend utilizes a Siamese network with two innovative pretext tasks, mixture\npredictive coding (MPC) and mixture invariant coding (MIC), to capture shared\ncontextual cues between real and synthetic unlabeled mixtures. Subsequently, we\nfreeze the DIP frontend as a feature extractor when training the downstream\nspeech separation models on synthetic data. By pretraining the DIP frontend\nwith the contextual cues, we expect that the speech separation skills learned\nfrom synthetic data can be effectively transferred to real data. To benefit\nfrom the DIP frontend, we introduce a novel separation pipeline to align the\nfeature resolution of the separation models. We evaluate the speech separation\nquality on standard benchmarks and real-world datasets. The results confirm the\nsuperiority of our DIP frontend over existing speech separation models. This\nstudy underscores the potential of large-scale pretraining to enhance the\nquality and intelligibility of speech separation in real-world applications.", "published": "2024-11-05 13:30:27", "link": "http://arxiv.org/abs/2411.03085v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues", "abstract": "TSE(Target Speaker Extraction) aims to extract the clean speech of the target\nspeaker in an audio mixture, thus eliminating irrelevant background noise and\nspeech. While prior work has explored various auxiliary cues including\npre-recorded speech, visual information (e.g., lip motions and gestures), and\nspatial information, the acquisition and selection of such strong cues are\ninfeasible in many practical scenarios. Unlike all existing work, in this\npaper, we condition the TSE algorithm on semantic cues extracted from limited\nand unaligned text content, such as condensed points from a presentation slide.\nThis method is particularly useful in scenarios like meetings, poster sessions,\nor lecture presentations, where acquiring other cues in real-time is\nchallenging. To this end, we design two different networks. Specifically, our\nproposed TPE fuses audio features with content-based semantic cues to\nfacilitate time-frequency mask generation to filter out extraneous noise, while\nanother proposal, namely TSR, employs the contrastive learning technique to\nassociate blindly separated speech signals with semantic cues. The experimental\nresults show the efficacy in accurately identifying the target speaker by\nutilizing semantic cues derived from limited and unaligned text, resulting in\nSI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150,\nrespectively. Dataset and source code will be publicly available. Project demo\npage: https://slideTSE.github.io/.", "published": "2024-11-05 13:56:44", "link": "http://arxiv.org/abs/2411.03109v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Estimation of Sub-band Acoustic Parameters from Ambisonics\n  Recordings using Spectro-Spatial Covariance Features", "abstract": "Estimating frequency-varying acoustic parameters is essential for enhancing\nimmersive perception in realistic spatial audio creation. In this paper, we\npropose a unified framework that blindly estimates reverberation time (T60),\ndirect-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands\nusing first-order Ambisonics (FOA) speech recordings as inputs. The proposed\nframework utilizes a novel feature named Spectro-Spatial Covariance Vector\n(SSCV), efficiently representing temporal, spectral as well as spatial\ninformation of the FOA signal. Our models significantly outperform existing\nsingle-channel methods with only spectral information, reducing estimation\nerrors by more than half for all three acoustic parameters. Additionally, we\nintroduce FOA-Conv3D, a novel back-end network for effectively utilising the\nSSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the\nconvolutional neural network (CNN) and recurrent convolutional neural network\n(CRNN) backends, achieving lower estimation errors and accounting for a higher\nproportion of variance (PoV) for all 3 acoustic parameters.", "published": "2024-11-05 15:20:23", "link": "http://arxiv.org/abs/2411.03172v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Continual Audio-Visual Sound Separation", "abstract": "In this paper, we introduce a novel continual audio-visual sound separation\ntask, aiming to continuously separate sound sources for new classes while\npreserving performance on previously learned classes, with the aid of visual\nguidance. This problem is crucial for practical visually guided auditory\nperception as it can significantly enhance the adaptability and robustness of\naudio-visual sound separation models, making them more applicable for\nreal-world scenarios where encountering new sound sources is commonplace. The\ntask is inherently challenging as our models must not only effectively utilize\ninformation from both modalities in current tasks but also preserve their\ncross-modal association in old tasks to mitigate catastrophic forgetting during\naudio-visual continual learning. To address these challenges, we propose a\nnovel approach named ContAV-Sep (\\textbf{Cont}inual\n\\textbf{A}udio-\\textbf{V}isual Sound \\textbf{Sep}aration). ContAV-Sep presents\na novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the\ncross-modal semantic similarity through incremental tasks and retain previously\nacquired knowledge of semantic similarity in old models, mitigating the risk of\ncatastrophic forgetting. The CrossSDC can seamlessly integrate into the\ntraining process of different audio-visual sound separation frameworks.\nExperiments demonstrate that ContAV-Sep can effectively mitigate catastrophic\nforgetting and achieve significantly better performance compared to other\ncontinual learning baselines for audio-visual sound separation. Code is\navailable at: \\url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.", "published": "2024-11-05 07:09:14", "link": "http://arxiv.org/abs/2411.02860v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
