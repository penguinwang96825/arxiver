{"title": "Modelling Domain Relationships for Transfer Learning on Retrieval-based\n  Question Answering Systems in E-commerce", "abstract": "In this paper, we study transfer learning for the PI and NLI problems, aiming\nto propose a general framework, which can effectively and efficiently adapt the\nshared knowledge learned from a resource-rich source domain to a resource- poor\ntarget domain. Specifically, since most existing transfer learning methods only\nfocus on learning a shared feature space across domains while ignoring the\nrelationship between the source and target domains, we propose to\nsimultaneously learn shared representations and domain relationships in a\nunified framework. Furthermore, we propose an efficient and effective hybrid\nmodel by combining a sentence encoding- based method and a sentence\ninteraction-based method as our base model. Extensive experiments on both\nparaphrase identification and natural language inference demonstrate that our\nbase model is efficient and has promising performance compared to the competing\nmodels, and our transfer learning method can help to significantly boost the\nperformance. Further analysis shows that the inter-domain and intra-domain\nrelationship captured by our model are insightful. Last but not least, we\ndeploy our transfer learning model for PI into our online chatbot system, which\ncan bring in significant improvements over our existing system. Finally, we\nlaunch our new system on the chatbot platform Eva in our E-commerce site\nAliExpress.", "published": "2017-11-23 15:00:43", "link": "http://arxiv.org/abs/1711.08726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPINE: SParse Interpretable Neural Embeddings", "abstract": "Prediction without justification has limited utility. Much of the success of\nneural models can be attributed to their ability to learn rich, dense and\nexpressive representations. While these representations capture the underlying\ncomplexity and latent trends in the data, they are far from being\ninterpretable. We propose a novel variant of denoising k-sparse autoencoders\nthat generates highly efficient and interpretable distributed word\nrepresentations (word embeddings), beginning with existing word representations\nfrom state-of-the-art methods like GloVe and word2vec. Through large scale\nhuman evaluation, we report that our resulting word embedddings are much more\ninterpretable than the original GloVe and word2vec embeddings. Moreover, our\nembeddings outperform existing popular word embeddings on a diverse suite of\nbenchmark downstream tasks.", "published": "2017-11-23 18:00:29", "link": "http://arxiv.org/abs/1711.08792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Accuracy of Pre-trained Word Embeddings for Sentiment\n  Analysis", "abstract": "Sentiment analysis is one of the well-known tasks and fast growing research\nareas in natural language processing (NLP) and text classifications. This\ntechnique has become an essential part of a wide range of applications\nincluding politics, business, advertising and marketing. There are various\ntechniques for sentiment analysis, but recently word embeddings methods have\nbeen widely used in sentiment classification tasks. Word2Vec and GloVe are\ncurrently among the most accurate and usable word embedding methods which can\nconvert words into meaningful vectors. However, these methods ignore sentiment\ninformation of texts and need a huge corpus of texts for training and\ngenerating exact vectors which are used as inputs of deep learning models. As a\nresult, because of the small size of some corpuses, researcher often have to\nuse pre-trained word embeddings which were trained on other large text corpus\nsuch as Google News with about 100 billion words. The increasing accuracy of\npre-trained word embeddings has a great impact on sentiment analysis research.\nIn this paper we propose a novel method, Improved Word Vectors (IWV), which\nincreases the accuracy of pre-trained word embeddings in sentiment analysis.\nOur method is based on Part-of-Speech (POS) tagging techniques, lexicon-based\napproaches and Word2Vec/GloVe methods. We tested the accuracy of our method via\ndifferent deep learning models and sentiment datasets. Our experiment results\nshow that Improved Word Vectors (IWV) are very effective for sentiment\nanalysis.", "published": "2017-11-23 08:25:23", "link": "http://arxiv.org/abs/1711.08609v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Counterfactual Learning for Machine Translation: Degeneracies and\n  Solutions", "abstract": "Counterfactual learning is a natural scenario to improve web-based machine\ntranslation services by offline learning from feedback logged during user\ninteractions. In order to avoid the risk of showing inferior translations to\nusers, in such scenarios mostly exploration-free deterministic logging policies\nare in place. We analyze possible degeneracies of inverse and reweighted\npropensity scoring estimators, in stochastic and deterministic settings, and\nrelate them to recently proposed techniques for counterfactual learning under\ndeterministic logging.", "published": "2017-11-23 08:54:05", "link": "http://arxiv.org/abs/1711.08621v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Singing voice correction using canonical time warping", "abstract": "Expressive singing voice correction is an appealing but challenging problem.\nA robust time-warping algorithm which synchronizes two singing recordings can\nprovide a promising solution. We thereby propose to address the problem by\ncanonical time warping (CTW) which aligns amateur singing recordings to\nprofessional ones. A new pitch contour is generated given the alignment\ninformation, and a pitch-corrected singing is synthesized back through the\nvocoder. The objective evaluation shows that CTW is robust against\npitch-shifting and time-stretching effects, and the subjective test\ndemonstrates that CTW prevails the other methods including DTW and the\ncommercial auto-tuning software. Finally, we demonstrate the applicability of\nthe proposed method in a practical, real-world scenario.", "published": "2017-11-23 07:45:25", "link": "http://arxiv.org/abs/1711.08600v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Visual Speech Enhancement", "abstract": "When video is shot in noisy environment, the voice of a speaker seen in the\nvideo can be enhanced using the visible mouth movements, reducing background\nnoise. While most existing methods use audio-only inputs, improved performance\nis obtained with our visual speech enhancement, based on an audio-visual neural\nnetwork. We include in the training data videos to which we added the voice of\nthe target speaker as background noise. Since the audio input is not sufficient\nto separate the voice of a speaker from his own voice, the trained model better\nexploits the visual input and generalizes well to different noise types. The\nproposed model outperforms prior audio visual methods on two public lipreading\ndatasets. It is also the first to be demonstrated on a dataset not designed for\nlipreading, such as the weekly addresses of Barack Obama.", "published": "2017-11-23 17:51:46", "link": "http://arxiv.org/abs/1711.08789v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
