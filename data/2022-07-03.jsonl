{"title": "Generating Repetitions with Appropriate Repeated Words", "abstract": "A repetition is a response that repeats words in the previous speaker's\nutterance in a dialogue. Repetitions are essential in communication to build\ntrust with others, as investigated in linguistic studies. In this work, we\nfocus on repetition generation. To the best of our knowledge, this is the first\nneural approach to address repetition generation. We propose Weighted Label\nSmoothing, a smoothing method for explicitly learning which words to repeat\nduring fine-tuning, and a repetition scoring method that can output more\nappropriate repetitions during decoding. We conducted automatic and human\nevaluations involving applying these methods to the pre-trained language model\nT5 for generating repetitions. The experimental results indicate that our\nmethods outperformed baselines in both evaluations.", "published": "2022-07-03 01:21:49", "link": "http://arxiv.org/abs/2207.00929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Survey on Long Document Summarization: Datasets, Models and\n  Metrics", "abstract": "Long documents such as academic articles and business reports have been the\nstandard format to detail out important issues and complicated subjects that\nrequire extra attention. An automatic summarization system that can effectively\ncondense long documents into short and concise texts to encapsulate the most\nimportant information would thus be significant in aiding the reader's\ncomprehension. Recently, with the advent of neural architectures, significant\nresearch efforts have been made to advance automatic text summarization\nsystems, and numerous studies on the challenges of extending these systems to\nthe long document domain have emerged. In this survey, we provide a\ncomprehensive overview of the research on long document summarization and a\nsystematic evaluation across the three principal components of its research\nsetting: benchmark datasets, summarization models, and evaluation metrics. For\neach component, we organize the literature within the context of long document\nsummarization and conduct an empirical analysis to broaden the perspective on\ncurrent research progress. The empirical analysis includes a study on the\nintrinsic characteristics of benchmark datasets, a multi-dimensional analysis\nof summarization models, and a review of the summarization evaluation metrics.\nBased on the overall findings, we conclude by proposing possible directions for\nfuture exploration in this rapidly growing field.", "published": "2022-07-03 02:57:22", "link": "http://arxiv.org/abs/2207.00939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Tieq Viet with Deep Learning Models", "abstract": "Deep learning is a powerful approach in recovering lost information as well\nas harder inverse function computation problems. When applied in natural\nlanguage processing, this approach is essentially making use of context as a\nmean to recover information through likelihood maximization. Not long ago, a\nlinguistic study called Tieq Viet was controversial among both researchers and\nsociety. We find this a great example to demonstrate the ability of deep\nlearning models to recover lost information. In the proposal of Tieq Viet, some\nconsonants in the standard Vietnamese are replaced. A sentence written in this\nproposal can be interpreted into multiple sentences in the standard version,\nwith different meanings. The hypothesis that we want to test is whether a deep\nlearning model can recover the lost information if we translate the text from\nVietnamese to Tieq Viet.", "published": "2022-07-03 08:05:57", "link": "http://arxiv.org/abs/2207.00975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-aspect Multilingual and Cross-lingual Parliamentary Speech\n  Analysis", "abstract": "Parliamentary and legislative debate transcripts provide informative insight\ninto elected politicians' opinions, positions, and policy preferences. They are\ninteresting for political and social sciences as well as linguistics and\nnatural language processing (NLP) research. While existing research studied\nindividual parliaments, we apply advanced NLP methods to a joint and\ncomparative analysis of six national parliaments (Bulgarian, Czech, French,\nSlovene, Spanish, and United Kingdom) between 2017 and 2020. We analyze\nemotions and sentiment in the transcripts from the ParlaMint dataset collection\nand assess if the age, gender, and political orientation of speakers can be\ndetected from their speeches. The results show some commonalities and many\nsurprising differences among the analyzed countries.", "published": "2022-07-03 14:31:32", "link": "http://arxiv.org/abs/2207.01054v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation", "abstract": "End-to-end speech-to-text translation models are often initialized with\npre-trained speech encoder and pre-trained text decoder. This leads to a\nsignificant training gap between pre-training and fine-tuning, largely due to\nthe modality differences between speech outputs from the encoder and text\ninputs to the decoder. In this work, we aim to bridge the modality gap between\nspeech and text to improve translation quality. We propose M-Adapter, a novel\nTransformer-based module, to adapt speech representations to text. While\nshrinking the speech sequence, M-Adapter produces features desired for\nspeech-to-text translation via modelling global and local dependencies of a\nspeech sequence. Our experimental results show that our model outperforms a\nstrong baseline by up to 1 BLEU score on the Must-C En$\\rightarrow$DE\ndataset.\\footnote{Our code is available at\nhttps://github.com/mingzi151/w2v2-st.}", "published": "2022-07-03 04:26:53", "link": "http://arxiv.org/abs/2207.00952v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mental Illness Classification on Social Media Texts using Deep Learning\n  and Transfer Learning", "abstract": "Given the current social distance restrictions across the world, most\nindividuals now use social media as their major medium of communication.\nMillions of people suffering from mental diseases have been isolated due to\nthis, and they are unable to get help in person. They have become more reliant\non online venues to express themselves and seek advice on dealing with their\nmental disorders. According to the World health organization (WHO),\napproximately 450 million people are affected. Mental illnesses, such as\ndepression, anxiety, etc., are immensely common and have affected an\nindividuals' physical health. Recently Artificial Intelligence (AI) methods\nhave been presented to help mental health providers, including psychiatrists\nand psychologists, in decision making based on patients' authentic information\n(e.g., medical records, behavioral data, social media utilization, etc.). AI\ninnovations have demonstrated predominant execution in numerous real-world\napplications broadening from computer vision to healthcare. This study analyzes\nunstructured user data on the Reddit platform and classifies five common mental\nillnesses: depression, anxiety, bipolar disorder, ADHD, and PTSD. We trained\ntraditional machine learning, deep learning, and transfer learning multi-class\nmodels to detect mental disorders of individuals. This effort will benefit the\npublic health system by automating the detection process and informing\nappropriate authorities about people who require emergency assistance.", "published": "2022-07-03 11:33:52", "link": "http://arxiv.org/abs/2207.01012v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Leveraging Acoustic Contextual Representation by Audio-textual\n  Cross-modal Learning for Conversational ASR", "abstract": "Leveraging context information is an intuitive idea to improve performance on\nconversational automatic speech recognition(ASR). Previous works usually adopt\nrecognized hypotheses of historical utterances as preceding context, which may\nbias the current recognized hypothesis due to the inevitable\nhistoricalrecognition errors. To avoid this problem, we propose an\naudio-textual cross-modal representation extractor to learn contextual\nrepresentations directly from preceding speech. Specifically, it consists of\ntwo modal-related encoders, extracting high-level latent features from speech\nand the corresponding text, and a cross-modal encoder, which aims to learn the\ncorrelation between speech and text. We randomly mask some input tokens and\ninput sequences of each modality. Then a token-missing or modal-missing\nprediction with a modal-level CTC loss on the cross-modal encoder is performed.\nThus, the model captures not only the bi-directional context dependencies in a\nspecific modality but also relationships between different modalities. Then,\nduring the training of the conversational ASR system, the extractor will be\nfrozen to extract the textual representation of preceding speech, while such\nrepresentation is used as context fed to the ASR decoder through attention\nmechanism. The effectiveness of the proposed approach is validated on several\nMandarin conversation corpora and the highest character error rate (CER)\nreduction up to 16% is achieved on the MagicData dataset.", "published": "2022-07-03 13:32:24", "link": "http://arxiv.org/abs/2207.01039v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech", "abstract": "The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.", "published": "2022-07-03 15:07:41", "link": "http://arxiv.org/abs/2207.01063v3", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Can Language Understand Depth?", "abstract": "Besides image classification, Contrastive Language-Image Pre-training (CLIP)\nhas accomplished extraordinary success for a wide range of vision tasks,\nincluding object-level and 3D space understanding. However, it's still\nchallenging to transfer semantic knowledge learned from CLIP into more\nintricate tasks of quantified targets, such as depth estimation with geometric\ninformation. In this paper, we propose to apply CLIP for zero-shot monocular\ndepth estimation, named DepthCLIP. We found that the patches of the input image\ncould respond to a certain semantic distance token and then be projected to a\nquantified depth bin for coarse estimation. Without any training, our DepthCLIP\nsurpasses existing unsupervised methods and even approaches the early\nfully-supervised networks. To our best knowledge, we are the first to conduct\nzero-shot adaptation from the semantic language knowledge to quantified\ndownstream tasks and perform zero-shot monocular depth estimation. We hope our\nwork could cast a light on future research. The code is available at\nhttps://github.com/Adonis-galaxy/DepthCLIP.", "published": "2022-07-03 16:51:11", "link": "http://arxiv.org/abs/2207.01077v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "DiSCoMaT: Distantly Supervised Composition Extraction from Tables in\n  Materials Science Articles", "abstract": "A crucial component in the curation of KB for a scientific domain (e.g.,\nmaterials science, foods & nutrition, fuels) is information extraction from\ntables in the domain's published research articles. To facilitate research in\nthis direction, we define a novel NLP task of extracting compositions of\nmaterials (e.g., glasses) from tables in materials science papers. The task\ninvolves solving several challenges in concert, such as tables that mention\ncompositions have highly varying structures; text in captions and full paper\nneeds to be incorporated along with data in tables; and regular languages for\nnumbers, chemical compounds and composition expressions must be integrated into\nthe model. We release a training dataset comprising 4,408 distantly supervised\ntables, along with 1,475 manually annotated dev and test tables. We also\npresent a strong baseline DISCOMAT, that combines multiple graph neural\nnetworks with several task-specific regular expressions, features, and\nconstraints. We show that DISCOMAT outperforms recent table processing\narchitectures by significant margins.", "published": "2022-07-03 17:11:17", "link": "http://arxiv.org/abs/2207.01079v4", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Hashtags to Analyze Purpose and Technology Application of\n  Open-Source Project Related to COVID-19", "abstract": "COVID-19 has had a profound impact on the lives of all human beings. Emerging\ntechnologies have made significant contributions to the fight against the\npandemic. An extensive review of the application of technology will help\nfacilitate future research and technology development to provide better\nsolutions for future pandemics. In contrast to the extensive surveys of\nacademic communities that have already been conducted, this study explores the\nIT community of practice. Using GitHub as the study target, we analyzed the\nmain functionalities of the projects submitted during the pandemic. This study\nexamines trends in projects with different functionalities and the relationship\nbetween functionalities and technologies. The study results show an imbalance\nin the number of projects with varying functionalities in the GitHub community,\ni.e., applications account for more than half of the projects. In contrast,\nother data analysis and AI projects account for a smaller share. This differs\nsignificantly from the survey of the academic community, where the findings\nfocus more on cutting-edge technologies while projects in the community of\npractice use more mature technologies. The spontaneous behavior of developers\nmay lack organization and make it challenging to target needs.", "published": "2022-07-03 02:37:31", "link": "http://arxiv.org/abs/2207.06219v1", "categories": ["cs.IR", "cs.CL", "cs.SE"], "primary_category": "cs.IR"}
{"title": "ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses\n  to Augmented Urban Soundscapes", "abstract": "Choosing optimal maskers for existing soundscapes to effect a desired\nperceptual change via soundscape augmentation is non-trivial due to extensive\nvarieties of maskers and a dearth of benchmark datasets with which to compare\nand develop soundscape augmentation models. To address this problem, we make\npublicly available the ARAUS (Affective Responses to Augmented Urban\nSoundscapes) dataset, which comprises a five-fold cross-validation set and\nindependent test set totaling 25,440 unique subjective perceptual responses to\naugmented soundscapes presented as audio-visual stimuli. Each augmented\nsoundscape is made by digitally adding \"maskers\" (bird, water, wind, traffic,\nconstruction, or silence) to urban soundscape recordings at fixed\nsoundscape-to-masker ratios. Responses were then collected by asking\nparticipants to rate how pleasant, annoying, eventful, uneventful, vibrant,\nmonotonous, chaotic, calm, and appropriate each augmented soundscape was, in\naccordance with ISO 12913-2:2018. Participants also provided relevant\ndemographic information and completed standard psychological questionnaires. We\nperform exploratory and statistical analysis of the responses obtained to\nverify internal consistency and agreement with known results in the literature.\nFinally, we demonstrate the benchmarking capability of the dataset by training\nand comparing four baseline models for urban soundscape pleasantness: a\nlow-parameter regression model, a high-parameter convolutional neural network,\nand two attention-based networks in the literature.", "published": "2022-07-03 17:09:09", "link": "http://arxiv.org/abs/2207.01078v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Graph Isomorphism Network with Weighted Multiple Aggregators for\n  Speech Emotion Recognition", "abstract": "Speech emotion recognition (SER) is an essential part of human-computer\ninteraction. In this paper, we propose an SER network based on a Graph\nIsomorphism Network with Weighted Multiple Aggregators (WMA-GIN), which can\neffectively handle the problem of information confusion when neighbour nodes'\nfeatures are aggregated together in GIN structure. Moreover, a Full-Adjacent\n(FA) layer is adopted for alleviating the over-squashing problem, which is\nexisted in all Graph Neural Network (GNN) structures, including GIN.\nFurthermore, a multi-phase attention mechanism and multi-loss training strategy\nare employed to avoid missing the useful emotional information in the stacked\nWMA-GIN layers. We evaluated the performance of our proposed WMA-GIN on the\npopular IEMOCAP dataset. The experimental results show that WMA-GIN outperforms\nother GNN-based methods and is comparable to some advanced non-graph-based\nmethods by achieving 72.48% of weighted accuracy (WA) and 67.72% of unweighted\naccuracy (UA).", "published": "2022-07-03 02:58:42", "link": "http://arxiv.org/abs/2207.00940v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Error-Resilient Neural Speech Coding", "abstract": "Neural audio coding has shown very promising results recently in the\nliterature to largely outperform traditional codecs but limited attention has\nbeen paid on its error resilience. Neural codecs trained considering only\nsource coding tend to be extremely sensitive to channel noises, especially in\nwireless channels with high error rate. In this paper, we investigate how to\nelevate the error resilience of neural audio codecs for packet losses that\noften occur during real-time communications. We propose a feature-domain packet\nloss concealment algorithm (FD-PLC) for real-time neural speech coding.\nSpecifically, we introduce a self-attention-based module on the received latent\nfeatures to recover lost frames in the feature domain before the decoder. A\nhybrid segment-level and frame-level frequency-domain discriminator is employed\nto guide the network to focus on both the generative quality of lost frames and\nthe continuity with neighbouring frames. Experimental results on several error\npatterns show that the proposed scheme can achieve better robustness compared\nwith the corresponding error-free and error-resilient baselines. We also show\nthat feature-domain concealment is superior to waveform-domain counterpart as\npost-processing.", "published": "2022-07-03 09:38:30", "link": "http://arxiv.org/abs/2207.00993v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating gender-ambiguous voices for privacy-preserving speech\n  recognition", "abstract": "Our voice encodes a uniquely identifiable pattern which can be used to infer\nprivate attributes, such as gender or identity, that an individual might wish\nnot to reveal when using a speech recognition service. To prevent attribute\ninference attacks alongside speech recognition tasks, we present a generative\nadversarial network, GenGAN, that synthesises voices that conceal the gender or\nidentity of a speaker. The proposed network includes a generator with a U-Net\narchitecture that learns to fool a discriminator. We condition the generator\nonly on gender information and use an adversarial loss between signal\ndistortion and privacy preservation. We show that GenGAN improves the trade-off\nbetween privacy and utility compared to privacy-preserving representation\nlearning methods that consider gender information as a sensitive attribute to\nprotect.", "published": "2022-07-03 14:23:02", "link": "http://arxiv.org/abs/2207.01052v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer functions of FXLMS-based Multi-channel Multi-tone Active Noise\n  Equalizers", "abstract": "Multi-channel Multi-tone Active Noise Equalizers can achieve different\nuser-selected noise spectrum profiles even at different space positions. They\ncan apply a different equalization factor at each noise frequency component and\neach control point. Theoretically, the value of the transfer function at the\nfrequencies where the noise signal has energy is determined by the equalizer\nconfiguration. In this work, we show how to calculate these transfer functions\nwith a double aim: to verify that at the frequencies of interest the values\nimposed by the equalizer settings are obtained, and to characterize the\nbehavior of these transfer functions in the rest of the spectrum, as well as to\nget clues to predict the convergence behaviour of the algorithm. The\ninformation provided thanks to these transfer functions serves as a practical\nalternative to the cumbersome statistical analysis of convergence, whose\nresults are often of no practical use.", "published": "2022-07-03 18:57:35", "link": "http://arxiv.org/abs/2207.01102v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
