{"title": "OCNLI: Original Chinese Natural Language Inference", "abstract": "Despite the tremendous recent progress on natural language inference (NLI),\ndriven largely by large-scale investment in new datasets (e.g., SNLI, MNLI) and\nadvances in modeling, most progress has been limited to English due to a lack\nof reliable datasets for most of the world's languages. In this paper, we\npresent the first large-scale NLI dataset (consisting of ~56,000 annotated\nsentence pairs) for Chinese called the Original Chinese Natural Language\nInference dataset (OCNLI). Unlike recent attempts at extending NLI to other\nlanguages, our dataset does not rely on any automatic translation or non-expert\nannotation. Instead, we elicit annotations from native speakers specializing in\nlinguistics. We follow closely the annotation protocol used for MNLI, but\ncreate new strategies for eliciting diverse hypotheses. We establish several\nbaseline results on our dataset using state-of-the-art pre-trained models for\nChinese, and find even the best performing models to be far outpaced by human\nperformance (~12% absolute performance gap), making it a challenging new\nresource that we hope will help to accelerate progress in Chinese NLU. To the\nbest of our knowledge, this is the first human-elicited MNLI-style corpus for a\nnon-English language.", "published": "2020-10-12 04:25:48", "link": "http://arxiv.org/abs/2010.05444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collective Wisdom: Improving Low-resource Neural Machine Translation\n  using Adaptive Knowledge Distillation", "abstract": "Scarcity of parallel sentence-pairs poses a significant hurdle for training\nhigh-quality Neural Machine Translation (NMT) models in bilingually\nlow-resource scenarios. A standard approach is transfer learning, which\ninvolves taking a model trained on a high-resource language-pair and\nfine-tuning it on the data of the low-resource MT condition of interest.\nHowever, it is not clear generally which high-resource language-pair offers the\nbest transfer learning for the target MT setting. Furthermore, different\ntransferred models may have complementary semantic and/or syntactic strengths,\nhence using only one model may be sub-optimal. In this paper, we tackle this\nproblem using knowledge distillation, where we propose to distill the knowledge\nof ensemble of teacher models to a single student model. As the quality of\nthese teacher models varies, we propose an effective adaptive knowledge\ndistillation approach to dynamically adjust the contribution of the teacher\nmodels during the distillation process. Experiments on transferring from a\ncollection of six language pairs from IWSLT to five low-resource language-pairs\nfrom TED Talks demonstrate the effectiveness of our approach, achieving up to\n+0.9 BLEU score improvement compared to strong baselines.", "published": "2020-10-12 04:26:46", "link": "http://arxiv.org/abs/2010.05445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COGS: A Compositional Generalization Challenge Based on Semantic\n  Interpretation", "abstract": "Natural language is characterized by compositionality: the meaning of a\ncomplex expression is constructed from the meanings of its constituent parts.\nTo facilitate the evaluation of the compositional abilities of language\nprocessing architectures, we introduce COGS, a semantic parsing dataset based\non a fragment of English. The evaluation portion of COGS contains multiple\nsystematic gaps that can only be addressed by compositional generalization;\nthese include new combinations of familiar syntactic structures, or new\ncombinations of familiar words and familiar structures. In experiments with\nTransformers and LSTMs, we found that in-distribution accuracy on the COGS test\nset was near-perfect (96--99%), but generalization accuracy was substantially\nlower (16--35%) and showed high sensitivity to random seed ($\\pm$6--8%). These\nfindings indicate that contemporary standard NLP models are limited in their\ncompositional generalization capacity, and position COGS as a good way to\nmeasure progress.", "published": "2020-10-12 05:45:44", "link": "http://arxiv.org/abs/2010.05465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Factuality in Generation with Dependency-level Entailment", "abstract": "Despite significant progress in text generation models, a serious limitation\nis their tendency to produce text that is factually inconsistent with\ninformation in the input. Recent work has studied whether textual entailment\nsystems can be used to identify factual errors; however, these sentence-level\nentailment models are trained to solve a different problem than generation\nfiltering and they do not localize which part of a generation is non-factual.\nIn this paper, we propose a new formulation of entailment that decomposes it at\nthe level of dependency arcs. Rather than focusing on aggregate decisions, we\ninstead ask whether the semantic relationship manifested by individual\ndependency arcs in the generated output is supported by the input. Human\njudgments on this task are difficult to obtain; we therefore propose a method\nto automatically create data based on existing entailment or paraphrase\ncorpora. Experiments show that our dependency arc entailment model trained on\nthis data can identify factual inconsistencies in paraphrasing and\nsummarization better than sentence-level methods or those based on question\ngeneration, while additionally localizing the erroneous parts of the\ngeneration.", "published": "2020-10-12 06:43:10", "link": "http://arxiv.org/abs/2010.05478v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge\n  Graph", "abstract": "Generating a vivid, novel, and diverse essay with only several given topic\nwords is a challenging task of natural language generation. In previous work,\nthere are two problems left unsolved: neglect of sentiment beneath the text and\ninsufficient utilization of topic-related knowledge. Therefore, we propose a\nnovel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge\nGraph enhanced decoder, named SCTKG, which is based on the conditional\nvariational autoencoder (CVAE) framework. We firstly inject the sentiment\ninformation into the generator for controlling sentiment for each sentence,\nwhich leads to various generated essays. Then we design a Topic Knowledge Graph\nenhanced decoder. Unlike existing models that use knowledge entities\nseparately, our model treats the knowledge graph as a whole and encodes more\nstructured, connected semantic information in the graph to generate a more\nrelevant essay. Experimental results show that our SCTKG can generate sentiment\ncontrollable essays and outperform the state-of-the-art approach in terms of\ntopic relevance, fluency, and diversity on both automatic and human evaluation.", "published": "2020-10-12 08:06:12", "link": "http://arxiv.org/abs/2010.05511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-trained Language Model Based Active Learning for Sentence Matching", "abstract": "Active learning is able to significantly reduce the annotation cost for\ndata-driven techniques. However, previous active learning approaches for\nnatural language processing mainly depend on the entropy-based uncertainty\ncriterion, and ignore the characteristics of natural language. In this paper,\nwe propose a pre-trained language model based active learning approach for\nsentence matching. Differing from previous active learning, it can provide\nlinguistic criteria to measure instances and help select more efficient\ninstances for annotation. Experiments demonstrate our approach can achieve\ngreater accuracy with fewer labeled training instances.", "published": "2020-10-12 08:24:36", "link": "http://arxiv.org/abs/2010.05522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Cross-Lingual Definition Generation for Language Learners", "abstract": "Generating dictionary definitions automatically can prove useful for language\nlearners. However, it's still a challenging task of cross-lingual definition\ngeneration. In this work, we propose to generate definitions in English for\nwords in various languages. To achieve this, we present a simple yet effective\napproach based on publicly available pretrained language models. In this\napproach, models can be directly applied to other languages after trained on\nthe English dataset. We demonstrate the effectiveness of this approach on\nzero-shot definition generation. Experiments and manual analyses on newly\nconstructed datasets show that our models have a strong cross-lingual transfer\nability and can generate fluent English definitions for Chinese words. We\nfurther measure the lexical complexity of generated and reference definitions.\nThe results show that the generated definitions are much simpler, which is more\nsuitable for language learners.", "published": "2020-10-12 08:45:28", "link": "http://arxiv.org/abs/2010.05533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The National Corpus of Contemporary Welsh: Project Report | Y Corpws\n  Cenedlaethol Cymraeg Cyfoes: Adroddiad y Prosiect", "abstract": "This report provides an overview of the CorCenCC project and the online\ncorpus resource that was developed as a result of work on the project. The\nreport lays out the theoretical underpinnings of the research, demonstrating\nhow the project has built on and extended this theory. We also raise and\ndiscuss some of the key operational questions that arose during the course of\nthe project, outlining the ways in which they were answered, the impact of\nthese decisions on the resource that has been produced and the longer-term\ncontribution they will make to practices in corpus-building. Finally, we\ndiscuss some of the applications and the utility of the work, outlining the\nimpact that CorCenCC is set to have on a range of different individuals and\nuser groups.", "published": "2020-10-12 08:57:58", "link": "http://arxiv.org/abs/2010.05542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Low Resource Code-switched ASR using Augmented Code-switched\n  TTS", "abstract": "Building Automatic Speech Recognition (ASR) systems for code-switched speech\nhas recently gained renewed attention due to the widespread use of speech\ntechnologies in multilingual communities worldwide. End-to-end ASR systems are\na natural modeling choice due to their ease of use and superior performance in\nmonolingual settings. However, it is well known that end-to-end systems require\nlarge amounts of labeled speech. In this work, we investigate improving\ncode-switched ASR in low resource settings via data augmentation using\ncode-switched text-to-speech (TTS) synthesis. We propose two targeted\ntechniques to effectively leverage TTS speech samples: 1) Mixup, an existing\ntechnique to create new training samples via linear interpolation of existing\nsamples, applied to TTS and real speech samples, and 2) a new loss function,\nused in conjunction with TTS samples, to encourage code-switched predictions.\nWe report significant improvements in ASR performance achieving absolute word\nerror rate (WER) reductions of up to 5%, and measurable improvement in code\nswitching using our proposed techniques on a Hindi-English code-switched ASR\ntask.", "published": "2020-10-12 09:15:12", "link": "http://arxiv.org/abs/2010.05549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards", "abstract": "Coreference resolution and semantic role labeling are NLP tasks that capture\ndifferent aspects of semantics, indicating respectively, which expressions\nrefer to the same entity, and what semantic roles expressions serve in the\nsentence. However, they are often closely interdependent, and both generally\nnecessitate natural language understanding. Do they form a coherent abstract\nrepresentation of documents? We present a neural network architecture for joint\ncoreference resolution and semantic role labeling for English, and train graph\nneural networks to model the 'coherence' of the combined shallow semantic\ngraph. Using the resulting coherence score as a reward for our joint semantic\nanalyzer, we use reinforcement learning to encourage global coherence over the\ndocument and between semantic annotations. This leads to improvements on both\ntasks in multiple datasets from different domains, and across a range of\nencoders of different expressivity, calling, we believe, for a more holistic\napproach to semantics in NLP.", "published": "2020-10-12 09:36:24", "link": "http://arxiv.org/abs/2010.05567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Context Transformers for Domain-Specific Response Generation", "abstract": "Despite the tremendous success of neural dialogue models in recent years, it\nsuffers a lack of relevance, diversity, and some times coherence in generated\nresponses. Lately, transformer-based models, such as GPT-2, have revolutionized\nthe landscape of dialogue generation by capturing the long-range structures\nthrough language modeling. Though these models have exhibited excellent\nlanguage coherence, they often lack relevance and terms when used for\ndomain-specific response generation. In this paper, we present DSRNet (Domain\nSpecific Response Network), a transformer-based model for dialogue response\ngeneration by reinforcing domain-specific attributes. In particular, we extract\nmeta attributes from context and infuse them with the context utterances for\nbetter attention over domain-specific key terms and relevance. We study the use\nof DSRNet in a multi-turn multi-interlocutor environment for domain-specific\nresponse generation. In our experiments, we evaluate DSRNet on Ubuntu dialogue\ndatasets, which are mainly composed of various technical domain related\ndialogues for IT domain issue resolutions and also on CamRest676 dataset, which\ncontains restaurant domain conversations. Trained with maximum likelihood\nobjective, our model shows significant improvement over the state-of-the-art\nfor multi-turn dialogue systems supported by better BLEU and semantic\nsimilarity (BertScore) scores. Besides, we also observe that the responses\nproduced by our model carry higher relevance due to the presence of\ndomain-specific key attributes that exhibit better overlap with the attributes\nof the context. Our analysis shows that the performance improvement is mostly\ndue to the infusion of key terms along with dialogues which result in better\nattention over domain-relevant terms. Other contributing factors include joint\nmodeling of dialogue context with the domain-specific meta attributes and\ntopics.", "published": "2020-10-12 09:49:27", "link": "http://arxiv.org/abs/2010.05572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Variable Control for Robust and Interpretable Question\n  Answering", "abstract": "Deep neural network based question answering (QA) models are neither robust\nnor explainable in many cases. For example, a multiple-choice QA model, tested\nwithout any input of question, is surprisingly \"capable\" to predict the most of\ncorrect options. In this paper, we inspect such spurious \"capability\" of QA\nmodels using causal inference. We find the crux is the shortcut correlation,\ne.g., unrobust word alignment between passage and options learned by the\nmodels. We propose a novel approach called Counterfactual Variable Control\n(CVC) that explicitly mitigates any shortcut correlation and preserves the\ncomprehensive reasoning for robust QA. Specifically, we leverage multi-branch\narchitecture that allows us to disentangle robust and shortcut correlations in\nthe training process of QA. We then conduct two novel CVC inference methods (on\ntrained models) to capture the effect of comprehensive reasoning as the final\nprediction. For evaluation, we conduct extensive experiments using two BERT\nbackbones on both multi-choice and span-extraction QA benchmarks. The results\nshow that our CVC achieves high robustness against a variety of adversarial\nattacks in QA while maintaining good interpretation ability.", "published": "2020-10-12 10:09:05", "link": "http://arxiv.org/abs/2010.05581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Commonsense Reasoning with Multi-Head Knowledge Attention", "abstract": "Social Commonsense Reasoning requires understanding of text, knowledge about\nsocial events and their pragmatic implications, as well as commonsense\nreasoning skills. In this work we propose a novel multi-head knowledge\nattention model that encodes semi-structured commonsense inference rules and\nlearns to incorporate them in a transformer-based reasoning cell. We assess the\nmodel's performance on two tasks that require different reasoning skills:\nAbductive Natural Language Inference and Counterfactual Invariance Prediction\nas a new task. We show that our proposed model improves performance over strong\nstate-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we\nare, to the best of our knowledge, the first to demonstrate that a model that\nlearns to perform counterfactual reasoning helps predicting the best\nexplanation in an abductive reasoning task. We validate the robustness of the\nmodel's reasoning capabilities by perturbing the knowledge and provide\nqualitative analysis on the model's knowledge incorporation capabilities.", "published": "2020-10-12 10:24:40", "link": "http://arxiv.org/abs/2010.05587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced\n  with annotation corrections and co-reference annotation", "abstract": "Task-oriented dialogue systems have made unprecedented progress with multiple\nstate-of-the-art (SOTA) models underpinned by a number of publicly available\nMultiWOZ datasets. Dialogue state annotations are error-prone, leading to\nsub-optimal performance. Various efforts have been put in rectifying the\nannotation errors presented in the original MultiWOZ dataset. In this paper, we\nintroduce MultiWOZ 2.3, in which we differentiate incorrect annotations in\ndialogue acts from dialogue states, identifying a lack of co-reference when\npublishing the updated dataset. To ensure consistency between dialogue acts and\ndialogue states, we implement co-reference features and unify annotations of\ndialogue acts and dialogue states. We update the state of the art performance\nof natural language understanding and dialogue state tracking on MultiWOZ 2.3,\nwhere the results show significant improvements than on previous versions of\nMultiWOZ datasets (2.0-2.2).", "published": "2020-10-12 10:53:19", "link": "http://arxiv.org/abs/2010.05594v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The elephant in the interpretability room: Why use attention as\n  explanation when we have saliency methods?", "abstract": "There is a recent surge of interest in using attention as explanation of\nmodel predictions, with mixed evidence on whether attention can be used as\nsuch. While attention conveniently gives us one weight per input token and is\neasily extracted, it is often unclear toward what goal it is used as\nexplanation. We find that often that goal, whether explicitly stated or not, is\nto find out what input tokens are the most relevant to a prediction, and that\nthe implied user for the explanation is a model developer. For this goal and\nuser, we argue that input saliency methods are better suited, and that there\nare no compelling reasons to use attention, despite the coincidence that it\nprovides a weight for each input. With this position paper, we hope to shift\nsome of the recent focus on attention to saliency methods, and for authors to\nclearly state the goal and user for their explanations.", "published": "2020-10-12 11:27:47", "link": "http://arxiv.org/abs/2010.05607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Modulation for Relation-Level Metaphor Identification", "abstract": "Identifying metaphors in text is very challenging and requires comprehending\nthe underlying comparison. The automation of this cognitive process has gained\nwide attention lately. However, the majority of existing approaches concentrate\non word-level identification by treating the task as either single-word\nclassification or sequential labelling without explicitly modelling the\ninteraction between the metaphor components. On the other hand, while existing\nrelation-level approaches implicitly model this interaction, they ignore the\ncontext where the metaphor occurs. In this work, we address these limitations\nby introducing a novel architecture for identifying relation-level metaphoric\nexpressions of certain grammatical relations based on contextual modulation. In\na methodology inspired by works in visual reasoning, our approach is based on\nconditioning the neural network computation on the deep contextualised features\nof the candidate expressions using feature-wise linear modulation. We\ndemonstrate that the proposed architecture achieves state-of-the-art results on\nbenchmark datasets. The proposed methodology is generic and could be applied to\nother textual classification problems that benefit from contextual interaction.", "published": "2020-10-12 12:07:02", "link": "http://arxiv.org/abs/2010.05633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Compositional Generalization in Semantic Parsing", "abstract": "Generalization of models to out-of-distribution (OOD) data has captured\ntremendous attention recently. Specifically, compositional generalization,\ni.e., whether a model generalizes to new structures built of components\nobserved during training, has sparked substantial interest. In this work, we\ninvestigate compositional generalization in semantic parsing, a natural\ntest-bed for compositional generalization, as output programs are constructed\nfrom sub-components. We analyze a wide variety of models and propose multiple\nextensions to the attention module of the semantic parser, aiming to improve\ncompositional generalization. We find that the following factors improve\ncompositional generalization: (a) using contextual representations, such as\nELMo and BERT, (b) informing the decoder what input tokens have previously been\nattended to, (c) training the decoder attention to agree with pre-computed\ntoken alignments, and (d) downsampling examples corresponding to frequent\nprogram templates. While we substantially reduce the gap between\nin-distribution and OOD generalization, performance on OOD compositions is\nstill substantially lower.", "published": "2020-10-12 12:34:58", "link": "http://arxiv.org/abs/2010.05647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Hero to Z\u00e9roe: A Benchmark of Low-Level Adversarial Attacks", "abstract": "Adversarial attacks are label-preserving modifications to inputs of machine\nlearning classifiers designed to fool machines but not humans. Natural Language\nProcessing (NLP) has mostly focused on high-level attack scenarios such as\nparaphrasing input texts. We argue that these are less realistic in typical\napplication scenarios such as in social media, and instead focus on low-level\nattacks on the character-level. Guided by human cognitive abilities and human\nrobustness, we propose the first large-scale catalogue and benchmark of\nlow-level adversarial attacks, which we dub Z\\'eroe, encompassing nine\ndifferent attack modes including visual and phonetic adversaries. We show that\nRoBERTa, NLP's current workhorse, fails on our attacks. Our dataset provides a\nbenchmark for testing robustness of future more human-like NLP models.", "published": "2020-10-12 12:35:36", "link": "http://arxiv.org/abs/2010.05648v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Lexical Ambiguity with Density Matrices", "abstract": "Words can have multiple senses. Compositional distributional models of\nmeaning have been argued to deal well with finer shades of meaning variation\nknown as polysemy, but are not so well equipped to handle word senses that are\netymologically unrelated, or homonymy. Moving from vectors to density matrices\nallows us to encode a probability distribution over different senses of a word,\nand can also be accommodated within a compositional distributional model of\nmeaning. In this paper we present three new neural models for learning density\nmatrices from a corpus, and test their ability to discriminate between word\nsenses on a range of compositional datasets. When paired with a particular\ncomposition method, our best model outperforms existing vector-based\ncompositional models as well as strong sentence encoders.", "published": "2020-10-12 13:08:45", "link": "http://arxiv.org/abs/2010.05670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reformulating Unsupervised Style Transfer as Paraphrase Generation", "abstract": "Modern NLP defines the task of style transfer as modifying the style of a\ngiven sentence without appreciably changing its semantics, which implies that\nthe outputs of style transfer systems should be paraphrases of their inputs.\nHowever, many existing systems purportedly designed for style transfer\ninherently warp the input's meaning through attribute transfer, which changes\nsemantic properties such as sentiment. In this paper, we reformulate\nunsupervised style transfer as a paraphrase generation problem, and present a\nsimple methodology based on fine-tuning pretrained language models on\nautomatically generated paraphrase data. Despite its simplicity, our method\nsignificantly outperforms state-of-the-art style transfer systems on both human\nand automatic evaluations. We also survey 23 style transfer papers and discover\nthat existing automatic metrics can be easily gamed and propose fixed variants.\nFinally, we pivot to a more real-world style transfer setting by collecting a\nlarge dataset of 15M sentences in 11 diverse styles, which we use for an\nin-depth analysis of our system.", "published": "2020-10-12 13:31:01", "link": "http://arxiv.org/abs/2010.05700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Supervision Improves Few-Shot Learning and Syntactic\n  Generalization in Neural Language Models", "abstract": "Humans can learn structural properties about a word from minimal experience,\nand deploy their learned syntactic representations uniformly in different\ngrammatical contexts. We assess the ability of modern neural language models to\nreproduce this behavior in English and evaluate the effect of structural\nsupervision on learning outcomes. First, we assess few-shot learning\ncapabilities by developing controlled experiments that probe models' syntactic\nnominal number and verbal argument structure generalizations for tokens seen as\nfew as two times during training. Second, we assess invariance properties of\nlearned representation: the ability of a model to transfer syntactic\ngeneralizations from a base context (e.g., a simple declarative active-voice\nsentence) to a transformed context (e.g., an interrogative sentence). We test\nfour models trained on the same dataset: an n-gram baseline, an LSTM, and two\nLSTM-variants trained with explicit structural supervision (Dyer et al.,2016;\nCharniak et al., 2016). We find that in most cases, the neural models are able\nto induce the proper syntactic generalizations after minimal exposure, often\nfrom just two examples during training, and that the two structurally\nsupervised models generalize more accurately than the LSTM model. All neural\nmodels are able to leverage information learned in base contexts to drive\nexpectations in transformed contexts, indicating that they have learned some\ninvariance properties of syntax.", "published": "2020-10-12 14:12:37", "link": "http://arxiv.org/abs/2010.05725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Pretrained Language Models for Lexical Semantics", "abstract": "The success of large pretrained language models (LMs) such as BERT and\nRoBERTa has sparked interest in probing their representations, in order to\nunveil what types of knowledge they implicitly capture. While prior research\nfocused on morphosyntactic, semantic, and world knowledge, it remains unclear\nto which extent LMs also derive lexical type-level knowledge from words in\ncontext. In this work, we present a systematic empirical analysis across six\ntypologically diverse languages and five different lexical tasks, addressing\nthe following questions: 1) How do different lexical knowledge extraction\nstrategies (monolingual versus multilingual source LM, out-of-context versus\nin-context encoding, inclusion of special tokens, and layer-wise averaging)\nimpact performance? How consistent are the observed effects across tasks and\nlanguages? 2) Is lexical knowledge stored in few parameters, or is it scattered\nthroughout the network? 3) How do these representations fare against\ntraditional static word vectors in lexical tasks? 4) Does the lexical\ninformation emerging from independently trained monolingual LMs display latent\nsimilarities? Our main results indicate patterns and best practices that hold\nuniversally, but also point to prominent variations across languages and tasks.\nMoreover, we validate the claim that lower Transformer layers carry more\ntype-level lexical knowledge, but also show that this knowledge is distributed\nacross multiple layers.", "published": "2020-10-12 14:24:01", "link": "http://arxiv.org/abs/2010.05731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EFSG: Evolutionary Fooling Sentences Generator", "abstract": "Large pre-trained language representation models (LMs) have recently\ncollected a huge number of successes in many NLP tasks.\n  In 2018 BERT, and later its successors (e.g. RoBERTa), obtained\nstate-of-the-art results in classical benchmark tasks, such as GLUE benchmark.\n  After that, works about adversarial attacks have been published to test their\ngeneralization proprieties and robustness.\n  In this work, we design Evolutionary Fooling Sentences Generator (EFSG), a\nmodel- and task-agnostic adversarial attack algorithm built using an\nevolutionary approach to generate false-positive sentences for binary\nclassification tasks.\n  We successfully apply EFSG to CoLA and MRPC tasks, on BERT and RoBERTa,\ncomparing performances. Results prove the presence of weak spots in\nstate-of-the-art LMs.\n  We finally test adversarial training as a data augmentation defence approach\nagainst EFSG, obtaining stronger improved models with no loss of accuracy when\ntested on the original datasets.", "published": "2020-10-12 14:28:48", "link": "http://arxiv.org/abs/2010.05736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualize Knowledge Bases with Transformer for End-to-end\n  Task-Oriented Dialogue Systems", "abstract": "Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue\nsystems is challenging, since it requires to properly represent the entity of\nKB, which is associated with its KB context and dialogue context. The existing\nworks represent the entity with only perceiving a part of its KB context, which\ncan lead to the less effective representation due to the information loss, and\nadversely favor KB reasoning and response generation. To tackle this issue, we\nexplore to fully contextualize the entity representation by dynamically\nperceiving all the relevant entities} and dialogue history. To achieve this, we\npropose a COntext-aware Memory Enhanced Transformer framework (COMET), which\ntreats the KB as a sequence and leverages a novel Memory Mask to enforce the\nentity to only focus on its relevant entities and dialogue history, while\navoiding the distraction from the irrelevant entities. Through extensive\nexperiments, we show that our COMET framework can achieve superior performance\nover the state of the arts.", "published": "2020-10-12 14:34:07", "link": "http://arxiv.org/abs/2010.05740v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer-wise Guided Training for BERT: Learning Incrementally Refined\n  Document Representations", "abstract": "Although BERT is widely used by the NLP community, little is known about its\ninner workings. Several attempts have been made to shed light on certain\naspects of BERT, often with contradicting conclusions. A much raised concern\nfocuses on BERT's over-parameterization and under-utilization issues. To this\nend, we propose o novel approach to fine-tune BERT in a structured manner.\nSpecifically, we focus on Large Scale Multilabel Text Classification (LMTC)\nwhere documents are assigned with one or more labels from a large predefined\nset of hierarchically organized labels. Our approach guides specific BERT\nlayers to predict labels from specific hierarchy levels. Experimenting with two\nLMTC datasets we show that this structured fine-tuning approach not only yields\nbetter classification results but also leads to better parameter utilization.", "published": "2020-10-12 14:56:22", "link": "http://arxiv.org/abs/2010.05763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exemplar-Controllable Paraphrasing and Translation using Bitext", "abstract": "Most prior work on exemplar-based syntactically controlled paraphrase\ngeneration relies on automatically-constructed large-scale paraphrase datasets,\nwhich are costly to create. We sidestep this prerequisite by adapting models\nfrom prior work to be able to learn solely from bilingual text (bitext).\nDespite only using bitext for training, and in near zero-shot conditions, our\nsingle proposed model can perform four tasks: controlled paraphrase generation\nin both languages and controlled machine translation in both language\ndirections. To evaluate these tasks quantitatively, we create three novel\nevaluation datasets. Our experimental results show that our models achieve\ncompetitive results on controlled paraphrase generation and strong performance\non controlled machine translation. Analysis shows that our models learn to\ndisentangle semantics and syntax in their latent representations, but still\nsuffer from semantic drift.", "published": "2020-10-12 17:02:50", "link": "http://arxiv.org/abs/2010.05856v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Hallucinations: Learning to Generate Faithfully from Noisy\n  Data", "abstract": "Neural text generation (data- or text-to-text) demonstrates remarkable\nperformance when training data is abundant which for many applications is not\nthe case. To collect a large corpus of parallel data, heuristic rules are often\nused but they inevitably let noise into the data, such as phrases in the output\nwhich cannot be explained by the input. Consequently, models pick up on the\nnoise and may hallucinate--generate fluent but unsupported text. Our\ncontribution is a simple but powerful technique to treat such hallucinations as\na controllable aspect of the generated text, without dismissing any input and\nwithout modifying the model architecture. On the WikiBio corpus (Lebret et al.,\n2016), a particularly noisy dataset, we demonstrate the efficacy of the\ntechnique both in an automatic and in a human evaluation.", "published": "2020-10-12 17:25:02", "link": "http://arxiv.org/abs/2010.05873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Pre-training for Low-Resource Domain Adaptation", "abstract": "Transfer learning techniques are particularly useful in NLP tasks where a\nsizable amount of high-quality annotated data is difficult to obtain. Current\napproaches directly adapt a pre-trained language model (LM) on in-domain text\nbefore fine-tuning to downstream tasks. We show that extending the vocabulary\nof the LM with domain-specific terms leads to further gains. To a bigger\neffect, we utilize structure in the unlabeled data to create auxiliary\nsynthetic tasks, which helps the LM transfer to downstream tasks. We apply\nthese approaches incrementally on a pre-trained Roberta-large LM and show\nconsiderable performance gain on three tasks in the IT domain: Extractive\nReading Comprehension, Document Ranking and Duplicate Question Detection.", "published": "2020-10-12 17:57:00", "link": "http://arxiv.org/abs/2010.05904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs", "abstract": "Recent years have brought about a renewed interest in commonsense\nrepresentation and reasoning in the field of natural language understanding.\nThe development of new commonsense knowledge graphs (CSKG) has been central to\nthese advances as their diverse facts can be used and referenced by machine\nlearning models for tackling new and challenging tasks. At the same time, there\nremain questions about the quality and coverage of these resources due to the\nmassive scale required to comprehensively encompass general commonsense\nknowledge.\n  In this work, we posit that manually constructed CSKGs will never achieve the\ncoverage necessary to be applicable in all situations encountered by NLP\nagents. Therefore, we propose a new evaluation framework for testing the\nutility of KGs based on how effectively implicit knowledge representations can\nbe learned from them.\n  With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose\ncommonsense knowledge containing knowledge that is not readily available in\npretrained language models. We evaluate its properties in comparison with other\nleading CSKGs, performing the first large-scale pairwise study of commonsense\nknowledge resources. Next, we show that ATOMIC 2020 is better suited for\ntraining knowledge models that can generate accurate, representative knowledge\nfor new, unseen entities and events. Finally, through human evaluation, we show\nthat the few-shot performance of GPT-3 (175B parameters), while impressive,\nremains ~12 absolute points lower than a BART-based knowledge model trained on\nATOMIC 2020 despite using over 430x fewer parameters.", "published": "2020-10-12 18:27:05", "link": "http://arxiv.org/abs/2010.05953v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Induction of Structured Phoneme Inventories", "abstract": "This extended abstract surveying the work on phonological typology was\nprepared for \"SIGTYP 2020: The Second Workshop on Computational Research in\nLinguistic Typology\" to be held at EMNLP 2020.", "published": "2020-10-12 18:39:07", "link": "http://arxiv.org/abs/2010.05959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Extraordinary Failure of Complement Coercion Crowdsourcing", "abstract": "Crowdsourcing has eased and scaled up the collection of linguistic annotation\nin recent years. In this work, we follow known methodologies of collecting\nlabeled data for the complement coercion phenomenon. These are constructions\nwith an implied action -- e.g., \"I started a new book I bought last week\",\nwhere the implied action is reading. We aim to collect annotated data for this\nphenomenon by reducing it to either of two known tasks: Explicit Completion and\nNatural Language Inference. However, in both cases, crowdsourcing resulted in\nlow agreement scores, even though we followed the same methodologies as in\nprevious work. Why does the same process fail to yield high agreement scores?\nWe specify our modeling schemes, highlight the differences with previous work\nand provide some insights about the task and possible explanations for the\nfailure. We conclude that specific phenomena require tailored solutions, not\nonly in specialized algorithms, but also in data collection methods.", "published": "2020-10-12 19:04:04", "link": "http://arxiv.org/abs/2010.05971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Coreference and Bias Evaluation at WMT 2020", "abstract": "Gender bias in machine translation can manifest when choosing gender\ninflections based on spurious gender correlations. For example, always\ntranslating doctors as men and nurses as women. This can be particularly\nharmful as models become more popular and deployed within commercial systems.\nOur work presents the largest evidence for the phenomenon in more than 19\nsystems submitted to the WMT over four diverse target languages: Czech, German,\nPolish, and Russian. To achieve this, we use WinoMT, a recent automatic test\nsuite which examines gender coreference and bias when translating from English\nto languages with grammatical gender. We extend WinoMT to handle two new\nlanguages tested in WMT: Polish and Czech. We find that all systems\nconsistently use spurious correlations in the data rather than meaningful\ncontextual information.", "published": "2020-10-12 20:42:21", "link": "http://arxiv.org/abs/2010.06018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Synthetic Data Generation for Domain Adaptation of Question\n  Answering Systems", "abstract": "We propose an end-to-end approach for synthetic QA data generation. Our model\ncomprises a single transformer-based encoder-decoder network that is trained\nend-to-end to generate both answers and questions. In a nutshell, we feed a\npassage to the encoder and ask the decoder to generate a question and an answer\ntoken-by-token. The likelihood produced in the generation process is used as a\nfiltering score, which avoids the need for a separate filtering model. Our\ngenerator is trained by fine-tuning a pretrained LM using maximum likelihood\nestimation. The experimental results indicate significant improvements in the\ndomain adaptation of QA models outperforming current state-of-the-art methods.", "published": "2020-10-12 21:10:18", "link": "http://arxiv.org/abs/2010.06028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Reducing Gendered Correlations in Pre-trained Models", "abstract": "Pre-trained models have revolutionized natural language understanding.\nHowever, researchers have found they can encode artifacts undesired in many\napplications, such as professions correlating with one gender more than\nanother. We explore such gendered correlations as a case study for how to\naddress unintended correlations in pre-trained models. We define metrics and\nreveal that it is possible for models with similar accuracy to encode\ncorrelations at very different rates. We show how measured correlations can be\nreduced with general-purpose techniques, and highlight the trade offs different\nstrategies have. With these results, we make recommendations for training\nrobust models: (1) carefully evaluate unintended correlations, (2) be mindful\nof seemingly innocuous configuration differences, and (3) focus on general\nmitigations.", "published": "2020-10-12 21:15:29", "link": "http://arxiv.org/abs/2010.06032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Machine Translation for the Kurdish Language", "abstract": "Machine translation is the task of translating texts from one language to\nanother using computers. It has been one of the major tasks in natural language\nprocessing and computational linguistics and has been motivating to facilitate\nhuman communication. Kurdish, an Indo-European language, has received little\nattention in this realm due to the language being less-resourced. Therefore, in\nthis paper, we are addressing the main issues in creating a machine translation\nsystem for the Kurdish language, with a focus on the Sorani dialect. We\ndescribe the available scarce parallel data suitable for training a neural\nmachine translation model for Sorani Kurdish-English translation. We also\ndiscuss some of the major challenges in Kurdish language translation and\ndemonstrate how fundamental text processing tasks, such as tokenization, can\nimprove translation performance.", "published": "2020-10-12 21:28:57", "link": "http://arxiv.org/abs/2010.06041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioMegatron: Larger Biomedical Domain Language Model", "abstract": "There has been an influx of biomedical domain-specific language models,\nshowing language models pre-trained on biomedical text perform better on\nbiomedical domain benchmarks than those trained on general domain text corpora\nsuch as Wikipedia and Books. Yet, most works do not study the factors affecting\neach domain language application deeply. Additionally, the study of model size\non domain-specific models has been mostly missing. We empirically study and\nevaluate several factors that can affect performance on domain language\napplications, such as the sub-word vocabulary set, model size, pre-training\ncorpus, and domain transfer. We show consistent improvements on benchmarks with\nour larger BioMegatron model trained on a larger domain corpus, contributing to\nour understanding of domain language model applications. We demonstrate\nnoticeable improvements over the previous state-of-the-art (SOTA) on standard\nbiomedical NLP benchmarks of named entity recognition, relation extraction, and\nquestion answering. Model checkpoints and code are available at\n[https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].", "published": "2020-10-12 22:46:10", "link": "http://arxiv.org/abs/2010.06060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Some Words Worth More than Others?", "abstract": "Current evaluation metrics for language modeling and generation rely heavily\non the accuracy of predicted (or generated) words as compared to a reference\nground truth. While important, token-level accuracy only captures one aspect of\na language model's behavior, and ignores linguistic properties of words that\nmay allow some mis-predicted tokens to be useful in practice. Furthermore,\nstatistics directly tied to prediction accuracy (including perplexity) may be\nconfounded by the Zipfian nature of written language, as the majority of the\nprediction attempts will occur with frequently-occurring types. A model's\nperformance may vary greatly between high- and low-frequency words, which in\npractice could lead to failure modes such as repetitive and dull generated text\nbeing produced by a downstream consumer of a language model. To address this,\nwe propose two new intrinsic evaluation measures within the framework of a\nsimple word prediction task that are designed to give a more holistic picture\nof a language model's performance. We evaluate several commonly-used large\nEnglish language models using our proposed metrics, and demonstrate that our\napproach reveals functional differences in performance between the models that\nare obscured by more traditional metrics.", "published": "2020-10-12 23:12:11", "link": "http://arxiv.org/abs/2010.06069v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A BERT-based Distractor Generation Scheme with Multi-tasking and\n  Negative Answer Training Strategies", "abstract": "In this paper, we investigate the following two limitations for the existing\ndistractor generation (DG) methods. First, the quality of the existing DG\nmethods are still far from practical use. There is still room for DG quality\nimprovement. Second, the existing DG designs are mainly for single distractor\ngeneration. However, for practical MCQ preparation, multiple distractors are\ndesired. Aiming at these goals, in this paper, we present a new distractor\ngeneration scheme with multi-tasking and negative answer training strategies\nfor effectively generating \\textit{multiple} distractors. The experimental\nresults show that (1) our model advances the state-of-the-art result from 28.65\nto 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse\nand show strong distracting power for multiple choice question.", "published": "2020-10-12 01:22:29", "link": "http://arxiv.org/abs/2010.05384v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News\n  Articles", "abstract": "A popular multimedia news format nowadays is providing users with a lively\nvideo and a corresponding news article, which is employed by influential news\nmedia including CNN, BBC, and social media including Twitter and Weibo. In such\na case, automatically choosing a proper cover frame of the video and generating\nan appropriate textual summary of the article can help editors save time, and\nreaders make the decision more effectively. Hence, in this paper, we propose\nthe task of Video-based Multimodal Summarization with Multimodal Output (VMSMO)\nto tackle such a problem. The main challenge in this task is to jointly model\nthe temporal dependency of video with semantic meaning of article. To this end,\nwe propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of\na dual interaction module and multimodal generator. In the dual interaction\nmodule, we propose a conditional self-attention mechanism that captures local\nsemantic information within video and a global-attention mechanism that handles\nthe semantic relationship between news text and video from a high level.\nExtensive experiments conducted on a large-scale real-world VMSMO dataset show\nthat DIMS achieves the state-of-the-art performance in terms of both automatic\nmetrics and human evaluations.", "published": "2020-10-12 02:19:16", "link": "http://arxiv.org/abs/2010.05406v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Gradient-based Analysis of NLP Models is Manipulable", "abstract": "Gradient-based analysis methods, such as saliency map visualizations and\nadversarial input perturbations, have found widespread use in interpreting\nneural NLP models due to their simplicity, flexibility, and most importantly,\ntheir faithfulness. In this paper, however, we demonstrate that the gradients\nof a model are easily manipulable, and thus bring into question the reliability\nof gradient-based analyses. In particular, we merge the layers of a target\nmodel with a Facade that overwhelms the gradients without affecting the\npredictions. This Facade can be trained to have gradients that are misleading\nand irrelevant to the task, such as focusing only on the stop words in the\ninput. On a variety of NLP tasks (text classification, NLI, and QA), we show\nthat our method can manipulate numerous gradient-based analysis techniques:\nsaliency maps, input reduction, and adversarial perturbations all identify\nunimportant or targeted tokens as being highly important. The code and a\ntutorial of this paper is available at http://ucinlp.github.io/facade.", "published": "2020-10-12 02:54:22", "link": "http://arxiv.org/abs/2010.05419v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "It's not a Non-Issue: Negation as a Source of Error in Machine\n  Translation", "abstract": "As machine translation (MT) systems progress at a rapid pace, questions of\ntheir adequacy linger. In this study we focus on negation, a universal, core\nproperty of human language that significantly affects the semantics of an\nutterance. We investigate whether translating negation is an issue for modern\nMT systems using 17 translation directions as test bed. Through thorough\nanalysis, we find that indeed the presence of negation can significantly impact\ndownstream quality, in some cases resulting in quality reductions of more than\n60%. We also provide a linguistically motivated analysis that directly explains\nthe majority of our findings. We release our annotations and code to replicate\nour analysis here: https://github.com/mosharafhossain/negation-mt.", "published": "2020-10-12 03:34:44", "link": "http://arxiv.org/abs/2010.05432v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unseen Target Stance Detection with Adversarial Domain Generalization", "abstract": "Although stance detection has made great progress in the past few years, it\nis still facing the problem of unseen targets. In this study, we investigate\nthe domain difference between targets and thus incorporate attention-based\nconditional encoding with adversarial domain generalization to perform unseen\ntarget stance detection. Experimental results show that our approach achieves\nnew state-of-the-art performance on the SemEval-2016 dataset, demonstrating the\nimportance of domain difference between targets in unseen target stance\ndetection.", "published": "2020-10-12 06:12:18", "link": "http://arxiv.org/abs/2010.05471v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Feature Extraction of Text for Deep Learning Algorithms: Application on\n  Fake News Detection", "abstract": "Feature extraction is an important process of machine learning and deep\nlearning, as the process make algorithms function more efficiently, and also\naccurate. In natural language processing used in deception detection such as\nfake news detection, several ways of feature extraction in statistical aspect\nhad been introduced (e.g. N-gram). In this research, it will be shown that by\nusing deep learning algorithms and alphabet frequencies of the original text of\na news without any information about the sequence of the alphabet can actually\nbe used to classify fake news and trustworthy ones in high accuracy (85\\%). As\nthis pre-processing method makes the data notably compact but also include the\nfeature that is needed for the classifier, it seems that alphabet frequencies\ncontains some useful features for understanding complex context or meaning of\nthe original text.", "published": "2020-10-12 07:43:01", "link": "http://arxiv.org/abs/2010.05496v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Predicting Clinical Trial Results by Implicit Evidence Integration", "abstract": "Clinical trials provide essential guidance for practicing Evidence-Based\nMedicine, though often accompanying with unendurable costs and risks. To\noptimize the design of clinical trials, we introduce a novel Clinical Trial\nResult Prediction (CTRP) task. In the CTRP framework, a model takes a\nPICO-formatted clinical trial proposal with its background as input and\npredicts the result, i.e. how the Intervention group compares with the\nComparison group in terms of the measured Outcome in the studied Population.\nWhile structured clinical evidence is prohibitively expensive for manual\ncollection, we exploit large-scale unstructured sentences from medical\nliterature that implicitly contain PICOs and results as evidence. Specifically,\nwe pre-train a model to predict the disentangled results from such implicit\nevidence and fine-tune the model with limited data on the downstream datasets.\nExperiments on the benchmark Evidence Integration dataset show that the\nproposed model outperforms the baselines by large margins, e.g., with a 10.7%\nrelative gain over BioBERT in macro-F1. Moreover, the performance improvement\nis also validated on another dataset composed of clinical trials related to\nCOVID-19.", "published": "2020-10-12 12:25:41", "link": "http://arxiv.org/abs/2010.05639v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HUJI-KU at MRP~2020: Two Transition-based Neural Parsers", "abstract": "This paper describes the HUJI-KU system submission to the shared task on\nCross-Framework Meaning Representation Parsing (MRP) at the 2020 Conference for\nComputational Language Learning (CoNLL), employing TUPA and the HIT-SCIR\nparser, which were, respectively, the baseline system and winning system in the\n2019 MRP shared task. Both are transition-based parsers using BERT\ncontextualized embeddings. We generalized TUPA to support the newly-added MRP\nframeworks and languages, and experimented with multitask learning with the\nHIT-SCIR parser. We reached 4th place in both the cross-framework and\ncross-lingual tracks.", "published": "2020-10-12 13:41:32", "link": "http://arxiv.org/abs/2010.05710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Complementary Nature of Knowledge Graph Embedding, Fine Grain\n  Entity Types, and Language Modeling", "abstract": "We demonstrate the complementary natures of neural knowledge graph embedding,\nfine-grain entity type prediction, and neural language modeling. We show that a\nlanguage model-inspired knowledge graph embedding approach yields both improved\nknowledge graph embeddings and fine-grain entity type representations. Our work\nalso shows that jointly modeling both structured knowledge tuples and language\nimproves both.", "published": "2020-10-12 14:26:48", "link": "http://arxiv.org/abs/2010.05732v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Angina Symptoms from Clinical Notes Using Pre-Trained\n  Transformer Architectures", "abstract": "Anginal symptoms can connote increased cardiac risk and a need for change in\ncardiovascular management. This study evaluated the potential to extract these\nsymptoms from physician notes using the Bidirectional Encoder from Transformers\nlanguage model fine-tuned on a domain-specific corpus. The history of present\nillness section of 459 expert annotated primary care physician notes from\nconsecutive patients referred for cardiac testing without known atherosclerotic\ncardiovascular disease were included. Notes were annotated for positive and\nnegative mentions of chest pain and shortness of breath characterization. The\nresults demonstrate high sensitivity and specificity for the detection of chest\npain or discomfort, substernal chest pain, shortness of breath, and dyspnea on\nexertion. Small sample size limited extracting factors related to provocation\nand palliation of chest pain. This study provides a promising starting point\nfor the natural language processing of physician notes to characterize\nclinically actionable anginal symptoms.", "published": "2020-10-12 14:53:32", "link": "http://arxiv.org/abs/2010.05757v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Human-centric Dialog Training via Offline Reinforcement Learning", "abstract": "How can we train a dialog model to produce better conversations by learning\nfrom human feedback, without the risk of humans teaching it harmful chat\nbehaviors? We start by hosting models online, and gather human feedback from\nreal-time, open-ended conversations, which we then use to train and improve the\nmodels using offline reinforcement learning (RL). We identify implicit\nconversational cues including language similarity, elicitation of laughter,\nsentiment, and more, which indicate positive human feedback, and embed these in\nmultiple reward functions. A well-known challenge is that learning an RL policy\nin an offline setting usually fails due to the lack of ability to explore and\nthe tendency to make over-optimistic estimates of future reward. These problems\nbecome even harder when using RL for language models, which can easily have a\n20,000 action vocabulary and many possible reward functions. We solve the\nchallenge by developing a novel class of offline RL algorithms. These\nalgorithms use KL-control to penalize divergence from a pre-trained prior\nlanguage model, and use a new strategy to make the algorithm pessimistic,\ninstead of optimistic, in the face of uncertainty. We test the resulting dialog\nmodel with ratings from 80 users in an open-domain setting and find it achieves\nsignificant improvements over existing deep offline RL approaches. The novel\noffline RL method is viable for improving any existing generative dialog model\nusing a static dataset of human feedback.", "published": "2020-10-12 16:53:00", "link": "http://arxiv.org/abs/2010.05848v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in\n  Massively Multilingual Models", "abstract": "Massively multilingual models subsuming tens or even hundreds of languages\npose great challenges to multi-task optimization. While it is a common practice\nto apply a language-agnostic procedure optimizing a joint multilingual task\nobjective, how to properly characterize and take advantage of its underlying\nproblem structure for improving optimization efficiency remains under-explored.\nIn this paper, we attempt to peek into the black-box of multilingual\noptimization through the lens of loss function geometry. We find that gradient\nsimilarity measured along the optimization trajectory is an important signal,\nwhich correlates well with not only language proximity but also the overall\nmodel performance. Such observation helps us to identify a critical limitation\nof existing gradient-based multi-task learning methods, and thus we derive a\nsimple and scalable optimization procedure, named Gradient Vaccine, which\nencourages more geometrically aligned parameter updates for close tasks.\nEmpirically, our method obtains significant model performance gains on\nmultilingual machine translation and XTREME benchmark tasks for multilingual\nlanguage models. Our work reveals the importance of properly measuring and\nutilizing language proximity in multilingual optimization, and has broader\nimplications for multi-task learning beyond multilingual modeling.", "published": "2020-10-12 17:26:34", "link": "http://arxiv.org/abs/2010.05874v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Perceptimatic: A human speech perception benchmark for unsupervised\n  subword modelling", "abstract": "In this paper, we present a data set and methods to compare speech processing\nmodels and human behaviour on a phone discrimination task. We provide\nPerceptimatic, an open data set which consists of French and English speech\nstimuli, as well as the results of 91 English- and 93 French-speaking\nlisteners. The stimuli test a wide range of French and English contrasts, and\nare extracted directly from corpora of natural running read speech, used for\nthe 2017 Zero Resource Speech Challenge. We provide a method to compare humans'\nperceptual space with models' representational space, and we apply it to models\npreviously submitted to the Challenge. We show that, unlike unsupervised models\nand supervised multilingual models, a standard supervised monolingual HMM-GMM\nphone recognition system, while good at discriminating phones, yields a\nrepresentational space very different from that of human native listeners.", "published": "2020-10-12 18:40:08", "link": "http://arxiv.org/abs/2010.05961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Zero Resource Speech Challenge 2020: Discovering discrete subword\n  and word units", "abstract": "We present the Zero Resource Speech Challenge 2020, which aims at learning\nspeech representations from raw audio signals without any labels. It combines\nthe data sets and metrics from two previous benchmarks (2017 and 2019) and\nfeatures two tasks which tap into two levels of speech representation. The\nfirst task is to discover low bit-rate subword representations that optimize\nthe quality of speech synthesis; the second one is to discover word-like units\nfrom unsegmented raw speech. We present the results of the twenty submitted\nmodels and discuss the implications of the main findings for unsupervised\nspeech learning.", "published": "2020-10-12 18:56:48", "link": "http://arxiv.org/abs/2010.05967v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NEMO: Frequentist Inference Approach to Constrained Linguistic Typology\n  Feature Prediction in SIGTYP 2020 Shared Task", "abstract": "This paper describes the NEMO submission to SIGTYP 2020 shared task which\ndeals with prediction of linguistic typological features for multiple languages\nusing the data derived from World Atlas of Language Structures (WALS). We\nemploy frequentist inference to represent correlations between typological\nfeatures and use this representation to train simple multi-class estimators\nthat predict individual features. We describe two submitted ridge\nregression-based configurations which ranked second and third overall in the\nconstrained task. Our best configuration achieved the micro-averaged accuracy\nscore of 0.66 on 149 test languages.", "published": "2020-10-12 19:25:43", "link": "http://arxiv.org/abs/2010.05985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search", "abstract": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific\nliterature on the virus. Clinicians, researchers, and policy-makers need to be\nable to search these articles effectively. In this work, we present a zero-shot\nranking algorithm that adapts to COVID-related scientific literature. Our\napproach filters training data from another collection down to medical-related\nqueries, uses a neural re-ranking model pre-trained on scientific text\n(SciBERT), and filters the target document collection. This approach ranks top\namong zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a\nP@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2\njudgments. Despite not relying on TREC-COVID data, our method outperforms\nmodels that do. As one of the first search methods to thoroughly evaluate\nCOVID-19 search, we hope that this serves as a strong baseline and helps in the\nglobal crisis.", "published": "2020-10-12 19:28:29", "link": "http://arxiv.org/abs/2010.05987v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Chatbot Interaction with Artificial Intelligence: Human Data\n  Augmentation with T5 and Language Transformer Ensemble for Text\n  Classification", "abstract": "In this work, we present the Chatbot Interaction with Artificial Intelligence\n(CI-AI) framework as an approach to the training of deep learning chatbots for\ntask classification. The intelligent system augments human-sourced data via\nartificial paraphrasing in order to generate a large set of training data for\nfurther classical, attention, and language transformation-based learning\napproaches for Natural Language Processing. Human beings are asked to\nparaphrase commands and questions for task identification for further execution\nof a machine. The commands and questions are split into training and validation\nsets. A total of 483 responses were recorded. Secondly, the training set is\nparaphrased by the T5 model in order to augment it with further data. Seven\nstate-of-the-art transformer-based text classification algorithms (BERT,\nDistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are\nbenchmarked for both sets after fine-tuning on the training data for two\nepochs. We find that all models are improved when training data is augmented by\nthe T5 model, with an average increase of classification accuracy by 4.01%. The\nbest result was the RoBERTa model trained on T5 augmented data which achieved\n98.96% classification accuracy. Finally, we found that an ensemble of the five\nbest-performing transformer models via Logistic Regression of output label\npredictions led to an accuracy of 99.59% on the dataset of human responses. A\nhighly-performing model allows the intelligent system to interpret human\ncommands at the social-interaction level through a chatbot-like interface (e.g.\n\"Robot, can we have a conversation?\") and allows for better accessibility to AI\nby non-technical users.", "published": "2020-10-12 19:37:18", "link": "http://arxiv.org/abs/2010.05990v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vulgaris: Analysis of a Corpus for Middle-Age Varieties of Italian\n  Language", "abstract": "Italian is a Romance language that has its roots in Vulgar Latin. The birth\nof the modern Italian started in Tuscany around the 14th century, and it is\nmainly attributed to the works of Dante Alighieri, Francesco Petrarca and\nGiovanni Boccaccio, who are among the most acclaimed authors of the medieval\nage in Tuscany. However, Italy has been characterized by a high variety of\ndialects, which are often loosely related to each other, due to the past\nfragmentation of the territory. Italian has absorbed influences from many of\nthese dialects, as also from other languages due to dominion of portions of the\ncountry by other nations, such as Spain and France. In this work we present\nVulgaris, a project aimed at studying a corpus of Italian textual resources\nfrom authors of different regions, ranging in a time period between 1200 and\n1600. Each composition is associated to its author, and authors are also\ngrouped in families, i.e. sharing similar stylistic/chronological\ncharacteristics. Hence, the dataset is not only a valuable resource for\nstudying the diachronic evolution of Italian and the differences between its\ndialects, but it is also useful to investigate stylistic aspects between single\nauthors. We provide a detailed statistical analysis of the data, and a\ncorpus-driven study in dialectology and diachronic varieties.", "published": "2020-10-12 19:42:22", "link": "http://arxiv.org/abs/2010.05993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Text Generation with Student-Forcing Optimal Transport", "abstract": "Neural language models are often trained with maximum likelihood estimation\n(MLE), where the next word is generated conditioned on the ground-truth word\ntokens. During testing, however, the model is instead conditioned on previously\ngenerated tokens, resulting in what is termed exposure bias. To reduce this gap\nbetween training and testing, we propose using optimal transport (OT) to match\nthe sequences generated in these two modes. An extension is further proposed to\nimprove the OT learning, based on the structural and contextual information of\nthe text sequences. The effectiveness of the proposed method is validated on\nmachine translation, text summarization, and text generation tasks.", "published": "2020-10-12 19:42:25", "link": "http://arxiv.org/abs/2010.05994v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References", "abstract": "Understanding the relationship between figures and text is key to scientific\ndocument understanding. Medical figures in particular are quite complex, often\nconsisting of several subfigures (75% of figures in our dataset), with detailed\ntext describing their content. Previous work studying figures in scientific\npapers focused on classifying figure content rather than understanding how\nimages relate to the text. To address challenges in figure retrieval and\nfigure-to-text alignment, we introduce MedICaT, a dataset of medical images in\ncontext. MedICaT consists of 217K images from 131K open access biomedical\npapers, and includes captions, inline references for 74% of figures, and\nmanually annotated subfigures and subcaptions for a subset of figures. Using\nMedICaT, we introduce the task of subfigure to subcaption alignment in compound\nfigures and demonstrate the utility of inline references in image-text\nmatching. Our data and code can be accessed at\nhttps://github.com/allenai/medicat.", "published": "2020-10-12 19:56:08", "link": "http://arxiv.org/abs/2010.06000v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Self-supervised Pre-training via a Fully-Explored Masked\n  Language Model", "abstract": "Masked Language Model (MLM) framework has been widely adopted for\nself-supervised language pre-training. In this paper, we argue that randomly\nsampled masks in MLM would lead to undesirably large gradient variance. Thus,\nwe theoretically quantify the gradient variance via correlating the gradient\ncovariance with the Hamming distance between two different masks (given a\ncertain text sequence). To reduce the variance due to the sampling of masks, we\npropose a fully-explored masking strategy, where a text sequence is divided\ninto a certain number of non-overlapping segments. Thereafter, the tokens\nwithin one segment are masked for training. We prove, from a theoretical\nperspective, that the gradients derived from this new masking schema have a\nsmaller variance and can lead to more efficient self-supervised training. We\nconduct extensive experiments on both continual pre-training and general\npre-training from scratch. Empirical results confirm that this new masking\nstrategy can consistently outperform standard random masking. Detailed\nefficiency analysis and ablation studies further validate the advantages of our\nfully-explored masking strategy under the MLM framework.", "published": "2020-10-12 21:28:14", "link": "http://arxiv.org/abs/2010.06040v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Entity Linking with Efficient Long Range Sequence Modeling", "abstract": "This paper considers the problem of zero-shot entity linking, in which a link\nin the test time may not present in training. Following the prevailing\nBERT-based research efforts, we find a simple yet effective way is to expand\nthe long-range sequence modeling. Unlike many previous methods, our method does\nnot require expensive pre-training of BERT with long position embedding.\nInstead, we propose an efficient position embeddings initialization method\ncalled Embedding-repeat, which initializes larger position embeddings based on\nBERT-Base. On Wikia's zero-shot EL dataset, our method improves the SOTA from\n76.06% to 79.08%, and for its long data, the corresponding improvement is from\n74.57% to 82.14%. Our experiments suggest the effectiveness of long-range\nsequence modeling without retraining the BERT model.", "published": "2020-10-12 22:59:18", "link": "http://arxiv.org/abs/2010.06065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Siamese Networks for Semantic Code Search", "abstract": "With the increase in the number of open repositories and discussion forums,\nthe use of natural language for semantic code search has become increasingly\ncommon. The accuracy of the results returned by such systems, however, can be\nlow due to 1) limited shared vocabulary between code and user query and 2)\ninadequate semantic understanding of user query and its relation to code\nsyntax. Siamese networks are well suited to learning such joint relations\nbetween data, but have not been explored in the context of code search. In this\nwork, we evaluate Siamese networks for this task by exploring multiple\nextraction network architectures. These networks independently process code and\ntext descriptions before passing them to a Siamese network to learn embeddings\nin a common space. We experiment on two different datasets and discover that\nSiamese networks can act as strong regularizers on networks that extract rich\ninformation from code and text, which in turn helps achieve impressive\nperformance on code search beating previous baselines on $2$ programming\nlanguages. We also analyze the embedding space of these networks and provide\ndirections to fully leverage the power of Siamese networks for semantic code\nsearch.", "published": "2020-10-12 06:07:39", "link": "http://arxiv.org/abs/2011.01043v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase\n  Grounding", "abstract": "Phrase localization is a task that studies the mapping from textual phrases\nto regions of an image. Given difficulties in annotating phrase-to-object\ndatasets at scale, we develop a Multimodal Alignment Framework (MAF) to\nleverage more widely-available caption-image datasets, which can then be used\nas a form of weak supervision. We first present algorithms to model\nphrase-object relevance by leveraging fine-grained visual representations and\nvisually-aware language representations. By adopting a contrastive objective,\nour method uses information in caption-image pairs to boost the performance in\nweakly-supervised scenarios. Experiments conducted on the widely-adopted\nFlickr30k dataset show a significant improvement over existing\nweakly-supervised methods. With the help of the visually-aware language\nrepresentations, we can also improve the previous best unsupervised result by\n5.56%. We conduct ablation studies to show that both our novel model and our\nweakly-supervised strategies significantly contribute to our strong results.", "published": "2020-10-12 00:43:52", "link": "http://arxiv.org/abs/2010.05379v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FILM: A Fast, Interpretable, and Low-rank Metric Learning Approach for\n  Sentence Matching", "abstract": "Detection of semantic similarity plays a vital role in sentence matching. It\nrequires to learn discriminative representations of natural language. Recently,\nowing to more and more sophisticated model architecture, impressive progress\nhas been made, along with a time-consuming training process and\nnot-interpretable inference. To alleviate this problem, we explore a metric\nlearning approach, named FILM (Fast, Interpretable, and Low-rank Metric\nlearning) to efficiently find a high discriminative projection of the\nhigh-dimensional data. We construct this metric learning problem as a manifold\noptimization problem and solve it with the Cayley transformation method with\nthe Barzilai-Borwein step size. In experiments, we apply FILM with triplet loss\nminimization objective to the Quora Challenge and Semantic Textual Similarity\n(STS) Task. The results demonstrate that the FILM method achieves superior\nperformance as well as the fastest computation speed, which is consistent with\nour theoretical analysis of time complexity.", "published": "2020-10-12 08:24:41", "link": "http://arxiv.org/abs/2010.05523v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Carbon to Diamond: An Incident Remediation Assistant System From Site\n  Reliability Engineers' Conversations in Hybrid Cloud Operations", "abstract": "Conversational channels are changing the landscape of hybrid cloud service\nmanagement. These channels are becoming important avenues for Site Reliability\nEngineers (SREs) %Subject Matter Experts (SME) to collaboratively work together\nto resolve an incident or issue. Identifying segmented conversations and\nextracting key insights or artefacts from them can help engineers to improve\nthe efficiency of the incident remediation process by using information\nretrieval mechanisms for similar incidents. However, it has been empirically\nobserved that due to the semi-formal behavior of such conversations (human\nlanguage) they are very unique in nature and also contain lot of\ndomain-specific terms. This makes it difficult to use the standard natural\nlanguage processing frameworks directly, which are popularly used in standard\nNLP tasks. %It is important to identify the correct keywords and artefacts like\nsymptoms, issue etc., present in the conversation chats. In this paper, we\nbuild a framework that taps into the conversational channels and uses various\nlearning methods to (a) understand and extract key artefacts from conversations\nlike diagnostic steps and resolution actions taken, and (b) present an approach\nto identify past conversations about similar issues. Experimental results on\nour dataset show the efficacy of our proposed method.", "published": "2020-10-12 09:43:35", "link": "http://arxiv.org/abs/2010.05569v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Load What You Need: Smaller Versions of Multilingual BERT", "abstract": "Pre-trained Transformer-based models are achieving state-of-the-art results\non a variety of Natural Language Processing data sets. However, the size of\nthese models is often a drawback for their deployment in real production\napplications. In the case of multilingual models, most of the parameters are\nlocated in the embeddings layer. Therefore, reducing the vocabulary size should\nhave an important impact on the total number of parameters. In this paper, we\npropose to generate smaller models that handle fewer number of languages\naccording to the targeted corpora. We present an evaluation of smaller versions\nof multilingual BERT on the XNLI data set, but we believe that this method may\nbe applied to other multilingual transformers. The obtained results confirm\nthat we can generate smaller models that keep comparable results, while\nreducing up to 45% of the total number of parameters. We compared our models\nwith DistilmBERT (a distilled version of multilingual BERT) and showed that\nunlike language reduction, distillation induced a 1.7% to 6% drop in the\noverall accuracy on the XNLI data set. The presented models and code are\npublicly available.", "published": "2020-10-12 11:29:06", "link": "http://arxiv.org/abs/2010.05609v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Type Information to Improve Entity Coreference Resolution", "abstract": "Coreference resolution (CR) is an essential part of discourse analysis. Most\nrecently, neural approaches have been proposed to improve over SOTA models from\nearlier paradigms. So far none of the published neural models leverage external\nsemantic knowledge such as type information. This paper offers the first such\nmodel and evaluation, demonstrating modest gains in accuracy by introducing\neither gold standard or predicted types. In the proposed approach, type\ninformation serves both to (1) improve mention representation and (2) create a\nsoft type consistency check between coreference candidate mentions. Our\nevaluation covers two different grain sizes of types over four different\nbenchmark corpora.", "published": "2020-10-12 14:32:39", "link": "http://arxiv.org/abs/2010.05738v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Back to the Future: Unsupervised Backprop-based Decoding for\n  Counterfactual and Abductive Commonsense Reasoning", "abstract": "Abductive and counterfactual reasoning, core abilities of everyday human\ncognition, require reasoning about what might have happened at time t, while\nconditioning on multiple contexts from the relative past and future. However,\nsimultaneous incorporation of past and future contexts using generative\nlanguage models (LMs) can be challenging, as they are trained either to\ncondition only on the past context or to perform narrowly scoped\ntext-infilling. In this paper, we propose DeLorean, a new unsupervised decoding\nalgorithm that can flexibly incorporate both the past and future contexts using\nonly off-the-shelf, left-to-right language models and no supervision. The key\nintuition of our algorithm is incorporating the future through\nback-propagation, during which, we only update the internal representation of\nthe output while fixing the model parameters. By alternating between forward\nand backward propagation, DeLorean can decode the output representation that\nreflects both the left and right contexts. We demonstrate that our approach is\ngeneral and applicable to two nonmonotonic reasoning tasks: abductive text\ngeneration and counterfactual story revision, where DeLorean outperforms a\nrange of unsupervised and some supervised methods, based on automatic and human\nevaluation.", "published": "2020-10-12 17:58:43", "link": "http://arxiv.org/abs/2010.05906v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Look It Up: Bilingual Dictionaries Improve Neural Machine Translation", "abstract": "Despite advances in neural machine translation (NMT) quality, rare words\ncontinue to be problematic. For humans, the solution to the rare-word problem\nhas long been dictionaries, but dictionaries cannot be straightforwardly\nincorporated into NMT. In this paper, we describe a new method for \"attaching\"\ndictionary definitions to rare words so that the network can learn the best way\nto use them. We demonstrate improvements of up to 1.8 BLEU using bilingual\ndictionaries.", "published": "2020-10-12 19:53:08", "link": "http://arxiv.org/abs/2010.05997v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence, speech and language processing approaches to\n  monitoring Alzheimer's Disease: a systematic review", "abstract": "Language is a valuable source of clinical information in Alzheimer's Disease,\nas it declines concurrently with neurodegeneration. Consequently, speech and\nlanguage data have been extensively studied in connection with its diagnosis.\nThis paper summarises current findings on the use of artificial intelligence,\nspeech and language processing to predict cognitive decline in the context of\nAlzheimer's Disease, detailing current research procedures, highlighting their\nlimitations and suggesting strategies to address them. We conducted a\nsystematic review of original research between 2000 and 2019, registered in\nPROSPERO (reference CRD42018116606). An interdisciplinary search covered six\ndatabases on engineering (ACM and IEEE), psychology (PsycINFO), medicine\n(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were\nscreened until December 2019. From 3,654 search results 51 articles were\nselected against the eligibility criteria. Four tables summarise their\nfindings: study details (aim, population, interventions, comparisons, methods\nand outcomes), data details (size, type, modalities, annotation, balance,\navailability and language of study), methodology (pre-processing, feature\ngeneration, machine learning, evaluation and results) and clinical\napplicability (research implications, clinical potential, risk of bias and\nstrengths/limitations). While promising results are reported across nearly all\n51 studies, very few have been implemented in clinical research or practice. We\nconcluded that the main limitations of the field are poor standardisation,\nlimited comparability of results, and a degree of disconnect between study aims\nand clinical applications. Attempts to close these gaps should support\ntranslation of future research into clinical practice.", "published": "2020-10-12 21:43:04", "link": "http://arxiv.org/abs/2010.06047v1", "categories": ["cs.AI", "cs.CL", "eess.AS", "J.3; I.2.7; I.2.6; I.5.4"], "primary_category": "cs.AI"}
{"title": "Dual-mode ASR: Unify and Improve Streaming ASR with Full-context\n  Modeling", "abstract": "Streaming automatic speech recognition (ASR) aims to emit each hypothesized\nword as quickly and accurately as possible, while full-context ASR waits for\nthe completion of a full speech utterance before emitting completed hypotheses.\nIn this work, we propose a unified framework, Dual-mode ASR, to train a single\nend-to-end ASR model with shared weights for both streaming and full-context\nspeech recognition. We show that the latency and accuracy of streaming ASR\nsignificantly benefit from weight sharing and joint training of full-context\nASR, especially with inplace knowledge distillation during the training. The\nDual-mode ASR framework can be applied to recent state-of-the-art\nconvolution-based and transformer-based ASR networks. We present extensive\nexperiments with two state-of-the-art ASR networks, ContextNet and Conformer,\non two datasets, a widely used public dataset LibriSpeech and a large-scale\ndataset MultiDomain. Experiments and ablation studies demonstrate that\nDual-mode ASR not only simplifies the workflow of training and deploying\nstreaming and full-context ASR models, but also significantly improves both\nemission latency and recognition accuracy of streaming ASR. With Dual-mode ASR,\nwe achieve new state-of-the-art streaming ASR results on both LibriSpeech and\nMultiDomain in terms of accuracy and latency.", "published": "2020-10-12 21:12:56", "link": "http://arxiv.org/abs/2010.06030v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TextHide: Tackling Data Privacy in Language Understanding Tasks", "abstract": "An unsolved challenge in distributed or federated learning is to effectively\nmitigate privacy risks without slowing down training or reducing accuracy. In\nthis paper, we propose TextHide aiming at addressing this challenge for natural\nlanguage understanding tasks. It requires all participants to add a simple\nencryption step to prevent an eavesdropping attacker from recovering private\ntext data. Such an encryption step is efficient and only affects the task\nperformance slightly. In addition, TextHide fits well with the popular\nframework of fine-tuning pre-trained language models (e.g., BERT) for any\nsentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and\nour experiments show that TextHide can effectively defend attacks on shared\ngradients or representations and the averaged accuracy reduction is only\n$1.9\\%$. We also present an analysis of the security of TextHide using a\nconjecture about the computational intractability of a mathematical problem.\n  Our code is available at https://github.com/Hazelsuko07/TextHide", "published": "2020-10-12 22:22:15", "link": "http://arxiv.org/abs/2010.06053v1", "categories": ["cs.CL", "cs.CR", "cs.DS", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Designing a 9-channel location microphone from scratch", "abstract": "The design of a 9-channel microphone system for location recording of mainly\natmospheres will be described. The key concept is matching the recording and\nreproduction angles of the individual sectors. The rig is designed for the\nAURO-3D 9-channel playback system (4 height speakers). An analysis of the\nreproduction layout will be included, as well as recording concepts like the\nStereo Recording Angle (SRA), Williams curves, Scale Factors for different\nreproduction angles than 60 degrees and diffuse field decorrelation. Finally,\npractical aspects like microphone mounts and windshields for such a system will\nbe presented.", "published": "2020-10-12 17:27:39", "link": "http://arxiv.org/abs/2010.05877v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AI Song Contest: Human-AI Co-Creation in Songwriting", "abstract": "Machine learning is challenging the way we make music. Although research in\ndeep generative models has dramatically improved the capability and fluency of\nmusic models, recent work has shown that it can be challenging for humans to\npartner with this new class of algorithms. In this paper, we present findings\non what 13 musician/developer teams, a total of 61 users, needed when\nco-creating a song with AI, the challenges they faced, and how they leveraged\nand repurposed existing characteristics of AI to overcome some of these\nchallenges. Many teams adopted modular approaches, such as independently\nrunning multiple smaller models that align with the musical building blocks of\na song, before re-combining their results. As ML models are not easily\nsteerable, teams also generated massive numbers of samples and curated them\npost-hoc, or used a range of strategies to direct the generation, or\nalgorithmically ranked the samples. Ultimately, teams not only had to manage\nthe \"flare and focus\" aspects of the creative process, but also juggle them\nwith a parallel process of exploring and curating multiple ML models and\noutputs. These findings reflect a need to design machine learning-powered music\ninterfaces that are more decomposable, steerable, interpretable, and adaptive,\nwhich in return will enable artists to more effectively explore how AI can\nextend their personal expression.", "published": "2020-10-12 01:27:41", "link": "http://arxiv.org/abs/2010.05388v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS", "J.5; I.2"], "primary_category": "cs.SD"}
{"title": "Enhancement Of Coded Speech Using a Mask-Based Post-Filter", "abstract": "The quality of speech codecs deteriorates at low bitrates due to high\nquantization noise. A post-filter is generally employed to enhance the quality\nof the coded speech. In this paper, a data-driven post-filter relying on\nmasking in the time-frequency domain is proposed. A fully connected neural\nnetwork (FCNN), a convolutional encoder-decoder (CED) network and a long\nshort-term memory (LSTM) network are implemeted to estimate a real-valued mask\nper time-frequency bin. The proposed models were tested on the five lowest\noperating modes (6.65 kbps-15.85 kbps) of the Adaptive Multi-Rate Wideband\ncodec (AMR-WB). Both objective and subjective evaluations confirm the\nenhancement of the coded speech and also show the superiority of the mask-based\nneural network system over a conventional heuristic post-filter used in the\nstandard like ITU-T G.718.", "published": "2020-10-12 09:48:09", "link": "http://arxiv.org/abs/2010.05571v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High\n  Fidelity Speech Synthesis", "abstract": "Several recent work on speech synthesis have employed generative adversarial\nnetworks (GANs) to produce raw waveforms. Although such methods improve the\nsampling efficiency and memory usage, their sample quality has not yet reached\nthat of autoregressive and flow-based generative models. In this work, we\npropose HiFi-GAN, which achieves both efficient and high-fidelity speech\nsynthesis. As speech audio consists of sinusoidal signals with various periods,\nwe demonstrate that modeling periodic patterns of an audio is crucial for\nenhancing sample quality. A subjective human evaluation (mean opinion score,\nMOS) of a single speaker dataset indicates that our proposed method\ndemonstrates similarity to human quality while generating 22.05 kHz\nhigh-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We\nfurther show the generality of HiFi-GAN to the mel-spectrogram inversion of\nunseen speakers and end-to-end speech synthesis. Finally, a small footprint\nversion of HiFi-GAN generates samples 13.4 times faster than real-time on CPU\nwith comparable quality to an autoregressive counterpart.", "published": "2020-10-12 12:33:43", "link": "http://arxiv.org/abs/2010.05646v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditioning Trick for Training Stable GANs", "abstract": "In this paper we propose a conditioning trick, called difference departure\nfrom normality, applied on the generator network in response to instability\nissues during GAN training. We force the generator to get closer to the\ndeparture from normality function of real samples computed in the spectral\ndomain of Schur decomposition. This binding makes the generator amenable to\ntruncation and does not limit exploring all the possible modes. We slightly\nmodify the BigGAN architecture incorporating residual network for synthesizing\n2D representations of audio signals which enables reconstructing high quality\nsounds with some preserved phase information. Additionally, the proposed\nconditional training scenario makes a trade-off between fidelity and variety\nfor the generated spectrograms. The experimental results on UrbanSound8k and\nESC-50 environmental sound datasets and the Mozilla common voice dataset have\nshown that the proposed GAN configuration with the conditioning trick\nremarkably outperforms baseline architectures, according to three objective\nmetrics: inception score, Frechet inception distance, and signal-to-noise\nratio.", "published": "2020-10-12 16:50:22", "link": "http://arxiv.org/abs/2010.05844v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discriminative Sounding Objects Localization via Self-supervised\n  Audiovisual Matching", "abstract": "Discriminatively localizing sounding objects in cocktail-party, i.e., mixed\nsound scenes, is commonplace for humans, but still challenging for machines. In\nthis paper, we propose a two-stage learning framework to perform\nself-supervised class-aware sounding object localization. First, we propose to\nlearn robust object representations by aggregating the candidate sound\nlocalization results in the single source scenes. Then, class-aware object\nlocalization maps are generated in the cocktail-party scenarios by referring\nthe pre-learned object knowledge, and the sounding objects are accordingly\nselected by matching audio and visual object category distributions, where the\naudiovisual consistency is viewed as the self-supervised signal. Experimental\nresults in both realistic and synthesized cocktail-party videos demonstrate\nthat our model is superior in filtering out silent objects and pointing out the\nlocation of sounding objects of different classes. Code is available at\nhttps://github.com/DTaoo/Discriminative-Sounding-Objects-Localization.", "published": "2020-10-12 05:51:55", "link": "http://arxiv.org/abs/2010.05466v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
