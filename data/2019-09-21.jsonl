{"title": "Self-attention based end-to-end Hindi-English Neural Machine Translation", "abstract": "Machine Translation (MT) is a zone of concentrate in Natural Language\nprocessing which manages the programmed interpretation of human language,\nstarting with one language then onto the next by the PC. Having a rich research\nhistory spreading over about three decades, Machine interpretation is a\nstandout amongst the most looked for after region of research in the\ncomputational linguistics network. As a piece of this current ace's proposal,\nthe fundamental center examines the Deep-learning based strategies that have\ngained critical ground as of late and turning into the de facto strategy in MT.\nWe would like to point out the recent advances that have been put forward in\nthe field of Neural Translation models, different domains under which NMT has\nreplaced conventional SMT models and would also like to mention future avenues\nin the field. Consequently, we propose an end-to-end self-attention transformer\nnetwork for Neural Machine Translation, trained on Hindi-English parallel\ncorpus and compare the model's efficiency with other state of art models like\nencoder-decoder and attention-based encoder-decoder neural models on the basis\nof BLEU. We conclude this paper with a comparative analysis of the three\nproposed models.", "published": "2019-09-21 06:16:52", "link": "http://arxiv.org/abs/1909.09779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Rank Approximation of Matrices for PMI-based Word Embeddings", "abstract": "We perform an empirical evaluation of several methods of low-rank\napproximation in the problem of obtaining PMI-based word embeddings. All word\nvectors were trained on parts of a large corpus extracted from English\nWikipedia (enwik9) which was divided into two equal-sized datasets, from which\nPMI matrices were obtained. A repeated measures design was used in assigning a\nmethod of low-rank approximation (SVD, NMF, QR) and dimensionality of the\nvectors (250, 500) to each of the PMI matrix replicates. Our experiments show\nthat word vectors obtained from the truncated SVD achieve the best performance\non two downstream tasks, similarity and analogy, compare to the other two\nlow-rank approximation methods.", "published": "2019-09-21 16:58:46", "link": "http://arxiv.org/abs/1909.09855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Timelines by Modeling Semantic Change", "abstract": "Though languages can evolve slowly, they can also react strongly to dramatic\nworld events. By studying the connection between words and events, it is\npossible to identify which events change our vocabulary and in what way. In\nthis work, we tackle the task of creating timelines - records of historical\n\"turning points\", represented by either words or events, to understand the\ndynamics of a target word. Our approach identifies these points by leveraging\nboth static and time-varying word embeddings to measure the influence of words\nand events. In addition to quantifying changes, we show how our technique can\nhelp isolate semantic changes. Our qualitative and quantitative evaluations\nshow that we are able to capture this semantic change and event influence.", "published": "2019-09-21 21:57:38", "link": "http://arxiv.org/abs/1909.09907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role\n  Labeling", "abstract": "Semantic role labeling (SRL) is the task of identifying predicates and\nlabeling argument spans with semantic roles. Even though most semantic-role\nformalisms are built upon constituent syntax and only syntactic constituents\ncan be labeled as arguments (e.g., FrameNet and PropBank), all the recent work\non syntax-aware SRL relies on dependency representations of syntax. In\ncontrast, we show how graph convolutional networks (GCNs) can be used to encode\nconstituent structures and inform an SRL system. Nodes in our SpanGCN\ncorrespond to constituents. The computation is done in 3 stages. First, initial\nnode representations are produced by `composing' word representations of the\nfirst and the last word in the constituent. Second, graph convolutions relying\non the constituent tree are performed, yielding syntactically-informed\nconstituent representations. Finally, the constituent representations are\n`decomposed' back into word representations which in turn are used as input to\nthe SRL classifier. We evaluate SpanGCN against alternatives, including a model\nusing GCNs over dependency trees, and show its effectiveness on standard\nCoNLL-2005, CoNLL-2012, and FrameNet benchmarks.", "published": "2019-09-21 11:37:23", "link": "http://arxiv.org/abs/1909.09814v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visuallly Grounded Generation of Entailments from Premises", "abstract": "Natural Language Inference (NLI) is the task of determining the semantic\nrelationship between a premise and a hypothesis. In this paper, we focus on the\n{\\em generation} of hypotheses from premises in a multimodal setting, to\ngenerate a sentence (hypothesis) given an image and/or its description\n(premise) as the input. The main goals of this paper are (a) to investigate\nwhether it is reasonable to frame NLI as a generation task; and (b) to consider\nthe degree to which grounding textual premises in visual information is\nbeneficial to generation. We compare different neural architectures, showing\nthrough automatic and human evaluation that entailments can indeed be generated\nsuccessfully. We also show that multimodal models outperform unimodal models in\nthis task, albeit marginally.", "published": "2019-09-21 07:56:09", "link": "http://arxiv.org/abs/1909.09788v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Application of Fuzzy Clustering for Text Data Dimensionality Reduction", "abstract": "Large textual corpora are often represented by the document-term frequency\nmatrix whose elements are the frequency of terms; however, this matrix has two\nproblems: sparsity and high dimensionality. Four dimension reduction strategies\nare used to address these problems. Of the four strategies, unsupervised\nfeature transformation (UFT) is a popular and efficient strategy to map the\nterms to a new basis in the document-term frequency matrix. Although several\nUFT-based methods have been developed, fuzzy clustering has not been considered\nfor dimensionality reduction. This research explores fuzzy clustering as a new\nUFT-based approach to create a lower-dimensional representation of documents.\nPerformance of fuzzy clustering with and without using global term weighting\nmethods is shown to exceed principal component analysis and singular value\ndecomposition. This study also explores the effect of applying different\nfuzzifier values on fuzzy clustering for dimensionality reduction purpose.", "published": "2019-09-21 03:15:04", "link": "http://arxiv.org/abs/1909.10881v1", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "cs.CL"}
