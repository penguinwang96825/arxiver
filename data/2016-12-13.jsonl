{"title": "Evaluating Automatic Speech Recognition Systems in Comparison With Human\n  Perception Results Using Distinctive Feature Measures", "abstract": "This paper describes methods for evaluating automatic speech recognition\n(ASR) systems in comparison with human perception results, using measures\nderived from linguistic distinctive features. Error patterns in terms of\nmanner, place and voicing are presented, along with an examination of confusion\nmatrices via a distinctive-feature-distance metric. These evaluation methods\ncontrast with conventional performance criteria that focus on the phone or word\nlevel, and are intended to provide a more detailed profile of ASR system\nperformance,as well as a means for direct comparison with human perception\nresults at the sub-phonemic level.", "published": "2016-12-13 01:18:39", "link": "http://arxiv.org/abs/1612.03990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance Improvements of Probabilistic Transcript-adapted ASR with\n  Recurrent Neural Network and Language-specific Constraints", "abstract": "Mismatched transcriptions have been proposed as a mean to acquire\nprobabilistic transcriptions from non-native speakers of a language.Prior work\nhas demonstrated the value of these transcriptions by successfully adapting\ncross-lingual ASR systems for different tar-get languages. In this work, we\ndescribe two techniques to refine these probabilistic transcriptions: a\nnoisy-channel model of non-native phone misperception is trained using a\nrecurrent neural net-work, and decoded using minimally-resourced\nlanguage-dependent pronunciation constraints. Both innovations improve quality\nof the transcript, and both innovations reduce phone error rate of a\ntrainedASR, by 7% and 9% respectively", "published": "2016-12-13 01:25:14", "link": "http://arxiv.org/abs/1612.03991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vicinity-Driven Paragraph and Sentence Alignment for Comparable Corpora", "abstract": "Parallel corpora have driven great progress in the field of Text\nSimplification. However, most sentence alignment algorithms either offer a\nlimited range of alignment types supported, or simply ignore valuable clues\npresent in comparable documents. We address this problem by introducing a new\nset of flexible vicinity-driven paragraph and sentence alignment algorithms\nthat 1-N, N-1, N-N and long distance null alignments without the need for\nhard-to-replicate supervised models.", "published": "2016-12-13 12:03:39", "link": "http://arxiv.org/abs/1612.04113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Context Matching for Machine Comprehension", "abstract": "Previous machine comprehension (MC) datasets are either too small to train\nend-to-end deep learning models, or not difficult enough to evaluate the\nability of current MC techniques. The newly released SQuAD dataset alleviates\nthese limitations, and gives us a chance to develop more realistic MC models.\nBased on this dataset, we propose a Multi-Perspective Context Matching (MPCM)\nmodel, which is an end-to-end system that directly predicts the answer\nbeginning and ending points in a passage. Our model first adjusts each\nword-embedding vector in the passage by multiplying a relevancy weight computed\nagainst the question. Then, we encode the question and weighted passage by\nusing bi-directional LSTMs. For each point in the passage, our model matches\nthe context of this point against the encoded question from multiple\nperspectives and produces a matching vector. Given those matched vectors, we\nemploy another bi-directional LSTM to aggregate all the information and predict\nthe beginning and ending points. Experimental result on the test set of SQuAD\nshows that our model achieves a competitive result on the leaderboard.", "published": "2016-12-13 14:49:47", "link": "http://arxiv.org/abs/1612.04211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Large Machine Reading-Comprehension Datasets using Paragraph\n  Vectors", "abstract": "We present a dual contribution to the task of machine reading-comprehension:\na technique for creating large-sized machine-comprehension (MC) datasets using\nparagraph-vector models; and a novel, hybrid neural-network architecture that\ncombines the representation power of recurrent neural networks with the\ndiscriminative power of fully-connected multi-layered networks. We use the\nMC-dataset generation technique to build a dataset of around 2 million\nexamples, for which we empirically determine the high-ceiling of human\nperformance (around 91% accuracy), as well as the performance of a variety of\ncomputer models. Among all the models we have experimented with, our hybrid\nneural-network architecture achieves the highest performance (83.2% accuracy).\nThe remaining gap to the human-performance ceiling provides enough room for\nfuture model improvements.", "published": "2016-12-13 20:22:36", "link": "http://arxiv.org/abs/1612.04342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Hash-tag Videos with Tag2Vec", "abstract": "User-given tags or labels are valuable resources for semantic understanding\nof visual media such as images and videos. Recently, a new type of labeling\nmechanism known as hash-tags have become increasingly popular on social media\nsites. In this paper, we study the problem of generating relevant and useful\nhash-tags for short video clips. Traditional data-driven approaches for tag\nenrichment and recommendation use direct visual similarity for label transfer\nand propagation. We attempt to learn a direct low-cost mapping from video to\nhash-tags using a two step training process. We first employ a natural language\nprocessing (NLP) technique, skip-gram models with neural network training to\nlearn a low-dimensional vector representation of hash-tags (Tag2Vec) using a\ncorpus of 10 million hash-tags. We then train an embedding function to map\nvideo features to the low-dimensional Tag2vec space. We learn this embedding\nfor 29 categories of short video clips with hash-tags. A query video without\nany tag-information can then be directly mapped to the vector space of tags\nusing the learned embedding and relevant tags can be found by performing a\nsimple nearest-neighbor retrieval in the Tag2Vec space. We validate the\nrelevance of the tags suggested by our system qualitatively and quantitatively\nwith a user study.", "published": "2016-12-13 08:32:02", "link": "http://arxiv.org/abs/1612.04061v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Neural Language Models with a Continuous Cache", "abstract": "We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.", "published": "2016-12-13 23:09:49", "link": "http://arxiv.org/abs/1612.04426v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information Extraction with Character-level Neural Networks and Free\n  Noisy Supervision", "abstract": "We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text.", "published": "2016-12-13 12:12:20", "link": "http://arxiv.org/abs/1612.04118v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Models of retrieval in sentence comprehension: A computational\n  evaluation using Bayesian hierarchical modeling", "abstract": "Research on interference has provided evidence that the formation of\ndependencies between non-adjacent words relies on a cue-based retrieval\nmechanism. Two different models can account for one of the main predictions of\ninterference, i.e., a slowdown at a retrieval site, when several items share a\nfeature associated with a retrieval cue: Lewis and Vasishth's (2005)\nactivation-based model and McElree's (2000) direct access model. Even though\nthese two models have been used almost interchangeably, they are based on\ndifferent assumptions and predict differences in the relationship between\nreading times and response accuracy. The activation-based model follows the\nassumptions of ACT-R, and its retrieval process behaves as a lognormal race\nbetween accumulators of evidence with a single variance. Under this model,\naccuracy of the retrieval is determined by the winner of the race and retrieval\ntime by its rate of accumulation. In contrast, the direct access model assumes\na model of memory where only the probability of retrieval varies between items;\nin this model, differences in latencies are a by-product of the possibility and\nrepairing incorrect retrievals. We implemented both models in a Bayesian\nhierarchical framework in order to evaluate them and compare them. We show that\nsome aspects of the data are better fit under the direct access model than\nunder the activation-based model. We suggest that this finding does not rule\nout the possibility that retrieval may be behaving as a race model with\nassumptions that follow less closely the ones from the ACT-R framework. We show\nthat by introducing a modification of the activation model, i.e, by assuming\nthat the accumulation of evidence for retrieval of incorrect items is not only\nslower but noisier (i.e., different variances for the correct and incorrect\nitems), the model can provide a fit as good as the one of the direct access\nmodel.", "published": "2016-12-13 13:55:39", "link": "http://arxiv.org/abs/1612.04174v2", "categories": ["cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "You Are What You Eat... Listen to, Watch, and Read", "abstract": "This article describes a data driven method for deriving the relationship\nbetween personality and media preferences. A qunatifiable representation of\nsuch a relationship can be leveraged for use in recommendation systems and\nameliorate the \"cold start\" problem. Here, the data is comprised of an original\ncollection of 1,316 Okcupid dating profiles. Of these profiles, 800 are labeled\nwith one of 16 possible Myers-Briggs Type Indicators (MBTI). A personality\nspecific topic model describing a person's favorite books, movies, shows,\nmusic, and food was generated using latent Dirichlet allocation (LDA). There\nwere several significant findings, for example, intuitive thinking types\npreferred sci-fi/fantasy entertainment, extraversion correlated positively with\nupbeat dance music, and jazz, folk, and international cuisine correlated\npositively with those characterized by openness to experience. Many other\ncorrelations confirmed previous findings describing the relationship among\npersonality, writing style, and personal preferences. (For complete\nword/personality type assocations see the Appendix).", "published": "2016-12-13 21:29:05", "link": "http://arxiv.org/abs/1612.04403v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation", "abstract": "Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.", "published": "2016-12-13 22:09:24", "link": "http://arxiv.org/abs/1612.04418v1", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.LG", "stat.ML", "H.1.2; H.5.2"], "primary_category": "cs.IR"}
