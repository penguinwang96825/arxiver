{"title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "abstract": "There is an explosion of data, documents, and other content, and people\nrequire tools to analyze and interpret these, tools to turn the content into\ninformation and knowledge. Topic modeling have been developed to solve these\nproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns\nin data to be extracted automatically. When analyzing texts, these patterns are\ncalled topics. Among numerous extensions of LDA, few of them can reliably\nanalyze multiple groups of documents and extract topic similarities. Recently,\nthe introduction of differential topic modeling (SPDP) [Chen et. al. 2012]\nperforms uniformly better than many topic models in a discriminative setting.\n  There is also a need to improve the sampling speed for topic models. While\nsome effort has been made for distributed algorithms, there is no work\ncurrently done using graphical processing units (GPU). Note the GPU framework\nhas already become the most cost-efficient platform for many problems.\n  In this thesis, I propose and implement a scalable multi-GPU distributed\nparallel framework which approximates SPDP. Through experiments, I have shown\nmy algorithms have a gain in speed of about 50 times while being almost as\naccurate, with only one single cheap laptop GPU. Furthermore, I have shown the\nspeed improvement is sublinearly scalable when multiple GPUs are used, while\nfairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the\nspeed improvement could potentially reach a factor of a thousand.\n  Note SPDP is just a representative of other extensions of LDA. Although my\nalgorithm is implemented to work with SPDP, it is designed to be a general\nenough to work with other topic models. The speed-up on smaller collections\n(i.e., 1000s of documents), means that these more complex LDA extensions could\nnow be done in real-time, thus opening up a new way of using these LDA models\nin industry.", "published": "2015-10-22 09:40:54", "link": "http://arxiv.org/abs/1510.06549v1", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya\n  Parameters and Topic Models", "abstract": "Hyper-parameters play a major role in the learning and inference process of\nlatent Dirichlet allocation (LDA). In order to begin the LDA latent variables\nlearning process, these hyper-parameters values need to be pre-determined. We\npropose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs\nNewton' (LDA-GN), which places non-informative priors over these\nhyper-parameters and uses Gibbs sampling to learn appropriate values for them.\nAt the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new\ntechnique for learning the parameters of multivariate Polya distributions. We\nreport Gibbs-Newton performance results compared with two prominent existing\napproaches to the latter task: Minka's fixed-point iteration method and the\nMoments method. We then evaluate LDA-GN in two ways: (i) by comparing it with\nstandard LDA in terms of the ability of the resulting topic models to\ngeneralize to unseen documents; (ii) by comparing it with standard LDA in its\nperformance on a binary classification task.", "published": "2015-10-22 14:39:58", "link": "http://arxiv.org/abs/1510.06646v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet\n  Language", "abstract": "We present a new computational technique to detect and analyze statistically\nsignificant geographic variation in language. Our meta-analysis approach\ncaptures statistical properties of word usage across geographical regions and\nuses statistical methods to identify significant changes specific to regions.\nWhile previous approaches have primarily focused on lexical variation between\nregions, our method identifies words that demonstrate semantic and syntactic\nvariation as well.\n  We extend recently developed techniques for neural language models to learn\nword representations which capture differing semantics across geographical\nregions. In order to quantify this variation and ensure robust detection of\ntrue regional differences, we formulate a null model to determine whether\nobserved changes are statistically significant. Our method is the first such\napproach to explicitly account for random variation due to chance while\ndetecting regional variation in word meaning.\n  To validate our model, we study and analyze two different massive online data\nsets: millions of tweets from Twitter spanning not only four different\ncountries but also fifty states, as well as millions of phrases contained in\nthe Google Book Ngrams. Our analysis reveals interesting facets of language\nchange at multiple scales of geographic resolution -- from neighboring states\nto distant continents.\n  Finally, using our model, we propose a measure of semantic distance between\nlanguages. Our analysis of British and American English over a period of 100\nyears reveals that semantic variation between these dialects is shrinking.", "published": "2015-10-22 22:53:10", "link": "http://arxiv.org/abs/1510.06786v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
