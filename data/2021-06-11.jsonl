{"title": "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language\n  Generation", "abstract": "A well-known limitation in pretrain-finetune paradigm lies in its\ninflexibility caused by the one-size-fits-all vocabulary. This potentially\nweakens the effect when applying pretrained models into natural language\ngeneration (NLG) tasks, especially for the subword distributions between\nupstream and downstream tasks with significant discrepancy. Towards approaching\nthis problem, we extend the vanilla pretrain-finetune pipeline with an extra\nembedding transfer step. Specifically, a plug-and-play embedding generator is\nintroduced to produce the representation of any input token, according to\npre-trained embeddings of its morphologically similar ones. Thus, embeddings of\nmismatch tokens in downstream tasks can also be efficiently initialized. We\nconduct experiments on a variety of NLG tasks under the pretrain-finetune\nfashion. Experimental results and extensive analyses show that the proposed\nstrategy offers us opportunities to feel free to transfer the vocabulary,\nleading to more efficient and better performed downstream NLG models.", "published": "2021-06-11 02:16:13", "link": "http://arxiv.org/abs/2106.06125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives", "abstract": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.", "published": "2021-06-11 02:40:06", "link": "http://arxiv.org/abs/2106.06132v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards User-Driven Neural Machine Translation", "abstract": "A good translation should not only translate the original content\nsemantically, but also incarnate personal traits of the original text. For a\nreal-world neural machine translation (NMT) system, these user traits (e.g.,\ntopic preference, stylistic characteristics and expression habits) can be\npreserved in user behavior (e.g., historical inputs). However, current NMT\nsystems marginally consider the user behavior due to: 1) the difficulty of\nmodeling user portraits in zero-shot scenarios, and 2) the lack of\nuser-behavior annotated parallel dataset. To fill this gap, we introduce a\nnovel framework called user-driven NMT. Specifically, a cache-based module and\na user-driven contrastive learning method are proposed to offer NMT the ability\nto capture potential user traits from their historical inputs under a zero-shot\nlearning fashion. Furthermore, we contribute the first Chinese-English parallel\ncorpus annotated with user behavior called UDT-Corpus. Experimental results\nconfirm that the proposed user-driven NMT can generate user-specific\ntranslations.", "published": "2021-06-11 07:06:20", "link": "http://arxiv.org/abs/2106.06200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity\n  understanding and detection", "abstract": "Traditional toxicity detection models have focused on the single utterance\nlevel without deeper understanding of context. We introduce CONDA, a new\ndataset for in-game toxic language detection enabling joint intent\nclassification and slot filling analysis, which is the core task of Natural\nLanguage Understanding (NLU). The dataset consists of 45K utterances from 12K\nconversations from the chat logs of 1.9K completed Dota 2 matches. We propose a\nrobust dual semantic-level toxicity framework, which handles utterance and\ntoken-level patterns, and rich contextual chatting history. Accompanying the\ndataset is a thorough in-game toxicity analysis, which provides comprehensive\nunderstanding of context at utterance, token, and dual levels. Inspired by NLU,\nwe also apply its metrics to the toxicity detection tasks for assessing\ntoxicity and game-specific aspects. We evaluate strong NLU models on CONDA,\nproviding fine-grained results for different intent classes and slot classes.\nFurthermore, we examine the coverage of toxicity nature in our dataset by\ncomparing it with other toxicity datasets.", "published": "2021-06-11 07:42:12", "link": "http://arxiv.org/abs/2106.06213v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via\n  Synchronous Semantic Decoding", "abstract": "Semantic parsing is challenging due to the structure gap and the semantic gap\nbetween utterances and logical forms. In this paper, we propose an unsupervised\nsemantic parsing method - Synchronous Semantic Decoding (SSD), which can\nsimultaneously resolve the semantic gap and the structure gap by jointly\nleveraging paraphrasing and grammar constrained decoding. Specifically, we\nreformulate semantic parsing as a constrained paraphrasing problem: given an\nutterance, our model synchronously generates its canonical utterance and\nmeaning representation. During synchronous decoding: the utterance paraphrasing\nis constrained by the structure of the logical form, therefore the canonical\nutterance can be paraphrased controlledly; the semantic decoding is guided by\nthe semantics of the canonical utterance, therefore its logical form can be\ngenerated unsupervisedly. Experimental results show that SSD is a promising\napproach and can achieve competitive unsupervised semantic parsing performance\non multiple datasets.", "published": "2021-06-11 08:16:35", "link": "http://arxiv.org/abs/2106.06228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Discussion on Building Practical NLP Leaderboards: The Case of Machine\n  Translation", "abstract": "Recent advances in AI and ML applications have benefited from rapid progress\nin NLP research. Leaderboards have emerged as a popular mechanism to track and\naccelerate progress in NLP through competitive model development. While this\nhas increased interest and participation, the over-reliance on single, and\naccuracy-based metrics have shifted focus from other important metrics that\nmight be equally pertinent to consider in real-world contexts. In this paper,\nwe offer a preliminary discussion of the risks associated with focusing\nexclusively on accuracy metrics and draw on recent discussions to highlight\nprescriptive suggestions on how to develop more practical and effective\nleaderboards that can better reflect the real-world utility of models.", "published": "2021-06-11 10:24:35", "link": "http://arxiv.org/abs/2106.06292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word\n  Alignment", "abstract": "The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.", "published": "2021-06-11 13:36:01", "link": "http://arxiv.org/abs/2106.06381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised and Unsupervised Sense Annotation via Translations", "abstract": "Acquisition of multilingual training data continues to be a challenge in word\nsense disambiguation (WSD). To address this problem, unsupervised approaches\nhave been proposed to automatically generate sense annotations for training\nsupervised WSD systems. We present three new methods for creating\nsense-annotated corpora which leverage translations, parallel bitexts, lexical\nresources, as well as contextual and synset embeddings. Our semi-supervised\nmethod applies machine translation to transfer existing sense annotations to\nother languages. Our two unsupervised methods refine sense annotations produced\nby a knowledge-based WSD system via lexical translations in a parallel corpus.\nWe obtain state-of-the-art results on standard WSD benchmarks.", "published": "2021-06-11 15:32:46", "link": "http://arxiv.org/abs/2106.06462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Should Agents Ask Questions For Situated Learning? An Annotated\n  Dialogue Corpus", "abstract": "Intelligent agents that are confronted with novel concepts in situated\nenvironments will need to ask their human teammates questions to learn about\nthe physical world. To better understand this problem, we need data about\nasking questions in situated task-based interactions. To this end, we present\nthe Human-Robot Dialogue Learning (HuRDL) Corpus - a novel dialogue corpus\ncollected in an online interactive virtual environment in which human\nparticipants play the role of a robot performing a collaborative\ntool-organization task. We describe the corpus data and a corresponding\nannotation scheme to offer insight into the form and content of questions that\nhumans ask to facilitate learning in a situated environment. We provide the\ncorpus as an empirically-grounded resource for improving question generation in\nsituated intelligent agents.", "published": "2021-06-11 16:58:22", "link": "http://arxiv.org/abs/2106.06504v1", "categories": ["cs.CL", "I.2.7; J.4; J.5"], "primary_category": "cs.CL"}
{"title": "Sample-efficient Linguistic Generalizations through Program Synthesis:\n  Experiments with Phonology Problems", "abstract": "Neural models excel at extracting statistical patterns from large amounts of\ndata, but struggle to learn patterns or reason about language from only a few\nexamples. In this paper, we ask: Can we learn explicit rules that generalize\nwell from only a few examples? We explore this question using program\nsynthesis. We develop a synthesis model to learn phonology rules as programs in\na domain-specific language. We test the ability of our models to generalize\nfrom few training examples using our new dataset of problems from the\nLinguistics Olympiad, a challenging set of tasks that require strong linguistic\nreasoning ability. In addition to being highly sample-efficient, our approach\ngenerates human-readable programs, and allows control over the generalizability\nof the learnt programs.", "published": "2021-06-11 18:36:07", "link": "http://arxiv.org/abs/2106.06566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Language Usage and Listener Engagement in Podcasts", "abstract": "While there is an abundance of popular writing targeted to podcast creators\non how to speak in ways that engage their listeners, there has been little\ndata-driven analysis of podcasts that relates linguistic style with listener\nengagement. In this paper, we investigate how various factors -- vocabulary\ndiversity, distinctiveness, emotion, and syntax, among others -- correlate with\nengagement, based on analysis of the creators' written descriptions and\ntranscripts of the audio. We build models with different textual\nrepresentations, and show that the identified features are highly predictive of\nengagement. Our analysis tests popular wisdom about stylistic elements in\nhigh-engagement podcasts, corroborating some aspects, and adding new\nperspectives on others.", "published": "2021-06-11 20:40:15", "link": "http://arxiv.org/abs/2106.06605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Political Prudence of Open-domain Chatbots", "abstract": "Politically sensitive topics are still a challenge for open-domain chatbots.\nHowever, dealing with politically sensitive content in a responsible,\nnon-partisan, and safe behavior way is integral for these chatbots. Currently,\nthe main approach to handling political sensitivity is by simply changing such\na topic when it is detected. This is safe but evasive and results in a chatbot\nthat is less engaging. In this work, as a first step towards a politically safe\nchatbot, we propose a group of metrics for assessing their political prudence.\nWe then conduct political prudence analysis of various chatbots and discuss\ntheir behavior from multiple angles through our automatic metric and human\nevaluation metrics. The testsets and codebase are released to promote research\nin this area.", "published": "2021-06-11 04:03:53", "link": "http://arxiv.org/abs/2106.06157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from\n  Limited Personalized Data", "abstract": "Maintaining consistent personas is essential for dialogue agents. Although\ntremendous advancements have been brought, the limited-scale of annotated\npersona-dense data are still barriers towards training robust and consistent\npersona-based dialogue models. In this work, we show how the challenges can be\naddressed by disentangling persona-based dialogue generation into two sub-tasks\nwith a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a\nBERT-based encoder and two BERT-based decoders, where one decoder is for\nresponse generation, and another is for consistency understanding. In\nparticular, to learn the ability of consistency understanding from large-scale\nnon-dialogue inference data, we train the second decoder in an unlikelihood\nmanner. Under different limited data settings, both automatic and human\nevaluations demonstrate that the proposed model outperforms strong baselines in\nresponse quality and persona consistency.", "published": "2021-06-11 05:02:05", "link": "http://arxiv.org/abs/2106.06169v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving RNN-T ASR Performance with Date-Time and Location Awareness", "abstract": "In this paper, we explore the benefits of incorporating context into a\nRecurrent Neural Network (RNN-T) based Automatic Speech Recognition (ASR) model\nto improve the speech recognition for virtual assistants. Specifically, we use\nmeta information extracted from the time at which the utterance is spoken and\nthe approximate location information to make ASR context aware. We show that\nthese contextual information, when used individually, improves overall\nperformance by as much as 3.48% relative to the baseline and when the contexts\nare combined, the model learns complementary features and the recognition\nimproves by 4.62%. On specific domains, these contextual signals show\nimprovements as high as 11.5%, without any significant degradation on others.\nWe ran experiments with models trained on data of sizes 30K hours and 10K\nhours. We show that the scale of improvement with the 10K hours dataset is much\nhigher than the one obtained with 30K hours dataset. Our results indicate that\nwith limited data to train the ASR model, contextual signals can improve the\nperformance significantly.", "published": "2021-06-11 05:57:30", "link": "http://arxiv.org/abs/2106.06183v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Sprachsynthese -- State-of-the-Art in englischer und deutscher Sprache", "abstract": "Reading text aloud is an important feature for modern computer applications.\nIt not only facilitates access to information for visually impaired people, but\nis also a pleasant convenience for non-impaired users. In this article, the\nstate of the art of speech synthesis is presented separately for\nmel-spectrogram generation and vocoders. It concludes with an overview of\navailable data sets for English and German with a discussion of the\ntransferability of the good speech synthesis results from English to German\nlanguage.", "published": "2021-06-11 08:25:08", "link": "http://arxiv.org/abs/2106.06230v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedNLP: An interpretable NLP System to Decode Federal Reserve\n  Communications", "abstract": "The Federal Reserve System (the Fed) plays a significant role in affecting\nmonetary policy and financial conditions worldwide. Although it is important to\nanalyse the Fed's communications to extract useful information, it is generally\nlong-form and complex due to the ambiguous and esoteric nature of content. In\nthis paper, we present FedNLP, an interpretable multi-component Natural\nLanguage Processing system to decode Federal Reserve communications. This\nsystem is designed for end-users to explore how NLP techniques can assist their\nholistic understanding of the Fed's communications with NO coding. Behind the\nscenes, FedNLP uses multiple NLP models from traditional machine learning\nalgorithms to deep neural network architectures in each downstream task. The\ndemonstration shows multiple results at once including sentiment analysis,\nsummary of the document, prediction of the Federal Funds Rate movement and\nvisualization for interpreting the prediction model's result.", "published": "2021-06-11 08:58:36", "link": "http://arxiv.org/abs/2106.06247v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Language Models for Continuously Evolving Content", "abstract": "The content on the web is in a constant state of flux. New entities, issues,\nand ideas continuously emerge, while the semantics of the existing conversation\ntopics gradually shift. In recent years, pre-trained language models like BERT\ngreatly improved the state-of-the-art for a large spectrum of content\nunderstanding tasks. Therefore, in this paper, we aim to study how these\nlanguage models can be adapted to better handle continuously evolving web\ncontent. In our study, we first analyze the evolution of 2013 - 2019 Twitter\ndata, and unequivocally confirm that a BERT model trained on past tweets would\nheavily deteriorate when directly applied to data from later years. Then, we\ninvestigate two possible sources of the deterioration: the semantic shift of\nexisting tokens and the sub-optimal or failed understanding of new tokens. To\nthis end, we both explore two different vocabulary composition methods, as well\nas propose three sampling methods which help in efficient incremental training\nfor BERT-like models. Compared to a new model trained from scratch offline, our\nincremental training (a) reduces the training costs, (b) achieves better\nperformance on evolving content, and (c) is suitable for online deployment. The\nsuperiority of our methods is validated using two downstream tasks. We\ndemonstrate significant improvements when incrementally evolving the model from\na particular base year, on the task of Country Hashtag Prediction, as well as\non the OffensEval 2019 task.", "published": "2021-06-11 10:33:50", "link": "http://arxiv.org/abs/2106.06297v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word\n  Substitution", "abstract": "Recent studies show that neural natural language processing (NLP) models are\nvulnerable to backdoor attacks. Injected with backdoors, models perform\nnormally on benign examples but produce attacker-specified predictions when the\nbackdoor is activated, presenting serious security threats to real-world\napplications. Since existing textual backdoor attacks pay little attention to\nthe invisibility of backdoors, they can be easily detected and blocked. In this\nwork, we present invisible backdoors that are activated by a learnable\ncombination of word substitution. We show that NLP models can be injected with\nbackdoors that lead to a nearly 100% attack success rate, whereas being highly\ninvisible to existing defense strategies and even human inspections. The\nresults raise a serious alarm to the security of NLP models, which requires\nfurther research to be resolved. All the data and code of this paper are\nreleased at https://github.com/thunlp/BkdAtk-LWS.", "published": "2021-06-11 13:03:17", "link": "http://arxiv.org/abs/2106.06361v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "To Beam Or Not To Beam: That is a Question of Cooperation for Language\n  GANs", "abstract": "Due to the discrete nature of words, language GANs require to be optimized\nfrom rewards provided by discriminator networks, via reinforcement learning\nmethods. This is a much harder setting than for continuous tasks, which enjoy\ngradient flows from discriminators to generators, usually leading to dramatic\nlearning instabilities. However, we claim that this can be solved by making\ndiscriminator and generator networks cooperate to produce output sequences\nduring training. These cooperative outputs, inherently built to obtain higher\ndiscrimination scores, not only provide denser rewards for training, but also\nform a more compact artificial set for discriminator training, hence improving\nits accuracy and stability. In this paper, we show that our SelfGAN framework,\nbuilt on this cooperative principle, outperforms Teacher Forcing and obtains\nstate-of-the-art results on two challenging tasks, Summarization and Question\nGeneration.", "published": "2021-06-11 13:04:42", "link": "http://arxiv.org/abs/2106.06363v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers", "abstract": "Controlling neural network-based models for natural language generation (NLG)\nhas broad applications in numerous areas such as machine translation, document\nsummarization, and dialog systems. Approaches that enable such control in a\nzero-shot manner would be of great importance as, among other reasons, they\nremove the need for additional annotated data and training. In this work, we\npropose novel approaches for controlling encoder-decoder transformer-based NLG\nmodels in zero-shot. This is done by introducing three control knobs, namely,\nattention biasing, decoder mixing, and context augmentation, that are applied\nto these models at generation time. These knobs control the generation process\nby directly manipulating trained NLG models (e.g., biasing cross-attention\nlayers) to realize the desired attributes in the generated outputs. We show\nthat not only are these NLG models robust to such manipulations, but also their\nbehavior could be controlled without an impact on their generation performance.\nThese results, to the best of our knowledge, are the first of their kind.\nThrough these control knobs, we also investigate the role of transformer\ndecoder's self-attention module and show strong evidence that its primary role\nis maintaining fluency of sentences generated by these models. Based on this\nhypothesis, we show that alternative architectures for transformer decoders\ncould be viable options. We also study how this hypothesis could lead to more\nefficient ways for training encoder-decoder transformer models.", "published": "2021-06-11 14:07:19", "link": "http://arxiv.org/abs/2106.06411v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WAX-ML: A Python library for machine learning and feedback loops on\n  streaming data", "abstract": "Wax is what you put on a surfboard to avoid slipping. It is an essential tool\nto go surfing... We introduce WAX-ML a research-oriented Python library\nproviding tools to design powerful machine learning algorithms and feedback\nloops working on streaming data. It strives to complement JAX with tools\ndedicated to time series. WAX-ML makes JAX-based programs easy to use for\nend-users working with pandas and xarray for data manipulation. It provides a\nsimple mechanism for implementing feedback loops, allows the implementation of\nonline learning and reinforcement learning algorithms with functions, and makes\nthem easy to integrate by end-users working with the object-oriented\nreinforcement learning framework from the Gym library. It is released with an\nApache open-source license on GitHub at https://github.com/eserie/wax-ml.", "published": "2021-06-11 17:42:02", "link": "http://arxiv.org/abs/2106.06524v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Local Explanation of Dialogue Response Generation", "abstract": "In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task -- dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose a new method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.", "published": "2021-06-11 17:58:36", "link": "http://arxiv.org/abs/2106.06528v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Visualization Techniques to Enhance Automated Event Extraction", "abstract": "Robust visualization of complex data is critical for the effective use of NLP\nfor event classification, as the volume of data is large and the\nhigh-dimensional structure of text makes data challenging to summarize\nsuccinctly. In event extraction tasks in particular, visualization can aid in\nunderstanding and illustrating the textual relationships from which machine\nlearning tools produce insights. Through our case study which seeks to identify\npotential triggers of state-led mass killings from news articles using NLP, we\ndemonstrate how visualizations can aid in each stage, from exploratory analysis\nof raw data, to machine learning training analysis, and finally post-inference\nvalidation.", "published": "2021-06-11 19:24:54", "link": "http://arxiv.org/abs/2106.06588v1", "categories": ["cs.CL", "cs.GR"], "primary_category": "cs.CL"}
{"title": "Leveraging Pre-trained Language Model for Speech Sentiment Analysis", "abstract": "In this paper, we explore the use of pre-trained language models to learn\nsentiment information of written texts for speech sentiment analysis. First, we\ninvestigate how useful a pre-trained language model would be in a 2-step\npipeline approach employing Automatic Speech Recognition (ASR) and\ntranscripts-based sentiment analysis separately. Second, we propose a pseudo\nlabel-based semi-supervised training strategy using a language model on an\nend-to-end speech sentiment approach to take advantage of a large, but\nunlabeled speech dataset for training. Although spoken and written texts have\ndifferent linguistic characteristics, they can complement each other in\nunderstanding sentiment. Therefore, the proposed system can not only model\nacoustic characteristics to bear sentiment-specific information in speech\nsignals, but learn latent information to carry sentiments in the text\nrepresentation. In these experiments, we demonstrate the proposed approaches\nimprove F1 scores consistently compared to systems without a language model.\nMoreover, we also show that the proposed framework can reduce 65% of human\nsupervision by leveraging a large amount of data without human sentiment\nannotation and boost performance in a low-resource condition where the human\nsentiment annotation is not available enough.", "published": "2021-06-11 20:15:21", "link": "http://arxiv.org/abs/2106.06598v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from\n  Literature", "abstract": "To interpret the genetic profile present in a patient sample, it is necessary\nto know which mutations have important roles in the development of the\ncorresponding cancer type. Named entity recognition is a core step in the text\nmining pipeline which facilitates mining valuable cancer information from the\nscientific literature. However, due to the scarcity of related datasets,\nprevious NER attempts in this domain either suffer from low performance when\ndeep learning based models are deployed, or they apply feature based machine\nlearning models or rule based models to tackle this problem, which requires\nintensive efforts from domain experts, and limit the model generalization\ncapability. In this paper, we propose EPICURE, an ensemble pre trained model\nequipped with a conditional random field pattern layer and a span prediction\npattern layer to extract cancer mutations from text. We also adopt a data\naugmentation strategy to expand our training set from multiple datasets.\nExperimental results on three benchmark datasets show competitive results\ncompared to the baseline models.", "published": "2021-06-11 09:08:15", "link": "http://arxiv.org/abs/2106.07722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RefBERT: Compressing BERT by Referencing to Pre-computed Representations", "abstract": "Recently developed large pre-trained language models, e.g., BERT, have\nachieved remarkable performance in many downstream natural language processing\napplications. These pre-trained language models often contain hundreds of\nmillions of parameters and suffer from high computation and latency in\nreal-world applications. It is desirable to reduce the computation overhead of\nthe models for fast training and inference while keeping the model performance\nin downstream applications. Several lines of work utilize knowledge\ndistillation to compress the teacher model to a smaller student model. However,\nthey usually discard the teacher's knowledge when in inference. Differently, in\nthis paper, we propose RefBERT to leverage the knowledge learned from the\nteacher, i.e., facilitating the pre-computed BERT representation on the\nreference sample and compressing BERT into a smaller student model. To\nguarantee our proposal, we provide theoretical justification on the loss\nfunction and the usage of reference samples. Significantly, the theoretical\nresult shows that including the pre-computed teacher's representations on the\nreference samples indeed increases the mutual information in learning the\nstudent model. Finally, we conduct the empirical evaluation and show that our\nRefBERT can beat the vanilla TinyBERT over 8.1\\% and achieves more than 94\\% of\nthe performance of $\\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is\n7.4x smaller and 9.5x faster on inference than BERT$_{\\rm BASE}$.", "published": "2021-06-11 01:22:08", "link": "http://arxiv.org/abs/2106.08898v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A comprehensive solution to retrieval-based chatbot construction", "abstract": "In this paper we present the results of our experiments in training and\ndeploying a self-supervised retrieval-based chatbot trained with contrastive\nlearning for assisting customer support agents. In contrast to most existing\nresearch papers in this area where the focus is on solving just one component\nof a deployable chatbot, we present an end-to-end set of solutions to take the\nreader from an unlabelled chatlogs to a deployed chatbot. This set of solutions\nincludes creating a self-supervised dataset and a weakly labelled dataset from\nchatlogs, as well as a systematic approach to selecting a fixed list of canned\nresponses. We present a hierarchical-based RNN architecture for the response\nselection model, chosen for its ability to cache intermediate utterance\nembeddings, which helped to meet deployment inference speed requirements. We\ncompare the performance of this architecture across 3 different learning\nobjectives: self-supervised contrastive learning, binary classification, and\nmulti-class classification. We find that using a self-supervised contrastive\nlearning model outperforms training the binary and multi-class classification\nmodels on a weakly labelled dataset. Our results validate that the\nself-supervised contrastive learning approach can be effectively used for a\nreal-world chatbot scenario.", "published": "2021-06-11 02:54:33", "link": "http://arxiv.org/abs/2106.06139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NAAQA: A Neural Architecture for Acoustic Question Answering", "abstract": "The goal of the Acoustic Question Answering (AQA) task is to answer a\nfree-form text question about the content of an acoustic scene. It was inspired\nby the Visual Question Answering (VQA) task. In this paper, based on the\npreviously introduced CLEAR dataset, we propose a new benchmark for AQA, namely\nCLEAR2, that emphasizes the specific challenges of acoustic inputs. These\ninclude handling of variable duration scenes, and scenes built with elementary\nsounds that differ between training and test set. We also introduce NAAQA, a\nneural architecture that leverages specific properties of acoustic inputs. The\nuse of 1D convolutions in time and frequency to process 2D spectro-temporal\nrepresentations of acoustic content shows promising results and enables\nreductions in model complexity. We show that time coordinate maps augment\ntemporal localization capabilities which enhance performance of the network by\n~17 percentage points. On the other hand, frequency coordinate maps have little\ninfluence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with\n~4 times fewer parameters than the previously explored VQA model. We evaluate\nthe perfomance of NAAQA on an independent data set reconstructed from DAQA. We\nalso test the addition of a MALiMo module in our model on both CLEAR2 and DAQA.\nWe provide a detailed analysis of the results for the different question types.\nWe release the code to produce CLEAR2 as well as NAAQA to foster research in\nthis newly emerging machine learning task.", "published": "2021-06-11 03:05:48", "link": "http://arxiv.org/abs/2106.06147v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7; I.2.10; I.5.0"], "primary_category": "cs.CL"}
{"title": "Spoken Term Detection Methods for Sparse Transcription in Very\n  Low-resource Settings", "abstract": "We investigate the efficiency of two very different spoken term detection\napproaches for transcription when the available data is insufficient to train a\nrobust ASR system. This work is grounded in very low-resource language\ndocumentation scenario where only few minutes of recording have been\ntranscribed for a given language so far.Experiments on two oral languages show\nthat a pretrained universal phone recognizer, fine-tuned with only a few\nminutes of target language speech, can be used for spoken term detection with a\nbetter overall performance than a dynamic time warping approach. In addition,\nwe show that representing phoneme recognition ambiguity in a graph structure\ncan further boost the recall while maintaining high precision in the low\nresource spoken term detection task.", "published": "2021-06-11 04:09:54", "link": "http://arxiv.org/abs/2106.06160v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Nested and Balanced Entity Recognition using Multi-Task Learning", "abstract": "Entity Recognition (ER) within a text is a fundamental exercise in Natural\nLanguage Processing, enabling further depending tasks such as Knowledge\nExtraction, Text Summarisation, or Keyphrase Extraction. An entity consists of\nsingle words or of a consecutive sequence of terms, constituting the basic\nbuilding blocks for communication. Mainstream ER approaches are mainly limited\nto flat structures, concentrating on the outermost entities while ignoring the\ninner ones. This paper introduces a partly-layered network architecture that\ndeals with the complexity of overlapping and nested cases. The proposed\narchitecture consists of two parts: (1) a shared Sequence Layer and (2) a\nstacked component with multiple Tagging Layers. The adoption of such an\narchitecture has the advantage of preventing overfit to a specific word-length,\nthus maintaining performance for longer entities despite their lower frequency.\nTo verify the proposed architecture's effectiveness, we train and evaluate this\narchitecture to recognise two kinds of entities - Concepts (CR) and Named\nEntities (NER). Our approach achieves state-of-the-art NER performances, while\nit outperforms previous CR approaches. Considering these promising results, we\nsee the possibility to evolve the architecture for other cases such as the\nextraction of events or the detection of argumentative components.", "published": "2021-06-11 07:52:32", "link": "http://arxiv.org/abs/2106.06216v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "I.2.7; I.1.3; F.4.1"], "primary_category": "cs.CL"}
{"title": "Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis\n  with Graph-based Multi-modal Context Modeling", "abstract": "Comparing with traditional text-to-speech (TTS) systems, conversational TTS\nsystems are required to synthesize speeches with proper speaking style\nconfirming to the conversational context. However, state-of-the-art context\nmodeling methods in conversational TTS only model the textual information in\ncontext with a recurrent neural network (RNN). Such methods have limited\nability in modeling the inter-speaker influence in conversations, and also\nneglect the speaking styles and the intra-speaker inertia inside each speaker.\nInspired by DialogueGCN and its superiority in modeling such conversational\ninfluences than RNN based approaches, we propose a graph-based multi-modal\ncontext modeling method and adopt it to conversational TTS to enhance the\nspeaking styles of synthesized speeches. Both the textual and speaking style\ninformation in the context are extracted and processed by DialogueGCN to model\nthe inter- and intra-speaker influence in conversations. The outputs of\nDialogueGCN are then summarized by attention mechanism, and converted to the\nenhanced speaking style for current utterance. An English conversation corpus\nis collected and annotated for our research and released to public. Experiment\nresults on this corpus demonstrate the effectiveness of our proposed approach,\nwhich outperforms the state-of-the-art context modeling method in\nconversational TTS in both MOS and ABX preference rate.", "published": "2021-06-11 08:33:52", "link": "http://arxiv.org/abs/2106.06233v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HUI-Audio-Corpus-German: A high quality TTS dataset", "abstract": "The increasing availability of audio data on the internet lead to a multitude\nof datasets for development and training of text to speech applications, based\non neural networks. Highly differing quality of voice, low sampling rates, lack\nof text normalization and disadvantageous alignment of audio samples to\ncorresponding transcript sentences still limit the performance of deep neural\nnetworks trained on this task. Additionally, data resources in languages like\nGerman are still very limited. We introduce the \"HUI-Audio-Corpus-German\", a\nlarge, open-source dataset for TTS engines, created with a processing pipeline,\nwhich produces high quality audio to transcription alignments and decreases\nmanual effort needed for creation.", "published": "2021-06-11 10:59:09", "link": "http://arxiv.org/abs/2106.06309v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR\n  Hypotheses", "abstract": "Spoken Language Understanding (SLU) systems parse speech into semantic\nstructures like dialog acts and slots. This involves the use of an Automatic\nSpeech Recognizer (ASR) to transcribe speech into multiple text alternatives\n(hypotheses). Transcription errors, common in ASRs, impact downstream SLU\nperformance negatively. Approaches to mitigate such errors involve using richer\ninformation from the ASR, either in form of N-best hypotheses or word-lattices.\nWe hypothesize that transformer models learn better with a simpler utterance\nrepresentation using the concatenation of the N-best ASR alternatives, where\neach alternative is separated by a special delimiter [SEP]. In our work, we\ntest our hypothesis by using concatenated N-best ASR alternatives as the input\nto transformer encoder models, namely BERT and XLM-RoBERTa, and achieve\nperformance equivalent to the prior state-of-the-art model on DSTC2 dataset. We\nalso show that our approach significantly outperforms the prior\nstate-of-the-art when subjected to the low data regime. Additionally, this\nmethodology is accessible to users of third-party ASR APIs which do not provide\nword-lattice information.", "published": "2021-06-11 17:29:00", "link": "http://arxiv.org/abs/2106.06519v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Break-It-Fix-It: Unsupervised Learning for Program Repair", "abstract": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.", "published": "2021-06-11 20:31:04", "link": "http://arxiv.org/abs/2106.06600v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized\n  Streaming ASR", "abstract": "Simultaneous speech-to-text translation is widely useful in many scenarios.\nThe conventional cascaded approach uses a pipeline of streaming ASR followed by\nsimultaneous MT, but suffers from error propagation and extra latency. To\nalleviate these issues, recent efforts attempt to directly translate the source\nspeech into target text simultaneously, but this is much harder due to the\ncombination of two separate tasks. We instead propose a new paradigm with the\nadvantages of both cascaded and end-to-end approaches. The key idea is to use\ntwo separate, but synchronized, decoders on streaming ASR and direct\nspeech-to-text translation (ST), respectively, and the intermediate results of\nASR guide the decoding policy of (but is not fed as input to) ST. During\ntraining time, we use multitask learning to jointly learn these two tasks with\na shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset\ndemonstrate that our proposed technique achieves substantially better\ntranslation quality at similar levels of latency.", "published": "2021-06-11 23:22:37", "link": "http://arxiv.org/abs/2106.06636v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Acoustic Models", "abstract": "There is a recent trend in machine learning to increase model quality by\ngrowing models to sizes previously thought to be unreasonable. Recent work has\nshown that autoregressive generative models with cross-entropy objective\nfunctions exhibit smooth power-law relationships, or scaling laws, that predict\nmodel quality from model size, training set size, and the available compute\nbudget. These scaling laws allow one to choose nearly optimal hyper-parameters\ngiven constraints on available training data, model parameter count, or\ntraining computation budget. In this paper, we demonstrate that acoustic models\ntrained with an auto-predictive coding loss behave as if they are subject to\nsimilar scaling laws. We extend previous work to jointly predict loss due to\nmodel size, to training set size, and to the inherent \"irreducible loss\" of the\ntask. We find that the scaling laws accurately match model performance over two\norders of magnitude in both model size and training set size, and make\npredictions about the limits of model performance.", "published": "2021-06-11 18:59:24", "link": "http://arxiv.org/abs/2106.09488v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards End-to-End Synthetic Speech Detection", "abstract": "The constant Q transform (CQT) has been shown to be one of the most effective\nspeech signal pre-transforms to facilitate synthetic speech detection, followed\nby either hand-crafted (subband) constant Q cepstral coefficient (CQCC) feature\nextraction and a back-end binary classifier, or a deep neural network (DNN)\ndirectly for further feature extraction and classification. Despite the rich\nliterature on such a pipeline, we show in this paper that the pre-transform and\nhand-crafted features could simply be replaced by end-to-end DNNs.\nSpecifically, we experimentally verify that by only using standard components,\na light-weight neural network could outperform the state-of-the-art methods for\nthe ASVspoof2019 challenge. The proposed model is termed Time-domain Synthetic\nSpeech Detection Net (TSSDNet), having ResNet- or Inception-style structures.\nWe further demonstrate that the proposed models also have attractive\ngeneralization capability. Trained on ASVspoof2019, they could achieve\npromising detection performance when tested on disjoint ASVspoof2015,\nsignificantly better than the existing cross-dataset results. This paper\nreveals the great potential of end-to-end DNNs for synthetic speech detection,\nwithout hand-crafted features.", "published": "2021-06-11 12:25:26", "link": "http://arxiv.org/abs/2106.06341v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Conditional Variational Autoencoder with Adversarial Learning for\n  End-to-End Text-to-Speech", "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage\ntraining and parallel sampling have been proposed, but their sample quality\ndoes not match that of two-stage TTS systems. In this work, we present a\nparallel end-to-end TTS method that generates more natural sounding audio than\ncurrent two-stage models. Our method adopts variational inference augmented\nwith normalizing flows and an adversarial training process, which improves the\nexpressive power of generative modeling. We also propose a stochastic duration\npredictor to synthesize speech with diverse rhythms from input text. With the\nuncertainty modeling over latent variables and the stochastic duration\npredictor, our method expresses the natural one-to-many relationship in which a\ntext input can be spoken in multiple ways with different pitches and rhythms. A\nsubjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a\nsingle speaker dataset, shows that our method outperforms the best publicly\navailable TTS systems and achieves a MOS comparable to ground truth.", "published": "2021-06-11 01:07:12", "link": "http://arxiv.org/abs/2106.06103v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Benchmark of Dynamical Variational Autoencoders applied to Speech\n  Spectrogram Modeling", "abstract": "The Variational Autoencoder (VAE) is a powerful deep generative model that is\nnow extensively used to represent high-dimensional complex data via a\nlow-dimensional latent space learned in an unsupervised manner. In the original\nVAE model, input data vectors are processed independently. In recent years, a\nseries of papers have presented different extensions of the VAE to process\nsequential data, that not only model the latent space, but also model the\ntemporal dependencies within a sequence of data vectors and corresponding\nlatent vectors, relying on recurrent neural networks. We recently performed a\ncomprehensive review of those models and unified them into a general class\ncalled Dynamical Variational Autoencoders (DVAEs). In the present paper, we\npresent the results of an experimental benchmark comparing six of those DVAE\nmodels on the speech analysis-resynthesis task, as an illustration of the high\npotential of DVAEs for speech modeling.", "published": "2021-06-11 16:53:20", "link": "http://arxiv.org/abs/2106.06500v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Large-scale Teacher-Student Training for On-device Acoustic\n  Models", "abstract": "We present results from Alexa speech teams on semi-supervised learning (SSL)\nof acoustic models (AM) with experiments spanning over 3000 hours of GPU time,\nmaking our study one of the largest of its kind. We discuss SSL for AMs in a\nsmall footprint setting, showing that a smaller capacity model trained with 1\nmillion hours of unsupervised data can outperform a baseline supervised system\nby 14.3% word error rate reduction (WERR). When increasing the supervised data\nto seven-fold, our gains diminish to 7.1% WERR; to improve SSL efficiency at\nlarger supervised data regimes, we employ a step-wise distillation into a\nsmaller model, obtaining a WERR of 14.4%. We then switch to SSL using larger\nstudent models in low data regimes; while learning efficiency with unsupervised\ndata is higher, student models may outperform teacher models in such a setting.\nWe develop a theoretical sketch to explain this behavior.", "published": "2021-06-11 02:23:40", "link": "http://arxiv.org/abs/2106.06126v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anomalous Sound Detection Using a Binary Classification Model and Class\n  Centroids", "abstract": "An anomalous sound detection system to detect unknown anomalous sounds\nusually needs to be built using only normal sound data. Moreover, it is\ndesirable to improve the system by effectively using a small amount of\nanomalous sound data, which will be accumulated through the system's operation.\nAs one of the methods to meet these requirements, we focus on a binary\nclassification model that is developed by using not only normal data but also\noutlier data in the other domains as pseudo-anomalous sound data, which can be\neasily updated by using anomalous data. In this paper, we implement a new loss\nfunction based on metric learning to learn the distance relationship from each\nclass centroid in feature space for the binary classification model. The\nproposed multi-task learning of the binary classification and the metric\nlearning makes it possible to build the feature space where the within-class\nvariance is minimized and the between-class variance is maximized while keeping\nnormal and anomalous classes linearly separable. We also investigate the\neffectiveness of additionally using anomalous sound data for further improving\nthe binary classification model. Our results showed that multi-task learning\nusing binary classification and metric learning to consider the distance from\neach class centroid in the feature space is effective, and performance can be\nsignificantly improved by using even a small amount of anomalous data during\ntraining.", "published": "2021-06-11 03:35:06", "link": "http://arxiv.org/abs/2106.06151v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker\n  Verification and Voice Anti-Spoofing", "abstract": "Whether it be for results summarization, or the analysis of classifier\nfusion, some means to compare different classifiers can often provide\nilluminating insight into their behaviour, (dis)similarity or complementarity.\nWe propose a simple method to derive 2D representation from detection scores\nproduced by an arbitrary set of binary classifiers in response to a common\ndataset. Based upon rank correlations, our method facilitates a visual\ncomparison of classifiers with arbitrary scores and with close relation to\nreceiver operating characteristic (ROC) and detection error trade-off (DET)\nanalyses. While the approach is fully versatile and can be applied to any\ndetection task, we demonstrate the method using scores produced by automatic\nspeaker verification and voice anti-spoofing systems. The former are produced\nby a Gaussian mixture model system trained with VoxCeleb data whereas the\nlatter stem from submissions to the ASVspoof 2019 challenge.", "published": "2021-06-11 13:03:33", "link": "http://arxiv.org/abs/2106.06362v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "PriorGrad: Improving Conditional Denoising Diffusion Models with\n  Data-Dependent Adaptive Prior", "abstract": "Denoising diffusion probabilistic models have been recently proposed to\ngenerate high-quality samples by estimating the gradient of the data density.\nThe framework defines the prior noise as a standard Gaussian distribution,\nwhereas the corresponding data distribution may be more complicated than the\nstandard Gaussian distribution, which potentially introduces inefficiency in\ndenoising the prior noise into the data sample because of the discrepancy\nbetween the data and the prior. In this paper, we propose PriorGrad to improve\nthe efficiency of the conditional diffusion model for speech synthesis (for\nexample, a vocoder using a mel-spectrogram as the condition) by applying an\nadaptive prior derived from the data statistics based on the conditional\ninformation. We formulate the training and sampling procedures of PriorGrad and\ndemonstrate the advantages of an adaptive prior through a theoretical analysis.\nFocusing on the speech synthesis domain, we consider the recently proposed\ndiffusion-based speech generative models based on both the spectral and time\ndomains and show that PriorGrad achieves faster convergence and inference with\nsuperior performance, leading to an improved perceptual quality and robustness\nto a smaller network capacity, and thereby demonstrating the efficiency of a\ndata-dependent adaptive prior.", "published": "2021-06-11 14:04:03", "link": "http://arxiv.org/abs/2106.06406v2", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Catch-A-Waveform: Learning to Generate Audio from a Single Short Example", "abstract": "Models for audio generation are typically trained on hours of recordings.\nHere, we illustrate that capturing the essence of an audio source is typically\npossible from as little as a few tens of seconds from a single training signal.\nSpecifically, we present a GAN-based generative model that can be trained on\none short audio signal from any domain (e.g. speech, music, etc.) and does not\nrequire pre-training or any other form of external supervision. Once trained,\nour model can generate random samples of arbitrary duration that maintain\nsemantic similarity to the training waveform, yet exhibit new compositions of\nits audio primitives. This enables a long line of interesting applications,\nincluding generating new jazz improvisations or new a-cappella rap variants\nbased on a single short example, producing coherent modifications to famous\nsongs (e.g. adding a new verse to a Beatles song based solely on the original\nrecording), filling-in of missing parts (inpainting), extending the bandwidth\nof a speech signal (super-resolution), and enhancing old recordings without\naccess to any clean training example. We show that in all cases, no more than\n20 seconds of training audio commonly suffice for our model to achieve\nstate-of-the-art results. This is despite its complete lack of prior knowledge\nabout the nature of audio signals in general.", "published": "2021-06-11 14:35:11", "link": "http://arxiv.org/abs/2106.06426v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
