{"title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages", "abstract": "Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses.", "published": "2024-03-11 00:46:56", "link": "http://arxiv.org/abs/2403.06350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages", "abstract": "Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible\nproficiency at natural language processing tasks and have even begun to excel\nat tasks across other modalities such as vision and audio. Despite their\nsuccess, LLMs often struggle to perform well on low-resource languages because\nthere is so little training data available. This shortcoming is especially\nprevalent with open source models. In this work, we explore training LLaMA-2 to\nspeak Amharic, a language which is spoken by over 50 million people world wide,\nbut has orders of magnitude less data available than languages like English. We\nemploy methods previously used for training LLMs on other languages with data\nscarcity, and use open source translation models to perform data augmentation\nand grow our dataset from millions of tokens to billions. We further enhance\nthe capabilities of our model by connecting an image encoder and training on a\ntranslated visual instruction tuning dataset in the same manner as LLaVA,\nresulting in a multimodal Amharic LLM that can understand images along with\ntext. We introduce an Amharic version of a popular benchmarking dataset to\nevaluate our work. Our models and dataset are open sourced and available on\nGitHub.", "published": "2024-03-11 01:04:36", "link": "http://arxiv.org/abs/2403.06354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlossLM: A Massively Multilingual Corpus and Pretrained Model for\n  Interlinear Glossed Text", "abstract": "Language documentation projects often involve the creation of annotated text\nin a format such as interlinear glossed text (IGT), which captures fine-grained\nmorphosyntactic analyses in a morpheme-by-morpheme format. However, there are\nfew existing resources providing large amounts of standardized, easily\naccessible IGT data, limiting their applicability to linguistic research, and\nmaking it difficult to use such data in NLP modeling.\n  We compile the largest existing corpus of IGT data from a variety of sources,\ncovering over 450k examples across 1.8k languages, to enable research on\ncrosslingual transfer and IGT generation. We normalize much of our data to\nfollow a standard set of labels across languages.\n  Furthermore, we explore the task of automatically generating IGT in order to\naid documentation projects. As many languages lack sufficient monolingual data,\nwe pretrain a large multilingual model on our corpus. We demonstrate the\nutility of this model by finetuning it on monolingual corpora, outperforming\nSOTA models by up to 6.6\\%. Our pretrained model and dataset are available on\nHugging Face.", "published": "2024-03-11 03:21:15", "link": "http://arxiv.org/abs/2403.06399v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean", "abstract": "Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.", "published": "2024-03-11 03:54:33", "link": "http://arxiv.org/abs/2403.06412v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolving Knowledge Distillation with Large Language Models and Active\n  Learning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks. However, their computational costs are prohibitively high.\nTo address this issue, previous research has attempted to distill the knowledge\nof LLMs into smaller models by generating annotated data. Nonetheless, these\nworks have mainly focused on the direct use of LLMs for text generation and\nlabeling, without fully exploring their potential to comprehend the target task\nand acquire valuable knowledge. In this paper, we propose EvoKD: Evolving\nKnowledge Distillation, which leverages the concept of active learning to\ninteractively enhance the process of data generation using large language\nmodels, simultaneously improving the task capabilities of small domain model\n(student model). Different from previous work, we actively analyze the student\nmodel's weaknesses, and then synthesize labeled samples based on the analysis.\nIn addition, we provide iterative feedback to the LLMs regarding the student\nmodel's performance to continuously construct diversified and challenging\nsamples. Experiments and analysis on different NLP tasks, namely, text\nclassification and named entity recognition show the effectiveness of EvoKD.", "published": "2024-03-11 03:55:24", "link": "http://arxiv.org/abs/2403.06414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Consideration of AI Openness: Can Good Intent Be Abused?", "abstract": "Open source is a driving force behind scientific advancement.However, this\nopenness is also a double-edged sword, with the inherent risk that innovative\ntechnologies can be misused for purposes harmful to society. What is the\nlikelihood that an open source AI model or dataset will be used to commit a\nreal-world crime, and if a criminal does exploit it, will the people behind the\ntechnology be able to escape legal liability? To address these questions, we\nexplore a legal domain where individual choices can have a significant impact\non society. Specifically, we build the EVE-V1 dataset that comprises 200\nquestion-answer pairs related to criminal offenses based on 200 Korean\nprecedents first to explore the possibility of malicious models emerging. We\nfurther developed EVE-V2 using 600 fraud-related precedents to confirm the\nexistence of malicious models that can provide harmful advice on a wide range\nof criminal topics to test the domain generalization ability. Remarkably,\nwidely used open-source large-scale language models (LLMs) provide unethical\nand detailed information about criminal activities when fine-tuned with EVE. We\nalso take an in-depth look at the legal issues that malicious language models\nand their builders could realistically face. Our findings highlight the\nparadoxical dilemma that open source accelerates scientific progress, but\nrequires great care to minimize the potential for misuse. Warning: This paper\ncontains content that some may find unethical.", "published": "2024-03-11 09:24:06", "link": "http://arxiv.org/abs/2403.06537v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting\n  Applications", "abstract": "Past studies on end-to-end meeting transcription have focused on model\narchitecture and have mostly been evaluated on simulated meeting data. We\npresent a novel study aiming to optimize the use of a Speaker-Attributed ASR\n(SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for\nimproved speaker assignment of speech segments. First, we propose a pipeline\ntailored to real-life applications involving Voice Activity Detection (VAD),\nSpeaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output\nsegments to fine-tune the SA-ASR model, considering that it is also applied to\nVAD segments during test, and show that this results in a relative reduction of\nSpeaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance\nthe extraction of the speaker embedding templates used as inputs by the SA-ASR\nsystem. We show that extracting them from SD output rather than annotated\nspeaker segments results in a relative SER reduction up to 20%.", "published": "2024-03-11 10:11:29", "link": "http://arxiv.org/abs/2403.06570v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large\n  Language Models", "abstract": "Given the importance of ancient Chinese in capturing the essence of rich\nhistorical and cultural heritage, the rapid advancements in Large Language\nModels (LLMs) necessitate benchmarks that can effectively evaluate their\nunderstanding of ancient contexts. To meet this need, we present AC-EVAL, an\ninnovative benchmark designed to assess the advanced knowledge and reasoning\ncapabilities of LLMs within the context of ancient Chinese. AC-EVAL is\nstructured across three levels of difficulty reflecting different facets of\nlanguage comprehension: general historical knowledge, short text understanding,\nand long text comprehension. The benchmark comprises 13 tasks, spanning\nhistorical facts, geography, social customs, art, philosophy, classical poetry\nand prose, providing a comprehensive assessment framework. Our extensive\nevaluation of top-performing LLMs, tailored for both English and Chinese,\nreveals a substantial potential for enhancing ancient text comprehension. By\nhighlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote\ntheir development and application forward in the realms of ancient Chinese\nlanguage education and scholarly research. The AC-EVAL data and evaluation code\nare available at https://github.com/yuting-wei/AC-EVAL.", "published": "2024-03-11 10:24:37", "link": "http://arxiv.org/abs/2403.06574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model", "abstract": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.", "published": "2024-03-11 14:35:45", "link": "http://arxiv.org/abs/2403.06765v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strength Lies in Differences! Improving Strategy Planning for\n  Non-collaborative Dialogues via Diversified User Simulation", "abstract": "We investigate non-collaborative dialogue agents, which are expected to\nengage in strategic conversations with diverse users, for securing a mutual\nagreement that leans favorably towards the system's objectives. This poses two\nmain challenges for existing dialogue agents: 1) The inability to integrate\nuser-specific characteristics into the strategic planning, and 2) The\ndifficulty of training strategic planners that can be generalized to diverse\nusers. To address these challenges, we propose Trip to enhance the capability\nin tailored strategic planning, incorporating a user-aware strategic planning\nmodule and a population-based training paradigm. Through experiments on\nbenchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of\nTrip in catering to diverse users.", "published": "2024-03-11 14:38:16", "link": "http://arxiv.org/abs/2403.06769v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of a Reliable and Accessible Caregiving Language Model\n  (CaLM)", "abstract": "Unlike professional caregivers, family caregivers often assume this role\nwithout formal preparation or training. Because of this, there is an urgent\nneed to enhance the capacity of family caregivers to provide quality care.\nLarge language models can potentially be used as a foundation technology for\nsupporting caregivers as educational tools or as adjunct to care. This study\naimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a\ncaregiving knowledge base, develop an accessible CaLM using a small FM that\nrequires fewer computing resources, and evaluate the performance of the model\ncompared to a large FM. We developed CaLM using the Retrieval Augmented\nGeneration (RAG) framework combined with FM fine-tuning for improving the\nquality of FM answers by grounding the model on a caregiving knowledge base. We\nused two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B\nparameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving\nknowledge base by gathering various types of documents from the Internet. In\nthis study, we focused on caregivers of individuals with Alzheimer's Disease\nRelated Dementias. We evaluated the models' performance using the benchmark\nmetrics commonly used in evaluating language models and their reliability to\nprovide accurate references with the answers. The RAG framework improved the\nperformance of all FMs used in this study across all measures. As expected, the\nlarge FM performed better than small FMs across all metrics. The most\ninteresting result is that small fine-tuned FMs with RAG performed\nsignificantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2\nsmall FM performed better than GPT 3.5 (even with RAG) in returning references\nwith the answers. The study shows that reliable and accessible CaLM can be\ndeveloped by using small FMs with a knowledge base specific to the caregiving\ndomain.", "published": "2024-03-11 16:12:34", "link": "http://arxiv.org/abs/2403.06857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis", "abstract": "Large language models (LLMs) have achieved commendable accomplishments in\nvarious natural language processing tasks. However, LLMs still encounter\nsignificant challenges when dealing with complex scenarios involving multiple\nentities. These challenges arise from the presence of implicit relationships\nthat demand multi-step reasoning. In this paper, we propose a novel approach\nERA-CoT, which aids LLMs in understanding context by capturing relationships\nbetween entities and supports the reasoning of diverse tasks through\nChain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates\nthe superior performance of our proposed method compared to current CoT\nprompting methods, achieving a significant improvement of an average of 5.1\\%\non GPT3.5 compared to previous SOTA baselines. Our analysis indicates that\nERA-CoT increases the LLM's understanding of entity relationships,\nsignificantly improves the accuracy of question answering, and enhances the\nreasoning ability of LLMs.", "published": "2024-03-11 17:18:53", "link": "http://arxiv.org/abs/2403.06932v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs", "abstract": "While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision & Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, similar patterns of variation are\nobserved among human speakers for highly context-sensitive expressions, such as\nthe quantifiers 'few' or 'most'. In our work, we evaluate VLLMs (FROMAGe,\nBLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where\nhumans show great subjective variability concerning the distribution over\nplausible labels, using datasets and resources mostly under-explored in\nprevious work. Our results reveal mixed evidence on the ability of VLLMs to\ncapture human naming preferences at generation time: while some models are good\nat mimicking human distributions for nouns and attributes, all of them fail to\nassign quantifiers, a task that requires more accurate, high-level reasoning.", "published": "2024-03-11 17:20:12", "link": "http://arxiv.org/abs/2403.06935v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare\n  Linguistic Phenomena", "abstract": "Argument Structure Constructions (ASCs) are one of the most well-studied\nconstruction groups, providing a unique opportunity to demonstrate the\nusefulness of Construction Grammar (CxG). For example, the caused-motion\nconstruction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates\nthat constructions must carry meaning, otherwise the fact that ``sneeze'' in\nthis context causes movement cannot be explained. We form the hypothesis that\nthis remains challenging even for state-of-the-art Large Language Models\n(LLMs), for which we devise a test based on substituting the verb with a\nprototypical motion verb. To be able to perform this test at statistically\nsignificant scale, in the absence of adequate CxG corpora, we develop a novel\npipeline of NLP-assisted collection of linguistically annotated text. We show\nhow dependency parsing and GPT-3.5 can be used to significantly reduce\nannotation cost and thus enable the annotation of rare phenomena at scale. We\nthen evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of\nthe CMC using the newly collected corpus. We find that all models struggle with\nunderstanding the motion component that the CMC adds to a sentence.", "published": "2024-03-11 17:47:47", "link": "http://arxiv.org/abs/2403.06965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MRL Parsing Without Tears: The Case of Hebrew", "abstract": "Syntactic parsing remains a critical tool for relation extraction and\ninformation extraction, especially in resource-scarce languages where LLMs are\nlacking. Yet in morphologically rich languages (MRLs), where parsers need to\nidentify multiple lexical units in each token, existing systems suffer in\nlatency and setup complexity. Some use a pipeline to peel away the layers:\nfirst segmentation, then morphology tagging, and then syntax parsing; however,\nerrors in earlier layers are then propagated forward. Others use a joint\narchitecture to evaluate all permutations at once; while this improves\naccuracy, it is notoriously slow. In contrast, and taking Hebrew as a test\ncase, we present a new \"flipped pipeline\": decisions are made directly on the\nwhole-token units by expert classifiers, each one dedicated to one specific\ntask. The classifiers are independent of one another, and only at the end do we\nsynthesize their predictions. This blazingly fast approach sets a new SOTA in\nHebrew POS tagging and dependency parsing, while also reaching near-SOTA\nperformance on other Hebrew NLP tasks. Because our architecture does not rely\non any language-specific resources, it can serve as a model to develop similar\nparsers for other MRLs.", "published": "2024-03-11 17:54:33", "link": "http://arxiv.org/abs/2403.06970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation with Casual Inference", "abstract": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require substantial memory storage\non low-resource devices. More critically, the computational speed on these\ndevices is also severely limited. In this paper, we propose SPA(Side Plugin\nAdaption), a lightweight architecture for fast on-devices inference on the\nconstraints of strict on-devices computation and memory constraints. Compared\nwith other on-devices seq2seq generation, SPA could make a fast and stable\ninference on low-resource constraints, allowing it to obtain cost effiency. Our\nmethod establish an interaction between a pretrained LLMs on-cloud and additive\nparameters on-devices, which could provide the knowledge on both pretrained\nLLMs and featured personal feature. Further more, SPA provides a framework to\nkeep feature-base parameters on low computational devices while leave the\nparameters containing general information on the high computational devices.", "published": "2024-03-11 18:26:02", "link": "http://arxiv.org/abs/2403.07088v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narrating Causal Graphs with Large Language Models", "abstract": "The use of generative AI to create text descriptions from graphs has mostly\nfocused on knowledge graphs, which connect concepts using facts. In this work\nwe explore the capability of large pretrained language models to generate text\nfrom causal graphs, where salient concepts are represented as nodes and\ncausality is represented via directed, typed edges. The causal reasoning\nencoded in these graphs can support applications as diverse as healthcare or\nmarketing. Using two publicly available causal graph datasets, we empirically\ninvestigate the performance of four GPT-3 models under various settings. Our\nresults indicate that while causal text descriptions improve with training\ndata, compared to fact-based graphs, they are harder to generate under\nzero-shot settings. Results further suggest that users of generative AI can\ndeploy future applications faster since similar performances are obtained when\ntraining a model with only a few examples as compared to fine-tuning via a\nlarge curated dataset.", "published": "2024-03-11 19:19:59", "link": "http://arxiv.org/abs/2403.07118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thought Graph: Generating Thought Process for Biological Reasoning", "abstract": "We present the Thought Graph as a novel framework to support complex\nreasoning and use gene set analysis as an example to uncover semantic\nrelationships between biological processes. Our framework stands out for its\nability to provide a deeper understanding of gene sets, significantly\nsurpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity\nto human annotations. Our analysis further provides insights into future\ndirections of biological processes naming, and implications for bioinformatics\nand precision medicine.", "published": "2024-03-11 20:28:27", "link": "http://arxiv.org/abs/2403.07144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPAWNing Structural Priming Predictions from a Cognitively Motivated\n  Parser", "abstract": "Structural priming is a widely used psycholinguistic paradigm to study human\nsentence representations. In this work we introduce SPAWN, a cognitively\nmotivated parser that can generate quantitative priming predictions from\ncontemporary theories in syntax which assume a lexicalized grammar. By\ngenerating and testing priming predictions from competing theoretical accounts,\nwe can infer which assumptions from syntactic theory are useful for\ncharacterizing the representations humans build when processing sentences. As a\ncase study, we use SPAWN to generate priming predictions from two theories\n(Whiz-Deletion and Participial-Phase) which make different assumptions about\nthe structure of English relative clauses. By modulating the reanalysis\nmechanism that the parser uses and strength of the parser's prior knowledge, we\ngenerated nine sets of predictions from each of the two theories. Then, we\ntested these predictions using a novel web-based comprehension-to-production\npriming paradigm. We found that while the some of the predictions from the\nParticipial-Phase theory aligned with human behavior, none of the predictions\nfrom the the Whiz-Deletion theory did, thus suggesting that the\nParticipial-Phase theory might better characterize human relative clause\nrepresentations.", "published": "2024-03-11 22:58:58", "link": "http://arxiv.org/abs/2403.07202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Semantic Understanding with Contrastive Cross-modal Feature\n  Alignment", "abstract": "Multi-modal semantic understanding requires integrating information from\ndifferent modalities to extract users' real intention behind words. Most\nprevious work applies a dual-encoder structure to separately encode image and\ntext, but fails to learn cross-modal feature alignment, making it hard to\nachieve cross-modal deep information interaction. This paper proposes a novel\nCLIP-guided contrastive-learning-based architecture to perform multi-modal\nfeature alignment, which projects the features derived from different\nmodalities into a unified deep space. On multi-modal sarcasm detection (MMSD)\nand multi-modal sentiment analysis (MMSA) tasks, the experimental results show\nthat our proposed model significantly outperforms several baselines, and our\nfeature alignment strategy brings obvious performance gain over models with\ndifferent aggregating methods and models even enriched with knowledge. More\nimportantly, our model is simple to implement without using task-specific\nexternal knowledge, and thus can easily migrate to other multi-modal tasks. Our\nsource codes are available at https://github.com/ChangKe123/CLFA.", "published": "2024-03-11 01:07:36", "link": "http://arxiv.org/abs/2403.06355v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Human and Automatic Interpretation of Romanian Noun Compounds", "abstract": "Determining the intended, context-dependent meanings of noun compounds like\n\"shoe sale\" and \"fire sale\" remains a challenge for NLP. Previous work has\nrelied on inventories of semantic relations that capture the different meanings\nbetween compound members. Focusing on Romanian compounds, whose morphosyntax\ndiffers from that of their English counterparts, we propose a new set of\nrelations and test it with human annotators and a neural net classifier.\nResults show an alignment of the network's predictions and human judgments,\neven where the human agreement rate is low. Agreement tracks with the frequency\nof the selected relations, regardless of structural differences. However, the\nmost frequently selected relation was none of the sixteen labeled semantic\nrelations, indicating the need for a better relation inventory.", "published": "2024-03-11 01:18:00", "link": "http://arxiv.org/abs/2403.06360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning", "abstract": "In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples from a training set of labelled data to an LLM's prompt with\nan objective to effectively control the generative process seeking to improve\nthe downstream task performance. Existing ICL approaches use an identical\nnumber of examples (a pre-configured hyper-parameter) for each data instance.\nOur work alleviates the limitations of this 'one fits all' approach by\ndynamically predicting the number of examples for each data instance to be used\nin few-shot inference with LLMs. In particular, we employ a multi-label\nclassifier, the parameters of which are fitted using a training set, where the\nlabel for each instance in this training set indicates if using a specific\nvalue of k (number of most similar examples from 0 up to a maximum value) leads\nto correct k-shot downstream predictions. Our experiments on a number of text\nclassification benchmarks show that AICL substantially outperforms standard ICL\nby up to 17%.", "published": "2024-03-11 03:28:13", "link": "http://arxiv.org/abs/2403.06402v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Logical Pattern Memory Pre-trained Model for Entailment Tree\n  Generation", "abstract": "Generating coherent and credible explanations remains a significant challenge\nin the field of AI. In recent years, researchers have delved into the\nutilization of entailment trees to depict explanations, which exhibit a\nreasoning process of how a hypothesis is deduced from the supporting facts.\nHowever, existing models often overlook the importance of generating\nintermediate conclusions with logical consistency from the given facts, leading\nto inaccurate conclusions and undermining the overall credibility of entailment\ntrees. To address this limitation, we propose the logical pattern memory\npre-trained model (LMPM). LMPM incorporates an external memory structure to\nlearn and store the latent representations of logical patterns, which aids in\ngenerating logically consistent conclusions. Furthermore, to mitigate the\ninfluence of logically irrelevant domain knowledge in the Wikipedia-based data,\nwe introduce an entity abstraction approach to construct the dataset for\npre-training LMPM. The experimental results highlight the effectiveness of our\napproach in improving the quality of entailment tree generation. By leveraging\nlogical entailment patterns, our model produces more coherent and reasonable\nconclusions that closely align with the underlying premises. Code and Data are\nreleased at https://github.com/YuanLi95/T5-LMPM", "published": "2024-03-11 03:45:09", "link": "http://arxiv.org/abs/2403.06410v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Real-Time Hallucination Detection based on the Internal\n  States of Large Language Models", "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.", "published": "2024-03-11 05:51:03", "link": "http://arxiv.org/abs/2403.06448v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Understand Named Entities: Using Common Sense for News Captioning", "abstract": "News captioning aims to describe an image with its news article body as\ninput. It greatly relies on a set of detected named entities, including\nreal-world people, organizations, and places. This paper exploits commonsense\nknowledge to understand named entities for news captioning. By ``understand'',\nwe mean correlating the news content with common sense in the wild, which helps\nan agent to 1) distinguish semantically similar named entities and 2) describe\nnamed entities using words outside of training corpora. Our approach consists\nof three modules: (a) Filter Module aims to clarify the common sense concerning\na named entity from two aspects: what does it mean? and what is it related to?,\nwhich divide the common sense into explanatory knowledge and relevant\nknowledge, respectively. (b) Distinguish Module aggregates explanatory\nknowledge from node-degree, dependency, and distinguish three aspects to\ndistinguish semantically similar named entities. (c) Enrich Module attaches\nrelevant knowledge to named entities to enrich the entity description by\ncommonsense information (e.g., identity and social position). Finally, the\nprobability distributions from both modules are integrated to generate the news\ncaptions. Extensive experiments on two challenging datasets (i.e., GoodNews and\nNYTimes) demonstrate the superiority of our method. Ablation studies and\nvisualization further validate its effectiveness in understanding named\nentities.", "published": "2024-03-11 08:52:52", "link": "http://arxiv.org/abs/2403.06520v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unraveling the Mystery of Scaling Laws: Part I", "abstract": "Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.", "published": "2024-03-11 10:05:29", "link": "http://arxiv.org/abs/2403.06563v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Academically intelligent LLMs are not necessarily socially intelligent", "abstract": "The academic intelligence of large language models (LLMs) has made remarkable\nprogress in recent times, but their social intelligence performance remains\nunclear. Inspired by established human social intelligence frameworks,\nparticularly Daniel Goleman's social intelligence theory, we have developed a\nstandardized social intelligence test based on real-world social scenarios to\ncomprehensively assess the social intelligence of LLMs, termed as the\nSituational Evaluation of Social Intelligence (SESI). We conducted an extensive\nevaluation with 13 recent popular and state-of-art LLM agents on SESI. The\nresults indicate the social intelligence of LLMs still has significant room for\nimprovement, with superficially friendliness as a primary reason for errors.\nMoreover, there exists a relatively low correlation between the social\nintelligence and academic intelligence exhibited by LLMs, suggesting that\nsocial intelligence is distinct from academic intelligence for LLMs.\nAdditionally, while it is observed that LLMs can't ``understand'' what social\nintelligence is, their social intelligence, similar to that of humans, is\ninfluenced by social factors.", "published": "2024-03-11 10:35:53", "link": "http://arxiv.org/abs/2403.06591v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Guiding Clinical Reasoning with Large Language Models via Knowledge\n  Seeds", "abstract": "Clinical reasoning refers to the cognitive process that physicians employ in\nevaluating and managing patients. This process typically involves suggesting\nnecessary examinations, diagnosing patients' diseases, and deciding on\nappropriate therapies, etc. Accurate clinical reasoning requires extensive\nmedical knowledge and rich clinical experience, setting a high bar for\nphysicians. This is particularly challenging in developing countries due to the\noverwhelming number of patients and limited physician resources, contributing\nsignificantly to global health inequity and necessitating automated clinical\nreasoning approaches. Recently, the emergence of large language models (LLMs)\nsuch as ChatGPT and GPT-4 have demonstrated their potential in clinical\nreasoning. However, these LLMs are prone to hallucination problems, and the\nreasoning process of LLMs may not align with the clinical decision path of\nphysicians. In this study, we introduce a novel framework, In-Context Padding\n(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer\ncritical clinical reasoning elements (referred to as knowledge seeds) and use\nthese as anchors to guide the generation process of LLMs. Experiments on two\nclinical question datasets demonstrate that ICP significantly improves the\nclinical reasoning ability of LLMs.", "published": "2024-03-11 10:53:20", "link": "http://arxiv.org/abs/2403.06609v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway\n  Encoding", "abstract": "With appropriate data selection and training techniques, Large Language\nModels (LLMs) have demonstrated exceptional success in various medical\nexaminations and multiple-choice questions. However, the application of LLMs in\nmedical dialogue generation-a task more closely aligned with actual medical\npractice-has been less explored. This gap is attributed to the insufficient\nmedical knowledge of LLMs, which leads to inaccuracies and hallucinated\ninformation in the generated medical responses. In this work, we introduce the\nMedical dialogue with Knowledge enhancement and clinical Pathway encoding\n(MedKP) framework, which integrates an external knowledge enhancement module\nthrough a medical knowledge graph and an internal clinical pathway encoding via\nmedical entities and physician actions. Evaluated with comprehensive metrics,\nour experiments on two large-scale, real-world online medical consultation\ndatasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines\nand mitigates the incidence of hallucinations, achieving a new\nstate-of-the-art. Extensive ablation studies further reveal the effectiveness\nof each component of MedKP. This enhancement advances the development of\nreliable, automated medical consultation responses using LLMs, thereby\nbroadening the potential accessibility of precise and real-time medical\nassistance.", "published": "2024-03-11 10:57:45", "link": "http://arxiv.org/abs/2403.06611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Elephants Never Forget: Testing Language Models for Memorization of\n  Tabular Data", "abstract": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Starting with simple qualitative tests for whether an LLM knows\nthe names and values of features, we introduce a variety of different\ntechniques to assess the degrees of contamination, including statistical tests\nfor conditional distribution modeling and four tests that identify\nmemorization. Our investigation reveals that LLMs are pre-trained on many\npopular tabular datasets. This exposure can lead to invalid performance\nevaluation on downstream tasks because the LLMs have, in effect, been fit to\nthe test set. Interestingly, we also identify a regime where the language model\nreproduces important statistics of the data, but fails to reproduce the dataset\nverbatim. On these datasets, although seen during training, good performance on\ndownstream tasks might not be due to overfitting. Our findings underscore the\nneed for ensuring data integrity in machine learning tasks with LLMs. To\nfacilitate future research, we release an open-source tool that can perform\nvarious tests for memorization\n\\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.", "published": "2024-03-11 12:07:13", "link": "http://arxiv.org/abs/2403.06644v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine\n  Translation", "abstract": "Large language model (LLM) has achieved promising performance in multilingual\nmachine translation tasks through zero/few-shot prompts or prompt-tuning.\nHowever, due to the mixture of multilingual data during the pre-training of\nLLM, the LLM-based translation models face the off-target issue in both\nprompt-based methods, including a series of phenomena, namely instruction\nmisunderstanding, translation with wrong language and over-generation. For this\nissue, this paper introduces an\n\\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction\n\\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual\n\\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine\n\\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised\nfine-tuning mechanism and orthogonal to the traditional prompt-based methods.\nIn this method, \\model automatically constructs a constrained template in the\ntarget side by adding trigger tokens ahead of the ground truth. Furthermore,\ntrigger tokens can be arranged and combined freely to represent different task\nsemantics, and they can be iteratively updated to maximize the label\nlikelihood. Experiments are performed on WMT test sets with multiple metrics,\nand the experimental results demonstrate that \\model achieves substantially\nimproved performance across multiple translation directions and reduce the\noff-target phenomena in the translation.", "published": "2024-03-11 14:10:57", "link": "http://arxiv.org/abs/2403.06745v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPLADE-v3: New baselines for SPLADE", "abstract": "A companion to the release of the latest version of the SPLADE library. We\ndescribe changes to the training structure and present our latest series of\nmodels -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as\nre-rankers, and showcase its effectiveness via a meta-analysis over more than\n40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is\nstatistically significantly more effective than both BM25 and SPLADE++, while\ncomparing well to cross-encoder re-rankers. Specifically, it gets more than 40\nMRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on\nthe BEIR benchmark.", "published": "2024-03-11 15:04:55", "link": "http://arxiv.org/abs/2403.06789v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Noise-powered Multi-modal Knowledge Graph Representation Framework", "abstract": "The rise of Multi-modal Pre-training highlights the necessity for a unified\nMulti-Modal Knowledge Graph (MMKG) representation learning framework. Such a\nframework is essential for embedding structured knowledge into multi-modal\nLarge Language Models effectively, alleviating issues like knowledge\nmisconceptions and multi-modal hallucinations. In this work, we explore the\nefficacy of models in accurately embedding entities within MMKGs through two\npivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal\nEntity Alignment (MMEA). Building on this foundation, we propose a novel SNAG\nmethod that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking to robustly integrate multi-modal entity features\nin KGs. By incorporating specific training objectives for both MKGC and MMEA,\nour approach achieves SOTA performance across a total of ten datasets,\ndemonstrating its versatility. Moreover, SNAG can not only function as a\nstandalone model but also enhance other existing methods, providing stable\nperformance improvements. Code and data are available at\nhttps://github.com/zjukg/SNAG.", "published": "2024-03-11 15:48:43", "link": "http://arxiv.org/abs/2403.06832v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?", "abstract": "Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.", "published": "2024-03-11 15:48:56", "link": "http://arxiv.org/abs/2403.06833v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RA-ISF: Learning to Answer and Understand from Retrieval Augmentation\n  via Iterative Self-Feedback", "abstract": "Large language models (LLMs) demonstrate exceptional performance in numerous\ntasks but still heavily rely on knowledge stored in their parameters. Moreover,\nupdating this knowledge incurs high training costs. Retrieval-augmented\ngeneration (RAG) methods address this issue by integrating external knowledge.\nThe model can answer questions it couldn't previously by retrieving knowledge\nrelevant to the query. This approach improves performance in certain scenarios\nfor specific tasks. However, if irrelevant texts are retrieved, it may impair\nmodel performance. In this paper, we propose Retrieval Augmented Iterative\nSelf-Feedback (RA-ISF), a framework that iteratively decomposes tasks and\nprocesses them in three submodules to enhance the model's problem-solving\ncapabilities. Experiments show that our method outperforms existing benchmarks,\nperforming well on models like GPT3.5, Llama2, significantly enhancing factual\nreasoning capabilities and reducing hallucinations.", "published": "2024-03-11 16:01:05", "link": "http://arxiv.org/abs/2403.06840v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models and Hierarchical Frameworks for\n  Classification of Large Unstructured Legal Documents", "abstract": "Legal judgment prediction suffers from the problem of long case documents\nexceeding tens of thousands of words, in general, and having a non-uniform\nstructure. Predicting judgments from such documents becomes a challenging task,\nmore so on documents with no structural annotation. We explore the\nclassification of these large legal documents and their lack of structural\ninformation with a deep-learning-based hierarchical framework which we call\nMESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. Specifically, we divide a document into parts to extract their\nembeddings from the last four layers of a custom fine-tuned Large Language\nModel, and try to approximate their structure through unsupervised clustering.\nWhich we use in another set of transformer encoder layers to learn the\ninter-chunk representations. We analyze the adaptability of Large Language\nModels (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the\nhierarchical framework of MESc and compare them with their standalone\nperformance on legal texts. We also study their intra-domain(legal) transfer\nlearning capability and the impact of combining embeddings from their last\nlayers in MESc. We test these methods and their effectiveness with extensive\nexperiments and ablation studies on legal documents from India, the European\nUnion, and the United States with the ILDC dataset and a subset of the LexGLUE\ndataset. Our approach achieves a minimum total performance gain of\napproximately 2 points over previous state-of-the-art methods.", "published": "2024-03-11 16:24:08", "link": "http://arxiv.org/abs/2403.06872v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head", "abstract": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}", "published": "2024-03-11 16:48:25", "link": "http://arxiv.org/abs/2403.06892v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective\n  In-Context Learning", "abstract": "Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models", "published": "2024-03-11 17:03:04", "link": "http://arxiv.org/abs/2403.06914v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Materials science in the era of large language models: a perspective", "abstract": "Large Language Models (LLMs) have garnered considerable interest due to their\nimpressive natural language capabilities, which in conjunction with various\nemergent properties make them versatile tools in workflows ranging from complex\ncode generation to heuristic finding for combinatorial problems. In this paper\nwe offer a perspective on their applicability to materials science research,\narguing their ability to handle ambiguous requirements across a range of tasks\nand disciplines mean they could be a powerful tool to aid researchers. We\nqualitatively examine basic LLM theory, connecting it to relevant properties\nand techniques in the literature before providing two case studies that\ndemonstrate their use in task automation and knowledge extraction at-scale. At\ntheir current stage of development, we argue LLMs should be viewed less as\noracles of novel insight, and more as tireless workers that can accelerate and\nunify exploration across domains. It is our hope that this paper can\nfamiliarise material science researchers with the concepts needed to leverage\nthese tools in their own research.", "published": "2024-03-11 17:34:25", "link": "http://arxiv.org/abs/2403.06949v1", "categories": ["cond-mat.mtrl-sci", "cs.CL"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "LSTM-Based Text Generation: A Study on Historical Datasets", "abstract": "This paper presents an exploration of Long Short-Term Memory (LSTM) networks\nin the realm of text generation, focusing on the utilization of historical\ndatasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in\nhandling sequential data, are applied here to model complex language patterns\nand structures inherent in historical texts. The study demonstrates that\nLSTM-based models, when trained on historical datasets, can not only generate\ntext that is linguistically rich and contextually relevant but also provide\ninsights into the evolution of language patterns over time. The finding\npresents models that are highly accurate and efficient in predicting text from\nworks of Nietzsche, with low loss values and a training time of 100 iterations.\nThe accuracy of the model is 0.9521, indicating high accuracy. The loss of the\nmodel is 0.2518, indicating its effectiveness. The accuracy of the model in\npredicting text from the work of Shakespeare is 0.9125, indicating a low error\nrate. The training time of the model is 100, mirroring the efficiency of the\nNietzsche dataset. This efficiency demonstrates the effectiveness of the model\ndesign and training methodology, especially when handling complex literary\ntexts. This research contributes to the field of natural language processing by\nshowcasing the versatility of LSTM networks in text generation and offering a\npathway for future explorations in historical linguistics and beyond.", "published": "2024-03-11 18:25:01", "link": "http://arxiv.org/abs/2403.07087v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model\n  Editing", "abstract": "Recent work using Rank-One Model Editing (ROME), a popular model editing\nmethod, has shown that there are certain facts that the algorithm is unable to\nedit without breaking the model. Such edits have previously been called\ndisabling edits. These disabling edits cause immediate model collapse and\nlimits the use of ROME for sequential editing. In this paper, we show that\ndisabling edits are an artifact of irregularities in the implementation of\nROME. With this paper, we provide a more stable implementation ROME, which we\ncall r-ROME and show that model collapse is no longer observed when making\nlarge scale sequential edits with r-ROME, while further improving\ngeneralization and locality of model editing compared to the original\nimplementation of ROME. We also provide a detailed mathematical explanation of\nthe reason behind disabling edits.", "published": "2024-03-11 21:33:05", "link": "http://arxiv.org/abs/2403.07175v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CuentosIE: can a chatbot about \"tales with a message\" help to teach\n  emotional intelligence?", "abstract": "In this article, we present CuentosIE (TalesEI: chatbot of tales with a\nmessage to develop Emotional Intelligence), an educational chatbot on emotions\nthat also provides teachers and psychologists with a tool to monitor their\nstudents/patients through indicators and data compiled by CuentosIE. The use of\n\"tales with a message\" is justified by their simplicity and easy understanding,\nthanks to their moral or associated metaphors. The main contributions of\nCuentosIE are the selection, collection, and classification of a set of highly\nspecialized tales, as well as the provision of tools (searching, reading\ncomprehension, chatting, recommending, and classifying) that are useful for\nboth educating users about emotions and monitoring their emotional development.\nThe preliminary evaluation of the tool has obtained encouraging results, which\nprovides an affirmative answer to the question posed in the title of the\narticle.", "published": "2024-03-11 22:27:16", "link": "http://arxiv.org/abs/2403.07193v1", "categories": ["cs.CL", "cs.AI", "I.2.0"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Injected Curriculum Pretraining Framework for Question\n  Answering", "abstract": "Knowledge-based question answering (KBQA) is a key task in NLP research, and\nalso an approach to access the web data and knowledge, which requires\nexploiting knowledge graphs (KGs) for reasoning. In the literature, one\npromising solution for KBQA is to incorporate the pretrained language model\n(LM) with KGs by generating KG-centered pretraining corpus, which has shown its\nsuperiority. However, these methods often depend on specific techniques and\nresources to work, which may not always be available and restrict its\napplication. Moreover, existing methods focus more on improving language\nunderstanding with KGs, while neglect the more important human-like complex\nreasoning. To this end, in this paper, we propose a general Knowledge-Injected\nCurriculum Pretraining framework (KICP) to achieve comprehensive KG learning\nand exploitation for KBQA tasks, which is composed of knowledge injection (KI),\nknowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI\nmodule first injects knowledge into the LM by generating KG-centered\npretraining corpus, and generalizes the process into three key steps that could\nwork with different implementations for flexible application. Next, the KA\nmodule learns knowledge from the generated corpus with LM equipped with an\nadapter as well as keeps its original natural language understanding ability to\nreduce the negative impacts of the difference between the generated and natural\ncorpus. Last, to enable the LM with complex reasoning, the CR module follows\nhuman reasoning patterns to construct three corpora with increasing\ndifficulties of reasoning, and further trains the LM from easy to hard in a\ncurriculum manner. We provide an implementation of the general framework, and\nevaluate the proposed KICP on four real-word datasets. The results demonstrate\nthat our framework can achieve higher performances.", "published": "2024-03-11 03:42:03", "link": "http://arxiv.org/abs/2403.09712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linguistic Structure Induction from Language Models", "abstract": "Linear sequences of words are implicitly represented in our brains by\nhierarchical structures that organize the composition of words in sentences.\nLinguists formalize different frameworks to model this hierarchy; two of the\nmost common syntactic frameworks are Constituency and Dependency. Constituency\nrepresents sentences as nested groups of phrases, while dependency represents a\nsentence by assigning relations between its words. Recently, the pursuit of\nintelligent machines has produced Language Models (LMs) capable of solving many\nlanguage tasks with a human-level performance. Many studies now question\nwhether LMs implicitly represent syntactic hierarchies. This thesis focuses on\nproducing constituency and dependency structures from LMs in an unsupervised\nsetting. I review the critical methods in this field and highlight a line of\nwork that utilizes a numerical representation for binary constituency trees\n(Syntactic Distance). I present a detailed study on StructFormer (SF) (Shen et\nal., 2021), which retrofits a transformer encoder architecture with a parser\nnetwork to produce constituency and dependency structures. I present six\nexperiments to analyze and address this field's challenges; experiments include\ninvestigating the effect of repositioning the parser network within the SF\narchitecture, evaluating subword-based induced trees, and benchmarking the\nmodels developed in the thesis experiments on linguistic tasks. Models\nbenchmarking is performed by participating in the BabyLM challenge, published\nat CoNLL 2023 (Momen et al., 2023). The results of this thesis encourage\nfurther development in the direction of retrofitting transformer-based models\nto induce syntactic structures, supported by the acceptable performance of SF\nin different experimental settings and the observed limitations that require\ninnovative solutions to advance the state of syntactic structure induction.", "published": "2024-03-11 16:54:49", "link": "http://arxiv.org/abs/2403.09714v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid\n  Approach", "abstract": "Due to the scale and complexity of cloud systems, a system failure would\ntrigger an \"alert storm\", i.e., massive correlated alerts. Although these\nalerts can be traced back to a few root causes, the overwhelming number makes\nit infeasible for manual handling. Alert aggregation is thus critical to help\nengineers concentrate on the root cause and facilitate failure resolution.\nExisting methods typically utilize semantic similarity-based methods or\nstatistical methods to aggregate alerts. However, semantic similarity-based\nmethods overlook the causal rationale of alerts, while statistical methods can\nhardly handle infrequent alerts.\n  To tackle these limitations, we introduce leveraging external knowledge,\ni.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose\nCOLA, a novel hybrid approach based on correlation mining and LLM (Large\nLanguage Model) reasoning for online alert aggregation. The correlation mining\nmodule effectively captures the temporal and spatial relations between alerts,\nmeasuring their correlations in an efficient manner. Subsequently, only\nuncertain pairs with low confidence are forwarded to the LLM reasoning module\nfor detailed analysis. This hybrid design harnesses both statistical evidence\nfor frequent alerts and the reasoning capabilities of computationally intensive\nLLMs, ensuring the overall efficiency of COLA in handling large volumes of\nalerts in practical scenarios. We evaluate COLA on three datasets collected\nfrom the production environment of a large-scale cloud platform. The\nexperimental results show COLA achieves F1-scores from 0.901 to 0.930,\noutperforming state-of-the-art methods and achieving comparable efficiency. We\nalso share our experience in deploying COLA in our real-world cloud system,\nCloud X.", "published": "2024-03-11 07:48:35", "link": "http://arxiv.org/abs/2403.06485v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Multilingual Turn-taking Prediction Using Voice Activity Projection", "abstract": "This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).", "published": "2024-03-11 07:50:29", "link": "http://arxiv.org/abs/2403.06487v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Python Programs Using Context-Free Grammars", "abstract": "In recent years, data has emerged as the new gold, serving as a powerful tool\nfor creating intelligent systems. However, procuring high-quality data remains\nchallenging, especially for code. To address this, we developed TinyPy\nGenerator, a tool that generates random Python programs using a context-free\ngrammar. The generated programs are guaranteed to be correct by construction.\nOur system uses custom production rules (in the Backus-Naur Form (BNF) format)\nto recursively generate code. This allows us to generate code with different\nlevels of complexity, ranging from code containing only assignments to more\ncomplex code containing conditionals and loops. Our proposed tool enables\neffortless large-scale Python code generation, beneficial for a wide range of\napplications. TinyPy Generator is particularly useful in the field of machine\nlearning, where it can generate substantial amounts of Python code for training\nPython language models. Additionally, researchers who are studying programming\nlanguages can utilize this tool to create datasets for their experiments, which\ncan help validate the robustness of code interpreters or compilers. Unlike\nexisting research, we have open-sourced our implementation. This allows\ncustomization according to user needs and extends potential usage to other\nlanguages.", "published": "2024-03-11 08:25:52", "link": "http://arxiv.org/abs/2403.06503v1", "categories": ["cs.PL", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity\n  Recognition Models", "abstract": "Context-aware Human Activity Recognition (HAR) is a hot research area in\nmobile computing, and the most effective solutions in the literature are based\non supervised deep learning models. However, the actual deployment of these\nsystems is limited by the scarcity of labeled data that is required for\ntraining. Neuro-Symbolic AI (NeSy) provides an interesting research direction\nto mitigate this issue, by infusing common-sense knowledge about human\nactivities and the contexts in which they can be performed into HAR deep\nlearning classifiers. Existing NeSy methods for context-aware HAR rely on\nknowledge encoded in logic-based models (e.g., ontologies) whose design,\nimplementation, and maintenance to capture new activities and contexts require\nsignificant human engineering efforts, technical knowledge, and domain\nexpertise. Recent works show that pre-trained Large Language Models (LLMs)\neffectively encode common-sense knowledge about human activities. In this work,\nwe propose ContextGPT: a novel prompt engineering approach to retrieve from\nLLMs common-sense knowledge about the relationship between human activities and\nthe context in which they are performed. Unlike ontologies, ContextGPT requires\nlimited human effort and expertise. An extensive evaluation carried out on two\npublic datasets shows how a NeSy model obtained by infusing common-sense\nknowledge from ContextGPT is effective in data scarcity scenarios, leading to\nsimilar (and sometimes better) recognition rates than logic-based approaches\nwith a fraction of the effort.", "published": "2024-03-11 10:32:23", "link": "http://arxiv.org/abs/2403.06586v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TRAWL: External Knowledge-Enhanced Recommendation with LLM Assistance", "abstract": "Combining semantic information with behavioral data is a crucial research\narea in recommender systems. A promising approach involves leveraging external\nknowledge to enrich behavioral-based recommender systems with abundant semantic\ninformation. However, this approach faces two primary challenges: denoising raw\nexternal knowledge and adapting semantic representations. To address these\nchallenges, we propose an External Knowledge-Enhanced Recommendation method\nwith LLM Assistance (TRAWL). This method utilizes large language models (LLMs)\nto extract relevant recommendation knowledge from raw external data and employs\na contrastive learning strategy for adapter training. Experiments on public\ndatasets and real-world online recommender systems validate the effectiveness\nof our approach.", "published": "2024-03-11 12:04:20", "link": "http://arxiv.org/abs/2403.06642v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Restoring Ancient Ideograph: A Multimodal Multitask Neural Network\n  Approach", "abstract": "Cultural heritage serves as the enduring record of human thought and history.\nDespite significant efforts dedicated to the preservation of cultural relics,\nmany ancient artefacts have been ravaged irreversibly by natural deterioration\nand human actions. Deep learning technology has emerged as a valuable tool for\nrestoring various kinds of cultural heritages, including ancient text\nrestoration. Previous research has approached ancient text restoration from\neither visual or textual perspectives, often overlooking the potential of\nsynergizing multimodal information. This paper proposes a novel Multimodal\nMultitask Restoring Model (MMRM) to restore ancient texts, particularly\nemphasising the ideograph. This model combines context understanding with\nresidual visual information from damaged ancient artefacts, enabling it to\npredict damaged characters and generate restored images simultaneously. We\ntested the MMRM model through experiments conducted on both simulated datasets\nand authentic ancient inscriptions. The results show that the proposed method\ngives insightful restoration suggestions in both simulation experiments and\nreal-world scenarios. To the best of our knowledge, this work represents the\npioneering application of multimodal deep learning in ancient text restoration,\nwhich will contribute to the understanding of ancient society and culture in\ndigital humanities fields.", "published": "2024-03-11 12:57:28", "link": "http://arxiv.org/abs/2403.06682v1", "categories": ["cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning", "abstract": "Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://github.com/rattlesnakey/LoReKT.", "published": "2024-03-11 13:44:43", "link": "http://arxiv.org/abs/2403.06725v4", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Real-Time Multimodal Cognitive Assistant for Emergency Medical Services", "abstract": "Emergency Medical Services (EMS) responders often operate under\ntime-sensitive conditions, facing cognitive overload and inherent risks,\nrequiring essential skills in critical thinking and rapid decision-making. This\npaper presents CognitiveEMS, an end-to-end wearable cognitive assistant system\nthat can act as a collaborative virtual partner engaging in the real-time\nacquisition and analysis of multimodal data from an emergency scene and\ninteracting with EMS responders through Augmented Reality (AR) smart glasses.\nCognitiveEMS processes the continuous streams of data in real-time and\nleverages edge computing to provide assistance in EMS protocol selection and\nintervention recognition. We address key technical challenges in real-time\ncognitive assistance by introducing three novel components: (i) a Speech\nRecognition model that is fine-tuned for real-world medical emergency\nconversations using simulated EMS audio recordings, augmented with synthetic\ndata generated by large language models (LLMs); (ii) an EMS Protocol Prediction\nmodel that combines state-of-the-art (SOTA) tiny language models with EMS\ndomain knowledge using graph-based attention mechanisms; (iii) an EMS Action\nRecognition module which leverages multimodal audio and video data and protocol\npredictions to infer the intervention/treatment actions taken by the responders\nat the incident scene. Our results show that for speech recognition we achieve\nsuperior performance compared to SOTA (WER of 0.290 vs. 0.618) on\nconversational data. Our protocol prediction component also significantly\noutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition\nachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s\nfor protocol prediction on the edge and 0.31s on the server.", "published": "2024-03-11 13:56:57", "link": "http://arxiv.org/abs/2403.06734v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling", "abstract": "We introduce ALaRM, the first framework modeling hierarchical rewards in\nreinforcement learning from human feedback (RLHF), which is designed to enhance\nthe alignment of large language models (LLMs) with human preferences. The\nframework addresses the limitations of current alignment approaches, which\noften struggle with the inconsistency and sparsity of human supervision\nsignals, by integrating holistic rewards with aspect-specific rewards. This\nintegration enables more precise and consistent guidance of language models\ntowards desired outcomes, particularly in complex and open text generation\ntasks. By employing a methodology that filters and combines multiple rewards\nbased on their consistency, the framework provides a reliable mechanism for\nimproving model alignment. We validate our approach through applications in\nlong-form question answering and machine translation tasks, employing\ngpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over\nexisting baselines. Our work underscores the effectiveness of hierarchical\nrewards modeling in refining LLM training processes for better human preference\nalignment. We release our code at https://ALaRM-fdu.github.io.", "published": "2024-03-11 14:28:40", "link": "http://arxiv.org/abs/2403.06754v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models", "abstract": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.", "published": "2024-03-11 14:35:32", "link": "http://arxiv.org/abs/2403.06764v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Medical Image Synthesis via Fine-Grained Image-Text Alignment and\n  Anatomy-Pathology Prompting", "abstract": "Data scarcity and privacy concerns limit the availability of high-quality\nmedical images for public use, which can be mitigated through medical image\nsynthesis. However, current medical image synthesis methods often struggle to\naccurately capture the complexity of detailed anatomical structures and\npathological conditions. To address these challenges, we propose a novel\nmedical image synthesis model that leverages fine-grained image-text alignment\nand anatomy-pathology prompts to generate highly detailed and accurate\nsynthetic medical images. Our method integrates advanced natural language\nprocessing techniques with image generative modeling, enabling precise\nalignment between descriptive text prompts and the synthesized images'\nanatomical and pathological details. The proposed approach consists of two key\ncomponents: an anatomy-pathology prompting module and a fine-grained\nalignment-based synthesis module. The anatomy-pathology prompting module\nautomatically generates descriptive prompts for high-quality medical images. To\nfurther synthesize high-quality medical images from the generated prompts, the\nfine-grained alignment-based synthesis module pre-defines a visual codebook for\nthe radiology dataset and performs fine-grained alignment between the codebook\nand generated prompts to obtain key patches as visual clues, facilitating\naccurate image synthesis. We validate the superiority of our method through\nexperiments on public chest X-ray datasets and demonstrate that our synthetic\nimages preserve accurate semantic information, making them valuable for various\nmedical applications.", "published": "2024-03-11 15:56:17", "link": "http://arxiv.org/abs/2403.06835v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Impact of Noisy Supervision in Foundation Model Learning", "abstract": "Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.", "published": "2024-03-11 16:22:41", "link": "http://arxiv.org/abs/2403.06869v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications", "abstract": "Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.", "published": "2024-03-11 17:12:09", "link": "http://arxiv.org/abs/2403.06925v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Counterfactual Reasoning with Knowledge Graph Embeddings", "abstract": "Knowledge graph embeddings (KGEs) were originally developed to infer true but\nmissing facts in incomplete knowledge repositories. In this paper, we link\nknowledge graph completion and counterfactual reasoning via our new task CFKGR.\nWe model the original world state as a knowledge graph, hypothetical scenarios\nas edges added to the graph, and plausible changes to the graph as inferences\nfrom logical rules. We create corresponding benchmark datasets, which contain\ndiverse hypothetical scenarios with plausible changes to the original knowledge\ngraph and facts that should be retained. We develop COULDD, a general method\nfor adapting existing knowledge graph embeddings given a hypothetical premise,\nand evaluate it on our benchmark. Our results indicate that KGEs learn patterns\nin the graph without explicit training. We further observe that KGEs adapted\nwith COULDD solidly detect plausible counterfactual changes to the graph that\nfollow these patterns. An evaluation on human-annotated data reveals that KGEs\nadapted with COULDD are mostly unable to recognize changes to the graph that do\nnot follow learned inference rules. In contrast, ChatGPT mostly outperforms\nKGEs in detecting plausible changes to the graph but has poor knowledge\nretention. In summary, CFKGR connects two previously distinct areas, namely KG\ncompletion and counterfactual reasoning.", "published": "2024-03-11 17:21:39", "link": "http://arxiv.org/abs/2403.06936v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data", "abstract": "Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.", "published": "2024-03-11 17:35:33", "link": "http://arxiv.org/abs/2403.06952v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The pitfalls of next-token prediction", "abstract": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this emerging concern and correct popular misconceptions\nsurrounding it, and advocate a simple multi-token objective.\n  As a starting point, we argue that the two often-conflated phases of\nnext-token prediction -- autoregressive inference and teacher-forced training\n-- must be treated distinctly. The popular criticism that errors can compound\nduring autoregressive inference, crucially assumes that teacher-forcing has\nlearned an accurate next-token predictor. This assumption sidesteps a more\ndeep-rooted problem we expose: in certain classes of tasks, teacher-forcing can\nsimply fail to learn an accurate next-token predictor in the first place. We\ndescribe a general mechanism of how teacher-forcing can fail, and design a\nminimal planning task where both the Transformer and the Mamba architecture\nempirically fail in that manner -- remarkably, despite the task being\nstraightforward to learn.\n  Finally, we provide preliminary evidence that this failure can be resolved\nusing a simple modification that predicts multiple tokens in advance. We hope\nthis finding can ground future debates and inspire explorations beyond the\nnext-token prediction paradigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures", "published": "2024-03-11 17:47:30", "link": "http://arxiv.org/abs/2403.06963v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From English to ASIC: Hardware Implementation with Large Language Model", "abstract": "In the realm of ASIC engineering, the landscape has been significantly\nreshaped by the rapid development of LLM, paralleled by an increase in the\ncomplexity of modern digital circuits. This complexity has escalated the\nrequirements for HDL coding, necessitating a higher degree of precision and\nsophistication. However, challenges have been faced due to the\nless-than-optimal performance of modern language models in generating hardware\ndescription code, a situation further exacerbated by the scarcity of the\ncorresponding high-quality code datasets. These challenges have highlighted the\ngap between the potential of LLMs to revolutionize digital circuit design and\ntheir current capabilities in accurately interpreting and implementing hardware\nspecifications. To address these challenges, a strategy focusing on the\nfine-tuning of the leading-edge nature language model and the reshuffling of\nthe HDL code dataset has been developed. The fine-tuning aims to enhance\nmodels' proficiency in generating precise and efficient ASIC design, while the\ndataset reshuffling is intended to broaden the scope and improve the quality of\ntraining material. The model demonstrated significant improvements compared to\nthe base model, with approximately 10% to 20% increase in accuracy across a\nwide range of temperature for the pass@1 metric. This approach is expected to\nfacilitate a simplified and more efficient LLM-assisted framework for complex\ncircuit design, leveraging their capabilities to meet the sophisticated demands\nof HDL coding and thus streamlining the ASIC development process.", "published": "2024-03-11 09:57:16", "link": "http://arxiv.org/abs/2403.07039v1", "categories": ["cs.AR", "cs.AI", "cs.CL"], "primary_category": "cs.AR"}
{"title": "One Category One Prompt: Dataset Distillation using Diffusion Models", "abstract": "The extensive amounts of data required for training deep neural networks pose\nsignificant challenges on storage and transmission fronts. Dataset distillation\nhas emerged as a promising technique to condense the information of massive\ndatasets into a much smaller yet representative set of synthetic samples.\nHowever, traditional dataset distillation approaches often struggle to scale\neffectively with high-resolution images and more complex architectures due to\nthe limitations in bi-level optimization. Recently, several works have proposed\nexploiting knowledge distillation with decoupled optimization schemes to scale\nup dataset distillation. Although these methods effectively address the\nscalability issue, they rely on extensive image augmentations requiring the\nstorage of soft labels for augmented images. In this paper, we introduce\nDataset Distillation using Diffusion Models (D3M) as a novel paradigm for\ndataset distillation, leveraging recent advancements in generative\ntext-to-image foundation models. Our approach utilizes textual inversion, a\ntechnique for fine-tuning text-to-image generative models, to create concise\nand informative representations for large datasets. By employing these learned\ntext prompts, we can efficiently store and infer new samples for introducing\ndata variability within a fixed memory budget. We show the effectiveness of our\nmethod through extensive experiments across various computer vision benchmark\ndatasets with different memory budgets.", "published": "2024-03-11 20:23:59", "link": "http://arxiv.org/abs/2403.07142v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "3M-Diffusion: Latent Multi-Modal Diffusion for Language-Guided Molecular\n  Structure Generation", "abstract": "Generating molecular structures with desired properties is a critical task\nwith broad applications in drug discovery and materials design. We propose\n3M-Diffusion, a novel multi-modal molecular graph generation method, to\ngenerate diverse, ideally novel molecular structures with desired properties.\n3M-Diffusion encodes molecular graphs into a graph latent space which it then\naligns with the text space learned by encoder-based LLMs from textual\ndescriptions. It then reconstructs the molecular structure and atomic\nattributes based on the given text descriptions using the molecule decoder. It\nthen learns a probabilistic mapping from the text space to the latent molecular\ngraph space using a diffusion model. The results of our extensive experiments\non several datasets demonstrate that 3M-Diffusion can generate high-quality,\nnovel and diverse molecular graphs that semantically match the textual\ndescription provided.", "published": "2024-03-11 21:44:54", "link": "http://arxiv.org/abs/2403.07179v2", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of\n  ChatGPT on AI Conference Peer Reviews", "abstract": "We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.", "published": "2024-03-11 21:51:39", "link": "http://arxiv.org/abs/2403.07183v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "$\\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking\n  Reinforcement Learning Algorithms in Generative Language Model", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the\nperformance of language models at scale. Yet, there is a noticeable absence of\na cost-effective and standardized testbed tailored to evaluating and comparing\nthese algorithms. To bridge this gap, we present a generalized version of the\n24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a\ntarget value $K$ with $N$ integers. We evaluate the effectiveness of\nestablished RL algorithms such as Proximal Policy Optimization (PPO), alongside\nnovel approaches like Identity Policy Optimization (IPO) and Direct Policy\nOptimization (DPO).", "published": "2024-03-11 22:24:14", "link": "http://arxiv.org/abs/2403.07191v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The evaluation of a code-switched Sepedi-English automatic speech\n  recognition system", "abstract": "Speech technology is a field that encompasses various techniques and tools\nused to enable machines to interact with speech, such as automatic speech\nrecognition (ASR), spoken dialog systems, and others, allowing a device to\ncapture spoken words through a microphone from a human speaker. End-to-end\napproaches such as Connectionist Temporal Classification (CTC) and\nattention-based methods are the most used for the development of ASR systems.\nHowever, these techniques were commonly used for research and development for\nmany high-resourced languages with large amounts of speech data for training\nand evaluation, leaving low-resource languages relatively underdeveloped. While\nthe CTC method has been successfully used for other languages, its\neffectiveness for the Sepedi language remains uncertain. In this study, we\npresent the evaluation of the Sepedi-English code-switched automatic speech\nrecognition system. This end-to-end system was developed using the Sepedi\nPrompted Code Switching corpus and the CTC approach. The performance of the\nsystem was evaluated using both the NCHLT Sepedi test corpus and the Sepedi\nPrompted Code Switching corpus. The model produced the lowest WER of 41.9%,\nhowever, the model faced challenges in recognizing the Sepedi only text.", "published": "2024-03-11 15:11:28", "link": "http://arxiv.org/abs/2403.07947v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Hybrid Intelligence Method for Argument Mining", "abstract": "Large-scale survey tools enable the collection of citizen feedback in opinion\ncorpora. Extracting the key arguments from a large and noisy set of opinions\nhelps in understanding the opinions quickly and accurately. Fully automated\nmethods can extract arguments but (1) require large labeled datasets that\ninduce large annotation costs and (2) work well for known viewpoints, but not\nfor novel points of view. We propose HyEnA, a hybrid (human + AI) method for\nextracting arguments from opinionated texts, combining the speed of automated\nprocessing with the understanding and reasoning capabilities of humans. We\nevaluate HyEnA on three citizen feedback corpora. We find that, on the one\nhand, HyEnA achieves higher coverage and precision than a state-of-the-art\nautomated method when compared to a common set of diverse opinions, justifying\nthe need for human insight. On the other hand, HyEnA requires less human effort\nand does not compromise quality compared to (fully manual) expert analysis,\ndemonstrating the benefit of combining human and artificial intelligence.", "published": "2024-03-11 15:15:27", "link": "http://arxiv.org/abs/2403.09713v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Textual analysis of End User License Agreement for red-flagging\n  potentially malicious software", "abstract": "New software and updates are downloaded by end users every day. Each\ndowloaded software has associated with it an End Users License Agreements\n(EULA), but this is rarely read. An EULA includes information to avoid legal\nrepercussions. However,this proposes a host of potential problems such as\nspyware or producing an unwanted affect in the target system. End users do not\nread these EULA's because of length of the document and users find it extremely\ndifficult to understand. Text summarization is one of the relevant solution to\nthese kind of problems. This require a solution which can summarize the EULA\nand classify the EULA as \"Benign\" or \"Malicious\". We propose a solution in\nwhich we have summarize the EULA and classify the EULA as \"Benign\" or\n\"Malicious\". We extract EULA text of different sofware's then we classify the\ntext using eight different supervised classifiers. we use ensemble learning to\nclassify the EULA as benign or malicious using five different text\nsummarization methods. An accuracy of $95.8$\\% shows the effectiveness of the\npresented approach.", "published": "2024-03-11 20:45:27", "link": "http://arxiv.org/abs/2403.09715v1", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Prompt Selection and Augmentation for Few Examples Code Generation in\n  Large Language Model and its Application in Robotics Control", "abstract": "Few-shot prompting and step-by-step reasoning have enhanced the capabilities\nof Large Language Models (LLMs) in tackling complex tasks including code\ngeneration. In this paper, we introduce a prompt selection and augmentation\nalgorithm aimed at improving mathematical reasoning and robot arm operations.\nOur approach incorporates a multi-stage example augmentation scheme combined\nwith an example selection scheme. This algorithm improves LLM performance by\nselecting a set of examples that increase diversity, minimize redundancy, and\nincrease relevance to the question. When combined with the Program-of-Thought\nprompting, our algorithm demonstrates an improvement in performance on the\nGSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively.\nFurthermore, in simulated tabletop environments, our algorithm surpasses the\nCode-as-Policies approach by achieving a 3.4% increase in successful task\ncompletions and a decrease of over 70% in the number of examples used. Its\nability to discard examples that contribute little to solving the problem\nreduces the inferencing time of an LLM-powered robotics system. This algorithm\nalso offers important benefits for industrial process automation by\nstreamlining the development and deployment process, reducing manual\nprogramming effort, and enhancing code reusability.", "published": "2024-03-11 04:13:29", "link": "http://arxiv.org/abs/2403.12999v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "SMART: Automatically Scaling Down Language Models with Accuracy\n  Guarantees for Reduced Processing Fees", "abstract": "The advancement of Large Language Models (LLMs) has significantly boosted\nperformance in natural language processing (NLP) tasks. However, the deployment\nof high-performance LLMs incurs substantial costs, primarily due to the\nincreased number of parameters aimed at enhancing model performance. This has\nmade the use of state-of-the-art LLMs more expensive for end-users. AI service\nproviders, such as OpenAI and Anthropic, often offer multiple versions of LLMs\nwith varying prices and performance. However, end-users still face challenges\nin choosing the appropriate LLM for their tasks that balance result quality\nwith cost.\n  We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel\nLLM framework designed to minimize the inference costs of NLP tasks while\nensuring sufficient result quality. It enables users to specify an accuracy\nconstraint in terms of the equivalence of outputs to those of the most powerful\nLLM. SMART then generates results that deviate from the outputs of this LLM\nonly with a probability below a user-defined threshold. SMART employs a\nprofiling phase that evaluates the performance of multiple LLMs to identify\nthose that meet the user-defined accuracy level. SMART optimizes the tradeoff\nbetween profiling overheads and the anticipated cost savings resulting from\nprofiling. Moreover, our approach significantly reduces inference costs by\nstrategically leveraging a mix of LLMs. Our experiments on three real-world\ndatasets show that, based on OpenAI models, SMART achieves significant cost\nsavings, up to 25.6x in comparison to GPT-4.", "published": "2024-03-11 17:45:47", "link": "http://arxiv.org/abs/2403.13835v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Concurrent Speaker Detection: A multi-microphone Transformer-Based\n  Approach", "abstract": "We present a deep-learning approach for the task of Concurrent Speaker\nDetection (CSD) using a modified transformer model. Our model is designed to\nhandle multi-microphone data but can also work in the single-microphone case.\nThe method can classify audio segments into one of three classes: 1) no speech\nactivity (noise only), 2) only a single speaker is active, and 3) more than one\nspeaker is active. We incorporate a Cost-Sensitive (CS) loss and a confidence\ncalibration to the training procedure. The approach is evaluated using three\nreal-world databases: AMI, AliMeeting, and CHiME 5, demonstrating an\nimprovement over existing approaches.", "published": "2024-03-11 16:12:08", "link": "http://arxiv.org/abs/2403.06856v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Decoupling Frontend Enhancement and Backend Recognition in\n  Monaural Robust ASR", "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement (SE) algorithms. However, monaural SE has not been\nestablished as an effective frontend for automatic speech recognition (ASR) in\nnoisy conditions compared to an ASR model trained on noisy speech directly. The\ndivide between SE and ASR impedes the progress of robust ASR systems,\nespecially as SE has made major advances in recent years. This paper focuses on\neliminating this divide with an ARN (attentive recurrent network) time-domain\nand a CrossNet time-frequency domain enhancement models. The proposed systems\nfully decouple frontend enhancement and backend ASR trained only on clean\nspeech. Results on the WSJ, CHiME-2, LibriSpeech, and CHiME-4 corpora\ndemonstrate that ARN and CrossNet enhanced speech both translate to improved\nASR results in noisy and reverberant environments, and generalize well to real\nacoustic scenarios. The proposed system outperforms the baselines trained on\ncorrupted speech directly. Furthermore, it cuts the previous best word error\nrate (WER) on CHiME-2 by $28.4\\%$ relatively with a $5.57\\%$ WER, and achieves\n$3.32/4.44\\%$ WER on single-channel CHiME-4 simulated/real test data without\ntraining on CHiME-4.", "published": "2024-03-11 02:45:06", "link": "http://arxiv.org/abs/2403.06387v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SonoTraceLab -- A Raytracing-Based Acoustic Modelling System for\n  Simulating Echolocation Behavior of Bats", "abstract": "Echolocation is the prime sensing modality for many species of bats, who show\nthe intricate ability to perform a plethora of tasks in complex and\nunstructured environments. Understanding this exceptional feat of sensorimotor\ninteraction is a key aspect into building more robust and performant man-made\nsonar sensors. In order to better understand the underlying perception\nmechanisms it is important to get a good insight into the nature of the\nreflected signals that the bat perceives. While ensonification experiments are\nin important way to better understand the nature of these signals, they are as\ntime-consuming to perform as they are informative. In this paper we present\nSonoTraceLab, an open-source software package for simulating both technical as\nwell as biological sonar systems in complex scenes. Using simulation approaches\ncan drastically increase insights into the nature of biological echolocation\nsystems, while reducing the time- and material complexity of performing them.", "published": "2024-03-11 16:04:18", "link": "http://arxiv.org/abs/2403.06847v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cosine Scoring with Uncertainty for Neural Speaker Embedding", "abstract": "Uncertainty modeling in speaker representation aims to learn the variability\npresent in speech utterances. While the conventional cosine-scoring is\ncomputationally efficient and prevalent in speaker recognition, it lacks the\ncapability to handle uncertainty. To address this challenge, this paper\nproposes an approach for estimating uncertainty at the speaker embedding\nfront-end and propagating it to the cosine scoring back-end. Experiments\nconducted on the VoxCeleb and SITW datasets confirmed the efficacy of the\nproposed method in handling uncertainty arising from embedding estimation. It\nachieved improvement with 8.5% and 9.8% average reductions in EER and minDCF\ncompared to the conventional cosine similarity. It is also computationally\nefficient in practice.", "published": "2024-03-11 03:31:35", "link": "http://arxiv.org/abs/2403.06404v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
