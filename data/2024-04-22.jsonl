{"title": "Understanding the role of FFNs in driving multilingual behaviour in LLMs", "abstract": "Multilingualism in Large Language Models (LLMs) is an yet under-explored\narea. In this paper, we conduct an in-depth analysis of the multilingual\ncapabilities of a family of a Large Language Model, examining its architecture,\nactivation patterns, and processing mechanisms across languages. We introduce\nnovel metrics to probe the model's multilingual behaviour at different layers\nand shed light on the impact of architectural choices on multilingual\nprocessing.\n  Our findings reveal different patterns of multilinugal processing in the\nsublayers of Feed-Forward Networks of the models. Furthermore, we uncover the\nphenomenon of \"over-layerization\" in certain model configurations, where\nincreasing layer depth without corresponding adjustments to other parameters\nmay degrade model performance. Through comparisons within and across languages,\nwe demonstrate the interplay between model architecture, layer depth, and\nmultilingual processing capabilities of LLMs trained on multiple languages.", "published": "2024-04-22 03:47:00", "link": "http://arxiv.org/abs/2404.13855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Enhanced Language Models for Generating Multi-Paper Citations", "abstract": "Citation text plays a pivotal role in elucidating the connection between\nscientific documents, demanding an in-depth comprehension of the cited paper.\nConstructing citations is often time-consuming, requiring researchers to delve\ninto extensive literature and grapple with articulating relevant content. To\naddress this challenge, the field of citation text generation (CTG) has\nemerged. However, while earlier methods have primarily centered on creating\nsingle-sentence citations, practical scenarios frequently necessitate citing\nmultiple papers within a single paragraph. To bridge this gap, we propose a\nmethod that leverages Large Language Models (LLMs) to generate multi-citation\nsentences. Our approach involves a single source paper and a collection of\ntarget papers, culminating in a coherent paragraph containing multi-sentence\ncitation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,\ncomposed of English-language academic research papers in Computer Science,\nshowcasing multiple citation instances. In our experiments, we evaluate three\nLLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this\nendeavor. Additionally, we exhibit enhanced performance by integrating\nknowledge graphs from target papers into the prompts for generating citation\ntext. This research underscores the potential of harnessing LLMs for citation\ngeneration, opening a compelling avenue for exploring the intricate connections\nbetween scientific documents.", "published": "2024-04-22 04:30:36", "link": "http://arxiv.org/abs/2404.13865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical\n  dataset evaluation toolkit", "abstract": "Large language models (LLMs) have been explored in a variety of reasoning\ntasks including solving of mathematical problems. Each math dataset typically\nincludes its own specially designed evaluation script, which, while suitable\nfor its intended use, lacks generalizability across different datasets.\nConsequently, updates and adaptations to these evaluation tools tend to occur\nwithout being systematically reported, leading to inconsistencies and obstacles\nto fair comparison across studies. To bridge this gap, we introduce a\ncomprehensive mathematical evaluation toolkit that not only utilizes a python\ncomputer algebra system (CAS) for its numerical accuracy, but also integrates\nan optional LLM, known for its considerable natural language processing\ncapabilities. To validate the effectiveness of our toolkit, we manually\nannotated two distinct datasets. Our experiments demonstrate that the toolkit\nyields more robust evaluation results compared to prior works, even without an\nLLM. Furthermore, when an LLM is incorporated, there is a notable enhancement.\nThe code for our method will be made available at\n\\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}.", "published": "2024-04-22 07:03:44", "link": "http://arxiv.org/abs/2404.13925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language\n  Models", "abstract": "Large language models (LLMs) are essential tools that users employ across\nvarious scenarios, so evaluating their performance and guiding users in\nselecting the suitable service is important. Although many benchmarks exist,\nthey mainly focus on specific predefined model abilities, such as world\nknowledge, reasoning, etc. Based on these ability scores, it is hard for users\nto determine which LLM best suits their particular needs. To address these\nissues, we propose to evaluate LLMs from a user-centric perspective and design\nthis benchmark to measure their efficacy in satisfying user needs under\ndistinct intents. Firstly, we collect 1,846 real-world use cases from a user\nstudy with 712 participants from 23 countries. This first-hand data helps us\nunderstand actual user intents and needs in LLM interactions, forming the User\nReported Scenarios (URS) dataset, which is categorized with six types of user\nintents. Secondly, based on this authentic dataset, we benchmark 10 LLM\nservices with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well\nwith human preference in both real-world experience and pair-wise annotations,\nachieving Pearson correlations of 0.95 and 0.94, respectively. This alignment\nconfirms that the URS dataset and our evaluation method establish an effective\nuser-centric benchmark. The dataset, code, and process data are available at\nhttps://github.com/Alice1998/URS.", "published": "2024-04-22 07:32:03", "link": "http://arxiv.org/abs/2404.13940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\n  Simulating Documents in the Wild via Low-level Perturbations", "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly\ncrucial as their applicability expands across various domains and real-world\napplications. Retrieval-Augmented Generation (RAG) is a promising solution for\naddressing the limitations of LLMs, yet existing studies on the robustness of\nRAG often overlook the interconnected relationships between RAG components or\nthe potential threats prevalent in real-world databases, such as minor textual\nerrors. In this work, we investigate two underexplored aspects when assessing\nthe robustness of RAG: 1) vulnerability to noisy documents through low-level\nperturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we\nintroduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}),\nwhich targets these aspects. Specifically, GARAG is designed to reveal\nvulnerabilities within each component and test the overall system functionality\nagainst noisy documents. We validate RAG robustness by applying our\n\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and\nLLMs. The experimental results show that GARAG consistently achieves high\nattack success rates. Also, it significantly devastates the performance of each\ncomponent and their synergy, highlighting the substantial risk that minor\ntextual inaccuracies pose in disrupting RAG systems in the real world.", "published": "2024-04-22 07:49:36", "link": "http://arxiv.org/abs/2404.13948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability\n  with ECHO", "abstract": "The role-play ability of Large Language Models (LLMs) has emerged as a\npopular research direction. However, existing studies focus on imitating\nwell-known public figures or fictional characters, overlooking the potential\nfor simulating ordinary individuals. Such an oversight limits the potential for\nadvancements in digital human clones and non-player characters in video games.\nTo bridge this gap, we introduce ECHO, an evaluative framework inspired by the\nTuring test. This framework engages the acquaintances of the target individuals\nto distinguish between human and machine-generated responses. Notably, our\nframework focuses on emulating average individuals rather than historical or\nfictional figures, presenting a unique advantage to apply the Turing Test. We\nevaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as\nfoundational models, alongside the online application GPTs from OpenAI. Our\nresults demonstrate that GPT-4 more effectively deceives human evaluators, and\nGPTs achieves a leading success rate of 48.3%. Furthermore, we investigated\nwhether LLMs could discern between human-generated and machine-generated texts.\nWhile GPT-4 can identify differences, it could not determine which texts were\nhuman-produced. Our code and results of reproducing the role-playing LLMs are\nmade publicly available via https://github.com/CUHK-ARISE/ECHO.", "published": "2024-04-22 08:00:51", "link": "http://arxiv.org/abs/2404.13957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Re-Organization Improves Reasoning in Large Language Models", "abstract": "Improving the reasoning capabilities of large language models (LLMs) has\nattracted considerable interest. Recent approaches primarily focus on improving\nthe reasoning process to yield a more precise final answer. However, in\nscenarios involving contextually aware reasoning, these methods neglect the\nimportance of first identifying logical relationships from the context before\nproceeding with the reasoning. This oversight could lead to a superficial\nunderstanding and interaction with the context, potentially undermining the\nquality and reliability of the reasoning outcomes. In this paper, we propose an\ninformation re-organization (InfoRE) method before proceeding with the\nreasoning to enhance the reasoning ability of LLMs. Our re-organization method\ninvolves initially extracting logical relationships from the contextual\ncontent, such as documents or paragraphs, and subsequently pruning redundant\ncontent to minimize noise. Then, we utilize the re-organized information in the\nreasoning process. This enables LLMs to deeply understand the contextual\ncontent by clearly perceiving these logical relationships, while also ensuring\nhigh-quality responses by eliminating potential noise. To demonstrate the\neffectiveness of our approach in improving the reasoning ability, we conduct\nexperiments using Llama2-70B, GPT-3.5, and GPT-4 on various contextually aware\nmulti-hop reasoning tasks. Using only a zero-shot setting, our method achieves\nan average absolute improvement of 4% across all tasks, highlighting its\npotential to improve the reasoning performance of LLMs. Our source code is\navailable at https://github.com/hustcxx/InfoRE.", "published": "2024-04-22 08:47:27", "link": "http://arxiv.org/abs/2404.13985v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Know What They Need: Leveraging a Missing Information Guided\n  Framework to Empower Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating\noutdated knowledge or hallucination by supplying LLMs with updated and relevant\nknowledge. However, there are still several difficulties for RAG in\nunderstanding complex multi-hop query and retrieving relevant documents, which\nrequire LLMs to perform reasoning and retrieve step by step. Inspired by\nhuman's reasoning process in which they gradually search for the required\ninformation, it is natural to ask whether the LLMs could notice the missing\ninformation in each reasoning step. In this work, we first experimentally\nverified the ability of LLMs to extract information as well as to know the\nmissing. Based on the above discovery, we propose a Missing Information Guided\nRetrieve-Extraction-Solving paradigm (MIGRES), where we leverage the\nidentification of missing information to generate a targeted query that steers\nthe subsequent knowledge retrieval. Besides, we design a sentence-level\nre-ranking filtering approach to filter the irrelevant content out from\ndocument, along with the information extraction capability of LLMs to extract\nuseful information from cleaned-up documents, which in turn to bolster the\noverall efficacy of RAG. Extensive experiments conducted on multiple public\ndatasets reveal the superiority of the proposed MIGRES method, and analytical\nexperiments demonstrate the effectiveness of our proposed modules.", "published": "2024-04-22 09:56:59", "link": "http://arxiv.org/abs/2404.14043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bored to Death: Artificial Intelligence Research Reveals the Role of\n  Boredom in Suicide Behavior", "abstract": "Background: Recent advancements in Artificial Intelligence (AI) contributed\nsignificantly to suicide assessment, however, our theoretical understanding of\nthis complex behavior is still limited. Objective: This study aimed to harness\nAI methodologies to uncover hidden risk factors that trigger or aggravate\nsuicide behaviors. Method: The primary dataset included 228,052 Facebook\npostings by 1,006 users who completed the gold-standard Columbia Suicide\nSeverity Rating Scale. This dataset was analyzed using a bottom-up research\npipeline without a-priory hypotheses and its findings were validated using a\ntop-down analysis of a new dataset. This secondary dataset included responses\nby 1,062 participants to the same suicide scale as well as to well-validated\nscales measuring depression and boredom. Results: An almost fully automated,\nAI-guided research pipeline resulted in four Facebook topics that predicted the\nrisk of suicide, of which the strongest predictor was boredom. A comprehensive\nliterature review using APA PsycInfo revealed that boredom is rarely perceived\nas a unique risk factor of suicide. A complementing top-down path analysis of\nthe secondary dataset uncovered an indirect relationship between boredom and\nsuicide, which was mediated by depression. An equivalent mediated relationship\nwas observed in the primary Facebook dataset as well. However, here, a direct\nrelationship between boredom and suicide risk was also observed. Conclusions:\nIntegrating AI methods allowed the discovery of an under-researched risk factor\nof suicide. The study signals boredom as a maladaptive 'ingredient' that might\ntrigger suicide behaviors, regardless of depression. Further studies are\nrecommended to direct clinicians' attention to this burdening, and sometimes\nexistential experience.", "published": "2024-04-22 10:16:02", "link": "http://arxiv.org/abs/2404.14057v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy\n  Data in Misaligned Languages Suffice?", "abstract": "Traditionally, success in multilingual machine translation can be attributed\nto three key factors in training data: large volume, diverse translation\ndirections, and high quality. In the current practice of fine-tuning large\nlanguage models (LLMs) for translation, we revisit the importance of these\nfactors. We find that LLMs display strong translation capability after being\nfine-tuned on as few as 32 parallel sentences and that fine-tuning on a single\ntranslation direction enables translation in multiple directions. However, the\nchoice of direction is critical: fine-tuning LLMs with only English on the\ntarget side can lead to task misinterpretation, which hinders translation into\nnon-English languages. Problems also arise when noisy synthetic data is placed\non the target side, especially when the target language is well-represented in\nLLM pre-training. Yet interestingly, synthesized data in an under-represented\nlanguage has a less pronounced effect. Our findings suggest that when adapting\nLLMs to translation, the requirement on data quantity can be eased but careful\nconsiderations are still crucial to prevent an LLM from exploiting unintended\ndata biases.", "published": "2024-04-22 12:21:12", "link": "http://arxiv.org/abs/2404.14122v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual\n  Machine-Generated Text Detection", "abstract": "We present the results and the main findings of SemEval-2024 Task 8:\nMultigenerator, Multidomain, and Multilingual Machine-Generated Text Detection.\nThe task featured three subtasks. Subtask A is a binary classification task\ndetermining whether a text is written by a human or generated by a machine.\nThis subtask has two tracks: a monolingual track focused solely on English\ntexts and a multilingual track. Subtask B is to detect the exact source of a\ntext, discerning whether it is written by a human or generated by a specific\nLLM. Subtask C aims to identify the changing point within a text, at which the\nauthorship transitions from human to machine. The task attracted a large number\nof participants: subtask A monolingual (126), subtask A multilingual (59),\nsubtask B (70), and subtask C (30). In this paper, we present the task, analyze\nthe results, and discuss the system submissions and the methods they used. For\nall subtasks, the best systems used LLMs.", "published": "2024-04-22 13:56:07", "link": "http://arxiv.org/abs/2404.14183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EnzChemRED, a rich enzyme chemistry relation extraction dataset", "abstract": "Expert curation is essential to capture knowledge of enzyme functions from\nthe scientific literature in FAIR open knowledgebases but cannot keep pace with\nthe rate of new discoveries and new publications. In this work we present\nEnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training\nand benchmarking dataset to support the development of Natural Language\nProcessing (NLP) methods such as (large) language models that can assist enzyme\ncuration. EnzChemRED consists of 1,210 expert curated PubMed abstracts in which\nenzymes and the chemical reactions they catalyze are annotated using\nidentifiers from the UniProt Knowledgebase (UniProtKB) and the ontology of\nChemical Entities of Biological Interest (ChEBI). We show that fine-tuning\npre-trained language models with EnzChemRED can significantly boost their\nability to identify mentions of proteins and chemicals in text (Named Entity\nRecognition, or NER) and to extract the chemical conversions in which they\nparticipate (Relation Extraction, or RE), with average F1 score of 86.30% for\nNER, 86.66% for RE for chemical conversion pairs, and 83.79% for RE for\nchemical conversion pairs and linked enzymes. We combine the best performing\nmethods after fine-tuning using EnzChemRED to create an end-to-end pipeline for\nknowledge extraction from text and apply this to abstracts at PubMed scale to\ncreate a draft map of enzyme functions in literature to guide curation efforts\nin UniProtKB and the reaction knowledgebase Rhea. The EnzChemRED corpus is\nfreely available at https://ftp.expasy.org/databases/rhea/nlp/.", "published": "2024-04-22 14:18:34", "link": "http://arxiv.org/abs/2404.14209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table\n  Generation via Global Tuple Extraction", "abstract": "The task of condensing large chunks of textual information into concise and\nstructured tables has gained attention recently due to the emergence of Large\nLanguage Models (LLMs) and their potential benefit for downstream tasks, such\nas text summarization and text mining. Previous approaches often generate\ntables that directly replicate information from the text, limiting their\napplicability in broader contexts, as text-to-table generation in real-life\nscenarios necessitates information extraction, reasoning, and integration.\nHowever, there is a lack of both datasets and methodologies towards this task.\nIn this paper, we introduce LiveSum, a new benchmark dataset created for\ngenerating summary tables of competitions based on real-time commentary texts.\nWe evaluate the performances of state-of-the-art LLMs on this task in both\nfine-tuning and zero-shot settings, and additionally propose a novel pipeline\ncalled $T^3$(Text-Tuple-Table) to improve their performances. Extensive\nexperimental results demonstrate that LLMs still struggle with this task even\nafter fine-tuning, while our approach can offer substantial performance gains\nwithout explicit training. Further analyses demonstrate that our method\nexhibits strong generalization abilities, surpassing previous approaches on\nseveral other text-to-table datasets. Our code and data can be found at\nhttps://github.com/HKUST-KnowComp/LiveSum.", "published": "2024-04-22 14:31:28", "link": "http://arxiv.org/abs/2404.14215v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marking: Visual Grading with Highlighting Errors and Annotating Missing\n  Bits", "abstract": "In this paper, we introduce \"Marking\", a novel grading task that enhances\nautomated grading systems by performing an in-depth analysis of student\nresponses and providing students with visual highlights. Unlike traditional\nsystems that provide binary scores, \"marking\" identifies and categorizes\nsegments of the student response as correct, incorrect, or irrelevant and\ndetects omissions from gold answers. We introduce a new dataset meticulously\ncurated by Subject Matter Experts specifically for this task. We frame\n\"Marking\" as an extension of the Natural Language Inference (NLI) task, which\nis extensively explored in the field of Natural Language Processing. The gold\nanswer and the student response play the roles of premise and hypothesis in\nNLI, respectively. We subsequently train language models to identify\nentailment, contradiction, and neutrality from student response, akin to NLI,\nand with the added dimension of identifying omissions from gold answers. Our\nexperimental setup involves the use of transformer models, specifically BERT\nand RoBERTa, and an intelligent training step using the e-SNLI dataset. We\npresent extensive baseline results highlighting the complexity of the \"Marking\"\ntask, which sets a clear trajectory for the upcoming study. Our work not only\nopens up new avenues for research in AI-powered educational assessment tools,\nbut also provides a valuable benchmark for the AI in education community to\nengage with and improve upon in the future. The code and dataset can be found\nat https://github.com/luffycodes/marking.", "published": "2024-04-22 16:00:46", "link": "http://arxiv.org/abs/2404.14301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Alignment with Mutual Information: Learning to Follow\n  Principles without Preference Labels", "abstract": "When prompting a language model (LM), users often expect the model to adhere\nto a set of behavioral principles across diverse tasks, such as producing\ninsightful content while avoiding harmful or biased language. Instilling such\nprinciples (i.e., a constitution) into a model is resource-intensive,\ntechnically challenging, and generally requires human preference labels or\nexamples. We introduce SAMI, an iterative algorithm that finetunes a pretrained\nlanguage model (without requiring preference labels or demonstrations) to\nincrease the conditional mutual information between constitutions and\nself-generated responses given queries from a dataset. On single-turn dialogue\nand summarization, a SAMI-trained mistral-7b outperforms the initial pretrained\nmodel, with win rates between 66% and 77%. Strikingly, it also surpasses an\ninstruction-finetuned baseline (mistral-7b-instruct) with win rates between 55%\nand 57% on single-turn dialogue. SAMI requires a model that writes the\nprinciples. To avoid dependence on strong models for writing principles, we\nalign a strong pretrained model (mixtral-8x7b) using constitutions written by a\nweak instruction-finetuned model (mistral-7b-instruct), achieving a 65% win\nrate on summarization. Finally, we investigate whether SAMI generalizes to\ndiverse summarization principles (e.g., \"summaries should be scientific\") and\nscales to stronger models (llama3-70b), finding that it achieves win rates of\nup to 68% for learned and 67% for held-out principles compared to the base\nmodel. Our results show that a pretrained LM can learn to follow constitutions\nwithout using preference labels, demonstrations, or human oversight.", "published": "2024-04-22 16:20:36", "link": "http://arxiv.org/abs/2404.14313v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Long Answer Grading with RiceChem Dataset", "abstract": "We introduce a new area of study in the field of educational Natural Language\nProcessing: Automated Long Answer Grading (ALAG). Distinguishing itself from\nAutomated Short Answer Grading (ASAG) and Automated Essay Grading (AEG), ALAG\npresents unique challenges due to the complexity and multifaceted nature of\nfact-based long answers. To study ALAG, we introduce RiceChem, a dataset\nderived from a college chemistry course, featuring real student responses to\nlong-answer questions with an average word count notably higher than typical\nASAG datasets. We propose a novel approach to ALAG by formulating it as a\nrubric entailment problem, employing natural language inference models to\nverify whether each criterion, represented by a rubric item, is addressed in\nthe student's response. This formulation enables the effective use of MNLI for\ntransfer learning, significantly improving the performance of models on the\nRiceChem dataset. We demonstrate the importance of rubric-based formulation in\nALAG, showcasing its superiority over traditional score-based approaches in\ncapturing the nuances of student responses. We also investigate the performance\nof models in cold start scenarios, providing valuable insights into the\npractical deployment considerations in educational settings. Lastly, we\nbenchmark state-of-the-art open-sourced Large Language Models (LLMs) on\nRiceChem and compare their results to GPT models, highlighting the increased\ncomplexity of ALAG compared to ASAG. Despite leveraging the benefits of a\nrubric-based approach and transfer learning from MNLI, the lower performance of\nLLMs on RiceChem underscores the significant difficulty posed by the ALAG task.\nWith this work, we offer a fresh perspective on grading long, fact-based\nanswers and introduce a new dataset to stimulate further research in this\nimportant area. Code:\n\\url{https://github.com/luffycodes/Automated-Long-Answer-Grading}.", "published": "2024-04-22 16:28:09", "link": "http://arxiv.org/abs/2404.14316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-lingual Stance Detection via Adversarial Language\n  Adaptation", "abstract": "Stance detection has been widely studied as the task of determining if a\nsocial media post is positive, negative or neutral towards a specific issue,\nsuch as support towards vaccines. Research in stance detection has however\noften been limited to a single language and, where more than one language has\nbeen studied, research has focused on few-shot settings, overlooking the\nchallenges of developing a zero-shot cross-lingual stance detection model. This\npaper makes the first such effort by introducing a novel approach to zero-shot\ncross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB),\naiming to enhance the performance of a cross-lingual classifier in the absence\nof explicit training data for target languages. Our technique employs\ntranslation augmentation to improve zero-shot performance and pairs it with\nadversarial learning to further boost model efficacy. Through experiments on\ndatasets labeled for stance towards vaccines in four languages English, German,\nFrench, Italian. We demonstrate the effectiveness of our proposed approach,\nshowcasing improved results in comparison to a strong baseline model as well as\nablated versions of our model. Our experiments demonstrate the effectiveness of\nmodel components, not least the translation-augmented data as well as the\nadversarial learning component, to the improved performance of the model. We\nhave made our source code accessible on GitHub.", "published": "2024-04-22 16:56:43", "link": "http://arxiv.org/abs/2404.14339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Synthetic Data by Retrieving and Transforming Existing Datasets", "abstract": "Despite recent advances in large language models, building dependable and\ndeployable NLP models typically requires abundant, high-quality training data.\nHowever, task-specific data is not available for many use cases, and manually\ncurating task-specific data is labor-intensive. Recent work has studied\nprompt-driven synthetic data generation using large language models, but these\ngenerated datasets tend to lack complexity and diversity. To address these\nlimitations, we introduce a method, DataTune, to make better use of existing,\npublicly available datasets to improve automatic dataset generation. DataTune\nperforms dataset transformation, enabling the repurposing of publicly available\ndatasets into a format that is directly aligned with the specific requirements\nof target tasks. On a diverse set of language-based tasks from the BIG-Bench\nbenchmark, we find that finetuning language models via DataTune improves over a\nfew-shot prompting baseline by 49% and improves over existing methods that use\nsynthetic or retrieved training data by 34%. We find that dataset\ntransformation significantly increases the diversity and difficulty of\ngenerated data on many tasks. We integrate DataTune into an open-source\nrepository to make this method accessible to the community:\nhttps://github.com/neulab/prompt2model.", "published": "2024-04-22 17:15:32", "link": "http://arxiv.org/abs/2404.14361v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical\n  Error Detection and Correction", "abstract": "Medical errors in clinical text pose significant risks to patient safety. The\nMEDIQA-CORR 2024 shared task focuses on detecting and correcting these errors\nacross three subtasks: identifying the presence of an error, extracting the\nerroneous sentence, and generating a corrected sentence. In this paper, we\npresent our approach that achieved top performance in all three subtasks. For\nthe MS dataset, which contains subtle errors, we developed a retrieval-based\nsystem leveraging external medical question-answering datasets. For the UW\ndataset, reflecting more realistic clinical notes, we created a pipeline of\nmodules to detect, localize, and correct errors. Both approaches utilized the\nDSPy framework for optimizing prompts and few-shot examples in large language\nmodel (LLM) based programs. Our results demonstrate the effectiveness of LLM\nbased programs for medical error correction. However, our approach has\nlimitations in addressing the full diversity of potential errors in medical\ndocumentation. We discuss the implications of our work and highlight future\nresearch directions to advance the robustness and applicability of medical\nerror detection and correction systems.", "published": "2024-04-22 19:31:45", "link": "http://arxiv.org/abs/2404.14544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using\n  Large Language Models", "abstract": "This paper outlines our submission to the MEDIQA2024 Multilingual and\nMultimodal Medical Answer Generation (M3G) shared task. We report results for\ntwo standalone solutions under the English category of the task, the first\ninvolving two consecutive API calls to the Claude 3 Opus API and the second\ninvolving training an image-disease label joint embedding in the style of CLIP\nfor image classification. These two solutions scored 1st and 2nd place\nrespectively on the competition leaderboard, substantially outperforming the\nnext best solution. Additionally, we discuss insights gained from\npost-competition experiments. While the performance of these two solutions have\nsignificant room for improvement due to the difficulty of the shared task and\nthe challenging nature of medical visual question answering in general, we\nidentify the multi-stage LLM approach and the CLIP image classification\napproach as promising avenues for further investigation.", "published": "2024-04-22 20:29:58", "link": "http://arxiv.org/abs/2404.14567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Describe-then-Reason: Improving Multimodal Mathematical Reasoning\n  through Visual Comprehension Training", "abstract": "Open-source multimodal large language models (MLLMs) excel in various tasks\ninvolving textual and visual inputs but still struggle with complex multimodal\nmathematical reasoning, lagging behind proprietary models like GPT-4V(ision)\nand Gemini-Pro. Although fine-tuning with intermediate steps (i.e., rationales)\nelicits some mathematical reasoning skills, the resulting models still fall\nshort in visual comprehension due to inadequate visual-centric supervision,\nwhich leads to inaccurate interpretation of math figures. To address this\nissue, we propose a two-step training pipeline VCAR, which emphasizes the\nVisual Comprehension training in Addition to mathematical Reasoning learning.\nIt first improves the visual comprehension ability of MLLMs through the visual\ndescription generation task, followed by another training step on generating\nrationales with the assistance of descriptions. Experimental results on two\npopular benchmarks demonstrate that VCAR substantially outperforms baseline\nmethods solely relying on rationale supervision, especially on problems with\nhigh visual demands.", "published": "2024-04-22 21:59:35", "link": "http://arxiv.org/abs/2404.14604v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language\n  Learning", "abstract": "This paper introduces \\textbf{Q-tuning}, a novel approach for continual\nprompt tuning that enables the lifelong learning of a pre-trained language\nmodel. When learning a new task, Q-tuning trains a task-specific prompt by\nadding it to a prompt queue consisting of the prompts from older tasks. To\nbetter transfer the knowledge of old tasks, we design an adaptive knowledge\naggregation technique that reweighs previous prompts in the queue with a\nlearnable low-rank matrix. Once the prompt queue reaches its maximum capacity,\nwe leverage a PCA-based eviction rule to reduce the queue's size, allowing the\nnewly trained prompt to be added while preserving the primary knowledge of old\ntasks. In order to mitigate the accumulation of information loss caused by the\neviction, we additionally propose a globally shared prefix prompt and a memory\nretention regularization based on information theory. Extensive experiments\ndemonstrate that our approach outperforms the state-of-the-art methods\nsubstantially on continual prompt tuning benchmarks. Moreover, our approach\nenables lifelong learning on linearly growing task sequences while requiring\nconstant complexity for training and inference.", "published": "2024-04-22 22:04:16", "link": "http://arxiv.org/abs/2404.14607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From LLM to NMT: Advancing Low-Resource Machine Translation with Claude", "abstract": "We show that Claude 3 Opus, a large language model (LLM) released by\nAnthropic in March 2024, exhibits stronger machine translation competence than\nother LLMs. Though we find evidence of data contamination with Claude on\nFLORES-200, we curate new benchmarks that corroborate the effectiveness of\nClaude for low-resource machine translation into English. We find that Claude\nhas remarkable \\textit{resource efficiency} -- the degree to which the quality\nof the translation model depends on a language pair's resource level. Finally,\nwe show that advancements in LLM translation can be compressed into traditional\nneural machine translation (NMT) models. Using Claude to generate synthetic\ndata, we demonstrate that knowledge distillation advances the state-of-the-art\nin Yoruba-English translation, meeting or surpassing strong baselines like\nNLLB-54B and Google Translate.", "published": "2024-04-22 01:22:23", "link": "http://arxiv.org/abs/2404.13813v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking\n  Enhances Visual Commonsense Reasoning", "abstract": "Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to\nanswer visual questions requiring human commonsense, and to provide rationales\nexplaining why the answers are correct. With emergence of Large Language Models\n(LLMs), it is natural and imperative to explore their applicability to VCR.\nHowever, VCR task demands more external knowledge to tackle its challenging\nquestions, necessitating special designs to activate LLMs' commonsense\nreasoning abilities. Also, most existing Multimodal LLMs adopted an abstraction\nof entire input image, which makes it difficult to comprehend VCR's unique\nco-reference tags between image regions and text, posing challenges for\nfine-grained alignment. To address these issues, we propose EventLens that\nleverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR.\nFirst, by emulating the cognitive process of human reasoning, an Event-Aware\nPretraining auxiliary task is introduced to better activate LLM's global\ncomprehension of intricate scenarios. Second, during fine-tuning, we further\nutilize reference tags to bridge RoI features with texts, while preserving both\nmodality semantics. Finally, we use instruct-style prompts to narrow the gap\nbetween pretraining and fine-tuning, and task-specific adapters to better\nintegrate LLM's inherent knowledge with new commonsense. Experimental results\nshow the effectiveness of our proposed auxiliary task and fine-grained linking\nstrategy.", "published": "2024-04-22 03:05:32", "link": "http://arxiv.org/abs/2404.13847v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large\n  Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) suffer from hallucination issues,\nwherein the models generate plausible-sounding but factually incorrect outputs,\nundermining their reliability. A comprehensive quantitative evaluation is\nnecessary to identify and understand the extent of hallucinations in these\nmodels. However, existing benchmarks are often limited in scope, focusing\nmainly on object hallucinations. Furthermore, current evaluation methods\nstruggle to effectively address the subtle semantic distinctions between model\noutputs and reference data, as well as the balance between hallucination and\ninformativeness. To address these issues, we introduce a multi-dimensional\nbenchmark covering objects, attributes, and relations, with challenging images\nselected based on associative biases. Moreover, we propose a large language\nmodel (LLM)-based two-stage evaluation framework that generalizes the popular\nCHAIR metric and incorporates both faithfulness and coverage into the\nevaluation. Experiments on 10 established LVLMs demonstrate that our evaluation\nmetric is more comprehensive and better correlated with humans than existing\nwork when evaluating on our challenging human-annotated benchmark dataset. Our\nwork also highlights the critical balance between faithfulness and coverage of\nmodel outputs, and encourages future works to address hallucinations in LVLMs\nwhile keeping their outputs informative.", "published": "2024-04-22 04:49:22", "link": "http://arxiv.org/abs/2404.13874v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generating Attractive and Authentic Copywriting from Customer Reviews", "abstract": "The goal of product copywriting is to capture the interest of potential\nbuyers by emphasizing the features of products through text descriptions. As\ne-commerce platforms offer a wide range of services, it's becoming essential to\ndynamically adjust the styles of these auto-generated descriptions. Typical\napproaches to copywriting generation often rely solely on specified product\nattributes, which may result in dull and repetitive content. To tackle this\nissue, we propose to generate copywriting based on customer reviews, as they\nprovide firsthand practical experiences with products, offering a richer source\nof information than just product attributes. We have developed a\nsequence-to-sequence framework, enhanced with reinforcement learning, to\nproduce copywriting that is attractive, authentic, and rich in information. Our\nframework outperforms all existing baseline and zero-shot large language\nmodels, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness\nand faithfulness. Furthermore, this work features the use of LLMs for\naspect-based summaries collection and argument allure assessment. Experiments\ndemonstrate the effectiveness of using LLMs for marketing domain corpus\nconstruction. The code and the dataset is publicly available at:\nhttps://github.com/YuXiangLin1234/Copywriting-Generation.", "published": "2024-04-22 06:33:28", "link": "http://arxiv.org/abs/2404.13906v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring neural oscillations during speech perception via surrogate\n  gradient spiking neural networks", "abstract": "Understanding cognitive processes in the brain demands sophisticated models\ncapable of replicating neural dynamics at large scales. We present a\nphysiologically inspired speech recognition architecture, compatible and\nscalable with deep learning frameworks, and demonstrate that end-to-end\ngradient descent training leads to the emergence of neural oscillations in the\ncentral spiking neural network. Significant cross-frequency couplings,\nindicative of these oscillations, are measured within and across network layers\nduring speech processing, whereas no such interactions are observed when\nhandling background noise inputs. Furthermore, our findings highlight the\ncrucial inhibitory role of feedback mechanisms, such as spike frequency\nadaptation and recurrent connections, in regulating and synchronising neural\nactivity to improve recognition performance. Overall, on top of developing our\nunderstanding of synchronisation phenomena notably observed in the human\nauditory pathway, our architecture exhibits dynamic and efficient information\nprocessing, with relevance to neuromorphic technology.", "published": "2024-04-22 09:40:07", "link": "http://arxiv.org/abs/2404.14024v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Differential contributions of machine learning and statistical analysis\n  to language and cognitive sciences", "abstract": "Data-driven approaches have revolutionized scientific research, with machine\nlearning and statistical analysis being commonly used methodologies. Despite\ntheir widespread use, these approaches differ significantly in their\ntechniques, objectives and implementations. Few studies have systematically\napplied both methods to identical datasets to highlight potential differences,\nparticularly in language and cognitive sciences. This study employs the Buckeye\nSpeech Corpus to illustrate how machine learning and statistical analysis are\napplied in data-driven research to obtain distinct insights on language\nproduction. We demonstrate the theoretical differences, implementation steps,\nand unique objectives of each approach through a comprehensive, tutorial-like\ncomparison. Our analysis reveals that while machine learning excels at pattern\nrecognition and prediction, statistical methods provide deeper insights into\nrelationships between variables. The study highlights how semantic relevance, a\nnovel metric measuring contextual influence on target words, contributes to\nunderstanding word duration in speech. We also systematically compare the\ndifferences between regression models used in machine learning and statistical\nanalysis, particularly focusing on the training and fitting processes.\nAdditionally, we clarify several common misconceptions that contribute to the\nconfusion between these two approaches. Overall, by elucidating the\ncomplementary strengths of machine learning and statistics, this research\nenhances our understanding of diverse data-driven strategies in language and\ncognitive sciences, offering researchers valuable guidance on when and how to\neffectively apply these approaches in different research contexts.", "published": "2024-04-22 10:06:21", "link": "http://arxiv.org/abs/2404.14052v2", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Swap distance minimization beyond entropy minimization in word order\n  variation", "abstract": "Here we consider the problem of all the possible orders of a linguistic\nstructure formed by $n$ elements, for instance, subject, direct object and verb\n($n=3$) or subject, direct object, indirect object and verb ($n=4$). We\ninvestigate if the frequency of the $n!$ possible orders is constrained by two\nprinciples. First, entropy minimization, a principle that has been suggested to\nshape natural communication systems at distinct levels of organization. Second,\nswap distance minimization, namely a preference for word orders that require\nfewer swaps of adjacent elements to be produced from a source order. Here we\npresent average swap distance, a novel score for research on swap distance\nminimization, and investigate the theoretical distribution of that score for\nany $n$: its minimum and maximum values and its expected value in die rolling\nexperiments or when the word order frequencies are shuffled. We investigate\nwhether entropy and average swap distance are significantly small in distinct\nlinguistic structures with $n=3$ or $n=4$ in agreement with the corresponding\nminimization principles. We find strong evidence of entropy minimization and\nswap distance minimization with respect to a die rolling experiment. The\nevidence of these two forces with respect to a Polya urn process is strong for\n$n=4$ but weaker for $n=3$. We still find evidence of swap distance\nminimization when word order frequencies are shuffled, indicating that swap\ndistance minimization effects are beyond pressure to minimize word order\nentropy.", "published": "2024-04-22 14:01:09", "link": "http://arxiv.org/abs/2404.14192v4", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. Our training dataset is a\nscaled-up version of the one used for phi-2, composed of heavily filtered\npublicly available web data and synthetic data. The model is also further\naligned for robustness, safety, and chat format. We also provide\nparameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called\nphi-3-small, phi-3-medium, both significantly more capable than phi-3-mini\n(e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance\nmultilingual, multimodal, and long-context capabilities, we introduce three\nmodels in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision.\nThe phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters,\nachieves superior performance in language reasoning, math, and code tasks\ncompared to other open-source models of similar scale, such as Llama 3.1 and\nthe Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini.\nMeanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from\nphi-3.5-mini, excels in reasoning tasks and is adept at handling both\nsingle-image and text prompts, as well as multi-image and text prompts.", "published": "2024-04-22 14:32:33", "link": "http://arxiv.org/abs/2404.14219v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What do Transformers Know about Government?", "abstract": "This paper investigates what insights about linguistic features and what\nknowledge about the structure of natural language can be obtained from the\nencodings in transformer language models.In particular, we explore how BERT\nencodes the government relation between constituents in a sentence. We use\nseveral probing classifiers, and data from two morphologically rich languages.\nOur experiments show that information about government is encoded across all\ntransformer layers, but predominantly in the early layers of the model. We find\nthat, for both languages, a small number of attention heads encode enough\ninformation about the government relations to enable us to train a classifier\ncapable of discovering new, previously unknown types of government, never seen\nin the training data. Currently, data is lacking for the research community\nworking on grammatical constructions, and government in particular. We release\nthe Government Bank -- a dataset defining the government relations for\nthousands of lemmas in the languages in our experiments.", "published": "2024-04-22 15:15:50", "link": "http://arxiv.org/abs/2404.14270v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Efficient Inference for Large Language Models", "abstract": "Large Language Models (LLMs) have attracted extensive attention due to their\nremarkable performance across various tasks. However, the substantial\ncomputational and memory requirements of LLM inference pose challenges for\ndeployment in resource-constrained scenarios. Efforts within the field have\nbeen directed towards developing techniques aimed at enhancing the efficiency\nof LLM inference. This paper presents a comprehensive survey of the existing\nliterature on efficient LLM inference. We start by analyzing the primary causes\nof the inefficient LLM inference, i.e., the large model size, the\nquadratic-complexity attention operation, and the auto-regressive decoding\napproach. Then, we introduce a comprehensive taxonomy that organizes the\ncurrent literature into data-level, model-level, and system-level optimization.\nMoreover, the paper includes comparative experiments on representative methods\nwithin critical sub-fields to provide quantitative insights. Last but not\nleast, we provide some knowledge summary and discuss future research\ndirections.", "published": "2024-04-22 15:53:08", "link": "http://arxiv.org/abs/2404.14294v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language\n  Models", "abstract": "Quantitative and numerical comprehension in language is an important task in\nmany fields like education and finance, but still remains a challenging task\nfor language models. While tool and calculator usage has shown to be helpful to\nimprove mathematical reasoning in large pretrained decoder-only language\nmodels, this remains unexplored for smaller language models with encoders. In\nthis paper, we propose Pre-Calc, a simple pre-finetuning objective of learning\nto use the calculator for both encoder-only and encoder-decoder architectures,\nformulated as a discriminative and generative task respectively. We pre-train\nBERT and RoBERTa for discriminative calculator use and Flan-T5 for generative\ncalculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves\nperformance on downstream tasks that require numerical understanding. Our code\nand data are available at https://github.com/calc-cmu/pre-calc.", "published": "2024-04-22 17:07:25", "link": "http://arxiv.org/abs/2404.14355v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Scaling: Predicting Patent Approval with Domain-specific\n  Fine-grained Claim Dependency Graph", "abstract": "Model scaling is becoming the default choice for many language tasks due to\nthe success of large language models (LLMs). However, it can fall short in\nspecific scenarios where simple customized methods excel. In this paper, we\ndelve into the patent approval pre-diction task and unveil that simple\ndomain-specific graph methods outperform enlarging the model, using the\nintrinsic dependencies within the patent data. Specifically, we first extend\nthe embedding-based state-of-the-art (SOTA) by scaling up its backbone model\nwith various sizes of open-source LLMs, then explore prompt-based methods to\nharness proprietary LLMs' potential, but find the best results close to random\nguessing, underlining the ineffectiveness of model scaling-up. Hence, we\npropose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous\npatent data analyses, capturing the inherent dependencies across segments of\nthe patent text. As it is model-agnostic, we apply cost-effective graph models\nto our FLAN Graph to obtain representations for approval prediction. Extensive\nexperiments and detailed analyses prove that incorporating FLAN Graph via\nvarious graph models consistently outperforms all LLM baselines significantly.\nWe hope that our observations and analyses in this paper can bring more\nattention to this challenging task and prompt further research into the\nlimitations of LLMs. Our source code and dataset can be obtained from\nhttp://github.com/ShangDataLab/FLAN-Graph.", "published": "2024-04-22 17:22:31", "link": "http://arxiv.org/abs/2404.14372v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Self-Evolution of Large Language Models", "abstract": "Large language models (LLMs) have significantly advanced in various fields\nand intelligent agent applications. However, current LLMs that learn from human\nor external model supervision are costly and may face performance ceilings as\ntask complexity and diversity increase. To address this issue, self-evolution\napproaches that enable LLM to autonomously acquire, refine, and learn from\nexperiences generated by the model itself are rapidly growing. This new\ntraining paradigm inspired by the human experiential learning process offers\nthe potential to scale LLMs towards superintelligence. In this work, we present\na comprehensive survey of self-evolution approaches in LLMs. We first propose a\nconceptual framework for self-evolution and outline the evolving process as\niterative cycles composed of four phases: experience acquisition, experience\nrefinement, updating, and evaluation. Second, we categorize the evolution\nobjectives of LLMs and LLM-based agents; then, we summarize the literature and\nprovide taxonomy and insights for each module. Lastly, we pinpoint existing\nchallenges and propose future directions to improve self-evolution frameworks,\nequipping researchers with critical insights to fast-track the development of\nself-evolving LLMs. Our corresponding GitHub repository is available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM", "published": "2024-04-22 17:43:23", "link": "http://arxiv.org/abs/2404.14387v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Chemistry Knowledge in Large Language Models via Prompt\n  Engineering", "abstract": "This paper presents a study on the integration of domain-specific knowledge\nin prompt engineering to enhance the performance of large language models\n(LLMs) in scientific domains. A benchmark dataset is curated to encapsulate the\nintricate physical-chemical properties of small molecules, their drugability\nfor pharmacology, alongside the functional attributes of enzymes and crystal\nmaterials, underscoring the relevance and applicability across biological and\nchemical domains.The proposed domain-knowledge embedded prompt engineering\nmethod outperforms traditional prompt engineering strategies on various\nmetrics, including capability, accuracy, F1 score, and hallucination drop. The\neffectiveness of the method is demonstrated through case studies on complex\nmaterials including the MacMillan catalyst, paclitaxel, and lithium cobalt\noxide. The results suggest that domain-knowledge prompts can guide LLMs to\ngenerate more accurate and relevant responses, highlighting the potential of\nLLMs as powerful tools for scientific discovery and innovation when equipped\nwith domain-specific prompts. The study also discusses limitations and future\ndirections for domain-specific prompt engineering development.", "published": "2024-04-22 16:55:44", "link": "http://arxiv.org/abs/2404.14467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SnapKV: LLM Knows What You are Looking for Before Generation", "abstract": "Large Language Models (LLMs) have made remarkable progress in processing\nextensive contexts, with the Key-Value (KV) cache playing a vital role in\nenhancing their performance. However, the growth of the KV cache in response to\nincreasing input length poses challenges to memory and time efficiency. To\naddress this problem, this paper introduces SnapKV, an innovative and\nfine-tuning-free approach that efficiently minimizes KV cache size while still\ndelivering comparable performance in real-world applications.\n  We discover that each attention head in the model consistently focuses on\nspecific prompt attention features during generation. Meanwhile, this robust\npattern can be obtained from an 'observation' window located at the end of the\nprompts. Drawing on this insight, SnapKV automatically compresses KV caches by\nselecting clustered important KV positions for each attention head. Our\napproach significantly reduces the growing computational overhead and memory\nfootprint when processing long input sequences. Specifically, SnapKV achieves a\nconsistent decoding speed with a 3.6x increase in generation speed and an 8.2x\nenhancement in memory efficiency compared to the baseline when processing\ninputs of 16K tokens. At the same time, it maintains comparable performance to\nthe baseline models across 16 long sequence datasets. Moreover, SnapKV can\nprocess up to 380K context tokens on a single A100-80GB GPU using HuggingFace\nimplementation with minor changes, exhibiting only a negligible accuracy drop\nin the Needle-in-a-Haystack test. Further comprehensive studies suggest\nSnapKV's potential for practical applications.", "published": "2024-04-22 17:42:58", "link": "http://arxiv.org/abs/2404.14469v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive\n  Generation through Simultaneous Decoding", "abstract": "This paper introduces PAG-a novel optimization and decoding approach that\nguides autoregressive generation of document identifiers in generative\nretrieval models through simultaneous decoding. To this aim, PAG constructs a\nset-based and sequential identifier for each document. Motivated by the\nbag-of-words assumption in information retrieval, the set-based identifier is\nbuilt on lexical tokens. The sequential identifier, on the other hand, is\nobtained via quantizing relevance-based representations of documents. Extensive\nexperiments on MSMARCO and TREC Deep Learning Track data reveal that PAG\noutperforms the state-of-the-art generative retrieval model by a large margin\n(e.g., 15.6% MRR improvements on MS MARCO), while achieving 22x speed up in\nterms of query latency.", "published": "2024-04-22 21:50:01", "link": "http://arxiv.org/abs/2404.14600v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Do not think about pink elephant!", "abstract": "Large Models (LMs) have heightened expectations for the potential of general\nAI as they are akin to human intelligence. This paper shows that recent large\nmodels such as Stable Diffusion and DALL-E3 also share the vulnerability of\nhuman intelligence, namely the \"white bear phenomenon\". We investigate the\ncauses of the white bear phenomenon by analyzing their representation space.\nBased on this analysis, we propose a simple prompt-based attack method, which\ngenerates figures prohibited by the LM provider's policy. To counter these\nattacks, we introduce prompt-based defense strategies inspired by cognitive\ntherapy techniques, successfully mitigating attacks by up to 48.22\\%.", "published": "2024-04-22 08:28:13", "link": "http://arxiv.org/abs/2404.15154v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FASTTRACK: Fast and Accurate Fact Tracing for LLMs", "abstract": "Fact tracing seeks to identify specific training examples that serve as the\nknowledge source for a given query. Existing approaches to fact tracing rely on\nassessing the similarity between each training sample and the query along a\ncertain dimension, such as lexical similarity, gradient, or embedding space.\nHowever, these methods fall short of effectively distinguishing between samples\nthat are merely relevant and those that actually provide supportive evidence\nfor the information sought by the query. This limitation often results in\nsuboptimal effectiveness. Moreover, these approaches necessitate the\nexamination of the similarity of individual training points for each query,\nimposing significant computational demands and creating a substantial barrier\nfor practical applications. This paper introduces FASTTRACK, a novel approach\nthat harnesses the capabilities of Large Language Models (LLMs) to validate\nsupportive evidence for queries and at the same time clusters the training\ndatabase towards a reduced extent for LLMs to trace facts. Our experiments show\nthat FASTTRACK substantially outperforms existing methods in both accuracy and\nefficiency, achieving more than 100\\% improvement in F1 score over the\nstate-of-the-art methods while being X33 faster than \\texttt{TracIn}.", "published": "2024-04-22 00:07:55", "link": "http://arxiv.org/abs/2404.15157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based\n  Mixture of Experts", "abstract": "Fine-tuning Large Language Models (LLMs) is a common practice to adapt\npre-trained models for specific applications. While methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multi-task scenarios. In contrast,\nMixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable\nperformance in multi-task learning scenarios while maintaining a reduced\nparameter count. However, the resource requirements of these MoEs remain\nchallenging, particularly for consumer-grade GPUs with less than 24GB memory.\nTo tackle these challenges, we propose MixLoRA, an approach to construct a\nresource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple\nLoRA-based experts within the feed-forward network block of a frozen\npre-trained dense model and employs a commonly used top-k router. Unlike other\nLoRA-based MoE methods, MixLoRA enhances model performance by utilizing\nindependent attention-layer LoRA adapters. Additionally, an auxiliary load\nbalance loss is employed to address the imbalance problem of the router. Our\nevaluations show that MixLoRA improves about 9% accuracy compared to\nstate-of-the-art PEFT methods in multi-task learning scenarios. We also propose\na new high-throughput framework to alleviate the computation and memory\nbottlenecks during the training and inference of MOE models. This framework\nreduces GPU memory consumption by 40% and token computation latency by 30%\nduring both training and inference.", "published": "2024-04-22 02:15:52", "link": "http://arxiv.org/abs/2404.15159v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on the Real Power of ChatGPT", "abstract": "ChatGPT has changed the AI community and an active research line is the\nperformance evaluation of ChatGPT. A key challenge for the evaluation is that\nChatGPT is still closed-source and traditional benchmark datasets may have been\nused by ChatGPT as the training data. In this paper, (i) we survey recent\nstudies which uncover the real performance levels of ChatGPT in seven\ncategories of NLP tasks, (ii) review the social implications and safety issues\nof ChatGPT, and (iii) emphasize key challenges and opportunities for its\nevaluation. We hope our survey can shed some light on its blackbox manner, so\nthat researchers are not misleaded by its surface generation.", "published": "2024-04-22 23:31:28", "link": "http://arxiv.org/abs/2405.00704v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Filtered Direct Preference Optimization", "abstract": "Reinforcement learning from human feedback (RLHF) plays a crucial role in\naligning language models with human preferences. While the significance of\ndataset quality is generally recognized, explicit investigations into its\nimpact within the RLHF framework, to our knowledge, have been limited. This\npaper addresses the issue of text quality within the preference dataset by\nfocusing on direct preference optimization (DPO), an increasingly adopted\nreward-model-free RLHF method. We confirm that text quality significantly\ninfluences the performance of models optimized with DPO more than those\noptimized with reward-model-based RLHF. Building on this new insight, we\npropose an extension of DPO, termed filtered direct preference optimization\n(fDPO). fDPO uses a trained reward model to monitor the quality of texts within\nthe preference dataset during DPO training. Samples of lower quality are\ndiscarded based on comparisons with texts generated by the model being\noptimized, resulting in a more accurate dataset. Experimental results\ndemonstrate that fDPO enhances the final model performance. Our code is\navailable at https://github.com/CyberAgentAILab/filtered-dpo.", "published": "2024-04-22 03:05:19", "link": "http://arxiv.org/abs/2404.13846v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Surveying Attitudinal Alignment Between Large Language Models Vs. Humans\n  Towards 17 Sustainable Development Goals", "abstract": "Large Language Models (LLMs) have emerged as potent tools for advancing the\nUnited Nations' Sustainable Development Goals (SDGs). However, the attitudinal\ndisparities between LLMs and humans towards these goals can pose significant\nchallenges. This study conducts a comprehensive review and analysis of the\nexisting literature on the attitudes of LLMs towards the 17 SDGs, emphasizing\nthe comparison between their attitudes and support for each goal and those of\nhumans. We examine the potential disparities, primarily focusing on aspects\nsuch as understanding and emotions, cultural and regional differences, task\nobjective variations, and factors considered in the decision-making process.\nThese disparities arise from the underrepresentation and imbalance in LLM\ntraining data, historical biases, quality issues, lack of contextual\nunderstanding, and skewed ethical values reflected. The study also investigates\nthe risks and harms that may arise from neglecting the attitudes of LLMs\ntowards the SDGs, including the exacerbation of social inequalities, racial\ndiscrimination, environmental destruction, and resource wastage. To address\nthese challenges, we propose strategies and recommendations to guide and\nregulate the application of LLMs, ensuring their alignment with the principles\nand goals of the SDGs, and therefore creating a more just, inclusive, and\nsustainable future.", "published": "2024-04-22 05:12:52", "link": "http://arxiv.org/abs/2404.13885v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Towards Better Text-to-Image Generation Alignment via Attention\n  Modulation", "abstract": "In text-to-image generation tasks, the advancements of diffusion models have\nfacilitated the fidelity of generated results. However, these models encounter\nchallenges when processing text prompts containing multiple entities and\nattributes. The uneven distribution of attention results in the issues of\nentity leakage and attribute misalignment. Training from scratch to address\nthis issue requires numerous labeled data and is resource-consuming. Motivated\nby this, we propose an attribution-focusing mechanism, a training-free\nphase-wise mechanism by modulation of attention for diffusion model. One of our\ncore ideas is to guide the model to concentrate on the corresponding syntactic\ncomponents of the prompt at distinct timesteps. To achieve this, we incorporate\na temperature control mechanism within the early phases of the self-attention\nmodules to mitigate entity leakage issues. An object-focused masking scheme and\na phase-wise dynamic weight control mechanism are integrated into the\ncross-attention modules, enabling the model to discern the affiliation of\nsemantic information between entities more effectively. The experimental\nresults in various alignment scenarios demonstrate that our model attain better\nimage-text alignment with minimal additional computational cost.", "published": "2024-04-22 06:18:37", "link": "http://arxiv.org/abs/2404.13899v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Navigating the Path of Writing: Outline-guided Text Generation with\n  Large Language Models", "abstract": "Large Language Models (LLMs) have impacted the writing process, enhancing\nproductivity by collaborating with humans in content creation platforms.\nHowever, generating high-quality, user-aligned text to satisfy real-world\ncontent creation needs remains challenging. We propose WritingPath, a framework\nthat uses explicit outlines to guide LLMs in generating goal-oriented,\nhigh-quality text. Our approach draws inspiration from structured writing\nplanning and reasoning paths, focusing on reflecting user intentions throughout\nthe writing process. To validate our approach in real-world scenarios, we\nconstruct a diverse dataset from unstructured blog posts to benchmark writing\nperformance and introduce a comprehensive evaluation framework assessing the\nquality of outlines and generated texts. Our evaluations with various LLMs\ndemonstrate that the WritingPath approach significantly enhances text quality\naccording to evaluations by both LLMs and professional writers.", "published": "2024-04-22 06:57:43", "link": "http://arxiv.org/abs/2404.13919v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Protecting Your LLMs with Information Bottleneck", "abstract": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models.", "published": "2024-04-22 08:16:07", "link": "http://arxiv.org/abs/2404.13968v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Detecting and Mitigating Hallucination in Large Vision Language Models\n  via Fine-Grained AI Feedback", "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.", "published": "2024-04-22 14:46:10", "link": "http://arxiv.org/abs/2404.14233v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Graphic Design with Large Multimodal Model", "abstract": "In the field of graphic design, automating the integration of design elements\ninto a cohesive multi-layered artwork not only boosts productivity but also\npaves the way for the democratization of graphic design. One existing practice\nis Graphic Layout Generation (GLG), which aims to layout sequential design\nelements. It has been constrained by the necessity for a predefined correct\nsequence of layers, thus limiting creative potential and increasing user\nworkload. In this paper, we present Hierarchical Layout Generation (HLG) as a\nmore flexible and pragmatic setup, which creates graphic composition from\nunordered sets of design elements. To tackle the HLG task, we introduce\nGraphist, the first layout generation model based on large multimodal models.\nGraphist efficiently reframes the HLG as a sequence generation problem,\nutilizing RGB-A images as input, outputs a JSON draft protocol, indicating the\ncoordinates, size, and order of each element. We develop new evaluation metrics\nfor HLG. Graphist outperforms prior arts and establishes a strong baseline for\nthis field. Project homepage: https://github.com/graphic-design-ai/graphist", "published": "2024-04-22 17:20:38", "link": "http://arxiv.org/abs/2404.14368v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Multimodal Automated Interpretability Agent", "abstract": "This paper describes MAIA, a Multimodal Automated Interpretability Agent.\nMAIA is a system that uses neural models to automate neural model understanding\ntasks like feature interpretation and failure mode discovery. It equips a\npre-trained vision-language model with a set of tools that support iterative\nexperimentation on subcomponents of other models to explain their behavior.\nThese include tools commonly used by human interpretability researchers: for\nsynthesizing and editing inputs, computing maximally activating exemplars from\nreal-world datasets, and summarizing and describing experimental results.\nInterpretability experiments proposed by MAIA compose these tools to describe\nand explain system behavior. We evaluate applications of MAIA to computer\nvision models. We first characterize MAIA's ability to describe (neuron-level)\nfeatures in learned representations of images. Across several trained models\nand a novel dataset of synthetic vision neurons with paired ground-truth\ndescriptions, MAIA produces descriptions comparable to those generated by\nexpert human experimenters. We then show that MAIA can aid in two additional\ninterpretability tasks: reducing sensitivity to spurious features, and\nautomatically identifying inputs likely to be mis-classified.", "published": "2024-04-22 17:55:11", "link": "http://arxiv.org/abs/2404.14394v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?", "abstract": "In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf .", "published": "2024-04-22 17:55:56", "link": "http://arxiv.org/abs/2404.14395v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?", "abstract": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.", "published": "2024-04-22 17:56:26", "link": "http://arxiv.org/abs/2404.14397v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling", "abstract": "Tokenization is widely used in large language models because it significantly\nimproves performance. However, tokenization imposes several disadvantages, such\nas performance biases, increased adversarial vulnerability, decreased\ncharacter-level modeling performance, and increased modeling complexity. To\naddress these disadvantages without sacrificing performance, we propose\nSpaceByte, a novel byte-level decoder architecture that closes the performance\ngap between byte-level and subword autoregressive language modeling. SpaceByte\nconsists of a byte-level Transformer model, but with extra larger transformer\nblocks inserted in the middle of the layers. We find that performance is\nsignificantly improved by applying these larger blocks only after certain\nbytes, such as space characters, which typically denote word boundaries. Our\nexperiments show that for a fixed training and inference compute budget,\nSpaceByte outperforms other byte-level architectures and roughly matches the\nperformance of tokenized Transformer architectures.", "published": "2024-04-22 17:59:29", "link": "http://arxiv.org/abs/2404.14408v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Competition Report: Finding Universal Jailbreak Backdoors in Aligned\n  LLMs", "abstract": "Large language models are aligned to be safe, preventing users from\ngenerating harmful content like misinformation or instructions for illegal\nactivities. However, previous work has shown that the alignment process is\nvulnerable to poisoning attacks. Adversaries can manipulate the safety training\ndata to inject backdoors that act like a universal sudo command: adding the\nbackdoor string to any prompt enables harmful responses from models that,\notherwise, behave safely. Our competition, co-located at IEEE SaTML 2024,\nchallenged participants to find universal backdoors in several large language\nmodels. This report summarizes the key findings and promising ideas for future\nresearch.", "published": "2024-04-22 05:08:53", "link": "http://arxiv.org/abs/2404.14461v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAIC-WOZ: On the Validity of Using the Therapist's prompts in Automatic\n  Depression Detection from Clinical Interviews", "abstract": "Automatic depression detection from conversational data has gained\nsignificant interest in recent years. The DAIC-WOZ dataset, interviews\nconducted by a human-controlled virtual agent, has been widely used for this\ntask. Recent studies have reported enhanced performance when incorporating\ninterviewer's prompts into the model. In this work, we hypothesize that this\nimprovement might be mainly due to a bias present in these prompts, rather than\nthe proposed architectures and methods. Through ablation experiments and\nqualitative analysis, we discover that models using interviewer's prompts learn\nto focus on a specific region of the interviews, where questions about past\nexperiences with mental health issues are asked, and use them as discriminative\nshortcuts to detect depressed participants. In contrast, models using\nparticipant responses gather evidence from across the entire interview.\nFinally, to highlight the magnitude of this bias, we achieve a 0.90 F1 score by\nintentionally exploiting it, the highest result reported to date on this\ndataset using only textual information. Our findings underline the need for\ncaution when incorporating interviewers' prompts into models, as they may\ninadvertently learn to exploit targeted prompts, rather than learning to\ncharacterize the language and behavior that are genuinely indicative of the\npatient's mental health condition.", "published": "2024-04-22 09:07:50", "link": "http://arxiv.org/abs/2404.14463v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for\n  Multi-hop Question Answering", "abstract": "Multi-hop question answering is a knowledge-intensive complex problem. Large\nLanguage Models (LLMs) use their Chain of Thoughts (CoT) capability to reason\ncomplex problems step by step, and retrieval-augmentation can effectively\nalleviate factual errors caused by outdated and unknown knowledge in LLMs.\nRecent works have introduced retrieval-augmentation in the CoT reasoning to\nsolve multi-hop question answering. However, these chain methods have the\nfollowing problems: 1) Retrieved irrelevant paragraphs may mislead the\nreasoning; 2) An error in the chain structure may lead to a cascade of errors.\n  In this paper, we propose a dynamic retrieval framework called Tree of\nReviews (ToR), where the root node is the question, and the other nodes are\nparagraphs from retrieval, extending different reasoning paths from the root\nnode to other nodes. Our framework dynamically decides to initiate a new\nsearch, reject, or accept based on the paragraphs on the reasoning paths.\nCompared to related work, we introduce a tree structure to handle each\nretrieved paragraph separately, alleviating the misleading effect of irrelevant\nparagraphs on the reasoning path; the diversity of reasoning path extension\nreduces the impact of a single reasoning error on the whole. We conducted\nexperiments on three different multi-hop question answering datasets. The\nresults show that compared to the baseline methods, ToR achieves\nstate-of-the-art performance in both retrieval and response generation. In\naddition, we propose two tree-based search optimization strategies, pruning and\neffective expansion, to reduce time overhead and increase the diversity of path\nextension. We will release our code.", "published": "2024-04-22 09:25:05", "link": "http://arxiv.org/abs/2404.14464v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Benchmarking Advanced Text Anonymisation Methods: A Comparative Study on\n  Novel and Traditional Approaches", "abstract": "In the realm of data privacy, the ability to effectively anonymise text is\nparamount. With the proliferation of deep learning and, in particular,\ntransformer architectures, there is a burgeoning interest in leveraging these\nadvanced models for text anonymisation tasks. This paper presents a\ncomprehensive benchmarking study comparing the performance of transformer-based\nmodels and Large Language Models(LLM) against traditional architectures for\ntext anonymisation. Utilising the CoNLL-2003 dataset, known for its robustness\nand diversity, we evaluate several models. Our results showcase the strengths\nand weaknesses of each approach, offering a clear perspective on the efficacy\nof modern versus traditional methods. Notably, while modern models exhibit\nadvanced capabilities in capturing con textual nuances, certain traditional\narchitectures still keep high performance. This work aims to guide researchers\nin selecting the most suitable model for their anonymisation needs, while also\nshedding light on potential paths for future advancements in the field.", "published": "2024-04-22 12:06:54", "link": "http://arxiv.org/abs/2404.14465v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing", "abstract": "Large language models (LLMs) excel in most NLP tasks but also require\nexpensive cloud servers for deployment due to their size, while smaller models\nthat can be deployed on lower cost (e.g., edge) devices, tend to lag behind in\nterms of response quality. Therefore in this work we propose a hybrid inference\napproach which combines their respective strengths to save cost and maintain\nquality. Our approach uses a router that assigns queries to the small or large\nmodel based on the predicted query difficulty and the desired quality level.\nThe desired quality level can be tuned dynamically at test time to seamlessly\ntrade quality for cost as per the scenario requirements. In experiments our\napproach allows us to make up to 40% fewer calls to the large model, with no\ndrop in response quality.", "published": "2024-04-22 23:06:42", "link": "http://arxiv.org/abs/2404.14618v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OpenELM: An Efficient Language Model Family with Open Training and\n  Inference Framework", "abstract": "The reproducibility and transparency of large language models are crucial for\nadvancing open research, ensuring the trustworthiness of results, and enabling\ninvestigations into data and model biases, as well as potential risks. To this\nend, we release OpenELM, a state-of-the-art open language model. OpenELM uses a\nlayer-wise scaling strategy to efficiently allocate parameters within each\nlayer of the transformer model, leading to enhanced accuracy. For example, with\na parameter budget of approximately one billion parameters, OpenELM exhibits a\n2.36% improvement in accuracy compared to OLMo while requiring $2\\times$ fewer\npre-training tokens.\n  Diverging from prior practices that only provide model weights and inference\ncode, and pre-train on private datasets, our release includes the complete\nframework for training and evaluation of the language model on publicly\navailable datasets, including training logs, multiple checkpoints, and\npre-training configurations. We also release code to convert models to MLX\nlibrary for inference and fine-tuning on Apple devices. This comprehensive\nrelease aims to empower and strengthen the open research community, paving the\nway for future open research endeavors.\n  Our source code along with pre-trained model weights and training recipes is\navailable at \\url{https://github.com/apple/corenet}. Additionally, \\model\nmodels can be found on HuggingFace at:\n\\url{https://huggingface.co/apple/OpenELM}.", "published": "2024-04-22 23:12:03", "link": "http://arxiv.org/abs/2404.14619v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Performance Characterization of Expert Router for Scalable LLM Inference", "abstract": "Large Language Models (LLMs) have experienced widespread adoption across\nscientific and industrial domains due to their versatility and utility for\ndiverse tasks. Nevertheless, deploying and serving these models at scale with\noptimal throughput and latency remains a significant challenge, primarily\nbecause of LLMs' high computational and memory demands. Specialized models\noptimized for specific tasks can be combined through a routing mechanism to\naddress these challenges, creating a modular inference system. This paper\nintroduces Expert Router, a scalable routing architecture that directs prompts\nto specialized expert models. We characterize multiple Expert Router\nconfigurations, including different LLama 3 models with quantized and\nnon-quantized weights under up to 1,000 concurrent users. Our findings reveal\nthat Expert Router introduces minimal latency overhead, with the configuration\nof expert models being a dominating factor in performance outcomes.\nHigh-parameter expert models deliver stable throughput and latency under\nmoderate concurrency levels. In contrast, smaller expert models maintain\ncompetitive performance across a wider range of concurrent users compared to\ntensor-parallelized baseline models. This highlights the potential of Expert\nRouter for efficient and scalable LLM deployment.", "published": "2024-04-22 16:33:42", "link": "http://arxiv.org/abs/2404.15153v2", "categories": ["cs.CL", "cs.AI", "cs.PF"], "primary_category": "cs.CL"}
{"title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making", "abstract": "Foundation models are becoming valuable tools in medicine. Yet despite their\npromise, the best way to leverage Large Language Models (LLMs) in complex\nmedical tasks remains an open question. We introduce a novel multi-agent\nframework, named Medical Decision-making Agents (MDAgents) that helps address\nthis gap by automatically assigning a collaboration structure to a team of\nLLMs. The assigned solo or group collaboration structure is tailored to the\nmedical task at hand, emulating real-world medical decision-making processes\nadapted to tasks of varying complexities. We evaluate our framework and\nbaseline methods using state-of-the-art LLMs across a suite of real-world\nmedical knowledge and medical diagnosis benchmarks, including a comparison of\nLLMs' medical complexity classification against human physicians. MDAgents\nachieved the best performance in seven out of ten benchmarks on tasks requiring\nan understanding of medical knowledge and multi-modal reasoning, showing a\nsignificant improvement of up to 4.2% (p < 0.05) compared to previous methods'\nbest performances. Ablation studies reveal that MDAgents effectively determines\nmedical complexity to optimize for efficiency and accuracy across diverse\nmedical tasks. Notably, the combination of moderator review and external\nmedical knowledge in group collaboration resulted in an average accuracy\nimprovement of 11.8%. Our code can be found at\nhttps://github.com/mitmedialab/MDAgents.", "published": "2024-04-22 06:30:05", "link": "http://arxiv.org/abs/2404.15155v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pixels and Predictions: Potential of GPT-4V in Meteorological Imagery\n  Analysis and Forecast Communication", "abstract": "Generative AI, such as OpenAI's GPT-4V large-language model, has rapidly\nentered mainstream discourse. Novel capabilities in image processing and\nnatural-language communication may augment existing forecasting methods. Large\nlanguage models further display potential to better communicate weather hazards\nin a style honed for diverse communities and different languages. This study\nevaluates GPT-4V's ability to interpret meteorological charts and communicate\nweather hazards appropriately to the user, despite challenges of\nhallucinations, where generative AI delivers coherent, confident, but incorrect\nresponses. We assess GPT-4V's competence via its web interface ChatGPT in two\ntasks: (1) generating a severe-weather outlook from weather-chart analysis and\nconducting self-evaluation, revealing an outlook that corresponds well with a\nStorm Prediction Center human-issued forecast; and (2) producing hazard\nsummaries in Spanish and English from weather charts. Responses in Spanish,\nhowever, resemble direct (not idiomatic) translations from English to Spanish,\nyielding poorly translated summaries that lose critical idiomatic precision\nrequired for optimal communication. Our findings advocate for cautious\nintegration of tools like GPT-4V in meteorology, underscoring the necessity of\nhuman oversight and development of trustworthy, explainable AI.", "published": "2024-04-22 17:36:33", "link": "http://arxiv.org/abs/2404.15166v2", "categories": ["cs.CL", "cs.AI", "physics.ao-ph"], "primary_category": "cs.CL"}
{"title": "SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", "abstract": "While vertical thinking relies on logical and commonsense reasoning, lateral\nthinking requires systems to defy commonsense associations and overwrite them\nthrough unconventional thinking. Lateral thinking has been shown to be\nchallenging for current models but has received little attention. A recent\nbenchmark, BRAINTEASER, aims to evaluate current models' lateral thinking\nability in a zero-shot setting. In this paper, we split the original benchmark\nto also support fine-tuning setting and present SemEval Task 9:\nBRAIN-TEASER(S), the first task at this competition designed to test the\nsystem's reasoning and lateral thinking ability. As a popular task,\nBRAINTEASER(S)'s two subtasks receive 483 team submissions from 182\nparticipants during the competition. This paper provides a fine-grained system\nanalysis of the competition results, together with a reflection on what this\nmeans for the ability of the systems to reason laterally. We hope that the\nBRAINTEASER(S) subtasks and findings in this paper can stimulate future work on\nlateral thinking and robust reasoning by computational models.", "published": "2024-04-22 07:21:27", "link": "http://arxiv.org/abs/2404.16068v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Less Peaky and More Accurate CTC Forced Alignment by Label Priors", "abstract": "Connectionist temporal classification (CTC) models are known to have peaky\noutput distributions. Such behavior is not a problem for automatic speech\nrecognition (ASR), but it can cause inaccurate forced alignments (FA),\nespecially at finer granularity, e.g., phoneme level. This paper aims at\nalleviating the peaky behavior for CTC and improve its suitability for forced\nalignment generation, by leveraging label priors, so that the scores of\nalignment paths containing fewer blanks are boosted and maximized during\ntraining. As a result, our CTC model produces less peaky posteriors and is able\nto more accurately predict the offset of the tokens besides their onset. It\noutperforms the standard CTC model and a heuristics-based approach for\nobtaining CTC's token offset timestamps by 12-40% in phoneme and word boundary\nerrors (PBE and WBE) measured on the Buckeye and TIMIT data. Compared with the\nmost widely used FA toolkit Montreal Forced Aligner (MFA), our method performs\nsimilarly on PBE/WBE on Buckeye, yet falls behind MFA on TIMIT. Nevertheless,\nour method has a much simpler training pipeline and better runtime efficiency.\nOur training recipe and pretrained model are released in TorchAudio.", "published": "2024-04-22 17:40:08", "link": "http://arxiv.org/abs/2406.02560v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Exploring the Potential of Data-Driven Spatial Audio Enhancement Using a\n  Single-Channel Model", "abstract": "One key aspect differentiating data-driven single- and multi-channel speech\nenhancement and dereverberation methods is that both the problem formulation\nand complexity of the solutions are considerably more challenging in the latter\ncase. Additionally, with limited computational resources, it is cumbersome to\ntrain models that require the management of larger datasets or those with more\ncomplex designs. In this scenario, an unverified hypothesis that single-channel\nmethods can be adapted to multi-channel scenarios simply by processing each\nchannel independently holds significant implications, boosting compatibility\nbetween sound scene capture and system input-output formats, while also\nallowing modern research to focus on other challenging aspects, such as\nfull-bandwidth audio enhancement, competitive noise suppression, and\nunsupervised learning. This study verifies this hypothesis by comparing the\nenhancement promoted by a basic single-channel speech enhancement and\ndereverberation model with two other multi-channel models tailored to separate\nclean speech from noisy 3D mixes. A direction of arrival estimation model was\nused to objectively evaluate its capacity to preserve spatial information by\ncomparing the output signals with ground-truth coordinate values. Consequently,\na trade-off arises between preserving spatial information with a more\nstraightforward single-channel solution at the cost of obtaining lower gains in\nintelligibility scores.", "published": "2024-04-22 20:19:01", "link": "http://arxiv.org/abs/2404.14564v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robotic Blended Sonification: Consequential Robot Sound as Creative\n  Material for Human-Robot Interaction", "abstract": "Current research in robotic sounds generally focuses on either masking the\nconsequential sound produced by the robot or on sonifying data about the robot\nto create a synthetic robot sound. We propose to capture, modify, and utilise\nrather than mask the sounds that robots are already producing. In short, this\napproach relies on capturing a robot's sounds, processing them according to\ncontextual information (e.g., collaborators' proximity or particular work\nsequences), and playing back the modified sound. Previous research indicates\nthe usefulness of non-semantic, and even mechanical, sounds as a communication\ntool for conveying robotic affect and function. Adding to this, this paper\npresents a novel approach which makes two key contributions: (1) a technique\nfor real-time capture and processing of consequential robot sounds, and (2) an\napproach to explore these sounds through direct human-robot interaction.\nDrawing on methodologies from design, human-robot interaction, and creative\npractice, the resulting 'Robotic Blended Sonification' is a concept which\ntransforms the consequential robot sounds into a creative material that can be\nexplored artistically and within application-based studies.", "published": "2024-04-22 01:51:22", "link": "http://arxiv.org/abs/2404.13821v1", "categories": ["cs.HC", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Retrieval-Augmented Audio Deepfake Detection", "abstract": "With recent advances in speech synthesis including text-to-speech (TTS) and\nvoice conversion (VC) systems enabling the generation of ultra-realistic audio\ndeepfakes, there is growing concern about their potential misuse. However, most\ndeepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a\nsingle model, resulting in performance bottlenecks and transparency issues.\nInspired by retrieval-augmented generation (RAG), we propose a\nretrieval-augmented detection (RAD) framework that augments test samples with\nsimilar retrieved samples for enhanced detection. We also extend the\nmulti-fusion attentive classifier to integrate it with our proposed RAD\nframework. Extensive experiments show the superior performance of the proposed\nRAD framework over baseline methods, achieving state-of-the-art results on the\nASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\nFurther sample analysis indicates that the retriever consistently retrieves\nsamples mostly from the same speaker with acoustic characteristics highly\nconsistent with the query audio, thereby improving detection performance.", "published": "2024-04-22 05:46:40", "link": "http://arxiv.org/abs/2404.13892v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Anti-Spoofing Detection: A Survey", "abstract": "The availability of smart devices leads to an exponential increase in\nmultimedia content. However, the rapid advancements in deep learning have given\nrise to sophisticated algorithms capable of manipulating or creating multimedia\nfake content, known as Deepfake. Audio Deepfakes pose a significant threat by\nproducing highly realistic voices, thus facilitating the spread of\nmisinformation. To address this issue, numerous audio anti-spoofing detection\nchallenges have been organized to foster the development of anti-spoofing\ncountermeasures. This survey paper presents a comprehensive review of every\ncomponent within the detection pipeline, including algorithm architectures,\noptimization techniques, application generalizability, evaluation metrics,\nperformance comparisons, available datasets, and open-source availability. For\neach aspect, we conduct a systematic evaluation of the recent advancements,\nalong with discussions on existing challenges. Additionally, we also explore\nemerging research topics on audio anti-spoofing, including partial spoofing\ndetection, cross-dataset evaluation, and adversarial attack defence, while\nproposing some promising research directions for future work. This survey paper\nnot only identifies the current state-of-the-art to establish strong baselines\nfor future experiments but also guides future researchers on a clear path for\nunderstanding and enhancing the audio anti-spoofing detection mechanisms.", "published": "2024-04-22 06:52:12", "link": "http://arxiv.org/abs/2404.13914v1", "categories": ["cs.SD", "cs.CR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LVNS-RAVE: Diversified audio generation with RAVE and Latent Vector\n  Novelty Search", "abstract": "Evolutionary Algorithms and Generative Deep Learning have been two of the\nmost powerful tools for sound generation tasks. However, they have limitations:\nEvolutionary Algorithms require complicated designs, posing challenges in\ncontrol and achieving realistic sound generation. Generative Deep Learning\nmodels often copy from the dataset and lack creativity. In this paper, we\npropose LVNS-RAVE, a method to combine Evolutionary Algorithms and Generative\nDeep Learning to produce realistic and novel sounds. We use the RAVE model as\nthe sound generator and the VGGish model as a novelty evaluator in the Latent\nVector Novelty Search (LVNS) algorithm. The reported experiments show that the\nmethod can successfully generate diversified, novel audio samples under\ndifferent mutation setups using different pre-trained RAVE models. The\ncharacteristics of the generation process can be easily controlled with the\nmutation parameters. The proposed algorithm can be a creative tool for sound\nartists and musicians.", "published": "2024-04-22 10:20:41", "link": "http://arxiv.org/abs/2404.14063v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
