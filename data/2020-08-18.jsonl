{"title": "NASE: Learning Knowledge Graph Embedding for Link Prediction via Neural\n  Architecture Search", "abstract": "Link prediction is the task of predicting missing connections between\nentities in the knowledge graph (KG). While various forms of models are\nproposed for the link prediction task, most of them are designed based on a few\nknown relation patterns in several well-known datasets. Due to the diversity\nand complexity nature of the real-world KGs, it is inherently difficult to\ndesign a model that fits all datasets well. To address this issue, previous\nwork has tried to use Automated Machine Learning (AutoML) to search for the\nbest model for a given dataset. However, their search space is limited only to\nbilinear model families. In this paper, we propose a novel Neural Architecture\nSearch (NAS) framework for the link prediction task. First, the embeddings of\nthe input triplet are refined by the Representation Search Module. Then, the\nprediction score is searched within the Score Function Search Module. This\nframework entails a more general search space, which enables us to take\nadvantage of several mainstream model families, and thus it can potentially\nachieve better performance. We relax the search space to be continuous so that\nthe architecture can be optimized efficiently using gradient-based search\nstrategies. Experimental results on several benchmark datasets demonstrate the\neffectiveness of our method compared with several state-of-the-art approaches.", "published": "2020-08-18 03:34:09", "link": "http://arxiv.org/abs/2008.07723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Very Deep Transformers for Neural Machine Translation", "abstract": "We explore the application of very deep Transformer models for Neural Machine\nTranslation (NMT). Using a simple yet effective initialization technique that\nstabilizes training, we show that it is feasible to build standard\nTransformer-based models with up to 60 encoder layers and 12 decoder layers.\nThese deep models outperform their baseline 6-layer counterparts by as much as\n2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14\nEnglish-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14\nEnglish-German (30.1 BLEU).The code and trained models will be publicly\navailable at: https://github.com/namisan/exdeep-nmt.", "published": "2020-08-18 07:14:54", "link": "http://arxiv.org/abs/2008.07772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation", "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at\nimproving the efficiency by parallel decoding without sacrificing the quality.\nHowever, existing NAT methods are either inferior to Transformer or require\nmultiple decoding passes, leading to reduced speedup. We propose the Glancing\nLanguage Model (GLM), a method to learn word interdependency for single-pass\nparallel generation models. With GLM, we develop Glancing Transformer (GLAT)\nfor machine translation. With only single-pass parallel decoding, GLAT is able\nto generate high-quality translation with 8-15 times speedup. Experiments on\nmultiple WMT language directions show that GLAT outperforms all previous single\npass non-autoregressive methods, and is nearly comparable to Transformer,\nreducing the gap to 0.25-0.9 BLEU points.", "published": "2020-08-18 13:04:03", "link": "http://arxiv.org/abs/2008.07905v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research", "abstract": "We present COVID-SEE, a system for medical literature discovery based on the\nconcept of information exploration, which builds on several distinct text\nanalysis and natural language processing methods to structure and organise\ninformation in publications, and augments search by providing a visual overview\nsupporting exploration of a collection to identify key articles of interest. We\ndeveloped this system over COVID-19 literature to help medical professionals\nand researchers explore the literature evidence, and improve findability of\nrelevant information. COVID-SEE is available at http://covid-see.com.", "published": "2020-08-18 12:14:36", "link": "http://arxiv.org/abs/2008.07880v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Deploying Lifelong Open-Domain Dialogue Learning", "abstract": "Much of NLP research has focused on crowdsourced static datasets and the\nsupervised learning paradigm of training once and then evaluating test\nperformance. As argued in de Vries et al. (2020), crowdsourced data has the\nissues of lack of naturalness and relevance to real-world use cases, while the\nstatic dataset paradigm does not allow for a model to learn from its\nexperiences of using language (Silver et al., 2013). In contrast, one might\nhope for machine learning systems that become more useful as they interact with\npeople. In this work, we build and deploy a role-playing game, whereby human\nplayers converse with learning agents situated in an open-domain fantasy world.\nWe show that by training models on the conversations they have with humans in\nthe game the models progressively improve, as measured by automatic metrics and\nonline engagement scores. This learning is shown to be more efficient than\ncrowdsourced data when applied to conversations with real users, as well as\nbeing far cheaper to collect.", "published": "2020-08-18 17:57:26", "link": "http://arxiv.org/abs/2008.08076v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Are Neural Open-Domain Dialog Systems Robust to Speech Recognition\n  Errors in the Dialog History? An Empirical Study", "abstract": "Large end-to-end neural open-domain chatbots are becoming increasingly\npopular. However, research on building such chatbots has typically assumed that\nthe user input is written in nature and it is not clear whether these chatbots\nwould seamlessly integrate with automatic speech recognition (ASR) models to\nserve the speech modality. We aim to bring attention to this important question\nby empirically studying the effects of various types of synthetic and actual\nASR hypotheses in the dialog history on TransferTransfo, a state-of-the-art\nGenerative Pre-trained Transformer (GPT) based neural open-domain dialog system\nfrom the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on\nwritten data is very sensitive to such hypotheses introduced to the dialog\nhistory during inference time. As a baseline mitigation strategy, we introduce\nsynthetic ASR hypotheses to the dialog history during training and observe\nmarginal improvements, demonstrating the need for further research into\ntechniques to make end-to-end open-domain chatbots fully speech-robust. To the\nbest of our knowledge, this is the first study to evaluate the effects of\nsynthetic and actual ASR hypotheses on a state-of-the-art neural open-domain\ndialog system and we hope it promotes speech-robustness as an evaluation\ncriterion in open-domain dialog.", "published": "2020-08-18 00:36:57", "link": "http://arxiv.org/abs/2008.07683v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Word2vec Skip-gram Dimensionality Selection via Sequential Normalized\n  Maximum Likelihood", "abstract": "In this paper, we propose a novel information criteria-based approach to\nselect the dimensionality of the word2vec Skip-gram (SG). From the perspective\nof the probability theory, SG is considered as an implicit probability\ndistribution estimation under the assumption that there exists a true\ncontextual distribution among words. Therefore, we apply information criteria\nwith the aim of selecting the best dimensionality so that the corresponding\nmodel can be as close as possible to the true distribution. We examine the\nfollowing information criteria for the dimensionality selection problem: the\nAkaike Information Criterion, Bayesian Information Criterion, and Sequential\nNormalized Maximum Likelihood (SNML) criterion. SNML is the total codelength\nrequired for the sequential encoding of a data sequence on the basis of the\nminimum description length. The proposed approach is applied to both the\noriginal SG model and the SG Negative Sampling model to clarify the idea of\nusing information criteria. Additionally, as the original SNML suffers from\ncomputational disadvantages, we introduce novel heuristics for its efficient\ncomputation. Moreover, we empirically demonstrate that SNML outperforms both\nBIC and AIC. In comparison with other evaluation methods for word embedding,\nthe dimensionality selected by SNML is significantly closer to the optimal\ndimensionality obtained by word analogy or word similarity tasks.", "published": "2020-08-18 03:24:21", "link": "http://arxiv.org/abs/2008.07720v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FANG: Leveraging Social Context for Fake News Detection Using Graph\n  Representation", "abstract": "We propose Factual News Graph (FANG), a novel graphical social context\nrepresentation and learning framework for fake news detection. Unlike previous\ncontextual models that have targeted performance, our focus is on\nrepresentation learning. Compared to transductive models, FANG is scalable in\ntraining as it does not have to maintain all nodes, and it is efficient at\ninference time, without the need to re-process the entire graph. Our\nexperimental results show that FANG is better at capturing the social context\ninto a high fidelity representation, compared to recent graphical and\nnon-graphical models. In particular, FANG yields significant improvements for\nthe task of fake news detection, and it is robust in the case of limited\ntraining data. We further demonstrate that the representations learned by FANG\ngeneralize to related tasks, such as predicting the factuality of reporting of\na news medium.", "published": "2020-08-18 14:05:16", "link": "http://arxiv.org/abs/2008.07939v2", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Relational Reflection Entity Alignment", "abstract": "Entity alignment aims to identify equivalent entity pairs from different\nKnowledge Graphs (KGs), which is essential in integrating multi-source KGs.\nRecently, with the introduction of GNNs into entity alignment, the\narchitectures of recent models have become more and more complicated. We even\nfind two counter-intuitive phenomena within these methods: (1) The standard\nlinear transformation in GNNs is not working well. (2) Many advanced KG\nembedding models designed for link prediction task perform poorly in entity\nalignment. In this paper, we abstract existing entity alignment methods into a\nunified framework, Shape-Builder & Alignment, which not only successfully\nexplains the above phenomena but also derives two key criteria for an ideal\ntransformation operation. Furthermore, we propose a novel GNNs-based method,\nRelational Reflection Entity Alignment (RREA). RREA leverages Relational\nReflection Transformation to obtain relation specific embeddings for each\nentity in a more efficient way. The experimental results on real-world datasets\nshow that our model significantly outperforms the state-of-the-art methods,\nexceeding by 5.8%-10.9% on Hits@1.", "published": "2020-08-18 14:49:31", "link": "http://arxiv.org/abs/2008.07962v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Complementary Language Model and Parallel Bi-LRNN for False Trigger\n  Mitigation", "abstract": "False triggers in voice assistants are unintended invocations of the\nassistant, which not only degrade the user experience but may also compromise\nprivacy. False trigger mitigation (FTM) is a process to detect the false\ntrigger events and respond appropriately to the user. In this paper, we propose\na novel solution to the FTM problem by introducing a parallel ASR decoding\nprocess with a special language model trained from \"out-of-domain\" data\nsources. Such language model is complementary to the existing language model\noptimized for the assistant task. A bidirectional lattice RNN (Bi-LRNN)\nclassifier trained from the lattices generated by the complementary language\nmodel shows a $38.34\\%$ relative reduction of the false trigger (FT) rate at\nthe fixed rate of $0.4\\%$ false suppression (FS) of correct invocations,\ncompared to the current Bi-LRNN model. In addition, we propose to train a\nparallel Bi-LRNN model based on the decoding lattices from both language\nmodels, and examine various ways of implementation. The resulting model leads\nto further reduction in the false trigger rate by $10.8\\%$.", "published": "2020-08-18 18:21:33", "link": "http://arxiv.org/abs/2008.08113v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PopMAG: Pop Music Accompaniment Generation", "abstract": "In pop music, accompaniments are usually played by multiple instruments\n(tracks) such as drum, bass, string and guitar, and can make a song more\nexpressive and contagious by arranging together with its melody. Previous works\nusually generate multiple tracks separately and the music notes from different\ntracks not explicitly depend on each other, which hurts the harmony modeling.\nTo improve harmony, in this paper, we propose a novel MUlti-track MIDI\nrepresentation (MuMIDI), which enables simultaneous multi-track generation in a\nsingle sequence and explicitly models the dependency of the notes from\ndifferent tracks. While this greatly improves harmony, unfortunately, it\nenlarges the sequence length and brings the new challenge of long-term music\nmodeling. We further introduce two new techniques to address this challenge: 1)\nWe model multiple note attributes (e.g., pitch, duration, velocity) of a\nmusical note in one step instead of multiple steps, which can shorten the\nlength of a MuMIDI sequence. 2) We introduce extra long-context as memory to\ncapture long-term dependency in music. We call our system for pop music\naccompaniment generation as PopMAG. We evaluate PopMAG on multiple datasets\n(LMD, FreeMidi and CPMD, a private dataset of Chinese pop songs) with both\nsubjective and objective metrics. The results demonstrate the effectiveness of\nPopMAG for multi-track harmony modeling and long-term context modeling.\nSpecifically, PopMAG wins 42\\%/38\\%/40\\% votes when comparing with ground truth\nmusical pieces on LMD, FreeMidi and CPMD datasets respectively and largely\noutperforms other state-of-the-art music accompaniment generation models and\nmulti-track MIDI representations in terms of subjective and objective metrics.", "published": "2020-08-18 02:28:36", "link": "http://arxiv.org/abs/2008.07703v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network\n  for End-to-end Speech Enhancement", "abstract": "In this paper, in order to further deal with the performance degradation\ncaused by ignoring the phase information in conventional speech enhancement\nsystems, we proposed a temporal dilated convolutional generative adversarial\nnetwork (TDCGAN) in the end-to-end based speech enhancement architecture. For\nthe first time, we introduced the temporal dilated convolutional network with\ndepthwise separable convolutions into the GAN structure so that the receptive\nfield can be greatly increased without increasing the number of parameters. We\nalso first explored the effect of signal-to-noise ratio (SNR) penalty item as\nregularization of the loss function of generator on improving the SNR of\nenhanced speech. The experimental results demonstrated that our proposed method\noutperformed the state-of-the-art end-to-end GAN-based speech enhancement.\nMoreover, compared with previous GAN-based methods, the proposed TDCGAN could\ngreatly decreased the number of parameters. As expected, the work also\ndemonstrated that the SNR penalty item as regularization was more effective\nthan $L1$ on improving the SNR of enhanced speech.", "published": "2020-08-18 07:50:17", "link": "http://arxiv.org/abs/2008.07787v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech\n  Conversion", "abstract": "Recently, Generative Adversarial Networks (GAN)-based methods have shown\nremarkable performance for the Voice Conversion and WHiSPer-to-normal SPeeCH\n(WHSP2SPCH) conversion. One of the key challenges in WHSP2SPCH conversion is\nthe prediction of fundamental frequency (F0). Recently, authors have proposed\nstate-of-the-art method Cycle-Consistent Generative Adversarial Networks\n(CycleGAN) for WHSP2SPCH conversion. The CycleGAN-based method uses two\ndifferent models, one for Mel Cepstral Coefficients (MCC) mapping, and another\nfor F0 prediction, where F0 is highly dependent on the pre-trained model of MCC\nmapping. This leads to additional non-linear noise in predicted F0. To suppress\nthis noise, we propose Cycle-in-Cycle GAN (i.e., CinC-GAN). It is specially\ndesigned to increase the effectiveness in F0 prediction without losing the\naccuracy of MCC mapping. We evaluated the proposed method on a non-parallel\nsetting and analyzed on speaker-specific, and gender-specific tasks. The\nobjective and subjective tests show that CinC-GAN significantly outperforms the\nCycleGAN. In addition, we analyze the CycleGAN and CinC-GAN for unseen speakers\nand the results show the clear superiority of CinC-GAN.", "published": "2020-08-18 07:56:16", "link": "http://arxiv.org/abs/2008.07788v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Adversarial Attack and Defense Strategies for Deep Speaker Recognition\n  Systems", "abstract": "Robust speaker recognition, including in the presence of malicious attacks,\nis becoming increasingly important and essential, especially due to the\nproliferation of several smart speakers and personal agents that interact with\nan individual's voice commands to perform diverse, and even sensitive tasks.\nAdversarial attack is a recently revived domain which is shown to be effective\nin breaking deep neural network-based classifiers, specifically, by forcing\nthem to change their posterior distribution by only perturbing the input\nsamples by a very small amount. Although, significant progress in this realm\nhas been made in the computer vision domain, advances within speaker\nrecognition is still limited. The present expository paper considers several\nstate-of-the-art adversarial attacks to a deep speaker recognition system,\nemploying strong defense methods as countermeasures, and reporting on several\nablation studies to obtain a comprehensive understanding of the problem. The\nexperiments show that the speaker recognition systems are vulnerable to\nadversarial attacks, and the strongest attacks can reduce the accuracy of the\nsystem from 94% to even 0%. The study also compares the performances of the\nemployed defense methods in detail, and finds adversarial training based on\nProjected Gradient Descent (PGD) to be the best defense method in our setting.\nWe hope that the experiments presented in this paper provide baselines that can\nbe useful for the research community interested in further studying adversarial\nrobustness of speaker recognition systems.", "published": "2020-08-18 00:58:19", "link": "http://arxiv.org/abs/2008.07685v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19\n  Infection", "abstract": "In this paper, we propose a real-time robot-based auxiliary system for risk\nevaluation of COVID-19 infection. It combines real-time speech recognition,\ntemperature measurement, keyword detection, cough detection and other functions\nin order to convert live audio into actionable structured data to achieve the\nCOVID-19 infection risk assessment function. In order to better evaluate the\nCOVID-19 infection, we propose an end-to-end method for cough detection and\nclassification for our proposed system. It is based on real conversation data\nfrom human-robot, which processes speech signals to detect cough and classifies\nit if detected. The structure of our model are maintained concise to be\nimplemented for real-time applications. And we further embed this entire\nauxiliary diagnostic system in the robot and it is placed in the communities,\nhospitals and supermarkets to support COVID-19 testing. The system can be\nfurther leveraged within a business rules engine, thus serving as a foundation\nfor real-time supervision and assistance applications. Our model utilizes a\npretrained, robust training environment that allows for efficient creation and\ncustomization of customer-specific health states.", "published": "2020-08-18 01:58:52", "link": "http://arxiv.org/abs/2008.07695v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generating Music with a Self-Correcting Non-Chronological Autoregressive\n  Model", "abstract": "We describe a novel approach for generating music using a self-correcting,\nnon-chronological, autoregressive model. We represent music as a sequence of\nedit events, each of which denotes either the addition or removal of a\nnote---even a note previously generated by the model. During inference, we\ngenerate one edit event at a time using direct ancestral sampling. Our approach\nallows the model to fix previous mistakes such as incorrectly sampled notes and\nprevent accumulation of errors which autoregressive models are prone to have.\nAnother benefit is a finer, note-by-note control during human and AI\ncollaborative composition. We show through quantitative metrics and human\nsurvey evaluation that our approach generates better results than orderless\nNADE and Gibbs sampling approaches.", "published": "2020-08-18 20:36:47", "link": "http://arxiv.org/abs/2008.08927v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
