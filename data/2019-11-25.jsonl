{"title": "Who did They Respond to? Conversation Structure Modeling using Masked\n  Hierarchical Transformer", "abstract": "Conversation structure is useful for both understanding the nature of\nconversation dynamics and for providing features for many downstream\napplications such as summarization of conversations. In this work, we define\nthe problem of conversation structure modeling as identifying the parent\nutterance(s) to which each utterance in the conversation responds to. Previous\nwork usually took a pair of utterances to decide whether one utterance is the\nparent of the other. We believe the entire ancestral history is a very\nimportant information source to make accurate prediction. Therefore, we design\na novel masking mechanism to guide the ancestor flow, and leverage the\ntransformer model to aggregate all ancestors to predict parent utterances. Our\nexperiments are performed on the Reddit dataset (Zhang, Culbertson, and\nParitosh 2017) and the Ubuntu IRC dataset (Kummerfeld et al. 2019). In\naddition, we also report experiments on a new larger corpus from the Reddit\nplatform and release this dataset. We show that the proposed model, that takes\ninto account the ancestral history of the conversation, significantly\noutperforms several strong baselines including the BERT model on all datasets", "published": "2019-11-25 02:12:45", "link": "http://arxiv.org/abs/1911.10666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus", "abstract": "Recent machine translation algorithms mainly rely on parallel corpora.\nHowever, since the availability of parallel corpora remains limited, only some\nresource-rich language pairs can benefit from them. We constructed a parallel\ncorpus for English-Japanese, for which the amount of publicly available\nparallel corpora is still limited. We constructed the parallel corpus by\nbroadly crawling the web and automatically aligning parallel sentences. Our\ncollected corpus, called JParaCrawl, amassed over 8.7 million sentence pairs.\nWe show how it includes a broader range of domains and how a neural machine\ntranslation model trained with it works as a good pre-trained model for\nfine-tuning specific domains. The pre-training and fine-tuning approaches\nachieved or surpassed performance comparable to model training from the initial\nstate and reduced the training time. Additionally, we trained the model with an\nin-domain dataset and JParaCrawl to show how we achieved the best performance\nwith them. JParaCrawl and the pre-trained models are freely available online\nfor research purposes.", "published": "2019-11-25 02:14:51", "link": "http://arxiv.org/abs/1911.10668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational implicatures in English dialogue: Annotated dataset", "abstract": "Human dialogue often contains utterances having meanings entirely different\nfrom the sentences used and are clearly understood by the interlocutors. But in\nhuman-computer interactions, the machine fails to understand the implicated\nmeaning unless it is trained with a dataset containing the implicated meaning\nof an utterance along with the utterance and the context in which it is\nuttered. In linguistic terms, conversational implicatures are the meanings of\nthe speaker's utterance that are not part of what is explicitly said. In this\npaper, we introduce a dataset of dialogue snippets with three constituents,\nwhich are the context, the utterance, and the implicated meanings. These\nimplicated meanings are the conversational implicatures. The utterances are\ncollected by transcribing from listening comprehension sections of English\ntests like TOEFL (Test of English as a Foreign Language) as well as scraping\ndialogues from movie scripts available on IMSDb (Internet Movie Script\nDatabase). The utterances are manually annotated with implicatures.", "published": "2019-11-25 04:57:08", "link": "http://arxiv.org/abs/1911.10704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Reuse Translations: Guiding Neural Machine Translation with\n  Examples", "abstract": "In this paper, we study the problem of enabling neural machine translation\n(NMT) to reuse previous translations from similar examples in target\nprediction. Distinguishing reusable translations from noisy segments and\nlearning to reuse them in NMT are non-trivial. To solve these challenges, we\npropose an Example-Guided NMT (EGNMT) framework with two models: (1) a\nnoise-masked encoder model that masks out noisy words according to word\nalignments and encodes the noise-masked sentences with an additional example\nencoder and (2) an auxiliary decoder model that predicts reusable words via an\nauxiliary decoder sharing parameters with the primary decoder. We define and\nimplement the two models with the state-of-the-art Transformer. Experiments\nshow that the noise-masked encoder model allows NMT to learn useful information\nfrom examples with low fuzzy match scores (FMS) while the auxiliary decoder\nmodel is good for high-FMS examples. More experiments on Chinese-English,\nEnglish-German and English-Spanish translation demonstrate that the combination\nof the two EGNMT models can achieve improvements of up to +9 BLEU points over\nthe baseline system and +7 BLEU points over a two-encoder Transformer.", "published": "2019-11-25 07:22:47", "link": "http://arxiv.org/abs/1911.10732v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Spelling Error Detection Using a Fusion Lattice LSTM", "abstract": "Spelling error detection serves as a crucial preprocessing in many natural\nlanguage processing applications. Due to the characteristics of Chinese\nLanguage, Chinese spelling error detection is more challenging than error\ndetection in English. Existing methods are mainly under a pipeline framework,\nwhich artificially divides error detection process into two steps. Thus, these\nmethods bring error propagation and cannot always work well due to the\ncomplexity of the language environment. Besides existing methods only adopt\ncharacter or word information, and ignore the positive effect of fusing\ncharacter, word, pinyin1 information together. We propose an LF-LSTM-CRF model,\nwhich is an extension of the LSTMCRF with word lattices and\ncharacter-pinyin-fusion inputs. Our model takes advantage of the end-to-end\nframework to detect errors as a whole process, and dynamically integrates\ncharacter, word and pinyin information. Experiments on the SIGHAN data show\nthat our LF-LSTM-CRF outperforms existing methods with similar external\nresources consistently, and confirm the feasibility of adopting the end-to-end\nframework and the availability of integrating of character, word and pinyin\ninformation.", "published": "2019-11-25 07:58:00", "link": "http://arxiv.org/abs/1911.10750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation of Language Models for Reading\n  Comprehension", "abstract": "This study tackles unsupervised domain adaptation of reading comprehension\n(UDARC). Reading comprehension (RC) is a task to learn the capability for\nquestion answering with textual sources. State-of-the-art models on RC still do\nnot have general linguistic intelligence; i.e., their accuracy worsens for\nout-domain datasets that are not used in the training. We hypothesize that this\ndiscrepancy is caused by a lack of the language modeling (LM) capability for\nthe out-domain. The UDARC task allows models to use supervised RC training data\nin the source domain and only unlabeled passages in the target domain. To solve\nthe UDARC problem, we provide two domain adaptation models. The first one\nlearns the out-domain LM and in-domain RC task sequentially. The second one is\nthe proposed model that uses a multi-task learning approach of LM and RC. The\nmodels can retain both the RC capability acquired from the supervised data in\nthe source domain and the LM capability from the unlabeled data in the target\ndomain. We evaluated the models on UDARC with five datasets in different\ndomains. The models outperformed the model without domain adaptation. In\nparticular, the proposed model yielded an improvement of 4.3/4.2 points in\nEM/F1 in an unseen biomedical domain.", "published": "2019-11-25 08:40:34", "link": "http://arxiv.org/abs/1911.10768v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Financial Event Extraction Using Wikipedia-Based Weak Supervision", "abstract": "Extraction of financial and economic events from text has previously been\ndone mostly using rule-based methods, with more recent works employing machine\nlearning techniques. This work is in line with this latter approach, leveraging\nrelevant Wikipedia sections to extract weak labels for sentences describing\neconomic events. Whereas previous weakly supervised approaches required a\nknowledge-base of such events, or corresponding financial figures, our approach\nrequires no such additional data, and can be employed to extract economic\nevents related to companies which are not even mentioned in the training data.", "published": "2019-11-25 09:35:02", "link": "http://arxiv.org/abs/1911.10783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Outbound Translation User Interface Ptakopet: A Pilot Study", "abstract": "It is not uncommon for Internet users to have to produce a text in a foreign\nlanguage they have very little knowledge of and are unable to verify the\ntranslation quality. We call the task \"outbound translation\" and explore it by\nintroducing an open-source modular system Ptakop\\v{e}t. Its main purpose is to\ninspect human interaction with MT systems enhanced with additional subsystems,\nsuch as backward translation and quality estimation. We follow up with an\nexperiment on (Czech) human annotators tasked to produce questions in a\nlanguage they do not speak (German), with the help of Ptakop\\v{e}t. We focus on\nthree real-world use cases (communication with IT support, describing\nadministrative issues and asking encyclopedic questions) from which we gain\ninsight into different strategies users take when faced with outbound\ntranslation tasks. Round trip translation is known to be unreliable for\nevaluating MT systems but our experimental evaluation documents that it works\nvery well for users, at least on MT systems of mid-range quality.", "published": "2019-11-25 11:22:45", "link": "http://arxiv.org/abs/1911.10835v2", "categories": ["cs.CL", "I.2.7, H.5.2", "I.2.7; H.5.2"], "primary_category": "cs.CL"}
{"title": "Towards robust word embeddings for noisy texts", "abstract": "Research on word embeddings has mainly focused on improving their performance\non standard corpora, disregarding the difficulties posed by noisy texts in the\nform of tweets and other types of non-standard writing from social media. In\nthis work, we propose a simple extension to the skipgram model in which we\nintroduce the concept of bridge-words, which are artificial words added to the\nmodel to strengthen the similarity between standard words and their noisy\nvariants. Our new embeddings outperform baseline models on noisy texts on a\nwide range of evaluation tasks, both intrinsic and extrinsic, while retaining a\ngood performance on standard texts. To the best of our knowledge, this is the\nfirst explicit approach at dealing with this type of noisy texts at the word\nembedding level that goes beyond the support for out-of-vocabulary words.", "published": "2019-11-25 12:48:27", "link": "http://arxiv.org/abs/1911.10876v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Korean-to-Chinese Machine Translation using Chinese Character as Pivot\n  Clue", "abstract": "Korean-Chinese is a low resource language pair, but Korean and Chinese have a\nlot in common in terms of vocabulary. Sino-Korean words, which can be converted\ninto corresponding Chinese characters, account for more than fifty of the\nentire Korean vocabulary. Motivated by this, we propose a simple linguistically\nmotivated solution to improve the performance of the Korean-to-Chinese neural\nmachine translation model by using their common vocabulary. We adopt Chinese\ncharacters as a translation pivot by converting Sino-Korean words in Korean\nsentences to Chinese characters and then train the machine translation model\nwith the converted Korean sentences as source sentences. The experimental\nresults on Korean-to-Chinese translation demonstrate that the models with the\nproposed method improve translation quality up to 1.5 BLEU points in comparison\nto the baseline models.", "published": "2019-11-25 15:55:56", "link": "http://arxiv.org/abs/1911.11008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotional Neural Language Generation Grounded in Situational Contexts", "abstract": "Emotional language generation is one of the keys to human-like artificial\nintelligence. Humans use different type of emotions depending on the situation\nof the conversation. Emotions also play an important role in mediating the\nengagement level with conversational partners. However, current conversational\nagents do not effectively account for emotional content in the language\ngeneration process. To address this problem, we develop a language modeling\napproach that generates affective content when the dialogue is situated in a\ngiven context. We use the recently released Empathetic-Dialogues corpus to\nbuild our models. Through detailed experiments, we find that our approach\noutperforms the state-of-the-art method on the perplexity metric by about 5\npoints and achieves a higher BLEU metric score.", "published": "2019-11-25 19:01:36", "link": "http://arxiv.org/abs/1911.11161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Importance-Aware Learning for Neural Headline Editing", "abstract": "Many social media news writers are not professionally trained. Therefore,\nsocial media platforms have to hire professional editors to adjust amateur\nheadlines to attract more readers. We propose to automate this headline editing\nprocess through neural network models to provide more immediate writing support\nfor these social media news writers. To train such a neural headline editing\nmodel, we collected a dataset which contains articles with original headlines\nand professionally edited headlines. However, it is expensive to collect a\nlarge number of professionally edited headlines. To solve this low-resource\nproblem, we design an encoder-decoder model which leverages large scale\npre-trained language models. We further improve the pre-trained model's quality\nby introducing a headline generation task as an intermediate task before the\nheadline editing task. Also, we propose Self Importance-Aware (SIA) loss to\naddress the different levels of editing in the dataset by down-weighting the\nimportance of easily classified tokens and sentences. With the help of\nPre-training, Adaptation, and SIA, the model learns to generate headlines in\nthe professional editor's style. Experimental results show that our method\nsignificantly improves the quality of headline editing comparing against\nprevious methods.", "published": "2019-11-25 06:42:02", "link": "http://arxiv.org/abs/1912.01114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Transformer by Position Learning", "abstract": "Non-autoregressive models are promising on various text generation tasks.\nPrevious work hardly considers to explicitly model the positions of generated\nwords. However, position modeling is an essential problem in non-autoregressive\ntext generation. In this study, we propose PNAT, which incorporates positions\nas a latent variable into the text generative process. Experimental results\nshow that PNAT achieves top results on machine translation and paraphrase\ngeneration tasks, outperforming several strong baselines.", "published": "2019-11-25 03:08:42", "link": "http://arxiv.org/abs/1911.10677v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Filling Conversation Ellipsis for Better Social Dialog Understanding", "abstract": "The phenomenon of ellipsis is prevalent in social conversations. Ellipsis\nincreases the difficulty of a series of downstream language understanding\ntasks, such as dialog act prediction and semantic role labeling. We propose to\nresolve ellipsis through automatic sentence completion to improve language\nunderstanding. However, automatic ellipsis completion can result in output\nwhich does not accurately reflect user intent. To address this issue, we\npropose a method which considers both the original utterance that has ellipsis\nand the automatically completed utterance in dialog act and semantic role\nlabeling tasks. Specifically, we first complete user utterances to resolve\nellipsis using an end-to-end pointer network model. We then train a prediction\nmodel using both utterances containing ellipsis and our automatically completed\nutterances. Finally, we combine the prediction results from these two\nutterances using a selection model that is guided by expert knowledge. Our\napproach improves dialog act prediction and semantic role labeling by 1.3% and\n2.5% in F1 score respectively in social conversations. We also present an\nopen-domain human-machine conversation dataset with manually completed user\nutterances and annotated semantic role labeling after manual completion.", "published": "2019-11-25 09:21:17", "link": "http://arxiv.org/abs/1911.10776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Causal Inference Method for Reducing Gender Bias in Word Embedding\n  Relations", "abstract": "Word embedding has become essential for natural language processing as it\nboosts empirical performances of various tasks. However, recent research\ndiscovers that gender bias is incorporated in neural word embeddings, and\ndownstream tasks that rely on these biased word vectors also produce\ngender-biased results. While some word-embedding gender-debiasing methods have\nbeen developed, these methods mainly focus on reducing gender bias associated\nwith gender direction and fail to reduce the gender bias presented in word\nembedding relations. In this paper, we design a causal and simple approach for\nmitigating gender bias in word vector relation by utilizing the statistical\ndependency between gender-definition word embeddings and gender-biased word\nembeddings. Our method attains state-of-the-art results on gender-debiasing\ntasks, lexical- and sentence-level evaluation tasks, and downstream coreference\nresolution tasks.", "published": "2019-11-25 09:47:11", "link": "http://arxiv.org/abs/1911.10787v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SWift -- A SignWriting improved fast transcriber", "abstract": "We present SWift (SignWriting improved fast transcriber), an advanced editor\nfor computer-aided writing and transcribing using SignWriting (SW). SW is\ndevised to allow deaf people and linguists alike to exploit an easy-to-grasp\nwritten form of (any) sign language. Similarly, SWift has been developed for\neveryone who masters SW, and is not exclusively deaf-oriented. Using SWift, it\nis possible to compose and save any sign, using elementary components called\nglyphs. A guided procedure facilitates the composition process. SWift is aimed\nat helping to break down the \"electronic\" barriers that keep the deaf community\naway from Information and Communication Technology (ICT). The editor has been\ndeveloped modularly and can be integrated everywhere the use of SW, as an\nalternative to written vocal language, may be advisable.", "published": "2019-11-25 12:54:58", "link": "http://arxiv.org/abs/1911.10882v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "FLATM: A Fuzzy Logic Approach Topic Model for Medical Documents", "abstract": "One of the challenges for text analysis in medical domains is analyzing\nlarge-scale medical documents. As a consequence, finding relevant documents has\nbecome more difficult. One of the popular methods to retrieve information based\non discovering the themes in the documents is topic modeling. The themes in the\ndocuments help to retrieve documents on the same topic with and without a\nquery. In this paper, we present a novel approach to topic modeling using fuzzy\nclustering. To evaluate our model, we experiment with two text datasets of\nmedical documents. The evaluation metrics carried out through document\nclassification and document modeling show that our model produces better\nperformance than LDA, indicating that fuzzy set theory can improve the\nperformance of topic models in medical domains.", "published": "2019-11-25 14:55:11", "link": "http://arxiv.org/abs/1911.10953v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional\n  Neural Network", "abstract": "Automated ICD coding, which assigns the International Classification of\nDisease codes to patient visits, has attracted much research attention since it\ncan save time and labor for billing. The previous state-of-the-art model\nutilized one convolutional layer to build document representations for\npredicting ICD codes. However, the lengths and grammar of text fragments, which\nare closely related to ICD coding, vary a lot in different documents.\nTherefore, a flat and fixed-length convolutional architecture may not be\ncapable of learning good document representations. In this paper, we proposed a\nMulti-Filter Residual Convolutional Neural Network (MultiResCNN) for ICD\ncoding. The innovations of our model are two-folds: it utilizes a multi-filter\nconvolutional layer to capture various text patterns with different lengths and\na residual convolutional layer to enlarge the receptive field. We evaluated the\neffectiveness of our model on the widely-used MIMIC dataset. On the full code\nset of MIMIC-III, our model outperformed the state-of-the-art model in 4 out of\n6 evaluation metrics. On the top-50 code set of MIMIC-III and the full code set\nof MIMIC-II, our model outperformed all the existing and state-of-the-art\nmodels in all evaluation metrics. The code is available at\nhttps://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network.", "published": "2019-11-25 11:23:04", "link": "http://arxiv.org/abs/1912.00862v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Independent language modeling architecture for end-to-end ASR", "abstract": "The attention-based end-to-end (E2E) automatic speech recognition (ASR)\narchitecture allows for joint optimization of acoustic and language models\nwithin a single network. However, in a vanilla E2E ASR architecture, the\ndecoder sub-network (subnet), which incorporates the role of the language model\n(LM), is conditioned on the encoder output. This means that the acoustic\nencoder and the language model are entangled that doesn't allow language model\nto be trained separately from external text data. To address this problem, in\nthis work, we propose a new architecture that separates the decoder subnet from\nthe encoder output. In this way, the decoupled subnet becomes an independently\ntrainable LM subnet, which can easily be updated using the external text data.\nWe study two strategies for updating the new architecture. Experimental results\nshow that, 1) the independent LM architecture benefits from external text data,\nachieving 9.3% and 22.8% relative character and word error rate reduction on\nMandarin HKUST and English NSC datasets respectively; 2)the proposed\narchitecture works well with external LM and can be generalized to different\namount of labelled data.", "published": "2019-11-25 07:35:16", "link": "http://arxiv.org/abs/1912.00863v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "hauWE: Hausa Words Embedding for Natural Language Processing", "abstract": "Words embedding (distributed word vector representations) have become an\nessential component of many natural language processing (NLP) tasks such as\nmachine translation, sentiment analysis, word analogy, named entity recognition\nand word similarity. Despite this, the only work that provides word vectors for\nHausa language is that of Bojanowski et al. [1] trained using fastText,\nconsisting of only a few words vectors. This work presents words embedding\nmodels using Word2Vec's Continuous Bag of Words (CBoW) and Skip Gram (SG)\nmodels. The models, hauWE (Hausa Words Embedding), are bigger and better than\nthe only previous model, making them more useful in NLP tasks. To compare the\nmodels, they were used to predict the 10 most similar words to 30 randomly\nselected Hausa words. hauWE CBoW's 88.7% and hauWE SG's 79.3% prediction\naccuracy greatly outperformed Bojanowski et al. [1]'s 22.3%.", "published": "2019-11-25 05:46:56", "link": "http://arxiv.org/abs/1911.10708v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Trainable Non-Collaborative Dialog System", "abstract": "End-to-end task-oriented dialog models have achieved promising performance on\ncollaborative tasks where users willingly coordinate with the system to\ncomplete a given task. While in non-collaborative settings, for example,\nnegotiation and persuasion, users and systems do not share a common goal. As a\nresult, compared to collaborate tasks, people use social content to build\nrapport and trust in these non-collaborative settings in order to advance their\ngoals. To handle social content, we introduce a hierarchical intent annotation\nscheme, which can be generalized to different non-collaborative dialog tasks.\nBuilding upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end\nneural network model to generate diverse coherent responses. Our model utilizes\nintent and semantic slots as the intermediate sentence representation to guide\nthe generation process. In addition, we design a filter to select appropriate\nresponses based on whether these intermediate representations fit the designed\ntask and conversation constraints. Our non-collaborative dialog model guides\nusers to complete the task while simultaneously keeps them engaged. We test our\napproach on our newly proposed ANTISCAM dataset and an existing\nPERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that\nour model outperforms multiple baselines in these two non-collaborative tasks.", "published": "2019-11-25 07:34:37", "link": "http://arxiv.org/abs/1911.10742v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Corpus Wide Argument Mining -- a Working Solution", "abstract": "One of the main tasks in argument mining is the retrieval of argumentative\ncontent pertaining to a given topic. Most previous work addressed this task by\nretrieving a relatively small number of relevant documents as the initial\nsource for such content. This line of research yielded moderate success, which\nis of limited use in a real-world system. Furthermore, for such a system to\nyield a comprehensive set of relevant arguments, over a wide range of topics,\nit requires leveraging a large and diverse corpus in an appropriate manner.\nHere we present a first end-to-end high-precision, corpus-wide argument mining\nsystem. This is made possible by combining sentence-level queries over an\nappropriate indexing of a very large corpus of newspaper articles, with an\niterative annotation scheme. This scheme addresses the inherent label bias in\nthe data and pinpoints the regions of the sample space whose manual labeling is\nrequired to obtain high-precision among top-ranked candidates.", "published": "2019-11-25 08:29:37", "link": "http://arxiv.org/abs/1911.10763v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Discovering topics with neural topic models built from PLSA assumptions", "abstract": "In this paper we present a model for unsupervised topic discovery in texts\ncorpora. The proposed model uses documents, words, and topics lookup table\nembedding as neural network model parameters to build probabilities of words\ngiven topics, and probabilities of topics given documents. These probabilities\nare used to recover by marginalization probabilities of words given documents.\nFor very large corpora where the number of documents can be in the order of\nbillions, using a neural auto-encoder based document embedding is more scalable\nthen using a lookup table embedding as classically done. We thus extended the\nlookup based document embedding model to continuous auto-encoder based model.\nOur models are trained using probabilistic latent semantic analysis (PLSA)\nassumptions. We evaluated our models on six datasets with a rich variety of\ncontents. Conducted experiments demonstrate that the proposed neural topic\nmodels are very effective in capturing relevant topics. Furthermore,\nconsidering perplexity metric, conducted evaluation benchmarks show that our\ntopic models outperform latent Dirichlet allocation (LDA) model which is\nclassically used to address topic discovery tasks.", "published": "2019-11-25 13:59:05", "link": "http://arxiv.org/abs/1911.10924v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Women, politics and Twitter: Using machine learning to change the\n  discourse", "abstract": "Including diverse voices in political decision-making strengthens our\ndemocratic institutions. Within the Canadian political system, there is gender\ninequality across all levels of elected government. Online abuse, such as\nhateful tweets, leveled at women engaged in politics contributes to this\ninequity, particularly tweets focusing on their gender. In this paper, we\npresent ParityBOT: a Twitter bot which counters abusive tweets aimed at women\nin politics by sending supportive tweets about influential female leaders and\nfacts about women in public life. ParityBOT is the first artificial\nintelligence-based intervention aimed at affecting online discourse for women\nin politics for the better. The goal of this project is to: $1$) raise\nawareness of issues relating to gender inequity in politics, and $2$)\npositively influence public discourse in politics. The main contribution of\nthis paper is a scalable model to classify and respond to hateful tweets with\nquantitative and qualitative assessments. The ParityBOT abusive classification\nsystem was validated on public online harassment datasets. We conclude with\nanalysis of the impact of ParityBOT, drawing from data gathered during\ninterventions in both the $2019$ Alberta provincial and $2019$ Canadian federal\nelections.", "published": "2019-11-25 16:15:42", "link": "http://arxiv.org/abs/1911.11025v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Examining the Role of Clickbait Headlines to Engage Readers with\n  Reliable Health-related Information", "abstract": "Clickbait headlines are frequently used to attract readers to read articles.\nAlthough this headline type has turned out to be a technique to engage readers\nwith misleading items, it is still unknown whether the technique can be used to\nattract readers to reliable pieces. This study takes the opportunity to test\nits efficacy to engage readers with reliable health articles. A set of online\nsurveys would be conducted to test readers' engagement with and perception\nabout clickbait headlines with reliable articles. After that, we would design\nan automation system to generate clickabit headlines to maximize user\nengagement.", "published": "2019-11-25 20:29:01", "link": "http://arxiv.org/abs/1911.11214v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning to Learn Words from Visual Scenes", "abstract": "Language acquisition is the process of learning words from the surrounding\nscene. We introduce a meta-learning framework that learns how to learn word\nrepresentations from unconstrained scenes. We leverage the natural\ncompositional structure of language to create training episodes that cause a\nmeta-learner to learn strong policies for language acquisition. Experiments on\ntwo datasets show that our approach is able to more rapidly acquire novel words\nas well as more robustly generalize to unseen compositions, significantly\noutperforming established baselines. A key advantage of our approach is that it\nis data efficient, allowing representations to be learned from scratch without\nlanguage pre-training. Visualizations and analysis suggest visual information\nhelps our approach learn a rich cross-modal representation from minimal\nexamples. Project webpage is available at https://expert.cs.columbia.edu/", "published": "2019-11-25 21:19:31", "link": "http://arxiv.org/abs/1911.11237v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FairyTED: A Fair Rating Predictor for TED Talk Data", "abstract": "With the recent trend of applying machine learning in every aspect of human\nlife, it is important to incorporate fairness into the core of the predictive\nalgorithms. We address the problem of predicting the quality of public speeches\nwhile being fair with respect to sensitive attributes of the speakers, e.g.\ngender and race. We use the TED talks as an input repository of public speeches\nbecause it consists of speakers from a diverse community and has a wide\noutreach. Utilizing the theories of Causal Models, Counterfactual Fairness and\nstate-of-the-art neural language models, we propose a mathematical framework\nfor fair prediction of the public speaking quality. We employ grounded\nassumptions to construct a causal model capturing how different attributes\naffect public speaking quality. This causal model contributes in generating\ncounterfactual data to train a fair predictive model. Our framework is general\nenough to utilize any assumption within the causal model. Experimental results\nshow that while prediction accuracy is comparable to recent work on this\ndataset, our predictions are counterfactually fair with respect to a novel\nmetric when compared to true data labels. The FairyTED setup not only allows\norganizers to make informed and diverse selection of speakers from the\nunobserved counterfactual possibilities but it also ensures that viewers and\nnew users are not influenced by unfair and unbalanced ratings from arbitrary\nvisitors to the www.ted.com website when deciding to view a talk.", "published": "2019-11-25 09:55:52", "link": "http://arxiv.org/abs/1911.11558v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Conclusion-Supplement Answer Generation for Non-Factoid Questions", "abstract": "This paper tackles the goal of conclusion-supplement answer generation for\nnon-factoid questions, which is a critical issue in the field of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), as users often\nrequire supplementary information before accepting a conclusion. The current\nencoder-decoder framework, however, has difficulty generating such answers,\nsince it may become confused when it tries to learn several different long\nanswers to the same non-factoid question. Our solution, called an ensemble\nnetwork, goes beyond single short sentences and fuses logically connected\nconclusion statements and supplementary statements. It extracts the context\nfrom the conclusion decoder's output sequence and uses it to create\nsupplementary decoder states on the basis of an attention mechanism. It also\nassesses the closeness of the question encoder's output sequence and the\nseparate outputs of the conclusion and supplement decoders as well as their\ncombination. As a result, it generates answers that match the questions and\nhave natural-sounding supplementary sequences in line with the context\nexpressed by the conclusion sequence. Evaluations conducted on datasets\nincluding \"Love Advice\" and \"Arts & Humanities\" categories indicate that our\nmodel outputs much more accurate results than the tested baseline models do.", "published": "2019-11-25 07:06:25", "link": "http://arxiv.org/abs/1912.00864v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Summarization of Scholarly Videos using Word Embeddings and\n  Keyphrase Extraction", "abstract": "Effective learning with audiovisual content depends on many factors. Besides\nthe quality of the learning resource's content, it is essential to discover the\nmost relevant and suitable video in order to support the learning process most\neffectively. Video summarization techniques facilitate this goal by providing a\nquick overview over the content. It is especially useful for longer recordings\nsuch as conference presentations or lectures. In this paper, we present an\napproach that generates a visual summary of video content based on semantic\nword embeddings and keyphrase extraction. For this purpose, we exploit video\nannotations that are automatically generated by speech recognition and video\nOCR (optical character recognition).", "published": "2019-11-25 12:02:15", "link": "http://arxiv.org/abs/1912.10809v1", "categories": ["cs.MM", "cs.CL", "cs.DL"], "primary_category": "cs.MM"}
{"title": "Sound event detection via dilated convolutional recurrent neural\n  networks", "abstract": "Convolutional recurrent neural networks (CRNNs) have achieved\nstate-of-the-art performance for sound event detection (SED). In this paper, we\npropose to use a dilated CRNN, namely a CRNN with a dilated convolutional\nkernel, as the classifier for the task of SED. We investigate the effectiveness\nof dilation operations which provide a CRNN with expanded receptive fields to\ncapture long temporal context without increasing the amount of CRNN's\nparameters. Compared to the classifier of the baseline CRNN, the classifier of\nthe dilated CRNN obtains a maximum increase of 1.9%, 6.3% and 2.5% at F1 score\nand a maximum decrease of 1.7%, 4.1% and 3.9% at error rate (ER), on the\npublicly available audio corpora of the TUT-SED Synthetic 2016, the TUT Sound\nEvent 2016 and the TUT Sound Event 2017, respectively.", "published": "2019-11-25 13:07:06", "link": "http://arxiv.org/abs/1911.10888v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Invertible DNN-based nonlinear time-frequency transform for speech\n  enhancement", "abstract": "We propose an end-to-end speech enhancement method with trainable\ntime-frequency~(T-F) transform based on invertible deep neural network~(DNN).\nThe resent development of speech enhancement is brought by using DNN. The\nordinary DNN-based speech enhancement employs T-F transform, typically the\nshort-time Fourier transform~(STFT), and estimates a T-F mask using DNN. On the\nother hand, some methods have considered end-to-end networks which directly\nestimate the enhanced signals without T-F transform. While end-to-end methods\nhave shown promising results, they are black boxes and hard to understand.\nTherefore, some end-to-end methods used a DNN to learn the linear T-F transform\nwhich is much easier to understand. However, the learned transform may not have\na property important for ordinary signal processing. In this paper, as the\nimportant property of the T-F transform, perfect reconstruction is considered.\nAn invertible nonlinear T-F transform is constructed by DNNs and learned from\ndata so that the obtained transform is perfectly reconstructing filterbank.", "published": "2019-11-25 08:35:34", "link": "http://arxiv.org/abs/1911.10764v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Narrow-band Deep Filtering for Multichannel Speech Enhancement", "abstract": "In this paper, we address the problem of multichannel speech enhancement in\nthe short-time Fourier transform (STFT) domain. A long short-time memory (LSTM)\nnetwork takes as input a sequence of STFT coefficients associated with a\nfrequency bin of multichannel noisy-speech signals. The network's output is the\ncorresponding sequence of single-channel cleaned speech. We propose several\nclean-speech network targets, namely, the magnitude ratio mask, the complex\nSTFT coefficients and the (smoothed) spatial filter. A prominent feature of the\nproposed model is that the same LSTM architecture, with identical parameters,\nis trained across frequency bins. The proposed method is referred to as\nnarrow-band deep filtering. This choice stays in contrast with traditional\nwide-band speech enhancement methods. The proposed deep filtering is able to\ndiscriminate between speech and noise by exploiting their different temporal\nand spatial characteristics: speech is non-stationary and spatially coherent\nwhile noise is relatively stationary and weakly correlated across channels.\nThis is similar in spirit with unsupervised techniques, such as spectral\nsubtraction and beamforming. We describe extensive experiments with both mixed\nsignals (noise is added to clean speech) and real signals (live recordings). We\nempirically evaluate the proposed architecture variants using speech\nenhancement and speech recognition metrics, and we compare our results with the\nresults obtained with several state of the art methods. In the light of these\nexperiments we conclude that narrow-band deep filtering has very good speech\nenhancement and speech recognition performance, and excellent generalization\ncapabilities in terms of speaker variability and noise type.", "published": "2019-11-25 09:52:51", "link": "http://arxiv.org/abs/1911.10791v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Percussive Synthesis Parameterised by High-Level Timbral Features", "abstract": "We present a deep neural network-based methodology for synthesising\npercussive sounds with control over high-level timbral characteristics of the\nsounds. This approach allows for intuitive control of a synthesizer, enabling\nthe user to shape sounds without extensive knowledge of signal processing. We\nuse a feedforward convolutional neural network-based architecture, which is\nable to map input parameters to the corresponding waveform. We propose two\ndatasets to evaluate our approach on both a restrictive context, and in one\ncovering a broader spectrum of sounds. The timbral features used as parameters\nare taken from recent literature in signal processing. We also use these\nfeatures for evaluation and validation of the presented model, to ensure that\nchanging the input parameters produces a congruent waveform with the desired\ncharacteristics. Finally, we evaluate the quality of the output sound using a\nsubjective listening test. We provide sound examples and the system's source\ncode for reproducibility.", "published": "2019-11-25 11:26:51", "link": "http://arxiv.org/abs/1911.11853v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Voice Biomarker Identification for Effects of Deep-Brain Stimulation on\n  Parkinson's Disease", "abstract": "Deep-Brain Stimulation (DBS) is a therapy used in conjunction with medication\nto help alleviate the motor symptoms of Parkinson's Disease (PD). However, the\nmonitoring and adjustment of DBS settings is tedious and expensive, requiring\nlong programming appointments every few months. We investigated the possible\ncorrelation between PD motor score severity and digitally extracted patient\nvoice features to potentially aid clinicians in their monitoring and treatment\nof PD with DBS, and eventually enable a closed-loop DBS system. 5 DBS PD\npatients were enrolled. Voice samples were collected for various voice tasks\n(single phoneme vocalization, free speech task, sentence reading task, counting\nbackward task, categorical fluency task) for DBS ON and OFF states. Motor\nscores per the Unified Parkinson's Disease Rating Scale (UPDRS) were also\ncollected for DBS ON and OFF states. Voice samples were then analyzed to\nextract voice features using publicly available voice feature library sets, and\nstatistically compared for DBS ON and OFF. Of the feature categories explored\n(Acoustic, Prosodic, Linguistic) 6 features from the GeMAPS feature set for\nacoustic features demonstrated significant differences with DBS ON and OFF\n(p<0.05). Prosodic features such as pause length/percentage were found to be\nnegatively correlated with increased motor symptom severity. Non-significant\ndifferences were found for linguistic features. These findings provide\npreliminary evidence for acoustic and prosodic speech features to act as\npotential biomarkers for PD disease severity in DBS patients. We hope to\nexplore further by expanding our data set, identifying other features, applying\nmachine learning models, and working towards a closed-loop DBS system that can\nauto-tune itself based on changes in a patient's voice.", "published": "2019-11-25 07:46:27", "link": "http://arxiv.org/abs/1912.00866v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
