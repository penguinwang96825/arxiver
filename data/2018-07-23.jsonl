{"title": "Deep Dialog Act Recognition using Multiple Token, Segment, and Context\n  Information Representations", "abstract": "Dialog act (DA) recognition is a task that has been widely explored over the\nyears. Recently, most approaches to the task explored different DNN\narchitectures to combine the representations of the words in a segment and\ngenerate a segment representation that provides cues for intention. In this\nstudy, we explore means to generate more informative segment representations,\nnot only by exploring different network architectures, but also by considering\ndifferent token representations, not only at the word level, but also at the\ncharacter and functional levels. At the word level, in addition to the commonly\nused uncontextualized embeddings, we explore the use of contextualized\nrepresentations, which provide information concerning word sense and segment\nstructure. Character-level tokenization is important to capture\nintention-related morphological aspects that cannot be captured at the word\nlevel. Finally, the functional level provides an abstraction from words, which\nshifts the focus to the structure of the segment. We also explore approaches to\nenrich the segment representation with context information from the history of\nthe dialog, both in terms of the classifications of the surrounding segments\nand the turn-taking history. This kind of information has already been proved\nimportant for the disambiguation of DAs in previous studies. Nevertheless, we\nare able to capture additional information by considering a summary of the\ndialog history and a wider turn-taking context. By combining the best\napproaches at each step, we achieve results that surpass the previous\nstate-of-the-art on generic DA recognition on both SwDA and MRDA, two of the\nmost widely explored corpora for the task. Furthermore, by considering both\npast and future context, simulating annotation scenario, our approach achieves\na performance similar to that of a human annotator on SwDA and surpasses it on\nMRDA.", "published": "2018-07-23 13:12:28", "link": "http://arxiv.org/abs/1807.08587v2", "categories": ["cs.CL", "H.1.2; H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Classification based on Multiple Block Convolutional Highways", "abstract": "In the Text Classification areas of Sentiment Analysis,\nSubjectivity/Objectivity Analysis, and Opinion Polarity, Convolutional Neural\nNetworks have gained special attention because of their performance and\naccuracy. In this work, we applied recent advances in CNNs and propose a novel\narchitecture, Multiple Block Convolutional Highways (MBCH), which achieves\nimproved accuracy on multiple popular benchmark datasets, compared to previous\narchitectures. The MBCH is based on new techniques and architectures including\nhighway networks, DenseNet, batch normalization and bottleneck layers. In\naddition, to cope with the limitations of existing pre-trained word vectors\nwhich are used as inputs for the CNN, we propose a novel method, Improved Word\nVectors (IWV). The IWV improves the accuracy of CNNs which are used for text\nclassification tasks.", "published": "2018-07-23 13:58:38", "link": "http://arxiv.org/abs/1807.09602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Relevance in Visual Question Answering", "abstract": "Free-form and open-ended Visual Question Answering systems solve the problem\nof providing an accurate natural language answer to a question pertaining to an\nimage. Current VQA systems do not evaluate if the posed question is relevant to\nthe input image and hence provide nonsensical answers when posed with\nirrelevant questions to an image. In this paper, we solve the problem of\nidentifying the relevance of the posed question to an image. We address the\nproblem as two sub-problems. We first identify if the question is visual or\nnot. If the question is visual, we then determine if it's relevant to the image\nor not. For the second problem, we generate a large dataset from existing\nvisual question answering datasets in order to enable the training of complex\narchitectures and model the relevance of a visual question to an image. We also\ncompare the results of our Long Short-Term Memory Recurrent Neural Network\nbased models to Logistic Regression, XGBoost and multi-layer perceptron based\napproaches to the problem.", "published": "2018-07-23 06:01:44", "link": "http://arxiv.org/abs/1807.08435v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ASR-free CNN-DTW keyword spotting using multilingual bottleneck features\n  for almost zero-resource languages", "abstract": "We consider multilingual bottleneck features (BNFs) for nearly zero-resource\nkeyword spotting. This forms part of a United Nations effort using keyword\nspotting to support humanitarian relief programmes in parts of Africa where\nlanguages are severely under-resourced. We use 1920 isolated keywords (40\ntypes, 34 minutes) as exemplars for dynamic time warping (DTW) template\nmatching, which is performed on a much larger body of untranscribed speech.\nThese DTW costs are used as targets for a convolutional neural network (CNN)\nkeyword spotter, giving a much faster system than direct DTW. Here we consider\nhow available data from well-resourced languages can improve this CNN-DTW\napproach. We show that multilingual BNFs trained on ten languages improve the\narea under the ROC curve of a CNN-DTW system by 10.9% absolute relative to the\nMFCC baseline. By combining low-resource DTW-based supervision with information\nfrom well-resourced languages, CNN-DTW is a competitive option for low-resource\nkeyword spotting.", "published": "2018-07-23 15:14:32", "link": "http://arxiv.org/abs/1807.08666v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition for Humanitarian Applications in Somali", "abstract": "We present our first efforts in building an automatic speech recognition\nsystem for Somali, an under-resourced language, using 1.57 hrs of annotated\nspeech for acoustic model training. The system is part of an ongoing effort by\nthe United Nations (UN) to implement keyword spotting systems supporting\nhumanitarian relief programmes in parts of Africa where languages are severely\nunder-resourced. We evaluate several types of acoustic model, including recent\nneural architectures. Language model data augmentation using a combination of\nrecurrent neural networks (RNN) and long short-term memory neural networks\n(LSTMs) as well as the perturbation of acoustic data are also considered. We\nfind that both types of data augmentation are beneficial to performance, with\nour best system using a combination of convolutional neural networks (CNNs),\ntime-delay neural networks (TDNNs) and bi-directional long short term memory\n(BLSTMs) to achieve a word error rate of 53.75%.", "published": "2018-07-23 15:17:04", "link": "http://arxiv.org/abs/1807.08669v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LinkNBed: Multi-Graph Representation Learning with Entity Linkage", "abstract": "Knowledge graphs have emerged as an important model for studying complex\nmulti-relational data. This has given rise to the construction of numerous\nlarge scale but incomplete knowledge graphs encoding information extracted from\nvarious resources. An effective and scalable approach to jointly learn over\nmultiple graphs and eventually construct a unified graph is a crucial next step\nfor the success of knowledge-based inference for many downstream applications.\nTo this end, we propose LinkNBed, a deep relational learning framework that\nlearns entity and relationship representations across multiple graphs. We\nidentify entity linkage across graphs as a vital component to achieve our goal.\nWe design a novel objective that leverage entity linkage and build an efficient\nmulti-task training procedure. Experiments on link prediction and entity\nlinkage demonstrate substantial improvements over the state-of-the-art\nrelational learning approaches.", "published": "2018-07-23 06:47:57", "link": "http://arxiv.org/abs/1807.08447v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multimodal Social Media Analysis for Gang Violence Prevention", "abstract": "Gang violence is a severe issue in major cities across the U.S. and recent\nstudies [Patton et al. 2017] have found evidence of social media communications\nthat can be linked to such violence in communities with high rates of exposure\nto gang activity. In this paper we partnered computer scientists with social\nwork researchers, who have domain expertise in gang violence, to analyze how\npublic tweets with images posted by youth who mention gang associations on\nTwitter can be leveraged to automatically detect psychosocial factors and\nconditions that could potentially assist social workers and violence outreach\nworkers in prevention and early intervention programs. To this end, we\ndeveloped a rigorous methodology for collecting and annotating tweets. We\ngathered 1,851 tweets and accompanying annotations related to visual concepts\nand the psychosocial codes: aggression, loss, and substance use. These codes\nare relevant to social work interventions, as they represent possible pathways\nto violence on social media. We compare various methods for classifying tweets\ninto these three classes, using only the text of the tweet, only the image of\nthe tweet, or both modalities as input to the classifier. In particular, we\nanalyze the usefulness of mid-level visual concepts and the role of different\nmodalities for this tweet classification task. Our experiments show that\nindividually, text information dominates classification performance of the loss\nclass, while image information dominates the aggression and substance use\nclasses. Our multimodal approach provides a very promising improvement (18%\nrelative in mean average precision) over the best single modality approach.\nFinally, we also illustrate the complexity of understanding social media data\nand elaborate on open challenges.", "published": "2018-07-23 07:52:52", "link": "http://arxiv.org/abs/1807.08465v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "AceKG: A Large-scale Knowledge Graph for Academic Data Mining", "abstract": "Most existing knowledge graphs (KGs) in academic domains suffer from problems\nof insufficient multi-relational information, name ambiguity and improper data\nformat for large-scale machine processing. In this paper, we present AceKG, a\nnew large-scale KG in academic domain. AceKG not only provides clean academic\ninformation, but also offers a large-scale benchmark dataset for researchers to\nconduct challenging data mining projects including link prediction, community\ndetection and scholar classification. Specifically, AceKG describes 3.13\nbillion triples of academic facts based on a consistent ontology, including\nnecessary properties of papers, authors, fields of study, venues and\ninstitutes, as well as the relations among them. To enrich the proposed\nknowledge graph, we also perform entity alignment with existing databases and\nrule-based inference. Based on AceKG, we conduct experiments of three typical\nacademic data mining tasks and evaluate several state-of- the-art knowledge\nembedding and network representation learning approaches on the benchmark\ndatasets built from AceKG. Finally, we discuss several promising research\ndirections that benefit from AceKG.", "published": "2018-07-23 08:57:44", "link": "http://arxiv.org/abs/1807.08484v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Acoustic-to-Word Recognition with Sequence-to-Sequence Models", "abstract": "Acoustic-to-Word recognition provides a straightforward solution to\nend-to-end speech recognition without needing external decoding, language model\nre-scoring or lexicon. While character-based models offer a natural solution to\nthe out-of-vocabulary problem, word models can be simpler to decode and may\nalso be able to directly recognize semantically meaningful units. We present\neffective methods to train Sequence-to-Sequence models for direct word-level\nrecognition (and character-level recognition) and show an absolute improvement\nof 4.4-5.0\\% in Word Error Rate on the Switchboard corpus compared to prior\nwork. In addition to these promising results, word-based models are more\ninterpretable than character models, which have to be composed into words using\na separate decoding step. We analyze the encoder hidden states and the\nattention behavior, and show that location-aware attention naturally represents\nwords as a single speech-word-vector, despite spanning multiple frames in the\ninput. We finally show that the Acoustic-to-Word model also learns to segment\nspeech into words with a mean standard deviation of 3 frames as compared with\nhuman annotated forced-alignments for the Switchboard corpus.", "published": "2018-07-23 06:29:43", "link": "http://arxiv.org/abs/1807.09597v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Auto-adaptive Resonance Equalization using Dilated Residual Networks", "abstract": "In music and audio production, attenuation of spectral resonances is an\nimportant step towards a technically correct result. In this paper we present a\ntwo-component system to automate the task of resonance equalization. The first\ncomponent is a dynamic equalizer that automatically detects resonances and\noffers to attenuate them by a user-specified factor. The second component is a\ndeep neural network that predicts the optimal attenuation factor based on the\nwindowed audio. The network is trained and validated on empirical data gathered\nfrom an experiment in which sound engineers choose their preferred attenuation\nfactors for a set of tracks. We test two distinct network architectures for the\npredictive model and find that a dilated residual network operating directly on\nthe audio signal is on a par with a network architecture that requires a prior\naudio feature extraction stage. Both architectures predict human-preferred\nresonance attenuation factors significantly better than a baseline approach.", "published": "2018-07-23 14:18:56", "link": "http://arxiv.org/abs/1807.08636v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
