{"title": "Multimodal Social Media Analysis for Gang Violence Prevention", "abstract": "Gang violence is a severe issue in major cities across the U.S. and recent studies [Patton et al. 2017] have found evidence of social media communications that can be linked to such violence in communities with high rates of exposure to gang activity. In this paper we partnered computer scientists with social work researchers, who have domain expertise in gang violence, to analyze how public tweets with images posted by youth who mention gang associations on Twitter can be leveraged to automatically detect psychosocial factors and conditions that could potentially assist social workers and violence outreach workers in prevention and early intervention programs. To this end, we developed a rigorous methodology for collecting and annotating tweets. We gathered 1,851 tweets and accompanying annotations related to visual concepts and the psychosocial codes: aggression, loss, and substance use. These codes are relevant to social work interventions, as they represent possible pathways to violence on social media. We compare various methods for classifying tweets into these three classes, using only the text of the tweet, only the image of the tweet, or both modalities as input to the classifier. In particular, we analyze the usefulness of mid-level visual concepts and the role of different modalities for this tweet classification task. Our experiments show that individually, text information dominates classification performance of the loss class, while image information dominates the aggression and substance use classes. Our multimodal approach provides a very promising improvement (18% relative in mean average precision) over the best single modality approach. Finally, we also illustrate the complexity of understanding social media data and elaborate on open challenges.", "published": "2018-07-23 07:52:52", "link": "http://arxiv.org/abs/1807.08465v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MVDepthNet: Real-time Multiview Depth Estimation Neural Network", "abstract": "Although deep neural networks have been widely applied to computer vision problems, extending them into multiview depth estimation is non-trivial. In this paper, we present MVDepthNet, a convolutional network to solve the depth estimation problem given several image-pose pairs from a localized monocular camera in neighbor viewpoints. Multiview observations are encoded in a cost volume and then combined with the reference image to estimate the depth map using an encoder-decoder network. By encoding the information from multiview observations into the cost volume, our method achieves real-time performance and the flexibility of traditional methods that can be applied regardless of the camera intrinsic parameters and the number of images. Geometric data augmentation is used to train MVDepthNet. We further apply MVDepthNet in a monocular dense mapping system that continuously estimates depth maps using a single localized moving camera. Experiments show that our method can generate depth maps efficiently and precisely.", "published": "2018-07-23 12:37:13", "link": "http://arxiv.org/abs/1807.08563v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Average Case - Worst Case Tradeoffs for Evacuating 2 Robots from the Disk in the Face-to-Face Model", "abstract": "The problem of evacuating two robots from the disk in the face-to-face model was first introduced in [Czyzowicz et al., DISC'14], and extensively studied (along with many variations) ever since with respect to worst case analysis. We initiate the study of the same problem with respect to average case analysis, which is also equivalent to designing randomized algorithms for the problem. First we observe that algorithm $B_{2}$ of~[Czyzowicz et al., DISC'14] with worst case cost $WRS(B_{2}):=5.73906$ has average case cost $AVG(B_{2}):=5.1172$. Then we verify that none of the algorithms that induced worst case cost improvements in subsequent publications has better average case cost, hence concluding that our problem requires the invention of new algorithms. Then, we observe that a remarkable simple algorithm, $B_{1}$, has very small average case cost $AVG(B_{1}):=1+\u03c0$, but very high worst case cost $WRS(B_{1}):=1+2\u03c0$.\n  Motivated by the above, we introduce constrained optimization problem $_2Evac_{F2F}^w$, in which one is trying to minimize the average case cost of the evacuation algorithm given that the worst case cost does not exceed $w$. The problem is of special interest with respect to practical applications, since a common objective in search-and-rescue operations is to minimize the average completion time, given that a certain worst case threshold is not exceeded, e.g. for safety or limited energy reasons. Our main contribution is the design and analysis of families of new evacuation parameterized algorithms $A(p)$ which can solve $_2Evac_{F2F}^w$, for every $w \\in [WRS(B_{1}),WRS(B_{2})]$.", "published": "2018-07-23 14:34:07", "link": "http://arxiv.org/abs/1807.08640v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "NullaNet: Training Deep Neural Networks for Reduced-Memory-Access Inference", "abstract": "Deep neural networks have been successfully deployed in a wide variety of applications including computer vision and speech recognition. However, computational and storage complexity of these models has forced the majority of computations to be performed on high-end computing platforms or on the cloud. To cope with computational and storage complexity of these models, this paper presents a training method that enables a radically different approach for realization of deep neural networks through Boolean logic minimization. The aforementioned realization completely removes the energy-hungry step of accessing memory for obtaining model parameters, consumes about two orders of magnitude fewer computing resources compared to realizations that use floatingpoint operations, and has a substantially lower latency.", "published": "2018-07-23 16:50:31", "link": "http://arxiv.org/abs/1807.08716v2", "categories": ["cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Aligning Points to Lines: Provable Approximations", "abstract": "We suggest a new optimization technique for minimizing the sum $\\sum_{i=1}^n f_i(x)$ of $n$ non-convex real functions that satisfy a property that we call piecewise log-Lipschitz. This is by forging links between techniques in computational geometry, combinatorics and convex optimization. As an example application, we provide the first constant-factor approximation algorithms whose running-time is polynomial in $n$ for the fundamental problem of \\emph{Points-to-Lines alignment}: Given $n$ points $p_1,\\cdots,p_n$ and $n$ lines $\\ell_1,\\cdots,\\ell_n$ on the plane and $z>0$, compute the matching $\u03c0:[n]\\to[n]$ and alignment (rotation matrix $R$ and a translation vector $t$) that minimize the sum of Euclidean distances $\\sum_{i=1}^n \\mathrm{dist}(Rp_i-t,\\ell_{\u03c0(i)})^z$ between each point to its corresponding line.\n  This problem is non-trivial even if $z=1$ and the matching $\u03c0$ is given. If $\u03c0$ is given, the running time of our algorithms is $O(n^3)$, and even near-linear in $n$ using core-sets that support: streaming, dynamic, and distributed parallel computations in poly-logarithmic update time. Generalizations for handling e.g. outliers or pseudo-distances such as $M$-estimators for the problem are also provided.\n  Experimental results and open source code show that our provable algorithms improve existing heuristics also in practice. A companion demonstration video in the context of Augmented Reality shows how such algorithms may be used in real-time systems.", "published": "2018-07-23 06:45:15", "link": "http://arxiv.org/abs/1807.08446v3", "categories": ["cs.LG", "cs.CG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Play Pong using Policy Gradient Learning", "abstract": "Activities in reinforcement learning (RL) revolve around learning the Markov decision process (MDP) model, in particular, the following parameters: state values, V; state-action values, Q; and policy, pi. These parameters are commonly implemented as an array. Scaling up the problem means scaling up the size of the array and this will quickly lead to a computational bottleneck. To get around this, the RL problem is commonly formulated to learn a specific task using hand-crafted input features to curb the size of the array. In this report, we discuss an alternative end-to-end Deep Reinforcement Learning (DRL) approach where the DRL attempts to learn general task representations which in our context refers to learning to play the Pong game from a sequence of screen snapshots without game-specific hand-crafted features. We apply artificial neural networks (ANN) to approximate a policy of the RL model. The policy network, via Policy Gradients (PG) method, learns to play the Pong game from a sequence of frames without any extra semantics apart from the pixel information and the score. In contrast to the traditional tabular RL approach where the contents in the array have clear interpretations such as V or Q, the interpretation of knowledge content from the weights of the policy network is more illusive. In this work, we experiment with various Deep ANN architectures i.e., Feed forward ANN (FFNN), Convolution ANN (CNN) and Asynchronous Advantage Actor-Critic (A3C). We also examine the activation of hidden nodes and the weights between the input and the hidden layers, before and after the DRL has successfully learnt to play the Pong game. Insights into the internal learning mechanisms and future research directions are then discussed.", "published": "2018-07-23 06:55:23", "link": "http://arxiv.org/abs/1807.08452v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs", "abstract": "The recent empirical success of unsupervised cross-domain mapping algorithms, between two domains that share common characteristics, is not well-supported by theoretical justifications. This lacuna is especially troubling, given the clear ambiguity in such mappings.\n  We work with adversarial training methods based on IPMs and derive a novel risk bound, which upper bounds the risk between the learned mapping $h$ and the target mapping $y$, by a sum of three terms: (i) the risk between $h$ and the most distant alternative mapping that was learned by the same cross-domain mapping algorithm, (ii) the minimal discrepancy between the target domain and the domain obtained by applying a hypothesis $h^*$ on the samples of the source domain, where $h^*$ is a hypothesis selectable by the same algorithm. The bound is directly related to Occam's razor and encourages the selection of the minimal architecture that supports a small mapping discrepancy and (iii) an approximation error term that decreases as the complexity of the class of discriminators increases and is empirically shown to be small.\n  The bound leads to multiple algorithmic consequences, including a method for hyperparameters selection and for early stopping in cross-domain mapping GANs. We also demonstrate a novel capability for unsupervised learning of estimating confidence in the mapping of every specific sample.", "published": "2018-07-23 09:33:51", "link": "http://arxiv.org/abs/1807.08501v4", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multisensor Management Algorithm for Airborne Sensors Using Frank-Wolfe Method", "abstract": "This study proposes an airborne multisensor management algorithm for target tracking, taking each of multiple unmanned aircraft as a sensor. The purpose of the algorithm is to determine the configuration of the sensor deployment and to guide the mobile sensors to track moving targets in an optimal way. The cost function as a performance metric is defined as a combination of the D-optimality criterion of the Fisher information matrix. The convexity of the cost function is proved and the optimal solution for deployment and guidance problem is derived by the Frank-Wolfe method, also known as the conditional gradient descent method. An intuitive optimal approach to deal with the problem is to direct the sensor to the optimal position obtained by solving a nonlinear optimization problem. On the other hand, the proposed method takes the conditional gradient of the cost function as the command to the deployed sensors, so that the sensors are guaranteed to be in the feasible points and they achieve the current best performance. Simulation results demonstrate that the proposed algorithm provides better performance than directing each sensor to its optimal position.", "published": "2018-07-23 11:10:53", "link": "http://arxiv.org/abs/1807.08531v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
