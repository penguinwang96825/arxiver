{"title": "Text Complexity Classification Based on Linguistic Information:\n  Application to Intelligent Tutoring of ESL", "abstract": "The goal of this work is to build a classifier that can identify text\ncomplexity within the context of teaching reading to English as a Second\nLanguage (ESL) learners. To present language learners with texts that are\nsuitable to their level of English, a set of features that can describe the\nphonological, morphological, lexical, syntactic, discursive, and psychological\ncomplexity of a given text were identified. Using a corpus of 6171 texts, which\nhad already been classified into three different levels of difficulty by ESL\nexperts, different experiments were conducted with five machine learning\nalgorithms. The results showed that the adopted linguistic features provide a\ngood overall classification performance (F-Score = 0.97). A scalability\nevaluation was conducted to test if such a classifier could be used within real\napplications, where it can be, for example, plugged into a search engine or a\nweb-scraping module. In this evaluation, the texts in the test set are not only\ndifferent from those from the training set but also of different types (ESL\ntexts vs. children reading texts). Although the overall performance of the\nclassifier decreased significantly (F-Score = 0.65), the confusion matrix shows\nthat most of the classification errors are between the classes two and three\n(the middle-level classes) and that the system has a robust performance in\ncategorizing texts of class one and four. This behavior can be explained by the\ndifference in classification criteria between the two corpora. Hence, the\nobserved results confirm the usability of such a classifier within a real-world\napplication.", "published": "2020-01-07 02:42:57", "link": "http://arxiv.org/abs/2001.01863v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Opinions Transfer Network for Target-Oriented Opinion Words\n  Extraction", "abstract": "Target-oriented opinion words extraction (TOWE) is a new subtask of ABSA,\nwhich aims to extract the corresponding opinion words for a given opinion\ntarget in a sentence. Recently, neural network methods have been applied to\nthis task and achieve promising results. However, the difficulty of annotation\ncauses the datasets of TOWE to be insufficient, which heavily limits the\nperformance of neural models. By contrast, abundant review sentiment\nclassification data are easily available at online review sites. These reviews\ncontain substantial latent opinions information and semantic patterns. In this\npaper, we propose a novel model to transfer these opinions knowledge from\nresource-rich review sentiment classification datasets to low-resource task\nTOWE. To address the challenges in the transfer process, we design an effective\ntransformation method to obtain latent opinions, then integrate them into TOWE.\nExtensive experimental results show that our model achieves better performance\ncompared to other state-of-the-art methods and significantly outperforms the\nbase model without transferring opinions knowledge. Further analysis validates\nthe effectiveness of our model.", "published": "2020-01-07 11:50:54", "link": "http://arxiv.org/abs/2001.01989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Attention Network for Protein-Protein Interaction\n  Extraction", "abstract": "Protein-protein interaction (PPI) extraction from published scientific\nliterature provides additional support for precision medicine efforts. However,\nmany of the current PPI extraction methods need extensive feature engineering\nand cannot make full use of the prior knowledge in knowledge bases (KB). KBs\ncontain huge amounts of structured information about entities and\nrelationships, therefore plays a pivotal role in PPI extraction. This paper\nproposes a knowledge-aware attention network (KAN) to fuse prior knowledge\nabout protein-protein pairs and context information for PPI extraction. The\nproposed model first adopts a diagonal-disabled multi-head attention mechanism\nto encode context sequence along with knowledge representations learned from\nKB. Then a novel multi-dimensional attention mechanism is used to select the\nfeatures that can best describe the encoded context. Experiment results on the\nBioCreative VI PPI dataset show that the proposed approach could acquire\nknowledge-aware dependencies between different words in a sequence and lead to\na new state-of-the-art performance.", "published": "2020-01-07 15:02:28", "link": "http://arxiv.org/abs/2001.02091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Prior Knowledge for Protein-Protein Interaction Extraction\n  with Memory Network", "abstract": "Automatically extracting Protein-Protein Interactions (PPI) from biomedical\nliterature provides additional support for precision medicine efforts. This\npaper proposes a novel memory network-based model (MNM) for PPI extraction,\nwhich leverages prior knowledge about protein-protein pairs with memory\nnetworks. The proposed MNM captures important context clues related to\nknowledge representations learned from knowledge bases. Both entity embeddings\nand relation embeddings of prior knowledge are effective in improving the PPI\nextraction model, leading to a new state-of-the-art performance on the\nBioCreative VI PPI dataset. The paper also shows that multiple computational\nlayers over an external memory are superior to long short-term memory networks\nwith the local memories.", "published": "2020-01-07 15:11:27", "link": "http://arxiv.org/abs/2001.02107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention over Parameters for Dialogue Systems", "abstract": "Dialogue systems require a great deal of different but complementary\nexpertise to assist, inform, and entertain humans. For example, different\ndomains (e.g., restaurant reservation, train ticket booking) of goal-oriented\ndialogue systems can be viewed as different skills, and so does ordinary\nchatting abilities of chit-chat dialogue systems. In this paper, we propose to\nlearn a dialogue system that independently parameterizes different dialogue\nskills, and learns to select and combine each of them through Attention over\nParameters (AoP). The experimental results show that this approach achieves\ncompetitive performance on a combined dataset of MultiWOZ, In-Car Assistant,\nand Persona-Chat. Finally, we demonstrate that each dialogue skill is\neffectively learned and can be combined with other skills to produce selective\nresponses.", "published": "2020-01-07 03:10:42", "link": "http://arxiv.org/abs/2001.01871v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Paraphrase Generation with Latent Bag of Words", "abstract": "Paraphrase generation is a longstanding important problem in natural language\nprocessing.\n  In addition, recent progress in deep generative models has shown promising\nresults on discrete latent variables for text generation.\n  Inspired by variational autoencoders with discrete latent structures, in this\nwork, we propose a latent bag of words (BOW) model for paraphrase generation.\n  We ground the semantics of a discrete latent variable by the BOW from the\ntarget sentences.\n  We use this latent variable to build a fully differentiable content planning\nand surface realization model.\n  Specifically, we use source words to predict their neighbors and model the\ntarget BOW with a mixture of softmax.\n  We use Gumbel top-k reparameterization to perform differentiable subset\nsampling from the predicted BOW distribution.\n  We retrieve the sampled word embeddings and use them to augment the decoder\nand guide its generation search space.\n  Our latent BOW model not only enhances the decoder, but also exhibits clear\ninterpretability.\n  We show the model interpretability with regard to \\emph{(i)} unsupervised\nlearning of word neighbors \\emph{(ii)} the step-by-step generation procedure.\n  Extensive experiments demonstrate the transparent and effective generation\nprocess of this model.\\footnote{Our code can be found at\n\\url{https://github.com/FranxYao/dgm_latent_bow}}", "published": "2020-01-07 09:22:58", "link": "http://arxiv.org/abs/2001.01941v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heaps' law and Heaps functions in tagged texts: Evidences of their\n  linguistic relevance", "abstract": "We study the relationship between vocabulary size and text length in a corpus\nof $75$ literary works in English, authored by six writers, distinguishing\nbetween the contributions of three grammatical classes (or ``tags,'' namely,\n{\\it nouns}, {\\it verbs}, and {\\it others}), and analyze the progressive\nappearance of new words of each tag along each individual text. While the\npower-law relation prescribed by Heaps' law is satisfactorily fulfilled by\ntotal vocabulary sizes and text lengths, the appearance of new words in each\ntext is on the whole well described by the average of random shufflings of the\ntext, which does not obey a power law. Deviations from this average, however,\nare statistically significant and show a systematic trend across the corpus.\nSpecifically, they reveal that the appearance of new words along each text is\npredominantly retarded with respect to the average of random shufflings.\nMoreover, different tags are shown to add systematically distinct contributions\nto this tendency, with {\\it verbs} and {\\it others} being respectively more and\nless retarded than the mean trend, and {\\it nouns} following instead this\noverall mean. These statistical systematicities are likely to point to the\nexistence of linguistically relevant information stored in the different\nvariants of Heaps' law, a feature that is still in need of extensive\nassessment.", "published": "2020-01-07 17:05:16", "link": "http://arxiv.org/abs/2001.02178v1", "categories": ["cs.CL", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "Multipurpose Intelligent Process Automation via Conversational Assistant", "abstract": "Intelligent Process Automation (IPA) is an emerging technology with a primary\ngoal to assist the knowledge worker by taking care of repetitive, routine and\nlow-cognitive tasks. Conversational agents that can interact with users in a\nnatural language are potential application for IPA systems. Such intelligent\nagents can assist the user by answering specific questions and executing\nroutine tasks that are ordinarily performed in a natural language (i.e.,\ncustomer support). In this work, we tackle a challenge of implementing an IPA\nconversational assistant in a real-world industrial setting with a lack of\nstructured training data. Our proposed system brings two significant benefits:\nFirst, it reduces repetitive and time-consuming activities and, therefore,\nallows workers to focus on more intelligent processes. Second, by interacting\nwith users, it augments the resources with structured and to some extent\nlabeled training data. We showcase the usage of the latter by re-implementing\nseveral components of our system with Transfer Learning (TL) methods.", "published": "2020-01-07 21:47:37", "link": "http://arxiv.org/abs/2001.02284v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RECAST: Interactive Auditing of Automatic Toxicity Detection Models", "abstract": "As toxic language becomes nearly pervasive online, there has been increasing\ninterest in leveraging the advancements in natural language processing (NLP),\nfrom very large transformer models to automatically detecting and removing\ntoxic comments. Despite the fairness concerns, lack of adversarial robustness,\nand limited prediction explainability for deep learning systems, there is\ncurrently little work for auditing these systems and understanding how they\nwork for both developers and users. We present our ongoing work, RECAST, an\ninteractive tool for examining toxicity detection models by visualizing\nexplanations for predictions and providing alternative wordings for detected\ntoxic speech.", "published": "2020-01-07 00:17:52", "link": "http://arxiv.org/abs/2001.01819v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2; I.7; J.4; K.4"], "primary_category": "cs.CL"}
{"title": "Machine-learning classifiers for logographic name matching in public\n  health applications: approaches for incorporating phonetic, visual, and\n  keystroke similarity in large-scale probabilistic record linkage", "abstract": "Approximate string-matching methods to account for complex variation in\nhighly discriminatory text fields, such as personal names, can enhance\nprobabilistic record linkage. However, discriminating between matching and\nnon-matching strings is challenging for logographic scripts, where similarities\nin pronunciation, appearance, or keystroke sequence are not directly encoded in\nthe string data. We leverage a large Chinese administrative dataset with known\nmatch status to develop logistic regression and Xgboost classifiers integrating\nmeasures of visual, phonetic, and keystroke similarity to enhance\nidentification of potentially-matching name pairs. We evaluate three methods of\nleveraging name similarity scores in large-scale probabilistic record linkage,\nwhich can adapt to varying match prevalence and information in supporting\nfields: (1) setting a threshold score based on predicted quality of\nname-matching across all record pairs; (2) setting a threshold score based on\npredicted discriminatory power of the linkage model; and (3) using empirical\nscore distributions among matches and nonmatches to perform Bayesian adjustment\nof matching probabilities estimated from exact-agreement linkage. In\nexperiments on holdout data, as well as data simulated with varying name error\nrates and supporting fields, a logistic regression classifier incorporated via\nthe Bayesian method demonstrated marked improvements over exact-agreement\nlinkage with respect to discriminatory power, match probability estimation, and\naccuracy, reducing the total number of misclassified record pairs by 21% in\ntest data and up to an average of 93% in simulated datasets. Our results\ndemonstrate the value of incorporating visual, phonetic, and keystroke\nsimilarity for logographic name matching, as well as the promise of our\nBayesian approach to leverage name-matching within large-scale record linkage.", "published": "2020-01-07 05:21:21", "link": "http://arxiv.org/abs/2001.01895v1", "categories": ["cs.IR", "cs.CL", "stat.AP"], "primary_category": "cs.IR"}
{"title": "Learning Speaker Embedding with Momentum Contrast", "abstract": "Speaker verification can be formulated as a representation learning task,\nwhere speaker-discriminative embeddings are extracted from utterances of\nvariable lengths. Momentum Contrast (MoCo) is a recently proposed unsupervised\nrepresentation learning framework, and has shown its effectiveness for learning\ngood feature representation for downstream vision tasks. In this work, we apply\nMoCo to learn speaker embedding from speech segments. We explore MoCo for both\nunsupervised learning and pretraining settings. In the unsupervised scenario,\nembedding is learned by MoCo from audio data without using any speaker specific\ninformation. On a large scale dataset with $2,500$ speakers, MoCo can achieve\nEER $4.275\\%$ trained unsupervisedly, and the EER can decrease further to\n$3.58\\%$ if extra unlabelled data are used. In the pretraining scenario,\nencoder trained by MoCo is used to initialize the downstream supervised\ntraining. With finetuning on the MoCo trained model, the equal error rate (EER)\nreduces $13.7\\%$ relative ($1.44\\%$ to $1.242\\%$) compared to a carefully tuned\nbaseline training from scratch. Comparative study confirms the effectiveness of\nMoCo learning good speaker embedding.", "published": "2020-01-07 11:47:05", "link": "http://arxiv.org/abs/2001.01986v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attributed Multi-Relational Attention Network for Fact-checking URL\n  Recommendation", "abstract": "To combat fake news, researchers mostly focused on detecting fake news and\njournalists built and maintained fact-checking sites (e.g., Snopes.com and\nPolitifact.com). However, fake news dissemination has been greatly promoted via\nsocial media sites, and these fact-checking sites have not been fully utilized.\nTo overcome these problems and complement existing methods against fake news,\nin this paper we propose a deep-learning based fact-checking URL recommender\nsystem to mitigate impact of fake news in social media sites such as Twitter\nand Facebook. In particular, our proposed framework consists of a\nmulti-relational attentive module and a heterogeneous graph attention network\nto learn complex/semantic relationship between user-URL pairs, user-user pairs,\nand URL-URL pairs. Extensive experiments on a real-world dataset show that our\nproposed framework outperforms eight state-of-the-art recommendation models,\nachieving at least 3~5.3% improvement.", "published": "2020-01-07 18:26:38", "link": "http://arxiv.org/abs/2001.02214v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
