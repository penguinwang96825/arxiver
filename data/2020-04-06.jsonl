{"title": "Learning to Recover Reasoning Chains for Multi-Hop Question Answering\n  via Cooperative Games", "abstract": "We propose the new problem of learning to recover reasoning chains from\nweakly supervised signals, i.e., the question-answer pairs. We propose a\ncooperative game approach to deal with this problem, in which how the evidence\npassages are selected and how the selected passages are connected are handled\nby two models that cooperate to select the most confident chains from a large\nset of candidates (from distant supervision). For evaluation, we created\nbenchmarks based on two multi-hop QA datasets, HotpotQA and MedHop; and\nhand-labeled reasoning chains for the latter. The experimental results\ndemonstrate the effectiveness of our proposed approach.", "published": "2020-04-06 03:54:38", "link": "http://arxiv.org/abs/2004.02393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PONE: A Novel Automatic Evaluation Metric for Open-Domain Generative\n  Dialogue Systems", "abstract": "Open-domain generative dialogue systems have attracted considerable attention\nover the past few years. Currently, how to automatically evaluate them, is\nstill a big challenge problem. As far as we know, there are three kinds of\nautomatic methods to evaluate the open-domain generative dialogue systems: (1)\nWord-overlap-based metrics; (2) Embedding-based metrics; (3) Learning-based\nmetrics. Due to the lack of systematic comparison, it is not clear which kind\nof metrics are more effective. In this paper, we will first measure\nsystematically all kinds of automatic evaluation metrics over the same\nexperimental setting to check which kind is best. Through extensive\nexperiments, the learning-based metrics are demonstrated that they are the most\neffective evaluation metrics for open-domain generative dialogue systems.\nMoreover, we observe that nearly all learning-based metrics depend on the\nnegative sampling mechanism, which obtains an extremely imbalanced and\nlow-quality dataset to train a score model. In order to address this issue, we\npropose a novel and feasible learning-based metric that can significantly\nimprove the correlation with human judgments by using augmented POsitive\nsamples and valuable NEgative samples, called PONE. Extensive experiments\ndemonstrate that our proposed evaluation method significantly outperforms the\nstate-of-the-art learning-based evaluation methods, with an average correlation\nimprovement of 13.18%. In addition, we have publicly released the codes of our\nproposed method and state-of-the-art baselines.", "published": "2020-04-06 04:36:33", "link": "http://arxiv.org/abs/2004.02399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The World is Not Binary: Learning to Rank with Grayscale Data for\n  Dialogue Response Selection", "abstract": "Response selection plays a vital role in building retrieval-based\nconversation systems. Despite that response selection is naturally a\nlearning-to-rank problem, most prior works take a point-wise view and train\nbinary classifiers for this task: each response candidate is labeled either\nrelevant (one) or irrelevant (zero). On the one hand, this formalization can be\nsub-optimal due to its ignorance of the diversity of response quality. On the\nother hand, annotating grayscale data for learning-to-rank can be prohibitively\nexpensive and challenging. In this work, we show that grayscale data can be\nautomatically constructed without human effort. Our method employs\noff-the-shelf response retrieval models and response generation models as\nautomatic grayscale data generators. With the constructed grayscale data, we\npropose multi-level ranking objectives for training, which can (1) teach a\nmatching model to capture more fine-grained context-response relevance\ndifference and (2) reduce the train-test discrepancy in terms of distractor\nstrength. Our method is simple, effective, and universal. Experiments on three\nbenchmark datasets and four state-of-the-art matching models show that the\nproposed approach brings significant and consistent performance improvements.", "published": "2020-04-06 06:34:54", "link": "http://arxiv.org/abs/2004.02421v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of the Utility of Explicit Negative Examples to Improve the\n  Syntactic Abilities of Neural Language Models", "abstract": "We explore the utilities of explicit negative examples in training neural\nlanguage models. Negative examples here are incorrect words in a sentence, such\nas \"barks\" in \"*The dogs barks\". Neural language models are commonly trained\nonly on positive examples, a set of sentences in the training data, but recent\nstudies suggest that the models trained in this way are not capable of robustly\nhandling complex syntactic constructions, such as long-distance agreement. In\nthis paper, using English data, we first demonstrate that appropriately using\nnegative examples about particular constructions (e.g., subject-verb agreement)\nwill boost the model's robustness on them, with a negligible loss of\nperplexity. The key to our success is an additional margin loss between the\nlog-likelihoods of a correct word and an incorrect word. We then provide a\ndetailed analysis of the trained models. One of our findings is the difficulty\nof object-relative clauses for RNNs. We find that even with our direct learning\nsignals the models still suffer from resolving agreement across an\nobject-relative clause. Augmentation of training sentences involving the\nconstructions somewhat helps, but the accuracy still does not reach the level\nof subject-relative clauses. Although not directly cognitively appealing, our\nmethod can be a tool to analyze the true architectural limitation of neural\nmodels on challenging linguistic constructions.", "published": "2020-04-06 07:47:34", "link": "http://arxiv.org/abs/2004.02451v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Norwegian Lexical Resource for Medical Entity Recognition", "abstract": "We present a large Norwegian lexical resource of categorized medical terms.\nThe resource merges information from large medical databases, and contains over\n77,000 unique entries, including automatically mapped terms from a Norwegian\nmedical dictionary. We describe the methodology behind this automatic\ndictionary entry mapping based on keywords and suffixes and further present the\nresults of a manual evaluation performed on a subset by a domain expert. The\nevaluation indicated that ca. 80% of the mappings were correct.", "published": "2020-04-06 09:24:11", "link": "http://arxiv.org/abs/2004.02509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dictionary-based Data Augmentation for Cross-Domain Neural Machine\n  Translation", "abstract": "Existing data augmentation approaches for neural machine translation (NMT)\nhave predominantly relied on back-translating in-domain (IND) monolingual\ncorpora. These methods suffer from issues associated with a domain information\ngap, which leads to translation errors for low frequency and out-of-vocabulary\nterminology. This paper proposes a dictionary-based data augmentation (DDA)\nmethod for cross-domain NMT. DDA synthesizes a domain-specific dictionary with\ngeneral domain corpora to automatically generate a large-scale pseudo-IND\nparallel corpus. The generated pseudo-IND data can be used to enhance a general\ndomain trained baseline. The experiments show that the DDA-enhanced NMT models\ndemonstrate consistent significant improvements, outperforming the baseline\nmodels by 3.75-11.53 BLEU. The proposed method is also able to further improve\nthe performance of the back-translation based and IND-finetuned NMT models. The\nimprovement is associated with the enhanced domain coverage produced by DDA.", "published": "2020-04-06 11:50:49", "link": "http://arxiv.org/abs/2004.02577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping a Crosslingual Semantic Parser", "abstract": "Recent progress in semantic parsing scarcely considers languages other than\nEnglish but professional translation can be prohibitively expensive. We adapt a\nsemantic parser trained on a single language, such as English, to new languages\nand multiple domains with minimal annotation. We query if machine translation\nis an adequate substitute for training data, and extend this to investigate\nbootstrapping using joint training with English, paraphrasing, and multilingual\npre-trained models. We develop a Transformer-based parser combining paraphrases\nby ensembling attention over multiple encoders and present new versions of ATIS\nand Overnight in German and Chinese for evaluation. Experimental results\nindicate that MT can approximate training data in a new language for accurate\nparsing when augmented with paraphrasing through multiple MT engines.\nConsidering when MT is inadequate, we also find that using our approach\nachieves parsing accuracy within 2% of complete translation using only 50% of\ntraining data.", "published": "2020-04-06 12:05:02", "link": "http://arxiv.org/abs/2004.02585v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Summarize Passages: Mining Passage-Summary Pairs from\n  Wikipedia Revision Histories", "abstract": "In this paper, we propose a method for automatically constructing a\npassage-to-summary dataset by mining the Wikipedia page revision histories. In\nparticular, the method mines the main body passages and the introduction\nsentences which are added to the pages simultaneously. The constructed dataset\ncontains more than one hundred thousand passage-summary pairs. The quality\nanalysis shows that it is promising that the dataset can be used as a training\nand validation set for passage summarization. We validate and analyze the\nperformance of various summarization systems on the proposed dataset. The\ndataset will be available online at https://res.qyzhou.me.", "published": "2020-04-06 12:11:50", "link": "http://arxiv.org/abs/2004.02592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Text Generation", "abstract": "Current state-of-the-art text generators build on powerful language models\nsuch as GPT-2, achieving impressive performance. However, to avoid degenerate\ntext, they require sampling from a modified softmax, via temperature parameters\nor ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This\ncreates a mismatch between training and testing conditions. In this paper, we\nuse the recently introduced entmax transformation to train and sample from a\nnatively sparse language model, avoiding this mismatch. The result is a text\ngenerator with favorable performance in terms of fluency and consistency, fewer\nrepetitions, and n-gram diversity closer to human text. In order to evaluate\nour model, we propose three new metrics for comparing sparse or truncated\ndistributions: $\\epsilon$-perplexity, sparsemax score, and Jensen-Shannon\ndivergence. Human-evaluated experiments in story completion and dialogue\ngeneration show that entmax sampling leads to more engaging and coherent\nstories and conversations.", "published": "2020-04-06 13:09:10", "link": "http://arxiv.org/abs/2004.02644v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "At Which Level Should We Extract? An Empirical Analysis on Extractive\n  Document Summarization", "abstract": "Extractive methods have been proven effective in automatic document\nsummarization. Previous works perform this task by identifying informative\ncontents at sentence level. However, it is unclear whether performing\nextraction at sentence level is the best solution. In this work, we show that\nunnecessity and redundancy issues exist when extracting full sentences, and\nextracting sub-sentential units is a promising alternative. Specifically, we\npropose extracting sub-sentential units based on the constituency parsing tree.\nA neural extractive model which leverages the sub-sentential information and\nextracts them is presented. Extensive experiments and analyses show that\nextracting sub-sentential units performs competitively comparing to full\nsentence extraction under the evaluation of both automatic and human\nevaluations. Hopefully, our work could provide some inspiration of the basic\nextraction units in extractive summarization for future research.", "published": "2020-04-06 13:35:10", "link": "http://arxiv.org/abs/2004.02664v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Models' Local Decision Boundaries via Contrast Sets", "abstract": "Standard test sets for supervised learning evaluate in-distribution\ngeneralization. Unfortunately, when a dataset has systematic gaps (e.g.,\nannotation artifacts), these evaluations are misleading: a model can learn\nsimple decision rules that perform well on the test set but do not capture a\ndataset's intended capabilities. We propose a new annotation paradigm for NLP\nthat helps to close systematic gaps in the test data. In particular, after a\ndataset is constructed, we recommend that the dataset authors manually perturb\nthe test instances in small but meaningful ways that (typically) change the\ngold label, creating contrast sets. Contrast sets provide a local view of a\nmodel's decision boundary, which can be used to more accurately evaluate a\nmodel's true linguistic capabilities. We demonstrate the efficacy of contrast\nsets by creating them for 10 diverse NLP datasets (e.g., DROP reading\ncomprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets\nare not explicitly adversarial, model performance is significantly lower on\nthem than on the original test sets---up to 25\\% in some cases. We release our\ncontrast sets as new evaluation benchmarks and encourage future dataset\nconstruction efforts to follow similar annotation processes.", "published": "2020-04-06 14:47:18", "link": "http://arxiv.org/abs/2004.02709v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Few-Shot NMT Adaptation", "abstract": "We present META-MT, a meta-learning approach to adapt Neural Machine\nTranslation (NMT) systems in a few-shot setting. META-MT provides a new\napproach to make NMT models easily adaptable to many target domains with the\nminimal amount of in-domain data. We frame the adaptation of NMT systems as a\nmeta-learning problem, where we learn to adapt to new unseen domains based on\nsimulated offline meta-training domain adaptation tasks. We evaluate the\nproposed meta-learning strategy on ten domains with general large scale NMT\nsystems. We show that META-MT significantly outperforms classical domain\nadaptation when very few in-domain examples are available. Our experiments\nshows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU\npoints after seeing only 4, 000 translated words (300 parallel sentences).", "published": "2020-04-06 15:38:55", "link": "http://arxiv.org/abs/2004.02745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper\n  Headlines", "abstract": "The extraction of anglicisms (lexical borrowings from English) is relevant\nboth for lexicographic purposes and for NLP downstream tasks. We introduce a\ncorpus of European Spanish newspaper headlines annotated with anglicisms and a\nbaseline model for anglicism extraction. In this paper we present: (1) a corpus\nof 21,570 newspaper headlines written in European Spanish annotated with\nemergent anglicisms and (2) a conditional random field baseline model with\nhandcrafted features for anglicism extraction. We present the newspaper\nheadlines corpus, describe the annotation tagset and guidelines and introduce a\nCRF model that can serve as baseline for the task of detecting anglicisms. The\npresented work is a first step towards the creation of an anglicism extractor\nfor Spanish newswire.", "published": "2020-04-06 18:41:43", "link": "http://arxiv.org/abs/2004.02929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Learning of Text Adventure Games with Sentence-Level Semantics", "abstract": "Reinforcement learning algorithms such as Q-learning have shown great promise\nin training models to learn the optimal action to take for a given system\nstate; a goal in applications with an exploratory or adversarial nature such as\ntask-oriented dialogues or games. However, models that do not have direct\naccess to their state are harder to train; when the only state access is via\nthe medium of language, this can be particularly pronounced. We introduce a new\nmodel amenable to deep Q-learning that incorporates a Siamese neural network\narchitecture and a novel refactoring of the Q-value function in order to better\nrepresent system state given its approximation over a language channel. We\nevaluate the model in the context of zero-shot text-based adventure game\nlearning. Extrinsically, our model reaches the baseline's convergence\nperformance point needing only 15% of its iterations, reaches a convergence\nperformance point 15% higher than the baseline's, and is able to play unseen,\nunrelated games with no fine-tuning. We probe our new model's representation\nspace to determine that intrinsically, this is due to the appropriate\nclustering of different linguistic mediation into the same state.", "published": "2020-04-06 20:24:33", "link": "http://arxiv.org/abs/2004.02986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Evaluation of Diversity in Natural Language Generation", "abstract": "Despite growing interest in natural language generation (NLG) models that\nproduce diverse outputs, there is currently no principled method for evaluating\nthe diversity of an NLG system. In this work, we propose a framework for\nevaluating diversity metrics. The framework measures the correlation between a\nproposed diversity metric and a diversity parameter, a single parameter that\ncontrols some aspect of diversity in generated text. For example, a diversity\nparameter might be a binary variable used to instruct crowdsourcing workers to\ngenerate text with either low or high content diversity. We demonstrate the\nutility of our framework by: (a) establishing best practices for eliciting\ndiversity judgments from humans, (b) showing that humans substantially\noutperform automatic metrics in estimating content diversity, and (c)\ndemonstrating that existing methods for controlling diversity by tuning a\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\nadvance the understanding of different diversity metrics, an essential step on\nthe road towards better NLG systems.", "published": "2020-04-06 20:44:10", "link": "http://arxiv.org/abs/2004.02990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Step Inference for Reasoning Over Paragraphs", "abstract": "Complex reasoning over text requires understanding and chaining together\nfree-form predicates and logical connectives. Prior work has largely tried to\ndo this either symbolically or with black-box transformers. We present a middle\nground between these two extremes: a compositional model reminiscent of neural\nmodule networks that can perform chained logical reasoning. This model first\nfinds relevant sentences in the context and then chains them together using\nneural modules. Our model gives significant performance improvements (up to\n29\\% relative error reduction when comfibined with a reranker) on ROPES, a\nrecently introduced complex reasoning dataset.", "published": "2020-04-06 21:12:53", "link": "http://arxiv.org/abs/2004.02995v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"You are grounded!\": Latent Name Artifacts in Pre-trained Language\n  Models", "abstract": "Pre-trained language models (LMs) may perpetuate biases originating in their\ntraining corpus to downstream models. We focus on artifacts associated with the\nrepresentation of given names (e.g., Donald), which, depending on the corpus,\nmay be associated with specific entities, as indicated by next token prediction\n(e.g., Trump). While helpful in some contexts, grounding happens also in\nunder-specified or inappropriate contexts. For example, endings generated for\n`Donald is a' substantially differ from those of other names, and often have\nmore-than-average negative sentiment. We demonstrate the potential effect on\ndownstream tasks with reading comprehension probes where name perturbation\nchanges the model answers. As a silver lining, our experiments suggest that\nadditional pre-training on different corpora may mitigate this bias.", "published": "2020-04-06 21:48:39", "link": "http://arxiv.org/abs/2004.03012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Review Comprehension with Domain-Specific Commonsense", "abstract": "Review comprehension has played an increasingly important role in improving\nthe quality of online services and products and commonsense knowledge can\nfurther enhance review comprehension. However, existing general-purpose\ncommonsense knowledge bases lack sufficient coverage and precision to\nmeaningfully improve the comprehension of domain-specific reviews. In this\npaper, we introduce xSense, an effective system for review comprehension using\ndomain-specific commonsense knowledge bases (xSense KBs). We show that xSense\nKBs can be constructed inexpensively and present a knowledge distillation\nmethod that enables us to use xSense KBs along with BERT to boost the\nperformance of various review comprehension tasks. We evaluate xSense over\nthree review comprehension tasks: aspect extraction, aspect sentiment\nclassification, and question answering. We find that xSense outperforms the\nstate-of-the-art models for the first two tasks and improves the baseline BERT\nQA model significantly, demonstrating the usefulness of incorporating\ncommonsense into review comprehension pipelines. To facilitate future research\nand applications, we publicly release three domain-specific knowledge bases and\na domain-specific question answering benchmark along with this paper.", "published": "2020-04-06 22:11:30", "link": "http://arxiv.org/abs/2004.03020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Analysis of Morphological Content in BERT Models for\n  Multiple Languages", "abstract": "This work describes experiments which probe the hidden representations of\nseveral BERT-style models for morphological content. The goal is to examine the\nextent to which discrete linguistic structure, in the form of morphological\nfeatures and feature values, presents itself in the vector representations and\nattention distributions of pre-trained language models for five European\nlanguages. The experiments contained herein show that (i) Transformer\narchitectures largely partition their embedding space into convex sub-regions\nhighly correlated with morphological feature value, (ii) the contextualized\nnature of transformer embeddings allows models to distinguish ambiguous\nmorphological forms in many, but not all cases, and (iii) very specific\nattention head/layer combinations appear to hone in on subject-verb agreement.", "published": "2020-04-06 22:50:27", "link": "http://arxiv.org/abs/2004.03032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Pragmatic and Discourse Context in Determining Argument\n  Impact", "abstract": "Research in the social sciences and psychology has shown that the\npersuasiveness of an argument depends not only the language employed, but also\non attributes of the source/communicator, the audience, and the appropriateness\nand strength of the argument's claims given the pragmatic and discourse context\nof the argument. Among these characteristics of persuasive arguments, prior\nwork in NLP does not explicitly investigate the effect of the pragmatic and\ndiscourse context when determining argument quality. This paper presents a new\ndataset to initiate the study of this aspect of argumentation: it consists of a\ndiverse collection of arguments covering 741 controversial topics and\ncomprising over 47,000 claims. We further propose predictive models that\nincorporate the pragmatic and discourse context of argumentative claims and\nshow that they outperform models that rely only on claim-specific linguistic\nfeatures for predicting the perceived impact of individual claims within a\nparticular line of argument.", "published": "2020-04-06 23:00:37", "link": "http://arxiv.org/abs/2004.03034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Early Prediction of Buyer-Seller Negotiation Outcomes", "abstract": "Agents that negotiate with humans find broad applications in pedagogy and\nconversational AI. Most efforts in human-agent negotiations rely on restrictive\nmenu-driven interfaces for communication. To advance the research in\nlanguage-based negotiation systems, we explore a novel task of early prediction\nof buyer-seller negotiation outcomes, by varying the fraction of utterances\nthat the model can access. We explore the feasibility of early prediction by\nusing traditional feature-based methods, as well as by incorporating the\nnon-linguistic task context into a pretrained language model using sentence\ntemplates. We further quantify the extent to which linguistic features help in\nmaking better predictions apart from the task-specific price information.\nFinally, probing the pretrained model helps us to identify specific features,\nsuch as trust and agreement, that contribute to the prediction performance.", "published": "2020-04-06 00:49:20", "link": "http://arxiv.org/abs/2004.02363v2", "categories": ["cs.CL", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SelfORE: Self-supervised Relational Feature Learning for Open Relation\n  Extraction", "abstract": "Open relation extraction is the task of extracting open-domain relation facts\nfrom natural language sentences. Existing works either utilize heuristics or\ndistant-supervised annotations to train a supervised classifier over\npre-defined relations, or adopt unsupervised methods with additional\nassumptions that have less discriminative power. In this work, we proposed a\nself-supervised framework named SelfORE, which exploits weak, self-supervised\nsignals by leveraging large pretrained language model for adaptive clustering\non contextualized relational features, and bootstraps the self-supervised\nsignals by improving contextualized features in relation classification.\nExperimental results on three datasets show the effectiveness and robustness of\nSelfORE on open-domain Relation Extraction when comparing with competitive\nbaselines.", "published": "2020-04-06 07:23:17", "link": "http://arxiv.org/abs/2004.02438v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distinguish Confusing Law Articles for Legal Judgment Prediction", "abstract": "Legal Judgment Prediction (LJP) is the task of automatically predicting a law\ncase's judgment results given a text describing its facts, which has excellent\nprospects in judicial assistance systems and convenient services for the\npublic. In practice, confusing charges are frequent, because law cases\napplicable to similar law articles are easily misjudged. For addressing this\nissue, the existing method relies heavily on domain experts, which hinders its\napplication in different law systems. In this paper, we present an end-to-end\nmodel, LADAN, to solve the task of LJP. To distinguish confusing charges, we\npropose a novel graph neural network to automatically learn subtle differences\nbetween confusing law articles and design a novel attention mechanism that\nfully exploits the learned differences to extract compelling discriminative\nfeatures from fact descriptions attentively. Experiments conducted on\nreal-world datasets demonstrate the superiority of our LADAN.", "published": "2020-04-06 11:09:44", "link": "http://arxiv.org/abs/2004.02557v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantum Inspired Word Representation and Computation", "abstract": "Word meaning has different aspects, while the existing word representation\n\"compresses\" these aspects into a single vector, and it needs further analysis\nto recover the information in different dimensions. Inspired by quantum\nprobability, we represent words as density matrices, which are inherently\ncapable of representing mixed states. The experiment shows that the density\nmatrix representation can effectively capture different aspects of word meaning\nwhile maintaining comparable reliability with the vector representation.\nFurthermore, we propose a novel method to combine the coherent summation and\nincoherent summation in the computation of both vectors and density matrices.\nIt achieves consistent improvement on word analogy task.", "published": "2020-04-06 14:37:39", "link": "http://arxiv.org/abs/2004.02705v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job\n  Ontology Expansion", "abstract": "Machine learning plays an ever-bigger part in online recruitment, powering\nintelligent matchmaking and job recommendations across many of the world's\nlargest job platforms. However, the main text is rarely enough to fully\nunderstand a job posting: more often than not, much of the required information\nis condensed into the job title. Several organised efforts have been made to\nmap job titles onto a hand-made knowledge base as to provide this information,\nbut these only cover around 60\\% of online vacancies. We introduce a novel,\npurely data-driven approach towards the detection of new job titles. Our method\nis conceptually simple, extremely efficient and competitive with traditional\nNER-based approaches. Although the standalone application of our method does\nnot outperform a finetuned BERT model, it can be applied as a preprocessing\nstep as well, substantially boosting accuracy across several architectures.", "published": "2020-04-06 16:55:41", "link": "http://arxiv.org/abs/2004.02814v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Code Summarization via a Graph Neural Network", "abstract": "Automatic source code summarization is the task of generating natural\nlanguage descriptions for source code. Automatic code summarization is a\nrapidly expanding research area, especially as the community has taken greater\nadvantage of advances in neural network and AI technologies. In general, source\ncode summarization techniques use the source code as input and outputs a\nnatural language description. Yet a strong consensus is developing that using\nstructural information as input leads to improved performance. The first\napproaches to use structural information flattened the AST into a sequence.\nRecently, more complex approaches based on random AST paths or graph neural\nnetworks have improved on the models using flattened ASTs. However, the\nliterature still does not describe the using a graph neural network together\nwith source code sequence as separate inputs to a model. Therefore, in this\npaper, we present an approach that uses a graph-based neural architecture that\nbetter matches the default structure of the AST to generate these summaries. We\nevaluate our technique using a data set of 2.1 million Java method-comment\npairs and show improvement over four baseline techniques, two from the software\nengineering literature, and two from machine learning literature.", "published": "2020-04-06 17:36:42", "link": "http://arxiv.org/abs/2004.02843v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Speaker-change Aware CRF for Dialogue Act Classification", "abstract": "Recent work in Dialogue Act (DA) classification approaches the task as a\nsequence labeling problem, using neural network models coupled with a\nConditional Random Field (CRF) as the last layer. CRF models the conditional\nprobability of the target DA label sequence given the input utterance sequence.\nHowever, the task involves another important input sequence, that of speakers,\nwhich is ignored by previous work. To address this limitation, this paper\nproposes a simple modification of the CRF layer that takes speaker-change into\naccount. Experiments on the SwDA corpus show that our modified CRF layer\noutperforms the original one, with very wide margins for some DA labels.\nFurther, visualizations demonstrate that our CRF layer can learn meaningful,\nsophisticated transition patterns between DA label pairs conditioned on\nspeaker-change in an end-to-end way. Code is publicly available.", "published": "2020-04-06 18:03:06", "link": "http://arxiv.org/abs/2004.02913v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "abstract": "Natural Language Processing (NLP) has recently achieved great success by\nusing huge pre-trained models with hundreds of millions of parameters. However,\nthese models suffer from heavy model sizes and high latency such that they\ncannot be deployed to resource-limited mobile devices. In this paper, we\npropose MobileBERT for compressing and accelerating the popular BERT model.\nLike the original BERT, MobileBERT is task-agnostic, that is, it can be\ngenerically applied to various downstream NLP tasks via simple fine-tuning.\nBasically, MobileBERT is a thin version of BERT_LARGE, while equipped with\nbottleneck structures and a carefully designed balance between self-attentions\nand feed-forward networks. To train MobileBERT, we first train a specially\ndesigned teacher model, an inverted-bottleneck incorporated BERT_LARGE model.\nThen, we conduct knowledge transfer from this teacher to MobileBERT. Empirical\nstudies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE\nwhile achieving competitive results on well-known benchmarks. On the natural\nlanguage inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6\nlower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD\nv1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n90.0/79.2 (1.5/2.1 higher than BERT_BASE).", "published": "2020-04-06 20:20:58", "link": "http://arxiv.org/abs/2004.02984v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Applying Cyclical Learning Rate to Neural Machine Translation", "abstract": "In training deep learning networks, the optimizer and related learning rate\nare often used without much thought or with minimal tuning, even though it is\ncrucial in ensuring a fast convergence to a good quality minimum of the loss\nfunction that can also generalize well on the test dataset. Drawing inspiration\nfrom the successful application of cyclical learning rate policy for computer\nvision related convolutional networks and datasets, we explore how cyclical\nlearning rate can be applied to train transformer-based neural networks for\nneural machine translation. From our carefully designed experiments, we show\nthat the choice of optimizers and the associated cyclical learning rate policy\ncan have a significant impact on the performance. In addition, we establish\nguidelines when applying cyclical learning rates to neural machine translation\ntasks. Thus with our work, we hope to raise awareness of the importance of\nselecting the right optimizers and the accompanying learning rate policy, at\nthe same time, encourage further research into easy-to-use learning rate\npolicies.", "published": "2020-04-06 04:45:49", "link": "http://arxiv.org/abs/2004.02401v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Joint Embedding of Words and Category Labels for Hierarchical\n  Multi-label Text Classification", "abstract": "Text classification has become increasingly challenging due to the continuous\nrefinement of classification label granularity and the expansion of\nclassification label scale. To address that, some research has been applied\nonto strategies that exploit the hierarchical structure in problems with a\nlarge number of categories. At present, hierarchical text classification (HTC)\nhas received extensive attention and has broad application prospects. Making\nfull use of the relationship between parent category and child category in text\nclassification task can greatly improve the performance of classification. In\nthis paper, We propose a joint embedding of text and parent category based on\nhierarchical fine-tuning ordered neurons LSTM (HFT-ONLSTM) for HTC. Our method\nmakes full use of the connection between the upper-level and lower-level\nlabels. Experiments show that our model outperforms the state-of-the-art\nhierarchical model at a lower computation cost.", "published": "2020-04-06 11:06:08", "link": "http://arxiv.org/abs/2004.02555v3", "categories": ["cs.NE", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.NE"}
{"title": "Data Manipulation: Towards Effective Instance Learning for Neural\n  Dialogue Generation via Learning to Augment and Reweight", "abstract": "Current state-of-the-art neural dialogue models learn from human\nconversations following the data-driven paradigm. As such, a reliable training\ncorpus is the crux of building a robust and well-behaved dialogue model.\nHowever, due to the open-ended nature of human conversations, the quality of\nuser-generated training data varies greatly, and effective training samples are\ntypically insufficient while noisy samples frequently appear. This impedes the\nlearning of those data-driven neural dialogue models. Therefore, effective\ndialogue learning requires not only more reliable learning samples, but also\nfewer noisy samples. In this paper, we propose a data manipulation framework to\nproactively reshape the data distribution towards reliable samples by\naugmenting and highlighting effective learning samples as well as reducing the\neffect of inefficient samples simultaneously. In particular, the data\nmanipulation model selectively augments the training samples and assigns an\nimportance weight to each instance to reform the training data. Note that, the\nproposed data manipulation framework is fully data-driven and learnable. It not\nonly manipulates training samples to optimize the dialogue generation model,\nbut also learns to increase its manipulation skills through gradient descent\nwith validation samples. Extensive experiments show that our framework can\nimprove the dialogue generation performance with respect to various automatic\nevaluation metrics and human judgments.", "published": "2020-04-06 12:14:09", "link": "http://arxiv.org/abs/2004.02594v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous\n  Environments", "abstract": "We develop a language-guided navigation task set in a continuous 3D\nenvironment where agents must execute low-level actions to follow natural\nlanguage navigation directions. By being situated in continuous environments,\nthis setting lifts a number of assumptions implicit in prior work that\nrepresents environments as a sparse graph of panoramas with edges corresponding\nto navigability. Specifically, our setting drops the presumptions of known\nenvironment topologies, short-range oracle navigation, and perfect agent\nlocalization. To contextualize this new task, we develop models that mirror\nmany of the advances made in prior settings as well as single-modality\nbaselines. While some of these techniques transfer, we find significantly lower\nabsolute performance in the continuous setting -- suggesting that performance\nin prior `navigation-graph' settings may be inflated by the strong implicit\nassumptions.", "published": "2020-04-06 17:49:12", "link": "http://arxiv.org/abs/2004.02857v2", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Predicting Strategic Behavior from Free Text", "abstract": "The connection between messaging and action is fundamental both to web\napplications, such as web search and sentiment analysis, and to economics.\nHowever, while prominent online applications exploit messaging in natural\n(human) language in order to predict non-strategic action selection, the\neconomics literature focuses on the connection between structured stylized\nmessaging to strategic decisions in games and multi-agent encounters. This\npaper aims to connect these two strands of research, which we consider highly\ntimely and important due to the vast online textual communication on the web.\nParticularly, we introduce the following question: can free text expressed in\nnatural language serve for the prediction of action selection in an economic\ncontext, modeled as a game?\n  In order to initiate the research on this question, we introduce the study of\nan individual's action prediction in a one-shot game based on free text he/she\nprovides, while being unaware of the game to be played. We approach the problem\nby attributing commonsensical personality attributes via crowd-sourcing to free\ntexts written by individuals, and employing transductive learning to predict\nactions taken by these individuals in one-shot games based on these attributes.\nOur approach allows us to train a single classifier that can make predictions\nwith respect to actions taken in multiple games. In experiments with three\nwell-studied games, our algorithm compares favorably with strong alternative\napproaches. In ablation analysis, we demonstrate the importance of our modeling\nchoices---the representation of the text with the commonsensical personality\nattributes and our classifier---to the predictive power of our model.", "published": "2020-04-06 20:05:30", "link": "http://arxiv.org/abs/2004.02973v2", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
{"title": "Query Focused Multi-Document Summarization with Distant Supervision", "abstract": "We consider the problem of better modeling query-cluster interactions to\nfacilitate query focused multi-document summarization (QFS). Due to the lack of\ntraining data, existing work relies heavily on retrieval-style methods for\nestimating the relevance between queries and text segments. In this work, we\nleverage distant supervision from question answering where various resources\nare available to more explicitly capture the relationship between queries and\ndocuments. We propose a coarse-to-fine modeling framework which introduces\nseparate modules for estimating whether segments are relevant to the query,\nlikely to contain an answer, and central. Under this framework, a trained\nevidence estimator further discerns which retrieved segments might answer the\nquery for final selection in the summary. We demonstrate that our framework\noutperforms strong comparison systems on standard QFS benchmarks.", "published": "2020-04-06 22:35:19", "link": "http://arxiv.org/abs/2004.03027v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning Based Text Classification: A Comprehensive Review", "abstract": "Deep learning based models have surpassed classical machine learning based\napproaches in various text classification tasks, including sentiment analysis,\nnews categorization, question answering, and natural language inference. In\nthis paper, we provide a comprehensive review of more than 150 deep learning\nbased models for text classification developed in recent years, and discuss\ntheir technical contributions, similarities, and strengths. We also provide a\nsummary of more than 40 popular datasets widely used for text classification.\nFinally, we provide a quantitative analysis of the performance of different\ndeep learning models on popular benchmarks, and discuss future research\ndirections.", "published": "2020-04-06 02:00:30", "link": "http://arxiv.org/abs/2004.03705v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Geometry-aware Domain Adaptation for Unsupervised Alignment of Word\n  Embeddings", "abstract": "We propose a novel manifold based geometric approach for learning\nunsupervised alignment of word embeddings between the source and the target\nlanguages. Our approach formulates the alignment learning problem as a domain\nadaptation problem over the manifold of doubly stochastic matrices. This\nviewpoint arises from the aim to align the second order information of the two\nlanguage spaces. The rich geometry of the doubly stochastic manifold allows to\nemploy efficient Riemannian conjugate gradient algorithm for the proposed\nformulation. Empirically, the proposed approach outperforms state-of-the-art\noptimal transport based approach on the bilingual lexicon induction task across\nseveral language pairs. The performance improvement is more significant for\ndistant language pairs.", "published": "2020-04-06 04:41:06", "link": "http://arxiv.org/abs/2004.08243v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Residual Energy-Based Models for Text", "abstract": "Current large-scale auto-regressive language models display impressive\nfluency and can generate convincing text. In this work we start by asking the\nquestion: Can the generations of these models be reliably distinguished from\nreal text by statistical discriminators? We find experimentally that the answer\nis affirmative when we have access to the training data for the model, and\nguardedly affirmative even if we do not.\n  This suggests that the auto-regressive models can be improved by\nincorporating the (globally normalized) discriminators into the generative\nprocess. We give a formalism for this using the Energy-Based Model framework,\nand show that it indeed improves the results of the generative models, measured\nboth in terms of perplexity and in terms of human evaluation.", "published": "2020-04-06 13:44:03", "link": "http://arxiv.org/abs/2004.10188v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DARE: Data Augmented Relation Extraction with GPT-2", "abstract": "Real-world Relation Extraction (RE) tasks are challenging to deal with,\neither due to limited training data or class imbalance issues. In this work, we\npresent Data Augmented Relation Extraction(DARE), a simple method to augment\ntraining data by properly fine-tuning GPT-2 to generate examples for specific\nrelation types. The generated training data is then used in combination with\nthe gold dataset to train a BERT-based RE classifier. In a series of\nexperiments we show the advantages of our method, which leads in improvements\nof up to 11 F1 score points against a strong base-line. Also, DARE achieves new\nstate of the art in three widely used biomedical RE datasets surpassing the\nprevious best results by 4.7 F1 points on average.", "published": "2020-04-06 14:38:36", "link": "http://arxiv.org/abs/2004.13845v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Character-level Japanese Text Generation with Attention Mechanism for\n  Chest Radiography Diagnosis", "abstract": "Chest radiography is a general method for diagnosing a patient's condition\nand identifying important information; therefore, radiography is used\nextensively in routine medical practice in various situations, such as\nemergency medical care and medical checkup. However, a high level of expertise\nis required to interpret chest radiographs. Thus, medical specialists spend\nconsiderable time in diagnosing such huge numbers of radiographs. In order to\nsolve these problems, methods for generating findings have been proposed.\nHowever, the study of generating chest radiograph findings has primarily\nfocused on the English language, and to the best of our knowledge, no studies\nhave studied Japanese data on this subject. There are two challenges involved\nin generating findings in the Japanese language. The first challenge is that\nword splitting is difficult because the boundaries of Japanese word are not\nclear. The second challenge is that there are numerous orthographic variants.\nFor deal with these two challenges, we proposed an end-to-end model that\ngenerates Japanese findings at the character-level from chest radiographs. In\naddition, we introduced the attention mechanism to improve not only the\naccuracy, but also the interpretation ability of the results. We evaluated the\nproposed method using a public dataset with Japanese findings. The\neffectiveness of the proposed method was confirmed using the Bilingual\nEvaluation Understudy score. And, we were confirmed from the generated findings\nthat the proposed method was able to consider the orthographic variants.\nFurthermore, we confirmed via visual inspection that the attention mechanism\ncaptures the features and positional information of radiographs.", "published": "2020-04-06 18:19:27", "link": "http://arxiv.org/abs/2004.13846v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Multilayer Perceptrons for Dimensional Speech Emotion Recognition", "abstract": "Modern deep learning architectures are ordinarily performed on\nhigh-performance computing facilities due to the large size of the input\nfeatures and complexity of its model. This paper proposes traditional\nmultilayer perceptrons (MLP) with deep layers and small input size to tackle\nthat computation requirement limitation. The result shows that our proposed\ndeep MLP outperformed modern deep learning architectures, i.e., LSTM and CNN,\non the same number of layers and value of parameters. The deep MLP exhibited\nthe highest performance on both speaker-dependent and speaker-independent\nscenarios on IEMOCAP and MSP-IMPROV corpus.", "published": "2020-04-06 00:03:43", "link": "http://arxiv.org/abs/2004.02355v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Leveraging GANs to Improve Continuous Path Keyboard Input Models", "abstract": "Continuous path keyboard input has higher inherent ambiguity than standard\ntapping, because the path trace may exhibit not only local\novershoots/undershoots (as in tapping) but also, depending on the user,\nsubstantial mid-path excursions. Deploying a robust solution thus requires a\nlarge amount of high-quality training data, which is difficult to\ncollect/annotate. In this work, we address this challenge by using GANs to\naugment our training corpus with user-realistic synthetic data. Experiments\nshow that, even though GAN-generated data does not capture all the\ncharacteristics of real user data, it still provides a substantial boost in\naccuracy at a 5:1 GAN-to-real ratio. GANs therefore inject more robustness in\nthe model through greatly increased word coverage and path diversity.", "published": "2020-04-06 22:42:29", "link": "http://arxiv.org/abs/2004.07800v2", "categories": ["cs.HC", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Simultaneous Denoising and Dereverberation Using Deep Embedding Features", "abstract": "Monaural speech dereverberation is a very challenging task because no spatial\ncues can be used. When the additive noises exist, this task becomes more\nchallenging. In this paper, we propose a joint training method for simultaneous\nspeech denoising and dereverberation using deep embedding features, which is\nbased on the deep clustering (DC). DC is a state-of-the-art method for speech\nseparation that includes embedding learning and K-means clustering. As for our\nproposed method, it contains two stages: denoising and dereverberation. At the\ndenoising stage, the DC network is leveraged to extract noise-free deep\nembedding features. These embedding features are generated from the anechoic\nspeech and residual reverberation signals. They can represent the inferred\nspectral masking patterns of the desired signals, which are discriminative\nfeatures. At the dereverberation stage, instead of using the unsupervised\nK-means clustering algorithm, another supervised neural network is utilized to\nestimate the anechoic speech from these deep embedding features. Finally, the\ndenoising stage and dereverberation stage are optimized by the joint training\nmethod. Experimental results show that the proposed method outperforms the WPE\nand BLSTM baselines, especially in the low SNR condition.", "published": "2020-04-06 06:34:01", "link": "http://arxiv.org/abs/2004.02420v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A bio-inspired geometric model for sound reconstruction", "abstract": "The reconstruction mechanisms built by the human auditory system during sound\nreconstruction are still a matter of debate. The purpose of this study is to\npropose a mathematical model of sound reconstruction based on the functional\narchitecture of the auditory cortex (A1). The model is inspired by the\ngeometrical modelling of vision, which has undergone a great development in the\nlast ten years. There are however fundamental dissimilarities, due to the\ndifferent role played by the time and the different group of symmetries. The\nalgorithm transforms the degraded sound in an 'image' in the time-frequency\ndomain via a short-time Fourier transform. Such an image is then lifted in the\nHeisenberg group and it is reconstructed via a Wilson-Cowan differo-integral\nequation. Preliminary numerical experiments are provided, showing the good\nreconstruction properties of the algorithm on synthetic sounds concentrated\naround two frequencies.", "published": "2020-04-06 07:47:32", "link": "http://arxiv.org/abs/2004.02450v2", "categories": ["eess.AS", "math.AP", "math.OC", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "Vocoder-Based Speech Synthesis from Silent Videos", "abstract": "Both acoustic and visual information influence human perception of speech.\nFor this reason, the lack of audio in a video sequence determines an extremely\nlow speech intelligibility for untrained lip readers. In this paper, we present\na way to synthesise speech from the silent video of a talker using deep\nlearning. The system learns a mapping function from raw video frames to\nacoustic features and reconstructs the speech with a vocoder synthesis\nalgorithm. To improve speech reconstruction performance, our model is also\ntrained to predict text information in a multi-task learning fashion and it is\nable to simultaneously reconstruct and recognise speech in real time. The\nresults in terms of estimated speech quality and intelligibility show the\neffectiveness of our method, which exhibits an improvement over existing\nvideo-to-speech approaches.", "published": "2020-04-06 10:22:04", "link": "http://arxiv.org/abs/2004.02541v2", "categories": ["eess.AS", "cs.CV", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Meta-Learning for Short Utterance Speaker Recognition with Imbalance\n  Length Pairs", "abstract": "In practical settings, a speaker recognition system needs to identify a\nspeaker given a short utterance, while the enrollment utterance may be\nrelatively long. However, existing speaker recognition models perform poorly\nwith such short utterances. To solve this problem, we introduce a meta-learning\nframework for imbalance length pairs. Specifically, we use a Prototypical\nNetworks and train it with a support set of long utterances and a query set of\nshort utterances of varying lengths. Further, since optimizing only for the\nclasses in the given episode may be insufficient for learning discriminative\nembeddings for unseen classes, we additionally enforce the model to classify\nboth the support and the query set against the entire set of classes in the\ntraining set. By combining these two learning schemes, our model outperforms\nexisting state-of-the-art speaker verification models learned with a standard\nsupervised learning framework on short utterance (1-2 seconds) on the VoxCeleb\ndatasets. We also validate our proposed model for unseen speaker\nidentification, on which it also achieves significant performance gains over\nthe existing approaches. The codes are available at\nhttps://github.com/seongmin-kye/meta-SR.", "published": "2020-04-06 17:53:14", "link": "http://arxiv.org/abs/2004.02863v5", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Probabilistic embeddings for speaker diarization", "abstract": "Speaker embeddings (x-vectors) extracted from very short segments of speech\nhave recently been shown to give competitive performance in speaker\ndiarization. We generalize this recipe by extracting from each speech segment,\nin parallel with the x-vector, also a diagonal precision matrix, thus providing\na path for the propagation of information about the quality of the speech\nsegment into a PLDA scoring backend. These precisions quantify the uncertainty\nabout what the values of the embeddings might have been if they had been\nextracted from high quality speech segments. The proposed probabilistic\nembeddings (x-vectors with precisions) are interfaced with the PLDA model by\ntreating the x-vectors as hidden variables and marginalizing them out. We apply\nthe proposed probabilistic embeddings as input to an agglomerative hierarchical\nclustering (AHC) algorithm to do diarization in the DIHARD'19 evaluation set.\nWe compute the full PLDA likelihood 'by the book' for each clustering\nhypothesis that is considered by AHC. We do joint discriminative training of\nthe PLDA parameters and of the probabilistic x-vector extractor. We demonstrate\naccuracy gains relative to a baseline AHC algorithm, applied to traditional\nxvectors (without uncertainty), and which uses averaging of binary\nlog-likelihood-ratios, rather than by-the-book scoring.", "published": "2020-04-06 14:51:01", "link": "http://arxiv.org/abs/2004.04096v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "WaveCRN: An Efficient Convolutional Recurrent Neural Network for\n  End-to-end Speech Enhancement", "abstract": "Due to the simple design pipeline, end-to-end (E2E) neural models for speech\nenhancement (SE) have attracted great interest. In order to improve the\nperformance of the E2E model, the locality and temporal sequential properties\nof speech should be efficiently taken into account when modelling. However, in\nmost current E2E models for SE, these properties are either not fully\nconsidered or are too complex to be realized. In this paper, we propose an\nefficient E2E SE model, termed WaveCRN. In WaveCRN, the speech locality feature\nis captured by a convolutional neural network (CNN), while the temporal\nsequential property of the locality feature is modeled by stacked simple\nrecurrent units (SRU). Unlike a conventional temporal sequential model that\nuses a long short-term memory (LSTM) network, which is difficult to\nparallelize, SRU can be efficiently parallelized in calculation with even fewer\nmodel parameters. In addition, in order to more effectively suppress the noise\ncomponents in the input noisy speech, we derive a novel restricted feature\nmasking (RFM) approach that performs enhancement on the feature maps in the\nhidden layers; this is different from the approach that applies the estimated\nratio mask on the noisy spectral features, which is commonly used in speech\nseparation methods. Experimental results on speech denoising and compressed\nspeech restoration tasks confirm that with the lightweight architecture of SRU\nand the feature-mapping-based RFM, WaveCRN performs comparably with other\nstate-of-the-art approaches with notably reduced model complexity and inference\ntime.", "published": "2020-04-06 13:48:05", "link": "http://arxiv.org/abs/2004.04098v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences", "abstract": "Attention is a commonly used mechanism in sequence processing, but it is of\nO(n^2) complexity which prevents its application to long sequences. The\nrecently introduced neural Shuffle-Exchange network offers a\ncomputation-efficient alternative, enabling the modelling of long-range\ndependencies in O(n log n) time. The model, however, is quite complex,\ninvolving a sophisticated gating mechanism derived from the Gated Recurrent\nUnit. In this paper, we present a simple and lightweight variant of the\nShuffle-Exchange network, which is based on a residual network employing GELU\nand Layer Normalization. The proposed architecture not only scales to longer\nsequences but also converges faster and provides better accuracy. It surpasses\nthe Shuffle-Exchange network on the LAMBADA language modelling task and\nachieves state-of-the-art performance on the MusicNet dataset for music\ntranscription while being efficient in the number of parameters. We show how to\ncombine the improved Shuffle-Exchange network with convolutional layers,\nestablishing it as a useful building block in long sequence processing\napplications.", "published": "2020-04-06 12:44:22", "link": "http://arxiv.org/abs/2004.04662v4", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
