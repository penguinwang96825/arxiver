{"title": "AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance\n  Detection for Fact Checking", "abstract": "With the continuing spread of misinformation and disinformation online, it is\nof increasing importance to develop combating mechanisms at scale in the form\nof automated systems that support multiple languages. One task of interest is\nclaim veracity prediction, which can be addressed using stance detection with\nrespect to relevant documents retrieved online. To this end, we present our new\nArabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from\na diverse set of sources comprising three fact-checking websites and one news\nwebsite. AraStance covers false and true claims from multiple domains (e.g.,\npolitics, sports, health) and several Arab countries, and it is well-balanced\nbetween related and unrelated documents with respect to the claims. We\nbenchmark AraStance, along with two other stance detection datasets, using a\nnumber of BERT-based models. Our best model achieves an accuracy of 85\\% and a\nmacro F1 score of 78\\%, which leaves room for improvement and reflects the\nchallenging nature of AraStance and the task of stance detection in general.", "published": "2021-04-28 03:38:24", "link": "http://arxiv.org/abs/2104.13559v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-view Inference for Relation Extraction with Uncertain Knowledge", "abstract": "Knowledge graphs (KGs) are widely used to facilitate relation extraction (RE)\ntasks. While most previous RE methods focus on leveraging deterministic KGs,\nuncertain KGs, which assign a confidence score for each relation instance, can\nprovide prior probability distributions of relational facts as valuable\nexternal knowledge for RE models. This paper proposes to exploit uncertain\nknowledge to improve relation extraction. Specifically, we introduce ProBase,\nan uncertain KG that indicates to what extent a target entity belongs to a\nconcept, into our RE architecture. We then design a novel multi-view inference\nframework to systematically integrate local context and global knowledge across\nthree views: mention-, entity- and concept-view. The experimental results show\nthat our model achieves competitive performances on both sentence- and\ndocument-level relation extraction, which verifies the effectiveness of\nintroducing uncertain knowledge and the multi-view inference framework that we\ndesign.", "published": "2021-04-28 05:56:33", "link": "http://arxiv.org/abs/2104.13579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SELF & FEIL: Emotion and Intensity Lexicons for Finnish", "abstract": "This paper introduces a Sentiment and Emotion Lexicon for Finnish (SELF) and\na Finnish Emotion Intensity Lexicon (FEIL). We describe the lexicon creation\nprocess and evaluate the lexicon using some commonly available tools. The\nlexicon uses annotations projected from the NRC Emotion Lexicon with carefully\nedited translations. To our knowledge, this is the first comprehensive\nsentiment and emotion lexicon for Finnish.", "published": "2021-04-28 10:28:35", "link": "http://arxiv.org/abs/2104.13691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with\n  Many Symbols", "abstract": "Probabilistic context-free grammars (PCFGs) with neural parameterization have\nbeen shown to be effective in unsupervised phrase-structure grammar induction.\nHowever, due to the cubic computational complexity of PCFG representation and\nparsing, previous approaches cannot scale up to a relatively large number of\n(nonterminal and preterminal) symbols. In this work, we present a new\nparameterization form of PCFGs based on tensor decomposition, which has at most\nquadratic computational complexity in the symbol number and therefore allows us\nto use a much larger number of symbols. We further use neural parameterization\nfor the new form to improve unsupervised parsing performance. We evaluate our\nmodel across ten languages and empirically demonstrate the effectiveness of\nusing more symbols. Our code: https://github.com/sustcsonglin/TN-PCFG", "published": "2021-04-28 12:25:27", "link": "http://arxiv.org/abs/2104.13727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation\n  Extraction", "abstract": "Contrastive learning has been used to learn a high-quality representation of\nthe image in computer vision. However, contrastive learning is not widely\nutilized in natural language processing due to the lack of a general method of\ndata augmentation for text data. In this work, we explore the method of\nemploying contrastive learning to improve the text representation from the BERT\nmodel for relation extraction. The key knob of our framework is a unique\ncontrastive pre-training step tailored for the relation extraction tasks by\nseamlessly integrating linguistic knowledge into the data augmentation.\nFurthermore, we investigate how large-scale data constructed from the external\nknowledge bases can enhance the generality of contrastive pre-training of BERT.\nThe experimental results on three relation extraction benchmark datasets\ndemonstrate that our method can improve the BERT model representation and\nachieve state-of-the-art performance. In addition, we explore the\ninterpretability of models by showing that BERT with contrastive pre-training\nrelies more on rationales for prediction. Our code and data are publicly\navailable at: https://github.com/udel-biotm-lab/BERT-CLRE.", "published": "2021-04-28 17:50:24", "link": "http://arxiv.org/abs/2104.13913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MelBERT: Metaphor Detection via Contextualized Late Interaction using\n  Metaphorical Identification Theories", "abstract": "Automated metaphor detection is a challenging task to identify metaphorical\nexpressions of words in a sentence. To tackle this problem, we adopt\npre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we\npropose a novel metaphor detection model, namely metaphor-aware late\ninteraction over BERT (MelBERT). Our model not only leverages contextualized\nword representation but also benefits from linguistic metaphor identification\ntheories to distinguish between the contextual and literal meaning of words.\nOur empirical results demonstrate that MelBERT outperforms several strong\nbaselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.", "published": "2021-04-28 07:52:01", "link": "http://arxiv.org/abs/2104.13615v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Document Representations for Content-based Legal Literature\n  Recommendations", "abstract": "Recommender systems assist legal professionals in finding relevant literature\nfor supporting their case. Despite its importance for the profession, legal\napplications do not reflect the latest advances in recommender systems and\nrepresentation learning research. Simultaneously, legal recommender systems are\ntypically evaluated in small-scale user study without any public available\nbenchmark datasets. Thus, these studies have limited reproducibility. To\naddress the gap between research and practice, we explore a set of\nstate-of-the-art document representation methods for the task of retrieving\nsemantically related US case law. We evaluate text-based (e.g., fastText,\nTransformers), citation-based (e.g., DeepWalk, Poincar\\'e), and hybrid methods.\nWe compare in total 27 methods using two silver standards with annotations for\n2,964 documents. The silver standards are newly created from Open Case Book and\nWikisource and can be reused under an open license facilitating\nreproducibility. Our experiments show that document representations from\naveraged fastText word vectors (trained on legal corpora) yield the best\nresults, closely followed by Poincar\\'e citation embeddings. Combining fastText\nand Poincar\\'e in a hybrid manner further improves the overall result. Besides\nthe overall performance, we analyze the methods depending on document length,\ncitation count, and the coverage of their recommendations. We make our source\ncode, models, and datasets publicly available at\nhttps://github.com/malteos/legal-document-similarity/.", "published": "2021-04-28 15:48:19", "link": "http://arxiv.org/abs/2104.13841v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Removing Word-Level Spurious Alignment between Images and\n  Pseudo-Captions in Unsupervised Image Captioning", "abstract": "Unsupervised image captioning is a challenging task that aims at generating\ncaptions without the supervision of image-sentence pairs, but only with images\nand sentences drawn from different sources and object labels detected from the\nimages. In previous work, pseudo-captions, i.e., sentences that contain the\ndetected object labels, were assigned to a given image. The focus of the\nprevious work was on the alignment of input images and pseudo-captions at the\nsentence level. However, pseudo-captions contain many words that are irrelevant\nto a given image. In this work, we investigate the effect of removing\nmismatched words from image-sentence alignment to determine how they make this\ntask difficult. We propose a simple gating mechanism that is trained to align\nimage features with only the most reliable words in pseudo-captions: the\ndetected object labels. The experimental results show that our proposed method\noutperforms the previous methods without introducing complex sentence-level\nlearning objectives. Combined with the sentence-level alignment method of\nprevious work, our method further improves its performance. These results\nconfirm the importance of careful alignment in word-level details.", "published": "2021-04-28 16:36:52", "link": "http://arxiv.org/abs/2104.13872v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Syntax from Naturally-Occurring Bracketings", "abstract": "Naturally-occurring bracketings, such as answer fragments to natural language\nquestions and hyperlinks on webpages, can reflect human syntactic intuition\nregarding phrasal boundaries. Their availability and approximate correspondence\nto syntax make them appealing as distant information sources to incorporate\ninto unsupervised constituency parsing. But they are noisy and incomplete; to\naddress this challenge, we develop a partial-brackets-aware structured ramp\nloss in learning. Experiments demonstrate that our distantly-supervised models\ntrained on naturally-occurring bracketing data are more accurate in inducing\nsyntactic structures than competing unsupervised systems. On the English WSJ\ncorpus, our models achieve an unlabeled F1 score of 68.9 for constituency\nparsing.", "published": "2021-04-28 18:00:02", "link": "http://arxiv.org/abs/2104.13933v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning of Query Intent and Named Entities using Transfer\n  Learning", "abstract": "Named entity recognition (NER) has been studied extensively and the earlier\nalgorithms were based on sequence labeling like Hidden Markov Models (HMM) and\nconditional random fields (CRF). These were followed by neural network based\ndeep learning models. Recently, BERT has shown new state of the art accuracy in\nsequence labeling tasks like NER. In this short article, we study various\napproaches to task specific NER. Task specific NER has two components -\nidentifying the intent of a piece of text (like search queries), and then\nlabeling the query with task specific named entities. For example, we consider\nthe task of labeling Target store locations in a search query (which could be\nentered in a search box or spoken in a device like Alexa or Google Home). Store\nlocations are highly ambiguous and sometimes it is difficult to differentiate\nbetween say a location and a non-location. For example, \"pickup my order at\norange store\" has \"orange\" as the store location, while \"buy orange at target\"\nhas \"orange\" as a fruit. We explore this difficulty by doing multi-task\nlearning which we call global to local transfer of information. We jointly\nlearn the query intent (i.e. store lookup) and the named entities by using\nmultiple loss functions in our BERT based model and find interesting results.", "published": "2021-04-28 23:59:00", "link": "http://arxiv.org/abs/2105.03316v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Evolution of Rumors on a Closed Platform during COVID-19", "abstract": "In this work we looked into a dataset of 114 thousands of suspicious messages\ncollected from the most popular closed messaging platform in Taiwan between\nJanuary and July, 2020. We proposed an hybrid algorithm that could efficiently\ncluster a large number of text messages according their topics and narratives.\nThat is, we obtained groups of messages that are within a limited content\nalterations within each other. By employing the algorithm to the dataset, we\nwere able to look at the content alterations and the temporal dynamics of each\nparticular rumor over time. With qualitative case studies of three COVID-19\nrelated rumors, we have found that key authoritative figures were often\nmisquoted in false information. It was an effective measure to increase the\npopularity of one false information. In addition, fact-check was not effective\nin stopping misinformation from getting attention. In fact, the popularity of\none false information was often more influenced by major societal events and\neffective content alterations.", "published": "2021-04-28 15:04:22", "link": "http://arxiv.org/abs/2104.13816v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Diversity-Aware Batch Active Learning for Dependency Parsing", "abstract": "While the predictive performance of modern statistical dependency parsers\nrelies heavily on the availability of expensive expert-annotated treebank data,\nnot all annotations contribute equally to the training of the parsers. In this\npaper, we attempt to reduce the number of labeled examples needed to train a\nstrong dependency parser using batch active learning (AL). In particular, we\ninvestigate whether enforcing diversity in the sampled batches, using\ndeterminantal point processes (DPPs), can improve over their diversity-agnostic\ncounterparts. Simulation experiments on an English newswire corpus show that\nselecting diverse batches with DPPs is superior to strong selection strategies\nthat do not enforce batch diversity, especially during the initial stages of\nthe learning process. Additionally, our diversityaware strategy is robust under\na corpus duplication setting, where diversity-agnostic sampling strategies\nexhibit significant degradation.", "published": "2021-04-28 18:00:05", "link": "http://arxiv.org/abs/2104.13936v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Neuromorphic Computing is Turing-Complete", "abstract": "Neuromorphic computing is a non-von Neumann computing paradigm that performs\ncomputation by emulating the human brain. Neuromorphic systems are extremely\nenergy-efficient and known to consume thousands of times less power than CPUs\nand GPUs. They have the potential to drive critical use cases such as\nautonomous vehicles, edge computing and internet of things in the future. For\nthis reason, they are sought to be an indispensable part of the future\ncomputing landscape. Neuromorphic systems are mainly used for spike-based\nmachine learning applications, although there are some non-machine learning\napplications in graph theory, differential equations, and spike-based\nsimulations. These applications suggest that neuromorphic computing might be\ncapable of general-purpose computing. However, general-purpose computability of\nneuromorphic computing has not been established yet. In this work, we prove\nthat neuromorphic computing is Turing-complete and therefore capable of\ngeneral-purpose computing. Specifically, we present a model of neuromorphic\ncomputing, with just two neuron parameters (threshold and leak), and two\nsynaptic parameters (weight and delay). We devise neuromorphic circuits for\ncomputing all the {\\mu}-recursive functions (i.e., constant, successor and\nprojection functions) and all the {\\mu}-recursive operators (i.e., composition,\nprimitive recursion and minimization operators). Given that the {\\mu}-recursive\nfunctions and operators are precisely the ones that can be computed using a\nTuring machine, this work establishes the Turing-completeness of neuromorphic\ncomputing.", "published": "2021-04-28 19:25:01", "link": "http://arxiv.org/abs/2104.13983v1", "categories": ["cs.NE", "cs.CC", "cs.CL", "cs.ET", "68Q07, 03D10", "F.1.1; D.1.m"], "primary_category": "cs.NE"}
{"title": "AMSS-Net: Audio Manipulation on User-Specified Sources with Textual\n  Queries", "abstract": "This paper proposes a neural network that performs audio transformations to\nuser-specified sources (e.g., vocals) of a given audio track according to a\ngiven description while preserving other sources not mentioned in the\ndescription. Audio Manipulation on a Specific Source (AMSS) is challenging\nbecause a sound object (i.e., a waveform sample or frequency bin) is\n`transparent'; it usually carries information from multiple sources, in\ncontrast to a pixel in an image. To address this challenging problem, we\npropose AMSS-Net, which extracts latent sources and selectively manipulates\nthem while preserving irrelevant sources. We also propose an evaluation\nbenchmark for several AMSS tasks, and we show that AMSS-Net outperforms\nbaselines on several AMSS tasks via objective metrics and empirical\nverification.", "published": "2021-04-28 03:27:01", "link": "http://arxiv.org/abs/2104.13553v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "IDMT-Traffic: An Open Benchmark Dataset for Acoustic Traffic Monitoring\n  Research", "abstract": "In many urban areas, traffic load and noise pollution are constantly\nincreasing. Automated systems for traffic monitoring are promising\ncountermeasures, which allow to systematically quantify and predict local\ntraffic flow in order to to support municipal traffic planning decisions. In\nthis paper, we present a novel open benchmark dataset, containing 2.5 hours of\nstereo audio recordings of 4718 vehicle passing events captured with both\nhigh-quality sE8 and medium-quality MEMS microphones. This dataset is well\nsuited to evaluate the use-case of deploying audio classification algorithms to\nembedded sensor devices with restricted microphone quality and hardware\nprocessing power. In addition, this paper provides a detailed review of recent\nacoustic traffic monitoring (ATM) algorithms as well as the results of two\nbenchmark experiments on vehicle type classification and direction of movement\nestimation using four state-of-the-art convolutional neural network\narchitectures.", "published": "2021-04-28 07:58:37", "link": "http://arxiv.org/abs/2104.13620v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized Keyphrase Detection using Speaker and Environment\n  Information", "abstract": "In this paper, we introduce a streaming keyphrase detection system that can\nbe easily customized to accurately detect any phrase composed of words from a\nlarge vocabulary. The system is implemented with an end-to-end trained\nautomatic speech recognition (ASR) model and a text-independent speaker\nverification model. To address the challenge of detecting these keyphrases\nunder various noisy conditions, a speaker separation model is added to the\nfeature frontend of the speaker verification model, and an adaptive noise\ncancellation (ANC) algorithm is included to exploit cross-microphone noise\ncoherence. Our experiments show that the text-independent speaker verification\nmodel largely reduces the false triggering rate of the keyphrase detection,\nwhile the speaker separation model and adaptive noise cancellation largely\nreduce false rejections.", "published": "2021-04-28 18:50:19", "link": "http://arxiv.org/abs/2104.13970v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
