{"title": "EA$^2$E: Improving Consistency with Event Awareness for Document-Level\n  Argument Extraction", "abstract": "Events are inter-related in documents. Motivated by the\none-sense-per-discourse theory, we hypothesize that a participant tends to play\nconsistent roles across multiple events in the same document. However recent\nwork on document-level event argument extraction models each individual event\nin isolation and therefore causes inconsistency among extracted arguments\nacross events, which will further cause discrepancy for downstream applications\nsuch as event knowledge base population, question answering, and hypothesis\ngeneration. In this work, we formulate event argument consistency as the\nconstraints from event-event relations under the document-level setting. To\nimprove consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)\nmodel with augmented context for training and inference. Experiment results on\nWIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E\ncompared to baseline methods.", "published": "2022-05-30 04:33:51", "link": "http://arxiv.org/abs/2205.14847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language\n  Understanding and Generation", "abstract": "Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale\npretraining language models. However, the prior seq2seq pretraining models\ngenerally focus on reconstructive objectives on the decoder side and neglect\nthe effect of encoder-side supervision, which we argue may lead to sub-optimal\nperformance. To verify our hypothesis, we first empirically study the\nfunctionalities of the encoder and decoder in seq2seq pretrained language\nmodels, and find that the encoder takes an important but under-exploitation\nrole than the decoder regarding the downstream performance and neuron\nactivation. Therefore, we propose an encoding-enhanced seq2seq pretraining\nstrategy, namely E2S2, which improves the seq2seq models via integrating more\nefficient self-supervised information into the encoders. Specifically, E2S2\nadopts two self-supervised objectives on the encoder side from two aspects: 1)\nlocally denoising the corrupted sentence (denoising objective); and 2) globally\nlearning better sentence representations (contrastive objective). With the help\nof both objectives, the encoder can effectively distinguish the noise tokens\nand capture high-level (i.e., syntactic and semantic) knowledge, thus\nstrengthening the ability of seq2seq model to accurately achieve the\nconditional generation. On a large diversity of downstream natural language\nunderstanding and generation tasks, E2S2 dominantly improves the performance of\nits powerful backbone models, e.g., BART and T5. For example, upon BART\nbackbone, we achieve +1.1% averaged gain on the general language understanding\nevaluation (GLUE) benchmark and +1.75% F_0.5 score improvement on CoNLL2014\ndataset. We also provide in-depth analyses to show the improvement stems from\nbetter linguistic representation. We hope that our work will foster future\nself-supervision research on seq2seq language model pretraining.", "published": "2022-05-30 08:25:36", "link": "http://arxiv.org/abs/2205.14912v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual\n  Open-retrieval Question Answering System", "abstract": "This paper introduces our proposed system for the MIA Shared Task on\nCross-lingual Open-retrieval Question Answering (COQA). In this challenging\nscenario, given an input question the system has to gather evidence documents\nfrom a multilingual pool and generate from them an answer in the language of\nthe question. We devised several approaches combining different model variants\nfor three main components: Data Augmentation, Passage Retrieval, and Answer\nGeneration. For passage retrieval, we evaluated the monolingual BM25 ranker\nagainst the ensemble of re-rankers based on multilingual pretrained language\nmodels (PLMs) and also variants of the shared task baseline, re-training it\nfrom scratch using a recently introduced contrastive loss that maintains a\nstrong gradient signal throughout training by means of mixed negative samples.\nFor answer generation, we focused on language- and domain-specialization by\nmeans of continued language model (LM) pretraining of existing multilingual\nencoders. Additionally, for both passage retrieval and answer generation, we\naugmented the training data provided by the task organizers with automatically\ngenerated question-answer pairs created from Wikipedia passages to mitigate the\nissue of data scarcity, particularly for the low-resource languages for which\nno training data were provided. Our results show that language- and\ndomain-specialization as well as data augmentation help, especially for\nlow-resource languages.", "published": "2022-05-30 10:31:08", "link": "http://arxiv.org/abs/2205.14981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents", "abstract": "The number of scientific publications nowadays is rapidly increasing, causing\ninformation overload for researchers and making it hard for scholars to keep up\nto date with current trends and lines of work. Consequently, recent work on\napplying text mining technologies for scholarly publications has investigated\nthe application of automatic text summarization technologies, including extreme\nsummarization, for this domain. However, previous work has concentrated only on\nmonolingual settings, primarily in English. In this paper, we fill this\nresearch gap and present an abstractive cross-lingual summarization dataset for\nfour different languages in the scholarly domain, which enables us to train and\nevaluate models that process English papers and generate summaries in German,\nItalian, Chinese and Japanese. We present our new X-SCITLDR dataset for\nmultilingual summarization and thoroughly benchmark different models based on a\nstate-of-the-art multilingual pre-trained model, including a two-stage\n`summarize and translate' approach and a direct cross-lingual model. We\nadditionally explore the benefits of intermediate-stage training using English\nmonolingual summarization and machine translation as intermediate tasks and\nanalyze performance in zero- and few-shot scenarios.", "published": "2022-05-30 12:31:28", "link": "http://arxiv.org/abs/2205.15051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue\n  Systems", "abstract": "In this paper, we present Duplex Conversation, a multi-turn, multimodal\nspoken dialogue system that enables telephone-based agents to interact with\ncustomers like a human. We use the concept of full-duplex in telecommunication\nto demonstrate what a human-like interactive experience should be and how to\nachieve smooth turn-taking through three subtasks: user state detection,\nbackchannel selection, and barge-in detection. Besides, we propose\nsemi-supervised learning with multimodal data augmentation to leverage\nunlabeled data to increase model generalization. Experimental results on three\nsub-tasks show that the proposed method achieves consistent improvements\ncompared with baselines. We deploy the Duplex Conversation to Alibaba\nintelligent customer service and share lessons learned in production. Online\nA/B experiments show that the proposed system can significantly reduce response\nlatency by 50%.", "published": "2022-05-30 12:41:23", "link": "http://arxiv.org/abs/2205.15060v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Billions of Parameters Are Worth More Than In-domain Training Data: A\n  case study in the Legal Case Entailment Task", "abstract": "Recent work has shown that language models scaled to billions of parameters,\nsuch as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In\nthis work, we experiment with zero-shot models in the legal case entailment\ntask of the COLIEE 2022 competition. Our experiments show that scaling the\nnumber of parameters in a language model improves the F1 score of our previous\nzero-shot result by more than 6 points, suggesting that stronger zero-shot\ncapability may be a characteristic of larger models, at least for this task.\nOur 3B-parameter zero-shot model outperforms all models, including ensembles,\nin the COLIEE 2021 test set and also achieves the best performance of a single\nmodel in the COLIEE 2022 competition, second only to the ensemble composed of\nthe 3B model itself and a smaller version of the same model. Despite the\nchallenges posed by large language models, mainly due to latency constraints in\nreal-time applications, we provide a demonstration of our zero-shot monoT5-3b\nmodel being used in production as a search engine, including for legal\ndocuments. The code for our submission and the demo of our system are available\nat https://github.com/neuralmind-ai/coliee and\nhttps://neuralsearchx.neuralmind.ai, respectively.", "published": "2022-05-30 15:21:26", "link": "http://arxiv.org/abs/2205.15172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey in Mathematical Language Processing", "abstract": "Informal mathematical text underpins real-world quantitative reasoning and\ncommunication. Developing sophisticated methods of retrieval and abstraction\nfrom this dual modality is crucial in the pursuit of the vision of automating\ndiscovery in quantitative science and mathematics. We track the development of\ninformal mathematical language processing approaches across five strategic\nsub-areas in recent years, highlighting the prevailing successful\nmethodological elements along with existing limitations.", "published": "2022-05-30 16:41:58", "link": "http://arxiv.org/abs/2205.15231v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Transformer be Too Compositional? Analysing Idiom Processing in\n  Neural Machine Translation", "abstract": "Unlike literal expressions, idioms' meanings do not directly follow from\ntheir parts, posing a challenge for neural machine translation (NMT). NMT\nmodels are often unable to translate idioms accurately and over-generate\ncompositional, literal translations. In this work, we investigate whether the\nnon-compositionality of idioms is reflected in the mechanics of the dominant\nNMT model, Transformer, by analysing the hidden states and attention patterns\nfor models with English as source language and one of seven European languages\nas target language. When Transformer emits a non-literal translation - i.e.\nidentifies the expression as idiomatic - the encoder processes idioms more\nstrongly as single lexical units compared to literal expressions. This\nmanifests in idioms' parts being grouped through attention and in reduced\ninteraction between idioms and their context. In the decoder's cross-attention,\nfigurative inputs result in reduced attention on source-side tokens. These\nresults suggest that Transformer's tendency to process idioms as compositional\nexpressions contributes to literal translations of idioms.", "published": "2022-05-30 17:59:32", "link": "http://arxiv.org/abs/2205.15301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Modality Robustness in Multimodal Sentiment Analysis", "abstract": "Building robust multimodal models are crucial for achieving reliable\ndeployment in the wild. Despite its importance, less attention has been paid to\nidentifying and improving the robustness of Multimodal Sentiment Analysis (MSA)\nmodels. In this work, we hope to address that by (i) Proposing simple\ndiagnostic checks for modality robustness in a trained multimodal model. Using\nthese checks, we find MSA models to be highly sensitive to a single modality,\nwhich creates issues in their robustness; (ii) We analyze well-known robust\ntraining strategies to alleviate the issues. Critically, we observe that\nrobustness can be achieved without compromising on the original performance. We\nhope our extensive study-performed across five models and two benchmark\ndatasets-and proposed procedures would make robustness an integral component in\nMSA research. Our diagnostic checks and robust training solutions are simple to\nimplement and available at https://github. com/declare-lab/MSA-Robustness.", "published": "2022-05-30 23:30:16", "link": "http://arxiv.org/abs/2205.15465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive color transfer from images to terrain visualizations", "abstract": "Terrain mapping is not only dedicated to communicating how high or how steep\na landscape is but can also help to narrate how we feel about a place. However,\ncrafting effective and expressive hypsometric tints is challenging for both\nnonexperts and experts. In this paper, we present a two-step image-to-terrain\ncolor transfer method that can transfer color from arbitrary images to diverse\nterrain models. First, we present a new image color organization method that\norganizes discrete, irregular image colors into a continuous, regular color\ngrid that facilitates a series of color operations, such as local and global\nsearching, categorical color selection and sequential color interpolation.\nSecond, we quantify a series of subjective concerns about elevation color\ncrafting, such as \"the lower, the higher\" principle, color conventions, and\naerial perspectives. We also define color similarity between image and terrain\nvisualization with aesthetic quality. We then mathematically formulate\nimage-to-terrain color transfer as a dual-objective optimization problem and\noffer a heuristic searching method to solve the problem. Finally, we compare\nelevation tints from our method with a standard color scheme on four test\nterrains. The evaluations show that the hypsometric tints from the proposed\nmethod can work as effectively as the standard scheme and that our tints are\nmore visually favorable. We also showcase that our method can transfer emotion\nfrom image to terrain visualization.", "published": "2022-05-30 08:03:30", "link": "http://arxiv.org/abs/2205.14908v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detecting fake news by enhanced text representation with\n  multi-EDU-structure awareness", "abstract": "Since fake news poses a serious threat to society and individuals, numerous\nstudies have been brought by considering text, propagation and user profiles.\nDue to the data collection problem, these methods based on propagation and user\nprofiles are less applicable in the early stages. A good alternative method is\nto detect news based on text as soon as they are released, and a lot of\ntext-based methods were proposed, which usually utilized words, sentences or\nparagraphs as basic units. But, word is a too fine-grained unit to express\ncoherent information well, sentence or paragraph is too coarse to show specific\ninformation. Which granularity is better and how to utilize it to enhance text\nrepresentation for fake news detection are two key problems. In this paper, we\nintroduce Elementary Discourse Unit (EDU) whose granularity is between word and\nsentence, and propose a multi-EDU-structure awareness model to improve text\nrepresentation for fake news detection, namely EDU4FD. For the\nmulti-EDU-structure awareness, we build the sequence-based EDU representations\nand the graph-based EDU representations. The former is gotten by modeling the\ncoherence between consecutive EDUs with TextCNN that reflect the semantic\ncoherence. For the latter, we first extract rhetorical relations to build the\nEDU dependency graph, which can show the global narrative logic and help\ndeliver the main idea truthfully. Then a Relation Graph Attention Network\n(RGAT) is set to get the graph-based EDU representation. Finally, the two EDU\nrepresentations are incorporated as the enhanced text representation for fake\nnews detection, using a gated recursive unit combined with a global attention\nmechanism. Experiments on four cross-source fake news datasets show that our\nmodel outperforms the state-of-the-art text-based methods.", "published": "2022-05-30 14:32:14", "link": "http://arxiv.org/abs/2205.15139v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Short Math Answer Grading via In-context Meta-learning", "abstract": "Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.", "published": "2022-05-30 16:26:02", "link": "http://arxiv.org/abs/2205.15219v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained\n  Models", "abstract": "Pre-trained masked language models successfully perform few-shot learning by\nformulating downstream tasks as text infilling. However, as a strong\nalternative in full-shot settings, discriminative pre-trained models like\nELECTRA do not fit into the paradigm. In this work, we adapt prompt-based\nfew-shot learning to ELECTRA and show that it outperforms masked language\nmodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if a\ntoken is generated or original. We naturally extend that to prompt-based\nfew-shot learning by training to score the originality of the target options\nwithout introducing new parameters. Our method can be easily adapted to tasks\ninvolving multi-token predictions without extra computation overhead. Analysis\nshows that ELECTRA learns distributions that align better with downstream\ntasks.", "published": "2022-05-30 16:32:30", "link": "http://arxiv.org/abs/2205.15223v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Open Domain Multi-hop Search Using Reinforcement Learning", "abstract": "We propose a method to teach an automated agent to learn how to search for\nmulti-hop paths of relations between entities in an open domain. The method\nlearns a policy for directing existing information retrieval and machine\nreading resources to focus on relevant regions of a corpus. The approach\nformulates the learning problem as a Markov decision process with a state\nrepresentation that encodes the dynamics of the search process and a reward\nstructure that minimizes the number of documents that must be processed while\nstill finding multi-hop paths. We implement the method in an actor-critic\nreinforcement learning algorithm and evaluate it on a dataset of search\nproblems derived from a subset of English Wikipedia. The algorithm finds a\nfamily of policies that succeeds in extracting the desired information while\nprocessing fewer documents compared to several baseline heuristic algorithms.", "published": "2022-05-30 17:44:19", "link": "http://arxiv.org/abs/2205.15281v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer with Tree-order Encoding for Neural Program Generation", "abstract": "While a considerable amount of semantic parsing approaches have employed RNN\narchitectures for code generation tasks, there have been only few attempts to\ninvestigate the applicability of Transformers for this task. Including\nhierarchical information of the underlying programming language syntax has\nproven to be effective for code generation. Since the positional encoding of\nthe Transformer can only represent positions in a flat sequence, we have\nextended the encoding scheme to allow the attention mechanism to also attend\nover hierarchical positions in the input. Furthermore, we have realized a\ndecoder based on a restrictive grammar graph model to improve the generation\naccuracy and ensure the well-formedness of the generated code. While we did not\nsurpass the state of the art, our findings suggest that employing a tree-based\npositional encoding in combination with a shared natural-language subword\nvocabulary improves generation performance over sequential positional\nencodings.", "published": "2022-05-30 12:27:48", "link": "http://arxiv.org/abs/2206.13354v1", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "From Representation to Reasoning: Towards both Evidence and Commonsense\n  Reasoning for Video Question-Answering", "abstract": "Video understanding has achieved great success in representation learning,\nsuch as video caption, video object grounding, and video descriptive\nquestion-answer. However, current methods still struggle on video reasoning,\nincluding evidence reasoning and commonsense reasoning. To facilitate deeper\nvideo understanding towards video reasoning, we present the task of\nCausal-VidQA, which includes four types of questions ranging from scene\ndescription (description) to evidence reasoning (explanation) and commonsense\nreasoning (prediction and counterfactual). For commonsense reasoning, we set up\na two-step solution by answering the question and providing a proper reason.\nThrough extensive experiments on existing VideoQA methods, we find that the\nstate-of-the-art methods are strong in descriptions but weak in reasoning. We\nhope that Causal-VidQA can guide the research of video understanding from\nrepresentation learning to deeper reasoning. The dataset and related resources\nare available at \\url{https://github.com/bcmi/Causal-VidQA.git}.", "published": "2022-05-30 07:26:54", "link": "http://arxiv.org/abs/2205.14895v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "An Efficient Modern Baseline for FloodNet VQA", "abstract": "Designing efficient and reliable VQA systems remains a challenging problem,\nmore so in the case of disaster management and response systems. In this work,\nwe revisit fundamental combination methods like concatenation, addition and\nelement-wise multiplication with modern image and text feature abstraction\nmodels. We design a simple and efficient system which outperforms pre-existing\nmethods on the FloodNet dataset and achieves state-of-the-art performance. This\nsimplified system requires significantly less training and inference time than\nmodern VQA architectures. We also study the performance of various backbones\nand report their consolidated results. Code is available at\nhttps://github.com/sahilkhose/floodnet_vqa.", "published": "2022-05-30 12:04:49", "link": "http://arxiv.org/abs/2205.15025v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks", "abstract": "Societal biases are reflected in large pre-trained language models and their\nfine-tuned versions on downstream tasks. Common in-processing bias mitigation\napproaches, such as adversarial training and mutual information removal,\nintroduce additional optimization criteria, and update the model to reach a new\ndebiased state. However, in practice, end-users and practitioners might prefer\nto switch back to the original model, or apply debiasing only on a specific\nsubset of protected attributes. To enable this, we propose a novel modular bias\nmitigation approach, consisting of stand-alone highly sparse debiasing\nsubnetworks, where each debiasing module can be integrated into the core model\non-demand at inference time. Our approach draws from the concept of \\emph{diff}\npruning, and proposes a novel training regime adaptable to various\nrepresentation disentanglement optimizations. We conduct experiments on three\nclassification tasks with gender, race, and age as protected attributes. The\nresults show that our modular approach, while maintaining task performance,\nimproves (or at least remains on-par with) the effectiveness of bias mitigation\nin comparison with baseline finetuning. Particularly on a two-attribute\ndataset, our approach with separately learned debiasing subnetworks shows\neffective utilization of either or both the subnetworks for selective bias\nmitigation.", "published": "2022-05-30 15:21:25", "link": "http://arxiv.org/abs/2205.15171v5", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models", "abstract": "Recent advances in vision-language pre-training (VLP) have demonstrated\nimpressive performance in a range of vision-language (VL) tasks. However, there\nexist several challenges for measuring the community's progress in building\ngeneral multi-modal intelligence. First, most of the downstream VL datasets are\nannotated using raw images that are already seen during pre-training, which may\nresult in an overestimation of current VLP models' generalization ability.\nSecond, recent VLP work mainly focuses on absolute performance but overlooks\nthe efficiency-performance trade-off, which is also an important indicator for\nmeasuring progress.\n  To this end, we introduce the Vision-Language Understanding Evaluation (VLUE)\nbenchmark, a multi-task multi-dimension benchmark for evaluating the\ngeneralization capabilities and the efficiency-performance trade-off (``Pareto\nSOTA'') of VLP models. We demonstrate that there is a sizable generalization\ngap for all VLP models when testing on out-of-distribution test sets annotated\non images from a more diverse distribution that spreads across cultures.\nMoreover, we find that measuring the efficiency-performance trade-off of VLP\nmodels leads to complementary insights for several design choices of VLP. We\nrelease the VLUE benchmark to promote research on building vision-language\nmodels that generalize well to more diverse images and concepts unseen during\npre-training, and are practical in terms of efficiency-performance trade-off.", "published": "2022-05-30 16:52:30", "link": "http://arxiv.org/abs/2205.15237v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "StyleTTS: A Style-Based Generative Model for Natural and Diverse\n  Text-to-Speech Synthesis", "abstract": "Text-to-Speech (TTS) has recently seen great progress in synthesizing\nhigh-quality speech owing to the rapid development of parallel TTS systems, but\nproducing speech with naturalistic prosodic variations, speaking styles and\nemotional tones remains challenging. Moreover, since duration and speech are\ngenerated separately, parallel TTS models still have problems finding the best\nmonotonic alignments that are crucial for naturalistic speech synthesis. Here,\nwe propose StyleTTS, a style-based generative model for parallel TTS that can\nsynthesize diverse speech with natural prosody from a reference speech\nutterance. With novel Transferable Monotonic Aligner (TMA) and\nduration-invariant data augmentation schemes, our method significantly\noutperforms state-of-the-art models on both single and multi-speaker datasets\nin subjective tests of speech naturalness and speaker similarity. Through\nself-supervised learning of the speaking styles, our model can synthesize\nspeech with the same prosodic and emotional tone as any given reference speech\nwithout the need for explicitly labeling these categories.", "published": "2022-05-30 21:34:40", "link": "http://arxiv.org/abs/2205.15439v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Contextualization for the Organization of Text Documents Streams", "abstract": "There has been a significant effort by the research community to address the\nproblem of providing methods to organize documentation with the help of\ninformation Retrieval methods. In this report paper, we present several\nexperiments with some stream analysis methods to explore streams of text\ndocuments. We use only dynamic algorithms to explore, analyze, and organize the\nflux of text documents. This document shows a case study with developed\narchitectures of a Text Document Stream Organization, using incremental\nalgorithms like Incremental TextRank, and IS-TFIDF. Both these algorithms are\nbased on the assumption that the mapping of text documents and their\ndocument-term matrix in lower-dimensional evolving networks provides faster\nprocessing when compared to batch algorithms. With this architecture, and by\nusing FastText Embedding to retrieve similarity between documents, we compare\nmethods with large text datasets and ground truth evaluation of clustering\ncapacities. The datasets used were Reuters and COVID-19 emotions. The results\nprovide a new view for the contextualization of similarity when approaching\nflux of documents organization tasks, based on the similarity between documents\nin the flux, and by using mentioned algorithms.", "published": "2022-05-30 22:25:40", "link": "http://arxiv.org/abs/2206.02632v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "cs.IR"}
{"title": "Rites de Passage: Elucidating Displacement to Emplacement of Refugees on\n  Twitter", "abstract": "Social media deliberations allow to explore refugee-related is-sues. AI-based\nstudies have investigated refugee issues mostly around a specific event and\nconsidered unimodal approaches. Contrarily, we have employed a multimodal\narchitecture for probing the refugee journeys from their home to host nations.\nWe draw insights from Arnold van Gennep's anthropological work 'Les Rites de\nPassage', which systematically analyzed an individual's transition from one\ngroup or society to another. Based on Gennep's\nseparation-transition-incorporation framework, we have identified four phases\nof refugee journeys: Arrival of Refugees, Temporal stay at Asylums,\nRehabilitation, and Integration of Refugees into the host nation. We collected\n0.23 million multimodal tweets from April 2020 to March 2021 for testing this\nproposed frame-work. We find that a combination of transformer-based language\nmodels and state-of-the-art image recognition models, such as fusion of\nBERT+LSTM and InceptionV4, can out-perform unimodal models. Subsequently, to\ntest the practical implication of our proposed model in real-time, we have\nconsidered 0.01 million multimodal tweets related to the 2022 Ukrainian refugee\ncrisis. An F1-score of 71.88 % for this 2022 crisis confirms the\ngeneralizability of our proposed framework.", "published": "2022-05-30 05:12:34", "link": "http://arxiv.org/abs/2206.03248v2", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme\n  price movement prediction of Bitcoin", "abstract": "Bitcoin, with its ever-growing popularity, has demonstrated extreme price\nvolatility since its origin. This volatility, together with its decentralised\nnature, make Bitcoin highly subjective to speculative trading as compared to\nmore traditional assets. In this paper, we propose a multimodal model for\npredicting extreme price fluctuations. This model takes as input a variety of\ncorrelated assets, technical indicators, as well as Twitter content. In an\nin-depth study, we explore whether social media discussions from the general\npublic on Bitcoin have predictive power for extreme price movements. A dataset\nof 5,000 tweets per day containing the keyword `Bitcoin' was collected from\n2015 to 2021. This dataset, called PreBit, is made available online. In our\nhybrid model, we use sentence-level FinBERT embeddings, pretrained on financial\nlexicons, so as to capture the full contents of the tweets and feed it to the\nmodel in an understandable way. By combining these embeddings with a\nConvolutional Neural Network, we built a predictive model for significant\nmarket movements. The final multimodal ensemble model includes this NLP model\ntogether with a model based on candlestick data, technical indicators and\ncorrelated asset prices. In an ablation study, we explore the contribution of\nthe individual modalities. Finally, we propose and backtest a trading strategy\nbased on the predictions of our models with varying prediction threshold and\nshow that it can used to build a profitable trading strategy with a reduced\nrisk over a `hold' or moving average strategy.", "published": "2022-05-30 19:25:12", "link": "http://arxiv.org/abs/2206.00648v2", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Personalized Acoustic Echo Cancellation for Full-duplex Communications", "abstract": "Deep neural networks (DNNs) have shown promising results for acoustic echo\ncancellation (AEC). But the DNN-based AEC models let through all near-end\nspeakers including the interfering speech. In light of recent studies on\npersonalized speech enhancement, we investigate the feasibility of personalized\nacoustic echo cancellation (PAEC) in this paper for full-duplex communications,\nwhere background noise and interfering speakers may coexist with acoustic\nechoes. Specifically, we first propose a novel backbone neural network termed\nas gated temporal convolutional neural network (GTCNN) that outperforms\nstate-of-the-art AEC models in performance. Speaker embeddings like d-vectors\nare further adopted as auxiliary information to guide the GTCNN to focus on the\ntarget speaker. A special case in PAEC is that speech snippets of both parties\non the call are enrolled. Experimental results show that auxiliary information\nfrom either the near-end speaker or the far-end speaker can improve the\nDNN-based AEC performance. Nevertheless, there is still much room for\nimprovement in the utilization of the finite-dimensional speaker embeddings.", "published": "2022-05-30 15:47:12", "link": "http://arxiv.org/abs/2205.15195v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for\n  Binaural Audio Synthesis", "abstract": "Binaural audio plays a significant role in constructing immersive augmented\nand virtual realities. As it is expensive to record binaural audio from the\nreal world, synthesizing them from mono audio has attracted increasing\nattention. This synthesis process involves not only the basic physical warping\nof the mono audio, but also room reverberations and head/ear related\nfiltrations, which, however, are difficult to accurately simulate in\ntraditional digital signal processing. In this paper, we formulate the\nsynthesis process from a different perspective by decomposing the binaural\naudio into a common part that shared by the left and right channels as well as\na specific part that differs in each channel. Accordingly, we propose\nBinauralGrad, a novel two-stage framework equipped with diffusion models to\nsynthesize them respectively. Specifically, in the first stage, the common\ninformation of the binaural audio is generated with a single-channel diffusion\nmodel conditioned on the mono audio, based on which the binaural audio is\ngenerated by a two-channel diffusion model in the second stage. Combining this\nnovel perspective of two-stage synthesis with advanced generative models (i.e.,\nthe diffusion models),the proposed BinauralGrad is able to generate accurate\nand high-fidelity binaural audio samples. Experiment results show that on a\nbenchmark dataset, BinauralGrad outperforms the existing baselines by a large\nmargin in terms of both object and subject evaluation metrics (Wave L2: 0.128\nvs. 0.157, MOS: 3.80 vs. 3.61). The generated audio samples\n(https://speechresearch.github.io/binauralgrad) and code\n(https://github.com/microsoft/NeuralSpeech/tree/master/BinauralGrad) are\navailable online.", "published": "2022-05-30 02:09:26", "link": "http://arxiv.org/abs/2205.14807v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual\n  Imitation Learning", "abstract": "Humans are capable of completing a range of challenging manipulation tasks\nthat require reasoning jointly over modalities such as vision, touch, and\nsound. Moreover, many such tasks are partially-observed; for example, taking a\nnotebook out of a backpack will lead to visual occlusion and require reasoning\nover the history of audio or tactile information. While robust tactile sensing\ncan be costly to capture on robots, microphones near or on a robot's gripper\nare a cheap and easy way to acquire audio feedback of contact events, which can\nbe a surprisingly valuable data source for perception in the absence of vision.\nMotivated by the potential for sound to mitigate visual occlusion, we aim to\nlearn a set of challenging partially-observed manipulation tasks from visual\nand audio inputs. Our proposed system learns these tasks by combining offline\nimitation learning from a modest number of tele-operated demonstrations and\nonline finetuning using human provided interventions. In a set of simulated\ntasks, we find that our system benefits from using audio, and that by using\nonline interventions we are able to improve the success rate of offline\nimitation learning by ~20%. Finally, we find that our system can complete a set\nof challenging, partially-observed tasks on a Franka Emika Panda robot, like\nextracting keys from a bag, with a 70% success rate, 50% higher than a policy\nthat does not use audio.", "published": "2022-05-30 04:52:58", "link": "http://arxiv.org/abs/2205.14850v1", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech\n  with Untranscribed Data", "abstract": "We propose Guided-TTS 2, a diffusion-based generative model for high-quality\nadaptive TTS using untranscribed data. Guided-TTS 2 combines a\nspeaker-conditional diffusion model with a speaker-dependent phoneme classifier\nfor adaptive text-to-speech. We train the speaker-conditional diffusion model\non large-scale untranscribed datasets for a classifier-free guidance method and\nfurther fine-tune the diffusion model on the reference speech of the target\nspeaker for adaptation, which only takes 40 seconds. We demonstrate that\nGuided-TTS 2 shows comparable performance to high-quality single-speaker TTS\nbaselines in terms of speech quality and speaker similarity with only a\nten-second untranscribed data. We further show that Guided-TTS 2 outperforms\nadaptive TTS baselines on multi-speaker datasets even with a zero-shot\nadaptation setting. Guided-TTS 2 can adapt to a wide range of voices only using\nuntranscribed speech, which enables adaptive TTS with the voice of non-human\ncharacters such as Gollum in \\textit{\"The Lord of the Rings\"}.", "published": "2022-05-30 18:30:20", "link": "http://arxiv.org/abs/2205.15370v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial synthesis based data-augmentation for code-switched spoken\n  language identification", "abstract": "Spoken Language Identification (LID) is an important sub-task of Automatic\nSpeech Recognition(ASR) that is used to classify the language(s) in an audio\nsegment. Automatic LID plays an useful role in multilingual countries. In\nvarious countries, identifying a language becomes hard, due to the multilingual\nscenario where two or more than two languages are mixed together during\nconversation. Such phenomenon of speech is called as code-mixing or\ncode-switching. This nature is followed not only in India but also in many\nAsian countries. Such code-mixed data is hard to find, which further reduces\nthe capabilities of the spoken LID. Hence, this work primarily addresses this\nproblem using data augmentation as a solution on the on the data scarcity of\nthe code-switched class. This study focuses on Indic language code-mixed with\nEnglish. Spoken LID is performed on Hindi, code-mixed with English. This\nresearch proposes Generative Adversarial Network (GAN) based data augmentation\ntechnique performed using Mel spectrograms for audio data. GANs have already\nbeen proven to be accurate in representing the real data distribution in the\nimage domain. Proposed research exploits these capabilities of GANs in speech\ndomains such as speech classification, automatic speech recognition, etc. GANs\nare trained to generate Mel spectrograms of the minority code-mixed class which\nare then used to augment data for the classifier. Utilizing GANs give an\noverall improvement on Unweighted Average Recall by an amount of 3.5% as\ncompared to a Convolutional Recurrent Neural Network (CRNN) classifier used as\nthe baseline reference.", "published": "2022-05-30 06:41:13", "link": "http://arxiv.org/abs/2205.15747v2", "categories": ["eess.AS", "cs.CV", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AI-enabled Sound Pattern Recognition on Asthma Medication Adherence:\n  Evaluation with the RDA Benchmark Suite", "abstract": "Asthma is a common, usually long-term respiratory disease with negative\nimpact on global society and economy. Treatment involves using medical devices\n(inhalers) that distribute medication to the airways and its efficiency depends\non the precision of the inhalation technique. There is a clinical need for\nobjective methods to assess the inhalation technique, during clinical\nconsultation. Integrated health monitoring systems, equipped with sensors,\nenable the recognition of drug actuation, embedded with sound signal detection,\nanalysis and identification, from intelligent structures, that could provide\npowerful tools for reliable content management. Health monitoring systems\nequipped with sensors, embedded with sound signal detection, enable the\nrecognition of drug actuation and could be used for effective audio content\nanalysis. This paper revisits sound pattern recognition with machine learning\ntechniques for asthma medication adherence assessment and presents the\nRespiratory and Drug Actuation (RDA) Suite\n(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for\nbenchmarking and further research. The RDA Suite includes a set of tools for\naudio processing, feature extraction and classification procedures and is\nprovided along with a dataset, consisting of respiratory and drug actuation\nsounds. The classification models in RDA are implemented based on conventional\nand advanced machine learning and deep networks' architectures. This study\nprovides a comparative evaluation of the implemented approaches, examines\npotential improvements and discusses on challenges and future tendencies.", "published": "2022-05-30 18:08:28", "link": "http://arxiv.org/abs/2205.15360v3", "categories": ["cs.SD", "cs.CV", "cs.CY", "cs.GL", "eess.AS"], "primary_category": "cs.SD"}
