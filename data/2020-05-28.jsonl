{"title": "The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm\n  Completion", "abstract": "In this paper, we describe the findings of the SIGMORPHON 2020 shared task on\nunsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a\nnovel task in the field of inflectional morphology. Participants were asked to\nsubmit systems which take raw text and a list of lemmas as input, and output\nall inflected forms, i.e., the entire morphological paradigm, of each lemma. In\norder to simulate a realistic use case, we first released data for 5\ndevelopment languages. However, systems were officially evaluated on 9 surprise\nlanguages, which were only revealed a few days before the submission deadline.\nWe provided a modular baseline system, which is a pipeline of 4 components. 3\nteams submitted a total of 7 systems, but, surprisingly, none of the submitted\nsystems was able to improve over the baseline on average over all 9 test\nlanguages. Only on 3 languages did a submitted system obtain the best results.\nThis shows that unsupervised morphological paradigm completion is still largely\nunsolved. We present an analysis here, so that this shared task will ground\nfurther research on the topic.", "published": "2020-05-28 03:09:58", "link": "http://arxiv.org/abs/2005.13756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConCET: Entity-Aware Topic Classification for Open-Domain Conversational\n  Agents", "abstract": "Identifying the topic (domain) of each user's utterance in open-domain\nconversational systems is a crucial step for all subsequent language\nunderstanding and response tasks. In particular, for complex domains, an\nutterance is often routed to a single component responsible for that domain.\nThus, correctly mapping a user utterance to the right domain is critical. To\naddress this problem, we introduce ConCET: a Concurrent Entity-aware\nconversational Topic classifier, which incorporates entity-type information\ntogether with the utterance content features. Specifically, ConCET utilizes\nentity information to enrich the utterance representation, combining character,\nword, and entity-type embeddings into a single representation. However, for\nrich domains with millions of available entities, unrealistic amounts of\nlabeled training data would be required. To complement our model, we propose a\nsimple and effective method for generating synthetic training data, to augment\nthe typically limited amounts of labeled training data, using commonly\navailable knowledge bases to generate additional labeled utterances. We\nextensively evaluate ConCET and our proposed training method first on an openly\navailable human-human conversational dataset called Self-Dialogue, to calibrate\nour approach against previous state-of-the-art methods; second, we evaluate\nConCET on a large dataset of human-machine conversations with real users,\ncollected as part of the Amazon Alexa Prize. Our results show that ConCET\nsignificantly improves topic classification performance on both datasets,\nincluding 8-10% improvements over state-of-the-art deep learning methods. We\ncomplement our quantitative results with detailed analysis of system\nperformance, which could be used for further improvements of conversational\nagents.", "published": "2020-05-28 06:29:08", "link": "http://arxiv.org/abs/2005.13798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Would you Like to Talk about Sports Now? Towards Contextual Topic\n  Suggestion for Open-Domain Conversational Agents", "abstract": "To hold a true conversation, an intelligent agent should be able to\noccasionally take initiative and recommend the next natural conversation topic.\nThis is a challenging task. A topic suggested by the agent should be relevant\nto the person, appropriate for the conversation context, and the agent should\nhave something interesting to say about it. Thus, a scripted, or\none-size-fits-all, popularity-based topic suggestion is doomed to fail.\nInstead, we explore different methods for a personalized, contextual topic\nsuggestion for open-domain conversations. We formalize the Conversational Topic\nSuggestion problem (CTS) to more clearly identify the assumptions and\nrequirements. We also explore three possible approaches to solve this problem:\n(1) model-based sequential topic suggestion to capture the conversation context\n(CTS-Seq), (2) Collaborative Filtering-based suggestion to capture previous\nsuccessful conversations from similar users (CTS-CF), and (3) a hybrid approach\ncombining both conversation context and collaborative filtering. To evaluate\nthe effectiveness of these methods, we use real conversations collected as part\nof the Amazon Alexa Prize 2018 Conversational AI challenge. The results are\npromising: the CTS-Seq model suggests topics with 23% higher accuracy than the\nbaseline, and incorporating collaborative filtering signals into a hybrid\nCTS-Seq-CF model further improves recommendation accuracy by 12%. Together, our\nproposed models, experiments, and analysis significantly advance the study of\nopen-domain conversational agents, and suggest promising directions for future\nimprovements.", "published": "2020-05-28 06:41:18", "link": "http://arxiv.org/abs/2005.13803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Dialogue Act Classification for Open-Domain Conversational\n  Agents", "abstract": "Classifying the general intent of the user utterance in a conversation, also\nknown as Dialogue Act (DA), e.g., open-ended question, statement of opinion, or\nrequest for an opinion, is a key step in Natural Language Understanding (NLU)\nfor conversational agents. While DA classification has been extensively studied\nin human-human conversations, it has not been sufficiently explored for the\nemerging open-domain automated conversational agents. Moreover, despite\nsignificant advances in utterance-level DA classification, full understanding\nof dialogue utterances requires conversational context. Another challenge is\nthe lack of available labeled data for open-domain human-machine conversations.\nTo address these problems, we propose a novel method, CDAC (Contextual Dialogue\nAct Classifier), a simple yet effective deep learning approach for contextual\ndialogue act classification. Specifically, we use transfer learning to adapt\nmodels trained on human-human conversations to predict dialogue acts in\nhuman-machine dialogues. To investigate the effectiveness of our method, we\ntrain our model on the well-known Switchboard human-human dialogue dataset, and\nfine-tune it for predicting dialogue acts in human-machine conversation data,\ncollected as part of the Amazon Alexa Prize 2018 competition. The results show\nthat the CDAC model outperforms an utterance-level state of the art baseline by\n8.0% on the Switchboard dataset, and is comparable to the latest reported\nstate-of-the-art contextual DA classification results. Furthermore, our results\nshow that fine-tuning the CDAC model on a small sample of manually labeled\nhuman-machine conversations allows CDAC to more accurately predict dialogue\nacts in real users' conversations, suggesting a promising direction for future\nimprovements.", "published": "2020-05-28 06:48:10", "link": "http://arxiv.org/abs/2005.13804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Corpus for Large-Scale Phonetic Typology", "abstract": "A major hurdle in data-driven research on typology is having sufficient data\nin many languages to draw meaningful conclusions. We present VoxClamantis v1.0,\nthe first large-scale corpus for phonetic typology, with aligned segments and\nestimated phoneme-level labels in 690 readings spanning 635 languages, along\nwith acoustic-phonetic measures of vowels and sibilants. Access to such data\ncan greatly facilitate investigation of phonetic typology at a large scale and\nacross many languages. However, it is non-trivial and computationally intensive\nto obtain such alignments for hundreds of languages, many of which have few to\nno resources presently available. We describe the methodology to create our\ncorpus, discuss caveats with current methods and their impact on the utility of\nthis data, and illustrate possible research directions through a series of case\nstudies on the 48 highest-quality readings. Our corpus and scripts are publicly\navailable for non-commercial use at https://voxclamantisproject.github.io.", "published": "2020-05-28 13:03:51", "link": "http://arxiv.org/abs/2005.13962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Neural Machine Translation with Normalizing Flows", "abstract": "Variational Neural Machine Translation (VNMT) is an attractive framework for\nmodeling the generation of target translations, conditioned not only on the\nsource sentence but also on some latent random variables. The latent variable\nmodeling may introduce useful statistical dependencies that can improve\ntranslation accuracy. Unfortunately, learning informative latent variables is\nnon-trivial, as the latent space can be prohibitively large, and the latent\ncodes are prone to be ignored by many translation models at training time.\nPrevious works impose strong assumptions on the distribution of the latent code\nand limit the choice of the NMT architecture. In this paper, we propose to\napply the VNMT framework to the state-of-the-art Transformer and introduce a\nmore flexible approximate posterior based on normalizing flows. We demonstrate\nthe efficacy of our proposal under both in-domain and out-of-domain conditions,\nsignificantly outperforming strong baselines.", "published": "2020-05-28 13:30:53", "link": "http://arxiv.org/abs/2005.13978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks and Defense on Texts: A Survey", "abstract": "Deep learning models have been used widely for various purposes in recent\nyears in object recognition, self-driving cars, face recognition, speech\nrecognition, sentiment analysis, and many others. However, in recent years it\nhas been shown that these models possess weakness to noises which force the\nmodel to misclassify. This issue has been studied profoundly in the image and\naudio domain. Very little has been studied on this issue concerning textual\ndata. Even less survey on this topic has been performed to understand different\ntypes of attacks and defense techniques. In this manuscript, we accumulated and\nanalyzed different attacking techniques and various defense models to provide a\nmore comprehensive idea. Later we point out some of the interesting findings of\nall papers and challenges that need to be overcome to move forward in this\nfield.", "published": "2020-05-28 15:58:45", "link": "http://arxiv.org/abs/2005.14108v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.", "published": "2020-05-28 17:29:03", "link": "http://arxiv.org/abs/2005.14165v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Incorporating Structural Information to improve Dialogue Response\n  Generation", "abstract": "We consider the task of generating dialogue responses from background\nknowledge comprising of domain specific resources. Specifically, given a\nconversation around a movie, the task is to generate the next response based on\nbackground knowledge about the movie such as the plot, review, Reddit comments\netc. This requires capturing structural, sequential and semantic information\nfrom the conversation context and the background resources. This is a new task\nand has not received much attention from the community. We propose a new\narchitecture that uses the ability of BERT to capture deep contextualized\nrepresentations in conjunction with explicit structure and sequence\ninformation. More specifically, we use (i) Graph Convolutional Networks (GCNs)\nto capture structural information, (ii) LSTMs to capture sequential information\nand (iii) BERT for the deep contextualized representations that capture\nsemantic information. We analyze the proposed architecture extensively. To this\nend, we propose a plug-and-play Semantics-Sequences-Structures (SSS) framework\nwhich allows us to effectively combine such linguistic information. Through a\nseries of experiments we make some interesting observations. First, we observe\nthat the popular adaptation of the GCN model for NLP tasks where structural\ninformation (GCNs) was added on top of sequential information (LSTMs) performs\npoorly on our task. This leads us to explore interesting ways of combining\nsemantic and structural information to improve the performance. Second, we\nobserve that while BERT already outperforms other deep contextualized\nrepresentations such as ELMo, it still benefits from the additional structural\ninformation explicitly added using GCNs. This is a bit surprising given the\nrecent claims that BERT already captures structural information. Lastly, the\nproposed SSS framework gives an improvement of 7.95% over the baseline.", "published": "2020-05-28 22:06:03", "link": "http://arxiv.org/abs/2005.14315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JointMap: Joint Query Intent Understanding For Modeling Intent\n  Hierarchies in E-commerce Search", "abstract": "An accurate understanding of a user's query intent can help improve the\nperformance of downstream tasks such as query scoping and ranking. In the\ne-commerce domain, recent work in query understanding focuses on the query to\nproduct-category mapping. But, a small yet significant percentage of queries\n(in our website 1.5% or 33M queries in 2019) have non-commercial intent\nassociated with them. These intents are usually associated with non-commercial\ninformation seeking needs such as discounts, store hours, installation guides,\netc. In this paper, we introduce Joint Query Intent Understanding (JointMap), a\ndeep learning model to simultaneously learn two different high-level user\nintent tasks: 1) identifying a query's commercial vs. non-commercial intent,\nand 2) associating a set of relevant product categories in taxonomy to a\nproduct query. JointMap model works by leveraging the transfer bias that exists\nbetween these two related tasks through a joint-learning process. As curating a\nlabeled data set for these tasks can be expensive and time-consuming, we\npropose a distant supervision approach in conjunction with an active learning\nmodel to generate high-quality training data sets. To demonstrate the\neffectiveness of JointMap, we use search queries collected from a large\ncommercial website. Our results show that JointMap significantly improves both\n\"commercial vs. non-commercial\" intent prediction and product category mapping\nby 2.3% and 10% on average over state-of-the-art deep learning methods. Our\nfindings suggest a promising direction to model the intent hierarchies in an\ne-commerce search engine.", "published": "2020-05-28 05:20:00", "link": "http://arxiv.org/abs/2005.13783v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "User Intent Inference for Web Search and Conversational Agents", "abstract": "User intent understanding is a crucial step in designing both conversational\nagents and search engines. Detecting or inferring user intent is challenging,\nsince the user utterances or queries can be short, ambiguous, and contextually\ndependent. To address these research challenges, my thesis work focuses on: 1)\nUtterance topic and intent classification for conversational agents 2) Query\nintent mining and classification for Web search engines, focusing on the\ne-commerce domain. To address the first topic, I proposed novel models to\nincorporate entity information and conversation-context clues to predict both\ntopic and intent of the user's utterances. For the second research topic, I\nplan to extend the existing state of the art methods in Web search intent\nprediction to the e-commerce domain, via: 1) Developing a joint learning model\nto predict search queries' intents and the product categories associated with\nthem, 2) Discovering new hidden users' intents. All the models will be\nevaluated on the real queries available from a major e-commerce site search\nengine. The results from these studies can be leveraged to improve performance\nof various tasks such as natural language understanding, query scoping, query\nsuggestion, and ranking, resulting in an enriched user experience.", "published": "2020-05-28 07:04:42", "link": "http://arxiv.org/abs/2005.13808v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Generating Diverse and Consistent QA pairs from Contexts with\n  Information-Maximizing Hierarchical Conditional VAEs", "abstract": "One of the most crucial challenges in question answering (QA) is the scarcity\nof labeled data, since it is costly to obtain question-answer (QA) pairs for a\ntarget text domain with human annotation. An alternative approach to tackle the\nproblem is to use automatically generated QA pairs from either the problem\ncontext or from large amount of unstructured texts (e.g. Wikipedia). In this\nwork, we propose a hierarchical conditional variational autoencoder (HCVAE) for\ngenerating QA pairs given unstructured texts as contexts, while maximizing the\nmutual information between generated QA pairs to ensure their consistency. We\nvalidate our Information Maximizing Hierarchical Conditional Variational\nAutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the\nperformance of the QA model (BERT-base) using only the generated QA pairs\n(QA-based evaluation) or by using both the generated and human-labeled pairs\n(semi-supervised learning) for training, against state-of-the-art baseline\nmodels. The results show that our model obtains impressive performance gains\nover all baselines on both tasks, using only a fraction of data for training.", "published": "2020-05-28 08:26:06", "link": "http://arxiv.org/abs/2005.13837v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Modelling of Emotion and Abusive Language Detection", "abstract": "The rise of online communication platforms has been accompanied by some\nundesirable effects, such as the proliferation of aggressive and abusive\nbehaviour online. Aiming to tackle this problem, the natural language\nprocessing (NLP) community has experimented with a range of techniques for\nabuse detection. While achieving substantial success, these methods have so far\nonly focused on modelling the linguistic properties of the comments and the\nonline communities of users, disregarding the emotional state of the users and\nhow this might affect their language. The latter is, however, inextricably\nlinked to abusive behaviour. In this paper, we present the first joint model of\nemotion and abusive language detection, experimenting in a multi-task learning\nframework that allows one task to inform the other. Our results demonstrate\nthat incorporating affective features leads to significant improvements in\nabuse detection performance across datasets.", "published": "2020-05-28 14:08:40", "link": "http://arxiv.org/abs/2005.14028v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing \"bias\" is an inherently normative process. We\nfurther find that these papers' proposed quantitative techniques for measuring\nor mitigating \"bias\" are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing \"bias\" in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of \"bias\"---i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements---and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.", "published": "2020-05-28 14:32:08", "link": "http://arxiv.org/abs/2005.14050v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cats climb entails mammals move: preserving hyponymy in compositional\n  distributional semantics", "abstract": "To give vector-based representations of meaning more structure, one approach\nis to use positive semidefinite (psd) matrices. These allow us to model\nsimilarity of words as well as the hyponymy or is-a relationship. Psd matrices\ncan be learnt relatively easily in a given vector space $M\\otimes M^*$, but to\ncompose words to form phrases and sentences, we need representations in larger\nspaces. In this paper, we introduce a generic way of composing the psd matrices\ncorresponding to words. We propose that psd matrices for verbs, adjectives, and\nother functional words be lifted to completely positive (CP) maps that match\ntheir grammatical type. This lifting is carried out by our composition rule\ncalled Compression, Compr. In contrast to previous composition rules like Fuzz\nand Phaser (a.k.a. KMult and BMult), Compr preserves hyponymy. Mathematically,\nCompr is itself a CP map, and is therefore linear and generally\nnon-commutative. We give a number of proposals for the structure of Compr,\nbased on spiders, cups and caps, and generate a range of composition rules. We\ntest these rules on a small sentence entailment dataset, and see some\nimprovements over the performance of Fuzz and Phaser.", "published": "2020-05-28 16:31:59", "link": "http://arxiv.org/abs/2005.14134v2", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Pretraining Strategies for Supervised Entity\n  Linking", "abstract": "In this work, we present an entity linking model which combines a Transformer\narchitecture with large scale pretraining from Wikipedia links. Our model\nachieves the state-of-the-art on two commonly used entity linking datasets:\n96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand\nwhat design choices are important for entity linking, including choices of\nnegative entity candidates, Transformer architecture, and input perturbations.\nLastly, we present promising results on more challenging settings such as\nend-to-end entity linking and entity linking without in-domain training data.", "published": "2020-05-28 19:32:52", "link": "http://arxiv.org/abs/2005.14253v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What is SemEval evaluating? A Systematic Analysis of Evaluation\n  Campaigns in NLP", "abstract": "SemEval is the primary venue in the NLP community for the proposal of new\nchallenges and for the systematic empirical evaluation of NLP systems. This\npaper provides a systematic quantitative analysis of SemEval aiming to evidence\nthe patterns of the contributions behind SemEval. By understanding the\ndistribution of task types, metrics, architectures, participation and citations\nover time we aim to answer the question on what is being evaluated by SemEval.", "published": "2020-05-28 21:17:43", "link": "http://arxiv.org/abs/2005.14299v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Comparison of Popular End-to-End Models for Large Scale Speech\n  Recognition", "abstract": "Recently, there has been a strong push to transition from hybrid models to\nend-to-end (E2E) models for automatic speech recognition. Currently, there are\nthree promising E2E methods: recurrent neural network transducer (RNN-T), RNN\nattention-based encoder-decoder (AED), and Transformer-AED. In this study, we\nconduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models,\nin both non-streaming and streaming modes. We use 65 thousand hours of\nMicrosoft anonymized training data to train these models. As E2E models are\nmore data hungry, it is better to compare their effectiveness with large amount\nof training data. To the best of our knowledge, no such comprehensive study has\nbeen conducted yet. We show that although AED models are stronger than RNN-T in\nthe non-streaming mode, RNN-T is very competitive in streaming mode if its\nencoder can be properly initialized. Among all three E2E models,\ntransformer-AED achieved the best accuracy in both streaming and non-streaming\nmode. We show that both streaming RNN-T and transformer-AED models can obtain\nbetter accuracy than a highly-optimized hybrid model.", "published": "2020-05-28 22:30:57", "link": "http://arxiv.org/abs/2005.14327v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Complex networks for event detection in heterogeneous high volume news\n  streams", "abstract": "Detecting important events in high volume news streams is an important task\nfor a variety of purposes.The volume and rate of online news increases the need\nfor automated event detection methods thatcan operate in real time. In this\npaper we develop a network-based approach that makes the workingassumption that\nimportant news events always involve named entities (such as persons,\nlocationsand organizations) that are linked in news articles. Our approach uses\nnatural language processingtechniques to detect these entities in a stream of\nnews articles and then creates a time-stamped seriesof networks in which the\ndetected entities are linked by co-occurrence in articles and sentences. Inthis\nprototype, weighted node degree is tracked over time and change-point detection\nused to locateimportant events. Potential events are characterized and\ndistinguished using community detectionon KeyGraphs that relate named entities\nand informative noun-phrases from related articles. Thismethodology already\nproduces promising results and will be extended in future to include a\nwidervariety of complex network analysis techniques.", "published": "2020-05-28 02:45:43", "link": "http://arxiv.org/abs/2005.13751v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search", "abstract": "In spoken Keyword Search, the query may contain out-of-vocabulary (OOV) words\nnot observed when training the speech recognition system. Using subword\nlanguage models (LMs) in the first-pass recognition makes it possible to\nrecognize the OOV words, but even the subword n-gram LMs suffer from data\nsparsity. Recurrent Neural Network (RNN) LMs alleviate the sparsity problems\nbut are not suitable for first-pass recognition as such. One way to solve this\nis to approximate the RNNLMs by back-off n-gram models. In this paper, we\npropose to interpolate the conventional n-gram models and the RNNLM\napproximation for better OOV recognition. Furthermore, we develop a new RNNLM\napproximation method suitable for subword units: It produces variable-order\nn-grams to include long-span approximations and considers also n-grams that\nwere not originally observed in the training corpus. To evaluate these models\non OOVs, we setup Arabic and Finnish Keyword Search tasks concentrating only on\nOOV words. On these tasks, interpolating the baseline RNNLM approximation and a\nconventional LM outperforms the conventional LM in terms of the Maximum Term\nWeighted Value for single-character subwords. Moreover, replacing the baseline\napproximation with the proposed method achieves the best performance on both\nmulti- and single-character subwords.", "published": "2020-05-28 07:59:06", "link": "http://arxiv.org/abs/2005.13827v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Various Length Dependence by Dual Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are widely used as a memory model for\nsequence-related problems. Many variants of RNN have been proposed to solve the\ngradient problems of training RNNs and process long sequences. Although some\nclassical models have been proposed, capturing long-term dependence while\nresponding to short-term changes remains a challenge. To this problem, we\npropose a new model named Dual Recurrent Neural Networks (DuRNN). The DuRNN\nconsists of two parts to learn the short-term dependence and progressively\nlearn the long-term dependence. The first part is a recurrent neural network\nwith constrained full recurrent connections to deal with short-term dependence\nin sequence and generate short-term memory. Another part is a recurrent neural\nnetwork with independent recurrent connections which helps to learn long-term\ndependence and generate long-term memory. A selection mechanism is added\nbetween two parts to help the needed long-term information transfer to the\nindependent neurons. Multiple modules can be stacked to form a multi-layer\nmodel for better performance. Our contributions are: 1) a new recurrent model\ndeveloped based on the divide-and-conquer strategy to learn long and short-term\ndependence separately, and 2) a selection mechanism to enhance the separating\nand learning of different temporal scales of dependence. Both theoretical\nanalysis and extensive experiments are conducted to validate the performance of\nour model, and we also conduct simple visualization experiments and ablation\nanalyses for the model interpretability. Experimental results indicate that the\nproposed DuRNN model can handle not only very long sequences (over 5000 time\nsteps), but also short sequences very well. Compared with many state-of-the-art\nRNN models, our model has demonstrated efficient and better performance.", "published": "2020-05-28 09:30:01", "link": "http://arxiv.org/abs/2005.13867v1", "categories": ["cs.NE", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.NE"}
{"title": "When Can Self-Attention Be Replaced by Feed Forward Layers?", "abstract": "Recently, self-attention models such as Transformers have given competitive\nresults compared to recurrent neural network systems in speech recognition. The\nkey factor for the outstanding performance of self-attention models is their\nability to capture temporal relationships without being limited by the distance\nbetween two related events. However, we note that the range of the learned\ncontext progressively increases from the lower to upper self-attention layers,\nwhilst acoustic events often happen within short time spans in a left-to-right\norder. This leads to a question: for speech recognition, is a global view of\nthe entire sequence still important for the upper self-attention layers in the\nencoder of Transformers? To investigate this, we replace these self-attention\nlayers with feed forward layers. In our speech recognition experiments (Wall\nStreet Journal and Switchboard), we indeed observe an interesting result:\nreplacing the upper self-attention layers in the encoder with feed forward\nlayers leads to no performance drop, and even minor gains. Our experiments\noffer insights to how self-attention layers process the speech signal, leading\nto the conclusion that the lower self-attention layers of the encoder encode a\nsufficiently wide range of inputs, hence learning further contextual\ninformation in the upper layers is unnecessary.", "published": "2020-05-28 10:35:49", "link": "http://arxiv.org/abs/2005.13895v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Variational Autoencoder with Embedded Student-$t$ Mixture Model for\n  Authorship Attribution", "abstract": "Traditional computational authorship attribution describes a classification\ntask in a closed-set scenario. Given a finite set of candidate authors and\ncorresponding labeled texts, the objective is to determine which of the authors\nhas written another set of anonymous or disputed texts. In this work, we\npropose a probabilistic autoencoding framework to deal with this supervised\nclassification task. More precisely, we are extending a variational autoencoder\n(VAE) with embedded Gaussian mixture model to a Student-$t$ mixture model.\nAutoencoders have had tremendous success in learning latent representations.\nHowever, existing VAEs are currently still bound by limitations imposed by the\nassumed Gaussianity of the underlying probability distributions in the latent\nspace. In this work, we are extending the Gaussian model for the VAE to a\nStudent-$t$ model, which allows for an independent control of the \"heaviness\"\nof the respective tails of the implied probability densities. Experiments over\nan Amazon review dataset indicate superior performance of the proposed method.", "published": "2020-05-28 11:52:32", "link": "http://arxiv.org/abs/2005.13930v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "HAT: Hardware-Aware Transformers for Efficient Natural Language\n  Processing", "abstract": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but\nthey are difficult to be deployed on hardware due to the intensive computation.\nTo enable low-latency inference on resource-constrained hardware platforms, we\npropose to design Hardware-Aware Transformers (HAT) with neural architecture\nsearch. We first construct a large design space with $\\textit{arbitrary\nencoder-decoder attention}$ and $\\textit{heterogeneous layers}$. Then we train\na $\\textit{SuperTransformer}$ that covers all candidates in the design space,\nand efficiently produces many $\\textit{SubTransformers}$ with weight sharing.\nFinally, we perform an evolutionary search with a hardware latency constraint\nto find a specialized $\\textit{SubTransformer}$ dedicated to run fast on the\ntarget hardware. Extensive experiments on four machine translation tasks\ndemonstrate that HAT can discover efficient models for different hardware (CPU,\nGPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT\ncan achieve $\\textbf{3}\\times$ speedup, $\\textbf{3.7}\\times$ smaller size over\nbaseline Transformer; $\\textbf{2.7}\\times$ speedup, $\\textbf{3.6}\\times$\nsmaller size over Evolved Transformer with $\\textbf{12,041}\\times$ less search\ncost and no performance loss. HAT code is\nhttps://github.com/mit-han-lab/hardware-aware-transformers.git", "published": "2020-05-28 17:58:56", "link": "http://arxiv.org/abs/2005.14187v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Unsupervised Audio Source Separation using Generative Priors", "abstract": "State-of-the-art under-determined audio source separation systems rely on\nsupervised end-end training of carefully tailored neural network architectures\noperating either in the time or the spectral domain. However, these methods are\nseverely challenged in terms of requiring access to expensive source level\nlabeled data and being specific to a given set of sources and the mixing\nprocess, which demands complete re-training when those assumptions change. This\nstrongly emphasizes the need for unsupervised methods that can leverage the\nrecent advances in data-driven modeling, and compensate for the lack of labeled\ndata through meaningful priors. To this end, we propose a novel approach for\naudio source separation based on generative priors trained on individual\nsources. Through the use of projected gradient descent optimization, our\napproach simultaneously searches in the source-specific latent spaces to\neffectively recover the constituent sources. Though the generative priors can\nbe defined in the time domain directly, e.g. WaveGAN, we find that using\nspectral domain loss functions for our optimization leads to good-quality\nsource estimates. Our empirical studies on standard spoken digit and instrument\ndatasets clearly demonstrate the effectiveness of our approach over classical\nas well as state-of-the-art unsupervised baselines.", "published": "2020-05-28 03:57:16", "link": "http://arxiv.org/abs/2005.13769v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake\n  Voices", "abstract": "With the recent advances in voice synthesis, AI-synthesized fake voices are\nindistinguishable to human ears and widely are applied to produce realistic and\nnatural DeepFakes, exhibiting real threats to our society. However, effective\nand robust detectors for synthesized fake voices are still in their infancy and\nare not ready to fully tackle this emerging threat. In this paper, we devise a\nnovel approach, named \\emph{DeepSonar}, based on monitoring neuron behaviors of\nspeaker recognition (SR) system, \\ie, a deep neural network (DNN), to discern\nAI-synthesized fake voices. Layer-wise neuron behaviors provide an important\ninsight to meticulously catch the differences among inputs, which are widely\nemployed for building safety, robust, and interpretable DNNs. In this work, we\nleverage the power of layer-wise neuron activation patterns with a conjecture\nthat they can capture the subtle differences between real and AI-synthesized\nfake voices, in providing a cleaner signal to classifiers than raw inputs.\nExperiments are conducted on three datasets (including commercial products from\nGoogle, Baidu, \\etc) containing both English and Chinese languages to\ncorroborate the high detection rates (98.1\\% average accuracy) and low false\nalarm rates (about 2\\% error rate) of DeepSonar in discerning fake voices.\nFurthermore, extensive experimental results also demonstrate its robustness\nagainst manipulation attacks (\\eg, voice conversion and additive real-world\nnoises). Our work further poses a new insight into adopting neuron behaviors\nfor effective and robust AI aided multimedia fakes forensics as an inside-out\napproach instead of being motivated and swayed by various artifacts introduced\nin synthesizing fakes.", "published": "2020-05-28 04:02:52", "link": "http://arxiv.org/abs/2005.13770v3", "categories": ["eess.AS", "cs.CR", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-to-Singing Conversion based on Boundary Equilibrium GAN", "abstract": "This paper investigates the use of generative adversarial network (GAN)-based\nmodels for converting the spectrogram of a speech signal into that of a singing\none, without reference to the phoneme sequence underlying the speech. This is\nachieved by viewing speech-to-singing conversion as a style transfer problem.\nSpecifically, given a speech input, and optionally the F0 contour of the target\nsinging, the proposed model generates as the output a singing signal with a\nprogressive-growing encoder/decoder architecture and boundary equilibrium GAN\nloss functions. Our quantitative and qualitative analysis show that the\nproposed model generates singing voices with much higher naturalness than an\nexisting non adversarially-trained baseline. For reproducibility, the code will\nbe publicly available at a GitHub repository upon paper publication.", "published": "2020-05-28 08:18:02", "link": "http://arxiv.org/abs/2005.13835v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Restoration of Audio Degraded by Low-Frequency Pulses Modeled\n  via Gaussian Process", "abstract": "A common defect found when reproducing old vinyl and gramophone recordings\nwith mechanical devices are the long pulses with significant low-frequency\ncontent caused by the interaction of the arm-needle system with deep scratches\nor even breakages on the media surface. Previous approaches to their\nsuppression on digital counterparts of the recordings depend on a prior\nestimation of the pulse location, usually performed via heuristic methods. This\npaper proposes a novel Bayesian approach capable of jointly estimating the\npulse location; interpolating the almost annihilated signal underlying the\nstrong discontinuity that initiates the pulse; and also estimating the long\npulse tail by a simple Gaussian Process, allowing its suppression from the\ncorrupted signal. The posterior distribution for the model parameters as well\nfor the pulse is explored via Markov-Chain Monte Carlo (MCMC) algorithms.\nControlled experiments indicate that the proposed method, while requiring\nsignificantly less user intervention, achieves perceptual results similar to\nthose of previous approaches and performs well when dealing with naturally\ndegraded signals.", "published": "2020-05-28 17:52:26", "link": "http://arxiv.org/abs/2005.14181v2", "categories": ["eess.AS", "cs.SD", "eess.SP", "stat.AP", "stat.ML"], "primary_category": "eess.AS"}
