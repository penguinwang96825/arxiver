{"title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence\n  Correction", "abstract": "We demonstrate that an attention-based encoder-decoder model can be used for\nsentence-level grammatical error identification for the Automated Evaluation of\nScientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder\nmodels can be used for the generation of corrections, in addition to error\nidentification, which is of interest for certain end-user applications. We show\nthat a character-based encoder-decoder model is particularly effective,\noutperforming other results on the AESW Shared Task on its own, and showing\ngains over a word-based counterpart. Our final model--a combination of three\ncharacter-based encoder-decoder models, one word-based encoder-decoder model,\nand a sentence-level CNN--is the highest performing system on the AESW 2016\nbinary prediction Shared Task.", "published": "2016-04-16 01:49:09", "link": "http://arxiv.org/abs/1604.04677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised and Unsupervised Ensembling for Knowledge Base Population", "abstract": "We present results on combining supervised and unsupervised methods to\nensemble multiple systems for two popular Knowledge Base Population (KBP)\ntasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and\nLinking (TEDL). We demonstrate that our combined system along with auxiliary\nfeatures outperforms the best performing system for both tasks in the 2015\ncompetition, several ensembling baselines, as well as the state-of-the-art\nstacking approach to ensembling KBP systems. The success of our technique on\ntwo different and challenging problems demonstrates the power and generality of\nour combined approach to ensembling.", "published": "2016-04-16 21:18:14", "link": "http://arxiv.org/abs/1604.04802v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
