{"title": "Athena: Constructing Dialogues Dynamically with Discourse Constraints", "abstract": "This report describes Athena, a dialogue system for spoken conversation on\npopular topics and current events. We develop a flexible topic-agnostic\napproach to dialogue management that dynamically configures dialogue based on\ngeneral principles of entity and topic coherence. Athena's dialogue manager\nuses a contract-based method where discourse constraints are dispatched to\nclusters of response generators. This allows Athena to procure responses from\ndynamic sources, such as knowledge graph traversals and feature-based\non-the-fly response retrieval methods. After describing the dialogue system\narchitecture, we perform an analysis of conversations that Athena participated\nin during the 2019 Alexa Prize Competition. We conclude with a report on\nseveral user studies we carried out to better understand how individual user\ncharacteristics affect system ratings.", "published": "2020-11-21 00:28:34", "link": "http://arxiv.org/abs/2011.10683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Semantic Accuracy of Data-to-Text Generation with Natural\n  Language Inference", "abstract": "A major challenge in evaluating data-to-text (D2T) generation is measuring\nthe semantic accuracy of the generated text, i.e. checking if the output text\ncontains all and only facts supported by the input data. We propose a new\nmetric for evaluating the semantic accuracy of D2T generation based on a neural\nmodel pretrained for natural language inference (NLI). We use the NLI model to\ncheck textual entailment between the input data and the output text in both\ndirections, allowing us to reveal omissions or hallucinations. Input data are\nconverted to text for NLI using trivial templates. Our experiments on two\nrecent D2T datasets show that our metric can achieve high accuracy in\nidentifying erroneous system outputs.", "published": "2020-11-21 16:37:28", "link": "http://arxiv.org/abs/2011.10819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sensing Ambiguity in Henry James' \"The Turn of the Screw\"", "abstract": "Fields such as the philosophy of language, continental philosophy, and\nliterary studies have long established that human language is, at its essence,\nambiguous and that this quality, although challenging to communication,\nenriches language and points to the complexity of human thought. On the other\nhand, in the NLP field there have been ongoing efforts aimed at disambiguation\nfor various downstream tasks. This work brings together computational text\nanalysis and literary analysis to demonstrate the extent to which ambiguity in\ncertain texts plays a key role in shaping meaning and thus requires analysis\nrather than elimination. We revisit the discussion, well known in the\nhumanities, about the role ambiguity plays in Henry James' 19th century\nnovella, The Turn of the Screw. We model each of the novella's two competing\ninterpretations as a topic and computationally demonstrate that the duality\nbetween them exists consistently throughout the work and shapes, rather than\nobscures, its meaning. We also demonstrate that cosine similarity and word\nmover's distance are sensitive enough to detect ambiguity in its most subtle\nliterary form, despite doubts to the contrary raised by literary scholars. Our\nanalysis is built on topic word lists and word embeddings from various sources.\nWe first claim, and then empirically show, the interdependence between\ncomputational analysis and close reading performed by a human expert.", "published": "2020-11-21 17:53:41", "link": "http://arxiv.org/abs/2011.10832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Group Testing to Accelerate Deep Learning", "abstract": "Recent advances in deep learning have made the use of large, deep neural\nnetworks with tens of millions of parameters. The sheer size of these networks\nimposes a challenging computational burden during inference. Existing work\nfocuses primarily on accelerating each forward pass of a neural network.\nInspired by the group testing strategy for efficient disease testing, we\npropose neural group testing, which accelerates by testing a group of samples\nin one forward pass. Groups of samples that test negative are ruled out. If a\ngroup tests positive, samples in that group are then retested adaptively. A key\nchallenge of neural group testing is to modify a deep neural network so that it\ncould test multiple samples in one forward pass. We propose three designs to\nachieve this without introducing any new parameters and evaluate their\nperformances. We applied neural group testing in an image moderation task to\ndetect rare but inappropriate images. We found that neural group testing can\ngroup up to 16 images in one forward pass and reduce the overall computation\ncost by over 73% while improving detection performance.", "published": "2020-11-21 02:23:54", "link": "http://arxiv.org/abs/2011.10704v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular\n  Supervision for Visual Question Answering", "abstract": "The predominant approach to visual question answering (VQA) relies on\nencoding the image and question with a \"black-box\" neural encoder and decoding\na single token as the answer like \"yes\" or \"no\". Despite this approach's strong\nquantitative results, it struggles to come up with intuitive, human-readable\nforms of justification for the prediction process. To address this\ninsufficiency, we reformulate VQA as a full answer generation task, which\nrequires the model to justify its predictions in natural language. We propose\nLRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning\nframework for visual question answering that solves the problem step-by-step\nlike humans and provides human-readable form of justification at each step.\nSpecifically, LRTA learns to first convert an image into a scene graph and\nparse a question into multiple reasoning instructions. It then executes the\nreasoning instructions one at a time by traversing the scene graph using a\nrecurrent neural-symbolic execution module. Finally, it generates a full answer\nto the given question with natural language justifications. Our experiments on\nGQA dataset show that LRTA outperforms the state-of-the-art model by a large\nmargin (43.1% v.s. 28.0%) on the full answer generation task. We also create a\nperturbed GQA test set by removing linguistic cues (attributes and relations)\nin the questions for analyzing whether a model is having a smart guess with\nsuperficial data correlations. We show that LRTA makes a step towards truly\nunderstanding the question while the state-of-the-art model tends to learn\nsuperficial correlations from the training data.", "published": "2020-11-21 06:39:42", "link": "http://arxiv.org/abs/2011.10731v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Denoising with Auditory Models", "abstract": "Contemporary speech enhancement predominantly relies on audio transforms that\nare trained to reconstruct a clean speech waveform. The development of\nhigh-performing neural network sound recognition systems has raised the\npossibility of using deep feature representations as 'perceptual' losses with\nwhich to train denoising systems. We explored their utility by first training\ndeep neural networks to classify either spoken words or environmental sounds\nfrom audio. We then trained an audio transform to map noisy speech to an audio\nwaveform that minimized the difference in the deep feature representations\nbetween the output audio and the corresponding clean audio. The resulting\ntransforms removed noise substantially better than baseline methods trained to\nreconstruct clean waveforms, and also outperformed previous methods using deep\nfeature losses. However, a similar benefit was obtained simply by using losses\nderived from the filter bank inputs to the deep networks. The results show that\ndeep features can guide speech enhancement, but suggest that they do not yet\noutperform simple alternatives that do not involve learned features.", "published": "2020-11-21 02:36:58", "link": "http://arxiv.org/abs/2011.10706v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Voice Conversion based Data Augmentation in Text-Dependent\n  Speaker Verification", "abstract": "In this paper, we focus on improving the performance of the text-dependent\nspeaker verification system in the scenario of limited training data. The\nspeaker verification system deep learning based text-dependent generally needs\na large scale text-dependent training data set which could be labor and cost\nexpensive, especially for customized new wake-up words. In recent studies,\nvoice conversion systems that can generate high quality synthesized speech of\nseen and unseen speakers have been proposed. Inspired by those works, we adopt\ntwo different voice conversion methods as well as the very simple re-sampling\napproach to generate new text-dependent speech samples for data augmentation\npurposes. Experimental results show that the proposed method significantly\nimproves the Equal Error Rare performance from 6.51% to 4.51% in the scenario\nof limited training data.", "published": "2020-11-21 02:55:47", "link": "http://arxiv.org/abs/2011.10710v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Better and Faster End-to-End Model for Streaming ASR", "abstract": "End-to-end (E2E) models have shown to outperform state-of-the-art\nconventional models for streaming speech recognition [1] across many\ndimensions, including quality (as measured by word error rate (WER)) and\nendpointer latency [2]. However, the model still tends to delay the predictions\ntowards the end and thus has much higher partial latency compared to a\nconventional ASR model. To address this issue, we look at encouraging the E2E\nmodel to emit words early, through an algorithm called FastEmit [3]. Naturally,\nimproving on latency results in a quality degradation. To address this, we\nexplore replacing the LSTM layers in the encoder of our E2E model with\nConformer layers [4], which has shown good improvements for ASR. Secondly, we\nalso explore running a 2nd-pass beam search to improve quality. In order to\nensure the 2nd-pass completes quickly, we explore non-causal Conformer layers\nthat feed into the same 1st-pass RNN-T decoder, an algorithm called Cascaded\nEncoders [5]. Overall, we find that the Conformer RNN-T with Cascaded Encoders\noffers a better quality and latency tradeoff for streaming ASR.", "published": "2020-11-21 14:17:40", "link": "http://arxiv.org/abs/2011.10798v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
