{"title": "Specificity measures and reference", "abstract": "In this paper we study empirically the validity of measures of referential\nsuccess for referring expressions involving gradual properties. More\nspecifically, we study the ability of several measures of referential success\nto predict the success of a user in choosing the right object, given a\nreferring expression. Experimental results indicate that certain fuzzy measures\nof success are able to predict human accuracy in reference resolution. Such\nmeasures are therefore suitable for the estimation of the success or otherwise\nof a referring expression produced by a generation algorithm, especially in\ncase the properties in a domain cannot be assumed to have crisp denotations.", "published": "2018-09-30 07:49:22", "link": "http://arxiv.org/abs/1810.00333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Morphing", "abstract": "In this paper, we introduce a novel natural language generation task, termed\nas text morphing, which targets at generating the intermediate sentences that\nare fluency and smooth with the two input sentences. We propose the Morphing\nNetworks consisting of the editing vector generation networks and the sentence\nediting networks which are trained jointly. Specifically, the editing vectors\nare generated with a recurrent neural networks model from the lexical gap\nbetween the source sentence and the target sentence. Then the sentence editing\nnetworks iteratively generate new sentences with the current editing vector and\nthe sentence generated in the previous step. We conduct experiments with 10\nmillion text morphing sequences which are extracted from the Yelp review\ndataset. Experiment results show that the proposed method outperforms baselines\non the text morphing task. We also discuss directions and opportunities for\nfuture research of text morphing.", "published": "2018-09-30 09:04:54", "link": "http://arxiv.org/abs/1810.00341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Incremental Iterated Response Model of Pragmatics", "abstract": "Recent Iterated Response (IR) models of pragmatics conceptualize language use\nas a recursive process in which agents reason about each other to increase\ncommunicative efficiency. These models are generally defined over complete\nutterances. However, there is substantial evidence that pragmatic reasoning\ntakes place incrementally during production and comprehension. We address this\nwith an incremental IR model. We compare the incremental and global versions\nusing computational simulations, and we assess the incremental model against\nexisting experimental data and in the TUNA corpus for referring expression\ngeneration, showing that the model can capture phenomena out of reach of global\nversions.", "published": "2018-09-30 12:52:03", "link": "http://arxiv.org/abs/1810.00367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-free Sentence Embedding via Orthogonal Basis", "abstract": "We propose a simple and robust non-parameterized approach for building\nsentence representations. Inspired by the Gram-Schmidt Process in geometric\ntheory, we build an orthogonal basis of the subspace spanned by a word and its\nsurrounding context in a sentence. We model the semantic meaning of a word in a\nsentence based on two aspects. One is its relatedness to the word vector\nsubspace already spanned by its contextual words. The other is the word's novel\nsemantic meaning which shall be introduced as a new basis vector perpendicular\nto this existing subspace. Following this motivation, we develop an innovative\nmethod based on orthogonal basis to combine pre-trained word embeddings into\nsentence representations. This approach requires zero parameters, along with\nefficient inference performance. We evaluate our approach on 11 downstream NLP\ntasks. Our model shows superior performance compared with non-parameterized\nalternatives and it is competitive to other approaches relying on either large\namounts of labelled data or prolonged training time.", "published": "2018-09-30 18:26:30", "link": "http://arxiv.org/abs/1810.00438v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation of Neural Personality-based Chatbots", "abstract": "Stylistic variation is critical to render the utterances generated by\nconversational agents natural and engaging. In this paper, we focus on\nsequence-to-sequence models for open-domain dialogue response generation and\npropose a new method to evaluate the extent to which such models are able to\ngenerate responses that reflect different personality traits.", "published": "2018-09-30 21:53:47", "link": "http://arxiv.org/abs/1810.00472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phrase-Based Attentions", "abstract": "Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata.", "published": "2018-09-30 16:28:40", "link": "http://arxiv.org/abs/1810.03444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Winograd Schema: Situating Language Understanding in the\n  Data-Information-Knowledge Continuum", "abstract": "The Winograd Schema (WS) challenge, proposed as an al-ternative to the Turing\nTest, has become the new standard for evaluating progress in natural language\nunderstanding (NLU). In this paper we will not however be concerned with how\nthis challenge might be addressed. Instead, our aim here is threefold: (i) we\nwill first formally 'situate' the WS challenge in the\ndata-information-knowledge continuum, suggesting where in that continuum a good\nWS resides; (ii) we will show that a WS is just special case of a more general\nphenomenon in language understanding, namely the missing text phenomenon\n(henceforth, MTP) - in particular, we will argue that what we usually call\nthinking in the process of language understanding involves discovering a\nsignificant amount of 'missing text' - text that is not explicitly stated, but\nis often implicitly assumed as shared background knowledge; and (iii) we\nconclude by a brief discussion on why MTP is inconsistent with the data-driven\nand machine learning approach to language understanding.", "published": "2018-09-30 06:25:31", "link": "http://arxiv.org/abs/1810.00324v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neural Entity Reasoner for Global Consistency in NER", "abstract": "We propose Neural Entity Reasoner (NE-Reasoner), a framework to introduce\nglobal consistency of recognized entities into Neural Reasoner over Named\nEntity Recognition (NER) task. Given an input sentence, the NE-Reasoner layer\ncan infer over multiple entities to increase the global consistency of output\nlabels, which then be transfered into entities for the input of next layer.\nNE-Reasoner inherits and develops some features from Neural Reasoner 1) a\nsymbolic memory, allowing it to exchange entities between layers. 2) the\nspecific interaction-pooling mechanism, allowing it to connect each local word\nto multiple global entities, and 3) the deep architecture, allowing it to\nbootstrap the recognized entity set from coarse to fine. Like human beings,\nNE-Reasoner is able to accommodate ambiguous words and Name Entities that\nrarely or never met before. Despite the symbolic information the model\nintroduced, NE-Reasoner can still be trained effectively in an end-to-end\nmanner via parameter sharing strategy. NE-Reasoner can outperform conventional\nNER models in most cases on both English and Chinese NER datasets. For example,\nit achieves state-of-art on CoNLL-2003 English NER dataset.", "published": "2018-09-30 09:28:57", "link": "http://arxiv.org/abs/1810.00347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Sequence Labeling with Actor-Critic Training", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field\n(CRF) to model their output dependencies, while Recurrent Neural Networks (RNN)\nare used for the same purpose in other tasks. We set out to establish RNNs as\nan attractive alternative to CRFs for sequence labeling. To do so, we address\none of the RNN's most prominent shortcomings, the fact that it is not exposed\nto its own errors with the maximum-likelihood training. We frame the prediction\nof the output sequence as a sequential decision-making process, where we train\nthe network with an adjusted actor-critic algorithm (AC-RNN). We\ncomprehensively compare this strategy with maximum-likelihood training for both\nRNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently\nmatches the performance of the CRF on NER and CCG tagging, and outperforms it\non Machine Transliteration. We also show that our training strategy is\nsignificantly better than other techniques for addressing RNN's exposure bias,\nsuch as Scheduled Sampling, and Self-Critical policy training.", "published": "2018-09-30 17:31:52", "link": "http://arxiv.org/abs/1810.00428v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
