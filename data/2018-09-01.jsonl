{"title": "Dependency-based Hybrid Trees for Semantic Parsing", "abstract": "We propose a novel dependency-based hybrid tree model for semantic parsing,\nwhich converts natural language utterance into machine interpretable meaning\nrepresentations. Unlike previous state-of-the-art models, the semantic\ninformation is interpreted as the latent dependency between the natural\nlanguage words in our joint representation. Such dependency information can\ncapture the interactions between the semantics and natural language words. We\nintegrate a neural component into our model and propose an efficient\ndynamic-programming algorithm to perform tractable inference. Through extensive\nexperiments on the standard multilingual GeoQuery dataset with eight languages,\nwe demonstrate that our proposed approach is able to achieve state-of-the-art\nperformance across several languages. Analysis also justifies the effectiveness\nof using our new dependency-based representation.", "published": "2018-09-01 03:07:17", "link": "http://arxiv.org/abs/1809.00107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Error Propagation in Neural Machine Translation: Characteristics\n  of Language Also Matter", "abstract": "Neural machine translation usually adopts autoregressive models and suffers\nfrom exposure bias as well as the consequent error propagation problem. Many\nprevious works have discussed the relationship between error propagation and\nthe \\emph{accuracy drop} (i.e., the left part of the translated sentence is\noften better than its right part in left-to-right decoding models) problem. In\nthis paper, we conduct a series of analyses to deeply understand this problem\nand get several interesting findings. (1) The role of error propagation on\naccuracy drop is overstated in the literature, although it indeed contributes\nto the accuracy drop problem. (2) Characteristics of a language play a more\nimportant role in causing the accuracy drop: the left part of the translation\nresult in a right-branching language (e.g., English) is more likely to be more\naccurate than its right part, while the right part is more accurate for a\nleft-branching language (e.g., Japanese). Our discoveries are confirmed on\ndifferent model structures including Transformer and RNN, and in other sequence\ngeneration tasks such as text summarization.", "published": "2018-09-01 06:06:20", "link": "http://arxiv.org/abs/1809.00120v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Fusion: Return of the Language Model", "abstract": "Neural Machine Translation (NMT) typically leverages monolingual data in\ntraining through backtranslation. We investigate an alternative simple method\nto use monolingual data for NMT training: We combine the scores of a\npre-trained and fixed language model (LM) with the scores of a translation\nmodel (TM) while the TM is trained from scratch. To achieve that, we train the\ntranslation model to predict the residual probability of the training data\nadded to the prediction of the LM. This enables the TM to focus its capacity on\nmodeling the source sentence since it can rely on the LM for fluency. We show\nthat our method outperforms previous approaches to integrate LMs into NMT while\nthe architecture is simpler as it does not require gating networks to balance\nTM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test\nsets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top\nof ensembles without LM. We compare our method with alternative ways to utilize\nmonolingual data such as backtranslation, shallow fusion, and cold fusion.", "published": "2018-09-01 06:39:56", "link": "http://arxiv.org/abs/1809.00125v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Encoding for Translation Quality Estimation", "abstract": "The task of word-level quality estimation (QE) consists of taking a source\nsentence and machine-generated translation, and predicting which words in the\noutput are correct and which are wrong.\n  In this paper, propose a method to effectively encode the local and global\ncontextual information for each target word using a three-part neural network\napproach.\n  The first part uses an embedding layer to represent words and their\npart-of-speech tags in both languages. The second part leverages a\none-dimensional convolution layer to integrate local context information for\neach target word. The third part applies a stack of feed-forward and recurrent\nneural networks to further encode the global context in the sentence before\nmaking the predictions. This model was submitted as the CMU entry to the\nWMT2018 shared task on QE, and achieves strong results, ranking first in three\nof the six tracks.", "published": "2018-09-01 08:01:29", "link": "http://arxiv.org/abs/1809.00129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why is unsupervised alignment of English embeddings from different\n  algorithms so hard?", "abstract": "This paper presents a challenge to the community: Generative adversarial\nnetworks (GANs) can perfectly align independent English word embeddings induced\nusing the same algorithm, based on distributional information alone; but fails\nto do so, for two different embeddings algorithms. Why is that? We believe\nunderstanding why, is key to understand both modern word embedding algorithms\nand the limitations and instability dynamics of GANs. This paper shows that (a)\nin all these cases, where alignment fails, there exists a linear transform\nbetween the two embeddings (so algorithm biases do not lead to non-linear\ndifferences), and (b) similar effects can not easily be obtained by varying\nhyper-parameters. One plausible suggestion based on our initial experiments is\nthat the differences in the inductive biases of the embedding algorithms lead\nto an optimization landscape that is riddled with local optima, leading to a\nvery small basin of convergence, but we present this more as a challenge paper\nthan a technical contribution.", "published": "2018-09-01 10:31:57", "link": "http://arxiv.org/abs/1809.00150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIUM-CVC Submissions for WMT18 Multimodal Translation Task", "abstract": "This paper describes the multimodal Neural Machine Translation systems\ndeveloped by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This\nyear we propose several modifications to our previous multimodal attention\narchitecture in order to better integrate convolutional features and refine\nthem using encoder-side information. Our final constrained submissions ranked\nfirst for English-French and second for English-German language pairs among the\nconstrained submissions according to the automatic evaluation metric METEOR.", "published": "2018-09-01 10:54:33", "link": "http://arxiv.org/abs/1809.00151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MS-UEdin Submission to the WMT2018 APE Shared Task: Dual-Source\n  Transformer for Automatic Post-Editing", "abstract": "This paper describes the Microsoft and University of Edinburgh submission to\nthe Automatic Post-editing shared task at WMT2018. Based on training data and\nsystems from the WMT2017 shared task, we re-implement our own models from the\nlast shared task and introduce improvements based on extensive parameter\nsharing. Next we experiment with our implementation of dual-source transformer\nmodels and data selection for the IT domain. Our submissions decisively wins\nthe SMT post-editing sub-task establishing the new state-of-the-art and is a\nvery close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on\nthe rather weak results in the NMT sub-task, we hypothesize that\nneural-on-neural APE might not be actually useful.", "published": "2018-09-01 14:10:50", "link": "http://arxiv.org/abs/1809.00188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Microsoft's Submission to the WMT2018 News Translation Task: How I\n  Learned to Stop Worrying and Love the Data", "abstract": "This paper describes the Microsoft submission to the WMT2018 news translation\nshared task. We participated in one language direction -- English-German. Our\nsystem follows current best-practice and combines state-of-the-art models with\nnew data filtering (dual conditional cross-entropy filtering) and sentence\nweighting methods. We trained fairly standard Transformer-big models with an\nupdated version of Edinburgh's training scheme for WMT2017 and experimented\nwith different filtering schemes for Paracrawl. According to automatic metrics\n(BLEU) we reached the highest score for this subtask with a nearly 2 BLEU point\nmargin over the next strongest system. Based on human evaluation we ranked\nfirst among constrained systems. We believe this is mostly caused by our data\nfiltering/weighting regime.", "published": "2018-09-01 14:33:11", "link": "http://arxiv.org/abs/1809.00196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora", "abstract": "In this work we introduce dual conditional cross-entropy filtering for noisy\nparallel data. For each sentence pair of the noisy parallel corpus we compute\ncross-entropy scores according to two inverse translation models trained on\nclean data. We penalize divergent cross-entropies and weigh the penalty by the\ncross-entropy average of both models. Sorting or thresholding according to\nthese scores results in better subsets of parallel data. We achieve higher BLEU\nscores with models trained on parallel data filtered only from Paracrawl than\nwith models trained on clean WMT data. We further evaluate our method in the\ncontext of the WMT2018 shared task on parallel corpus filtering and achieve the\noverall highest ranking scores of the shared task, scoring top in three out of\nfour subtasks.", "published": "2018-09-01 14:38:16", "link": "http://arxiv.org/abs/1809.00197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Information Extraction Pipeline for Investigative\n  Journalism", "abstract": "We introduce an advanced information extraction pipeline to automatically\nprocess very large collections of unstructured textual data for the purpose of\ninvestigative journalism. The pipeline serves as a new input processor for the\nupcoming major release of our New/s/leak 2.0 software, which we develop in\ncooperation with a large German news organization. The use case is that\njournalists receive a large collection of files up to several Gigabytes\ncontaining unknown contents. Collections may originate either from official\ndisclosures of documents, e.g. Freedom of Information Act requests, or\nunofficial data leaks. Our software prepares a visually-aided exploration of\nthe collection to quickly learn about potential stories contained in the data.\nIt is based on the automatic extraction of entities and their co-occurrence in\ndocuments. In contrast to comparable projects, we focus on the following three\nmajor requirements particularly serving the use case of investigative\njournalism in cross-border collaborations: 1) composition of multiple\nstate-of-the-art NLP tools for entity extraction, 2) support of multi-lingual\ndocument sets up to 40 languages, 3) fast and easy-to-use extraction of\nfull-text, metadata and entities from various file formats.", "published": "2018-09-01 16:54:15", "link": "http://arxiv.org/abs/1809.00221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter Sharing Methods for Multilingual Self-Attentional Translation\n  Models", "abstract": "In multilingual neural machine translation, it has been shown that sharing a\nsingle translation model between multiple languages can achieve competitive\nperformance, sometimes even leading to performance gains over bilingually\ntrained models. However, these improvements are not uniform; often multilingual\nparameter sharing results in a decrease in accuracy due to translation models\nnot being able to accommodate different languages in their limited parameter\nspace. In this work, we examine parameter sharing techniques that strike a\nhappy medium between full sharing and individual training, specifically\nfocusing on the self-attentional Transformer model. We find that the full\nparameter sharing approach leads to increases in BLEU scores mainly when the\ntarget languages are from a similar language family. However, even in the case\nwhere target languages are from different families where full parameter sharing\nleads to a noticeable drop in BLEU scores, our proposed methods for partial\nsharing of parameters can lead to substantial improvements in translation\naccuracy.", "published": "2018-09-01 21:12:09", "link": "http://arxiv.org/abs/1809.00252v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Visual Relationship Detection using Semantic Modeling of Scene\n  Descriptions", "abstract": "Structured scene descriptions of images are useful for the automatic\nprocessing and querying of large image databases. We show how the combination\nof a semantic and a visual statistical model can improve on the task of mapping\nimages to their associated scene description. In this paper we consider scene\ndescriptions which are represented as a set of triples (subject, predicate,\nobject), where each triple consists of a pair of visual objects, which appear\nin the image, and the relationship between them (e.g. man-riding-elephant,\nman-wearing-hat). We combine a standard visual model for object detection,\nbased on convolutional neural networks, with a latent variable model for link\nprediction. We apply multiple state-of-the-art link prediction methods and\ncompare their capability for visual relationship detection. One of the main\nadvantages of link prediction methods is that they can also generalize to\ntriples, which have never been observed in the training data. Our experimental\nresults on the recently published Stanford Visual Relationship dataset, a\nchallenging real world dataset, show that the integration of a semantic model\nusing link prediction methods can significantly improve the results for visual\nrelationship detection. Our combined approach achieves superior performance\ncompared to the state-of-the-art method from the Stanford computer vision\ngroup.", "published": "2018-09-01 15:11:12", "link": "http://arxiv.org/abs/1809.00204v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Finding the Answers with Definition Models", "abstract": "Inspired by a previous attempt to answer crossword questions using neural\nnetworks (Hill, Cho, Korhonen, & Bengio, 2015), this dissertation implements\nextensions to improve the performance of this existing definition model on the\ntask of answering crossword questions. A discussion and evaluation of the\noriginal implementation finds that there are some ways in which the recurrent\nneural model could be extended. Insights from related fields neural language\nmodeling and neural machine translation provide the justification and means\nrequired for these extensions. Two extensions are applied to the LSTM encoder,\nfirst taking the average of LSTM states across the sequence and secondly using\na bidirectional LSTM, both implementations serve to improve model performance\non a definitions and crossword test set. In order to improve performance on\ncrossword questions, the training data is increased to include crossword\nquestions and answers, and this serves to improve results on definitions as\nwell as crossword questions. The final experiments are conducted using sub-word\nunit segmentation, first on the source side and then later preliminary\nexperimentation is conducted to facilitate character-level output. Initially,\nan exact reproduction of the baseline results proves unsuccessful. Despite\nthis, the extensions improve performance, allowing the definition model to\nsurpass the performance of the recurrent neural network variants of the\nprevious work (Hill, et al., 2015).", "published": "2018-09-01 17:21:01", "link": "http://arxiv.org/abs/1809.00224v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Extractive Adversarial Networks: High-Recall Explanations for\n  Identifying Personal Attacks in Social Media Posts", "abstract": "We introduce an adversarial method for producing high-recall explanations of\nneural text classifier decisions. Building on an existing architecture for\nextractive explanations via hard attention, we add an adversarial layer which\nscans the residual of the attention for remaining predictive signal. Motivated\nby the important domain of detecting personal attacks in social media comments,\nwe additionally demonstrate the importance of manually setting a semantically\nappropriate `default' behavior for the model by explicitly manipulating its\nbias term. We develop a validation set of human-annotated personal attacks to\nevaluate the impact of these changes.", "published": "2018-09-01 00:15:30", "link": "http://arxiv.org/abs/1809.01499v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Machine Learning Driven IoT Solution for Noise Classification in Smart\n  Cities", "abstract": "We present a machine learning based method for noise classification using a\nlow-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients\nfor audio feature extraction and supervised classification algorithms (that is,\nsupport vector machine and k-nearest neighbors) for noise classification. We\nevaluate our approach experimentally with a dataset of about 3000 sound samples\ngrouped in eight sound classes (such as, car horn, jackhammer, or street\nmusic). We explore the parameter space of support vector machine and k-nearest\nneighbors algorithms to estimate the optimal parameter values for\nclassification of sound samples in the dataset under study. We achieve a noise\nclassification accuracy in the range 85% -- 100%. Training and testing of our\nk-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than\na second for a dataset with features of more than 3000 sound samples.", "published": "2018-09-01 19:11:53", "link": "http://arxiv.org/abs/1809.00238v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
