{"title": "STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative\n  Story Understanding and Generation", "abstract": "Collaborative stories, which are texts created through the collaborative\nefforts of multiple authors with different writing styles and intentions, pose\nunique challenges for NLP models. Understanding and generating such stories\nremains an underexplored area due to the lack of open-domain corpora. To\naddress this, we introduce STORYWARS, a new dataset of over 40,000\ncollaborative stories written by 9,400 different authors from an online\nplatform. We design 12 task types, comprising 7 understanding and 5 generation\ntask types, on STORYWARS, deriving 101 diverse story-related tasks in total as\na multi-task benchmark covering all fully-supervised, few-shot, and zero-shot\nscenarios. Furthermore, we present our instruction-tuned model, INSTRUCTSTORY,\nfor the story tasks showing that instruction tuning, in addition to achieving\nsuperior results in zero-shot and few-shot scenarios, can also obtain the best\nperformance on the fully-supervised tasks in STORYWARS, establishing strong\nmulti-task benchmark performances on STORYWARS.", "published": "2023-05-14 13:09:27", "link": "http://arxiv.org/abs/2305.08152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset\n  of Film Reviews", "abstract": "This paper introduces Cro-FiReDa, a sentiment-annotated dataset for Croatian\nin the domain of movie reviews. The dataset, which contains over 10,000\nsentences, has been annotated at the sentence level. In addition to presenting\nthe overall annotation process, we also present benchmark results based on the\ntransformer-based fine-tuning approach", "published": "2023-05-14 14:46:12", "link": "http://arxiv.org/abs/2305.08173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus", "abstract": "This article presents a sentence-level sentiment dataset for the Croatian\nnews domain. In addition to the 3K annotated texts already present, our dataset\ncontains 14.5K annotated sentence occurrences that have been tagged with 5\nclasses. We provide baseline scores in addition to the annotation process and\ninter-annotator agreement.", "published": "2023-05-14 15:53:54", "link": "http://arxiv.org/abs/2305.08187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Simulate Natural Language Feedback for Interactive Semantic\n  Parsing", "abstract": "Interactive semantic parsing based on natural language (NL) feedback, where\nusers provide feedback to correct the parser mistakes, has emerged as a more\npractical scenario than the traditional one-shot semantic parsing. However,\nprior work has heavily relied on human-annotated feedback data to train the\ninteractive semantic parser, which is prohibitively expensive and not scalable.\nIn this work, we propose a new task of simulating NL feedback for interactive\nsemantic parsing. We accompany the task with a novel feedback evaluator. The\nevaluator is specifically designed to assess the quality of the simulated\nfeedback, based on which we decide the best feedback simulator from our\nproposed variants. On a text-to-SQL dataset, we show that our feedback\nsimulator can generate high-quality NL feedback to boost the error correction\nability of a specific parser. In low-data settings, our feedback simulator can\nhelp achieve comparable error correction performance as trained using the\ncostly, full set of human annotations.", "published": "2023-05-14 16:20:09", "link": "http://arxiv.org/abs/2305.08195v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cognitive Stimulation Dialogue System with Multi-source Knowledge\n  Fusion for Elders with Cognitive Impairment", "abstract": "When communicating with elders with cognitive impairment, cognitive\nstimulation (CS) help to maintain the cognitive health of elders. Data sparsity\nis the main challenge in building CS-based dialogue systems, particularly in\nthe Chinese language. To fill this gap, we construct a Chinese CS conversation\n(CSConv) dataset, which contains about 2.6K groups of dialogues with CS\nprinciples and emotional support strategy labels. Making chit chat while\nproviding emotional support is overlooked by the majority of existing cognitive\ndialogue systems. In this paper, we propose a multi-source knowledge fusion\nmethod for CS dialogue (CSD), to generate open-ended responses guided by the CS\nprinciple and emotional support strategy. We first use a progressive mask\nmethod based on external knowledge to learn encoders as effective classifiers,\nwhich is the prerequisite to predict the CS principle and emotional support\nstrategy of the target response. Then a decoder interacts with the perceived CS\nprinciple and emotional support strategy to generate responses. Extensive\nexperiments conducted on the CSConv dataset demonstrate the effectiveness of\nthe proposed method, while there is still a large space for improvement\ncompared to human performance.", "published": "2023-05-14 16:52:20", "link": "http://arxiv.org/abs/2305.08200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$SmartProbe$: A Virtual Moderator for Market Research Surveys", "abstract": "Market research surveys are a powerful methodology for understanding consumer\nperspectives at scale, but are limited by depth of understanding and insights.\nA virtual moderator can introduce elements of qualitative research into\nsurveys, developing a rapport with survey participants and dynamically asking\nprobing questions, ultimately to elicit more useful information for market\nresearchers. In this work, we introduce ${\\tt SmartProbe}$, an API which\nleverages the adaptive capabilities of large language models (LLMs), and\nincorporates domain knowledge from market research, in order to generate\neffective probing questions in any market research survey. We outline the\nmodular processing flow of $\\tt SmartProbe$, and evaluate the quality and\neffectiveness of its generated probing questions. We believe our efforts will\ninspire industry practitioners to build real-world applications based on the\nlatest advances in LLMs. Our demo is publicly available at\nhttps://nexxt.in/smartprobe-demo", "published": "2023-05-14 22:36:08", "link": "http://arxiv.org/abs/2305.08271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactKB: Generalizable Factuality Evaluation using Language Models\n  Enhanced with Factual Knowledge", "abstract": "Evaluating the factual consistency of automatically generated summaries is\nessential for the progress and adoption of reliable summarization systems.\nDespite recent advances, existing factuality evaluation models are not robust,\nbeing especially prone to entity and relation errors in new domains. We propose\nFactKB, a simple new approach to factuality evaluation that is generalizable\nacross domains, in particular with respect to entities and relations. FactKB is\nbased on language models pretrained using facts extracted from external\nknowledge bases. We introduce three types of complementary factuality\npretraining objectives based on direct entity facts, facts grounded in\nauxiliary knowledge about entities, and facts constructed compositionally\nthrough knowledge base walks. The resulting factuality evaluation model\nachieves state-of-the-art performance on two in-domain news summarization\nbenchmarks as well as on three out-of-domain scientific literature datasets.\nFurther analysis of FactKB shows improved ability to detect erroneous entities\nand relations in summaries and is robust and generalizable across domains.", "published": "2023-05-14 23:58:05", "link": "http://arxiv.org/abs/2305.08281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Prompt-based Black-Box Tuning Colorful: Boosting Model\n  Generalization from Three Orthogonal Perspectives", "abstract": "Large language models (LLMs) have shown increasing power on various natural\nlanguage processing (NLP) tasks. However, tuning these models for downstream\ntasks usually needs exorbitant costs or is unavailable due to commercial\nconsiderations. Recently, black-box tuning has been proposed to address this\nproblem by optimizing task-specific prompts without accessing the gradients and\nhidden representations. However, most existing works have yet fully exploited\nthe potential of gradient-free optimization under the scenario of few-shot\nlearning. In this paper, we describe BBT-RGB, a suite of straightforward and\ncomplementary techniques for enhancing the efficiency and performance of\nblack-box optimization. Specifically, our method includes three plug-and-play\ncomponents: (1) Two-stage derivative-free optimization strategy that\nfacilitates fast convergence and mitigates overfitting; (2) Automatic\nverbalizer construction with its novel usage under few-shot settings; (3)\nBetter prompt initialization policy based on instruction search and\nauto-selected demonstration. Extensive experiments across various tasks on\nnatural language understanding and inference demonstrate the effectiveness of\nour method. Our codes are publicly available at\nhttps://github.com/QiushiSun/BBT-RGB.", "published": "2023-05-14 07:33:59", "link": "http://arxiv.org/abs/2305.08088v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Understanding and Improving Knowledge Distillation for Neural\n  Machine Translation", "abstract": "Knowledge distillation (KD) is a promising technique for model compression in\nneural machine translation. However, where the knowledge hides in KD is still\nnot clear, which may hinder the development of KD. In this work, we first\nunravel this mystery from an empirical perspective and show that the knowledge\ncomes from the top-1 predictions of teachers, which also helps us build a\npotential connection between word- and sequence-level KD. Further, we point out\ntwo inherent issues in vanilla word-level KD based on this finding. Firstly,\nthe current objective of KD spreads its focus to whole distributions to learn\nthe knowledge, yet lacks special treatment on the most crucial top-1\ninformation. Secondly, the knowledge is largely covered by the golden\ninformation due to the fact that most top-1 predictions of teachers overlap\nwith ground-truth tokens, which further restricts the potential of KD. To\naddress these issues, we propose a novel method named \\textbf{T}op-1\n\\textbf{I}nformation \\textbf{E}nhanced \\textbf{K}nowledge \\textbf{D}istillation\n(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the\nlearning of the top-1 information from the teacher. Additionally, we develop an\niterative KD procedure to infuse more additional knowledge by distilling on the\ndata without ground-truth targets. Experiments on WMT'14 English-German, WMT'14\nEnglish-French and WMT'16 English-Romanian demonstrate that our method can\nrespectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU\nscores and significantly outperform the vanilla word-level KD baseline.\nBesides, our method shows higher generalizability on different teacher-student\ncapacity gaps than existing KD techniques.", "published": "2023-05-14 08:23:03", "link": "http://arxiv.org/abs/2305.08096v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distinguish Before Answer: Generating Contrastive Explanation as\n  Knowledge for Commonsense Question Answering", "abstract": "Existing knowledge-enhanced methods have achieved remarkable results in\ncertain QA tasks via obtaining diverse knowledge from different knowledge\nbases. However, limited by the properties of retrieved knowledge, they still\nhave trouble benefiting from both the knowledge relevance and distinguishment\nsimultaneously. To address the challenge, we propose CPACE, a Concept-centric\nPrompt-bAsed Contrastive Explanation Generation model, which aims to convert\nobtained symbolic knowledge into a contrastive explanation for better\ndistinguishing the differences among given candidates. Firstly, following\nprevious works, we retrieve different types of symbolic knowledge with a\nconcept-centric knowledge extraction module. After that, we generate\ncorresponding contrastive explanations using acquired symbolic knowledge and\nexplanation prompts as guidance for better modeling the knowledge\ndistinguishment and interpretability. Finally, we regard the generated\ncontrastive explanation as external knowledge for downstream task enhancement.\nWe conduct a series of experiments on three widely-used question-answering\ndatasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the\nhelp of generated contrastive explanation, our CPACE model achieves new SOTA on\nCSQA (89.8% on the testing set, 0.9% higher than human performance), and gains\nimpressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).", "published": "2023-05-14 12:12:24", "link": "http://arxiv.org/abs/2305.08135v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ParaLS: Lexical Substitution via Pretrained Paraphraser", "abstract": "Lexical substitution (LS) aims at finding appropriate substitutes for a\ntarget word in a sentence. Recently, LS methods based on pretrained language\nmodels have made remarkable progress, generating potential substitutes for a\ntarget word through analysis of its contextual surroundings. However, these\nmethods tend to overlook the preservation of the sentence's meaning when\ngenerating the substitutes. This study explores how to generate the substitute\ncandidates from a paraphraser, as the generated paraphrases from a paraphraser\ncontain variations in word choice and preserve the sentence's meaning. Since we\ncannot directly generate the substitutes via commonly used decoding strategies,\nwe propose two simple decoding strategies that focus on the variations of the\ntarget word during decoding. Experimental results show that our methods\noutperform state-of-the-art LS methods based on pre-trained language models on\nthree benchmarks.", "published": "2023-05-14 12:49:16", "link": "http://arxiv.org/abs/2305.08146v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Generalize for Cross-domain QA", "abstract": "There have been growing concerns regarding the out-of-domain generalization\nability of natural language processing (NLP) models, particularly in\nquestion-answering (QA) tasks. Current synthesized data augmentation methods\nfor QA are hampered by increased training costs. To address this issue, we\npropose a novel approach that combines prompting methods and linear probing\nthen fine-tuning strategy, which does not entail additional cost. Our method\nhas been theoretically and empirically shown to be effective in enhancing the\ngeneralization ability of both generative and discriminative models. Our\napproach outperforms state-of-the-art baselines, with an average increase in F1\nscore of 4.5%-7.9%. Furthermore, our method can be easily integrated into any\npre-trained models and offers a promising solution to the under-explored\ncross-domain QA task. We release our source code at GitHub*.", "published": "2023-05-14 17:53:54", "link": "http://arxiv.org/abs/2305.08208v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Watermarking Text Generated by Black-Box Language Models", "abstract": "LLMs now exhibit human-like skills in various fields, leading to worries\nabout misuse. Thus, detecting generated text is crucial. However, passive\ndetection methods are stuck in domain specificity and limited adversarial\nrobustness. To achieve reliable detection, a watermark-based method was\nproposed for white-box LLMs, allowing them to embed watermarks during text\ngeneration. The method involves randomly dividing the model vocabulary to\nobtain a special list and adjusting the probability distribution to promote the\nselection of words in the list. A detection algorithm aware of the list can\nidentify the watermarked text. However, this method is not applicable in many\nreal-world scenarios where only black-box language models are available. For\ninstance, third-parties that develop API-based vertical applications cannot\nwatermark text themselves because API providers only supply generated text and\nwithhold probability distributions to shield their commercial interests. To\nallow third-parties to autonomously inject watermarks into generated text, we\ndevelop a watermarking framework for black-box language model usage scenarios.\nSpecifically, we first define a binary encoding function to compute a random\nbinary encoding corresponding to a word. The encodings computed for\nnon-watermarked text conform to a Bernoulli distribution, wherein the\nprobability of a word representing bit-1 being approximately 0.5. To inject a\nwatermark, we alter the distribution by selectively replacing words\nrepresenting bit-0 with context-based synonyms that represent bit-1. A\nstatistical test is then used to identify the watermark. Experiments\ndemonstrate the effectiveness of our method on both Chinese and English\ndatasets. Furthermore, results under re-translation, polishing, word deletion,\nand synonym substitution attacks reveal that it is arduous to remove the\nwatermark without compromising the original semantics.", "published": "2023-05-14 07:37:33", "link": "http://arxiv.org/abs/2305.08883v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic-aware Dynamic Retrospective-Prospective Reasoning for\n  Event-level Video Question Answering", "abstract": "Event-Level Video Question Answering (EVQA) requires complex reasoning across\nvideo events to obtain the visual information needed to provide optimal\nanswers. However, despite significant progress in model performance, few\nstudies have focused on using the explicit semantic connections between the\nquestion and visual information especially at the event level. There is need\nfor using such semantic connections to facilitate complex reasoning across\nvideo frames. Therefore, we propose a semantic-aware dynamic\nretrospective-prospective reasoning approach for video-based question\nanswering. Specifically, we explicitly use the Semantic Role Labeling (SRL)\nstructure of the question in the dynamic reasoning process where we decide to\nmove to the next frame based on which part of the SRL structure (agent, verb,\npatient, etc.) of the question is being focused on. We conduct experiments on a\nbenchmark EVQA dataset - TrafficQA. Results show that our proposed approach\nachieves superior performance compared to previous state-of-the-art models. Our\ncode will be made publicly available for research use.", "published": "2023-05-14 03:57:11", "link": "http://arxiv.org/abs/2305.08059v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving End-to-End SLU performance with Prosodic Attention and\n  Distillation", "abstract": "Most End-to-End SLU methods depend on the pretrained ASR or language model\nfeatures for intent prediction. However, other essential information in speech,\nsuch as prosody, is often ignored. Recent research has shown improved results\nin classifying dialogue acts by incorporating prosodic information. The margins\nof improvement in these methods are minimal as the neural models ignore\nprosodic features. In this work, we propose prosody-attention, which uses the\nprosodic features differently to generate attention maps across time frames of\nthe utterance. Then we propose prosody-distillation to explicitly learn the\nprosodic information in the acoustic encoder rather than concatenating the\nimplicit prosodic features. Both the proposed methods improve the baseline\nresults, and the prosody-distillation method gives an intent classification\naccuracy improvement of 8\\% and 2\\% on SLURP and STOP datasets over the prosody\nbaseline.", "published": "2023-05-14 04:38:20", "link": "http://arxiv.org/abs/2305.08067v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-supervised Neural Factor Analysis for Disentangling Utterance-level\n  Speech Representations", "abstract": "Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have\ndemonstrated state-of-the-art performance on automatic speech recognition (ASR)\nand proved to be extremely useful in low label-resource settings. However, the\nsuccess of SSL models has yet to transfer to utterance-level tasks such as\nspeaker, emotion, and language recognition, which still require supervised\nfine-tuning of the SSL models to obtain good performance. We argue that the\nproblem is caused by the lack of disentangled representations and an\nutterance-level learning objective for these tasks. Inspired by how HuBERT uses\nclustering to discover hidden acoustic units, we formulate a factor analysis\n(FA) model that uses the discovered hidden acoustic units to align the SSL\nfeatures. The underlying utterance-level representations are disentangled from\nthe content of speech using probabilistic inference on the aligned features.\nFurthermore, the variational lower bound derived from the FA model provides an\nutterance-level objective, allowing error gradients to be backpropagated to the\nTransformer layers to learn highly discriminative acoustic units. When used in\nconjunction with HuBERT's masked prediction training, our models outperform the\ncurrent best model, WavLM, on all utterance-level non-semantic tasks on the\nSUPERB benchmark with only 20% of labeled data.", "published": "2023-05-14 08:26:24", "link": "http://arxiv.org/abs/2305.08099v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement", "abstract": "Multi-frame algorithms for single-channel speech enhancement are able to take\nadvantage from short-time correlations within the speech signal. Deep Filtering\n(DF) was proposed to directly estimate a complex filter in frequency domain to\ntake advantage of these correlations. In this work, we present a real-time\nspeech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is\nenabled by exploiting domain knowledge of speech production and psychoacoustic\nperception. Our model is able to match state-of-the-art speech enhancement\nbenchmarks while achieving a real-time-factor of 0.19 on a single threaded\nnotebook CPU. The framework as well as pretrained weights have been published\nunder an open source license.", "published": "2023-05-14 19:09:35", "link": "http://arxiv.org/abs/2305.08227v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Non-linguistic Skills without Sacrificing Linguistic\n  Proficiency", "abstract": "The field of Math-NLP has witnessed significant growth in recent years,\nmotivated by the desire to expand LLM performance to the learning of\nnon-linguistic notions (numerals, and subsequently, arithmetic reasoning).\nHowever, non-linguistic skill injection typically comes at a cost for LLMs: it\nleads to catastrophic forgetting of core linguistic skills, a consequence that\noften remains unaddressed in the literature. As Math-NLP has been able to\ncreate LLMs that can closely approximate the mathematical skills of a\ngrade-schooler or the arithmetic reasoning skills of a calculator, the\npracticality of these models fail if they concomitantly shed their linguistic\ncapabilities. In this work, we take a closer look into the phenomena of\ncatastrophic forgetting as it pertains to LLMs and subsequently offer a novel\nframework for non-linguistic skill injection for LLMs based on information\ntheoretic interventions and skill-specific losses that enable the learning of\nstrict arithmetic reasoning. Our model outperforms the state-of-the-art both on\ninjected non-linguistic skills and on linguistic knowledge retention, and does\nso with a fraction of the non-linguistic training data (1/4) and zero\nadditional synthetic linguistic training data.", "published": "2023-05-14 20:57:11", "link": "http://arxiv.org/abs/2305.08246v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science\n  Language Tasks Using Text-to-Schema Modeling", "abstract": "We present MatSci-NLP, a natural language benchmark for evaluating the\nperformance of natural language processing (NLP) models on materials science\ntext. We construct the benchmark from publicly available materials science text\ndata to encompass seven different NLP tasks, including conventional NLP tasks\nlike named entity recognition and relation classification, as well as NLP tasks\nspecific to materials science, such as synthesis action retrieval which relates\nto creating synthesis procedures for materials. We study various BERT-based\nmodels pretrained on different scientific text corpora on MatSci-NLP to\nunderstand the impact of pretraining strategies on understanding materials\nscience text. Given the scarcity of high-quality annotated data in the\nmaterials science domain, we perform our fine-tuning experiments with limited\ntraining data to encourage the generalize across MatSci-NLP tasks. Our\nexperiments in this low-resource training setting show that language models\npretrained on scientific text outperform BERT trained on general text. MatBERT,\na model pretrained specifically on materials science journals, generally\nperforms best for most tasks. Moreover, we propose a unified text-to-schema for\nmultitask learning on \\benchmark and compare its performance with traditional\nfine-tuning methods. In our analysis of different training methods, we find\nthat our proposed text-to-schema methods inspired by question-answering\nconsistently outperform single and multitask NLP fine-tuning methods. The code\nand datasets are publicly available at\n\\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.", "published": "2023-05-14 22:01:24", "link": "http://arxiv.org/abs/2305.08264v1", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Multi-Frame Filtering for Hearing Aids", "abstract": "Multi-frame algorithms for single-channel speech enhancement are able to take\nadvantage from short-time correlations within the speech signal. Deep filtering\n(DF) recently demonstrated its capabilities for low-latency scenarios like\nhearing aids with its complex multi-frame (MF) filter. Alternatively, the\ncomplex filter can be estimated via an MF minimum variance distortionless\nresponse (MVDR), or MF Wiener filter (WF). Previous studies have shown that\nincorporating algorithm domain knowledge using an MVDR filter might be\nbeneficial compared to the direct filter estimation via DF. In this work, we\ncompare the usage of various multi-frame filters such as DF, MF-MVDR, or MF-WF\nfor HAs. We assess different covariance estimation methods for both MF-MVDR and\nMF-WF and objectively demonstrate an improved performance compared to direct DF\nestimation, significantly outperforming related work while improving the\nruntime performance.", "published": "2023-05-14 18:59:14", "link": "http://arxiv.org/abs/2305.08225v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "REMAST: Real-time Emotion-based Music Arrangement with Soft Transition", "abstract": "Music as an emotional intervention medium has important applications in\nscenarios such as music therapy, games, and movies. However, music needs\nreal-time arrangement according to changing emotions, bringing challenges to\nbalance emotion real-time fit and soft emotion transition due to the\nfine-grained and mutable nature of the target emotion. Existing studies mainly\nfocus on achieving emotion real-time fit, while the issue of smooth transition\nremains understudied, affecting the overall emotional coherence of the music.\nIn this paper, we propose REMAST to address this trade-off. Specifically, we\nrecognize the last timestep's music emotion and fuse it with the current\ntimestep's input emotion. The fused emotion then guides REMAST to generate the\nmusic based on the input melody. To adjust music similarity and emotion\nreal-time fit flexibly, we downsample the original melody and feed it into the\ngeneration model. Furthermore, we design four music theory features by domain\nknowledge to enhance emotion information and employ semi-supervised learning to\nmitigate the subjective bias introduced by manual dataset annotation. According\nto the evaluation results, REMAST surpasses the state-of-the-art methods in\nobjective and subjective metrics. These results demonstrate that REMAST\nachieves real-time fit and smooth transition simultaneously, enhancing the\ncoherence of the generated music.", "published": "2023-05-14 00:09:48", "link": "http://arxiv.org/abs/2305.08029v3", "categories": ["cs.SD", "cs.AI", "eess.AS", "H.5.5; F.2.2"], "primary_category": "cs.SD"}
