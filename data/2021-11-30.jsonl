{"title": "Learning to Predict Persona Information forDialogue Personalization\n  without Explicit Persona Description", "abstract": "Personalizing dialogue agents is important for dialogue systems to generate\nmore specific, consistent, and engaging responses. However, most current\ndialogue personalization approaches rely on explicit persona descriptions\nduring inference, which severely restricts its application. In this paper, we\npropose a novel approach that learns to predict persona information based on\nthe dialogue history to personalize the dialogue agent without relying on any\nexplicit persona descriptions during inference. Experimental results on the\nPersonaChat dataset show that the proposed method can improve the consistency\nof generated responses when conditioning on the predicted profile of the\ndialogue agent (i.e. \"self persona\"), and improve the engagingness of the\ngenerated responses when conditioning on the predicted persona of the dialogue\npartner (i.e. \"their persona\"). We also find that a trained persona prediction\nmodel can be successfully transferred to other datasets and help generate more\nrelevant responses.", "published": "2021-11-30 03:19:24", "link": "http://arxiv.org/abs/2111.15093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improvement in Machine Translation with Generative Adversarial Networks", "abstract": "In this paper, we explore machine translation improvement via Generative\nAdversarial Network (GAN) architecture. We take inspiration from RelGAN, a\nmodel for text generation, and NMT-GAN, an adversarial machine translation\nmodel, to implement a model that learns to transform awkward, non-fluent\nEnglish sentences to fluent ones, while only being trained on monolingual\ncorpora. We utilize a parameter $\\lambda$ to control the amount of deviation\nfrom the input sentence, i.e. a trade-off between keeping the original tokens\nand modifying it to be more fluent. Our results improved upon phrase-based\nmachine translation in some cases. Especially, GAN with a transformer generator\nshows some promising results. We suggests some directions for future works to\nbuild upon this proof-of-concept.", "published": "2021-11-30 06:51:13", "link": "http://arxiv.org/abs/2111.15166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards automatic identification of linguistic politeness in Hindi texts", "abstract": "In this paper I present a classifier for automatic identification of\nlinguistic politeness in Hindi texts. I have used the manually annotated corpus\nof over 25,000 blog comments to train an SVM. Making use of the discursive and\ninteractional approaches to politeness the paper gives an exposition of the\nnormative, conventionalised politeness structures of Hindi. It is seen that\nusing these manually recognised structures as features in training the SVM\nsignificantly improves the performance of the classifier on the test set. The\ntrained system gives a significantly high accuracy of over 77% which is within\n2% of human accuracy.", "published": "2021-11-30 10:32:17", "link": "http://arxiv.org/abs/2111.15268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Topic Models for Comparable Corpora", "abstract": "Probabilistic topic models like Latent Dirichlet Allocation (LDA) have been\npreviously extended to the bilingual setting. A fundamental modeling assumption\nin several of these extensions is that the input corpora are in the form of\ndocument pairs whose constituent documents share a single topic distribution.\nHowever, this assumption is strong for comparable corpora that consist of\ndocuments thematically similar to an extent only, which are, in turn, the most\ncommonly available or easy to obtain. In this paper we relax this assumption by\nproposing for the paired documents to have separate, yet bound topic\ndistributions. % a binding mechanism between the distributions of the paired\ndocuments. We suggest that the strength of the bound should depend on each\npair's semantic similarity. To estimate the similarity of documents that are\nwritten in different languages we use cross-lingual word embeddings that are\nlearned with shallow neural networks. We evaluate the proposed binding\nmechanism by extending two topic models: a bilingual adaptation of LDA that\nassumes bag-of-words inputs and a model that incorporates part of the text\nstructure in the form of boundaries of semantically coherent segments. To\nassess the performance of the novel topic models we conduct intrinsic and\nextrinsic experiments on five bilingual, comparable corpora of English\ndocuments with French, German, Italian, Spanish and Portuguese documents. The\nresults demonstrate the efficiency of our approach in terms of both topic\ncoherence measured by the normalized point-wise mutual information, and\ngeneralization performance measured by perplexity and in terms of Mean\nReciprocal Rank in a cross-lingual document retrieval task for each of the\nlanguage pairs.", "published": "2021-11-30 10:53:41", "link": "http://arxiv.org/abs/2111.15278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Developing LRs for Non-Scheduled Languages: A Case of\n  Magahi", "abstract": "Magahi is an Indo-Aryan Language, spoken mainly in the Eastern parts of\nIndia. Despite having a significant number of speakers, there has been\nvirtually no language resource (LR) or language technology (LT) developed for\nthe language, mainly because of its status as a non-scheduled language. The\npresent paper describes an attempt to develop an annotated corpus of Magahi.\nThe data is mainly taken from a couple of blogs in Magahi, some collection of\nstories in Magahi and the recordings of conversation in Magahi and it is\nannotated at the POS level using BIS tagset.", "published": "2021-11-30 12:07:23", "link": "http://arxiv.org/abs/2111.15322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minor changes make a difference: a case study on the consistency of\n  UD-based dependency parsers", "abstract": "Many downstream applications are using dependency trees, and are thus relying\non dependency parsers producing correct, or at least consistent, output.\nHowever, dependency parsers are trained using machine learning, and are\ntherefore susceptible to unwanted inconsistencies due to biases in the training\ndata. This paper explores the effects of such biases in four languages -\nEnglish, Swedish, Russian, and Ukrainian - though an experiment where we study\nthe effect of replacing numerals in sentences. We show that such seemingly\ninsignificant changes in the input can cause large differences in the output,\nand suggest that data augmentation can remedy the problems.", "published": "2021-11-30 14:06:55", "link": "http://arxiv.org/abs/2111.15413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Transformers on Word Sense Disambiguation", "abstract": "Recent years of research in Natural Language Processing (NLP) have witnessed\ndramatic growth in training large models for generating context-aware language\nrepresentations. In this regard, numerous NLP systems have leveraged the power\nof neural network-based architectures to incorporate sense information in\nembeddings, resulting in Contextualized Word Embeddings (CWEs). Despite this\nprogress, the NLP community has not witnessed any significant work performing a\ncomparative study on the contextualization power of such architectures. This\npaper presents a comparative study and an extensive analysis of nine widely\nadopted Transformer models. These models are BERT, CTRL, DistilBERT,\nOpenAI-GPT, OpenAI-GPT2, Transformer-XL, XLNet, ELECTRA, and ALBERT. We\nevaluate their contextualization power using two lexical sample Word Sense\nDisambiguation (WSD) tasks, SensEval-2 and SensEval-3. We adopt a simple yet\neffective approach to WSD that uses a k-Nearest Neighbor (kNN) classification\non CWEs. Experimental results show that the proposed techniques also achieve\nsuperior results over the current state-of-the-art on both the WSD tasks", "published": "2021-11-30 14:10:22", "link": "http://arxiv.org/abs/2111.15417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KARL-Trans-NER: Knowledge Aware Representation Learning for Named Entity\n  Recognition using Transformers", "abstract": "The inception of modeling contextual information using models such as BERT,\nELMo, and Flair has significantly improved representation learning for words.\nIt has also given SOTA results in almost every NLP task - Machine Translation,\nText Summarization and Named Entity Recognition, to name a few. In this work,\nin addition to using these dominant context-aware representations, we propose a\nKnowledge Aware Representation Learning (KARL) Network for Named Entity\nRecognition (NER). We discuss the challenges of using existing methods in\nincorporating world knowledge for NER and show how our proposed methods could\nbe leveraged to overcome those challenges. KARL is based on a Transformer\nEncoder that utilizes large knowledge bases represented as fact triplets,\nconverts them to a graph context, and extracts essential entity information\nresiding inside to generate contextualized triplet representation for feature\naugmentation. Experimental results show that the augmentation done using KARL\ncan considerably boost the performance of our NER system and achieve\nsignificantly better results than existing approaches in the literature on\nthree publicly available NER datasets, namely CoNLL 2003, CoNLL++, and\nOntoNotes v5. We also observe better generalization and application to a\nreal-world setting from KARL on unseen entities.", "published": "2021-11-30 14:29:33", "link": "http://arxiv.org/abs/2111.15436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Mining Drug/Chemical-Protein Interactions using an Ensemble of BERT\n  and T5 Based Models", "abstract": "In Track-1 of the BioCreative VII Challenge participants are asked to\nidentify interactions between drugs/chemicals and proteins. In-context named\nentity annotations for each drug/chemical and protein are provided and one of\nfourteen different interactions must be automatically predicted. For this\nrelation extraction task, we attempt both a BERT-based sentence classification\napproach, and a more novel text-to-text approach using a T5 model. We find that\nlarger BERT-based models perform better in general, with our BioMegatron-based\nmodel achieving the highest scores across all metrics, achieving 0.74 F1 score.\nThough our novel T5 text-to-text method did not perform as well as most of our\nBERT-based models, it outperformed those trained on similar data, showing\npromising results, achieving 0.65 F1 score. We believe a text-to-text approach\nto relation extraction has some competitive advantages and there is a lot of\nroom for research advancement.", "published": "2021-11-30 18:14:06", "link": "http://arxiv.org/abs/2111.15617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chemical Identification and Indexing in PubMed Articles via BERT and\n  Text-to-Text Approaches", "abstract": "The Biocreative VII Track-2 challenge consists of named entity recognition,\nentity-linking (or entity-normalization), and topic indexing tasks -- with\nentities and topics limited to chemicals for this challenge. Named entity\nrecognition is a well-established problem and we achieve our best performance\nwith BERT-based BioMegatron models. We extend our BERT-based approach to the\nentity linking task. After the second stage of pretraining BioBERT with a\nmetric-learning loss strategy called self-alignment pretraining (SAP), we link\nentities based on the cosine similarity between their SAP-BioBERT word\nembeddings. Despite the success of our named entity recognition experiments, we\nfind the chemical indexing task generally more challenging.\n  In addition to conventional NER methods, we attempt both named entity\nrecognition and entity linking with a novel text-to-text or \"prompt\" based\nmethod that uses generative language models such as T5 and GPT. We achieve\nencouraging results with this new approach.", "published": "2021-11-30 18:21:06", "link": "http://arxiv.org/abs/2111.15622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Medication Names in Tweets as Named Entity\n  Recognition", "abstract": "Social media posts contain potentially valuable information about medical\nconditions and health-related behavior. Biocreative VII Task 3 focuses on\nmining this information by recognizing mentions of medications and dietary\nsupplements in tweets. We approach this task by fine tuning multiple BERT-style\nlanguage models to perform token-level classification, and combining them into\nensembles to generate final predictions. Our best system consists of five\nMegatron-BERT-345M models and achieves a strict F1 score of 0.764 on unseen\ntest data.", "published": "2021-11-30 18:25:32", "link": "http://arxiv.org/abs/2111.15641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Rich Product Descriptions for Conversational E-commerce\n  Systems", "abstract": "Through recent advancements in speech technologies and introduction of smart\nassistants, such as Amazon Alexa, Apple Siri and Google Home, increasing number\nof users are interacting with various applications through voice commands.\nE-commerce companies typically display short product titles on their webpages,\neither human-curated or algorithmically generated, when brevity is required.\nHowever, these titles are dissimilar from natural spoken language. For example,\n\"Lucky Charms Gluten Free Break-fast Cereal, 20.5 oz a box Lucky Charms Gluten\nFree\" is acceptable to display on a webpage, while a similar title cannot be\nused in a voice based text-to-speech application. In such conversational\nsystems, an easy to comprehend sentence, such as \"a 20.5 ounce box of lucky\ncharms gluten free cereal\" is preferred. Compared to display devices, where\nimages and detailed product information can be presented to users, short titles\nfor products which convey the most important information, are necessary when\ninterfacing with voice assistants. We propose eBERT, a sequence-to-sequence\napproach by further pre-training the BERT embeddings on an e-commerce product\ndescription corpus, and then fine-tuning the resulting model to generate short,\nnatural, spoken language titles from input web titles. Our extensive\nexperiments on a real-world industry dataset, as well as human evaluation of\nmodel output, demonstrate that eBERT summarization outperforms comparable\nbaseline models. Owing to the efficacy of the model, a version of this model\nhas been deployed in real-world setting.", "published": "2021-11-30 11:22:43", "link": "http://arxiv.org/abs/2111.15298v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Do You See in this Patient? Behavioral Testing of Clinical NLP\n  Models", "abstract": "Decision support systems based on clinical notes have the potential to\nimprove patient care by pointing doctors towards overseen risks. Predicting a\npatient's outcome is an essential part of such systems, for which the use of\ndeep neural networks has shown promising results. However, the patterns learned\nby these networks are mostly opaque and previous work revealed flaws regarding\nthe reproduction of unintended biases. We thus introduce an extendable testing\nframework that evaluates the behavior of clinical outcome models regarding\nchanges of the input. The framework helps to understand learned patterns and\ntheir influence on model decisions. In this work, we apply it to analyse the\nchange in behavior with regard to the patient characteristics gender, age and\nethnicity. Our evaluation of three current clinical NLP models demonstrates the\nconcrete effects of these characteristics on the models' decisions. They show\nthat model behavior varies drastically even when fine-tuned on the same data\nand that allegedly best-performing models have not always learned the most\nmedically plausible patterns.", "published": "2021-11-30 15:52:04", "link": "http://arxiv.org/abs/2111.15512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards algorithm-free physical equilibrium model of computing", "abstract": "Our computers today, from sophisticated servers to small smartphones, operate\nbased on the same computing model, which requires running a sequence of\ndiscrete instructions, specified as an algorithm. This sequential computing\nparadigm has not yet led to a fast algorithm for an NP-complete problem despite\nnumerous attempts over the past half a century. Unfortunately, even after the\nintroduction of quantum mechanics to the world of computing, we still followed\na similar sequential paradigm, which has not yet helped us obtain such an\nalgorithm either. Here a completely different model of computing is proposed to\nreplace the sequential paradigm of algorithms with inherent parallelism of\nphysical processes. Using the proposed model, instead of writing algorithms to\nsolve NP-complete problems, we construct physical systems whose equilibrium\nstates correspond to the desired solutions and let them evolve to search for\nthe solutions. The main requirements of the model are identified and quantum\ncircuits are proposed for its potential implementation.", "published": "2021-11-30 09:48:39", "link": "http://arxiv.org/abs/2112.00006v1", "categories": ["cs.CC", "cs.CL"], "primary_category": "cs.CC"}
{"title": "Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic\n  benchmarking", "abstract": "While neural language models often perform surprisingly well on natural\nlanguage understanding (NLU) tasks, their strengths and limitations remain\npoorly understood. Controlled synthetic tasks are thus an increasingly\nimportant resource for diagnosing model behavior. In this work we focus on\nstory understanding, a core competency for NLU systems. However, the main\nsynthetic resource for story understanding, the bAbI benchmark, lacks such a\nsystematic mechanism for controllable task generation. We develop Dyna-bAbI, a\ndynamic framework providing fine-grained control over task generation in bAbI.\nWe demonstrate our ideas by constructing three new tasks requiring\ncompositional generalization, an important evaluation setting absent from the\noriginal benchmark. We tested both special-purpose models developed for bAbI as\nwell as state-of-the-art pre-trained methods, and found that while both\napproaches solve the original tasks (>99% accuracy), neither approach succeeded\nin the compositional generalization setting, indicating the limitations of the\noriginal training data. We explored ways to augment the original data, and\nfound that though diversifying training data was far more useful than simply\nincreasing dataset size, it was still insufficient for driving robust\ncompositional generalization (with <70% accuracy for complex compositions). Our\nresults underscore the importance of highly controllable task generators for\ncreating robust NLU systems through a virtuous cycle of model and data\ndevelopment.", "published": "2021-11-30 20:36:56", "link": "http://arxiv.org/abs/2112.00086v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Full-Fledged Argument Search: A Framework for Extracting and\n  Clustering Arguments from Unstructured Text", "abstract": "Argument search aims at identifying arguments in natural language texts. In\nthe past, this task has been addressed by a combination of keyword search and\nargument identification on the sentence- or document-level. However, existing\nframeworks often address only specific components of argument search and do not\naddress the following aspects: (1) argument-query matching: identifying\narguments that frame the topic slightly differently than the actual search\nquery; (2) argument identification: identifying arguments that consist of\nmultiple sentences; (3) argument clustering: selecting retrieved arguments by\ntopical aspects. In this paper, we propose a framework for addressing these\nshortcomings. We suggest (1) to combine the keyword search with precomputed\ntopic clusters for argument-query matching, (2) to apply a novel approach based\non sentence-level sequence-labeling for argument identification, and (3) to\npresent aggregated arguments to users based on topic-aware argument clustering.\nOur experiments on several real-world debate data sets demonstrate that\ndensity-based clustering algorithms, such as HDBSCAN, are particularly suitable\nfor argument-query matching. With our sentence-level, BiLSTM-based\nsequence-labeling approach we achieve a macro F1 score of 0.71. Finally,\nevaluating our argument clustering method indicates that a fine-grained\nclustering of arguments by subtopics remains challenging but is worthwhile to\nbe explored.", "published": "2021-11-30 23:05:05", "link": "http://arxiv.org/abs/2112.00160v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Refined Commonsense Knowledge from Large-Scale Web Contents", "abstract": "Commonsense knowledge (CSK) about concepts and their properties is helpful\nfor AI applications. Prior works, such as ConceptNet, have compiled large CSK\ncollections. However, they are restricted in their expressiveness to\nsubject-predicate-object (SPO) triples with simple concepts for S and strings\nfor P and O. This paper presents a method called ASCENT++ to automatically\nbuild a large-scale knowledge base (KB) of CSK assertions, with refined\nexpressiveness and both better precision and recall than prior works. ASCENT++\ngoes beyond SPO triples by capturing composite concepts with subgroups and\naspects, and by refining assertions with semantic facets. The latter is\nessential to express the temporal and spatial validity of assertions and\nfurther qualifiers. Furthermore, ASCENT++ combines open information extraction\n(OpenIE) with judicious cleaning and ranking by typicality and saliency scores.\nFor high coverage, our method taps into the large-scale crawl C4 with broad web\ncontents. The evaluation with human judgments shows the superior quality of the\nASCENT++ KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of ASCENT++. A web interface, data, and code can be accessed at\nhttps://ascentpp.mpi-inf.mpg.de/.", "published": "2021-11-30 20:26:09", "link": "http://arxiv.org/abs/2112.04596v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLP Techniques for Water Quality Analysis in Social Media Content", "abstract": "This paper presents our contributions to the MediaEval 2021 task namely\n\"WaterMM: Water Quality in Social Multimedia\". The task aims at analyzing\nsocial media posts relevant to water quality with particular focus on the\naspects like watercolor, smell, taste, and related illnesses. To this aim, a\nmultimodal dataset containing both textual and visual information along with\nmeta-data is provided. Considering the quality and quantity of available\ncontent, we mainly focus on textual information by employing three different\nmodels individually and jointly in a late-fusion manner. These models include\n(i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and a (iii) custom\nLong short-term memory (LSTM) model obtaining an overall F1-score of 0.794,\n0.717, 0.663 on the official test set, respectively. In the fusion scheme, all\nthe models are treated equally and no significant improvement is observed in\nthe performance over the best performing individual model.", "published": "2021-11-30 10:36:35", "link": "http://arxiv.org/abs/2112.11441v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automated Speech Scoring System Under The Lens: Evaluating and\n  interpreting the linguistic cues for language proficiency", "abstract": "English proficiency assessments have become a necessary metric for filtering\nand selecting prospective candidates for both academia and industry. With the\nrise in demand for such assessments, it has become increasingly necessary to\nhave the automated human-interpretable results to prevent inconsistencies and\nensure meaningful feedback to the second language learners. Feature-based\nclassical approaches have been more interpretable in understanding what the\nscoring model learns. Therefore, in this work, we utilize classical machine\nlearning models to formulate a speech scoring task as both a classification and\na regression problem, followed by a thorough study to interpret and study the\nrelation between the linguistic cues and the English proficiency level of the\nspeaker. First, we extract linguist features under five categories (fluency,\npronunciation, content, grammar and vocabulary, and acoustic) and train models\nto grade responses. In comparison, we find that the regression-based models\nperform equivalent to or better than the classification approach. Second, we\nperform ablation studies to understand the impact of each of the feature and\nfeature categories on the performance of proficiency grading. Further, to\nunderstand individual feature contributions, we present the importance of top\nfeatures on the best performing algorithm for the grading task. Third, we make\nuse of Partial Dependence Plots and Shapley values to explore feature\nimportance and conclude that the best performing trained model learns the\nunderlying rubrics used for grading the dataset used in this study.", "published": "2021-11-30 06:28:58", "link": "http://arxiv.org/abs/2111.15156v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Easy Semantification of Bioassays", "abstract": "Biological data and knowledge bases increasingly rely on Semantic Web\ntechnologies and the use of knowledge graphs for data integration, retrieval\nand federated queries. We propose a solution for automatically semantifying\nbiological assays. Our solution contrasts the problem of automated\nsemantification as labeling versus clustering where the two methods are on\nopposite ends of the method complexity spectrum. Characteristically modeling\nour problem, we find the clustering solution significantly outperforms a deep\nneural network state-of-the-art labeling approach. This novel contribution is\nbased on two factors: 1) a learning objective closely modeled after the data\noutperforms an alternative approach with sophisticated semantic modeling; 2)\nautomatically semantifying biological assays achieves a high performance F1 of\nnearly 83%, which to our knowledge is the first reported standardized\nevaluation of the task offering a strong benchmark model.", "published": "2021-11-30 07:46:07", "link": "http://arxiv.org/abs/2111.15182v2", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Text classification problems via BERT embedding method and graph\n  convolutional neural network", "abstract": "This paper presents the novel way combining the BERT embedding method and the\ngraph convolutional neural network. This combination is employed to solve the\ntext classification problem. Initially, we apply the BERT embedding method to\nthe texts (in the BBC news dataset and the IMDB movie reviews dataset) in order\nto transform all the texts to numerical vector. Then, the graph convolutional\nneural network will be applied to these numerical vectors to classify these\ntexts into their ap-propriate classes/labels. Experiments show that the\nperformance of the graph convolutional neural network model is better than the\nperfor-mances of the combination of the BERT embedding method with clas-sical\nmachine learning models.", "published": "2021-11-30 13:26:11", "link": "http://arxiv.org/abs/2111.15379v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Undecidability in Finite Transducers, Defense Systems and Finite\n  Substitutions", "abstract": "In this manuscript we present a detailed proof for undecidability of the\nequivalence of finite substitutions on regular language $b\\{0,1\\}^*c$. The\nproof is based on the works of Leonid P. Lisovik.", "published": "2021-11-30 14:14:32", "link": "http://arxiv.org/abs/2111.15420v1", "categories": ["cs.FL", "cs.CL", "cs.DM", "68Q45, 03D35,"], "primary_category": "cs.FL"}
{"title": "Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context\n  Images via Online Resources", "abstract": "Misinformation is now a major problem due to its potential high risks to our\ncore democratic and societal values and orders. Out-of-context misinformation\nis one of the easiest and effective ways used by adversaries to spread viral\nfalse stories. In this threat, a real image is re-purposed to support other\nnarratives by misrepresenting its context and/or elements. The internet is\nbeing used as the go-to way to verify information using different sources and\nmodalities. Our goal is an inspectable method that automates this\ntime-consuming and reasoning-intensive process by fact-checking the\nimage-caption pairing using Web evidence. To integrate evidence and cues from\nboth modalities, we introduce the concept of 'multi-modal cycle-consistency\ncheck'; starting from the image/caption, we gather textual/visual evidence,\nwhich will be compared against the other paired caption/image, respectively.\nMoreover, we propose a novel architecture, Consistency-Checking Network (CCN),\nthat mimics the layered human reasoning across the same and different\nmodalities: the caption vs. textual evidence, the image vs. visual evidence,\nand the image vs. caption. Our work offers the first step and benchmark for\nopen-domain, content-based, multi-modal fact-checking, and significantly\noutperforms previous baselines that did not leverage external evidence.", "published": "2021-11-30 19:36:20", "link": "http://arxiv.org/abs/2112.00061v3", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Finding, Scoring and Explaining Arguments in Bayesian Networks", "abstract": "We propose a new approach to explain Bayesian Networks. The approach revolves\naround a new definition of a probabilistic argument and the evidence it\nprovides. We define a notion of independent arguments, and propose an algorithm\nto extract a list of relevant, independent arguments given a Bayesian Network,\na target node and a set of observations. To demonstrate the relevance of the\narguments, we show how we can use the extracted arguments to approximate\nmessage passing. Finally, we show a simple scheme to explain the arguments in\nnatural language.", "published": "2021-11-30 12:41:04", "link": "http://arxiv.org/abs/2112.00799v1", "categories": ["cs.AI", "cs.CL", "stat.AP", "stat.CO"], "primary_category": "cs.AI"}
{"title": "Sentiment Analysis and Effect of COVID-19 Pandemic using College\n  SubReddit Data", "abstract": "Background: The COVID-19 pandemic has affected our society and human\nwell-being in various ways. In this study, we investigate how the pandemic has\ninfluenced people's emotions and psychological states compared to a\npre-pandemic period using real-world data from social media.\n  Method: We collected Reddit social media data from 2019 (pre-pandemic) and\n2020 (pandemic) from the subreddits communities associated with eight\nuniversities. We applied the pre-trained Robustly Optimized BERT pre-training\napproach (RoBERTa) to learn text embedding from the Reddit messages, and\nleveraged the relational information among posted messages to train a graph\nattention network (GAT) for sentiment classification. Finally, we applied model\nstacking to combine the prediction probabilities from RoBERTa and GAT to yield\nthe final classification on sentiment. With the model-predicted sentiment\nlabels on the collected data, we used a generalized linear mixed-effects model\nto estimate the effects of pandemic and in-person teaching during the pandemic\non sentiment.\n  Results: The results suggest that the odds of negative sentiments in 2020\n(pandemic) were 25.7% higher than the odds in 2019 (pre-pandemic) with a\n$p$-value $<0.001$; and the odds of negative sentiments associated in-person\nlearning were 48.3% higher than with remote learning in 2020 with a $p$-value\nof 0.029.\n  Conclusions: Our study results are consistent with the findings in the\nliterature on the negative impacts of the pandemic on people's emotions and\npsychological states. Our study contributes to the growing real-world evidence\non the various negative impacts of the pandemic on our society; it also\nprovides a good example of using both ML techniques and statistical modeling\nand inference to make better use of real-world data.", "published": "2021-11-30 19:15:06", "link": "http://arxiv.org/abs/2112.04351v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation learning through cross-modal conditional teacher-student\n  training for speech emotion recognition", "abstract": "Generic pre-trained speech and text representations promise to reduce the\nneed for large labeled datasets on specific speech and language tasks. However,\nit is not clear how to effectively adapt these representations for speech\nemotion recognition. Recent public benchmarks show the efficacy of several\npopular self-supervised speech representations for emotion classification. In\nthis study, we show that the primary difference between the top-performing\nrepresentations is in predicting valence while the differences in predicting\nactivation and dominance dimensions are less pronounced. However, we show that\neven the best-performing HuBERT representation underperforms on valence\nprediction compared to a multimodal model that also incorporates text\nrepresentation. We address this shortcoming by injecting lexical information\ninto the speech representation using the multimodal model as a teacher. To\nimprove the efficacy of our approach, we propose a novel estimate of the\nquality of the emotion predictions, to condition teacher-student training. We\nreport new audio-only state-of-the-art concordance correlation coefficient\n(CCC) values of 0.757, 0.627, 0.671 for activation, valence and dominance\npredictions, respectively, on the MSP-Podcast corpus, and also state-of-the-art\nvalues of 0.667, 0.582, 0.545 on the IEMOCAP corpus.", "published": "2021-11-30 23:02:33", "link": "http://arxiv.org/abs/2112.00158v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SP-SEDT: Self-supervised Pre-training for Sound Event Detection\n  Transformer", "abstract": "Recently, an event-based end-to-end model (SEDT) has been proposed for sound\nevent detection (SED) and achieves competitive performance. However, compared\nwith the frame-based model, it requires more training data with temporal\nannotations to improve the localization ability. Synthetic data is an\nalternative, but it suffers from a great domain gap with real recordings.\nInspired by the great success of UP-DETR in object detection, we propose to\nself-supervisedly pre-train SEDT (SP-SEDT) by detecting random patches (only\ncropped along the time axis). Experiments on the DCASE2019 task4 dataset show\nthe proposed SP-SEDT can outperform fine-tuned frame-based model. The ablation\nstudy is also conducted to investigate the impact of different loss functions\nand patch size.", "published": "2021-11-30 09:14:07", "link": "http://arxiv.org/abs/2111.15222v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model\n  with Transformer", "abstract": "In this study, we explore the transformer's ability to capture\nintra-relations among frames by augmenting the receptive field of models.\nConcretely, we propose a CycleGAN-based model with the transformer and\ninvestigate its ability in the emotional voice conversion task. In the training\nprocedure, we adopt curriculum learning to gradually increase the frame length\nso that the model can see from the short segment till the entire speech. The\nproposed method was evaluated on the Japanese emotional speech dataset and\ncompared to several baselines (ACVAE, CycleGAN) with objective and subjective\nevaluations. The results show that our proposed model is able to convert\nemotion with higher strength and quality.", "published": "2021-11-30 06:33:57", "link": "http://arxiv.org/abs/2111.15159v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound-Guided Semantic Image Manipulation", "abstract": "The recent success of the generative model shows that leveraging the\nmulti-modal embedding space can manipulate an image using text information.\nHowever, manipulating an image with other sources rather than text, such as\nsound, is not easy due to the dynamic characteristics of the sources.\nEspecially, sound can convey vivid emotions and dynamic expressions of the real\nworld. Here, we propose a framework that directly encodes sound into the\nmulti-modal (image-text) embedding space and manipulates an image from the\nspace. Our audio encoder is trained to produce a latent representation from an\naudio input, which is forced to be aligned with image and text representations\nin the multi-modal embedding space. We use a direct latent optimization method\nbased on aligned embeddings for sound-guided image manipulation. We also show\nthat our method can mix text and audio modalities, which enrich the variety of\nthe image modification. We verify the effectiveness of our sound-guided image\nmanipulation quantitatively and qualitatively. We also show that our method can\nmix different modalities, i.e., text and audio, which enrich the variety of the\nimage modification. The experiments on zero-shot audio classification and\nsemantic-level image classification show that our proposed model outperforms\nother text and sound-guided state-of-the-art methods.", "published": "2021-11-30 13:30:12", "link": "http://arxiv.org/abs/2112.00007v1", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
