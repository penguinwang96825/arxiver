{"title": "Can we obtain significant success in RST discourse parsing by using\n  Large Language Models?", "abstract": "Recently, decoder-only pre-trained large language models (LLMs), with several\ntens of billion parameters, have significantly impacted a wide range of natural\nlanguage processing (NLP) tasks. While encoder-only or encoder-decoder\npre-trained language models have already proved to be effective in discourse\nparsing, the extent to which LLMs can perform this task remains an open\nresearch question. Therefore, this paper explores how beneficial such LLMs are\nfor Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing\nprocess for both fundamental top-down and bottom-up strategies is converted\ninto prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with\nQLoRA, which has fewer parameters that can be tuned. Experimental results on\nthree benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate\nthat Llama 2 with 70 billion parameters in the bottom-up strategy obtained\nstate-of-the-art (SOTA) results with significant differences. Furthermore, our\nparsers demonstrated generalizability when evaluated on RST-DT, showing that,\nin spite of being trained with the GUM corpus, it obtained similar performances\nto those of existing parsers trained with RST-DT.", "published": "2024-03-08 05:34:29", "link": "http://arxiv.org/abs/2403.05065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROUGE-K: Do Your Summaries Have Keywords?", "abstract": "Keywords, that is, content-relevant words in summaries play an important role\nin efficient information conveyance, making it critical to assess if\nsystem-generated summaries contain such informative words during evaluation.\nHowever, existing evaluation metrics for extreme summarization models do not\npay explicit attention to keywords in summaries, leaving developers ignorant of\ntheir presence. To address this issue, we present a keyword-oriented evaluation\nmetric, dubbed ROUGE-K, which provides a quantitative answer to the question of\n-- \\textit{How well do summaries include keywords?} Through the lens of this\nkeyword-aware metric, we surprisingly find that a current strong baseline model\noften misses essential information in their summaries. Our analysis reveals\nthat human annotators indeed find the summaries with more keywords to be more\nrelevant to the source documents. This is an important yet previously\noverlooked aspect in evaluating summarization systems. Finally, to enhance\nkeyword inclusion, we propose four approaches for incorporating word importance\ninto a transformer-based model and experimentally show that it enables guiding\nmodels to include more keywords while keeping the overall quality. Our code is\nreleased at https://github.com/sobamchan/rougek.", "published": "2024-03-08 09:54:56", "link": "http://arxiv.org/abs/2403.05186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for\n  Monolingual Semantic Textual Similarity", "abstract": "Learning better sentence embeddings leads to improved performance for natural\nlanguage understanding tasks including semantic textual similarity (STS) and\nnatural language inference (NLI). As prior studies leverage large-scale labeled\nNLI datasets for fine-tuning masked language models to yield sentence\nembeddings, task performance for languages other than English is often left\nbehind. In this study, we directly compared two data augmentation techniques as\npotential solutions for monolingual STS: (a) cross-lingual transfer that\nexploits English resources alone as training data to yield non-English sentence\nembeddings as zero-shot inference, and (b) machine translation that coverts\nEnglish data into pseudo non-English training data in advance. In our\nexperiments on monolingual STS in Japanese and Korean, we find that the two\ndata techniques yield performance on par. Rather, we find a superiority of the\nWikipedia domain over the NLI domain for these languages, in contrast to prior\nstudies that focused on NLI as training data. Combining our findings, we\ndemonstrate that the cross-lingual transfer of Wikipedia data exhibits improved\nperformance, and that native Wikipedia data can further improve performance for\nmonolingual STS.", "published": "2024-03-08 12:28:15", "link": "http://arxiv.org/abs/2403.05257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACLSum: A New Dataset for Aspect-based Summarization of Scientific\n  Publications", "abstract": "Extensive efforts in the past have been directed toward the development of\nsummarization datasets. However, a predominant number of these resources have\nbeen (semi)-automatically generated, typically through web data crawling,\nresulting in subpar resources for training and evaluating summarization\nsystems, a quality compromise that is arguably due to the substantial costs\nassociated with generating ground-truth summaries, particularly for diverse\nlanguages and specialized domains. To address this issue, we present ACLSum, a\nnovel summarization dataset carefully crafted and evaluated by domain experts.\nIn contrast to previous datasets, ACLSum facilitates multi-aspect summarization\nof scientific papers, covering challenges, approaches, and outcomes in depth.\nThrough extensive experiments, we evaluate the quality of our resource and the\nperformance of models based on pretrained language models and state-of-the-art\nlarge language models (LLMs). Additionally, we explore the effectiveness of\nextractive versus abstractive summarization within the scholarly domain on the\nbasis of automatically discovered aspects. Our results corroborate previous\nfindings in the general domain and indicate the general superiority of\nend-to-end aspect-based summarization. Our data is released at\nhttps://github.com/sobamchan/aclsum.", "published": "2024-03-08 13:32:01", "link": "http://arxiv.org/abs/2403.05303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consecutive Batch Model Editing with HooK Layers", "abstract": "As the typical retraining paradigm is unacceptably time- and\nresource-consuming, researchers are turning to model editing to find an\neffective way that supports both consecutive and batch scenarios to edit the\nmodel behavior directly. Despite all these practical expectations, existing\nmodel editing methods fail to realize all of them. Furthermore, the memory\ndemands for such sequential model editing approaches tend to be prohibitive,\nfrequently necessitating an external memory that grows incrementally over time.\nTo cope with these challenges, we propose CoachHooK, a model editing method\nthat simultaneously supports sequential and batch editing. CoachHooK is\nmemory-friendly as it only needs a small amount of it to store several hook\nlayers whose size remains unchanged over time. Experimental results demonstrate\nthe superiority of our method over other batch-supportive model editing methods\nunder both single-round and consecutive batch editing scenarios. Extensive\nanalyses of CoachHooK have been conducted to verify the stability of our method\nover a number of consecutive steps.", "published": "2024-03-08 14:07:44", "link": "http://arxiv.org/abs/2403.05330v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Pre-Trained Language Models with Attribution Scores: An\n  Analysis in Low-Resource Settings", "abstract": "Attribution scores indicate the importance of different input parts and can,\nthus, explain model behaviour. Currently, prompt-based models are gaining\npopularity, i.a., due to their easier adaptability in low-resource settings.\nHowever, the quality of attribution scores extracted from prompt-based models\nhas not been investigated yet. In this work, we address this topic by analyzing\nattribution scores extracted from prompt-based models w.r.t. plausibility and\nfaithfulness and comparing them with attribution scores extracted from\nfine-tuned models and large language models. In contrast to previous work, we\nintroduce training size as another dimension into the analysis. We find that\nusing the prompting paradigm (with either encoder-based or decoder-based\nmodels) yields more plausible explanations than fine-tuning the models in\nlow-resource settings and Shapley Value Sampling consistently outperforms\nattention and Integrated Gradients in terms of leading to more plausible and\nfaithful explanations.", "published": "2024-03-08 14:14:37", "link": "http://arxiv.org/abs/2403.05338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks\n  Using Commercial LLMs", "abstract": "Large Language Models (LLMs) exhibit impressive zero/few-shot inference and\ngeneration quality for high-resource languages (HRLs). A few of them have been\ntrained on low-resource languages (LRLs) and give decent performance. Owing to\nthe prohibitive costs of training LLMs, they are usually used as a network\nservice, with the client charged by the count of input and output tokens. The\nnumber of tokens strongly depends on the script and language, as well as the\nLLM's subword vocabulary. We show that LRLs are at a pricing disadvantage,\nbecause the well-known LLMs produce more tokens for LRLs than HRLs. This is\nbecause most currently popular LLMs are optimized for HRL vocabularies. Our\nobjective is to level the playing field: reduce the cost of processing LRLs in\ncontemporary LLMs while ensuring that predictive and generative qualities are\nnot compromised. As means to reduce the number of tokens processed by the LLM,\nwe consider code-mixing, translation, and transliteration of LRLs to HRLs. We\nperform an extensive study using the IndicXTREME classification and six\ngenerative tasks dataset, covering 15 Indic and 3 other languages, while using\nGPT-4 (one of the costliest LLM services released so far) as a commercial LLM.\nWe observe and analyze interesting patterns involving token count, cost, and\nquality across a multitude of languages and tasks. We show that choosing the\nbest policy to interact with the LLM can reduce cost by 90% while giving better\nor comparable performance compared to communicating with the LLM in the\noriginal LRL.", "published": "2024-03-08 16:37:36", "link": "http://arxiv.org/abs/2403.05434v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FFSTC: Fongbe to French Speech Translation Corpus", "abstract": "In this paper, we introduce the Fongbe to French Speech Translation Corpus\n(FFSTC) for the first time. This corpus encompasses approximately 31 hours of\ncollected Fongbe language content, featuring both French transcriptions and\ncorresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset\ncompiled through various collection methods and the efforts of dedicated\nindividuals. Furthermore, we conduct baseline experiments using Fairseq's\ntransformer_s and conformer models to evaluate data quality and validity. Our\nresults indicate a score of 8.96 for the transformer_s model and 8.14 for the\nconformer model, establishing a baseline for the FFSTC corpus.", "published": "2024-03-08 17:53:58", "link": "http://arxiv.org/abs/2403.05488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Err Is Human, but Llamas Can Learn It Too", "abstract": "This study explores enhancing grammatical error correction (GEC) through\nartificial error generation (AEG) using language models (LMs). Specifically, we\nfine-tune Llama 2-based LMs for error generation and find that this approach\nyields synthetic errors akin to human errors. Next, we train GEC Llama models\nwith the help of these artificial errors and outperform previous\nstate-of-the-art error correction models, with gains ranging between 0.8 and 6\nF0.5 points across all tested languages (German, Ukrainian, and Estonian).\nMoreover, we demonstrate that generating errors by fine-tuning smaller\nsequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and\nGPT-4) also results in synthetic errors beneficially affecting error generation\nmodels.", "published": "2024-03-08 18:04:03", "link": "http://arxiv.org/abs/2403.05493v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT", "abstract": "Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.", "published": "2024-03-08 18:42:59", "link": "http://arxiv.org/abs/2403.05519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Preference Elicitation with Language Models", "abstract": "Aligning AI systems to users' interests requires understanding and\nincorporating humans' complex values and preferences. Recently, language models\n(LMs) have been used to gather information about the preferences of human\nusers. This preference data can be used to fine-tune or guide other LMs and/or\nAI systems. However, LMs have been shown to struggle with crucial aspects of\npreference learning: quantifying uncertainty, modeling human mental states, and\nasking informative questions. These challenges have been addressed in other\nareas of machine learning, such as Bayesian Optimal Experimental Design (BOED),\nwhich focus on designing informative queries within a well-defined feature\nspace. But these methods, in turn, are difficult to scale and apply to\nreal-world problems where simply identifying the relevant features can be\ndifficult. We introduce OPEN (Optimal Preference Elicitation with Natural\nlanguage) a framework that uses BOED to guide the choice of informative\nquestions and an LM to extract features and translate abstract BOED queries\ninto natural language questions. By combining the flexibility of LMs with the\nrigor of BOED, OPEN can optimize the informativity of queries while remaining\nadaptable to real-world domains. In user studies, we find that OPEN outperforms\nexisting LM- and BOED-based methods for preference elicitation.", "published": "2024-03-08 18:57:52", "link": "http://arxiv.org/abs/2403.05534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent\n  Classification", "abstract": "Intent classifiers must be able to distinguish when a user's utterance does\nnot belong to any supported intent to avoid producing incorrect and unrelated\nsystem responses. Although out-of-scope (OOS) detection for intent classifiers\nhas been studied, previous work has not yet studied changes in classifier\nperformance against hard-negative out-of-scope utterances (i.e., inputs that\nshare common features with in-scope data, but are actually out-of-scope). We\npresent an automated technique to generate hard-negative OOS data using\nChatGPT. We use our technique to build five new hard-negative OOS datasets, and\nevaluate each against three benchmark intent classifiers. We show that\nclassifiers struggle to correctly identify hard-negative OOS utterances more\nthan general OOS utterances. Finally, we show that incorporating hard-negative\nOOS data for training improves model robustness when detecting hard-negative\nOOS data and general OOS data. Our technique, datasets, and evaluation address\nan important void in the field, offering a straightforward and inexpensive way\nto collect hard-negative OOS data and improve intent classifiers' robustness.", "published": "2024-03-08 19:25:00", "link": "http://arxiv.org/abs/2403.05640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System\n  Co-design", "abstract": "Retrieval-augmented generation (RAG) can enhance the generation quality of\nlarge language models (LLMs) by incorporating external token databases.\nHowever, retrievals from large databases can constitute a substantial portion\nof the overall generation time, particularly when retrievals are periodically\nperformed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design\napproach to reduce generation latency and enhance generation quality. PipeRAG\nintegrates (1) pipeline parallelism to enable concurrent retrieval and\ngeneration processes, (2) flexible retrieval intervals to maximize the\nefficiency of pipeline parallelism, and (3) a performance model to\nautomatically balance retrieval quality and latency based on the generation\nstates and underlying hardware. Our evaluation shows that, by combining the\nthree aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in\nend-to-end generation latency while improving generation quality. These\npromising results showcase the effectiveness of co-designing algorithms with\nunderlying systems, paving the way for the adoption of PipeRAG in future RAG\nsystems.", "published": "2024-03-08 21:09:20", "link": "http://arxiv.org/abs/2403.05676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DADIT: A Dataset for Demographic Classification of Italian Twitter Users\n  and a Comparison of Prediction Methods", "abstract": "Social scientists increasingly use demographically stratified social media\ndata to study the attitudes, beliefs, and behavior of the general public. To\nfacilitate such analyses, we construct, validate, and release publicly the\nrepresentative DADIT dataset of 30M tweets of 20k Italian Twitter users, along\nwith their bios and profile pictures. We enrich the user data with high-quality\nlabels for gender, age, and location. DADIT enables us to train and compare the\nperformance of various state-of-the-art models for the prediction of the gender\nand age of social media users. In particular, we investigate if tweets contain\nvaluable information for the task, since popular classifiers like M3 don't\nleverage them. Our best XLM-based classifier improves upon the commonly used\ncompetitor M3 by up to 53% F1. Especially for age prediction, classifiers\nprofit from including tweets as features. We also confirm these findings on a\nGerman test set.", "published": "2024-03-08 22:18:13", "link": "http://arxiv.org/abs/2403.05700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generator-Guided Crowd Reaction Assessment", "abstract": "In the realm of social media, understanding and predicting post reach is a\nsignificant challenge. This paper presents a Crowd Reaction AssessMent (CReAM)\ntask designed to estimate if a given social media post will receive more\nreaction than another, a particularly essential task for digital marketers and\ncontent writers. We introduce the Crowd Reaction Estimation Dataset (CRED),\nconsisting of pairs of tweets from The White House with comparative measures of\nretweet count. The proposed Generator-Guided Estimation Approach (GGEA)\nleverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2,\nand Claude, to guide classification models for making better predictions. Our\nresults reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder\narchitecture with tweet content and responses generated by Claude, performs\noptimally. We further use a T5-based paraphraser to generate paraphrases of a\ngiven post and demonstrate GGEA's ability to predict which post will elicit the\nmost reactions. We believe this novel application of LLMs provides a\nsignificant advancement in predicting social media post reach.", "published": "2024-03-08 13:05:44", "link": "http://arxiv.org/abs/2403.09702v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An In-depth Evaluation of Large Language Models in Sentence\n  Simplification with Error-based Human Assessment", "abstract": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that GPT-4\ngenerally generates fewer erroneous simplification outputs compared to the\ncurrent state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs.", "published": "2024-03-08 00:19:24", "link": "http://arxiv.org/abs/2403.04963v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiffChat: Learning to Chat with Text-to-Image Synthesis Models for\n  Interactive Image Creation", "abstract": "We present DiffChat, a novel method to align Large Language Models (LLMs) to\n\"chat\" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable\nDiffusion) for interactive image creation. Given a raw prompt/image and a\nuser-specified instruction, DiffChat can effectively make appropriate\nmodifications and generate the target prompt, which can be leveraged to create\nthe target image of high quality. To achieve this, we first collect an\ninstruction-following prompt engineering dataset named InstructPE for the\nsupervised training of DiffChat. Next, we propose a reinforcement learning\nframework with the feedback of three core criteria for image creation, i.e.,\naesthetics, user preference, and content integrity. It involves an action-space\ndynamic modification technique to obtain more relevant positive samples and\nharder negative samples during the off-policy sampling. Content integrity is\nalso introduced into the value estimation function for further improvement of\nproduced images. Our method can exhibit superior performance than baseline\nmodels and strong competitors based on both automatic and human evaluations,\nwhich fully demonstrates its effectiveness.", "published": "2024-03-08 02:24:27", "link": "http://arxiv.org/abs/2403.04997v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Is this the real life? Is this just fantasy? The Misleading Success of\n  Simulating Social Interactions With LLMs", "abstract": "Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena. However, most\nrecent work has used a more omniscient perspective on these simulations (e.g.,\nsingle LLM to generate all interlocutors), which is fundamentally at odds with\nthe non-omniscient, information asymmetric interactions that involve humans and\nAI agents in the real world. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that LLMs perform\nbetter in unrealistic, omniscient simulation settings but struggle in ones that\nmore accurately reflect real-world conditions with information asymmetry. Our\nfindings indicate that addressing information asymmetry remains a fundamental\nchallenge for LLM-based agents.", "published": "2024-03-08 03:49:17", "link": "http://arxiv.org/abs/2403.05020v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Multimodal Sentiment Analysis Debiasing via Bias Purification", "abstract": "Multimodal Sentiment Analysis (MSA) aims to understand human intentions by\nintegrating emotion-related clues from diverse modalities, such as visual,\nlanguage, and audio. Unfortunately, the current MSA task invariably suffers\nfrom unplanned dataset biases, particularly multimodal utterance-level label\nbias and word-level context bias. These harmful biases potentially mislead\nmodels to focus on statistical shortcuts and spurious correlations, causing\nsevere performance bottlenecks. To alleviate these issues, we present a\nMultimodal Counterfactual Inference Sentiment (MCIS) analysis framework based\non causality rather than conventional likelihood. Concretely, we first\nformulate a causal graph to discover harmful biases from already-trained\nvanilla models. In the inference phase, given a factual multimodal input, MCIS\nimagines two counterfactual scenarios to purify and mitigate these biases.\nThen, MCIS can make unbiased decisions from biased observations by comparing\nfactual and counterfactual outcomes. We conduct extensive experiments on\nseveral standard MSA benchmarks. Qualitative and quantitative results show the\neffectiveness of the proposed framework.", "published": "2024-03-08 03:55:27", "link": "http://arxiv.org/abs/2403.05023v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Rule-driven News Captioning", "abstract": "News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.", "published": "2024-03-08 07:06:43", "link": "http://arxiv.org/abs/2403.05101v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatUIE: Exploring Chat-based Unified Information Extraction using Large\n  Language Models", "abstract": "Recent advancements in large language models have shown impressive\nperformance in general chat. However, their domain-specific capabilities,\nparticularly in information extraction, have certain limitations. Extracting\nstructured information from natural language that deviates from known schemas\nor instructions has proven challenging for previous prompt-based methods. This\nmotivated us to explore domain-specific modeling in chat-based language models\nas a solution for extracting structured information from natural language. In\nthis paper, we present ChatUIE, an innovative unified information extraction\nframework built upon ChatGLM. Simultaneously, reinforcement learning is\nemployed to improve and align various tasks that involve confusing and limited\nsamples. Furthermore, we integrate generation constraints to address the issue\nof generating elements that are not present in the input. Our experimental\nresults demonstrate that ChatUIE can significantly improve the performance of\ninformation extraction with a slight decrease in chatting ability.", "published": "2024-03-08 07:59:19", "link": "http://arxiv.org/abs/2403.05132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory", "abstract": "Large language models (LLMs), such as ChatGPT, have shown remarkable\nabilities in natural language processing, opening new avenues in psychological\nresearch. This study explores whether LLMs can predict human memory performance\nin tasks involving garden-path sentences and contextual information. In the\nfirst part, we used ChatGPT to rate the relatedness and memorability of\ngarden-path sentences preceded by either fitting or unfitting contexts. In the\nsecond part, human participants read the same sentences, rated their\nrelatedness, and completed a surprise memory test. The results demonstrated\nthat ChatGPT's relatedness ratings closely matched those of the human\nparticipants, and its memorability ratings effectively predicted human memory\nperformance. Both LLM and human data revealed that higher relatedness in the\nunfitting context condition was associated with better memory performance,\naligning with probabilistic frameworks of context-dependent learning. These\nfindings suggest that LLMs, despite lacking human-like memory mechanisms, can\nmodel aspects of human cognition and serve as valuable tools in psychological\nresearch. We propose the field of machine psychology to explore this interplay\nbetween human cognition and artificial intelligence, offering a bidirectional\napproach where LLMs can both benefit from and contribute to our understanding\nof human cognitive processes.", "published": "2024-03-08 08:41:14", "link": "http://arxiv.org/abs/2403.05152v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CommitBench: A Benchmark for Commit Message Generation", "abstract": "Writing commit messages is a tedious daily task for many software developers,\nand often remains neglected. Automating this task has the potential to save\ntime while ensuring that messages are informative. A high-quality dataset and\nan objective benchmark are vital preconditions for solid research and\nevaluation towards this goal. We show that existing datasets exhibit various\nproblems, such as the quality of the commit selection, small sample sizes,\nduplicates, privacy issues, and missing licenses for redistribution. This can\nlead to unusable models and skewed evaluations, where inferior models achieve\nhigher evaluation scores due to biases in the data. We compile a new\nlarge-scale dataset, CommitBench, adopting best practices for dataset creation.\nWe sample commits from diverse projects with licenses that permit\nredistribution and apply our filtering and dataset enhancements to improve the\nquality of generated commit messages. We use CommitBench to compare existing\nmodels and show that other approaches are outperformed by a Transformer model\npretrained on source code. We hope to accelerate future research by publishing\nthe source code( https://github.com/Maxscha/commitbench ).", "published": "2024-03-08 09:56:45", "link": "http://arxiv.org/abs/2403.05188v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Tracing the Roots of Facts in Multilingual Language Models: Independent,\n  Shared, and Transferred Knowledge", "abstract": "Acquiring factual knowledge for language models (LMs) in low-resource\nlanguages poses a serious challenge, thus resorting to cross-lingual transfer\nin multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and\nrepresent factual knowledge. Using the multilingual factual knowledge probing\ndataset, mLAMA, we first conducted a neuron investigation of ML-LMs\n(specifically, multilingual BERT). We then traced the roots of facts back to\nthe knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire\nspecific facts. We finally identified three patterns of acquiring and\nrepresenting facts in ML-LMs: language-independent, cross-lingual shared and\ntransferred, and devised methods for differentiating them. Our findings\nhighlight the challenge of maintaining consistent factual knowledge across\nlanguages, underscoring the need for better fact representation learning in\nML-LMs.", "published": "2024-03-08 10:09:57", "link": "http://arxiv.org/abs/2403.05189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot\n  Stance Detection in Social Media", "abstract": "Stance detection, as the task of determining the viewpoint of a social media\npost towards a target as 'favor' or 'against', has been understudied in the\nchallenging yet realistic scenario where there is limited labeled data for a\ncertain target. Our work advances research in few-shot stance detection by\nintroducing SocialPET, a socially informed approach to leveraging language\nmodels for the task. Our proposed approach builds on the Pattern Exploiting\nTraining (PET) technique, which addresses classification tasks as cloze\nquestions through the use of language models. To enhance the approach with\nsocial awareness, we exploit the social network structure surrounding social\nmedia posts. We prove the effectiveness of SocialPET on two stance datasets,\nMulti-target and P-Stance, outperforming competitive stance detection models as\nwell as the base model, PET, where the labeled instances for the target under\nstudy is as few as 100. When we delve into the results, we observe that\nSocialPET is comparatively strong in identifying instances of the `against'\nclass, where baseline models underperform.", "published": "2024-03-08 11:00:09", "link": "http://arxiv.org/abs/2403.05216v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Prompt Multi-task Network for Abuse Language Detection", "abstract": "The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.", "published": "2024-03-08 12:45:53", "link": "http://arxiv.org/abs/2403.05268v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM4Decompile: Decompiling Binary Code with Large Language Models", "abstract": "Decompilation aims to convert binary code to high-level source code, but\ntraditional tools like Ghidra often produce results that are difficult to read\nand execute. Motivated by the advancements in Large Language Models (LLMs), we\npropose LLM4Decompile, the first and largest open-source LLM series (1.3B to\n33B) trained to decompile binary code. We optimize the LLM training process and\nintroduce the LLM4Decompile-End models to decompile binary directly. The\nresulting models significantly outperform GPT-4o and Ghidra on the HumanEval\nand ExeBench benchmarks by over 100% in terms of re-executability rate.\nAdditionally, we improve the standard refinement approach to fine-tune the\nLLM4Decompile-Ref models, enabling them to effectively refine the decompiled\ncode from Ghidra and achieve a further 16.2% improvement over the\nLLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to\nrevolutionize binary code decompilation, delivering remarkable improvements in\nreadability and executability while complementing conventional tools for\noptimal results. Our code, dataset, and models are released at\nhttps://github.com/albertan017/LLM4Decompile", "published": "2024-03-08 13:10:59", "link": "http://arxiv.org/abs/2403.05286v3", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in\n  Long-Horizon Generation", "abstract": "We explore how iterative revising a chain of thoughts with the help of\ninformation retrieval significantly improves large language models' reasoning\nand generation ability in long-horizon generation tasks, while hugely\nmitigating hallucination. In particular, the proposed method --\n*retrieval-augmented thoughts* (RAT) -- revises each thought step one by one\nwith retrieved information relevant to the task query, the current and the past\nthought steps, after the initial zero-shot CoT is generated. Applying RAT to\nGPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on\nvarious long-horizon generation tasks; on average of relatively increasing\nrating scores by 13.63% on code generation, 16.96% on mathematical reasoning,\n19.2% on creative writing, and 42.78% on embodied task planning. The demo page\ncan be found at https://craftjarvis.github.io/RAT", "published": "2024-03-08 13:42:19", "link": "http://arxiv.org/abs/2403.05313v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\n  Dialogues", "abstract": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.", "published": "2024-03-08 14:05:36", "link": "http://arxiv.org/abs/2403.05326v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Impact of Quantization on the Robustness of Transformer-based Text\n  Classifiers", "abstract": "Transformer-based models have made remarkable advancements in various NLP\nareas. Nevertheless, these models often exhibit vulnerabilities when confronted\nwith adversarial attacks. In this paper, we explore the effect of quantization\non the robustness of Transformer-based models. Quantization usually involves\nmapping a high-precision real number to a lower-precision value, aiming at\nreducing the size of the model at hand. To the best of our knowledge, this work\nis the first application of quantization on the robustness of NLP models. In\nour experiments, we evaluate the impact of quantization on BERT and DistilBERT\nmodels in text classification using SST-2, Emotion, and MR datasets. We also\nevaluate the performance of these models against TextFooler, PWWS, and PSO\nadversarial attacks. Our findings show that quantization significantly improves\n(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,\nwe compare the effect of quantization versus that of the adversarial training\napproach on robustness. Our experiments indicate that quantization increases\nthe robustness of the model by 18.80% on average compared to adversarial\ntraining without imposing any extra computational overhead during training.\nTherefore, our results highlight the effectiveness of quantization in improving\nthe robustness of NLP models.", "published": "2024-03-08 14:55:05", "link": "http://arxiv.org/abs/2403.05365v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in\n  Chain-of-Thought", "abstract": "While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.", "published": "2024-03-08 18:41:42", "link": "http://arxiv.org/abs/2403.05518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.", "published": "2024-03-08 18:54:20", "link": "http://arxiv.org/abs/2403.05530v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tuning-Free Accountable Intervention for LLM Deployment -- A\n  Metacognitive Approach", "abstract": "Large Language Models (LLMs) have catalyzed transformative advances across a\nspectrum of natural language processing tasks through few-shot or zero-shot\nprompting, bypassing the need for parameter tuning. While convenient, this\nmodus operandi aggravates ``hallucination'' concerns, particularly given the\nenigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns\nare exacerbated in high-stakes applications (e.g., healthcare), where\nunaccountable decision errors can lead to devastating consequences. In\ncontrast, human decision-making relies on nuanced cognitive processes, such as\nthe ability to sense and adaptively correct misjudgments through conceptual\nunderstanding. Drawing inspiration from human cognition, we propose an\ninnovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip\nLLMs with capabilities for self-aware error identification and correction. Our\nframework facilitates the construction of concept-specific sparse subnetworks\nthat illuminate transparent decision pathways. This provides a novel interface\nfor model \\textit{intervention} after deployment. Our intervention offers\ncompelling advantages: (\\textit{i})~at deployment or inference time, our\nmetacognitive LLMs can self-consciously identify potential mispredictions with\nminimum human involvement, (\\textit{ii})~the model has the capability to\nself-correct its errors efficiently, obviating the need for additional tuning,\nand (\\textit{iii})~the rectification procedure is not only self-explanatory but\nalso user-friendly, enhancing the interpretability and accessibility of the\nmodel. By integrating these metacognitive features, our approach pioneers a new\npath toward engendering greater trustworthiness and accountability in the\ndeployment of LLMs.", "published": "2024-03-08 19:18:53", "link": "http://arxiv.org/abs/2403.05636v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes", "abstract": "While generative multilingual models are rapidly being deployed, their safety\nand fairness evaluations are largely limited to resources collected in English.\nThis is especially problematic for evaluations targeting inherently\nsocio-cultural phenomena such as stereotyping, where it is important to build\nmulti-lingual resources that reflect the stereotypes prevalent in respective\nlanguage communities. However, gathering these resources, at scale, in varied\nlanguages and regions pose a significant challenge as it requires broad\nsocio-cultural knowledge and can also be prohibitively expensive. To overcome\nthis critical gap, we employ a recently introduced approach that couples LLM\ngenerations for scale with culturally situated validations for reliability, and\nbuild SeeGULL Multilingual, a global-scale multilingual dataset of social\nstereotypes, containing over 25K stereotypes, spanning 20 languages, with human\nannotations across 23 regions, and demonstrate its utility in identifying gaps\nin model evaluations. Content warning: Stereotypes shared in this paper can be\noffensive.", "published": "2024-03-08 22:09:58", "link": "http://arxiv.org/abs/2403.05696v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Concept-aware Data Construction Improves In-context Learning of Language\n  Models", "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL),\nmanifested in the LMs' ability to perform a new task solely from\nnatural-language instruction. Previous work curating in-context learners\nassumes that ICL emerges from a vast over-parametrization or the scale of\nmulti-task training. However, recent theoretical work attributes the ICL\nability to concept-dependent training data and creates functional in-context\nlearners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL\nquality. We propose Concept-aware Training (CoAT), a framework for constructing\ntraining scenarios that make it beneficial for the LM to learn to utilize the\nanalogical reasoning concepts from demonstrations. We find that by using CoAT,\npre-trained transformers can learn to better utilise new latent concepts from\ndemonstrations and that such ability makes ICL more robust to the functional\ndeficiencies of the previous models. Finally, we show that concept-aware\nin-context learning is more effective for a majority of new tasks when compared\nto traditional instruction tuning, resulting in a performance comparable to the\nprevious in-context learners using magnitudes of more training data.", "published": "2024-03-08 19:07:47", "link": "http://arxiv.org/abs/2403.09703v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tell me the truth: A system to measure the trustworthiness of Large\n  Language Models", "abstract": "Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2022, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.", "published": "2024-03-08 00:27:57", "link": "http://arxiv.org/abs/2403.04964v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Can't Remember Details in Long Documents? You Need Some R&R", "abstract": "Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.", "published": "2024-03-08 03:03:20", "link": "http://arxiv.org/abs/2403.05004v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Human Conversations Special? A Large Language Model Perspective", "abstract": "This study analyzes changes in the attention mechanisms of large language\nmodels (LLMs) when used to understand natural conversations between humans\n(human-human). We analyze three use cases of LLMs: interactions over web\ncontent, code, and mathematical texts. By analyzing attention distance,\ndispersion, and interdependency across these domains, we highlight the unique\nchallenges posed by conversational data. Notably, conversations require nuanced\nhandling of long-term contextual relationships and exhibit higher complexity\nthrough their attention patterns. Our findings reveal that while language\nmodels exhibit domain-specific attention behaviors, there is a significant gap\nin their ability to specialize in human conversations. Through detailed\nattention entropy analysis and t-SNE visualizations, we demonstrate the need\nfor models trained with a diverse array of high-quality conversational data to\nenhance understanding and generation of human-like dialogue. This research\nhighlights the importance of domain specialization in language models and\nsuggests pathways for future advancement in modeling human conversational\nnuances.", "published": "2024-03-08 04:44:25", "link": "http://arxiv.org/abs/2403.05045v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing Multi-Role Capabilities of Large Language Models for\n  Open-Domain Question Answering", "abstract": "Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.", "published": "2024-03-08 11:09:13", "link": "http://arxiv.org/abs/2403.05217v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models", "abstract": "Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.", "published": "2024-03-08 12:42:36", "link": "http://arxiv.org/abs/2403.05266v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PEEB: Part-based Image Classifiers with an Explainable and Editable\n  Language Bottleneck", "abstract": "CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. Therefore, they perform poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of text\ndescriptors that describe the visual parts of that class; and (2) match the\nembeddings of the detected parts to their textual descriptors in each class to\ncompute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a huge margin (~10x in top-1\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20%\naccuracy on CUB-200 and Dogs-120, respectively) but also the first to enable\nusers to edit the text descriptors to form a new classifier without any\nre-training. Compared to concept bottleneck models, PEEB is also the SOTA in\nboth zero-shot and supervised-learning settings.", "published": "2024-03-08 13:24:46", "link": "http://arxiv.org/abs/2403.05297v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HistGen: Histopathology Report Generation via Local-Global Feature\n  Encoding and Cross-modal Context Interaction", "abstract": "Histopathology serves as the gold standard in cancer diagnosis, with clinical\nreports being vital in interpreting and understanding this process, guiding\ncancer treatment and patient care. The automation of histopathology report\ngeneration with deep learning stands to significantly enhance clinical\nefficiency and lessen the labor-intensive, time-consuming burden on\npathologists in report writing. In pursuit of this advancement, we introduce\nHistGen, a multiple instance learning-empowered framework for histopathology\nreport generation together with the first benchmark dataset for evaluation.\nInspired by diagnostic and report-writing workflows, HistGen features two\ndelicately designed modules, aiming to boost report generation by aligning\nwhole slide images (WSIs) and diagnostic reports from local and global\ngranularity. To achieve this, a local-global hierarchical encoder is developed\nfor efficient visual feature aggregation from a region-to-slide perspective.\nMeanwhile, a cross-modal context module is proposed to explicitly facilitate\nalignment and interaction between distinct modalities, effectively bridging the\ngap between the extensive visual sequences of WSIs and corresponding highly\nsummarized reports. Experimental results on WSI report generation show the\nproposed model outperforms state-of-the-art (SOTA) models by a large margin.\nMoreover, the results of fine-tuning our model on cancer subtyping and survival\nanalysis tasks further demonstrate superior performance compared to SOTA\nmethods, showcasing strong transfer learning capability. Dataset, model\nweights, and source code are available in\nhttps://github.com/dddavid4real/HistGen.", "published": "2024-03-08 15:51:43", "link": "http://arxiv.org/abs/2403.05396v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Will GPT-4 Run DOOM?", "abstract": "We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.", "published": "2024-03-08 17:30:41", "link": "http://arxiv.org/abs/2403.05468v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM", "abstract": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.", "published": "2024-03-08 18:48:30", "link": "http://arxiv.org/abs/2403.05527v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in\n  Images and Videos", "abstract": "We introduce LaGTran, a novel framework that utilizes text supervision to\nguide robust transfer of discriminative knowledge from labeled source to\nunlabeled target data with domain gaps. While unsupervised adaptation methods\nhave been established to address this problem, they show limitations in\nhandling challenging domain shifts due to their exclusive operation within the\npixel-space. Motivated by our observation that semantically richer text\nmodality has more favorable transfer properties, we devise a transfer mechanism\nto use a source-trained text-classifier to generate predictions on the target\ntext descriptions, and utilize these predictions as supervision for the\ncorresponding images. Our approach driven by language guidance is surprisingly\neasy and simple, yet significantly outperforms all prior approaches on\nchallenging datasets like GeoNet and DomainNet, validating its extreme\neffectiveness. To further extend the scope of our study beyond images, we\nintroduce a new benchmark called Ego2Exo to study ego-exo transfer in videos\nand find that our language-aided approach LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.", "published": "2024-03-08 18:58:46", "link": "http://arxiv.org/abs/2403.05535v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Extracting Protein-Protein Interactions (PPIs) from Biomedical\n  Literature using Attention-based Relational Context Information", "abstract": "Because protein-protein interactions (PPIs) are crucial to understand living\nsystems, harvesting these data is essential to probe disease development and\ndiscern gene/protein functions and biological processes. Some curated datasets\ncontain PPI data derived from the literature and other sources (e.g., IntAct,\nBioGrid, DIP, and HPRD). However, they are far from exhaustive, and their\nmaintenance is a labor-intensive process. On the other hand, machine learning\nmethods to automate PPI knowledge extraction from the scientific literature\nhave been limited by a shortage of appropriate annotated data. This work\npresents a unified, multi-source PPI corpora with vetted interaction\ndefinitions augmented by binary interaction type labels and a Transformer-based\ndeep learning method that exploits entities' relational context information for\nrelation representation to improve relation classification performance. The\nmodel's performance is evaluated on four widely studied biomedical relation\nextraction datasets, as well as this work's target PPI datasets, to observe the\neffectiveness of the representation to relation extraction tasks in various\ndata. Results show the model outperforms prior state-of-the-art models. The\ncode and data are available at:\nhttps://github.com/BNLNLP/PPI-Relation-Extraction", "published": "2024-03-08 01:43:21", "link": "http://arxiv.org/abs/2403.05602v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "A Concept-based Interpretable Model for the Diagnosis of Choroid\n  Neoplasias using Multimodal Data", "abstract": "Diagnosing rare diseases presents a common challenge in clinical practice,\nnecessitating the expertise of specialists for accurate identification. The\nadvent of machine learning offers a promising solution, while the development\nof such technologies is hindered by the scarcity of data on rare conditions and\nthe demand for models that are both interpretable and trustworthy in a clinical\ncontext. Interpretable AI, with its capacity for human-readable outputs, can\nfacilitate validation by clinicians and contribute to medical education. In the\ncurrent work, we focus on choroid neoplasias, the most prevalent form of eye\ncancer in adults, albeit rare with 5.1 per million. We built the so-far largest\ndataset consisting of 750 patients, incorporating three distinct imaging\nmodalities collected from 2004 to 2022. Our work introduces a concept-based\ninterpretable model that distinguishes between three types of choroidal tumors,\nintegrating insights from domain experts via radiological reports. Remarkably,\nthis model not only achieves an F1 score of 0.91, rivaling that of black-box\nmodels, but also boosts the diagnostic accuracy of junior doctors by 42%. This\nstudy highlights the significant potential of interpretable machine learning in\nimproving the diagnosis of rare diseases, laying a groundwork for future\nbreakthroughs in medical AI that could tackle a wider array of complex health\nscenarios.", "published": "2024-03-08 07:15:53", "link": "http://arxiv.org/abs/2403.05606v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate", "abstract": "Large language models are known to hallucinate when faced with unfamiliar\nqueries, but the underlying mechanism that govern how models hallucinate are\nnot yet fully understood. In this work, we find that unfamiliar examples in the\nmodels' finetuning data -- those that introduce concepts beyond the base\nmodel's scope of knowledge -- are crucial in shaping these errors. In\nparticular, we find that an LLM's hallucinated predictions tend to mirror the\nresponses associated with its unfamiliar finetuning examples. This suggests\nthat by modifying how unfamiliar finetuning examples are supervised, we can\ninfluence a model's responses to unfamiliar queries (e.g., say ``I don't\nknow''). We empirically validate this observation in a series of controlled\nexperiments involving SFT, RL, and reward model finetuning on TriviaQA and\nMMLU. Our work further investigates RL finetuning strategies for improving the\nfactuality of long-form model generations. We find that, while hallucinations\nfrom the reward model can significantly undermine the effectiveness of RL\nfactuality finetuning, strategically controlling how reward models hallucinate\ncan minimize these negative effects. Leveraging our previous observations on\ncontrolling hallucinations, we propose an approach for learning more reliable\nreward models, and show that they improve the efficacy of RL factuality\nfinetuning in long-form biography and book/movie plot generation tasks.", "published": "2024-03-08 18:28:13", "link": "http://arxiv.org/abs/2403.05612v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation\n  Framework for Analyses", "abstract": "Automatically interpreting CT scans can ease the workload of radiologists.\nHowever, this is challenging mainly due to the scarcity of adequate datasets\nand reference standards for evaluation. This study aims to bridge this gap by\nintroducing a novel evaluation framework, named ``GPTRadScore''. This framework\nassesses the capabilities of multi-modal LLMs, such as GPT-4 with Vision\n(GPT-4V), Gemini Pro Vision, LLaVA-Med, and RadFM, in generating descriptions\nfor prospectively-identified findings. By employing a decomposition technique\nbased on GPT-4, GPTRadScore compares these generated descriptions with\ngold-standard report sentences, analyzing their accuracy in terms of body part,\nlocation, and type of finding. Evaluations demonstrated a high correlation with\nclinician assessments and highlighted its potential over traditional metrics,\nsuch as BLEU, METEOR, and ROUGE. Furthermore, to contribute to future studies,\nwe plan to release a benchmark dataset annotated by clinicians. Using\nGPTRadScore, we found that while GPT-4V and Gemini Pro Vision fare better,\ntheir performance revealed significant areas for improvement, primarily due to\nlimitations in the dataset used for training these models. To demonstrate this\npotential, RadFM was fine-tuned and it resulted in significant accuracy\nimprovements: location accuracy rose from 3.41\\% to 12.8\\%, body part accuracy\nfrom 29.12\\% to 53\\%, and type accuracy from 9.24\\% to 30\\%, thereby validating\nour hypothesis.", "published": "2024-03-08 21:16:28", "link": "http://arxiv.org/abs/2403.05680v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models", "abstract": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\nUsing clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.", "published": "2024-03-08 23:17:55", "link": "http://arxiv.org/abs/2403.05720v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Robust Bench: A Robustness Benchmark For Speech Recognition", "abstract": "As Automatic Speech Recognition (ASR) models become ever more pervasive, it\nis important to ensure that they make reliable predictions under corruptions\npresent in the physical and digital world. We propose Speech Robust Bench\n(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to\ndiverse corruptions. SRB is composed of 114 input perturbations which simulate\nan heterogeneous range of corruptions that ASR models may encounter when\ndeployed in the wild. We use SRB to evaluate the robustness of several\nstate-of-the-art ASR models and observe that model size and certain modeling\nchoices such as the use of discrete representations, or self-training appear to\nbe conducive to robustness. We extend this analysis to measure the robustness\nof ASR models on data from various demographic subgroups, namely English and\nSpanish speakers, and males and females. Our results revealed noticeable\ndisparities in the model's robustness across subgroups. We believe that SRB\nwill significantly facilitate future research towards robust ASR models, by\nmaking it easier to conduct comprehensive and comparable robustness\nevaluations.", "published": "2024-03-08 08:10:29", "link": "http://arxiv.org/abs/2403.07937v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Alignment Studio: Aligning Large Language Models to Particular\n  Contextual Regulations", "abstract": "The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.", "published": "2024-03-08 21:26:49", "link": "http://arxiv.org/abs/2403.09704v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Nuanced Conversation Evaluation Framework for Large Language\n  Models in Mental Health", "abstract": "Understanding the conversation abilities of Large Language Models (LLMs) can\nhelp lead to its more cautious and appropriate deployment. This is especially\nimportant for safety-critical domains like mental health, where someone's life\nmay depend on the exact wording of a response to an urgent question. In this\npaper, we propose a novel framework for evaluating the nuanced conversation\nabilities of LLMs. Within it, we develop a series of quantitative metrics\ndeveloped from literature on using psychotherapy conversation analysis\nliterature. While we ensure that our framework and metrics are transferable by\nresearchers to relevant adjacent domains, we apply them to the mental health\nfield. We use our framework to evaluate several popular frontier LLMs,\nincluding some GPT and Llama models, through a verified mental health dataset.\nOur results show that GPT4 Turbo can perform significantly more similarly to\nverified therapists than other selected LLMs. We conduct additional analysis to\nexamine how LLM conversation performance varies across specific mental health\ntopics. Our results indicate that GPT4 Turbo performs well in achieving high\ncorrelation with verified therapists in particular topics such as Parenting and\nRelationships. We believe our contributions will help researchers develop\nbetter LLMs that, in turn, will more positively support people's lives.", "published": "2024-03-08 23:46:37", "link": "http://arxiv.org/abs/2403.09705v1", "categories": ["cs.CL", "cs.AI", "cs.ET"], "primary_category": "cs.CL"}
{"title": "IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic\n  Systems", "abstract": "Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nnatural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we evaluate whether these issues can be\naddressed through execution isolation and what that isolation might look like\nin the context of LLM-based systems, where there are arbitrary natural\nlanguage-based interactions between system components, between LLM and apps,\nand between apps. To that end, we propose IsolateGPT, a design architecture\nthat demonstrates the feasibility of execution isolation and provides a\nblueprint for implementing isolation, in LLM-based systems. We evaluate\nIsolateGPT against a number of attacks and demonstrate that it protects against\nmany security, privacy, and safety issues that exist in non-isolated LLM-based\nsystems, without any loss of functionality. The performance overhead incurred\nby IsolateGPT to improve security is under 30% for three-quarters of tested\nqueries.", "published": "2024-03-08 00:02:30", "link": "http://arxiv.org/abs/2403.04960v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Robust Semantic Communications for Speech Transmission", "abstract": "In this paper, we propose a robust semantic communication system for speech\ntransmission, named Ross-S2T, by delivering the essential semantic information.\nParticularly, we consider the speech-to-text translation (S2TT) as the\ntransmission goal. First, a deep semantic encoder is developed to directly\nconvert speech in the source language to textual features associated with the\ntarget language, facilitating the end-to-end (E2E) semantic exchange to perform\nthe S2TT task and reducing the transmission data without performance\ndegradation. To mitigate semantic impairments inherent in the corrupted speech,\na novel generative adversarial network (GAN)-enabled deep semantic compensator\nis established to estimate the lost semantic information within the speech and\nextract deep semantic features simultaneously, which enables robust semantic\ntransmission for corrupted speech. Furthermore, a semantic probe-aided\ncompensator is devised to enhance the semantic fidelity of recovered semantic\nfeatures and improve the understandability of the target text. According to\nsimulation results, the proposed Ross-S2T exhibits superior S2TT performance\ncompared to conventional approaches and high robustness against semantic\nimpairments.", "published": "2024-03-08 09:55:07", "link": "http://arxiv.org/abs/2403.05187v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Binaural Speech Enhancement Using Deep Complex Convolutional Transformer\n  Networks", "abstract": "Studies have shown that in noisy acoustic environments, providing binaural\nsignals to the user of an assistive listening device may improve speech\nintelligibility and spatial awareness. This paper presents a binaural speech\nenhancement method using a complex convolutional neural network with an\nencoder-decoder architecture and a complex multi-head attention transformer.\nThe model is trained to estimate individual complex ratio masks in the\ntime-frequency domain for the left and right-ear channels of binaural hearing\ndevices. The model is trained using a novel loss function that incorporates the\npreservation of spatial information along with speech intelligibility\nimprovement and noise reduction. Simulation results for acoustic scenarios with\na single target speaker and isotropic noise of various types show that the\nproposed method improves the estimated binaural speech intelligibility and\npreserves the binaural cues better in comparison with several baseline\nalgorithms.", "published": "2024-03-08 15:42:13", "link": "http://arxiv.org/abs/2403.05393v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction", "abstract": "Recent advancements in generative modeling have significantly enhanced the\nreconstruction of audio waveforms from various representations. While diffusion\nmodels are adept at this task, they are hindered by latency issues due to their\noperation at the individual sample point level and the need for numerous\nsampling steps. In this study, we introduce RFWave, a cutting-edge multi-band\nRectified Flow approach designed to reconstruct high-fidelity audio waveforms\nfrom Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates\ncomplex spectrograms and operates at the frame level, processing all subbands\nsimultaneously to boost efficiency. Leveraging Rectified Flow, which targets a\nstraight transport trajectory, RFWave achieves reconstruction with just 10\nsampling steps. Our empirical evaluations show that RFWave not only provides\noutstanding reconstruction quality but also offers vastly superior\ncomputational efficiency, enabling audio generation at speeds up to 160 times\nfaster than real-time on a GPU. An online demonstration is available at:\nhttps://rfwave-demo.github.io/rfwave/.", "published": "2024-03-08 03:16:47", "link": "http://arxiv.org/abs/2403.05010v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings", "abstract": "In the domain of music production and audio processing, the implementation of\nautomatic pitch correction of the singing voice, also known as Auto-Tune, has\nsignificantly transformed the landscape of vocal performance. While auto-tuning\ntechnology has offered musicians the ability to tune their vocal pitches and\nachieve a desired level of precision, its use has also sparked debates\nregarding its impact on authenticity and artistic integrity. As a result,\ndetecting and analyzing Auto-Tuned vocals in music recordings has become\nessential for music scholars, producers, and listeners. However, to the best of\nour knowledge, no prior effort has been made in this direction. This study\nintroduces a data-driven approach leveraging triplet networks for the detection\nof Auto-Tuned songs, backed by the creation of a dataset composed of original\nand Auto-Tuned audio clips. The experimental results demonstrate the\nsuperiority of the proposed method in both accuracy and robustness compared to\nRawnet2, an end-to-end model proposed for anti-spoofing and widely used for\nother audio forensic tasks.", "published": "2024-03-08 15:19:26", "link": "http://arxiv.org/abs/2403.05380v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-to-Audio Generation Synchronized with Videos", "abstract": "In recent times, the focus on text-to-audio (TTA) generation has intensified,\nas researchers strive to synthesize audio from textual descriptions. However,\nmost existing methods, though leveraging latent diffusion models to learn the\ncorrelation between audio and text embeddings, fall short when it comes to\nmaintaining a seamless synchronization between the produced audio and its\nvideo. This often results in discernible audio-visual mismatches. To bridge\nthis gap, we introduce a groundbreaking benchmark for Text-to-Audio generation\nthat aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself\nwith three novel metrics dedicated to evaluating visual alignment and temporal\nconsistency. To complement this, we also present a simple yet effective\nvideo-aligned TTA generation model, namely T2AV. Moving beyond traditional\nmethods, T2AV refines the latent diffusion approach by integrating\nvisual-aligned text embeddings as its conditional foundation. It employs a\ntemporal multi-head attention transformer to extract and understand temporal\nnuances from video data, a feat amplified by our Audio-Visual ControlNet that\nadeptly merges temporal visual representations with text embeddings. Further\nenhancing this integration, we weave in a contrastive learning objective,\ndesigned to ensure that the visual-aligned text embeddings resonate closely\nwith the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench\ndemonstrate that our T2AV sets a new standard for video-aligned TTA generation\nin ensuring visual alignment and temporal consistency.", "published": "2024-03-08 22:27:38", "link": "http://arxiv.org/abs/2403.07938v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
