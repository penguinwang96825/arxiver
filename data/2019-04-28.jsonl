{"title": "OPIEC: An Open Information Extraction Corpus", "abstract": "Open information extraction (OIE) systems extract relations and their\narguments from natural language text in an unsupervised manner. The resulting\nextractions are a valuable resource for downstream tasks such as knowledge base\nconstruction, open question answering, or event schema induction. In this\npaper, we release, describe, and analyze an OIE corpus called OPIEC, which was\nextracted from the text of English Wikipedia. OPIEC complements the available\nOIE resources: It is the largest OIE corpus publicly available to date (over\n340M triples) and contains valuable metadata such as provenance information,\nconfidence scores, linguistic annotations, and semantic annotations including\nspatial and temporal information. We analyze the OPIEC corpus by comparing its\ncontent with knowledge bases such as DBpedia or YAGO, which are also based on\nWikipedia. We found that most of the facts between entities present in OPIEC\ncannot be found in DBpedia and/or YAGO, that OIE facts often differ in the\nlevel of specificity compared to knowledge base facts, and that OIE open\nrelations are generally highly polysemous. We believe that the OPIEC corpus is\na valuable resource for future research on automated knowledge base\nconstruction.", "published": "2019-04-28 13:57:54", "link": "http://arxiv.org/abs/1904.12324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Recurrent Highway Networks", "abstract": "Recurrent Neural Networks have lately gained a lot of popularity in language\nmodelling tasks, especially in neural machine translation(NMT). Very recent NMT\nmodels are based on Encoder-Decoder, where a deep LSTM based encoder is used to\nproject the source sentence to a fixed dimensional vector and then another deep\nLSTM decodes the target sentence from the vector. However there has been very\nlittle work on exploring architectures that have more than one layer in\nspace(i.e. in each time step). This paper examines the effectiveness of the\nsimple Recurrent Highway Networks(RHN) in NMT tasks. The model uses Recurrent\nHighway Neural Network in encoder and decoder, with attention .We also explore\nthe reconstructor model to improve adequacy. We demonstrate the effectiveness\nof all three approaches on the IWSLT English-Vietnamese dataset. We see that\nRHN performs on par with LSTM based models and even better in some cases.We see\nthat deep RHN models are easy to train compared to deep LSTM based models\nbecause of highway connections. The paper also investigates the effects of\nincreasing recurrent depth in each time step.", "published": "2019-04-28 08:27:55", "link": "http://arxiv.org/abs/1905.01996v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conditional Teacher-Student Learning", "abstract": "The teacher-student (T/S) learning has been shown to be effective for a\nvariety of problems such as domain adaptation and model compression. One\nshortcoming of the T/S learning is that a teacher model, not always perfect,\nsporadically produces wrong guidance in form of posterior probabilities that\nmisleads the student model towards a suboptimal performance. To overcome this\nproblem, we propose a conditional T/S learning scheme, in which a \"smart\"\nstudent model selectively chooses to learn from either the teacher model or the\nground truth labels conditioned on whether the teacher can correctly predict\nthe ground truth. Unlike a naive linear combination of the two knowledge\nsources, the conditional learning is exclusively engaged with the teacher model\nwhen the teacher model's prediction is correct, and otherwise backs off to the\nground truth. Thus, the student model is able to learn effectively from the\nteacher and even potentially surpass the teacher. We examine the proposed\nlearning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker\nadaptation on Microsoft short message dictation dataset. The proposed method\nachieves 9.8% and 12.8% relative word error rate reductions, respectively, over\nT/S learning for environment adaptation and speaker-independent model for\nspeaker adaptation.", "published": "2019-04-28 23:43:20", "link": "http://arxiv.org/abs/1904.12399v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Attentive Adversarial Learning for Domain-Invariant Training", "abstract": "Adversarial domain-invariant training (ADIT) proves to be effective in\nsuppressing the effects of domain variability in acoustic modeling and has led\nto improved performance in automatic speech recognition (ASR). In ADIT, an\nauxiliary domain classifier takes in equally-weighted deep features from a deep\nneural network (DNN) acoustic model and is trained to improve their\ndomain-invariance by optimizing an adversarial loss function. In this work, we\npropose an attentive ADIT (AADIT) in which we advance the domain classifier\nwith an attention mechanism to automatically weight the input deep features\naccording to their importance in domain classification. With this attentive\nre-weighting, AADIT can focus on the domain normalization of phonetic\ncomponents that are more susceptible to domain variability and generates deep\nfeatures with improved domain-invariance and senone-discriminativity over ADIT.\nMost importantly, the attention block serves only as an external component to\nthe DNN acoustic model and is not involved in ASR, so AADIT can be used to\nimprove the acoustic modeling with any DNN architectures. More generally, the\nsame methodology can improve any adversarial learning system with an auxiliary\ndiscriminator. Evaluated on CHiME-3 dataset, the AADIT achieves 13.6% and 9.3%\nrelative WER improvements, respectively, over a multi-conditional model and a\nstrong ADIT baseline.", "published": "2019-04-28 23:44:29", "link": "http://arxiv.org/abs/1904.12400v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cough Detection Using Hidden Markov Models", "abstract": "Respiratory infections and chronic respiratory diseases impose a heavy health\nburden worldwide. Coughing is one of the most common symptoms of many such\ninfections, and can be indicative of flare-ups of chronic respiratory diseases.\nWhether at a clinical or public health level, the capacity to identify bouts of\ncoughing can aid understanding of population and individual health status.\nDeveloping health monitoring models in the context of respiratory diseases and\nalso seasonal diseases with symptoms such as cough has the potential to improve\nquality of life, help clinicians and public health authorities with their\ndecisions and decrease the cost of health services. In this paper, we\ninvestigated the ability to which a simple machine learning approach in the\nform of Hidden Markov Models (HMMs) could be used to classify different states\nof coughing using univariate (with a single energy band as the input feature)\nand multivariate (with a multiple energy band as the input features) binned\ntime series using both of cough data. We further used the model to distinguish\ncough events from other events and environmental noise. Our Hidden Markov\nalgorithm achieved 92% AUR (Area Under Receiver Operating Characteristic Curve)\nin classifying coughing events in noisy environments. Moreover, comparison of\nunivariate with multivariate HMMs suggest a high accuracy of multivariate HMMs\nfor cough event classifications.", "published": "2019-04-28 17:47:31", "link": "http://arxiv.org/abs/1904.12354v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
