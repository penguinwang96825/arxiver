{"title": "Conversation Modeling on Reddit using a Graph-Structured LSTM", "abstract": "This paper presents a novel approach for modeling threaded discussions on\nsocial media using a graph-structured bidirectional LSTM which represents both\nhierarchical and temporal conversation structure. In experiments with a task of\npredicting popularity of comments in Reddit discussions, the proposed model\noutperforms a node-independent architecture for different sets of input\nfeatures. Analyses show a benefit to the model over the full course of the\ndiscussion, improving detection in both early and late stages. Further, the use\nof language cues with the bidirectional tree state updates helps with\nidentifying controversial comments.", "published": "2017-04-07 03:27:54", "link": "http://arxiv.org/abs/1704.02080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adposition and Case Supersenses v2.6: Guidelines for English", "abstract": "This document offers a detailed linguistic description of SNACS (Semantic\nNetwork of Adposition and Case Supersenses; Schneider et al., 2018), an\ninventory of 52 semantic labels (\"supersenses\") that characterize the use of\nadpositions and case markers at a somewhat coarse level of granularity, as\ndemonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/ ;\nversion 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires\nto be universal, this document is specific to English; documentation for other\nlanguages will be published separately.\n  Version 2 is a revision of the supersense inventory proposed for English by\nSchneider et al. (2015, 2016) (henceforth \"v1\"), which in turn was based on\nprevious schemes. The present inventory was developed after extensive review of\nthe v1 corpus annotations for English, plus previously unanalyzed genitive case\npossessives (Blodgett and Schneider, 2018), as well as consideration of\nadposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et\nal. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et\nal. (2018) summarize the scheme, its application to English corpus data, and an\nautomatic disambiguation task. Liu et al. (2021) offer an English Lexical\nSemantic Recognition tagger that includes SNACS labels in its output.\n  This documentation can also be browsed alongside corpus data on the Xposition\nwebsite (Gessler et al., 2022): http://www.xposition.org/", "published": "2017-04-07 08:42:45", "link": "http://arxiv.org/abs/1704.02134v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural\n  Semantic Parsing", "abstract": "We evaluate a semantic parser based on a character-based sequence-to-sequence\nmodel in the context of the SemEval-2017 shared task on semantic parsing for\nAMRs. With data augmentation, super characters, and POS-tagging we gain major\nimprovements in performance compared to a baseline character-level model.\nAlthough we improve on previous character-based neural semantic parsing models,\nthe overall accuracy is still lower than a state-of-the-art AMR parser. An\nensemble combining our neural semantic parser with an existing, traditional\nparser, yields a small gain in performance.", "published": "2017-04-07 09:37:36", "link": "http://arxiv.org/abs/1704.02156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for\n  kEyphrase ClassificaTION", "abstract": "This paper describes our approach to the SemEval 2017 Task 10: \"Extracting\nKeyphrases and Relations from Scientific Publications\", specifically to Subtask\n(B): \"Classification of identified keyphrases\". We explored three different\ndeep learning approaches: a character-level convolutional neural network (CNN),\na stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM.\nFrom these approaches, we created an ensemble of differently\nhyper-parameterized systems, achieving a micro-F1-score of 0.63 on the test\ndata. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four\naccording to this official score. However, we erroneously trained 2 out of 3\nneural nets (the stacker and the CNN) on only roughly 15% of the full data,\nnamely, the original development set. When trained on the full data\n(training+development), our ensemble has a micro-F1-score of 0.69. Our code is\navailable from https://github.com/UKPLab/semeval2017-scienceie.", "published": "2017-04-07 13:07:15", "link": "http://arxiv.org/abs/1704.02215v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Global Algorithms in Word Sense Disambiguation", "abstract": "This article compares four probabilistic algorithms (global algorithms) for\nWord Sense Disambiguation (WSD) in terms of the number of scorer calls (local\nalgo- rithm) and the F1 score as determined by a gold-standard scorer. Two\nalgorithms come from the state of the art, a Simulated Annealing Algorithm\n(SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first\nadapt from WSD that are state of the art probabilistic search algorithms,\nnamely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD\nrequires to evaluate exponentially many word sense combinations (with branching\nfactors of up to 6 or more), probabilistic algorithms allow to find approximate\nsolution in a tractable time by sampling the search space. We find that CSA, GA\nand SA all eventually converge to similar results (0.98 F1 score), but CSA gets\nthere faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in\nfewer scorer calls. In BA a strict convergence criterion prevents it from\nreaching above 0.89 F1.", "published": "2017-04-07 17:04:51", "link": "http://arxiv.org/abs/1704.02293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Trolling Hierarchy in Social Media and A Conditional Random Field For\n  Trolling Detection", "abstract": "An-ever increasing number of social media websites, electronic newspapers and\nInternet forums allow visitors to leave comments for others to read and\ninteract. This exchange is not free from participants with malicious\nintentions, which do not contribute with the written conversation. Among\ndifferent communities users adopt strategies to handle such users. In this\npaper we present a comprehensive categorization of the trolling phenomena\nresource, inspired by politeness research and propose a model that jointly\npredicts four crucial aspects of trolling: intention, interpretation, intention\ndisclosure and response strategy. Finally, we present a new annotated dataset\ncontaining excerpts of conversations involving trolls and the interactions with\nother users that we hope will be a useful resource for the research community.", "published": "2017-04-07 22:08:39", "link": "http://arxiv.org/abs/1704.02385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conceptualization Topic Modeling", "abstract": "Recently, topic modeling has been widely used to discover the abstract topics\nin text corpora. Most of the existing topic models are based on the assumption\nof three-layer hierarchical Bayesian structure, i.e. each document is modeled\nas a probability distribution over topics, and each topic is a probability\ndistribution over words. However, the assumption is not optimal. Intuitively,\nit's more reasonable to assume that each topic is a probability distribution\nover concepts, and then each concept is a probability distribution over words,\ni.e. adding a latent concept layer between topic layer and word layer in\ntraditional three-layer assumption. In this paper, we verify the proposed\nassumption by incorporating the new assumption in two representative topic\nmodels, and obtain two novel topic models. Extensive experiments were conducted\namong the proposed models and corresponding baselines, and the results show\nthat the proposed models significantly outperform the baselines in terms of\ncase study and perplexity, which means the new assumption is more reasonable\nthan traditional one.", "published": "2017-04-07 05:12:38", "link": "http://arxiv.org/abs/1704.02090v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter\n  Sentiment Analysis", "abstract": "This paper describes our multi-view ensemble approach to SemEval-2017 Task 4\non Sentiment Analysis in Twitter, specifically, the Message Polarity\nClassification subtask for English (subtask A). Our system is a voting\nensemble, where each base classifier is trained in a different feature space.\nThe first space is a bag-of-words model and has a Linear SVM as base\nclassifier. The second and third spaces are two different strategies of\ncombining word embeddings to represent sentences and use a Linear SVM and a\nLogistic Regressor as base classifiers. The proposed system was ranked 18th out\nof 38 systems considering F1 score and 20th considering recall.", "published": "2017-04-07 15:27:10", "link": "http://arxiv.org/abs/1704.02263v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TransNets: Learning to Transform for Recommendation", "abstract": "Recently, deep learning methods have been shown to improve the performance of\nrecommender systems over traditional methods, especially when review text is\navailable. For example, a recent model, DeepCoNN, uses neural nets to learn one\nlatent representation for the text of all reviews written by a target user, and\na second latent representation for the text of all reviews for a target item,\nand then combines these latent representations to obtain state-of-the-art\nperformance on recommendation tasks. We show that (unsurprisingly) much of the\npredictive value of review text comes from reviews of the target user for the\ntarget item. We then introduce a way in which this information can be used in\nrecommendation, even when the target user's review for the target item is not\navailable. Our model, called TransNets, extends the DeepCoNN model by\nintroducing an additional latent layer representing the target user-target item\npair. We then regularize this layer, at training time, to be similar to another\nlatent representation of the target user's review of the target item. We show\nthat TransNets and extensions of it improve substantially over the previous\nstate-of-the-art.", "published": "2017-04-07 17:13:03", "link": "http://arxiv.org/abs/1704.02298v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Constrained Sequence-to-Sequence Neural Model for Sentence\n  Simplification", "abstract": "Sentence simplification reduces semantic complexity to benefit people with\nlanguage impairments. Previous simplification studies on the sentence level and\nword level have achieved promising results but also meet great challenges. For\nsentence-level studies, sentences after simplification are fluent but sometimes\nare not really simplified. For word-level studies, words are simplified but\nalso have potential grammar errors due to different usages of words before and\nafter simplification. In this paper, we propose a two-step simplification\nframework by combining both the word-level and the sentence-level\nsimplifications, making use of their corresponding advantages. Based on the\ntwo-step framework, we implement a novel constrained neural generation model to\nsimplify sentences given simplified words. The final results on Wikipedia and\nSimple Wikipedia aligned datasets indicate that our method yields better\nperformance than various baselines.", "published": "2017-04-07 17:53:24", "link": "http://arxiv.org/abs/1704.02312v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
