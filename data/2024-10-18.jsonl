{"title": "Dynamic Factor Allocation Leveraging Regime-Switching Signals", "abstract": "This article explores dynamic factor allocation by analyzing the cyclical\nperformance of factors through regime analysis. The authors focus on a U.S.\nequity investment universe comprising seven long-only indices representing the\nmarket and six style factors: value, size, momentum, quality, low volatility,\nand growth. Their approach integrates factor-specific regime inferences of each\nfactor index's active performance relative to the market into the\nBlack-Litterman model to construct a fully-invested, long-only multi-factor\nportfolio. First, the authors apply the sparse jump model (SJM) to identify\nbull and bear market regimes for individual factors, using a feature set based\non risk and return measures from historical factor active returns, as well as\nvariables reflecting the broader market environment. The regimes identified by\nthe SJM exhibit enhanced stability and interpretability compared to traditional\nmethods. A hypothetical single-factor long-short strategy is then used to\nassess these regime inferences and fine-tune hyperparameters, resulting in a\npositive Sharpe ratio of this strategy across all factors with low correlation\namong them. These regime inferences are then incorporated into the\nBlack-Litterman framework to dynamically adjust allocations among the seven\nindices, with an equally weighted (EW) portfolio serving as the benchmark.\nEmpirical results show that the constructed multi-factor portfolio\nsignificantly improves the information ratio (IR) relative to the market,\nraising it from just 0.05 for the EW benchmark to approximately 0.4. When\nmeasured relative to the EW benchmark itself, the dynamic allocation achieves\nan IR of around 0.4 to 0.5. The strategy also enhances absolute portfolio\nperformance across key metrics such as the Sharpe ratio and maximum drawdown.", "published": "2024-10-18 19:42:01", "link": "http://arxiv.org/abs/2410.14841v1", "categories": ["q-fin.PM", "q-fin.CP", "q-fin.ST"], "primary_category": "q-fin.PM"}
{"title": "Simultaneously Solving FBSDEs with Neural Operators of Logarithmic Depth, Constant Width, and Sub-Linear Rank", "abstract": "Forward-backwards stochastic differential equations (FBSDEs) are central in\noptimal control, game theory, economics, and mathematical finance.\nUnfortunately, the available FBSDE solvers operate on \\textit{individual}\nFBSDEs, meaning that they cannot provide a computationally feasible strategy\nfor solving large families of FBSDEs as these solvers must be re-run several\ntimes. \\textit{Neural operators} (NOs) offer an alternative approach for\n\\textit{simultaneously solving} large families of FBSDEs by directly\napproximating the solution operator mapping \\textit{inputs:} terminal\nconditions and dynamics of the backwards process to \\textit{outputs:} solutions\nto the associated FBSDE. Though universal approximation theorems (UATs)\nguarantee the existence of such NOs, these NOs are unrealistically large. We\nconfirm that ``small'' NOs can uniformly approximate the solution operator to\nstructured families of FBSDEs with random terminal time, uniformly on suitable\ncompact sets determined by Sobolev norms, to any prescribed error\n$\\varepsilon>0$ using a depth of $\\mathcal{O}(\\log(1/\\varepsilon))$, a width of\n$\\mathcal{O}(1)$, and a sub-linear rank; i.e. $\\mathcal{O}(1/\\varepsilon^r)$\nfor some $r<1$. This result is rooted in our second main contribution, which\nshows that convolutional NOs of similar depth, width, and rank can approximate\nthe solution operator to a broad class of Elliptic PDEs. A key insight here is\nthat the convolutional layers of our NO can efficiently encode the Green's\nfunction associated to the Elliptic PDEs linked to our FBSDEs. A byproduct of\nour analysis is the first theoretical justification for the benefit of lifting\nchannels in NOs: they exponentially decelerate the growth rate of the NO's\nrank.", "published": "2024-10-18 18:01:40", "link": "http://arxiv.org/abs/2410.14788v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "q-fin.CP"], "primary_category": "math.OC"}
{"title": "Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets", "abstract": "Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.", "published": "2024-10-18 16:37:52", "link": "http://arxiv.org/abs/2410.14587v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Reinforcement Learning in Non-Markov Market-Making", "abstract": "We develop a deep reinforcement learning (RL) framework for an optimal\nmarket-making (MM) trading problem, specifically focusing on price processes\nwith semi-Markov and Hawkes Jump-Diffusion dynamics. We begin by discussing the\nbasics of RL and the deep RL framework used, where we deployed the\nstate-of-the-art Soft Actor-Critic (SAC) algorithm for the deep learning part.\nThe SAC algorithm is an off-policy entropy maximization algorithm more suitable\nfor tackling complex, high-dimensional problems with continuous state and\naction spaces like in optimal market-making (MM). We introduce the optimal MM\nproblem considered, where we detail all the deterministic and stochastic\nprocesses that go into setting up an environment for simulating this strategy.\nHere we also give an in-depth overview of the jump-diffusion pricing dynamics\nused, our method for dealing with adverse selection within the limit order\nbook, and we highlight the working parts of our optimization problem. Next, we\ndiscuss training and testing results, where we give visuals of how important\ndeterministic and stochastic processes such as the bid/ask, trade executions,\ninventory, and the reward function evolved. We include a discussion on the\nlimitations of these results, which are important points to note for most\ndiffusion models in this setting.", "published": "2024-10-18 14:35:26", "link": "http://arxiv.org/abs/2410.14504v2", "categories": ["q-fin.CP", "q-fin.MF"], "primary_category": "q-fin.CP"}
{"title": "CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using\n  Large Language Models", "abstract": "Generating emotionally appropriate responses in conversations with large\nlanguage models presents a significant challenge due to the complexities of\nhuman emotions and cognitive processes, which remain largely underexplored in\ntheir critical role in social interactions. In this study, we introduce a\ntwo-stage automatic data generation framework to create CAPE, a Chinese dataset\nnamed Cognitive Appraisal theory-based Emotional corpus. This corpus\nfacilitates the generation of dialogues with contextually appropriate emotional\nresponses by accounting for diverse personal and situational factors. We\npropose two tasks utilizing this dataset: emotion prediction and next utterance\nprediction. Both automated and human evaluations demonstrate that agents\ntrained on our dataset can deliver responses that are more aligned with human\nemotional expressions. Our study shows the potential for advancing emotional\nexpression in conversational agents, paving the way for more nuanced and\nmeaningful human-computer interactions.", "published": "2024-10-18 03:33:18", "link": "http://arxiv.org/abs/2410.14145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy\n  with LLM-based Agent", "abstract": "Public scarce resource allocation plays a crucial role in economics as it\ndirectly influences the efficiency and equity in society. Traditional studies\nincluding theoretical model-based, empirical study-based and simulation-based\nmethods encounter limitations due to the idealized assumption of complete\ninformation and individual rationality, as well as constraints posed by limited\navailable data. In this work, we propose an innovative framework, SRAP-Agent\n(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based\nAgent), which integrates Large Language Models (LLMs) into economic\nsimulations, aiming to bridge the gap between theoretical models and real-world\ndynamics. Using public housing allocation scenarios as a case study, we conduct\nextensive policy simulation experiments to verify the feasibility and\neffectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm\nwith certain optimization objectives. The source code can be found in\nhttps://github.com/jijiarui-cather/SRAPAgent_Framework", "published": "2024-10-18 03:43:42", "link": "http://arxiv.org/abs/2410.14152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models", "abstract": "Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}", "published": "2024-10-18 03:45:42", "link": "http://arxiv.org/abs/2410.14155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Genre-Aware Article Scoring and Feedback Using Large Language\n  Models", "abstract": "This paper focuses on the development of an advanced intelligent article\nscoring system that not only assesses the overall quality of written work but\nalso offers detailed feature-based scoring tailored to various article genres.\nBy integrating the pre-trained BERT model with the large language model\nChat-GPT, the system gains a deep understanding of both the content and\nstructure of the text, enabling it to provide a thorough evaluation along with\ntargeted suggestions for improvement. Experimental results demonstrate that\nthis system outperforms traditional scoring methods across multiple public\ndatasets, particularly in feature-based assessments, offering a more accurate\nreflection of the quality of different article types. Moreover, the system\ngenerates personalized feedback to assist users in enhancing their writing\nskills, underscoring the potential and practical value of automated scoring\ntechnologies in educational contexts.", "published": "2024-10-18 04:13:51", "link": "http://arxiv.org/abs/2410.14165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting", "abstract": "Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.", "published": "2024-10-18 05:16:39", "link": "http://arxiv.org/abs/2410.14180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time", "abstract": "Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.", "published": "2024-10-18 05:31:13", "link": "http://arxiv.org/abs/2410.14184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MediTOD: An English Dialogue Dataset for Medical History Taking with\n  Comprehensive Annotations", "abstract": "Medical task-oriented dialogue systems can assist doctors by collecting\npatient medical history, aiding in diagnosis, or guiding treatment selection,\nthereby reducing doctor burnout and expanding access to medical services.\nHowever, doctor-patient dialogue datasets are not readily available, primarily\ndue to privacy regulations. Moreover, existing datasets lack comprehensive\nannotations involving medical slots and their different attributes, such as\nsymptoms and their onset, progression, and severity. These comprehensive\nannotations are crucial for accurate diagnosis. Finally, most existing datasets\nare non-English, limiting their utility for the larger research community.\n  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues\nin English for the medical history-taking task. Collaborating with doctors, we\ndevise a questionnaire-based labeling scheme tailored to the medical domain.\nThen, medical professionals create the dataset with high-quality comprehensive\nannotations, capturing medical slots and their attributes. We establish\nbenchmarks in supervised and few-shot settings on MediTOD for natural language\nunderstanding, policy learning, and natural language generation subtasks,\nevaluating models from both TOD and biomedical domains. We make MediTOD\npublicly available for future research.", "published": "2024-10-18 06:38:22", "link": "http://arxiv.org/abs/2410.14204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning", "abstract": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.", "published": "2024-10-18 06:57:19", "link": "http://arxiv.org/abs/2410.14211v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework", "abstract": "Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards.", "published": "2024-10-18 07:25:00", "link": "http://arxiv.org/abs/2410.14231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning", "abstract": "Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations.", "published": "2024-10-18 07:34:21", "link": "http://arxiv.org/abs/2410.14235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Method to Metigate Demographic and Expert Bias in ICD Coding\n  with Causal Inference", "abstract": "ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. Considering ICD coding as\na multi-label text classification task, researchers have developed\nsophisticated methods. Despite progress, these models often suffer from label\nimbalance and may develop spurious correlations with demographic factors.\nAdditionally, while human coders assign ICD codes, the inclusion of irrelevant\ninformation from unrelated experts introduces biases. To combat these issues,\nwe propose a novel method to mitigate Demographic and Expert biases in ICD\ncoding through Causal Inference (DECI). We provide a novel causality-based\ninterpretation in ICD Coding that models make predictions by three distinct\npathways. And based counterfactual reasoning, DECI mitigate demographic and\nexpert biases. Experimental results show that DECI outperforms state-of-the-art\nmodels, offering a significant advancement in accurate and unbiased ICD coding.", "published": "2024-10-18 07:36:57", "link": "http://arxiv.org/abs/2410.14236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models", "abstract": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.", "published": "2024-10-18 07:52:22", "link": "http://arxiv.org/abs/2410.14248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement", "abstract": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-LLM collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and two multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.", "published": "2024-10-18 08:14:10", "link": "http://arxiv.org/abs/2410.14259v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding", "abstract": "Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.", "published": "2024-10-18 08:31:22", "link": "http://arxiv.org/abs/2410.14276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Critical Questions Generation: Motivation and Challenges", "abstract": "The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.", "published": "2024-10-18 09:46:38", "link": "http://arxiv.org/abs/2410.14335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Computing Susceptibility to Context in Language Models", "abstract": "One strength of modern language models is their ability to incorporate\ninformation from a user-input context when answering queries. However, they are\nnot equally sensitive to the subtle changes to that context. To quantify this,\nDu et al. (2024) gives an information-theoretic metric to measure such\nsensitivity. Their metric, susceptibility, is defined as the degree to which\ncontexts can influence a model's response to a query at a distributional level.\nHowever, exactly computing susceptibility is difficult and, thus, Du et al.\n(2024) falls back on a Monte Carlo approximation. Due to the large number of\nsamples required, the Monte Carlo approximation is inefficient in practice. As\na faster alternative, we propose Fisher susceptibility, an efficient method to\nestimate the susceptibility based on Fisher information. Empirically, we\nvalidate that Fisher susceptibility is comparable to Monte Carlo estimated\nsusceptibility across a diverse set of query domains despite its being\n$70\\times$ faster. Exploiting the improved efficiency, we apply Fisher\nsusceptibility to analyze factors affecting the susceptibility of language\nmodels. We observe that larger models are as susceptible as smaller ones.", "published": "2024-10-18 10:40:47", "link": "http://arxiv.org/abs/2410.14361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do Multilingual Language Models Remember Facts?", "abstract": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has only focused on\nEnglish monolingual models. The question of how these mechanisms generalize to\nnon-English languages and multilingual LLMs remains unexplored. In this paper,\nwe address this gap by conducting a comprehensive analysis of three\nmultilingual LLMs. First, we show that previously identified recall mechanisms\nin English largely apply to multilingual contexts, with nuances based on\nlanguage and architecture. Next, through patching intermediate representations,\nwe localize the role of language during recall, finding that subject enrichment\nis language-independent, while object extraction is language-dependent.\nAdditionally, we discover that the last token representation acts as a Function\nVector (FV), encoding both the language of the query and the content to be\nextracted from the subject. Furthermore, in decoder-only LLMs, FVs compose\nthese two pieces of information in two separate stages. These insights reveal\nunique mechanisms in multilingual LLMs for recalling information, highlighting\nthe need for new methodologies--such as knowledge evaluation, fact editing, and\nknowledge acquisition--that are specifically tailored for multilingual LLMs.", "published": "2024-10-18 11:39:34", "link": "http://arxiv.org/abs/2410.14387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation", "abstract": "Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation.", "published": "2024-10-18 11:52:10", "link": "http://arxiv.org/abs/2410.14391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning", "abstract": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.", "published": "2024-10-18 12:02:41", "link": "http://arxiv.org/abs/2410.14399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion", "abstract": "Language models (LMs) can make a correct prediction based on many possible\nsignals in a prompt, not all corresponding to recall of factual associations.\nHowever, current interpretations of LMs fail to take this into account. For\nexample, given the query \"Astrid Lindgren was born in\" with the corresponding\ncompletion \"Sweden\", no difference is made between whether the prediction was\nbased on knowing where the author was born or assuming that a person with a\nSwedish-sounding name was born in Sweden. In this paper, we present a\nmodel-specific recipe - PrISM - for constructing datasets with examples of four\ndifferent prediction scenarios: generic language modeling, guesswork,\nheuristics recall and exact fact recall. We apply two popular interpretability\nmethods to the scenarios: causal tracing (CT) and information flow analysis. We\nfind that both yield distinct results for each scenario. Results for exact fact\nrecall and generic language modeling scenarios confirm previous conclusions\nabout the importance of mid-range MLP sublayers for fact recall, while results\nfor guesswork and heuristics indicate a critical role of late last token\nposition MLP sublayers. In summary, we contribute resources for a more\nextensive and granular study of fact completion in LMs, together with analyses\nthat provide a more nuanced understanding of how LMs process fact-related\nqueries.", "published": "2024-10-18 12:08:07", "link": "http://arxiv.org/abs/2410.14405v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference", "abstract": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.", "published": "2024-10-18 13:01:14", "link": "http://arxiv.org/abs/2410.14442v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models", "abstract": "As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.", "published": "2024-10-18 14:03:52", "link": "http://arxiv.org/abs/2410.14480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a\n  Continuum", "abstract": "There is increasing interest in looking at dialects in NLP. However, most\nwork to date still treats dialects as discrete categories. For instance,\nevaluative work in variation-oriented NLP for English often works with Indian\nEnglish or African-American Venacular English as homogeneous categories (Faisal\net al., 2024; Ziems et al., 2023), yet even within one variety there is\nsubstantial variation. We examine within-dialect variation and show that\nperformance critically varies within categories. We measure speech-to-text\nperformance on Italian dialects, and empirically observe a geographical\nperformance disparity. This disparity correlates substantially (-0.5) with\nlinguistic similarity to the highest performing dialect variety. We\ncross-examine our results against dialectometry methods, and interpret the\nperformance disparity to be due to a bias towards dialects that are more\nsimilar to the standard variety in the speech-to-text model examined. We\nadditionally leverage geostatistical methods to predict zero-shot performance\nat unseen sites, and find the incorporation of geographical information to\nsubstantially improve prediction performance, indicating there to be\ngeographical structure in the performance distribution.", "published": "2024-10-18 16:39:42", "link": "http://arxiv.org/abs/2410.14589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases", "abstract": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).", "published": "2024-10-18 16:44:22", "link": "http://arxiv.org/abs/2410.14594v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Shall Know a Tool by the Traces it Leaves: The Predictability of\n  Sentiment Analysis Tools", "abstract": "If sentiment analysis tools were valid classifiers, one would expect them to\nprovide comparable results for sentiment classification on different kinds of\ncorpora and for different languages. In line with results of previous studies\nwe show that sentiment analysis tools disagree on the same dataset. Going\nbeyond previous studies we show that the sentiment tool used for sentiment\nannotation can even be predicted from its outcome, revealing an algorithmic\nbias of sentiment analysis. Based on Twitter, Wikipedia and different news\ncorpora from the English, German and French languages, our classifiers separate\nsentiment tools with an averaged F1-score of 0.89 (for the English corpora). We\ntherefore warn against taking sentiment annotations as face value and argue for\nthe need of more and systematic NLP evaluation studies.", "published": "2024-10-18 17:27:38", "link": "http://arxiv.org/abs/2410.14626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverging Preferences: When do Annotators Disagree and do Models Know?", "abstract": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.", "published": "2024-10-18 17:32:22", "link": "http://arxiv.org/abs/2410.14632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps", "abstract": "Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.", "published": "2024-10-18 17:57:40", "link": "http://arxiv.org/abs/2410.14668v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts", "abstract": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld. The code is available at\nhttps://github.com/Advacheck-OU/ai-dataset-analysing.", "published": "2024-10-18 17:59:57", "link": "http://arxiv.org/abs/2410.14677v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Document Event-Keyed Summarization", "abstract": "Event-keyed summarization (EKS) requires summarizing a specific event\ndescribed in a document given the document text and an event representation\nextracted from it. In this work, we extend EKS to the cross-document setting\n(CDEKS), in which summaries must synthesize information from accounts of the\nsame event as given by multiple sources. We introduce SEAMUS (Summaries of\nEvents Across Multiple Sources), a high-quality dataset for CDEKS based on an\nexpert reannotation of the FAMUS dataset for cross-document argument\nextraction. We present a suite of baselines on SEAMUS -- covering both smaller,\nfine-tuned models, as well as zero- and few-shot prompted LLMs -- along with\ndetailed ablations and a human evaluation study, showing SEAMUS to be a\nvaluable benchmark for this new task.", "published": "2024-10-18 18:09:45", "link": "http://arxiv.org/abs/2410.14795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This Candidate is [MASK]. Letters of Reference and Job Market Outcomes\n  using LLMs", "abstract": "I implement a prompt-based learning strategy to extract measures of sentiment\nand other features from confidential reference letters. I show that the\ncontents of reference letters is clearly reflected in the performance of job\nmarket candidates in the Economics academic job market. In contrast, applying\ntraditional ``bag-of-words'' approaches produces measures of sentiment that,\nwhile positively correlated to my LLM-based measure, are not predictive of job\nmarket outcomes. Using a random forest, I show that both letter quality and\nlength are predictive of success in the job market. Letters authored by\nadvisers appear to be as important as those written by other referees.", "published": "2024-10-18 14:03:46", "link": "http://arxiv.org/abs/2410.16325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering", "abstract": "Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.", "published": "2024-10-18 03:00:03", "link": "http://arxiv.org/abs/2410.14132v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents", "abstract": "When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting.", "published": "2024-10-18 03:26:06", "link": "http://arxiv.org/abs/2410.14141v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "A Lightweight Multi Aspect Controlled Text Generation Solution For Large\n  Language Models", "abstract": "Large language models (LLMs) show remarkable abilities with instruction\ntuning. However, they fail to achieve ideal tasks when lacking high-quality\ninstruction tuning data on target tasks. Multi-Aspect Controllable Text\nGeneration (MCTG) is a representative task for this dilemma, where aspect\ndatasets are usually biased and correlated. Existing work exploits additional\nmodel structures and strategies for solutions, limiting adaptability to LLMs.\nTo activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based\non data augmentation. We analyze bias and correlations in traditional datasets,\nand address these concerns with augmented control attributes and sentences.\nAugmented datasets are feasible for instruction tuning. In our experiments,\nLLMs perform better in MCTG after data augmentation, with a 20% accuracy rise\nand less aspect correlations.", "published": "2024-10-18 03:32:00", "link": "http://arxiv.org/abs/2410.14144v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment", "abstract": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.", "published": "2024-10-18 03:34:32", "link": "http://arxiv.org/abs/2410.14148v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Utilizing Large Language Models for Event Deconstruction to Enhance\n  Multimodal Aspect-Based Sentiment Analysis", "abstract": "With the rapid development of the internet, the richness of User-Generated\nContentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis\n(MABSA) a research hotspot. Existing studies have achieved certain results in\nMABSA, but they have not effectively addressed the analytical challenges in\nscenarios where multiple entities and sentiments coexist. This paper\ninnovatively introduces Large Language Models (LLMs) for event decomposition\nand proposes a reinforcement learning framework for Multimodal Aspect-based\nSentiment Analysis (MABSA-RL) framework. This framework decomposes the original\ntext into a set of events using LLMs, reducing the complexity of analysis,\nintroducing reinforcement learning to optimize model parameters. Experimental\nresults show that MABSA-RL outperforms existing advanced methods on two\nbenchmark datasets. This paper provides a new research perspective and method\nfor multimodal aspect-level sentiment analysis.", "published": "2024-10-18 03:40:45", "link": "http://arxiv.org/abs/2410.14150v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning", "abstract": "Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-Granularity Diffusion Modeling (MGDM), which prioritizes subgoals based\non difficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MGDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MGDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks. All associated codes are\navailable at\n\\href{https://github.com/HKUNLP/diffusion-vs-ar}{https://github.com/HKUNLP/diffusion-vs-ar}.", "published": "2024-10-18 03:48:53", "link": "http://arxiv.org/abs/2410.14157v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with\n  Simple Word-based Counting Problems", "abstract": "Interestingly, LLMs yet struggle with some basic tasks that humans find\ntrivial to handle, e.g., counting the number of character r's in the word\n\"strawberry\". There are several popular conjectures (e.g., tokenization,\narchitecture and training data) regarding the reason for deficiency of LLMs in\nsimple word-based counting problems, sharing the similar belief that such\nfailure stems from model pretraining hence probably inevitable during\ndeployment. In this paper, we carefully design multiple evaluation settings to\ninvestigate validity of prevalent conjectures. Meanwhile, we measure\ntransferability of advanced mathematical and coding reasoning capabilities from\nspecialized LLMs to simple counting tasks. Although specialized LLMs suffer\nfrom counting problems as well, we find conjectures about inherent deficiency\nof LLMs invalid and further seek opportunities to elicit knowledge and\ncapabilities from LLMs that are beneficial to counting tasks. Compared with\nstrategies such as finetuning and in-context learning that are commonly adopted\nto enhance performance on new or challenging tasks, we show that engaging\nreasoning is the most robust and efficient way to help LLMs better perceive\ntasks with more accurate responses.\n  We hope our conjecture validation design could provide insights into the\nstudy of future critical failure modes of LLMs. Based on challenges in\ntransferring advanced capabilities to much simpler tasks, we call for more\nattention to model capability acquisition and evaluation. We also highlight the\nimportance of cultivating consciousness of \"reasoning before responding\" during\nmodel pretraining.", "published": "2024-10-18 04:17:16", "link": "http://arxiv.org/abs/2410.14166v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA", "published": "2024-10-18 05:15:50", "link": "http://arxiv.org/abs/2410.14179v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs", "abstract": "Artificial Intelligence (AI) is revolutionizing scientific research, yet its\ngrowing integration into laboratory environments presents critical safety\nchallenges. While large language models (LLMs) increasingly assist in tasks\nranging from procedural guidance to autonomous experiment orchestration, an\n\"illusion of understanding\" may lead researchers to overestimate their\nreliability. Such overreliance is especially hazardous in high-stakes\nlaboratory settings, where failures in hazard identification or risk assessment\ncan result in severe accidents. To address these concerns, we propose the\nLaboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that\nevaluates LLMs and vision language models (VLMs) on their ability to identify\npotential hazards, assess risks, and predict the consequences of unsafe actions\nin lab environments. LabSafety Bench comprises 765 multiple-choice questions\naligned with US Occupational Safety and Health Administration (OSHA) protocols,\nalong with 520 realistic laboratory scenarios featuring dual evaluation tasks:\nthe Hazards Identification Test and the Consequence Identification Test, with\n4090 open-ended questions in total. Evaluations across eight proprietary\nmodels, seven open-weight LLMs, and four VLMs reveal that, despite advanced\nperformance on structured assessments, no model achieves the safety threshold\nrequired for reliable operation. None scored above 75% on the Hazards\nIdentification Test. Moreover, while proprietary models tend to excel in\nmultiple-choice evaluations, their performance in open-ended, real-world\nscenario responses is comparable to that of open-source models. These findings\nunderscore the urgent need for specialized evaluation frameworks to ensure the\nsafe and responsible deployment of AI in laboratory settings.", "published": "2024-10-18 05:21:05", "link": "http://arxiv.org/abs/2410.14182v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speciesism in Natural Language Processing Research", "abstract": "Natural Language Processing (NLP) research on AI Safety and social bias in AI\nhas focused on safety for humans and social bias against human minorities.\nHowever, some AI ethicists have argued that the moral significance of nonhuman\nanimals has been ignored in AI research. Therefore, the purpose of this study\nis to investigate whether there is speciesism, i.e., discrimination against\nnonhuman animals, in NLP research. First, we explain why nonhuman animals are\nrelevant in NLP research. Next, we survey the findings of existing research on\nspeciesism in NLP researchers, data, and models and further investigate this\nproblem in this study. The findings of this study suggest that speciesism\nexists within researchers, data, and models, respectively. Specifically, our\nsurvey and experiments show that (a) among NLP researchers, even those who\nstudy social bias in AI, do not recognize speciesism or speciesist bias; (b)\namong NLP data, speciesist bias is inherent in the data annotated in the\ndatasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,\nexhibit speciesist bias by default. Finally, we discuss how we can reduce\nspeciesism in NLP research.", "published": "2024-10-18 06:09:41", "link": "http://arxiv.org/abs/2410.14194v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supervised Chain of Thought", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.", "published": "2024-10-18 06:25:27", "link": "http://arxiv.org/abs/2410.14198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs", "abstract": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git.", "published": "2024-10-18 06:35:17", "link": "http://arxiv.org/abs/2410.14202v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Joint Multimodal Entity-Relation Extraction via\n  Knowledge-Enhanced Cross-modal Prompt Model", "abstract": "Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task\nthat aims to extract entities and their relations from text-image pairs in\nsocial media posts. Existing methods for JMERE require large amounts of labeled\ndata. However, gathering and annotating fine-grained multimodal data for JMERE\nposes significant challenges. Initially, we construct diverse and comprehensive\nmultimodal few-shot datasets fitted to the original data distribution. To\naddress the insufficient information in the few-shot setting, we introduce the\n\\textbf{K}nowledge-\\textbf{E}nhanced \\textbf{C}ross-modal \\textbf{P}rompt\n\\textbf{M}odel (KECPM) for JMERE. This method can effectively address the\nproblem of insufficient information in the few-shot setting by guiding a large\nlanguage model to generate supplementary background knowledge. Our proposed\nmethod comprises two stages: (1) a knowledge ingestion stage that dynamically\nformulates prompts based on semantic similarity guide ChatGPT generating\nrelevant knowledge and employs self-reflection to refine the knowledge; (2) a\nknowledge-enhanced language model stage that merges the auxiliary knowledge\nwith the original input and utilizes a transformer-based model to align with\nJMERE's required output format. We extensively evaluate our approach on a\nfew-shot dataset derived from the JMERE dataset, demonstrating its superiority\nover strong baselines in terms of both micro and macro F$_1$ scores.\nAdditionally, we present qualitative analyses and case studies to elucidate the\neffectiveness of our model.", "published": "2024-10-18 07:14:54", "link": "http://arxiv.org/abs/2410.14225v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation", "abstract": "Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. However, its effectiveness depends on high-quality\ninstruction data, which is challenging to obtain in the real world due to\nprivacy concerns, data scarcity, and high annotation costs. To fill this gap,\ninspired by the recent success of using LLMs to simulate human society, we\npropose MATRIX, a multi-agent simulator that automatically generates diverse\ntext-based scenarios, capturing a wide range of real-world human needs in a\nrealistic and scalable manner. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. On AlpacaEval 2\nand Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets\nsynthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms\nMeta's Llama-3-8B-Instruct model, which was trained on over 10M pairs.", "published": "2024-10-18 08:01:39", "link": "http://arxiv.org/abs/2410.14251v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas", "abstract": "Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.", "published": "2024-10-18 08:04:36", "link": "http://arxiv.org/abs/2410.14255v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation", "abstract": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.", "published": "2024-10-18 08:18:18", "link": "http://arxiv.org/abs/2410.14262v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MoDification: Mixture of Depths Made Easy", "abstract": "Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.", "published": "2024-10-18 08:22:07", "link": "http://arxiv.org/abs/2410.14268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SwaQuAD-24: QA Benchmark Dataset in Swahili", "abstract": "This paper proposes the creation of a Swahili Question Answering (QA)\nbenchmark dataset, aimed at addressing the underrepresentation of Swahili in\nnatural language processing (NLP). Drawing from established benchmarks like\nSQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing\nhigh-quality, annotated question-answer pairs that capture the linguistic\ndiversity and complexity of Swahili. The dataset is designed to support a\nvariety of applications, including machine translation, information retrieval,\nand social services like healthcare chatbots. Ethical considerations, such as\ndata privacy, bias mitigation, and inclusivity, are central to the dataset\ndevelopment. Additionally, the paper outlines future expansion plans to include\ndomain-specific content, multimodal integration, and broader crowdsourcing\nefforts. The Swahili QA dataset aims to foster technological innovation in East\nAfrica and provide an essential resource for NLP research and applications in\nlow-resource languages.", "published": "2024-10-18 08:49:24", "link": "http://arxiv.org/abs/2410.14289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoGU: Long-form Generation with Uncertainty Expressions", "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.", "published": "2024-10-18 09:15:35", "link": "http://arxiv.org/abs/2410.14309v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning", "abstract": "The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.", "published": "2024-10-18 11:06:23", "link": "http://arxiv.org/abs/2410.14375v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SignAttention: On the Interpretability of Transformer Models for Sign\n  Language Translation", "abstract": "This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.", "published": "2024-10-18 14:38:37", "link": "http://arxiv.org/abs/2410.14506v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs \"know\" internally when they follow instructions?", "abstract": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents.", "published": "2024-10-18 14:55:14", "link": "http://arxiv.org/abs/2410.14516v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization", "abstract": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.", "published": "2024-10-18 15:40:48", "link": "http://arxiv.org/abs/2410.14545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs estimate uncertainty well in instruction-following?", "abstract": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.", "published": "2024-10-18 16:32:10", "link": "http://arxiv.org/abs/2410.14582v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Teaching Models to Balance Resisting and Accepting Persuasion", "abstract": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.", "published": "2024-10-18 16:49:36", "link": "http://arxiv.org/abs/2410.14596v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search", "abstract": "Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.", "published": "2024-10-18 17:03:17", "link": "http://arxiv.org/abs/2410.14609v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings", "abstract": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. GenEOL\nalso achieves notable gains in clustering, reranking, and pair-classification\ntasks from the MTEB benchmark. Additionally, GenEOL stabilizes representation\nquality across LLM layers and remains robust to perturbations of embedding\nprompts.", "published": "2024-10-18 17:36:53", "link": "http://arxiv.org/abs/2410.14635v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs", "abstract": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.", "published": "2024-10-18 17:41:19", "link": "http://arxiv.org/abs/2410.14641v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real-time Fake News from Adversarial Feedback", "abstract": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors -- even after their knowledge\ncutoffs. This suggests that recent popular fake news from such sources can be\neasily detected due to pre-training and retrieval corpus contamination or\nincreasingly salient shallow patterns. Instead, we argue that a proper fake\nnews detection dataset should test a model's ability to reason factually about\nthe current world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification ROC-AUC by an absolute 17.5 percent for a strong RAG-based\nGPT-4o detector. Our experiments reveal the important role of RAG in both\ndetecting and generating fake news, as retrieval-free LLM detectors are\nvulnerable to unseen events and adversarial attacks, while feedback from RAG\ndetection helps discover more deceitful patterns in fake news.", "published": "2024-10-18 17:47:11", "link": "http://arxiv.org/abs/2410.14651v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples", "abstract": "Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.", "published": "2024-10-18 17:58:21", "link": "http://arxiv.org/abs/2410.14669v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "To Trust or Not to Trust? Enhancing Large Language Models' Situated\n  Faithfulness to External Contexts", "abstract": "Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR.", "published": "2024-10-18 17:59:47", "link": "http://arxiv.org/abs/2410.14675v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment", "abstract": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.", "published": "2024-10-18 17:59:51", "link": "http://arxiv.org/abs/2410.14676v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TimeSeriesExam: A time series understanding exam", "abstract": "Large Language Models (LLMs) have recently demonstrated a remarkable ability\nto model time series data. These capabilities can be partly explained if LLMs\nunderstand basic time series concepts. However, our knowledge of what these\nmodels understand about time series data remains relatively limited. To address\nthis gap, we introduce TimeSeriesExam, a configurable and scalable\nmultiple-choice question exam designed to assess LLMs across five core time\nseries understanding categories: pattern recognition, noise understanding,\nsimilarity analysis, anomaly detection, and causality analysis. TimeSeriesExam\ncomprises of over 700 questions, procedurally generated using 104 carefully\ncurated templates and iteratively refined to balance difficulty and their\nability to discriminate good from bad models. We test 7 state-of-the-art LLMs\non the TimeSeriesExam and provide the first comprehensive evaluation of their\ntime series understanding abilities. Our results suggest that closed-source\nmodels such as GPT-4 and Gemini understand simple time series concepts\nsignificantly better than their open-source counterparts, while all models\nstruggle with complex concepts such as causality analysis. We believe that the\nability to programatically generate questions is fundamental to assessing and\nimproving LLM's ability to understand and reason about time series data.", "published": "2024-10-18 02:37:14", "link": "http://arxiv.org/abs/2410.14752v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enabling Scalable Evaluation of Bias Patterns in Medical LLMs", "abstract": "Large language models (LLMs) have shown impressive potential in helping with\nnumerous medical challenges. Deploying LLMs in high-stakes applications such as\nmedicine, however, brings in many concerns. One major area of concern relates\nto biased behaviors of LLMs in medical applications, leading to unfair\ntreatment of individuals. To pave the way for the responsible and impactful\ndeployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the\nhuge complexity and variability of different medical scenarios, existing work\nin this domain has primarily relied on using manually crafted datasets for bias\nevaluation. In this study, we present a new method to scale up such bias\nevaluations by automatically generating test cases based on rigorous medical\nevidence. We specifically target the challenges of a) domain-specificity of\nbias characterization, b) hallucinating while generating the test cases, and c)\nvarious dependencies between the health outcomes and sensitive attributes. To\nthat end, we offer new methods to address these challenges integrated with our\ngenerative pipeline, using medical knowledge graphs, medical ontologies, and\ncustomized general LLM evaluation frameworks in our method. Through a series of\nextensive experiments, we show that the test cases generated by our proposed\nmethod can effectively reveal bias patterns in Med LLMs at larger and more\nflexible scales than human-crafted datasets. We publish a large bias evaluation\ndataset using our pipeline, which is dedicated to a few medical case studies. A\nlive demo of our application for vignette generation is available at\nhttps://vignette.streamlit.app. Our code is also available at\nhttps://github.com/healthylaife/autofair.", "published": "2024-10-18 14:17:03", "link": "http://arxiv.org/abs/2410.14763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Isolated Causal Effects of Natural Language", "abstract": "As language technologies become widespread, it is important to understand how\nvariations in language affect reader perceptions -- formalized as the isolated\ncausal effect of some focal language-encoded intervention on an external\noutcome. A core challenge of estimating isolated effects is the need to\napproximate all non-focal language outside of the intervention. In this paper,\nwe introduce a formal estimation framework for isolated causal effects and\nexplore how different approximations of non-focal language impact effect\nestimates. Drawing on the principle of omitted variable bias, we present\nmetrics for evaluating the quality of isolated effect estimation and non-focal\nlanguage approximation along the axes of fidelity and overlap. In experiments\non semi-synthetic and real-world data, we validate the ability of our framework\nto recover ground truth isolated effects, and we demonstrate the utility of our\nproposed metrics as measures of quality for both isolated effect estimates and\nnon-focal language approximations.", "published": "2024-10-18 18:32:38", "link": "http://arxiv.org/abs/2410.14812v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Effects of Soft-Domain Transfer and Named Entity Information on\n  Deception Detection", "abstract": "In the modern age an enormous amount of communication occurs online, and it\nis difficult to know when something written is genuine or deceitful. There are\nmany reasons for someone to deceive online (e.g., monetary gain, political\ngain) and detecting this behavior without any physical interaction is a\ndifficult task. Additionally, deception occurs in several text-only domains and\nit is unclear if these various sources can be leveraged to improve detection.\nTo address this, eight datasets were utilized from various domains to evaluate\ntheir effect on classifier performance when combined with transfer learning via\nintermediate layer concatenation of fine-tuned BERT models. We find\nimprovements in accuracy over the baseline. Furthermore, we evaluate multiple\ndistance measurements between datasets and find that Jensen-Shannon distance\ncorrelates moderately with transfer learning performance. Finally, the impact\nwas evaluated of multiple methods, which produce additional information in a\ndataset's text via named entities, on BERT performance and we find notable\nimprovement in accuracy of up to 11.2%.", "published": "2024-10-18 18:35:13", "link": "http://arxiv.org/abs/2410.14814v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus", "abstract": "Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.", "published": "2024-10-18 18:35:19", "link": "http://arxiv.org/abs/2410.14815v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DFlow: Diverse Dialogue Flow Simulation with Large Language Models", "abstract": "Developing language model-based dialogue agents requires effective data to\ntrain models that can follow specific task logic. However, most existing data\nsimulation methods focus on increasing diversity in language, topics, or\ndialogue acts at the utterance level, largely neglecting a critical aspect of\ntask logic diversity at the dialogue level. This paper proposes a novel data\nsimulation method designed to enhance the diversity of synthetic dialogues by\nfocusing on task execution logic. Our method uses LLMs to generate decision\ntree-structured task plans, which enables the derivation of diverse dialogue\ntrajectories for a given task. Each trajectory, referred to as a \"dialog flow\",\nguides the generation of a multi-turn dialogue that follows a unique\ntrajectory. We apply this method to generate a task-oriented dialogue dataset\ncomprising 3,886 dialogue flows across 15 different domains. We validate the\neffectiveness of this dataset using the next action prediction task, where\nmodels fine-tuned on our dataset outperform strong baselines, including GPT-4.\nUpon acceptance of this paper, we plan to release the code and data publicly.", "published": "2024-10-18 20:35:28", "link": "http://arxiv.org/abs/2410.14853v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Which LLMs are Difficult to Detect? A Detailed Analysis of Potential\n  Factors Contributing to Difficulties in LLM Text Detection", "abstract": "As LLMs increase in accessibility, LLM-generated texts have proliferated\nacross several fields, such as scientific, academic, and creative writing.\nHowever, LLMs are not created equally; they may have different architectures\nand training datasets. Thus, some LLMs may be more challenging to detect than\nothers. Using two datasets spanning four total writing domains, we train\nAI-generated (AIG) text classifiers using the LibAUC library - a deep learning\nlibrary for training classifiers with imbalanced datasets. Our results in the\nDeepfake Text dataset show that AIG-text detection varies across domains, with\nscientific writing being relatively challenging. In the Rewritten Ivy Panda\n(RIP) dataset focusing on student essays, we find that the OpenAI family of\nLLMs was substantially difficult for our classifiers to distinguish from human\ntexts. Additionally, we explore possible factors that could explain the\ndifficulties in detecting OpenAI-generated texts.", "published": "2024-10-18 21:42:37", "link": "http://arxiv.org/abs/2410.14875v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented\n  Generation", "abstract": "Robust content moderation classifiers are essential for the safety of\nGenerative AI systems. In this task, differences between safe and unsafe inputs\nare often extremely subtle, making it difficult for classifiers (and indeed,\neven humans) to properly distinguish violating vs. benign samples without\ncontext or explanation. Scaling risk discovery and mitigation through\ncontinuous model fine-tuning is also slow, challenging and costly, preventing\ndevelopers from being able to respond quickly and effectively to emergent\nharms. We propose a Classification approach employing Retrieval-Augmented\nGeneration (Class-RAG). Class-RAG extends the capability of its base LLM\nthrough access to a retrieval library which can be dynamically updated to\nenable semantic hotfixing for immediate, flexible risk mitigation. Compared to\nmodel fine-tuning, Class-RAG demonstrates flexibility and transparency in\ndecision-making, outperforms on classification and is more robust against\nadversarial attack, as evidenced by empirical studies. Our findings also\nsuggest that Class-RAG performance scales with retrieval library size,\nindicating that increasing the library size is a viable and low-cost approach\nto improve content moderation.", "published": "2024-10-18 22:07:36", "link": "http://arxiv.org/abs/2410.14881v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense\n  Assessment Items", "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in\nanswering questions pertaining to natural language inference and commonsense\nreasoning. Composing these questions is itself a skilled writing task, so in\nthis paper we consider LLMs as authors of commonsense assessment items. We\nprompt LLMs to generate items in the style of a prominent benchmark for\ncommonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine\nthe outcome according to analyses facilitated by the LLMs and human annotation.\nWe find that LLMs that succeed in answering the original COPA benchmark are\nalso more successful in authoring their own items.", "published": "2024-10-18 22:42:23", "link": "http://arxiv.org/abs/2410.14897v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Contrastive Learning via Cluster-refined Negative Sampling for\n  Semi-supervised Text Classification", "abstract": "Graph contrastive learning (GCL) has been widely applied to text\nclassification tasks due to its ability to generate self-supervised signals\nfrom unlabeled data, thus facilitating model training. However, existing\nGCL-based text classification methods often suffer from negative sampling bias,\nwhere similar nodes are incorrectly paired as negative pairs. This can lead to\nover-clustering, where instances of the same class are divided into different\nclusters. To address the over-clustering issue, we propose an innovative\nGCL-based method of graph contrastive learning via cluster-refined negative\nsampling for semi-supervised text classification, namely ClusterText. Firstly,\nwe combine the pre-trained model Bert with graph neural networks to learn text\nrepresentations. Secondly, we introduce a clustering refinement strategy, which\nclusters the learned text representations to obtain pseudo labels. For each\ntext node, its negative sample set is drawn from different clusters.\nAdditionally, we propose a self-correction mechanism to mitigate the loss of\ntrue negative samples caused by clustering inconsistency. By calculating the\nEuclidean distance between each text node and other nodes within the same\ncluster, distant nodes are still selected as negative samples. Our proposed\nClusterText demonstrates good scalable computing, as it can effectively extract\nimportant information from from a large amount of data. Experimental results\ndemonstrate the superiority of ClusterText in text classification tasks.", "published": "2024-10-18 16:03:49", "link": "http://arxiv.org/abs/2410.18130v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model", "abstract": "The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.", "published": "2024-10-18 06:31:40", "link": "http://arxiv.org/abs/2410.14200v1", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning", "abstract": "Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.", "published": "2024-10-18 06:50:15", "link": "http://arxiv.org/abs/2410.14208v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REEF: Representation Encoding Fingerprints for Large Language Models", "abstract": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.", "published": "2024-10-18 08:27:02", "link": "http://arxiv.org/abs/2410.14273v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Generative AI, Pragmatics, and Authenticity in Second Language Learning", "abstract": "There are obvious benefits to integrating generative AI (artificial\nintelligence) into language learning and teaching. Those include using AI as a\nlanguage tutor, creating learning materials, or assessing learner output.\nHowever, due to how AI systems under-stand human language, based on a\nmathematical model using statistical probability, they lack the lived\nexperience to be able to use language with the same social aware-ness as\nhumans. Additionally, there are built-in linguistic and cultural biases based\non their training data which is mostly in English and predominantly from\nWestern sources. Those facts limit AI suitability for some language learning\ninteractions. Stud-ies have clearly shown that systems such as ChatGPT often do\nnot produce language that is pragmatically appropriate. The lack of linguistic\nand cultural authenticity has important implications for how AI is integrated\ninto second language acquisition as well as in instruction targeting\ndevelopment of intercultural communication compe-tence.", "published": "2024-10-18 11:58:03", "link": "http://arxiv.org/abs/2410.14395v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation", "abstract": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.", "published": "2024-10-18 12:39:32", "link": "http://arxiv.org/abs/2410.14425v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions", "abstract": "Large Language Models (LLMs) are widely used in Conversational AI systems to\ngenerate responses to user inquiries. However, many natural questions lack\nwell-defined answers. While existing studies primarily focus on question types\nsuch as false premises, they often overlook out-of-scope questions, where the\nprovided document is semantically highly similar to the query but does not\ncontain the required answer. In this paper, we propose a guided\nhallucination-based method to efficiently generate a diverse set of\nout-of-scope questions from a given document corpus. We then evaluate multiple\nLLMs based on their effectiveness in confusion detection and appropriate\nresponse generation. Furthermore, we introduce an improved method for detecting\nsuch out-of-scope questions, enhancing the reliability of LLM-based\nquestion-answering systems.", "published": "2024-10-18 16:11:29", "link": "http://arxiv.org/abs/2410.14567v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Overparameterized Text Encoders", "abstract": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.", "published": "2024-10-18 16:26:45", "link": "http://arxiv.org/abs/2410.14578v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection", "abstract": "Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.", "published": "2024-10-18 16:32:06", "link": "http://arxiv.org/abs/2410.14581v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CELI: Controller-Embedded Language Model Interactions", "abstract": "We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.", "published": "2024-10-18 17:29:56", "link": "http://arxiv.org/abs/2410.14627v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50, 68Q32, 68N19", "I.2.6; I.2.7; D.2.2"], "primary_category": "cs.SE"}
{"title": "DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph", "abstract": "Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.", "published": "2024-10-18 17:56:11", "link": "http://arxiv.org/abs/2410.14666v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Collaboratively adding new knowledge to an LLM", "abstract": "We address the question of how to successively add new knowledge to an LLM\nwhilst retaining previously-added knowledge. We consider two settings,\nsemi-cooperative and fully-cooperative. Overall, LoRA performs better in most\ncases than full-fine tuning of all parameters when both new knowledge\nacquisition and retention of old, including recent, knowledge are taken into\naccount. In the semi-cooperative setting, where datasets are not available\nafter training, MOE mixing, model merging, and LoRA-based orthogonal subspace\nsequential learning, using a small weight on the orthogonality term, perform\nwell. In the fully-cooperative setting where datasets remain available, joint\ntraining and sequential training with replay are both effective approaches with\nLoRA training generally preferable to full fine-tuning. The codes needed to\nreproduce the results are provided in an open source repository.", "published": "2024-10-18 04:04:51", "link": "http://arxiv.org/abs/2410.14753v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Controllable Discovery of Intents: Incremental Deep Clustering Using\n  Semi-Supervised Contrastive Learning", "abstract": "Deriving value from a conversational AI system depends on the capacity of a\nuser to translate the prior knowledge into a configuration. In most cases,\ndiscovering the set of relevant turn-level speaker intents is often one of the\nkey steps. Purely unsupervised algorithms provide a natural way to tackle\ndiscovery problems but make it difficult to incorporate constraints and only\noffer very limited control over the outcomes. Previous work has shown that\nsemi-supervised (deep) clustering techniques can allow the system to\nincorporate prior knowledge and constraints in the intent discovery process.\nHowever they did not address how to allow for control through human feedback.\nIn our Controllable Discovery of Intents (CDI) framework domain and prior\nknowledge are incorporated using a sequence of unsupervised contrastive\nlearning on unlabeled data followed by fine-tuning on partially labeled data,\nand finally iterative refinement of clustering and representations through\nrepeated clustering and pseudo-label fine-tuning. In addition, we draw from\ncontinual learning literature and use learning-without-forgetting to prevent\ncatastrophic forgetting across those training stages. Finally, we show how this\ndeep-clustering process can become part of an incremental discovery strategy\nwith human-in-the-loop. We report results on both CLINC and BANKING datasets.\nCDI outperforms previous works by a significant margin: 10.26% and 11.72%\nrespectively.", "published": "2024-10-18 07:24:02", "link": "http://arxiv.org/abs/2410.14755v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What's New in My Data? Novelty Exploration via Contrastive Generation", "abstract": "Fine-tuning is widely used to adapt language models for specific goals, often\nleveraging real-world data such as patient records, customer-service\ninteractions, or web content in languages not covered in pre-training. These\ndatasets are typically massive, noisy, and often confidential, making their\ndirect inspection challenging. However, understanding them is essential for\nguiding model deployment and informing decisions about data cleaning or\nsuppressing any harmful behaviors learned during fine-tuning. In this study, we\nintroduce the task of novelty discovery through generation, which aims to\nidentify novel properties of a fine-tuning dataset by generating examples that\nillustrate these properties. Our approach, Contrastive Generative Exploration\n(CGE), assumes no direct access to the data but instead relies on a pre-trained\nmodel and the same model after fine-tuning. By contrasting the predictions of\nthese two models, CGE can generate examples that highlight novel\ncharacteristics of the fine-tuning data. However, this simple approach may\nproduce examples that are too similar to one another, failing to capture the\nfull range of novel phenomena present in the dataset. We address this by\nintroducing an iterative version of CGE, where the previously generated\nexamples are used to update the pre-trained model, and this updated model is\nthen contrasted with the fully fine-tuned model to generate the next example,\npromoting diversity in the generated outputs. Our experiments demonstrate the\neffectiveness of CGE in detecting novel content, such as toxic language, as\nwell as new natural and programming languages. Furthermore, we show that CGE\nremains effective even when models are fine-tuned using differential privacy\ntechniques.", "published": "2024-10-18 15:24:05", "link": "http://arxiv.org/abs/2410.14765v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Complexity-Based Theory of Compositionality", "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it\nunderlies the structure of thought, language, and higher-level reasoning. In\nAI, compositional representations can enable a powerful form of\nout-of-distribution generalization, in which a model systematically adapts to\nnovel combinations of known concepts. However, while we have strong intuitions\nabout what compositionality is, there currently exists no formal definition for\nit that is measurable and mathematical. Here, we propose such a definition,\nwhich we call representational compositionality, that accounts for and extends\nour intuitions about compositionality. The definition is conceptually simple,\nquantitative, grounded in algorithmic information theory, and applicable to any\nrepresentation. Intuitively, representational compositionality states that a\ncompositional representation satisfies three properties. First, it must be\nexpressive. Second, it must be possible to re-describe the representation as a\nfunction of discrete symbolic sequences with re-combinable parts, analogous to\nsentences in natural language. Third, the function that relates these symbolic\nsequences to the representation, analogous to semantics in natural language,\nmust be simple. Through experiments on both synthetic and real world data, we\nvalidate our definition of compositionality and show how it unifies disparate\nintuitions from across the literature in both AI and cognitive science. We also\nshow that representational compositionality, while theoretically intractable,\ncan be readily estimated using standard deep learning tools. Our definition has\nthe potential to inspire the design of novel, theoretically-driven models that\nbetter capture the mechanisms of compositional thought.", "published": "2024-10-18 18:37:27", "link": "http://arxiv.org/abs/2410.14817v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPRIG: Improving Large Language Model Performance by System Prompt\n  Optimization", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many\nscenarios, but their performance depends, in part, on the choice of prompt.\nPast research has focused on optimizing prompts specific to a task. However,\nmuch less attention has been given to optimizing the general instructions\nincluded in a prompt, known as a system prompt. To address this gap, we propose\nSPRIG, an edit-based genetic algorithm that iteratively constructs prompts from\nprespecified components to maximize the model's performance in general\nscenarios. We evaluate the performance of system prompts on a collection of 47\ndifferent types of tasks to ensure generalizability. Our study finds that a\nsingle optimized system prompt performs on par with task prompts optimized for\neach individual task. Moreover, combining system and task-level optimizations\nleads to further improvement, which showcases their complementary nature.\nExperiments also reveal that the optimized system prompts generalize\neffectively across model families, parameter sizes, and languages. This study\nprovides insights into the role of system-level instructions in maximizing LLM\npotential.", "published": "2024-10-18 18:51:44", "link": "http://arxiv.org/abs/2410.14826v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment", "abstract": "In a prompt injection attack, an attacker injects a prompt into the original\none, aiming to make an LLM follow the injected prompt to perform an\nattacker-chosen task. Existing attacks primarily focus on how to blend the\ninjected prompt into the original prompt without altering the LLM itself. Our\nexperiments show that these attacks achieve some success, but there is still\nsignificant room for improvement. In this work, we show that an attacker can\nboost the success of prompt injection attacks by poisoning the LLM's alignment\nprocess. Specifically, we propose PoisonedAlign, a method to strategically\ncreate poisoned alignment samples. When even a small fraction of the alignment\ndata is poisoned using our method, the aligned LLM becomes more vulnerable to\nprompt injection while maintaining its foundational capabilities. The code is\navailable at https://github.com/Sadcardation/PoisonedAlign", "published": "2024-10-18 18:52:16", "link": "http://arxiv.org/abs/2410.14827v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "How to Evaluate Reward Models for RLHF", "abstract": "We introduce a new benchmark for reward models that quantifies their ability\nto produce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this process\nis prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks. These\nproxy tasks consist of a large-scale human preference and a verifiable\ncorrectness preference dataset, in which we measure 12 metrics across 12\ndomains. To investigate which reward model metrics are most correlated to\ngold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a\nlarge-scale crowdsourced human preference platform to view real reward model\ndownstream performance as ground truth. Ultimately, we compile our data and\nfindings into Preference Proxy Evaluations (PPE), the first reward model\nbenchmark explicitly linked to post-RLHF real-world human preference\nperformance, which we open-source for public use and further development. Our\ncode and evaluations can be found at https://github.com/lmarena/PPE .", "published": "2024-10-18 21:38:21", "link": "http://arxiv.org/abs/2410.14872v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Hybrid Defense Strategy for Boosting Adversarial Robustness in\n  Vision-Language Models", "abstract": "The robustness of Vision-Language Models (VLMs) such as CLIP is critical for\ntheir deployment in safety-critical applications like autonomous driving,\nhealthcare diagnostics, and security systems, where accurate interpretation of\nvisual and textual data is essential. However, these models are highly\nsusceptible to adversarial attacks, which can severely compromise their\nperformance and reliability in real-world scenarios. Previous methods have\nprimarily focused on improving robustness through adversarial training and\ngenerating adversarial examples using models like FGSM, AutoAttack, and\nDeepFool. However, these approaches often rely on strong assumptions, such as\nfixed perturbation norms or predefined attack patterns, and involve high\ncomputational complexity, making them challenging to implement in practical\nsettings. In this paper, we propose a novel adversarial training framework that\nintegrates multiple attack strategies and advanced machine learning techniques\nto significantly enhance the robustness of VLMs against a broad range of\nadversarial attacks. Experiments conducted on real-world datasets, including\nCIFAR-10 and CIFAR-100, demonstrate that the proposed method significantly\nenhances model robustness. The fine-tuned CLIP model achieved an accuracy of\n43.5% on adversarially perturbed images, compared to only 4% for the baseline\nmodel. The neural network model achieved a high accuracy of 98% in these\nchallenging classification tasks, while the XGBoost model reached a success\nrate of 85.26% in prediction tasks.", "published": "2024-10-18 23:47:46", "link": "http://arxiv.org/abs/2410.14911v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search\n  and Best-of-N Sampling", "abstract": "Inference-time alignment enhances the performance of large language models\nwithout requiring additional training or fine-tuning but presents challenges\ndue to balancing computational efficiency with high-quality output. Best-of-N\n(BoN) sampling, as a simple yet powerful approach, generates multiple responses\nand selects the best one, achieving improved performance but with a high\ncomputational cost. We propose TreeBoN, a novel framework that integrates a\nspeculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN\nmaintains a set of parent nodes, iteratively branching and pruning low-quality\nresponses, thereby reducing computational overhead while maintaining high\noutput quality. Our approach also leverages token-level rewards from Direct\nPreference Optimization (DPO) to guide tree expansion and prune low-quality\npaths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and\nTutorEval datasets, demonstrating consistent improvements. Specifically,\nTreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win\nrates across other different datasets, outperforming standard BoN with the same\ncomputational cost and showcasing its scalability and alignment efficacy.", "published": "2024-10-18 04:38:21", "link": "http://arxiv.org/abs/2410.16033v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Feint and Attack: Attention-Based Strategies for Jailbreaking and\n  Protecting LLMs", "abstract": "Jailbreak attack can be used to access the vulnerabilities of Large Language\nModels (LLMs) by inducing LLMs to generate the harmful content. And the most\ncommon method of the attack is to construct semantically ambiguous prompts to\nconfuse and mislead the LLMs. To access the security and reveal the intrinsic\nrelation between the input prompt and the output for LLMs, the distribution of\nattention weight is introduced to analyze the underlying reasons. By using\nstatistical analysis methods, some novel metrics are defined to better describe\nthe distribution of attention weight, such as the Attention Intensity on\nSensitive Words (Attn_SensWords), the Attention-based Contextual Dependency\nScore (Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy). By\nleveraging the distinct characteristics of these metrics, the beam search\nalgorithm and inspired by the military strategy \"Feint and Attack\", an\neffective jailbreak attack strategy named as Attention-Based Attack (ABA) is\nproposed. In the ABA, nested attack prompts are employed to divert the\nattention distribution of the LLMs. In this manner, more harmless parts of the\ninput can be used to attract the attention of the LLMs. In addition, motivated\nby ABA, an effective defense strategy called as Attention-Based Defense (ABD)\nis also put forward. Compared with ABA, the ABD can be used to enhance the\nrobustness of LLMs by calibrating the attention distribution of the input\nprompt. Some comparative experiments have been given to demonstrate the\neffectiveness of ABA and ABD. Therefore, both ABA and ABD can be used to access\nthe security of the LLMs. The comparative experiment results also give a\nlogical explanation that the distribution of attention weight can bring great\ninfluence on the output for LLMs.", "published": "2024-10-18 17:02:13", "link": "http://arxiv.org/abs/2410.16327v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Step Guided Reasoning: Improving Mathematical Reasoning using Guidance\n  Generation and Step Reasoning", "abstract": "Mathematical reasoning has been challenging for large language models (LLMs).\nHowever, the introduction of step-by-step Chain-of-Thought (CoT) inference has\nsignificantly advanced the mathematical capabilities of LLMs. Despite this\nprogress, current approaches either necessitate extensive inference datasets\nfor training or depend on few-shot methods that frequently compromise\ncomputational accuracy. To address these bottlenecks in mathematical reasoning,\nwe propose a novel method called Step Guidied Reasoning, which is more stable\nand generalizable than few-shot methods and does not involve further\nfine-tuning of the model. In this approach, LLMs reflect on small reasoning\nsteps, similar to how humans deliberate and focus attention on what to do next.\nBy incorporating this reflective process into the inference stage, LLMs can\neffectively guide their reasoning from one step to the next. Through extensive\nexperiments, we demonstrate the significant effect of Step Guidied Reasoning in\naugmenting mathematical performance in state-of-the-art language models.\nQwen2-72B-Instruct outperforms its math-specific counterpart,\nQwen2.5-72B-Math-Instruct, on MMLU- STEM with a score of 90.9%, compared to\n87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase\nfrom 27.1% to 36.3% and from 36.5% to 47.4% on the mathematics domain,\nrespectively.", "published": "2024-10-18 01:38:24", "link": "http://arxiv.org/abs/2410.19817v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "KeyInst: Keyword Instruction for Improving SQL Formulation in\n  Text-to-SQL", "abstract": "Text-to-SQL parsing involves the translation of natural language queries\n(NLQs) into their corresponding SQL commands. A principal challenge within this\ndomain is the formulation of SQL queries that are not only syntactically\ncorrect but also semantically aligned with the natural language input. However,\nthe intrinsic disparity between the NLQ and the SQL poses a significant\nchallenge. In this research, we introduce Keyword Instruction (KeyInst), a\nnovel method designed to enhance SQL formulation by Large Language Models\n(LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to\nbe part of the final query, thus facilitates a smoother SQL query formulation\nprocess. We explore two strategies for integrating KeyInst into Text-to-SQL\nparsing: a pipeline strategy and a single-pass strategy. The former first\ngenerates KeyInst for question, which are then used to prompt LLMs. The latter\nemploys a fine-tuned model to concurrently generate KeyInst and SQL in one\nstep. We developed StrucQL, a benchmark specifically designed for the\nevaluation of SQL formulation. Extensive experiments on StrucQL and other\nbenchmarks demonstrate that KeyInst significantly improves upon the existing\nText-to-SQL prompting techniques.", "published": "2024-10-18 02:45:36", "link": "http://arxiv.org/abs/2411.00788v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative\n  Mining", "abstract": "Ranking consistently emerges as a primary focus in information retrieval\nresearch. Retrieval and ranking models serve as the foundation for numerous\napplications, including web search, open domain QA, enterprise domain QA, and\ntext-based recommender systems. Typically, these models undergo training on\ntriplets consisting of binary relevance assignments, comprising one positive\nand one negative passage. However, their utilization involves a context where a\nsignificantly more nuanced understanding of relevance is necessary, especially\nwhen re-ranking a large pool of potentially relevant passages. Although\ncollecting positive examples through user feedback like impressions or clicks\nis straightforward, identifying suitable negative pairs from a vast pool of\npossibly millions or even billions of documents possess a greater challenge.\nGenerating a substantial number of negative pairs is often necessary to\nmaintain the high quality of the model. Several approaches have been suggested\nin literature to tackle the issue of selecting suitable negative pairs from an\nextensive corpus. This study focuses on explaining the crucial role of hard\nnegatives in the training process of cross-encoder models, specifically aiming\nto explain the performance gains observed with hard negative sampling compared\nto random sampling. We have developed a robust hard negative mining technique\nfor efficient training of cross-encoder re-rank models on an enterprise dataset\nwhich has domain specific context. We provide a novel perspective to enhance\nretrieval models, ultimately influencing the performance of advanced LLM\nsystems like Retrieval-Augmented Generation (RAG) and Reasoning and Action\nAgents (ReAct). The proposed approach demonstrates that learning both\nsimilarity and dissimilarity simultaneously with cross-encoders improves\nperformance of retrieval systems.", "published": "2024-10-18 05:23:39", "link": "http://arxiv.org/abs/2411.02404v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker", "abstract": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health.", "published": "2024-10-18 15:22:07", "link": "http://arxiv.org/abs/2411.02408v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts", "abstract": "Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.", "published": "2024-10-18 16:20:22", "link": "http://arxiv.org/abs/2410.14574v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Unified Framework for Collecting Text-to-Speech Synthesis Datasets for\n  22 Indian Languages", "abstract": "The performance of a text-to-speech (TTS) synthesis model depends on various\nfactors, of which the quality of the training data is of utmost importance.\nMillions of data are collected around the globe for various languages, but\nresources for Indian languages are few. Although there are many efforts\ninvolved in data collection, a common set of protocols for data collection\nbecomes necessary for building TTS systems in Indian languages primarily\nbecause of the need for a uniform development of TTS systems across languages.\nIn this paper, we present our learnings on data collection efforts' for Indic\nlanguages over 15 years. These databases have been used in unit selection\nsynthesis, hidden Markov model based, and end-to-end frameworks, and for\ngenerating prosodically rich TTS systems. The most significant feature of the\ndata collected is that data purity enables building high-quality TTS systems\nwith a comparatively small dataset compared to that of European/Chinese\nlanguages.", "published": "2024-10-18 06:19:27", "link": "http://arxiv.org/abs/2410.14197v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AC-Mix: Self-Supervised Adaptation for Low-Resource Automatic Speech\n  Recognition using Agnostic Contrastive Mixup", "abstract": "Self-supervised learning (SSL) leverages large amounts of unlabelled data to\nlearn rich speech representations, fostering improvements in automatic speech\nrecognition (ASR), even when only a small amount of labelled data is available\nfor fine-tuning. Despite the advances in SSL, a significant challenge remains\nwhen the data used for pre-training (source domain) mismatches the fine-tuning\ndata (target domain). To tackle this domain mismatch challenge, we propose a\nnew domain adaptation method for low-resource ASR focused on contrastive mixup\nfor joint-embedding architectures named AC-Mix (agnostic contrastive mixup). In\nthis approach, the SSL model is adapted through additional pre-training using\nmixed data views created by interpolating samples from the source and the\ntarget domains. Our proposed adaptation method consistently outperforms the\nbaseline system, using approximately 11 hours of adaptation data and requiring\nonly 1 hour of adaptation time on a single GPU with WavLM-Large.", "published": "2024-10-18 23:44:55", "link": "http://arxiv.org/abs/2410.14910v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Source Spatial Knowledge Understanding for Immersive Visual\n  Text-to-Speech", "abstract": "Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize reverberant speech for the spoken content. Previous works\nfocus on the RGB modality for global environmental modeling, overlooking the\npotential of multi-source spatial knowledge like depth, speaker position, and\nenvironmental semantics. To address these issues, we propose a novel\nmulti-source spatial knowledge understanding scheme for immersive VTTS, termed\nMS2KU-VTTS. Specifically, we first prioritize RGB image as the dominant source\nand consider depth image, speaker position knowledge from object detection, and\nGemini-generated semantic captions as supplementary sources. Afterwards, we\npropose a serial interaction mechanism to effectively integrate both dominant\nand supplementary sources. The resulting multi-source knowledge is dynamically\nintegrated based on the respective contributions of each source.This enriched\ninteraction and integration of multi-source spatial knowledge guides the speech\ngeneration model, enhancing the immersive speech experience. Experimental\nresults demonstrate that the MS$^2$KU-VTTS surpasses existing baselines in\ngenerating immersive speech. Demos and code are available at:\nhttps://github.com/AI-S2-Lab/MS2KU-VTTS.", "published": "2024-10-18 00:46:18", "link": "http://arxiv.org/abs/2410.14101v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SNAC: Multi-Scale Neural Audio Codec", "abstract": "Neural audio codecs have recently gained popularity because they can\nrepresent audio signals with high fidelity at very low bitrates, making it\nfeasible to use language modeling approaches for audio generation and\nunderstanding. Residual Vector Quantization (RVQ) has become the standard\ntechnique for neural audio compression using a cascade of VQ codebooks. This\npaper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ\nwhere the quantizers can operate at different temporal resolutions. By applying\na hierarchy of quantizers at variable frame rates, the codec adapts to the\naudio structure across multiple timescales. This leads to more efficient\ncompression, as demonstrated by extensive objective and subjective evaluations.\nThe code and model weights are open-sourced at\nhttps://github.com/hubertsiuzdak/snac.", "published": "2024-10-18 12:24:05", "link": "http://arxiv.org/abs/2410.14411v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Embodied Exploration of Latent Spaces and Explainable AI", "abstract": "In this paper, we explore how performers' embodied interactions with a Neural\nAudio Synthesis model allow the exploration of the latent space of such a\nmodel, mediated through movements sensed by e-textiles. We provide background\nand context for the performance, highlighting the potential of embodied\npractices to contribute to developing explainable AI systems. By integrating\nvarious artistic domains with explainable AI principles, our interdisciplinary\nexploration contributes to the discourse on art, embodiment, and AI, offering\ninsights into intuitive approaches found through bodily expression.", "published": "2024-10-18 16:40:34", "link": "http://arxiv.org/abs/2410.14590v1", "categories": ["cs.SD", "cs.CY", "cs.HC", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Towards Robust Transcription: Exploring Noise Injection Strategies for\n  Training Data Augmentation", "abstract": "Recent advancements in Automatic Piano Transcription (APT) have significantly\nimproved system performance, but the impact of noisy environments on the system\nperformance remains largely unexplored. This study investigates the impact of\nwhite noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art\nAPT models and evaluates the performance of the Onsets and Frames model when\ntrained on noise-augmented data. We hope this research provides valuable\ninsights as preliminary work toward developing transcription models that\nmaintain consistent performance across a range of acoustic conditions.", "published": "2024-10-18 02:31:36", "link": "http://arxiv.org/abs/2410.14122v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
