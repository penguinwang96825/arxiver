{"title": "Dynamic Factor Allocation Leveraging Regime-Switching Signals", "abstract": "This article explores dynamic factor allocation by analyzing the cyclical\nperformance of factors through regime analysis. The authors focus on a U.S.\nequity investment universe comprising seven long-only indices representing the\nmarket and six style factors: value, size, momentum, quality, low volatility,\nand growth. Their approach integrates factor-specific regime inferences of each\nfactor index's active performance relative to the market into the\nBlack-Litterman model to construct a fully-invested, long-only multi-factor\nportfolio. First, the authors apply the sparse jump model (SJM) to identify\nbull and bear market regimes for individual factors, using a feature set based\non risk and return measures from historical factor active returns, as well as\nvariables reflecting the broader market environment. The regimes identified by\nthe SJM exhibit enhanced stability and interpretability compared to traditional\nmethods. A hypothetical single-factor long-short strategy is then used to\nassess these regime inferences and fine-tune hyperparameters, resulting in a\npositive Sharpe ratio of this strategy across all factors with low correlation\namong them. These regime inferences are then incorporated into the\nBlack-Litterman framework to dynamically adjust allocations among the seven\nindices, with an equally weighted (EW) portfolio serving as the benchmark.\nEmpirical results show that the constructed multi-factor portfolio\nsignificantly improves the information ratio (IR) relative to the market,\nraising it from just 0.05 for the EW benchmark to approximately 0.4. When\nmeasured relative to the EW benchmark itself, the dynamic allocation achieves\nan IR of around 0.4 to 0.5. The strategy also enhances absolute portfolio\nperformance across key metrics such as the Sharpe ratio and maximum drawdown.", "published": "2024-10-18 19:42:01", "link": "http://arxiv.org/abs/2410.14841v1", "categories": ["q-fin.PM", "q-fin.CP", "q-fin.ST"], "primary_category": "q-fin.PM"}
{"title": "Simultaneously Solving FBSDEs with Neural Operators of Logarithmic Depth, Constant Width, and Sub-Linear Rank", "abstract": "Forward-backwards stochastic differential equations (FBSDEs) are central in\noptimal control, game theory, economics, and mathematical finance.\nUnfortunately, the available FBSDE solvers operate on \\textit{individual}\nFBSDEs, meaning that they cannot provide a computationally feasible strategy\nfor solving large families of FBSDEs as these solvers must be re-run several\ntimes. \\textit{Neural operators} (NOs) offer an alternative approach for\n\\textit{simultaneously solving} large families of FBSDEs by directly\napproximating the solution operator mapping \\textit{inputs:} terminal\nconditions and dynamics of the backwards process to \\textit{outputs:} solutions\nto the associated FBSDE. Though universal approximation theorems (UATs)\nguarantee the existence of such NOs, these NOs are unrealistically large. We\nconfirm that ``small'' NOs can uniformly approximate the solution operator to\nstructured families of FBSDEs with random terminal time, uniformly on suitable\ncompact sets determined by Sobolev norms, to any prescribed error\n$\\varepsilon>0$ using a depth of $\\mathcal{O}(\\log(1/\\varepsilon))$, a width of\n$\\mathcal{O}(1)$, and a sub-linear rank; i.e. $\\mathcal{O}(1/\\varepsilon^r)$\nfor some $r<1$. This result is rooted in our second main contribution, which\nshows that convolutional NOs of similar depth, width, and rank can approximate\nthe solution operator to a broad class of Elliptic PDEs. A key insight here is\nthat the convolutional layers of our NO can efficiently encode the Green's\nfunction associated to the Elliptic PDEs linked to our FBSDEs. A byproduct of\nour analysis is the first theoretical justification for the benefit of lifting\nchannels in NOs: they exponentially decelerate the growth rate of the NO's\nrank.", "published": "2024-10-18 18:01:40", "link": "http://arxiv.org/abs/2410.14788v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "q-fin.CP"], "primary_category": "math.OC"}
{"title": "Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets", "abstract": "Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.", "published": "2024-10-18 16:37:52", "link": "http://arxiv.org/abs/2410.14587v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Reinforcement Learning in Non-Markov Market-Making", "abstract": "We develop a deep reinforcement learning (RL) framework for an optimal\nmarket-making (MM) trading problem, specifically focusing on price processes\nwith semi-Markov and Hawkes Jump-Diffusion dynamics. We begin by discussing the\nbasics of RL and the deep RL framework used, where we deployed the\nstate-of-the-art Soft Actor-Critic (SAC) algorithm for the deep learning part.\nThe SAC algorithm is an off-policy entropy maximization algorithm more suitable\nfor tackling complex, high-dimensional problems with continuous state and\naction spaces like in optimal market-making (MM). We introduce the optimal MM\nproblem considered, where we detail all the deterministic and stochastic\nprocesses that go into setting up an environment for simulating this strategy.\nHere we also give an in-depth overview of the jump-diffusion pricing dynamics\nused, our method for dealing with adverse selection within the limit order\nbook, and we highlight the working parts of our optimization problem. Next, we\ndiscuss training and testing results, where we give visuals of how important\ndeterministic and stochastic processes such as the bid/ask, trade executions,\ninventory, and the reward function evolved. We include a discussion on the\nlimitations of these results, which are important points to note for most\ndiffusion models in this setting.", "published": "2024-10-18 14:35:26", "link": "http://arxiv.org/abs/2410.14504v2", "categories": ["q-fin.CP", "q-fin.MF"], "primary_category": "q-fin.CP"}
