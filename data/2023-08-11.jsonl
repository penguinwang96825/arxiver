{"title": "PIPPA: A Partially Synthetic Conversational Dataset", "abstract": "With the emergence of increasingly powerful large language models, there is a\nburgeoning interest in leveraging these models for casual conversation and\nrole-play applications. However, existing conversational and role-playing\ndatasets often fail to capture the diverse and nuanced interactions typically\nexhibited by real-world role-play participants. To address this limitation and\ncontribute to the rapidly growing field, we introduce a partially-synthetic\ndataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA\nis a result of a community-driven crowdsourcing effort involving a group of\nrole-play enthusiasts. The dataset comprises over 1 million utterances that are\ndistributed across 26,000 conversation sessions and provides a rich resource\nfor researchers and AI developers to explore and refine conversational AI\nsystems in the context of role-play scenarios.", "published": "2023-08-11 00:33:26", "link": "http://arxiv.org/abs/2308.05884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Case Study on Context Encoding in Multi-Encoder based Document-Level\n  Neural Machine Translation", "abstract": "Recent studies have shown that the multi-encoder models are agnostic to the\nchoice of context, and the context encoder generates noise which helps improve\nthe models in terms of BLEU score. In this paper, we further explore this idea\nby evaluating with context-aware pronoun translation test set by training\nmulti-encoder models trained on three different context settings viz, previous\ntwo sentences, random two sentences, and a mix of both as context.\nSpecifically, we evaluate the models on the ContraPro test set to study how\ndifferent contexts affect pronoun translation accuracy. The results show that\nthe model can perform well on the ContraPro test set even when the context is\nrandom. We also analyze the source representations to study whether the context\nencoder generates noise. Our analysis shows that the context encoder provides\nsufficient information to learn discourse-level information. Additionally, we\nobserve that mixing the selected context (the previous two sentences in this\ncase) and the random context is generally better than the other settings.", "published": "2023-08-11 10:35:53", "link": "http://arxiv.org/abs/2308.06063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fly-Swat or Cannon? Cost-Effective Language Model Choice via\n  Meta-Modeling", "abstract": "Generative language models (LMs) have become omnipresent across data science.\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\nfor an LM, from whose output the solution can then be extracted. LM performance\nhas consistently been increasing with model size - but so has the monetary cost\nof querying the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory solution,\nwhereas for others smaller LMs suffice. Based on this fact, we design a\nframework for cost-effective language model choice, called \"Fly-swat or cannon\"\n(FORC). Given a set of inputs and a set of candidate LMs, FORC judiciously\nassigns each input to an LM predicted to do well on the input according to a\nso-called meta-model, aiming to achieve high overall performance at low cost.\nThe cost-performance tradeoff can be flexibly tuned by the user. Options\ninclude, among others, maximizing total expected performance (or the number of\nprocessed inputs) while staying within a given cost budget, or minimizing total\ncost while processing all inputs. We evaluate FORC on 14 datasets covering five\nnatural language tasks, using four candidate LMs of vastly different size and\ncost. With FORC, we match the performance of the largest available LM while\nachieving a cost reduction of 63%. Via our publicly available library,\nresearchers as well as practitioners can thus save large amounts of money\nwithout sacrificing performance.", "published": "2023-08-11 11:29:51", "link": "http://arxiv.org/abs/2308.06077v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task Conditioned BERT for Joint Intent Detection and Slot-filling", "abstract": "Dialogue systems need to deal with the unpredictability of user intents to\ntrack dialogue state and the heterogeneity of slots to understand user\npreferences. In this paper we investigate the hypothesis that solving these\nchallenges as one unified model will allow the transfer of parameter support\ndata across the different tasks. The proposed principled model is based on a\nTransformer encoder, trained on multiple tasks, and leveraged by a rich input\nthat conditions the model on the target inferences. Conditioning the\nTransformer encoder on multiple target inferences over the same corpus, i.e.,\nintent and multiple slot types, allows learning richer language interactions\nthan a single-task model would be able to. In fact, experimental results\ndemonstrate that conditioning the model on an increasing number of dialogue\ninference tasks leads to improved results: on the MultiWOZ dataset, the joint\nintent and slot detection can be improved by 3.2\\% by conditioning on intent,\n10.8\\% by conditioning on slot and 14.4\\% by conditioning on both intent and\nslots. Moreover, on real conversations with Farfetch costumers, the proposed\nconditioned BERT can achieve high joint-goal and intent detection performance\nthroughout a dialogue.", "published": "2023-08-11 14:47:27", "link": "http://arxiv.org/abs/2308.06165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Text Classification on Free Text Comments in\n  Patient-Reported Outcome Measures", "abstract": "Free text comments (FTC) in patient-reported outcome measures (PROMs) data\nare typically analysed using manual methods, such as content analysis, which is\nlabour-intensive and time-consuming. Machine learning analysis methods are\nlargely unsupervised, necessitating post-analysis interpretation. Weakly\nsupervised text classification (WSTC) can be a valuable method of analysis to\nclassify domain-specific text data in which there is limited labelled data. In\nthis paper, we apply five WSTC techniques to FTC in PROMs data to identify\nhealth-related quality of life (HRQoL) themes reported by colorectal cancer\npatients. The WSTC methods label all the themes mentioned in the FTC. The\nresults showed moderate performance on the PROMs data, mainly due to the\nprecision of the models, and variation between themes. Evaluation of the\nclassification performance illustrated the potential and limitations of keyword\nbased WSTC to label PROMs FTC when labelled data is limited.", "published": "2023-08-11 15:47:49", "link": "http://arxiv.org/abs/2308.06199v1", "categories": ["cs.CL", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning\n  to boost Foundation Modals", "abstract": "Reasoning ability is one of the most crucial capabilities of a foundation\nmodel, signifying its capacity to address complex reasoning tasks.\nChain-of-Thought (CoT) technique is widely regarded as one of the effective\nmethods for enhancing the reasoning ability of foundation models and has\ngarnered significant attention. However, the reasoning process of CoT is\nlinear, step-by-step, similar to personal logical reasoning, suitable for\nsolving general and slightly complicated problems. On the contrary, the\nthinking pattern of an expert owns two prominent characteristics that cannot be\nhandled appropriately in CoT, i.e., high-order multi-hop reasoning and\nmultimodal comparative judgement. Therefore, the core motivation of this paper\nis transcending CoT to construct a reasoning paradigm that can think like an\nexpert. The hyperedge of a hypergraph could connect various vertices, making it\nnaturally suitable for modelling high-order relationships. Inspired by this,\nthis paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT)\nreasoning paradigm, which enables the foundation models to possess the\nexpert-level ability of high-order multi-hop reasoning and multimodal\ncomparative judgement. Specifically, a textual hypergraph-of-thought is\nconstructed utilizing triple as the primary thought to model higher-order\nrelationships, and a hyperedge-of-thought is generated through multi-hop\nwalking paths to achieve multi-hop inference. Furthermore, we devise a visual\nhypergraph-of-thought to interact with the textual hypergraph-of-thought via\nCross-modal Co-Attention Graph Learning for multimodal comparative\nverification. Experimentations on the ScienceQA benchmark demonstrate the\nproposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on par\nwith CoT-based GPT4 with a lower model size.", "published": "2023-08-11 16:13:04", "link": "http://arxiv.org/abs/2308.06207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KETM:A Knowledge-Enhanced Text Matching method", "abstract": "Text matching is the task of matching two texts and determining the\nrelationship between them, which has extensive applications in natural language\nprocessing tasks such as reading comprehension, and Question-Answering systems.\nThe mainstream approach is to compute text representations or to interact with\nthe text through attention mechanism, which is effective in text matching\ntasks. However, the performance of these models is insufficient for texts that\nrequire commonsense knowledge-based reasoning. To this end, in this paper, We\nintroduce a new model for text matching called the Knowledge Enhanced Text\nMatching model (KETM), to enrich contextual representations with real-world\ncommon-sense knowledge from external knowledge sources to enhance our model\nunderstanding and reasoning. First, we use Wiktionary to retrieve the text word\ndefinitions as our external knowledge. Secondly, we feed text and knowledge to\nthe text matching module to extract their feature vectors. The text matching\nmodule is used as an interaction module by integrating the encoder layer, the\nco-attention layer, and the aggregation layer. Specifically, the interaction\nprocess is iterated several times to obtain in-depth interaction information\nand extract the feature vectors of text and knowledge by multi-angle pooling.\nThen, we fuse text and knowledge using a gating mechanism to learn the ratio of\ntext and knowledge fusion by a neural network that prevents noise generated by\nknowledge. After that, experimental validation on four datasets are carried\nout, and the experimental results show that our proposed model performs well on\nall four datasets, and the performance of our method is improved compared to\nthe base model without adding external knowledge, which validates the\neffectiveness of our proposed method. The code is available at\nhttps://github.com/1094701018/KETM", "published": "2023-08-11 17:08:14", "link": "http://arxiv.org/abs/2308.06235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Alignment with Instruction Backtranslation", "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.", "published": "2023-08-11 17:47:54", "link": "http://arxiv.org/abs/2308.06259v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tweet Sentiment Extraction using Viterbi Algorithm with Transfer\n  Learning", "abstract": "Tweet sentiment extraction extracts the most significant portion of the\nsentence, determining whether the sentiment is positive or negative. This\nresearch aims to identify the part of tweet sentences that strikes any emotion.\nTo reach this objective, we continue improving the Viterbi algorithm previously\nmodified by the author to make it able to receive pre-trained model parameters.\nWe introduce the confidence score and vector as two indicators responsible for\nevaluating the model internally before assessing the final results. We then\npresent a method to fine-tune this nonparametric model. We found that the model\ngets highly explainable as the confidence score vector reveals precisely where\nthe least confidence predicted states are and if the modifications approved\nameliorate the confidence score or if the tuning is going in the wrong\ndirection.", "published": "2023-08-11 07:16:49", "link": "http://arxiv.org/abs/2308.05973v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Optimizing transformer-based machine translation model for single GPU\n  training: a hyperparameter ablation study", "abstract": "In machine translation tasks, the relationship between model complexity and\nperformance is often presumed to be linear, driving an increase in the number\nof parameters and consequent demands for computational resources like multiple\nGPUs. To explore this assumption, this study systematically investigates the\neffects of hyperparameters through ablation on a sequence-to-sequence machine\ntranslation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to\nexpectations, our experiments reveal that combinations with the most parameters\nwere not necessarily the most effective. This unexpected insight prompted a\ncareful reduction in parameter sizes, uncovering \"sweet spots\" that enable\ntraining sophisticated models on a single GPU without compromising translation\nquality. The findings demonstrate an intricate relationship between\nhyperparameter selection, model size, and computational resource needs. The\ninsights from this study contribute to the ongoing efforts to make machine\ntranslation more accessible and cost-effective, emphasizing the importance of\nprecise hyperparameter tuning over mere scaling.", "published": "2023-08-11 08:47:52", "link": "http://arxiv.org/abs/2308.06017v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Cryptocurrency Securities Cases: Can a GPT\n  Model Meaningfully Assist Lawyers?", "abstract": "Large Language Models (LLMs) could be a useful tool for lawyers. However,\nempirical research on their effectiveness in conducting legal tasks is scant.\nWe study securities cases involving cryptocurrencies as one of numerous\ncontexts where AI could support the legal process, studying GPT-3.5's legal\nreasoning and ChatGPT's legal drafting capabilities. We examine whether a)\nGPT-3.5 can accurately determine which laws are potentially being violated from\na fact pattern, and b) whether there is a difference in juror decision-making\nbased on complaints written by a lawyer compared to ChatGPT. We feed fact\npatterns from real-life cases to GPT-3.5 and evaluate its ability to determine\ncorrect potential violations from the scenario and exclude spurious violations.\nSecond, we had mock jurors assess complaints written by ChatGPT and lawyers.\nGPT-3.5's legal reasoning skills proved weak, though we expect improvement in\nfuture models, particularly given the violations it suggested tended to be\ncorrect (it merely missed additional, correct violations). ChatGPT performed\nbetter at legal drafting, and jurors' decisions were not statistically\nsignificantly associated with the author of the document upon which they based\ntheir decisions. Because GPT-3.5 cannot satisfactorily conduct legal reasoning\ntasks, it would be unlikely to be able to help lawyers in a meaningful way at\nthis stage. However, ChatGPT's drafting skills (though, perhaps, still inferior\nto lawyers) could assist lawyers in providing legal services. Our research is\nthe first to systematically study an LLM's legal drafting and reasoning\ncapabilities in litigation, as well as in securities law and\ncryptocurrency-related misconduct.", "published": "2023-08-11 09:23:11", "link": "http://arxiv.org/abs/2308.06032v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multimodality and Attention Increase Alignment in Natural Language\n  Prediction Between Humans and Computational Models", "abstract": "The potential of multimodal generative artificial intelligence (mAI) to\nreplicate human grounded language understanding, including the pragmatic,\ncontext-rich aspects of communication, remains to be clarified. Humans are\nknown to use salient multimodal features, such as visual cues, to facilitate\nthe processing of upcoming words. Correspondingly, multimodal computational\nmodels can integrate visual and linguistic data using a visual attention\nmechanism to assign next-word probabilities. To test whether these processes\nalign, we tasked both human participants (N = 200) as well as several\nstate-of-the-art computational models with evaluating the predictability of\nforthcoming words after viewing short audio-only or audio-visual clips with\nspeech. During the task, the model's attention weights were recorded and human\nattention was indexed via eye tracking. Results show that predictability\nestimates from humans aligned more closely with scores generated from\nmultimodal models vs. their unimodal counterparts. Furthermore, including an\nattention mechanism doubled alignment with human judgments when visual and\nlinguistic context facilitated predictions. In these cases, the model's\nattention patches and human eye tracking significantly overlapped. Our results\nindicate that improved modeling of naturalistic language processing in mAI does\nnot merely depend on training diet but can be driven by multimodality in\ncombination with attention-based architectures. Humans and computational models\nalike can leverage the predictive constraints of multimodal information by\nattending to relevant features in the input.", "published": "2023-08-11 09:30:07", "link": "http://arxiv.org/abs/2308.06035v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning to Guide Human Experts via Personalized Large Language Models", "abstract": "In learning to defer, a predictor identifies risky decisions and defers them\nto a human expert. One key issue with this setup is that the expert may end up\nover-relying on the machine's decisions, due to anchoring bias. At the same\ntime, whenever the machine chooses the deferral option the expert has to take\ndecisions entirely unassisted. As a remedy, we propose learning to guide (LTG),\nan alternative framework in which -- rather than suggesting ready-made\ndecisions -- the machine provides guidance useful to guide decision-making, and\nthe human is entirely responsible for coming up with a decision. We also\nintroduce SLOG, an LTG implementation that leverages (a small amount of) human\nsupervision to convert a generic large language model into a module capable of\ngenerating textual guidance, and present preliminary but promising results on a\nmedical diagnosis task.", "published": "2023-08-11 09:36:33", "link": "http://arxiv.org/abs/2308.06039v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving Zero-Shot Text Matching for Financial Auditing with Large\n  Language Models", "abstract": "Auditing financial documents is a very tedious and time-consuming process. As\nof today, it can already be simplified by employing AI-based solutions to\nrecommend relevant text passages from a report for each legal requirement of\nrigorous accounting standards. However, these methods need to be fine-tuned\nregularly, and they require abundant annotated data, which is often lacking in\nindustrial environments. Hence, we present ZeroShotALI, a novel recommender\nsystem that leverages a state-of-the-art large language model (LLM) in\nconjunction with a domain-specifically optimized transformer-based\ntext-matching solution. We find that a two-step approach of first retrieving a\nnumber of best matching document sections per legal requirement with a custom\nBERT-based model and second filtering these selections using an LLM yields\nsignificant performance improvements over existing approaches.", "published": "2023-08-11 12:55:09", "link": "http://arxiv.org/abs/2308.06111v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Guest Nationality Composition from Hotel Reviews", "abstract": "Many hotels target guest acquisition efforts to specific markets in order to\nbest anticipate individual preferences and needs of their guests. Likewise,\nsuch strategic positioning is a prerequisite for efficient marketing budget\nallocation. Official statistics report on the number of visitors from different\ncountries, but no fine-grained information on the guest composition of\nindividual businesses exists. There is, however, growing interest in such data\nfrom competitors, suppliers, researchers and the general public. We demonstrate\nhow machine learning can be leveraged to extract references to guest\nnationalities from unstructured text reviews in order to dynamically assess and\nmonitor the dynamics of guest composition of individual businesses. In\nparticular, we show that a rather simple architecture of pre-trained embeddings\nand stacked LSTM layers provides a better performance-runtime tradeoff than\nmore complex state-of-the-art language models.", "published": "2023-08-11 15:04:34", "link": "http://arxiv.org/abs/2308.06175v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Large Language Model Enhanced Conversational Recommender System", "abstract": "Conversational recommender systems (CRSs) aim to recommend high-quality items\nto users through a dialogue interface. It usually contains multiple sub-tasks,\nsuch as user preference elicitation, recommendation, explanation, and item\ninformation search. To develop effective CRSs, there are some challenges: 1)\nhow to properly manage sub-tasks; 2) how to effectively solve different\nsub-tasks; and 3) how to correctly generate responses that interact with users.\nRecently, Large Language Models (LLMs) have exhibited an unprecedented ability\nto reason and generate, presenting a new opportunity to develop more powerful\nCRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to\naddress the above challenges. For sub-task management, we leverage the\nreasoning ability of LLM to effectively manage sub-task. For sub-task solving,\nwe collaborate LLM with expert models of different sub-tasks to achieve the\nenhanced performance. For response generation, we utilize the generation\nability of LLM as a language interface to better interact with users.\nSpecifically, LLMCRS divides the workflow into four stages: sub-task detection,\nmodel matching, sub-task execution, and response generation. LLMCRS also\ndesigns schema-based instruction, demonstration-based instruction, dynamic\nsub-task and model matching, and summary-based generation to instruct LLM to\ngenerate desired results in the workflow. Finally, to adapt LLM to\nconversational recommendations, we also propose to fine-tune LLM with\nreinforcement learning from CRSs performance feedback, referred to as RLPF.\nExperimental results on benchmark datasets show that LLMCRS with RLPF\noutperforms the existing methods.", "published": "2023-08-11 16:30:44", "link": "http://arxiv.org/abs/2308.06212v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large Language Models to Identify Social Determinants of Health in\n  Electronic Health Records", "abstract": "Social determinants of health (SDoH) have an important impact on patient\noutcomes but are incompletely collected from the electronic health records\n(EHR). This study researched the ability of large language models to extract\nSDoH from free text in EHRs, where they are most commonly documented, and\nexplored the role of synthetic clinical text for improving the extraction of\nthese scarcely documented, yet extremely valuable, clinical data. 800 patient\nnotes were annotated for SDoH categories, and several transformer-based models\nwere evaluated. The study also experimented with synthetic data generation and\nassessed for algorithmic bias. Our best-performing models were fine-tuned\nFlan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The\nbenefit of augmenting fine-tuning with synthetic data varied across model\narchitecture and size, with smaller Flan-T5 models (base and large) showing the\ngreatest improvements in performance (delta F1 +0.12 to +0.23). Model\nperformance was similar on the in-hospital system dataset but worse on the\nMIMIC-III dataset. Our best-performing fine-tuned models outperformed zero- and\nfew-shot performance of ChatGPT-family models for both tasks. These fine-tuned\nmodels were less likely than ChatGPT to change their prediction when\nrace/ethnicity and gender descriptors were added to the text, suggesting less\nalgorithmic bias (p<0.05). At the patient-level, our models identified 93.8% of\npatients with adverse SDoH, while ICD-10 codes captured 2.0%. Our method can\neffectively extracted SDoH information from clinic notes, performing better\ncompare to GPT zero- and few-shot settings. These models could enhance\nreal-world evidence on SDoH and aid in identifying patients needing social\nsupport.", "published": "2023-08-11 19:18:35", "link": "http://arxiv.org/abs/2308.06354v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models and Knowledge Graphs: Opportunities and Challenges", "abstract": "Large Language Models (LLMs) have taken Knowledge Representation -- and the\nworld -- by storm. This inflection point marks a shift from explicit knowledge\nrepresentation to a renewed focus on the hybrid representation of both explicit\nknowledge and parametric knowledge. In this position paper, we will discuss\nsome of the common debate points within the community on LLMs (parametric\nknowledge) and Knowledge Graphs (explicit knowledge) and speculate on\nopportunities and visions that the renewed focus brings, as well as related\nresearch topics and challenges.", "published": "2023-08-11 20:16:57", "link": "http://arxiv.org/abs/2308.06374v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF", "abstract": "In this work, we address the problem of directing the text generation of a\nlanguage model (LM) towards a desired behavior, aligning the generated text\nwith the preferences of the human operator. We propose using another,\ninstruction-tuned language model as a critic reward model in a zero-shot way\nthanks to the prompt of a Yes-No question that represents the user preferences,\nwithout requiring further labeled data. This zero-shot reward model provides\nthe learning signal to further fine-tune the base LM using Reinforcement\nLearning from AI Feedback (RLAIF); yet our approach is also compatible in other\ncontexts such as quality-diversity search. Extensive evidence of the\ncapabilities of the proposed ZYN framework is provided through experiments in\ndifferent domains related to text generation, including detoxification;\noptimizing sentiment of movie reviews, or any other attribute; steering the\nopinion about a particular topic the model may have; and personalizing prompt\ngenerators for text-to-image tasks. Code available at\n\\url{https://github.com/vicgalle/zero-shot-reward-models/}.", "published": "2023-08-11 20:59:31", "link": "http://arxiv.org/abs/2308.06385v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Planning with a LLM", "abstract": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.", "published": "2023-08-11 21:17:13", "link": "http://arxiv.org/abs/2308.06391v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "LittleMu: Deploying an Online Virtual Teaching Assistant via\n  Heterogeneous Sources Integration and Chain of Teach Prompts", "abstract": "Teaching assistants have played essential roles in the long history of\neducation. However, few MOOC platforms are providing human or virtual teaching\nassistants to support learning for massive online students due to the\ncomplexity of real-world online education scenarios and the lack of training\ndata. In this paper, we present a virtual MOOC teaching assistant, LittleMu\nwith minimum labeled training data, to provide question answering and chit-chat\nservices. Consisting of two interactive modules of heterogeneous retrieval and\nlanguage model prompting, LittleMu first integrates structural, semi- and\nunstructured knowledge sources to support accurate answers for a wide range of\nquestions. Then, we design delicate demonstrations named \"Chain of Teach\"\nprompts to exploit the large-scale pre-trained model to handle complex\nuncollected questions. Except for question answering, we develop other\neducational services such as knowledge-grounded chit-chat. We test the system's\nperformance via both offline evaluation and online deployment. Since May 2020,\nour LittleMu system has served over 80,000 users with over 300,000 queries from\nover 500 courses on XuetangX MOOC platform, which continuously contributes to a\nmore convenient and fair education. Our code, services, and dataset will be\navailable at https://github.com/THU-KEG/VTA.", "published": "2023-08-11 04:36:26", "link": "http://arxiv.org/abs/2308.05935v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural Conversation Models and How to Rein Them in: A Survey of Failures\n  and Fixes", "abstract": "Recent conditional language models are able to continue any kind of text\nsource in an often seemingly fluent way. This fact encouraged research in the\narea of open-domain conversational systems that are based on powerful language\nmodels and aim to imitate an interlocutor by generating appropriate\ncontributions to a written dialogue. From a linguistic perspective, however,\nthe complexity of contributing to a conversation is high. In this survey, we\ninterpret Grice's maxims of cooperative conversation from the perspective of\nthis specific research area and systematize the literature under the aspect of\nwhat makes a contribution appropriate: A neural conversation model has to be\nfluent, informative, consistent, coherent, and follow social norms. In order to\nensure these qualities, recent approaches try to tame the underlying language\nmodels at various intervention points, such as data, training regime or\ndecoding. Sorted by these categories and intervention points, we discuss\npromising attempts and suggest novel ways for future research.", "published": "2023-08-11 12:07:45", "link": "http://arxiv.org/abs/2308.06095v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lip2Vec: Efficient and Robust Visual Speech Recognition via\n  Latent-to-Latent Visual to Audio Representation Mapping", "abstract": "Visual Speech Recognition (VSR) differs from the common perception tasks as\nit requires deeper reasoning over the video sequence, even by human experts.\nDespite the recent advances in VSR, current approaches rely on labeled data to\nfully train or finetune their models predicting the target speech. This hinders\ntheir ability to generalize well beyond the training set and leads to\nperformance degeneration under out-of-distribution challenging scenarios.\nUnlike previous works that involve auxiliary losses or complex training\nprocedures and architectures, we propose a simple approach, named Lip2Vec that\nis based on learning a prior model. Given a robust visual speech encoder, this\nnetwork maps the encoded latent representations of the lip sequence to their\ncorresponding latents from the audio pair, which are sufficiently invariant for\neffective text decoding. The generated audio representation is then decoded to\ntext using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed\nmodel compares favorably with fully-supervised learning methods on the LRS3\ndataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable\nperformance on the VoxCeleb test set. We believe that reprogramming the VSR as\nan ASR task narrows the performance gap between the two and paves the way for\nmore flexible formulations of lip reading.", "published": "2023-08-11 12:59:02", "link": "http://arxiv.org/abs/2308.06112v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Joint Speech-Text Representations Without Alignment", "abstract": "The last year has seen astonishing progress in text-prompted image generation\npremised on the idea of a cross-modal representation space in which the text\nand image domains are represented jointly. In ASR, this idea has found\napplication as joint speech-text encoders that can scale to the capacities of\nvery large parameter models by being trained on both unpaired speech and text.\nWhile these methods show promise, they have required special treatment of the\nsequence-length mismatch inherent in speech and text, either by up-sampling\nheuristics or an explicit alignment model. In this work, we offer evidence that\njoint speech-text encoders naturally achieve consistent representations across\nmodalities by disregarding sequence length, and argue that consistency losses\ncould forgive length differences and simply assume the best alignment. We show\nthat such a loss improves downstream WER in both a large-parameter monolingual\nand multilingual system.", "published": "2023-08-11 13:28:48", "link": "http://arxiv.org/abs/2308.06125v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Identification of the Relevance of Comments in Codes Using Bag of Words\n  and Transformer Based Models", "abstract": "The Forum for Information Retrieval (FIRE) started a shared task this year\nfor classification of comments of different code segments. This is binary text\nclassification task where the objective is to identify whether comments given\nfor certain code segments are relevant or not. The BioNLP-IISERB group at the\nIndian Institute of Science Education and Research Bhopal (IISERB) participated\nin this task and submitted five runs for five different models. The paper\npresents the overview of the models and other significant findings on the\ntraining corpus. The methods involve different feature engineering schemes and\ntext classification techniques. The performance of the classical bag of words\nmodel and transformer-based models were explored to identify significant\nfeatures from the given training corpus. We have explored different classifiers\nviz., random forest, support vector machine and logistic regression using the\nbag of words model. Furthermore, the pre-trained transformer based models like\nBERT, RoBERT and ALBERT were also used by fine-tuning them on the given\ntraining corpus. The performance of different such models over the training\ncorpus were reported and the best five models were implemented on the given\ntest corpus. The empirical results show that the bag of words model outperforms\nthe transformer based models, however, the performance of our runs are not\nreasonably well in both training and test corpus. This paper also addresses the\nlimitations of the models and scope for further improvement.", "published": "2023-08-11 14:06:41", "link": "http://arxiv.org/abs/2308.06144v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual\n  Loss", "abstract": "We introduce a bilingual solution to support English as secondary locale for\nmost primary locales in hybrid automatic speech recognition (ASR) settings. Our\nkey developments constitute: (a) pronunciation lexicon with grapheme units\ninstead of phone units, (b) a fully bilingual alignment model and subsequently\nbilingual streaming transformer model, (c) a parallel encoder structure with\nlanguage identification (LID) loss, (d) parallel encoder with an auxiliary loss\nfor monolingual projections. We conclude that in comparison to LID loss, our\nproposed auxiliary loss is superior in specializing the parallel encoders to\nrespective monolingual locales, and that contributes to stronger bilingual\nlearning. We evaluate our work on large-scale training and test tasks for\nbilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual\nmodels demonstrate strong English code-mixing capability. In particular, the\nbilingual IT model improves the word error rate (WER) for a code-mix IT task\nfrom 46.5% to 13.8%, while also achieving a close parity (9.6%) with the\nmonolingual IT model (9.5%) over IT tests.", "published": "2023-08-11 18:06:33", "link": "http://arxiv.org/abs/2308.06327v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic", "abstract": "We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.", "published": "2023-08-11 13:15:35", "link": "http://arxiv.org/abs/2308.07336v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Evaluating Picture Description Speech for Dementia Detection using\n  Image-text Alignment", "abstract": "Using picture description speech for dementia detection has been studied for\n30 years. Despite the long history, previous models focus on identifying the\ndifferences in speech patterns between healthy subjects and patients with\ndementia but do not utilize the picture information directly. In this paper, we\npropose the first dementia detection models that take both the picture and the\ndescription texts as inputs and incorporate knowledge from large pre-trained\nimage-text alignment models. We observe the difference between dementia and\nhealthy samples in terms of the text's relevance to the picture and the focused\narea of the picture. We thus consider such a difference could be used to\nenhance dementia detection accuracy. Specifically, we use the text's relevance\nto the picture to rank and filter the sentences of the samples. We also\nidentified focused areas of the picture as topics and categorized the sentences\naccording to the focused areas. We propose three advanced models that\npre-processed the samples based on their relevance to the picture, sub-image,\nand focused areas. The evaluation results show that our advanced models, with\nknowledge of the picture and large image-text alignment models, achieve\nstate-of-the-art performance with the best detection accuracy at 83.44%, which\nis higher than the text-only baseline model at 79.91%. Lastly, we visualize the\nsample and picture results to explain the advantages of our models.", "published": "2023-08-11 08:42:37", "link": "http://arxiv.org/abs/2308.07933v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Scale Learning on Overlapped Speech Detection: New Benchmark and\n  New General System", "abstract": "Overlapped Speech Detection (OSD) is an important part of speech applications\ninvolving analysis of multi-party conversations. However, most of existing OSD\nsystems are trained and evaluated on small datasets with limited application\ndomains, which led to the robustness of them lacks benchmark for evaluation and\nthe accuracy of them remains inadequate in realistic acoustic environments. To\nsolve these problem, we conduct a study of large-scale learning (LSL) in OSD\ntasks and propose a new general OSD system named CF-OSD with LSL based on\nConformer network and LSL. In our study, a large-scale test set consisting of\n151h labeled speech of different styles, languages and sound-source distances\nis produced and used as a new benchmark for evaluating the generality of OSD\nsystems. Rigorous comparative experiments are designed and used to evaluate the\neffectiveness of LSL in OSD tasks and define the OSD model of our general OSD\nsystem. The experiment results show that LSL can significantly improve the\naccuracy and robustness of OSD systems, and the CF-OSD with LSL system\nsignificantly outperforms other OSD systems on our proposed benchmark.\nMoreover, our system has also achieved state-of-the-art performance on existing\nsmall dataset benchmarks, reaching 81.6\\% and 53.8\\% in the Alimeeting testset\nand DIHARD II evaluation set, respectively.", "published": "2023-08-11 07:50:41", "link": "http://arxiv.org/abs/2308.05987v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion", "abstract": "Voice conversion (VC) aims at altering a person's voice to make it sound\nsimilar to the voice of another person while preserving linguistic content.\nExisting methods suffer from a dilemma between content intelligibility and\nspeaker similarity; i.e., methods with higher intelligibility usually have a\nlower speaker similarity, while methods with higher speaker similarity usually\nrequire plenty of target speaker voice data to achieve high intelligibility. In\nthis work, we propose a novel method \\textit{Phoneme Hallucinator} that\nachieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model;\nit adopts a novel model to hallucinate diversified and high-fidelity target\nspeaker phonemes based just on a short target speaker voice (e.g. 3 seconds).\nThe hallucinated phonemes are then exploited to perform neighbor-based voice\nconversion. Our model is a text-free, any-to-any VC model that requires no text\nannotations and supports conversion to any unseen speaker. Objective and\nsubjective evaluations show that \\textit{Phoneme Hallucinator} outperforms\nexisting VC methods for both intelligibility and speaker similarity.", "published": "2023-08-11 20:44:19", "link": "http://arxiv.org/abs/2308.06382v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio is all in one: speech-driven gesture synthetics using WavLM\n  pre-trained model", "abstract": "The generation of co-speech gestures for digital humans is an emerging area\nin the field of virtual human creation. Prior research has made progress by\nusing acoustic and semantic information as input and adopting classify method\nto identify the person's ID and emotion for driving co-speech gesture\ngeneration. However, this endeavour still faces significant challenges. These\nchallenges go beyond the intricate interplay between co-speech gestures, speech\nacoustic, and semantics; they also encompass the complexities associated with\npersonality, emotion, and other obscure but important factors. This paper\nintroduces \"diffmotion-v2,\" a speech-conditional diffusion-based and\nnon-autoregressive transformer-based generative model with WavLM pre-trained\nmodel. It can produce individual and stylized full-body co-speech gestures only\nusing raw speech audio, eliminating the need for complex multimodal processing\nand manually annotated. Firstly, considering that speech audio not only\ncontains acoustic and semantic features but also conveys personality traits,\nemotions, and more subtle information related to accompanying gestures, we\npioneer the adaptation of WavLM, a large-scale pre-trained model, to extract\nlow-level and high-level audio information. Secondly, we introduce an adaptive\nlayer norm architecture in the transformer-based layer to learn the\nrelationship between speech information and accompanying gestures. Extensive\nsubjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT\ndatasets to confirm the WavLM and the model's ability to synthesize natural\nco-speech gestures with various styles.", "published": "2023-08-11 08:03:28", "link": "http://arxiv.org/abs/2308.05995v3", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
