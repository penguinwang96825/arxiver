{"title": "Large Scale Distributed Acoustic Modeling With Back-off N-grams", "abstract": "The paper revives an older approach to acoustic modeling that borrows from\nn-gram language modeling in an attempt to scale up both the amount of training\ndata and model size (as measured by the number of parameters in the model), to\napproximately 100 times larger than current sizes used in automatic speech\nrecognition. In such a data-rich setting, we can expand the phonetic context\nsignificantly beyond triphones, as well as increase the number of Gaussian\nmixture components for the context-dependent states that allow it. We have\nexperimented with contexts that span seven or more context-independent phones,\nand up to 620 mixture components per state. Dealing with unseen phonetic\ncontexts is accomplished using the familiar back-off technique used in language\nmodeling due to implementation simplicity. The back-off acoustic model is\nestimated, stored and served using MapReduce distributed computing\ninfrastructure.\n  Speech recognition experiments are carried out in an N-best list rescoring\nframework for Google Voice Search. Training big models on large amounts of data\nproves to be an effective way to increase the accuracy of a state-of-the-art\nautomatic speech recognition system. We use 87,000 hours of training data\n(speech along with transcription) obtained by filtering utterances in Voice\nSearch logs on automatic speech recognition confidence. Models ranging in size\nbetween 20--40 million Gaussians are estimated using maximum likelihood\ntraining. They achieve relative reductions in word-error-rate of 11% and 6%\nwhen combined with first-pass models trained using maximum likelihood, and\nboosted maximum mutual information, respectively. Increasing the context size\nbeyond five phones (quinphones) does not help.", "published": "2013-02-05 17:09:49", "link": "http://arxiv.org/abs/1302.1123v1", "categories": ["cs.CL", "68T10", "I.2.7"], "primary_category": "cs.CL"}
