{"title": "Transfer learning of language-independent end-to-end ASR with language\n  model fusion", "abstract": "This work explores better adaptation methods to low-resource languages using\nan external language model (LM) under the framework of transfer learning. We\nfirst build a language-independent ASR system in a unified sequence-to-sequence\n(S2S) architecture with a shared vocabulary among all languages. During\nadaptation, we perform LM fusion transfer, where an external LM is integrated\ninto the decoder network of the attention-based S2S model in the whole\nadaptation stage, to effectively incorporate linguistic context of the target\nlanguage. We also investigate various seed models for transfer learning.\nExperimental evaluations using the IARPA BABEL data set show that LM fusion\ntransfer improves performances on all target five languages compared with\nsimple transfer learning when the external text data is available. Our final\nsystem drastically reduces the performance gap from the hybrid systems.", "published": "2018-11-06 02:46:23", "link": "http://arxiv.org/abs/1811.02134v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIS at TAC Cold Start 2015: Neural Networks and Coreference Resolution\n  for Slot Filling", "abstract": "This paper describes the CIS slot filling system for the TAC Cold Start\nevaluations 2015. It extends and improves the system we have built for the\nevaluation last year. This paper mainly describes the changes to our last\nyear's system. Especially, it focuses on the coreference and classification\ncomponent. For coreference, we have performed several analysis and prepared a\nresource to simplify our end-to-end system and improve its runtime. For\nclassification, we propose to use neural networks. We have trained\nconvolutional and recurrent neural networks and combined them with traditional\nevaluation methods, namely patterns and support vector machines. Our runs for\nthe 2015 evaluation have been designed to directly assess the effect of each\nnetwork on the end-to-end performance of the system. The CIS system achieved\nrank 3 of all slot filling systems participating in the task.", "published": "2018-11-06 08:57:53", "link": "http://arxiv.org/abs/1811.02230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Off-the-Shelf Unsupervised NMT", "abstract": "We frame unsupervised machine translation (MT) in the context of multi-task\nlearning (MTL), combining insights from both directions. We leverage\noff-the-shelf neural MT architectures to train unsupervised MT models with no\nparallel data and show that such models can achieve reasonably good\nperformance, competitive with models purpose-built for unsupervised MT.\nFinally, we propose improvements that allow us to apply our models to\nEnglish-Turkish, a truly low-resource language pair.", "published": "2018-11-06 10:50:04", "link": "http://arxiv.org/abs/1811.02278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Embed Sentences Using Attentive Recursive Trees", "abstract": "Sentence embedding is an effective feature representation for most deep\nlearning-based NLP tasks. One prevailing line of methods is using recursive\nlatent tree-structured networks to embed sentences with task-specific\nstructures. However, existing models have no explicit mechanism to emphasize\ntask-informative words in the tree structure. To this end, we propose an\nAttentive Recursive Tree model (AR-Tree), where the words are dynamically\nlocated according to their importance in the task. Specifically, we construct\nthe latent tree for a sentence in a proposed important-first strategy, and\nplace more attentive words nearer to the root; thus, AR-Tree can inherently\nemphasize important words during the bottom-up composition of the sentence\nembedding. We propose an end-to-end reinforced training strategy for AR-Tree,\nwhich is demonstrated to consistently outperform, or be at least comparable to,\nthe state-of-the-art sentence embedding methods on three sentence understanding\ntasks.", "published": "2018-11-06 13:12:22", "link": "http://arxiv.org/abs/1811.02338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-switching Sentence Generation by Generative Adversarial Networks\n  and its Application to Data Augmentation", "abstract": "Code-switching is about dealing with alternative languages in speech or text.\nIt is partially speaker-depend and domain-related, so completely explaining the\nphenomenon by linguistic rules is challenging. Compared to most monolingual\ntasks, insufficient data is an issue for code-switching. To mitigate the issue\nwithout expensive human annotation, we proposed an unsupervised method for\ncode-switching data augmentation. By utilizing a generative adversarial\nnetwork, we can generate intra-sentential code-switching sentences from\nmonolingual sentences. We applied proposed method on two corpora, and the\nresult shows that the generated code-switching sentences improve the\nperformance of code-switching language models.", "published": "2018-11-06 14:07:15", "link": "http://arxiv.org/abs/1811.02356v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Subword Segmentation for Text Comprehension", "abstract": "Representation learning is the foundation of machine reading comprehension\nand inference. In state-of-the-art models, character-level representations have\nbeen broadly adopted to alleviate the problem of effectively representing rare\nor complex words. However, character itself is not a natural minimal linguistic\nunit for representation or word embedding composing due to ignoring the\nlinguistic coherence of consecutive characters inside word. This paper presents\na general subword-augmented embedding framework for learning and composing\ncomputationally-derived subword-level representations. We survey a series of\nunsupervised segmentation methods for subword acquisition and different\nsubword-augmented strategies for text understanding, showing that\nsubword-augmented embedding significantly improves our baselines in various\ntypes of text understanding tasks on both English and Chinese benchmarks.", "published": "2018-11-06 14:24:37", "link": "http://arxiv.org/abs/1811.02364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepChannel: Salience Estimation by Contrastive Learning for Extractive\n  Document Summarization", "abstract": "We propose DeepChannel, a robust, data-efficient, and interpretable neural\nmodel for extractive document summarization. Given any document-summary pair,\nwe estimate a salience score, which is modeled using an attention-based deep\nneural network, to represent the salience degree of the summary for yielding\nthe document. We devise a contrastive training strategy to learn the salience\nestimation network, and then use the learned salience score as a guide and\niteratively extract the most salient sentences from the document as our\ngenerated summary. In experiments, our model not only achieves state-of-the-art\nROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the\nout-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1\nF-1 score of 39.41 on CNN/Daily Mail test set with merely $1 / 100$ training\nset, demonstrating a tremendous data efficiency.", "published": "2018-11-06 15:06:44", "link": "http://arxiv.org/abs/1811.02394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WordNet-feelings: A linguistic categorisation of human feelings", "abstract": "In this article, we present the first in depth linguistic study of human\nfeelings. While there has been substantial research on incorporating some\naffective categories into linguistic analysis (e.g. sentiment, and to a lesser\nextent, emotion), the more diverse category of human feelings has thus far not\nbeen investigated. We surveyed the extensive interdisciplinary literature\naround feelings to construct a working definition of what constitutes a feeling\nand propose 9 broad categories of feeling. We identified potential feeling\nwords based on their pointwise mutual information with morphological variants\nof the word `feel' in the Google n-gram corpus, and present a manual annotation\nexercise where 317 WordNet senses of one hundred of these words were\ncategorised as `not a feeling' or as one of the 9 proposed categories of\nfeeling. We then proceeded to annotate 11386 WordNet senses of all these words\nto create WordNet-feelings, a new affective dataset that identifies 3664 word\nsenses as feelings, and associates each of these with one of the 9 categories\nof feeling. WordNet-feelings can be used in conjunction with other datasets\nsuch as SentiWordNet that annotate word senses with complementary affective\nproperties such as valence and intensity.", "published": "2018-11-06 15:42:17", "link": "http://arxiv.org/abs/1811.02435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UAlacant machine translation quality estimation at WMT 2018: a simple\n  approach using phrase tables and feed-forward neural networks", "abstract": "We describe the Universitat d'Alacant submissions to the word- and\nsentence-level machine translation (MT) quality estimation (QE) shared task at\nWMT 2018. Our approach to word-level MT QE builds on previous work to mark the\nwords in the machine-translated sentence as \\textit{OK} or \\textit{BAD}, and is\nextended to determine if a word or sequence of words need to be inserted in the\ngap after each word. Our sentence-level submission simply uses the edit\noperations predicted by the word-level approach to approximate TER. The method\npresented ranked first in the sub-task of identifying insertions in gaps for\nthree out of the six datasets, and second in the rest of them.", "published": "2018-11-06 17:25:21", "link": "http://arxiv.org/abs/1811.02510v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Fast Neural Chinese Word Segmentation for Long Sentences", "abstract": "Rapidly developed neural models have achieved competitive performance in\nChinese word segmentation (CWS) as their traditional counterparts. However,\nmost of methods encounter the computational inefficiency especially for long\nsentences because of the increasing model complexity and slower decoders. This\npaper presents a simple neural segmenter which directly labels the gap\nexistence between adjacent characters to alleviate the existing drawback. Our\nsegmenter is fully end-to-end and capable of performing segmentation very fast.\nWe also show a performance difference with different tag sets. The experiments\nshow that our segmenter can provide comparable performance with\nstate-of-the-art.", "published": "2018-11-06 19:19:49", "link": "http://arxiv.org/abs/1811.02602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Ability of LSTMs to Learn Context-Free Grammars", "abstract": "While long short-term memory (LSTM) neural net architectures are designed to\ncapture sequence information, human language is generally composed of\nhierarchical structures. This raises the question as to whether LSTMs can learn\nhierarchical structures. We explore this question with a well-formed bracket\nprediction task using two types of brackets modeled by an LSTM. Demonstrating\nthat such a system is learnable by an LSTM is the first step in demonstrating\nthat the entire class of CFLs is also learnable. We observe that the model\nrequires exponential memory in terms of the number of characters and embedded\ndepth, where a sub-linear memory should suffice. Still, the model does more\nthan memorize the training input. It learns how to distinguish between relevant\nand irrelevant information. On the other hand, we also observe that the model\ndoes not generalize well. We conclude that LSTMs do not learn the relevant\nunderlying context-free rules, suggesting the good overall performance is\nattained rather by an efficient way of evaluating nuisance variables. LSTMs are\na way to quickly reach good results for many natural language tasks, but to\nunderstand and generate natural language one has to investigate other concepts\nthat can make more direct use of natural language's structural nature.", "published": "2018-11-06 20:07:47", "link": "http://arxiv.org/abs/1811.02611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Corpora for Single-Channel Speech Separation Across Multiple\n  Domains", "abstract": "To date, the bulk of research on single-channel speech separation has been\nconducted using clean, near-field, read speech, which is not representative of\nmany modern applications. In this work, we develop a procedure for constructing\nhigh-quality synthetic overlap datasets, necessary for most deep learning-based\nseparation frameworks. We produced datasets that are more representative of\nrealistic applications using the CHiME-5 and Mixer 6 corpora and evaluate\nstandard methods on this data to demonstrate the shortcomings of current\nsource-separation performance. We also demonstrate the value of a wide variety\nof data in training robust models that generalize well to multiple conditions.", "published": "2018-11-06 20:59:54", "link": "http://arxiv.org/abs/1811.02641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised\n  Neural Relation Extraction", "abstract": "Pattern-based labeling methods have achieved promising results in alleviating\nthe inevitable labeling noises of distantly supervised neural relation\nextraction. However, these methods require significant expert labor to write\nrelation-specific patterns, which makes them too sophisticated to generalize\nquickly.To ease the labor-intensive workload of pattern writing and enable the\nquick generalization to new relation types, we propose a neural pattern\ndiagnosis framework, DIAG-NRE, that can automatically summarize and refine\nhigh-quality relational patterns from noise data with human experts in the\nloop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two\nreal-world datasets and present both significant and interpretable improvements\nover state-of-the-art methods.", "published": "2018-11-06 05:08:59", "link": "http://arxiv.org/abs/1811.02166v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Skipping Networks for Entity Alignment", "abstract": "We consider the problem of learning knowledge graph (KG) embeddings for\nentity alignment (EA). Current methods use the embedding models mainly focusing\non triple-level learning, which lacks the ability of capturing long-term\ndependencies existing in KGs. Consequently, the embedding-based EA methods\nheavily rely on the amount of prior (known) alignment, due to the identity\ninformation in the prior alignment cannot be efficiently propagated from one KG\nto another. In this paper, we propose RSN4EA (recurrent skipping networks for\nEA), which leverages biased random walk sampling for generating long paths\nacross KGs and models the paths with a novel recurrent skipping network (RSN).\nRSN integrates the conventional recurrent neural network (RNN) with residual\nlearning and can largely improve the convergence speed and performance with\nonly a few more parameters. We evaluated RSN4EA on a series of datasets\nconstructed from real-world KGs. Our experimental results showed that it\noutperformed a number of state-of-the-art embedding-based EA methods and also\nachieved comparable performance for KG completion.", "published": "2018-11-06 12:28:58", "link": "http://arxiv.org/abs/1811.02318v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Neural Network Architecture In Keyword Spotting", "abstract": "Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.", "published": "2018-11-06 12:32:27", "link": "http://arxiv.org/abs/1811.02320v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Term \"Blurring\" and Stochastic \"Barcoding\" for Improved\n  Unsupervised Text Classification", "abstract": "The abundance of text data being produced in the modern age makes it\nincreasingly important to intuitively group, categorize, or classify text data\nby theme for efficient retrieval and search. Yet, the high dimensionality and\nimprecision of text data, or more generally language as a whole, prove to be\nchallenging when attempting to perform unsupervised document clustering. In\nthis thesis, we present two novel methods for improving unsupervised document\nclustering/classification by theme. The first is to improve document\nrepresentations. We look to exploit \"term neighborhoods\" and \"blur\" semantic\nweight across neighboring terms. These neighborhoods are located in the\nsemantic space afforded by \"word embeddings.\" The second method is for cluster\nrevision, based on what we deem as \"stochastic barcoding\", or \"S- Barcode\"\npatterns. Text data is inherently high dimensional, yet clustering typically\ntakes place in a low dimensional representation space. Our method utilizes\nlower dimension clustering results as initial cluster configurations, and\niteratively revises the configuration in the high dimensional space. We show\nwith experimental results how both of the two methods improve the quality of\ndocument clustering. While this thesis elaborates on the two new conceptual\ncontributions, a joint thesis by David Yan details the feature transformation\nand software architecture we developed for unsupervised document\nclassification.", "published": "2018-11-06 16:08:31", "link": "http://arxiv.org/abs/1811.02456v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative training of RNNLMs with the average word error criterion", "abstract": "In automatic speech recognition (ASR), recurrent neural language models\n(RNNLM) are typically used to refine hypotheses in the form of lattices or\nn-best lists, which are generated by a beam search decoder with a weaker\nlanguage model. The RNNLMs are usually trained generatively using the\nperplexity (PPL) criterion on large corpora of grammatically correct text.\nHowever, the hypotheses are noisy, and the RNNLM doesn't always make the\nchoices that minimise the metric we optimise for, the word error rate (WER). To\naddress this mismatch we propose to use a task specific loss to train an RNNLM\nto discriminate between multiple hypotheses within lattice rescoring scenario.\nBy fine-tuning the RNNLM on lattices with the average edit distance loss, we\nshow that we obtain a 1.9% relative improvement in word error rate over a\npurely generatively trained model.", "published": "2018-11-06 17:53:54", "link": "http://arxiv.org/abs/1811.02528v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language GANs Falling Short", "abstract": "Generating high-quality text with sufficient diversity is essential for a\nwide range of Natural Language Generation (NLG) tasks. Maximum-Likelihood (MLE)\nmodels trained with teacher forcing have consistently been reported as weak\nbaselines, where poor performance is attributed to exposure bias (Bengio et\nal., 2015; Ranzato et al., 2015); at inference time, the model is fed its own\nprediction instead of a ground-truth token, which can lead to accumulating\nerrors and poor samples. This line of reasoning has led to an outbreak of\nadversarial based approaches for NLG, on the account that GANs do not suffer\nfrom exposure bias. In this work, we make several surprising observations which\ncontradict common beliefs. First, we revisit the canonical evaluation framework\nfor NLG, and point out fundamental flaws with quality-only evaluation: we show\nthat one can outperform such metrics using a simple, well-known temperature\nparameter to artificially reduce the entropy of the model's conditional\ndistributions. Second, we leverage the control over the quality / diversity\ntrade-off given by this parameter to evaluate models over the whole\nquality-diversity spectrum and find MLE models constantly outperform the\nproposed GAN variants over the whole quality-diversity space. Our results have\nseveral implications: 1) The impact of exposure bias on sample quality is less\nsevere than previously thought, 2) temperature tuning provides a better quality\n/ diversity trade-off than adversarial training while being easier to train,\neasier to cross-validate, and less computationally expensive. Code to reproduce\nthe experiments is available at github.com/pclucas14/GansFallingShort", "published": "2018-11-06 18:44:11", "link": "http://arxiv.org/abs/1811.02549v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust and fine-grained prosody control of end-to-end speech synthesis", "abstract": "We propose prosody embeddings for emotional and expressive speech synthesis\nnetworks. The proposed methods introduce temporal structures in the embedding\nnetworks, thus enabling fine-grained control of the speaking style of the\nsynthesized speech. The temporal structures can be designed either on the\nspeech side or the text side, leading to different control resolutions in time.\nThe prosody embedding networks are plugged into end-to-end speech synthesis\nnetworks and trained without any other supervision except for the target speech\nfor synthesizing. It is demonstrated that the prosody embedding networks\nlearned to extract prosodic features. By adjusting the learned prosody\nfeatures, we could change the pitch and amplitude of the synthesized speech\nboth at the frame level and the phoneme level. We also introduce the temporal\nnormalization of prosody embeddings, which shows better robustness against\nspeaker perturbations during prosody transfer tasks.", "published": "2018-11-06 01:54:22", "link": "http://arxiv.org/abs/1811.02122v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural Phrase-to-Phrase Machine Translation", "abstract": "In this paper, we propose Neural Phrase-to-Phrase Machine Translation\n(NP$^2$MT). Our model uses a phrase attention mechanism to discover relevant\ninput (source) segments that are used by a decoder to generate output (target)\nphrases. We also design an efficient dynamic programming algorithm to decode\nsegments that allows the model to be trained faster than the existing neural\nphrase-based machine translation method by Huang et al. (2018). Furthermore,\nour method can naturally integrate with external phrase dictionaries during\ndecoding. Empirical experiments show that our method achieves comparable\nperformance with the state-of-the art methods on benchmark datasets. However,\nwhen the training and testing data are from different distributions or domains,\nour method performs better.", "published": "2018-11-06 05:47:52", "link": "http://arxiv.org/abs/1811.02172v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for\n  Speech Recognition", "abstract": "Many speech enhancement methods try to learn the relationship between noisy\nand clean speech, obtained using an acoustic room simulator. We point out\nseveral limitations of enhancement methods relying on clean speech targets; the\ngoal of this work is proposing an alternative learning algorithm, called\nacoustic and adversarial supervision (AAS). AAS makes the enhanced output both\nmaximizing the likelihood of transcription on the pre-trained acoustic model\nand having general characteristics of clean speech, which improve\ngeneralization on unseen noisy speeches. We employ the connectionist temporal\nclassification and the unpaired conditional boundary equilibrium generative\nadversarial network as the loss function of AAS. AAS is tested on two datasets\nincluding additive noise without and with reverberation, Librispeech + DEMAND\nand CHiME-4. By visualizing the enhanced speech with different loss\ncombinations, we demonstrate the role of each supervision. AAS achieves a lower\nword error rate than other state-of-the-art methods using the clean speech\ntarget in both datasets.", "published": "2018-11-06 06:23:57", "link": "http://arxiv.org/abs/1811.02182v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement\n  in Multi-Talker Environments", "abstract": "In this paper, we address the problem of enhancing the speech of a speaker of\ninterest in a cocktail party scenario when visual information of the speaker of\ninterest is available. Contrary to most previous studies, we do not learn\nvisual features on the typically small audio-visual datasets, but use an\nalready available face landmark detector (trained on a separate image dataset).\nThe landmarks are used by LSTM-based models to generate time-frequency masks\nwhich are applied to the acoustic mixed-speech spectrogram. Results show that:\n(i) landmark motion features are very effective features for this task, (ii)\nsimilarly to previous work, reconstruction of the target speaker's spectrogram\nmediated by masking is significantly more accurate than direct spectrogram\nreconstruction, and (iii) the best masks depend on both motion landmark\nfeatures and the input mixed-speech spectrogram. To the best of our knowledge,\nour proposed models are the first models trained and evaluated on the limited\nsize GRID and TCD-TIMIT datasets, that achieve speaker-independent speech\nenhancement in a multi-talker setting.", "published": "2018-11-06 16:35:01", "link": "http://arxiv.org/abs/1811.02480v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Proceedings of the 2018 Workshop on Compositional Approaches in Physics,\n  NLP, and Social Sciences", "abstract": "The ability to compose parts to form a more complex whole, and to analyze a\nwhole as a combination of elements, is desirable across disciplines. This\nworkshop bring together researchers applying compositional approaches to\nphysics, NLP, cognitive science, and game theory. Within NLP, a long-standing\naim is to represent how words can combine to form phrases and sentences. Within\nthe framework of distributional semantics, words are represented as vectors in\nvector spaces. The categorical model of Coecke et al. [2010], inspired by\nquantum protocols, has provided a convincing account of compositionality in\nvector space models of NLP. There is furthermore a history of vector space\nmodels in cognitive science. Theories of categorization such as those developed\nby Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between\nfeature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model\nof concepts in which conceptual spaces provide geometric structures, and\ninformation is represented by points, vectors and regions in vector spaces. The\nsame compositional approach has been applied to this formalism, giving\nconceptual spaces theory a richer model of compositionality than previously\n[Bolt et al., 2018]. Compositional approaches have also been applied in the\nstudy of strategic games and Nash equilibria. In contrast to classical game\ntheory, where games are studied monolithically as one global object,\ncompositional game theory works bottom-up by building large and complex games\nfrom smaller components. Such an approach is inherently difficult since the\ninteraction between games has to be considered. Research into categorical\ncompositional methods for this field have recently begun [Ghani et al., 2018].\nMoreover, the interaction between the three disciplines of cognitive science,\nlinguistics and game theory is a fertile ground for research. Game theory in\ncognitive science is a well-established area [Camerer, 2011]. Similarly game\ntheoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly,\nthe study of linguistics and cognitive science is intimately intertwined\n[Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies\ncompositional approaches via vector spaces and categorical quantum theory,\nallowing the interplay between the three disciplines to be examined.", "published": "2018-11-06 23:25:45", "link": "http://arxiv.org/abs/1811.02701v1", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Parser Extraction of Triples in Unstructured Text", "abstract": "The web contains vast repositories of unstructured text. We investigate the\nopportunity for building a knowledge graph from these text sources. We generate\na set of triples which can be used in knowledge gathering and integration. We\ndefine the architecture of a language compiler for processing\nsubject-predicate-object triples using the OpenNLP parser. We implement a\ndepth-first search traversal on the POS tagged syntactic tree appending\npredicate and object information. A parser enables higher precision and higher\nrecall extractions of syntactic relationships across conjunction boundaries. We\nare able to extract 2-2.5 times the correct extractions of ReVerb. The\nextractions are used in a variety of semantic web applications and question\nanswering. We verify extraction of 50,000 triples on the ClueWeb dataset.", "published": "2018-11-06 16:12:00", "link": "http://arxiv.org/abs/1811.05768v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FloWaveNet : A Generative Flow for Raw Audio", "abstract": "Most modern text-to-speech architectures use a WaveNet vocoder for\nsynthesizing high-fidelity waveform audio, but there have been limitations,\nsuch as high inference time, in its practical application due to its ancestral\nsampling scheme. The recently suggested Parallel WaveNet and ClariNet have\nachieved real-time audio synthesis capability by incorporating inverse\nautoregressive flow for parallel sampling. However, these approaches require a\ntwo-stage training pipeline with a well-trained teacher network and can only\nproduce natural sound by using probability distillation along with auxiliary\nloss terms. We propose FloWaveNet, a flow-based generative model for raw audio\nsynthesis. FloWaveNet requires only a single-stage training procedure and a\nsingle maximum likelihood loss, without any additional auxiliary terms, and it\nis inherently parallel due to the characteristics of generative flow. The model\ncan efficiently sample raw audio in real-time, with clarity comparable to\nprevious two-stage parallel models. The code and samples for all models,\nincluding our FloWaveNet, are publicly available.", "published": "2018-11-06 04:30:41", "link": "http://arxiv.org/abs/1811.02155v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language model integration based on memory control for sequence to\n  sequence speech recognition", "abstract": "In this paper, we explore several new schemes to train a seq2seq model to\nintegrate a pre-trained LM. Our proposed fusion methods focus on the memory\ncell state and the hidden state in the seq2seq decoder long short-term memory\n(LSTM), and the memory cell state is updated by the LM unlike the prior\nstudies. This means the memory retained by the main seq2seq would be adjusted\nby the external LM. These fusion methods have several variants depending on the\narchitecture of this memory cell update and the use of memory cell and hidden\nstates which directly affects the final label inference. We performed the\nexperiments to show the effectiveness of the proposed methods in a mono-lingual\nASR setup on the Librispeech corpus and in a transfer learning setup from a\nmultilingual ASR (MLASR) base model to a low-resourced language. In\nLibrispeech, our best model improved WER by 3.7%, 2.4% for test clean, test\nother relatively to the shallow fusion baseline, with multi-level decoding. In\ntransfer learning from an MLASR base model to the IARPA Babel Swahili model,\nthe best scheme improved the transferred model on eval set by 9.9%, 9.8% in\nCER, WER relatively to the 2-stage transfer baseline.", "published": "2018-11-06 04:56:44", "link": "http://arxiv.org/abs/1811.02162v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker verification using end-to-end adversarial language adaptation", "abstract": "In this paper we investigate the use of adversarial domain adaptation for\naddressing the problem of language mismatch between speaker recognition\ncorpora. In the context of speaker verification, adversarial domain adaptation\nmethods aim at minimizing certain divergences between the distribution that the\nutterance-level features follow (i.e. speaker embeddings) when drawn from\nsource and target domains (i.e. languages), while preserving their capacity in\nrecognizing speakers. Neural architectures for extracting utterance-level\nrepresentations enable us to apply adversarial adaptation methods in an\nend-to-end fashion and train the network jointly with the standard\ncross-entropy loss. We examine several configurations, such as the use of\n(pseudo-)labels on the target domain as well as domain labels in the feature\nextractor, and we demonstrate the effectiveness of our method on the\nchallenging NIST SRE16 and SRE18 benchmarks.", "published": "2018-11-06 12:56:01", "link": "http://arxiv.org/abs/1811.02331v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "User Specific Adaptation in Automatic Transcription of Vocalised\n  Percussion", "abstract": "The goal of this work is to develop an application that enables music\nproducers to use their voice to create drum patterns when composing in Digital\nAudio Workstations (DAWs). An easy-to-use and user-oriented system capable of\nautomatically transcribing vocalisations of percussion sounds, called LVT -\nLive Vocalised Transcription, is presented. LVT is developed as a Max for Live\ndevice which follows the `segment-and-classify' methodology for drum\ntranscription, and includes three modules: i) an onset detector to segment\nevents in time; ii) a module that extracts relevant features from the audio\ncontent; and iii) a machine-learning component that implements the k-Nearest\nNeighbours (kNN) algorithm for the classification of vocalised drum timbres.\n  Due to the wide differences in vocalisations from distinct users for the same\ndrum sound, a user-specific approach to vocalised transcription is proposed. In\nthis perspective, a given end-user trains the algorithm with their own\nvocalisations for each drum sound before inputting their desired pattern into\nthe DAW. The user adaption is achieved via a new Max external which implements\nSequential Forward Selection (SFS) for choosing the most relevant features for\na given set of input drum sounds.", "published": "2018-11-06 15:23:53", "link": "http://arxiv.org/abs/1811.02406v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An audio-only method for advertisement detection in broadcast television\n  content", "abstract": "We address the task of advertisement detection in broadcast television\ncontent. While typically approached from a video-only or audio-visual\nperspective, we present an audio-only method. Our approach centres on the\ndetection of short silences which exist at the boundaries between programming\nand advertising, as well as between the advertisements themselves. To identify\nadvertising regions we first locate all points within the broadcast content\nwith very low signal energy. Next, we use a multiple linear regression model to\nreject non-boundary silences based on features extracted from the local context\nimmediately surrounding the silence. Finally, we determine the advertising\nregions based on the long-term grouping of detected boundary silences. When\nevaluated over a 26 hour annotated database covering national and commercial\nPortuguese television channels we obtain a Matthews correlation coefficient in\nexcess of 0.87 and outperform a freely available audio-visual approach.", "published": "2018-11-06 15:28:26", "link": "http://arxiv.org/abs/1811.02411v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SDR - half-baked or well done?", "abstract": "In speech enhancement and source separation, signal-to-noise ratio is a\nubiquitous objective measure of denoising/separation quality. A decade ago, the\nBSS_eval toolkit was developed to give researchers worldwide a way to evaluate\nthe quality of their algorithms in a simple, fair, and hopefully insightful\nway: it attempted to account for channel variations, and to not only evaluate\nthe total distortion in the estimated signal but also split it in terms of\nvarious factors such as remaining interference, newly added artifacts, and\nchannel errors. In recent years, hundreds of papers have been relying on this\ntoolkit to evaluate their proposed methods and compare them to previous works,\noften arguing that differences on the order of 0.1 dB proved the effectiveness\nof a method over others. We argue here that the signal-to-distortion ratio\n(SDR) implemented in the BSS_eval toolkit has generally been improperly used\nand abused, especially in the case of single-channel separation, resulting in\nmisleading results. We propose to use a slightly modified definition, resulting\nin a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We\npresent various examples of critical failure of the original SDR that SI-SDR\novercomes.", "published": "2018-11-06 17:20:05", "link": "http://arxiv.org/abs/1811.02508v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel\n  Speech Enhancement", "abstract": "We apply a fast kernel method for mask-based single-channel speech\nenhancement. Specifically, our method solves a kernel regression problem\nassociated to a non-smooth kernel function (exponential power kernel) with a\nhighly efficient iterative method (EigenPro). Due to the simplicity of this\nmethod, its hyper-parameters such as kernel bandwidth can be automatically and\nefficiently selected using line search with subsamples of training data. We\nobserve an empirical correlation between the regression loss (mean square\nerror) and regular metrics for speech enhancement. This observation justifies\nour training target and motivates us to achieve lower regression loss by\ntraining separate kernel model per frequency subband. We compare our method\nwith the state-of-the-art deep neural networks on mask-based HINT and TIMIT.\nExperimental results show that our kernel method consistently outperforms deep\nneural networks while requiring less training time.", "published": "2018-11-06 00:04:55", "link": "http://arxiv.org/abs/1811.02095v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "NIPS4Bplus: a richly annotated birdsong audio dataset", "abstract": "Recent advances in birdsong detection and classification have approached a\nlimit due to the lack of fully annotated recordings. In this paper, we present\nNIPS4Bplus, the first richly annotated birdsong audio dataset, that is\ncomprised of recordings containing bird vocalisations along with their active\nspecies tags plus the temporal annotations acquired for them. Statistical\ninformation about the recordings, their species specific tags and their\ntemporal annotations are presented along with example uses. NIPS4Bplus could be\nused in various ecoacoustic tasks, such as training models for bird population\nmonitoring, species classification, birdsong vocalisation detection and\nclassification.", "published": "2018-11-06 10:46:27", "link": "http://arxiv.org/abs/1811.02275v2", "categories": ["cs.SD", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bootstrapping single-channel source separation via unsupervised spatial\n  clustering on stereo mixtures", "abstract": "Separating an audio scene into isolated sources is a fundamental problem in\ncomputer audition, analogous to image segmentation in visual scene analysis.\nSource separation systems based on deep learning are currently the most\nsuccessful approaches for solving the underdetermined separation problem, where\nthere are more sources than channels. Traditionally, such systems are trained\non sound mixtures where the ground truth decomposition is already known. Since\nmost real-world recordings do not have such a decomposition available, this\nlimits the range of mixtures one can train on, and the range of mixtures the\nlearned models may successfully separate. In this work, we use a simple blind\nspatial source separation algorithm to generate estimated decompositions of\nstereo mixtures. These estimates, together with a weighting scheme in the\ntime-frequency domain, based on confidence in the separation quality, are used\nto train a deep learning model that can be used for single-channel separation,\nwhere no source direction information is available. This demonstrates how a\nsimple cue such as the direction of origin of source can be used to bootstrap a\nmodel for source separation that can be used in situations where that cue is\nnot available.", "published": "2018-11-06 02:20:40", "link": "http://arxiv.org/abs/1811.02130v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Unifying Probabilistic Models for Time-Frequency Analysis", "abstract": "In audio signal processing, probabilistic time-frequency models have many\nbenefits over their non-probabilistic counterparts. They adapt to the incoming\nsignal, quantify uncertainty, and measure correlation between the signal's\namplitude and phase information, making time domain resynthesis\nstraightforward. However, these models are still not widely used since they\ncome at a high computational cost, and because they are formulated in such a\nway that it can be difficult to interpret all the modelling assumptions. By\nshowing their equivalence to Spectral Mixture Gaussian processes, we illuminate\nthe underlying model assumptions and provide a general framework for\nconstructing more complex models that better approximate real-world signals.\nOur interpretation makes it intuitive to inspect, compare, and alter the models\nsince all prior knowledge is encoded in the Gaussian process kernel functions.\nWe utilise a state space representation to perform efficient inference via\nKalman smoothing, and we demonstrate how our interpretation allows for\nefficient parameter learning in the frequency domain.", "published": "2018-11-06 17:00:19", "link": "http://arxiv.org/abs/1811.02489v6", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "eess.SP"}
{"title": "Bidirectional Quaternion Long-Short Term Memory Recurrent Neural\n  Networks for Speech Recognition", "abstract": "Recurrent neural networks (RNN) are at the core of modern automatic speech\nrecognition (ASR) systems. In particular, long-short term memory (LSTM)\nrecurrent neural networks have achieved state-of-the-art results in many speech\nrecognition tasks, due to their efficient representation of long and short term\ndependencies in sequences of inter-dependent features. Nonetheless, internal\ndependencies within the element composing multidimensional features are weakly\nconsidered by traditional real-valued representations. We propose a novel\nquaternion long-short term memory (QLSTM) recurrent neural network that takes\ninto account both the external relations between the features composing a\nsequence, and these internal latent structural dependencies with the quaternion\nalgebra. QLSTMs are compared to LSTMs during a memory copy-task and a realistic\napplication of speech recognition on the Wall Street Journal (WSJ) dataset.\nQLSTM reaches better performances during the two experiments with up to $2.8$\ntimes less learning parameters, leading to a more expressive representation of\nthe information.", "published": "2018-11-06 21:17:34", "link": "http://arxiv.org/abs/1811.02566v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Reconstructing Speech Stimuli From Human Auditory Cortex Activity Using\n  a WaveNet Approach", "abstract": "The superior temporal gyrus (STG) region of cortex critically contributes to\nspeech recognition. In this work, we show that a proposed WaveNet, with limited\navailable data, is able to reconstruct speech stimuli from STG intracranial\nrecordings. We further investigate the impulse response of the fitted model for\neach recording electrode and observe phoneme level temporospectral tuning\nproperties for the recorded area of cortex. This discovery is consistent with\nprevious studies implicating the posterior STG (pSTG) in a phonetic\nrepresentation of speech and provides detailed acoustic features that certain\nelectrode sites possibly extract during speech recognition.", "published": "2018-11-06 22:19:28", "link": "http://arxiv.org/abs/1811.02694v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.NC", "stat.ML"], "primary_category": "cs.SD"}
