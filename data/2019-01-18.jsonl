{"title": "Chinese Word Segmentation: Another Decade Review (2007-2017)", "abstract": "This paper reviews the development of Chinese word segmentation (CWS) in the\nmost recent decade, 2007-2017. Special attention was paid to the deep learning\ntechnologies that has already permeated into most areas of natural language\nprocessing (NLP). The basic view we have arrived at is that compared to\ntraditional supervised learning methods, neural network based methods have not\nshown any superior performance. The most critical challenge still lies on\nbalancing of recognition of in-vocabulary (IV) and out-of-vocabulary (OOV)\nwords. However, as neural models have potentials to capture the essential\nlinguistic structure of natural language, we are optimistic about significant\nprogresses may arrive in the near future.", "published": "2019-01-18 04:16:56", "link": "http://arxiv.org/abs/1901.06079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Semi-supervised Variational Autoencoders for Biomedical\n  Relation Extraction", "abstract": "The biomedical literature provides a rich source of knowledge such as\nprotein-protein interactions (PPIs), drug-drug interactions (DDIs) and\nchemical-protein interactions (CPIs). Biomedical relation extraction aims to\nautomatically extract biomedical relations from biomedical text for various\nbiomedical research. State-of-the-art methods for biomedical relation\nextraction are primarily based on supervised machine learning and therefore\ndepend on (sufficient) labeled data. However, creating large sets of training\ndata is prohibitively expensive and labor-intensive, especially so in\nbiomedicine as domain knowledge is required. In contrast, there is a large\namount of unlabeled biomedical text available in PubMed. Hence, computational\nmethods capable of employing unlabeled data to reduce the burden of manual\nannotation are of particular interest in biomedical relation extraction. We\npresent a novel semi-supervised approach based on variational autoencoder (VAE)\nfor biomedical relation extraction. Our model consists of the following three\nparts, a classifier, an encoder and a decoder. The classifier is implemented\nusing multi-layer convolutional neural networks (CNNs), and the encoder and\ndecoder are implemented using both bidirectional long short-term memory\nnetworks (Bi-LSTMs) and CNNs, respectively. The semi-supervised mechanism\nallows our model to learn features from both the labeled and unlabeled data. We\nevaluate our method on multiple public PPI, DDI and CPI corpora. Experimental\nresults show that our method effectively exploits the unlabeled data to improve\nthe performance and reduce the dependence on labeled data. To our best\nknowledge, this is the first semi-supervised VAE-based method for (biomedical)\nrelation extraction. Our results suggest that exploiting such unlabeled data\ncan be greatly beneficial to improved performance in various biomedical\nrelation extraction.", "published": "2019-01-18 06:48:53", "link": "http://arxiv.org/abs/1901.06103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sequence-to-Sequence Learning via Optimal Transport", "abstract": "Sequence-to-sequence models are commonly trained via maximum likelihood\nestimation (MLE). However, standard MLE training considers a word-level\nobjective, predicting the next word given the previous ground-truth partial\nsentence. This procedure focuses on modeling local syntactic patterns, and may\nfail to capture long-range semantic structure. We present a novel solution to\nalleviate these issues. Our approach imposes global sequence-level guidance via\nnew supervision based on optimal transport, enabling the overall\ncharacterization and preservation of semantic features. We further show that\nthis method can be understood as a Wasserstein gradient flow trying to match\nour model to the ground truth sequence distribution. Extensive experiments are\nconducted to validate the utility of the proposed approach, showing consistent\nimprovements over a wide variety of NLP tasks, including machine translation,\nabstractive text summarization, and image captioning.", "published": "2019-01-18 14:52:34", "link": "http://arxiv.org/abs/1901.06283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Latent Sentence Structure in Neural Machine Translation", "abstract": "Recently it was shown that linguistic structure predicted by a supervised\nparser can be beneficial for neural machine translation (NMT). In this work we\ninvestigate a more challenging setup: we incorporate sentence structure as a\nlatent variable in a standard NMT encoder-decoder and induce it in such a way\nas to benefit the translation task. We consider German-English and\nJapanese-English translation benchmarks and observe that when using RNN\nencoders the model makes no or very limited use of the structure induction\napparatus. In contrast, CNN and word-embedding-based encoders rely on latent\ngraphs and force them to encode useful, potentially long-distance,\ndependencies.", "published": "2019-01-18 22:43:17", "link": "http://arxiv.org/abs/1901.06436v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Keyboard Layout Design for Low-Resource Latin-Script Languages", "abstract": "We present our approach to automatically designing and implementing keyboard\nlayouts on mobile devices for typing low-resource languages written in the\nLatin script. For many speakers, one of the barriers in accessing and creating\ntext content on the web is the absence of input tools for their language. Ease\nin typing in these languages would lower technological barriers to online\ncommunication and collaboration, likely leading to the creation of more web\ncontent. Unfortunately, it can be time-consuming to develop layouts manually\neven for language communities that use a keyboard layout very similar to\nEnglish; starting from scratch requires many configuration files to describe\nmultiple possible behaviors for each key. With our approach, we only need a\nsmall amount of data in each language to generate keyboard layouts with very\nlittle human effort. This process can help serve speakers of low-resource\nlanguages in a scalable way, allowing us to develop input tools for more\nlanguages. Having input tools that reflect the linguistic diversity of the\nworld will let as many people as possible use technology to learn, communicate,\nand express themselves in their own native languages.", "published": "2019-01-18 00:09:24", "link": "http://arxiv.org/abs/1901.06039v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Identifying Unclear Questions in Community Question Answering Websites", "abstract": "Thousands of complex natural language questions are submitted to community\nquestion answering websites on a daily basis, rendering them as one of the most\nimportant information sources these days. However, oftentimes submitted\nquestions are unclear and cannot be answered without further clarification\nquestions by expert community members. This study is the first to investigate\nthe complex task of classifying a question as clear or unclear, i.e., if it\nrequires further clarification. We construct a novel dataset and propose a\nclassification approach that is based on the notion of similar questions. This\napproach is compared to state-of-the-art text classification baselines. Our\nmain finding is that the similar questions approach is a viable alternative\nthat can be used as a stepping stone towards the development of supportive user\ninterfaces for question formulation.", "published": "2019-01-18 10:29:20", "link": "http://arxiv.org/abs/1901.06168v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
