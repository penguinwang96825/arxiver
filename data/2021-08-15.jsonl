{"title": "Accurate, yet inconsistent? Consistency Analysis on Language\n  Understanding Models", "abstract": "Consistency, which refers to the capability of generating the same\npredictions for semantically similar contexts, is a highly desirable property\nfor a sound language understanding model. Although recent pretrained language\nmodels (PLMs) deliver outstanding performance in various downstream tasks, they\nshould exhibit consistent behaviour provided the models truly understand\nlanguage. In this paper, we propose a simple framework named consistency\nanalysis on language understanding models (CALUM)} to evaluate the model's\nlower-bound consistency ability. Through experiments, we confirmed that current\nPLMs are prone to generate inconsistent predictions even for semantically\nidentical inputs. We also observed that multi-task training with paraphrase\nidentification tasks is of benefit to improve consistency, increasing the\nconsistency by 13% on average.", "published": "2021-08-15 06:25:07", "link": "http://arxiv.org/abs/2108.06665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complex Knowledge Base Question Answering: A Survey", "abstract": "Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Early studies mainly focused on answering simple questions\nover KBs and achieved great success. However, their performance on complex\nquestions is still far from satisfactory. Therefore, in recent years,\nresearchers propose a large number of novel methods, which looked into the\nchallenges of answering complex questions. In this survey, we review recent\nadvances on KBQA with the focus on solving complex questions, which usually\ncontain multiple subjects, express compound relations, or involve numerical\noperations. In detail, we begin with introducing the complex KBQA task and\nrelevant background. Then, we describe benchmark datasets for complex KBQA task\nand introduce the construction process of these datasets. Next, we present two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. Specifically, we illustrate their procedures with flow designs and\ndiscuss their major differences and similarities. After that, we summarize the\nchallenges that these two categories of methods encounter when answering\ncomplex questions, and explicate advanced solutions and techniques used in\nexisting work. Finally, we conclude and discuss several promising directions\nrelated to complex KBQA for future research.", "published": "2021-08-15 08:14:54", "link": "http://arxiv.org/abs/2108.06688v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What can Neural Referential Form Selectors Learn?", "abstract": "Despite achieving encouraging results, neural Referring Expression Generation\nmodels are often thought to lack transparency. We probed neural Referential\nForm Selection (RFS) models to find out to what extent the linguistic features\ninfluencing the RE form are learnt and captured by state-of-the-art RFS models.\nThe results of 8 probing tasks show that all the defined features were learnt\nto some extent. The probing tasks pertaining to referential status and\nsyntactic position exhibited the highest performance. The lowest performance\nwas achieved by the probing models designed to predict discourse structure\nproperties beyond the sentence level.", "published": "2021-08-15 20:09:26", "link": "http://arxiv.org/abs/2108.06806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural\n  Language Generation", "abstract": "Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset to study question answering (QA) and natural\nlanguage generation (NLG) over hierarchical tables. HiTab is a cross-domain\ndataset constructed from a wealth of statistical reports (analyses) and\nWikipedia pages, and has unique characteristics: (1) nearly all tables are\nhierarchical, and (2) both target sentences for NLG and questions for QA are\nrevised from original, meaningful, and diverse descriptive sentences authored\nby analysts and professions of reports. (3) to reveal complex numerical\nreasoning in statistical analyses, we provide fine-grained annotations of\nentity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive\nsentences with well-annotated quantity and entity alignment on 3,597 tables\nwith broad coverage of table hierarchies and numerical reasoning types.\n  Targeting hierarchical structure, we devise a novel hierarchy-aware logical\nform for symbolic reasoning over tables, which shows high effectiveness.\nTargeting complex numerical reasoning, we propose partially supervised training\ngiven annotations of entity and quantity alignment, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.", "published": "2021-08-15 10:14:21", "link": "http://arxiv.org/abs/2108.06712v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring Generalization Ability of Pretrained Language Models on\n  Arithmetic and Logical Reasoning", "abstract": "To quantitatively and intuitively explore the generalization ability of\npre-trained language models (PLMs), we have designed several tasks of\narithmetic and logical reasoning. We both analyse how well PLMs generalize when\nthe test data is in the same distribution as the train data and when it is\ndifferent, for the latter analysis, we have also designed a cross-distribution\ntest set other than the in-distribution test set. We conduct experiments on one\nof the most advanced and publicly released generative PLM - BART. Our research\nfinds that the PLMs can easily generalize when the distribution is the same,\nhowever, it is still difficult for them to generalize out of the distribution.", "published": "2021-08-15 13:42:10", "link": "http://arxiv.org/abs/2108.06743v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual\n  Representations", "abstract": "Building an independent misspelling detector and serve it before correction\ncan bring multiple benefits to speller and other search components, which is\nparticularly true for the most commonly deployed noisy-channel based speller\nsystems. With rapid development of deep learning and substantial advancement in\ncontextual representation learning such as BERTology, building a decent\nmisspelling detector without having to rely on hand-crafted features associated\nwith noisy-channel architecture becomes more-than-ever accessible. However\nBERTolgy models are trained with natural language corpus but Maps Search is\nhighly domain specific, would BERTology continue its success. In this paper we\ndesign 4 stages of models for misspeling detection ranging from the most basic\nLSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in\nour case, other advanced BERTology family model such as RoBERTa does not\nnecessarily outperform BERT, and a classic cross-domain fine-tuned full BERT\neven underperforms a smaller single-domain fine-tuned BERT. We share more\nfindings through comprehensive modeling experiments and analysis, we also\nbriefly cover the data generation algorithm breakthrough.", "published": "2021-08-15 23:59:12", "link": "http://arxiv.org/abs/2108.06842v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Active Learning for Text Classification with Diverse\n  Interpretations", "abstract": "Recently, Deep Neural Networks (DNNs) have made remarkable progress for text\nclassification, which, however, still require a large number of labeled data.\nTo train high-performing models with the minimal annotation cost, active\nlearning is proposed to select and label the most informative samples, yet it\nis still challenging to measure informativeness of samples used in DNNs. In\nthis paper, inspired by piece-wise linear interpretability of DNNs, we propose\na novel Active Learning with DivErse iNterpretations (ALDEN) approach. With\nlocal interpretations in DNNs, ALDEN identifies linearly separable regions of\nsamples. Then, it selects samples according to their diversity of local\ninterpretations and queries their labels. To tackle the text classification\nproblem, we choose the word with the most diverse interpretations to represent\nthe whole sentence. Extensive experiments demonstrate that ALDEN consistently\noutperforms several state-of-the-art deep active learning methods.", "published": "2021-08-15 10:42:07", "link": "http://arxiv.org/abs/2108.10687v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition\n  in Virtual Assistants", "abstract": "Named entity recognition (NER) is usually developed and tested on text from\nwell-written sources. However, in intelligent voice assistants, where NER is an\nimportant component, input to NER may be noisy because of user or speech\nrecognition error. In applications, entity labels may change frequently, and\nnon-textual properties like topicality or popularity may be needed to choose\namong alternatives.\n  We describe a NER system intended to address these problems. We test and\ntrain this system on a proprietary user-derived dataset. We compare with a\nbaseline text-only NER system; the baseline enhanced with external gazetteers;\nand the baseline enhanced with the search and indirect labelling techniques we\ndescribe below. The final configuration gives around 6% reduction in NER error\nrate. We also show that this technique improves related tasks, such as semantic\nparsing, with an improvement of up to 5% in error rate.", "published": "2021-08-15 00:14:47", "link": "http://arxiv.org/abs/2108.06633v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation", "abstract": "We motivate and propose a suite of simple but effective improvements for\nconcept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc\nPHrase Infilling and REcombination. We demonstrate their effectiveness on\ngenerative commonsense reasoning, a.k.a. the CommonGen task, through\nexperiments using both BART and T5 models. Through extensive automatic and\nhuman evaluation, we show that SAPPHIRE noticeably improves model performance.\nAn in-depth qualitative analysis illustrates that SAPPHIRE effectively\naddresses many issues of the baseline model generations, including lack of\ncommonsense, insufficient specificity, and poor fluency.", "published": "2021-08-15 01:58:45", "link": "http://arxiv.org/abs/2108.06643v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Wikipedia Article Quality in One Dimension by Extending ORES\n  with Ordinal Regression", "abstract": "Organizing complex peer production projects and advancing scientific\nknowledge of open collaboration each depend on the ability to measure quality.\nArticle quality ratings on English language Wikipedia have been widely used by\nboth Wikipedia community members and academic researchers for purposes like\ntracking knowledge gaps and studying how political polarization shapes\ncollaboration. Even so, measuring quality presents many methodological\nchallenges. The most widely used systems use labels on discrete ordinal scales\nwhen assessing quality, but such labels can be inconvenient for statistics and\nmachine learning. Prior work handles this by assuming that different levels of\nquality are \"evenly spaced\" from one another. This assumption runs counter to\nintuitions about the relative degrees of effort needed to raise Wikipedia\nencyclopedia articles to different quality levels. Furthermore, models from\nprior work are fit to datasets that oversample high-quality articles. This\nlimits their accuracy for representative samples of articles or revisions. I\ndescribe a technique extending the Wikimedia Foundations' ORES article quality\nmodel to address these limitations. My method uses weighted ordinal regression\nmodels to construct one-dimensional continuous measures of quality. While\nscores from my technique and from prior approaches are correlated, my approach\nimproves accuracy for research datasets and provides evidence that the \"evenly\nspaced\" assumption is unfounded in practice on English Wikipedia. I conclude\nwith recommendations for using quality scores in future research and include\nthe full code, data, and models.", "published": "2021-08-15 23:05:28", "link": "http://arxiv.org/abs/2108.10684v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "H.0; J.4; K.4; I.2"], "primary_category": "cs.CL"}
