{"title": "Multi-Stage Coarse-to-Fine Contrastive Learning for Conversation Intent\n  Induction", "abstract": "Intent recognition is critical for task-oriented dialogue systems. However,\nfor emerging domains and new services, it is difficult to accurately identify\nthe key intent of a conversation due to time-consuming data annotation and\ncomparatively poor model transferability. Therefore, the automatic induction of\ndialogue intention is very important for intelligent dialogue systems. This\npaper presents our solution to Track 2 of Intent Induction from Conversations\nfor Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge\n(DSTC11). The essence of intention clustering lies in distinguishing the\nrepresentation of different dialogue utterances. The key to automatic intention\ninduction is that, for any given set of new data, the sentence representation\nobtained by the model can be well distinguished from different labels.\nTherefore, we propose a multi-stage coarse-to-fine contrastive learning model\ntraining scheme including unsupervised contrastive learning pre-training,\nsupervised contrastive learning pre-training, and fine-tuning with joint\ncontrastive learning and clustering to obtain a better dialogue utterance\nrepresentation model for the clustering task. In the released DSTC11 Track 2\nevaluation results, our proposed system ranked first on both of the two\nsubtasks of this Track.", "published": "2023-03-09 04:51:27", "link": "http://arxiv.org/abs/2303.05034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Language agnostic WER Standardization", "abstract": "Word error rate (WER) is a standard metric for the evaluation of Automated\nSpeech Recognition (ASR) systems. However, WER fails to provide a fair\nevaluation of human perceived quality in presence of spelling variations,\nabbreviations, or compound words arising out of agglutination. Multiple\nspelling variations might be acceptable based on locale/geography, alternative\nabbreviations, borrowed words, and transliteration of code-mixed words from a\nforeign language to the target language script. Similarly, in case of\nagglutination, often times the agglutinated, as well as the split forms, are\nacceptable. Previous work handled this problem by using manually identified\nnormalization pairs and applying them to both the transcription and the\nhypothesis before computing WER. In this paper, we propose an automatic WER\nnormalization system consisting of two modules: spelling normalization and\nsegmentation normalization. The proposed system is unsupervised and language\nagnostic, and therefore scalable. Experiments with ASR on 35K utterances across\nfour languages yielded an average WER reduction of 13.28%. Human judgements of\nthese automatically identified normalization pairs show that our WER-normalized\nevaluation is highly consistent with the perceived quality of ASR output.", "published": "2023-03-09 05:50:54", "link": "http://arxiv.org/abs/2303.05046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for\n  Document Information Extraction", "abstract": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting. Code is available at\nhttps://github.com/MAEHCM/ICL-D3IE.", "published": "2023-03-09 06:24:50", "link": "http://arxiv.org/abs/2303.05063v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction", "abstract": "Recently, many studies incorporate external knowledge into character-level\nfeature based models to improve the performance of Chinese relation extraction.\nHowever, these methods tend to ignore the internal information of the Chinese\ncharacter and cannot filter out the noisy information of external knowledge. To\naddress these issues, we propose a mixture-of-view-experts framework (MoVE) to\ndynamically learn multi-view features for Chinese relation extraction. With\nboth the internal and external knowledge of Chinese characters, our framework\ncan better capture the semantic information of Chinese characters. To\ndemonstrate the effectiveness of the proposed framework, we conduct extensive\nexperiments on three real-world datasets in distinct domains. Experimental\nresults show consistent and significant superiority and robustness of our\nproposed framework. Our code and dataset will be released at:\nhttps://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction", "published": "2023-03-09 07:35:31", "link": "http://arxiv.org/abs/2303.05082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geometry of Language", "abstract": "In this article, we present a fresh perspective on language, combining ideas\nfrom various sources, but mixed in a new synthesis. As in the minimalist\nprogram, the question is whether we can formulate an elegant formalism, a\nuniversal grammar or a mechanism which explains significant aspects of the\nhuman faculty of language, which in turn can be considered a natural\ndisposition for the evolution and deployment of the diverse human languages. We\ndescribe such a mechanism, which differs from existing logical and grammatical\napproaches by its geometric nature. Our main contribution is to explore the\nassumption that sentence recognition takes place by forming chains of tokens\nrepresenting words, followed by matching these chains with pre-existing chains\nrepresenting grammatical word orders. The aligned chains of tokens give rise to\ntwo- and three-dimensional complexes. The resulting model gives an alternative\npresentation for subtle rules, traditionally formalized using categorial\ngrammar.", "published": "2023-03-09 12:22:28", "link": "http://arxiv.org/abs/2303.05208v1", "categories": ["cs.CL", "03B65 (Primary) 91F20 (Secundary)"], "primary_category": "cs.CL"}
{"title": "Types of Approaches, Applications and Challenges in the Development of\n  Sentiment Analysis Systems", "abstract": "Today, the web has become a mandatory platform to express users' opinions,\nemotions and feelings about various events. Every person using his smartphone\ncan give his opinion about the purchase of a product, the occurrence of an\naccident, the occurrence of a new disease, etc. in blogs and social networks\nsuch as (Twitter, WhatsApp, Telegram and Instagram) register. Therefore,\nmillions of comments are recorded daily and it creates a huge volume of\nunstructured text data that can extract useful knowledge from this type of data\nby using natural language processing methods. Sentiment analysis is one of the\nimportant applications of natural language processing and machine learning,\nwhich allows us to analyze the sentiments of comments and other textual\ninformation recorded by web users. Therefore, the analysis of sentiments,\napproaches and challenges in this field will be explained in the following.", "published": "2023-03-09 15:18:34", "link": "http://arxiv.org/abs/2303.11176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the\n  LexGLUE benchmark", "abstract": "Following the hype around OpenAI's ChatGPT conversational agent, the last\nstraw in the recent development of Large Language Models (LLMs) that\ndemonstrate emergent unprecedented zero-shot capabilities, we audit the latest\nOpenAI's GPT-3.5 model, `gpt-3.5-turbo', the first available ChatGPT model, in\nthe LexGLUE benchmark in a zero-shot fashion providing examples in a templated\ninstruction-following format. The results indicate that ChatGPT achieves an\naverage micro-F1 score of 47.6% across LexGLUE tasks, surpassing the baseline\nguessing rates. Notably, the model performs exceptionally well in some\ndatasets, achieving micro-F1 scores of 62.8% and 70.2% in the ECtHR B and\nLEDGAR datasets, respectively. The code base and model predictions are\navailable for review on https://github.com/coastalcph/zeroshot_lexglue.", "published": "2023-03-09 16:42:29", "link": "http://arxiv.org/abs/2304.12202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Get Personal: Personal Questions Improve SocialBot Performance in\n  the Alexa Prize", "abstract": "There has been an increased focus on creating conversational open-domain\ndialogue systems in the spoken dialogue community. Unlike traditional dialogue\nsystems, these conversational systems cannot assume any specific information\nneed or domain restrictions, i.e., the only inherent goal is to converse with\nthe user on an unknown set of topics. While massive improvements in Natural\nLanguage Understanding (NLU) and the growth of available knowledge resources\ncan partially support a robust conversation, these conversations generally lack\nthe rapport between two humans that know each other. We developed a robust\nopen-domain conversational system, Athena, that real Amazon Echo users access\nand evaluate at scale in the context of the Alexa Prize competition. We\nexperiment with methods intended to increase intimacy between Athena and the\nuser by heuristically developing a rule-based user model that personalizes both\nthe current and subsequent conversations and evaluating specific personal\nopinion question strategies in A/B studies. Our results show a statistically\nsignificant positive impact on perceived conversation quality and length when\nemploying these strategies.", "published": "2023-03-09 00:10:29", "link": "http://arxiv.org/abs/2303.04953v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning the Legibility of Visual Text Perturbations", "abstract": "Many adversarial attacks in NLP perturb inputs to produce visually similar\nstrings ('ergo' $\\rightarrow$ '$\\epsilon$rgo') which are legible to humans but\ndegrade model performance. Although preserving legibility is a necessary\ncondition for text perturbation, little work has been done to systematically\ncharacterize it; instead, legibility is typically loosely enforced via\nintuitions around the nature and extent of perturbations. Particularly, it is\nunclear to what extent can inputs be perturbed while preserving legibility, or\nhow to quantify the legibility of a perturbed string. In this work, we address\nthis gap by learning models that predict the legibility of a perturbed string,\nand rank candidate perturbations based on their legibility. To do so, we\ncollect and release LEGIT, a human-annotated dataset comprising the legibility\nof visually perturbed text. Using this dataset, we build both text- and\nvision-based models which achieve up to $0.91$ F1 score in predicting whether\nan input is legible, and an accuracy of $0.86$ in predicting which of two given\nperturbations is more legible. Additionally, we discover that legible\nperturbations from the LEGIT dataset are more effective at lowering the\nperformance of NLP models than best-known attack strategies, suggesting that\ncurrent models may be vulnerable to a broad range of perturbations beyond what\nis captured by existing visual attacks. Data, code, and models are available at\nhttps://github.com/dvsth/learning-legibility-2023.", "published": "2023-03-09 07:22:07", "link": "http://arxiv.org/abs/2303.05077v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Revisiting the relevance of traditional genres: a network analysis of\n  fiction readers' preferences", "abstract": "We investigate how well traditional fiction genres like Fantasy, Thriller,\nand Literature represent readers' preferences. Using user data from Goodreads\nwe construct a book network where two books are strongly linked if the same\npeople tend to read or enjoy them both. We then partition this network into\ncommunities of similar books and assign each a list of subjects from The Open\nLibrary to serve as a proxy for traditional genres. Our analysis reveals that\nthe network communities correspond to existing combinations of traditional\ngenres, but that the exact communities differ depending on whether we consider\nbooks that people read or books that people enjoy.\n  In addition, we apply principal component analysis to the data and find that\nthe variance in the book communities is best explained by two factors: the\nmaturity/childishness and realism/fantastical nature of the books. We propose\nusing this maturity-realism plane as a coarse classification tool for stories.", "published": "2023-03-09 07:31:56", "link": "http://arxiv.org/abs/2303.05080v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Improving Video Retrieval by Adaptive Margin", "abstract": "Video retrieval is becoming increasingly important owing to the rapid\nemergence of videos on the Internet. The dominant paradigm for video retrieval\nlearns video-text representations by pushing the distance between the\nsimilarity of positive pairs and that of negative pairs apart from a fixed\nmargin. However, negative pairs used for training are sampled randomly, which\nindicates that the semantics between negative pairs may be related or even\nequivalent, while most methods still enforce dissimilar representations to\ndecrease their similarity. This phenomenon leads to inaccurate supervision and\npoor performance in learning video-text representations.\n  While most video retrieval methods overlook that phenomenon, we propose an\nadaptive margin changed with the distance between positive and negative pairs\nto solve the aforementioned issue. First, we design the calculation framework\nof the adaptive margin, including the method of distance measurement and the\nfunction between the distance and the margin. Then, we explore a novel\nimplementation called \"Cross-Modal Generalized Self-Distillation\" (CMGSD),\nwhich can be built on the top of most video retrieval models with few\nmodifications. Notably, CMGSD adds few computational overheads at train time\nand adds no computational overhead at test time. Experimental results on three\nwidely used datasets demonstrate that the proposed method can yield\nsignificantly better performance than the corresponding backbone model, and it\noutperforms state-of-the-art methods by a large margin.", "published": "2023-03-09 08:07:38", "link": "http://arxiv.org/abs/2303.05093v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ESCL: Equivariant Self-Contrastive Learning for Sentence Representations", "abstract": "Previous contrastive learning methods for sentence representations often\nfocus on insensitive transformations to produce positive pairs, but neglect the\nrole of sensitive transformations that are harmful to semantic representations.\nTherefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to\nmake full use of sensitive transformations, which encourages the learned\nrepresentations to be sensitive to certain types of transformations with an\nadditional equivariant learning task. Meanwhile, in order to improve\npracticability and generality, ESCL simplifies the implementations of\ntraditional equivariant contrastive methods to share model parameters from the\nperspective of multi-task learning. We evaluate our ESCL on semantic textual\nsimilarity tasks. The proposed method achieves better results while using fewer\nlearning parameters compared to previous methods.", "published": "2023-03-09 09:52:28", "link": "http://arxiv.org/abs/2303.05143v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural\n  Retrieval on Entity-centric Questions?", "abstract": "Neural document retrievers, including dense passage retrieval (DPR), have\noutperformed classical lexical-matching retrievers, such as BM25, when\nfine-tuned and tested on specific question-answering datasets. However, it has\nbeen shown that the existing dense retrievers do not generalize well not only\nout of domain but even in domain such as Wikipedia, especially when a named\nentity in a question is a dominant clue for retrieval. In this paper, we\npropose an approach toward in-domain generalization using the embeddings\ngenerated by the frozen language model trained with the entities in the domain.\nBy not fine-tuning, we explore the possibility that the rich knowledge\ncontained in a pretrained language model can be used for retrieval tasks. The\nproposed method outperforms conventional DPRs on entity-centric questions in\nWikipedia domain and achieves almost comparable performance to BM25 and\nstate-of-the-art SPAR model. We also show that the contextualized keys lead to\nstrong improvements compared to BM25 when the entity names consist of common\nwords. Our results demonstrate the feasibility of the zero-shot retrieval\nmethod for entity-centric questions of Wikipedia domain, where DPR has\nstruggled to perform.", "published": "2023-03-09 10:12:18", "link": "http://arxiv.org/abs/2303.05153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\u03c0$-augmented pregroups and applications to linguistics", "abstract": "We enrich pregroups with a mapping which allows us to locally apply precyclic\npermutations to designated substrings. We prove a normalisation theorem for\nsuch algebraic structures and briefly formalise some known applications of\npregroups to the analysis of clitic pronouns in certain natural languages.", "published": "2023-03-09 10:33:31", "link": "http://arxiv.org/abs/2303.05160v1", "categories": ["cs.CL", "math.LO"], "primary_category": "cs.CL"}
{"title": "SEAM: An Integrated Activation-Coupled Model of Sentence Processing and\n  Eye Movements in Reading", "abstract": "Models of eye-movement control during reading, developed largely within\npsychology, usually focus on visual, attentional, lexical, and motor processes\nbut neglect post-lexical language processing; by contrast, models of sentence\ncomprehension processes, developed largely within psycholinguistics, generally\nfocus only on post-lexical language processes. We present a model that combines\nthese two research threads, by integrating eye-movement control and sentence\nprocessing. Developing such an integrated model is extremely challenging and\ncomputationally demanding, but such an integration is an important step toward\ncomplete mathematical models of natural language comprehension in reading. We\ncombine the SWIFT model of eye-movement control (Seelig et al., 2020,\ndoi:10.1016/j.jmp.2019.102313) with key components of the Lewis and Vasishth\nsentence processing model (Lewis & Vasishth, 2005,\ndoi:10.1207/s15516709cog0000_25). This integration becomes possible, for the\nfirst time, due in part to recent advances in successful parameter\nidentification in dynamical models, which allows us to investigate profile\nlog-likelihoods for individual model parameters. We present a fully implemented\nproof-of-concept model demonstrating how such an integrated model can be\nachieved; our approach includes Bayesian model inference with Markov Chain\nMonte Carlo (MCMC) sampling as a key computational tool. The integrated\nSentence-Processing and Eye-Movement Activation-Coupled Model (SEAM) can\nsuccessfully reproduce eye movement patterns that arise due to similarity-based\ninterference in reading. To our knowledge, this is the first-ever integration\nof a complete process model of eye-movement control with linguistic dependency\ncompletion processes in sentence comprehension. In future work, this proof of\nconcept model will need to be evaluated using a comprehensive set of benchmark\ndata.", "published": "2023-03-09 12:50:34", "link": "http://arxiv.org/abs/2303.05221v4", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup\n  for Visual Speech Translation and Recognition", "abstract": "Multi-media communications facilitate global interaction among people.\nHowever, despite researchers exploring cross-lingual translation techniques\nsuch as machine translation and audio speech translation to overcome language\nbarriers, there is still a shortage of cross-lingual studies on visual speech.\nThis lack of research is mainly due to the absence of datasets containing\nvisual speech and translated text pairs. In this paper, we present\n\\textbf{AVMuST-TED}, the first dataset for \\textbf{A}udio-\\textbf{V}isual\n\\textbf{Mu}ltilingual \\textbf{S}peech \\textbf{T}ranslation, derived from\n\\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as\naudio speech, making it difficult to develop a mapping from source speech\nphonemes to the target language text. To address this issue, we propose\nMixSpeech, a cross-modality self-learning framework that utilizes audio speech\nto regularize the training of visual speech tasks. To further minimize the\ncross-modality gap and its impact on knowledge transfer, we suggest adopting\nmixed speech, which is created by interpolating audio and visual streams, along\nwith a curriculum learning strategy to adjust the mixing ratio as needed.\nMixSpeech enhances speech translation in noisy environments, improving BLEU\nscores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves\nstate-of-the-art performance in lip reading on CMLR (11.1\\%), LRS2 (25.5\\%),\nand LRS3 (28.0\\%).", "published": "2023-03-09 14:58:29", "link": "http://arxiv.org/abs/2303.05309v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Refined Vision-Language Modeling for Fine-grained Multi-modal\n  Pre-training", "abstract": "Fine-grained supervision based on object annotations has been widely used for\nvision and language pre-training (VLP). However, in real-world application\nscenarios, aligned multi-modal data is usually in the image-caption format,\nwhich only provides coarse-grained supervision. It is not only cost-expensive\nbut also compute-expensive to collect object annotations and build object\nannotation pre-extractor for different scenarios. In this paper, we propose a\nfine-grained VLP scheme without object annotations from the linguistic\nperspective. First, we propose a homonym sentence rewriting (HSR) algorithm to\nprovide token-level supervision. The algorithm replaces a\nverb/noun/adjective/quantifier word of the caption with its homonyms from\nWordNet. Correspondingly, we propose refined vision-language modeling (RVLM)\nframework to exploit the token-level supervision. Three refined tasks, i.e.,\nrefined image-text contrastive (RITC), refined image-text matching (RITM), and\nreplace language modeling (RLM) are proposed to learn the fine-grained\nalignment. Extensive experiments on several downstream tasks demonstrate the\nsuperior performance of the proposed method.", "published": "2023-03-09 15:01:12", "link": "http://arxiv.org/abs/2303.05313v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Personalisation within bounds: A risk taxonomy and policy framework for\n  the alignment of large language models with personalised feedback", "abstract": "Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.", "published": "2023-03-09 17:52:07", "link": "http://arxiv.org/abs/2303.05453v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Open World Classification with Adaptive Negative Samples", "abstract": "Open world classification is a task in natural language processing with key\npractical relevance and impact. Since the open or {\\em unknown} category data\nonly manifests in the inference phase, finding a model with a suitable decision\nboundary accommodating for the identification of known classes and\ndiscrimination of the open category is challenging. The performance of existing\nmodels is limited by the lack of effective open category data during the\ntraining stage or the lack of a good mechanism to learn appropriate decision\nboundaries. We propose an approach based on \\underline{a}daptive\n\\underline{n}egative \\underline{s}amples (ANS) designed to generate effective\nsynthetic open category samples in the training stage and without requiring any\nprior knowledge or external datasets. Empirically, we find a significant\nadvantage in using auxiliary one-versus-rest binary classifiers, which\neffectively utilize the generated negative samples and avoid the complex\nthreshold-seeking stage in previous works. Extensive experiments on three\nbenchmark datasets show that ANS achieves significant improvements over\nstate-of-the-art methods.", "published": "2023-03-09 21:12:46", "link": "http://arxiv.org/abs/2303.05581v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BeamAttack: Generating High-quality Textual Adversarial Examples through\n  Beam Search and Mixed Semantic Spaces", "abstract": "Natural language processing models based on neural networks are vulnerable to\nadversarial examples. These adversarial examples are imperceptible to human\nreaders but can mislead models to make the wrong predictions. In a black-box\nsetting, attacker can fool the model without knowing model's parameters and\narchitecture. Previous works on word-level attacks widely use single semantic\nspace and greedy search as a search strategy. However, these methods fail to\nbalance the attack success rate, quality of adversarial examples and time\nconsumption. In this paper, we propose BeamAttack, a textual attack algorithm\nthat makes use of mixed semantic spaces and improved beam search to craft\nhigh-quality adversarial examples. Extensive experiments demonstrate that\nBeamAttack can improve attack success rate while saving numerous queries and\ntime, e.g., improving at most 7\\% attack success rate than greedy search when\nattacking the examples from MR dataset. Compared with heuristic search,\nBeamAttack can save at most 85\\% model queries and achieve a competitive attack\nsuccess rate. The adversarial examples crafted by BeamAttack are highly\ntransferable and can effectively improve model's robustness during adversarial\ntraining. Code is available at\nhttps://github.com/zhuhai-ustc/beamattack/tree/master", "published": "2023-03-09 03:30:52", "link": "http://arxiv.org/abs/2303.07199v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Learning of Natural Language to Linear Temporal Logic\n  Translators for Robot Task Specification", "abstract": "To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.", "published": "2023-03-09 00:09:58", "link": "http://arxiv.org/abs/2303.08006v2", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions\n  about Code", "abstract": "We analyzed effectiveness of three generative pre-trained transformer (GPT)\nmodels in answering multiple-choice question (MCQ) assessments, often involving\nshort snippets of code, from introductory and intermediate programming courses\nat the postsecondary level. This emerging technology stirs countless\ndiscussions of its potential uses (e.g., exercise generation, code explanation)\nas well as misuses in programming education (e.g., cheating). However, the\ncapabilities of GPT models and their limitations to reason about and/or analyze\ncode in educational settings have been under-explored. We evaluated several\nOpenAI's GPT models on formative and summative MCQ assessments from three\nPython courses (530 questions). We found that MCQs containing code snippets are\nnot answered as successfully as those that only contain natural language. While\nquestions requiring to fill-in a blank in the code or completing a natural\nlanguage statement about the snippet are handled rather successfully, MCQs that\nrequire analysis and/or reasoning about the code (e.g., what is true/false\nabout the snippet, or what is its output) appear to be the most challenging.\nThese findings can be leveraged by educators to adapt their instructional\npractices and assessments in programming courses, so that GPT becomes a\nvaluable assistant for a learner as opposed to a source of confusion and/or\npotential hindrance in the learning process.", "published": "2023-03-09 16:52:12", "link": "http://arxiv.org/abs/2303.08033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test\n  Questions", "abstract": "Recently, more and more people study online for the convenience of access to\nmassive learning materials (e.g. test questions/notes), thus accurately\nunderstanding learning materials became a crucial issue, which is essential for\nmany educational applications. Previous studies focus on using language models\nto represent the question data. However, test questions (TQ) are usually\nheterogeneous and multi-modal, e.g., some of them may only contain text, while\nothers half contain images with information beyond their literal description.\nIn this context, both supervised and unsupervised methods are difficult to\nlearn a fused representation of questions. Meanwhile, this problem cannot be\nsolved by conventional methods such as image caption, as the images may contain\ninformation complementary rather than duplicate to the text. In this paper, we\nfirst improve previous text-only representation with a two-stage unsupervised\ninstance level contrastive based pre-training method (MCL: Mixture Unsupervised\nContrastive Learning). Then, TQ-Net was proposed to fuse the content of images\nto the representation of heterogeneous data. Finally, supervised contrastive\nlearning was conducted on relevance prediction-related downstream tasks, which\nhelped the model to learn the representation of questions effectively. We\nconducted extensive experiments on question-based tasks on large-scale,\nreal-world datasets, which demonstrated the effectiveness of TQ-Net and improve\nthe precision of downstream applications (e.g. similar questions +2.02% and\nknowledge point prediction +7.20%). Our code will be available, and we will\nopen-source a subset of our data to promote the development of relative\nstudies.", "published": "2023-03-09 10:55:48", "link": "http://arxiv.org/abs/2303.08039v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Dynamic Stashing Quantization for Efficient Transformer Training", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on a\nrange of Natural Language Processing (NLP) tasks. Unfortunately, the immense\namount of computations and memory accesses required for LLM training makes them\nprohibitively expensive in terms of hardware cost, and thus challenging to\ndeploy in use cases such as on-device learning. In this paper, motivated by the\nobservation that LLM training is memory-bound, we propose a novel dynamic\nquantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a\nspecial focus on reducing the memory operations, but also enjoys the other\nbenefits of low precision training, such as the reduced arithmetic cost. We\nconduct a thorough study on two translation tasks (trained-from-scratch) and\nthree classification tasks (fine-tuning). DSQ reduces the amount of arithmetic\noperations by $20.95\\times$ and the number of DRAM operations by $2.55\\times$\non IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in\non-device learning.", "published": "2023-03-09 14:44:31", "link": "http://arxiv.org/abs/2303.05295v1", "categories": ["cs.LG", "cs.CL", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data", "abstract": "Advanced large language models like ChatGPT have gained considerable\nattention recently, including among students. However, while the debate on\nChatGPT in academia is making waves, more understanding is needed among\nlecturers and teachers on how students use and perceive ChatGPT. To address\nthis gap, we analyzed the content on ChatGPT available on TikTok in February\n2023. TikTok is a rapidly growing social media platform popular among\nindividuals under 30. Specifically, we analyzed the content of the 100 most\npopular videos in English tagged with #chatgpt, which collectively garnered\nover 250 million views. Most of the videos we studied promoted the use of\nChatGPT for tasks like writing essays or code. In addition, many videos\ndiscussed AI detectors, with a focus on how other tools can help to transform\nChatGPT output to fool these detectors. This also mirrors the discussion among\neducators on how to treat ChatGPT as lecturers and teachers in teaching and\ngrading. What is, however, missing from the analyzed clips on TikTok are videos\nthat discuss ChatGPT producing content that is nonsensical or unfaithful to the\ntraining data.", "published": "2023-03-09 15:46:54", "link": "http://arxiv.org/abs/2303.05349v1", "categories": ["stat.AP", "cs.CL", "cs.CY"], "primary_category": "stat.AP"}
{"title": "Planning with Large Language Models for Code Generation", "abstract": "Existing large language model-based code generation pipelines typically use\nbeam search or sampling algorithms during the decoding process. Although the\nprograms they generate achieve high token-matching-based scores, they often\nfail to compile or generate incorrect outputs. The main reason is that\nconventional Transformer decoding algorithms may not be the best choice for\ncode generation. In this work, we propose a novel Transformer decoding\nalgorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning\nalgorithm to do lookahead search and guide the Transformer to generate better\nprograms. Specifically, instead of simply optimizing the likelihood of the\ngenerated sequences, the Transformer makes use of a planner to generate\ncandidate programs and test them on public test cases. The Transformer can\ntherefore make more informed decisions and generate tokens that will eventually\nlead to higher-quality programs. We also design a mechanism that shares\ninformation between the Transformer and the planner to make our algorithm\ncomputationally efficient. We empirically evaluate our framework with several\nlarge language models as backbones on public coding challenge benchmarks,\nshowing that 1) it can generate programs that consistently achieve higher\nperformance compared with competing baseline methods; 2) it enables\ncontrollable code generation, such as concise codes and highly-commented codes\nby optimizing modified objective.", "published": "2023-03-09 18:59:47", "link": "http://arxiv.org/abs/2303.05510v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "On the Robustness of Text Vectorizers", "abstract": "A fundamental issue in machine learning is the robustness of the model with\nrespect to changes in the input. In natural language processing, models\ntypically contain a first embedding layer, transforming a sequence of tokens\ninto vector representations. While the robustness with respect to changes of\ncontinuous inputs is well-understood, the situation is less clear when\nconsidering discrete changes, for instance replacing a word by another in an\ninput sentence. Our work formally proves that popular embedding schemes, such\nas concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit\nrobustness in the H\\\"older or Lipschitz sense with respect to the Hamming\ndistance. We provide quantitative bounds for these schemes and demonstrate how\nthe constants involved are affected by the length of the document. These\nfindings are exemplified through a series of numerical examples.", "published": "2023-03-09 16:37:37", "link": "http://arxiv.org/abs/2303.07203v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical\n  Text Reports", "abstract": "Electrocardiogram (ECG) synthesis is the area of research focused on\ngenerating realistic synthetic ECG signals for medical use without concerns\nover annotation costs or clinical data privacy restrictions. Traditional ECG\ngeneration models consider a single ECG lead and utilize GAN-based generative\nmodels. These models can only generate single lead samples and require separate\ntraining for each diagnosis class. The diagnosis classes of ECGs are\ninsufficient to capture the intricate differences between ECGs depending on\nvarious features (e.g. patient demographic details, co-existing diagnosis\nclasses, etc.). To alleviate these challenges, we present a text-to-ECG task,\nin which textual inputs are used to produce ECG outputs. Then we propose\nAuto-TTE, an autoregressive generative model conditioned on clinical text\nreports to synthesize 12-lead ECGs, for the first time to our knowledge. We\ncompare the performance of our model with other representative models in\ntext-to-speech and text-to-image. Experimental results show the superiority of\nour model in various quantitative evaluations and qualitative analysis.\nFinally, we conduct a user study with three board-certified cardiologists to\nconfirm the fidelity and semantic alignment of generated samples. our code will\nbe available at https://github.com/TClife/text_to_ecg", "published": "2023-03-09 11:58:38", "link": "http://arxiv.org/abs/2303.09395v1", "categories": ["cs.CL", "cs.LG", "eess.SP"], "primary_category": "cs.CL"}
{"title": "hierarchical network with decoupled knowledge distillation for speech\n  emotion recognition", "abstract": "The goal of Speech Emotion Recognition (SER) is to enable computers to\nrecognize the emotion category of a given utterance in the same way that humans\ndo. The accuracy of SER is strongly dependent on the validity of the\nutterance-level representation obtained by the model. Nevertheless, the ``dark\nknowledge\" carried by non-target classes is always ignored by previous studies.\nIn this paper, we propose a hierarchical network, called DKDFMH, which employs\ndecoupled knowledge distillation in a deep convolutional neural network with a\nfused multi-head attention mechanism. Our approach applies logit distillation\nto obtain higher-level semantic features from different scales of attention\nsets and delve into the knowledge carried by non-target classes, thus guiding\nthe model to focus more on the differences between sentiment features. To\nvalidate the effectiveness of our model, we conducted experiments on the\nInteractive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved\ncompetitive performance, with 79.1% weighted accuracy (WA) and 77.1% unweighted\naccuracy (UA). To the best of our knowledge, this is the first time since 2015\nthat logit distillation has been returned to state-of-the-art status.", "published": "2023-03-09 09:40:45", "link": "http://arxiv.org/abs/2303.05134v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "X-SepFormer: End-to-end Speaker Extraction Network with Explicit\n  Optimization on Speaker Confusion", "abstract": "Target speech extraction (TSE) systems are designed to extract target speech\nfrom a multi-talker mixture. The popular training objective for most prior TSE\nnetworks is to enhance reconstruction performance of extracted speech waveform.\nHowever, it has been reported that a TSE system delivers high reconstruction\nperformance may still suffer low-quality experience problems in practice. One\nsuch experience problem is wrong speaker extraction (called speaker confusion,\nSC), which leads to strong negative experience and hampers effective\nconversations. To mitigate the imperative SC issue, we reformulate the training\nobjective and propose two novel loss schemes that explore the metric of\nreconstruction improvement performance defined at small chunk-level and\nleverage the metric associated distribution information. Both loss schemes aim\nto encourage a TSE network to pay attention to those SC chunks based on the\nsaid distribution information. On this basis, we present X-SepFormer, an\nend-to-end TSE model with proposed loss schemes and a backbone of SepFormer.\nExperimental results on the benchmark WSJ0-2mix dataset validate the\neffectiveness of our proposals, showing consistent improvements on SC errors\n(by 14.8% relative). Moreover, with SI-SDRi of 19.4 dB and PESQ of 3.81, our\nbest system significantly outperforms the current SOTA systems and offers the\ntop TSE results reported till date on the WSJ0-2mix.", "published": "2023-03-09 04:00:29", "link": "http://arxiv.org/abs/2303.05023v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WASD: A Wilder Active Speaker Detection Dataset", "abstract": "Current Active Speaker Detection (ASD) models achieve great results on\nAVA-ActiveSpeaker (AVA), using only sound and facial features. Although this\napproach is applicable in movie setups (AVA), it is not suited for less\nconstrained conditions. To demonstrate this limitation, we propose a Wilder\nActive Speaker Detection (WASD) dataset, with increased difficulty by targeting\nthe two key components of current ASD: audio and face. Grouped into 5\ncategories, ranging from optimal conditions to surveillance settings, WASD\ncontains incremental challenges for ASD with tactical impairment of audio and\nface data. We select state-of-the-art models and assess their performance in\ntwo groups of WASD: Easy (cooperative settings) and Hard (audio and/or face are\nspecifically degraded). The results show that: 1) AVA trained models maintain a\nstate-of-the-art performance in WASD Easy group, while underperforming in the\nHard one, showing the 2) similarity between AVA and Easy data; and 3) training\nin WASD does not improve models performance to AVA levels, particularly for\naudio impairment and surveillance settings. This shows that AVA does not\nprepare models for wild ASD and current approaches are subpar to deal with such\nconditions. The proposed dataset also contains body data annotations to provide\na new source for ASD, and is available at https://github.com/Tiago-Roxo/WASD.", "published": "2023-03-09 15:13:22", "link": "http://arxiv.org/abs/2303.05321v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Improving Few-Shot Learning for Talking Face System with TTS Data\n  Augmentation", "abstract": "Audio-driven talking face has attracted broad interest from academia and\nindustry recently. However, data acquisition and labeling in audio-driven\ntalking face are labor-intensive and costly. The lack of data resource results\nin poor synthesis effect. To alleviate this issue, we propose to use TTS\n(Text-To-Speech) for data augmentation to improve few-shot ability of the\ntalking face system. The misalignment problem brought by the TTS audio is\nsolved with the introduction of soft-DTW, which is first adopted in the talking\nface task. Moreover, features extracted by HuBERT are explored to utilize\nunderlying information of audio, and found to be superior over other features.\nThe proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score\nand user study preference repectively over the baseline model, which shows the\neffectiveness of improving few-shot learning for talking face system with TTS\naugmentation.", "published": "2023-03-09 15:13:46", "link": "http://arxiv.org/abs/2303.05322v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual\n  Fine-Grained Learning", "abstract": "Audio-visual learning helps to comprehensively understand the world by fusing\npractical information from multiple modalities. However, recent studies show\nthat the imbalanced optimization of uni-modal encoders in a joint-learning\nmodel is a bottleneck to enhancing the model's performance. We further find\nthat the up-to-date imbalance-mitigating methods fail on some audio-visual\nfine-grained tasks, which have a higher demand for distinguishable feature\ndistribution. Fueled by the success of cosine loss that builds hyperspherical\nfeature spaces and achieves lower intra-class angular variability, this paper\nproposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$\nnormalization to features and weights towards balanced and better multi-modal\nfine-grained learning. We demonstrate that our method can alleviate the\nimbalanced optimization from the perspective of weight norm and fully exploit\nthe discriminability of the cosine metric. Extensive experiments prove the\neffectiveness of our method and the versatility with advanced multi-modal\nfusion strategies and up-to-date imbalance-mitigating methods.", "published": "2023-03-09 15:34:36", "link": "http://arxiv.org/abs/2303.05338v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Image-in-Audio Deep Steganography", "abstract": "The field of steganography has experienced a surge of interest due to the\nrecent advancements in AI-powered techniques, particularly in the context of\nmultimodal setups that enable the concealment of signals within signals of a\ndifferent nature. The primary objectives of all steganographic methods are to\nachieve perceptual transparency, robustness, and large embedding capacity -\nwhich often present conflicting goals that classical methods have struggled to\nreconcile. This paper extends and enhances an existing image-in-audio deep\nsteganography method by focusing on improving its robustness. The proposed\nenhancements include modifications to the loss function, utilization of the\nShort-Time Fourier Transform (STFT), introduction of redundancy in the encoding\nprocess for error correction, and buffering of additional information in the\npixel subconvolution operation. The results demonstrate that our approach\noutperforms the existing method in terms of robustness and perceptual\ntransparency.", "published": "2023-03-09 03:16:04", "link": "http://arxiv.org/abs/2303.05007v2", "categories": ["cs.CR", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68T99", "I.4.9; I.2.m"], "primary_category": "cs.CR"}
