{"title": "Effective Few-Shot Named Entity Linking by Meta-Learning", "abstract": "Entity linking aims to link ambiguous mentions to their corresponding\nentities in a knowledge base, which is significant and fundamental for various\ndownstream applications, e.g., knowledge base completion, question answering,\nand information extraction. While great efforts have been devoted to this task,\nmost of these studies follow the assumption that large-scale labeled data is\navailable. However, when the labeled data is insufficient for specific domains\ndue to labor-intensive annotation work, the performance of existing algorithms\nwill suffer an intolerable decline. In this paper, we endeavor to solve the\nproblem of few-shot entity linking, which only requires a minimal amount of\nin-domain labeled data and is more practical in real situations. Specifically,\nwe firstly propose a novel weak supervision strategy to generate non-trivial\nsynthetic entity-mention pairs based on mention rewriting. Since the quality of\nthe synthetic data has a critical impact on effective model training, we\nfurther design a meta-learning mechanism to assign different weights to each\nsynthetic entity-mention pair automatically. Through this way, we can\nprofoundly exploit rich and precious semantic information to derive a\nwell-trained entity linking model under the few-shot setting. The experiments\non real-world datasets show that the proposed method can extensively improve\nthe state-of-the-art few-shot entity linking model and achieve impressive\nperformance when only a small amount of labeled data is available. Moreover, we\nalso demonstrate the outstanding ability of the model's transferability.", "published": "2022-07-12 03:23:02", "link": "http://arxiv.org/abs/2207.05280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for\n  Speech Recognition", "abstract": "The Huqariq corpus is a multilingual collection of speech from native\nPeruvian languages. The transcribed corpus is intended for the research and\ndevelopment of speech technologies to preserve endangered languages in Peru.\nHuqariq is primarily designed for the development of automatic speech\nrecognition, language identification and text-to-speech tools. In order to\nachieve corpus collection sustainably, we employ the crowdsourcing methodology.\nHuqariq includes four native languages of Peru, and it is expected that by the\nend of the year 2022, it can reach up to 20 native languages out of the 48\nnative languages in Peru. The corpus has 220 hours of transcribed audio\nrecorded by more than 500 volunteers, making it the largest speech corpus for\nnative languages in Peru. In order to verify the quality of the corpus, we\npresent speech recognition experiments using 220 hours of fully transcribed\naudio.", "published": "2022-07-12 12:37:12", "link": "http://arxiv.org/abs/2207.05498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Paraphrases to Study Properties of Contextual Embeddings", "abstract": "We use paraphrases as a unique source of data to analyze contextualized\nembeddings, with a particular focus on BERT. Because paraphrases naturally\nencode consistent word and phrase semantics, they provide a unique lens for\ninvestigating properties of embeddings. Using the Paraphrase Database's\nalignments, we study words within paraphrases as well as phrase\nrepresentations. We find that contextual embeddings effectively handle\npolysemous words, but give synonyms surprisingly different representations in\nmany cases. We confirm previous findings that BERT is sensitive to word order,\nbut find slightly different patterns than prior work in terms of the level of\ncontextualization across BERT's layers.", "published": "2022-07-12 14:22:05", "link": "http://arxiv.org/abs/2207.05553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The expected sum of edge lengths in planar linearizations of trees.\n  Theory and applications", "abstract": "Dependency trees have proven to be a very successful model to represent the\nsyntactic structure of sentences of human languages. In these structures,\nvertices are words and edges connect syntactically-dependent words. The\ntendency of these dependencies to be short has been demonstrated using random\nbaselines for the sum of the lengths of the edges or its variants. A ubiquitous\nbaseline is the expected sum in projective orderings (wherein edges do not\ncross and the root word of the sentence is not covered by any edge), that can\nbe computed in time $O(n)$. Here we focus on a weaker formal constraint, namely\nplanarity. In the theoretical domain, we present a characterization of\nplanarity that, given a sentence, yields either the number of planar\npermutations or an efficient algorithm to generate uniformly random planar\npermutations of the words. We also show the relationship between the expected\nsum in planar arrangements and the expected sum in projective arrangements. In\nthe domain of applications, we derive a $O(n)$-time algorithm to calculate the\nexpected value of the sum of edge lengths. We also apply this research to a\nparallel corpus and find that the gap between actual dependency distance and\nthe random baseline reduces as the strength of the formal constraint on\ndependency structures increases, suggesting that formal constraints absorb part\nof the dependency distance minimization effect. Our research paves the way for\nreplicating past research on dependency distance minimization using random\nplanar linearizations as random baseline.", "published": "2022-07-12 14:35:07", "link": "http://arxiv.org/abs/2207.05564v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-lingual Transfer is Under-specified Optimization", "abstract": "Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but\noften produce unreliable models that exhibit high performance variance on the\ntarget language. We postulate that this high variance results from zero-shot\ncross-lingual transfer solving an under-specified optimization problem. We show\nthat any linear-interpolated model between the source language monolingual\nmodel and source + target bilingual model has equally low source language\ngeneralization error, yet the target language generalization error reduces\nsmoothly and linearly as we move from the monolingual to bilingual model,\nsuggesting that the model struggles to identify good solutions for both source\nand target languages using the source language alone. Additionally, we show\nthat zero-shot solution lies in non-flat region of target language error\ngeneralization surface, causing the high variance.", "published": "2022-07-12 16:49:28", "link": "http://arxiv.org/abs/2207.05666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do Multilingual Encoders Learn Cross-lingual Representation?", "abstract": "NLP systems typically require support for more than one language. As\ndifferent languages have different amounts of supervision, cross-lingual\ntransfer benefits languages with little to no training data by transferring\nfrom other languages. From an engineering perspective, multilingual NLP\nbenefits development and maintenance by serving multiple languages with a\nsingle system. Both cross-lingual transfer and multilingual NLP rely on\ncross-lingual representations serving as the foundation. As BERT revolutionized\nrepresentation learning and NLP, it also revolutionized cross-lingual\nrepresentations and cross-lingual transfer. Multilingual BERT was released as a\nreplacement for single-language BERT, trained with Wikipedia data in 104\nlanguages.\n  Surprisingly, without any explicit cross-lingual signal, multilingual BERT\nlearns cross-lingual representations in addition to representations for\nindividual languages. This thesis first shows such surprising cross-lingual\neffectiveness compared against prior art on various tasks. Naturally, it raises\na set of questions, most notably how do these multilingual encoders learn\ncross-lingual representations. In exploring these questions, this thesis will\nanalyze the behavior of multilingual models in a variety of settings on high\nand low resource languages. We also look at how to inject different\ncross-lingual signals into multilingual encoders, and the optimization behavior\nof cross-lingual transfer with these models. Together, they provide a better\nunderstanding of multilingual encoders on cross-lingual transfer. Our findings\nwill lead us to suggested improvements to multilingual encoders and\ncross-lingual transfer.", "published": "2022-07-12 17:57:05", "link": "http://arxiv.org/abs/2207.05737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sockeye 3: Fast Neural Machine Translation with PyTorch", "abstract": "Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine\nTranslation (NMT). Now based on PyTorch, Sockeye 3 provides faster model\nimplementations and more advanced features with a further streamlined codebase.\nThis enables broader experimentation with faster iteration, efficient training\nof stronger and faster models, and the flexibility to move new ideas quickly\nfrom research to production. When running comparable models, Sockeye 3 is up to\n126% faster than other PyTorch implementations on GPUs and up to 292% faster on\nCPUs. Sockeye 3 is open source software released under the Apache 2.0 license.", "published": "2022-07-12 21:29:35", "link": "http://arxiv.org/abs/2207.05851v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel DeBERTa-based Model for Financial Question Answering Task", "abstract": "As a rising star in the field of natural language processing, question\nanswering systems (Q&A Systems) are widely used in all walks of life. Compared\nwith other scenarios, the applicationin financial scenario has strong\nrequirements in the traceability and interpretability of the Q&A systems. In\naddition, since the demand for artificial intelligence technology has gradually\nshifted from the initial computational intelligence to cognitive intelligence,\nthis research mainly focuses on the financial numerical reasoning dataset -\nFinQA. In the shared task, the objective is to generate the reasoning program\nand the final answer according to the given financial report containing text\nand tables. We use the method based on DeBERTa pre-trained language model, with\nadditional optimization methods including multi-model fusion, training set\ncombination on this basis. We finally obtain an execution accuracy of 68.99 and\na program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.", "published": "2022-07-12 22:34:39", "link": "http://arxiv.org/abs/2207.05875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Table Question Answering: Recent Advances", "abstract": "Table Question Answering (Table QA) refers to providing precise answers from\ntables to answer a user's question. In recent years, there have been a lot of\nworks on table QA, but there is a lack of comprehensive surveys on this\nresearch topic. Hence, we aim to provide an overview of available datasets and\nrepresentative methods in table QA. We classify existing methods for table QA\ninto five categories according to their techniques, which include\nsemantic-parsing-based, generative, extractive, matching-based, and\nretriever-reader-based methods. Moreover, as table QA is still a challenging\ntask for existing methods, we also identify and outline several key challenges\nand discuss the potential future directions of table QA.", "published": "2022-07-12 02:44:40", "link": "http://arxiv.org/abs/2207.05270v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PLM-ICD: Automatic ICD Coding with Pretrained Language Models", "abstract": "Automatically classifying electronic health records (EHRs) into diagnostic\ncodes has been challenging to the NLP community. State-of-the-art methods\ntreated this problem as a multilabel classification problem and proposed\nvarious architectures to model this problem. However, these systems did not\nleverage the superb performance of pretrained language models, which achieved\nsuperb performance on natural language understanding tasks. Prior work has\nshown that pretrained language models underperformed on this task with the\nregular finetuning scheme. Therefore, this paper aims at analyzing the causes\nof the underperformance and developing a framework for automatic ICD coding\nwith pretrained language models. We spotted three main issues through the\nexperiments: 1) large label space, 2) long input sequences, and 3) domain\nmismatch between pretraining and fine-tuning. We propose PLMICD, a framework\nthat tackles the challenges with various strategies. The experimental results\nshow that our proposed framework can overcome the challenges and achieves\nstate-of-the-art performance in terms of multiple metrics on the benchmark\nMIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD", "published": "2022-07-12 03:56:28", "link": "http://arxiv.org/abs/2207.05289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum\n  Machines", "abstract": "Near-term quantum systems tend to be noisy. Crosstalk noise has been\nrecognized as one of several major types of noises in superconducting Noisy\nIntermediate-Scale Quantum (NISQ) devices. Crosstalk arises from the concurrent\nexecution of two-qubit gates on nearby qubits, such as \\texttt{CX}. It might\nsignificantly raise the error rate of gates in comparison to running them\nindividually. Crosstalk can be mitigated through scheduling or hardware machine\ntuning. Prior scientific studies, however, manage crosstalk at a really late\nphase in the compilation process, usually after hardware mapping is done. It\nmay miss great opportunities of optimizing algorithm logic, routing, and\ncrosstalk at the same time. In this paper, we push the envelope by considering\nall these factors simultaneously at the very early compilation stage. We\npropose a crosstalk-aware quantum program compilation framework called CQC that\ncan enhance crosstalk mitigation while achieving satisfactory circuit depth.\nMoreover, we identify opportunities for translation from intermediate\nrepresentation to the circuit for application-specific crosstalk mitigation,\nfor instance, the \\texttt{CX} ladder construction in variational quantum\neigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices\nshow that our framework can significantly reduce the error rate by up to\n6$\\times$, with only $\\sim$60\\% circuit depth compared to state-of-the-art gate\nscheduling approaches. In particular, for VQE, we demonstrate 49\\% circuit\ndepth reduction with 9.6\\% fidelity improvement over prior art on the H4\nmolecule using IBMQ Guadalupe. Our CQC framework will be released on GitHub.", "published": "2022-07-12 04:11:05", "link": "http://arxiv.org/abs/2207.05751v3", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Building Korean Sign Language Augmentation (KoSLA) Corpus with Data\n  Augmentation Technique", "abstract": "We present an efficient framework of corpus for sign language translation.\nAided with a simple but dramatic data augmentation technique, our method\nconverts text into annotated forms with minimum information loss. Sign\nlanguages are composed of manual signals, non-manual signals, and iconic\nfeatures. According to professional sign language interpreters, non-manual\nsignals such as facial expressions and gestures play an important role in\nconveying exact meaning. By considering the linguistic features of sign\nlanguage, our proposed framework is a first and unique attempt to build a\nmultimodal sign language augmentation corpus (hereinafter referred to as the\nKoSLA corpus) containing both manual and non-manual modalities. The corpus we\nbuilt demonstrates confident results in the hospital context, showing improved\nperformance with augmented datasets. To overcome data scarcity, we resorted to\ndata augmentation techniques such as synonym replacement to boost the\nefficiency of our translation model and available data, while maintaining\ngrammatical and semantic structures of sign language. For the experimental\nsupport, we verify the effectiveness of data augmentation technique and\nusefulness of our corpus by performing a translation task between normal\nsentences and sign language annotations on two tokenizers. The result was\nconvincing, proving that the BLEU scores with the KoSLA corpus were\nsignificant.", "published": "2022-07-12 02:12:36", "link": "http://arxiv.org/abs/2207.05261v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CompoundE: Knowledge Graph Embedding with Translation, Rotation and\n  Scaling Compound Operations", "abstract": "Translation, rotation, and scaling are three commonly used geometric\nmanipulation operations in image processing. Besides, some of them are\nsuccessfully used in developing effective knowledge graph embedding (KGE)\nmodels such as TransE and RotatE. Inspired by the synergy, we propose a new KGE\nmodel by leveraging all three operations in this work. Since translation,\nrotation, and scaling operations are cascaded to form a compound one, the new\nmodel is named CompoundE. By casting CompoundE in the framework of group\ntheory, we show that quite a few scoring-function-based KGE models are special\ncases of CompoundE. CompoundE extends the simple distance-based relation to\nrelation-dependent compound operations on head and/or tail entities. To\ndemonstrate the effectiveness of CompoundE, we conduct experiments on three\npopular KG completion datasets. Experimental results show that CompoundE\nconsistently achieves the state of-the-art performance.", "published": "2022-07-12 05:41:32", "link": "http://arxiv.org/abs/2207.05324v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval\n  and Span Extraction", "abstract": "Medical entity span extraction and linking are critical steps for many\nhealthcare NLP tasks. Most existing entity extraction methods either have a\nfixed vocabulary of medical entities or require span annotations. In this\npaper, we propose a method for linking an open set of entities that does not\nrequire any span annotations. Our method, Open Set Label Attention Transformer\n(OSLAT), uses the label-attention mechanism to learn candidate-entity\ncontextualized text representations. We find that OSLAT can not only link\nentities but is also able to implicitly learn spans associated with entities.\nWe evaluate OSLAT on two tasks: (1) span extraction trained without explicit\nspan annotations, and (2) entity linking trained without span-level annotation.\nWe test the generalizability of our method by training two separate models on\ntwo datasets with low entity overlap and comparing cross-dataset performance.", "published": "2022-07-12 20:22:55", "link": "http://arxiv.org/abs/2207.05817v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "abstract": "Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.", "published": "2022-07-12 15:20:48", "link": "http://arxiv.org/abs/2207.05608v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Exploiting Social Graph Networks for Emotion Prediction", "abstract": "Emotion prediction plays an essential role in mental health and emotion-aware\ncomputing. The complex nature of emotion resulting from its dependency on a\nperson's physiological health, mental state, and his surroundings makes its\nprediction a challenging task. In this work, we utilize mobile sensing data to\npredict happiness and stress. In addition to a person's physiological features,\nwe also incorporate the environment's impact through weather and social\nnetwork. To this end, we leverage phone data to construct social networks and\ndevelop a machine learning architecture that aggregates information from\nmultiple users of the graph network and integrates it with the temporal\ndynamics of data to predict emotion for all the users. The construction of\nsocial networks does not incur additional cost in terms of EMAs or data\ncollection from users and doesn't raise privacy concerns. We propose an\narchitecture that automates the integration of a user's social network affect\nprediction, is capable of dealing with the dynamic distribution of real-life\nsocial networks, making it scalable to large-scale networks. Our extensive\nevaluation highlights the improvement provided by the integration of social\nnetworks. We further investigate the impact of graph topology on model's\nperformance.", "published": "2022-07-12 20:24:39", "link": "http://arxiv.org/abs/2207.05820v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.SI"}
{"title": "Multitask Learning from Augmented Auxiliary Data for Improving Speech\n  Emotion Recognition", "abstract": "Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems lack generalisation across different conditions. A key\nunderlying reason for poor generalisation is the scarcity of emotion datasets,\nwhich is a significant roadblock to designing robust machine learning (ML)\nmodels. Recent works in SER focus on utilising multitask learning (MTL) methods\nto improve generalisation by learning shared representations. However, most of\nthese studies propose MTL solutions with the requirement of meta labels for\nauxiliary tasks, which limits the training of SER systems. This paper proposes\nan MTL framework (MTL-AUG) that learns generalised representations from\naugmented data. We utilise augmentation-type classification and unsupervised\nreconstruction as auxiliary tasks, which allow training SER systems on\naugmented data without requiring any meta labels for auxiliary tasks. The\nsemi-supervised nature of MTL-AUG allows for the exploitation of the abundant\nunlabelled data to further boost the performance of SER. We comprehensively\nevaluate the proposed framework in the following settings: (1) within corpus,\n(2) cross-corpus and cross-language, (3) noisy speech, (4) and adversarial\nattacks. Our evaluations using the widely used IEMOCAP, MSP-IMPROV, and EMODB\ndatasets show improved results compared to existing state-of-the-art methods.", "published": "2022-07-12 04:12:13", "link": "http://arxiv.org/abs/2207.05298v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistics of the interaural parameters for dichotic tones in diotic\n  noise ($N_0 S_\u03c8$)", "abstract": "Stimuli consisting of an interaurally phase-shifted tone in diotic noise --\noften referred to as $N_0 S_\\psi$ -- are commonly used in the field of binaural\nhearing. As a consequence of mixing diotic noise with a dichotic tone, this\ntype of stimulus contains random fluctuations in both interaural phase- and\nlevel-difference. This study reports the joint probability density functions of\nthe two interaural differences as a function of amplitude and interaural phase\nof the tone. Furthermore, a second joint probability density function for\ninteraural phase differences and the instantaneous power of the stimulus is\nderived.", "published": "2022-07-12 14:01:19", "link": "http://arxiv.org/abs/2207.05541v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NEC: Speaker Selective Cancellation via Neural Enhanced Ultrasound\n  Shadowing", "abstract": "In this paper, we propose NEC (Neural Enhanced Cancellation), a defense\nmechanism, which prevents unauthorized microphones from capturing a target\nspeaker's voice. Compared with the existing scrambling-based audio cancellation\napproaches, NEC can selectively remove a target speaker's voice from a mixed\nspeech without causing interference to others. Specifically, for a target\nspeaker, we design a Deep Neural Network (DNN) model to extract high-level\nspeaker-specific but utterance-independent vocal features from his/her\nreference audios. When the microphone is recording, the DNN generates a shadow\nsound to cancel the target voice in real-time. Moreover, we modulate the\naudible shadow sound onto an ultrasound frequency, making it inaudible for\nhumans. By leveraging the non-linearity of the microphone circuit, the\nmicrophone can accurately decode the shadow sound for target voice\ncancellation. We implement and evaluate NEC comprehensively with 8 smartphone\nmicrophones in different settings. The results show that NEC effectively mutes\nthe target speaker at a microphone without interfering with other users' normal\nconversations.", "published": "2022-07-12 21:26:06", "link": "http://arxiv.org/abs/2207.05848v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CFAD: A Chinese Dataset for Fake Audio Detection", "abstract": "Fake audio detection is a growing concern and some relevant datasets have\nbeen designed for research. However, there is no standard public Chinese\ndataset under complex conditions.In this paper, we aim to fill in the gap and\ndesign a Chinese fake audio detection dataset (CFAD) for studying more\ngeneralized detection methods. Twelve mainstream speech-generation techniques\nare used to generate fake audio. To simulate the real-life scenarios, three\nnoise datasets are selected for noise adding at five different signal-to-noise\nratios, and six codecs are considered for audio transcoding (format\nconversion). CFAD dataset can be used not only for fake audio detection but\nalso for detecting the algorithms of fake utterances for audio forensics.\nBaseline results are presented with analysis. The results that show fake audio\ndetection methods with generalization remain challenging. The CFAD dataset is\npublicly available at: https://zenodo.org/record/8122764.", "published": "2022-07-12 13:27:21", "link": "http://arxiv.org/abs/2207.12308v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Indoor optical fiber eavesdropping approach and its avoidance", "abstract": "The optical fiber network has become a worldwide infrastructure. In addition\nto the basic functions in telecommunication, its sensing ability has attracted\nmore and more attention. In this paper, we discuss the risk of household fiber\nbeing used for eavesdropping and demonstrate its performance in the lab. Using\na 3-meter tail fiber in front of the household optical modem, voices of normal\nhuman speech can be eavesdropped by a laser interferometer and recovered 1.1 km\naway. The detection distance limit and system noise are analyzed\nquantitatively. We also give some practical ways to prevent eavesdropping\nthrough household fiber.", "published": "2022-07-12 02:31:34", "link": "http://arxiv.org/abs/2207.05267v2", "categories": ["cs.SD", "eess.AS", "physics.ins-det", "physics.optics"], "primary_category": "cs.SD"}
{"title": "Western Mediterranean wetlands bird species classification: evaluating\n  small-footprint deep learning approaches on a new annotated dataset", "abstract": "The deployment of an expert system running over a wireless acoustic sensors\nnetwork made up of bioacoustic monitoring devices that recognise bird species\nfrom their sounds would enable the automation of many tasks of ecological\nvalue, including the analysis of bird population composition or the detection\nof endangered species in areas of environmental interest. Endowing these\ndevices with accurate audio classification capabilities is possible thanks to\nthe latest advances in artificial intelligence, among which deep learning\ntechniques excel. However, a key issue to make bioacoustic devices affordable\nis the use of small footprint deep neural networks that can be embedded in\nresource and battery constrained hardware platforms. For this reason, this work\npresents a critical comparative analysis between two heavy and large footprint\ndeep neural networks (VGG16 and ResNet50) and a lightweight alternative,\nMobileNetV2. Our experimental results reveal that MobileNetV2 achieves an\naverage F1-score less than a 5\\% lower than ResNet50 (0.789 vs. 0.834),\nperforming better than VGG16 with a footprint size nearly 40 times smaller.\nMoreover, to compare the models, we have created and made public the Western\nMediterranean Wetland Birds dataset, consisting of 201.6 minutes and 5,795\naudio excerpts of 20 endemic bird species of the Aiguamolls de l'Empord\\`a\nNatural Park.", "published": "2022-07-12 08:48:12", "link": "http://arxiv.org/abs/2207.05393v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Generative deep learning approach for shape recognition of arbitrary\n  objects from phaseless acoustic scattering data", "abstract": "We propose and demonstrate a generative deep learning approach for the shape\nrecognition of an arbitrary object from its acoustic scattering properties. The\nstrategy exploits deep neural networks to learn the mapping between the latent\nspace of a two-dimensional acoustic object and the far-field scattering\namplitudes. A neural network is designed as an Adversarial autoencoder and\ntrained via unsupervised learning to determine the latent space of the acoustic\nobject. Important structural features of the object are embedded in\nlower-dimensional latent space which supports the modeling of a shape generator\nand accelerates the learning in the inverse design process.The proposed inverse\ndesign uses the variational inference approach with encoder and decoder-like\narchitecture where the decoder is composed of two pretrained neural networks,\nthe generator and the forward model. The data-driven framework finds an\naccurate solution to the ill-posed inverse scattering problem, where non-unique\nsolution space is overcome by the multifrequency phaseless far-field patterns.\nThis inverse method is a powerful design tool that does not require complex\nanalytical calculation and opens up new avenues for practical realization,\nautomatic recognition of arbitrary shaped submarines or large fish, and other\nunderwater applications.", "published": "2022-07-12 09:56:29", "link": "http://arxiv.org/abs/2207.05433v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph", "physics.comp-ph"], "primary_category": "cs.SD"}
{"title": "End-to-end speech recognition modeling from de-identified data", "abstract": "De-identification of data used for automatic speech recognition modeling is a\ncritical component in protecting privacy, especially in the medical domain.\nHowever, simply removing all personally identifiable information (PII) from\nend-to-end model training data leads to a significant performance degradation\nin particular for the recognition of names, dates, locations, and words from\nsimilar categories. We propose and evaluate a two-step method for partially\nrecovering this loss. First, PII is identified, and each occurrence is replaced\nwith a random word sequence of the same category. Then, corresponding audio is\nproduced via text-to-speech or by splicing together matching audio fragments\nextracted from the corpus. These artificial audio/label pairs, together with\nspeaker turns from the original data without PII, are used to train models. We\nevaluate the performance of this method on in-house data of medical\nconversations and observe a recovery of almost the entire performance\ndegradation in the general word error rate while still maintaining a strong\ndiarization performance. Our main focus is the improvement of recall and\nprecision in the recognition of PII-related words. Depending on the PII\ncategory, between $50\\% - 90\\%$ of the performance degradation can be recovered\nusing our proposed method.", "published": "2022-07-12 11:29:52", "link": "http://arxiv.org/abs/2207.05469v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Label-Efficient Self-Supervised Speaker Verification With Information\n  Maximization and Contrastive Learning", "abstract": "State-of-the-art speaker verification systems are inherently dependent on\nsome kind of human supervision as they are trained on massive amounts of\nlabeled data. However, manually annotating utterances is slow, expensive and\nnot scalable to the amount of data available today. In this study, we explore\nself-supervised learning for speaker verification by learning representations\ndirectly from raw audio. The objective is to produce robust speaker embeddings\nthat have small intra-speaker and large inter-speaker variance. Our approach is\nbased on recent information maximization learning frameworks and an intensive\ndata augmentation pre-processing step. We evaluate the ability of these methods\nto work without contrastive samples before showing that they achieve better\nperformance when combined with a contrastive loss. Furthermore, we conduct\nexperiments to show that our method reaches competitive results compared to\nexisting techniques and can get better performances compared to a supervised\nbaseline when fine-tuned with a small portion of labeled data.", "published": "2022-07-12 13:01:55", "link": "http://arxiv.org/abs/2207.05506v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EfficientLEAF: A Faster LEarnable Audio Frontend of Questionable Use", "abstract": "In audio classification, differentiable auditory filterbanks with few\nparameters cover the middle ground between hard-coded spectrograms and raw\naudio. LEAF (arXiv:2101.08596), a Gabor-based filterbank combined with\nPer-Channel Energy Normalization (PCEN), has shown promising results, but is\ncomputationally expensive. With inhomogeneous convolution kernel sizes and\nstrides, and by replacing PCEN with better parallelizable operations, we can\nreach similar results more efficiently. In experiments on six audio\nclassification tasks, our frontend matches the accuracy of LEAF at 3% of the\ncost, but both fail to consistently outperform a fixed mel filterbank. The\nquest for learnable audio frontends is not solved.", "published": "2022-07-12 13:04:37", "link": "http://arxiv.org/abs/2207.05508v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReLyMe: Improving Lyric-to-Melody Generation by Incorporating\n  Lyric-Melody Relationships", "abstract": "Lyric-to-melody generation, which generates melody according to given lyrics,\nis one of the most important automatic music composition tasks. With the rapid\ndevelopment of deep learning, previous works address this task with end-to-end\nneural network models. However, deep learning models cannot well capture the\nstrict but subtle relationships between lyrics and melodies, which compromises\nthe harmony between lyrics and generated melodies. In this paper, we propose\nReLyMe, a method that incorporates Relationships between Lyrics and Melodies\nfrom music theory to ensure the harmony between lyrics and melodies.\nSpecifically, we first introduce several principles that lyrics and melodies\nshould follow in terms of tone, rhythm, and structure relationships. These\nprinciples are then integrated into neural network lyric-to-melody models by\nadding corresponding constraints during the decoding process to improve the\nharmony between lyrics and melodies. We use a series of objective and\nsubjective metrics to evaluate the generated melodies. Experiments on both\nEnglish and Chinese song datasets show the effectiveness of ReLyMe,\ndemonstrating the superiority of incorporating lyric-melody relationships from\nthe music domain into neural lyric-to-melody generation.", "published": "2022-07-12 17:09:44", "link": "http://arxiv.org/abs/2207.05688v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distilled Non-Semantic Speech Embeddings with Binary Neural Networks for\n  Low-Resource Devices", "abstract": "This work introduces BRILLsson, a novel binary neural network-based\nrepresentation learning model for a broad range of non-semantic speech tasks.\nWe train the model with knowledge distillation from a large and real-valued\nTRILLsson model with only a fraction of the dataset used to train TRILLsson.\nThe resulting BRILLsson models are only 2MB in size with a latency less than\n8ms, making them suitable for deployment in low-resource devices such as\nwearables. We evaluate BRILLsson on eight benchmark tasks (including but not\nlimited to spoken language identification, emotion recognition, health\ncondition diagnosis, and keyword spotting), and demonstrate that our proposed\nultra-light and low-latency models perform as well as large-scale models.", "published": "2022-07-12 18:32:53", "link": "http://arxiv.org/abs/2207.05784v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
