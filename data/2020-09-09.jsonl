{"title": "Central Yup'ik and Machine Translation of Low-Resource Polysynthetic\n  Languages", "abstract": "Machine translation tools do not yet exist for the Yup'ik language, a\npolysynthetic language spoken by around 8,000 people who live primarily in\nSouthwest Alaska. We compiled a parallel text corpus for Yup'ik and English and\ndeveloped a morphological parser for Yup'ik based on grammar rules. We trained\na seq2seq neural machine translation model with attention to translate Yup'ik\ninput into English. We then compared the influence of different tokenization\nmethods, namely rule-based, unsupervised (byte pair encoding), and unsupervised\nmorphological (Morfessor) parsing, on BLEU score accuracy for Yup'ik to English\ntranslation. We find that using tokenized input increases the translation\naccuracy compared to that of unparsed input. Although overall Morfessor did\nbest with a vocabulary size of 30k, our first experiments show that BPE\nperformed best with a reduced vocabulary size.", "published": "2020-09-09 03:11:43", "link": "http://arxiv.org/abs/2009.04087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Language Models on Cross-Domain Data with Model\n  Agnostic Explainability", "abstract": "With the recent influx of bidirectional contextualized transformer language\nmodels in the NLP, it becomes a necessity to have a systematic comparative\nstudy of these models on variety of datasets. Also, the performance of these\nlanguage models has not been explored on non-GLUE datasets. The study presented\nin paper compares the state-of-the-art language models - BERT, ELECTRA and its\nderivatives which include RoBERTa, ALBERT and DistilBERT. We conducted\nexperiments by finetuning these models for cross domain and disparate data and\npenned an in-depth analysis of model's performances. Moreover, an\nexplainability of language models coherent with pretraining is presented which\nverifies the context capturing capabilities of these models through a model\nagnostic approach. The experimental results establish new state-of-the-art for\nYelp 2013 rating classification task and Financial Phrasebank sentiment\ndetection task with 69% accuracy and 88.2% accuracy respectively. Finally, the\nstudy conferred here can greatly assist industry researchers in choosing the\nlanguage model effectively in terms of performance or compute efficiency.", "published": "2020-09-09 04:31:44", "link": "http://arxiv.org/abs/2009.04095v1", "categories": ["cs.CL", "68T50, 68T07 (Primary) 68-02 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Impact of News on the Commodity Market: Dataset and Results", "abstract": "Over the last few years, machine learning based methods have been applied to\nextract information from news flow in the financial domain. However, this\ninformation has mostly been in the form of the financial sentiments contained\nin the news headlines, primarily for the stock prices. In our current work, we\npropose that various other dimensions of information can be extracted from news\nheadlines, which will be of interest to investors, policy-makers and other\npractitioners. We propose a framework that extracts information such as past\nmovements and expected directionality in prices, asset comparison and other\ngeneral information that the news is referring to. We apply this framework to\nthe commodity \"Gold\" and train the machine learning models using a dataset of\n11,412 human-annotated news headlines (released with this study), collected\nfrom the period 2000-2019. We experiment to validate the causal effect of news\nflow on gold prices and observe that the information produced from our\nframework significantly impacts the future gold price.", "published": "2020-09-09 10:38:48", "link": "http://arxiv.org/abs/2009.04202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Textual Structures: Generative Grammar Induction using\n  Template Trees", "abstract": "Natural language generation provides designers with methods for automatically\ngenerating text, e.g. for creating summaries, chatbots and game content. In\npractise, text generators are often either learned and hard to interpret, or\ncreated by hand using techniques such as grammars and templates. In this paper,\nwe introduce a novel grammar induction algorithm for learning interpretable\ngrammars for generative purposes, called Gitta. We also introduce the novel\nnotion of template trees to discover latent templates in corpora to derive\nthese generative grammars. By using existing human-created grammars, we found\nthat the algorithm can reasonably approximate these grammars using only a few\nexamples. These results indicate that Gitta could be used to automatically\nlearn interpretable and easily modifiable grammars, and thus provide a stepping\nstone for human-machine co-creation of generative models.", "published": "2020-09-09 19:31:04", "link": "http://arxiv.org/abs/2009.04530v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Pay Attention when Required", "abstract": "Transformer-based models consist of interleaved feed-forward blocks - that\ncapture content meaning, and relatively more expensive self-attention blocks -\nthat capture context meaning. In this paper, we explored trade-offs and\nordering of the blocks to improve upon the current Transformer architecture and\nproposed PAR Transformer. It needs 35% lower compute time than Transformer-XL\nachieved by replacing ~63% of the self-attention blocks with feed-forward\nblocks, and retains the perplexity on WikiText-103 language modelling\nbenchmark. We further validated our results on text8 and enwiki8 datasets, as\nwell as on the BERT model.", "published": "2020-09-09 19:39:15", "link": "http://arxiv.org/abs/2009.04534v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploiting Multi-Modal Features From Pre-trained Networks for\n  Alzheimer's Dementia Recognition", "abstract": "Collecting and accessing a large amount of medical data is very\ntime-consuming and laborious, not only because it is difficult to find specific\npatients but also because it is required to resolve the confidentiality of a\npatient's medical records. On the other hand, there are deep learning models,\ntrained on easily collectible, large scale datasets such as Youtube or\nWikipedia, offering useful representations. It could therefore be very\nadvantageous to utilize the features from these pre-trained networks for\nhandling a small amount of data at hand. In this work, we exploit various\nmulti-modal features extracted from pre-trained networks to recognize\nAlzheimer's Dementia using a neural network, with a small dataset provided by\nthe ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern\npatients suspicious of Alzheimer's Dementia by providing acoustic and textual\ndata. With the multi-modal features, we modify a Convolutional Recurrent Neural\nNetwork based structure to perform classification and regression tasks\nsimultaneously and is capable of computing conversations with variable lengths.\nOur test results surpass baseline's accuracy by 18.75%, and our validation\nresult for the regression task shows the possibility of classifying 4 classes\nof cognitive impairment with an accuracy of 78.70%.", "published": "2020-09-09 02:08:47", "link": "http://arxiv.org/abs/2009.04070v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Aspect Classification for Legal Depositions", "abstract": "Attorneys and others have a strong interest in having a digital library with\nsuitable services (e.g., summarizing, searching, and browsing) to help them\nwork with large corpora of legal depositions. Their needs often involve\nunderstanding the semantics of such documents. That depends in part on the role\nof the deponent, e.g., plaintiff, defendant, law enforcement personnel, expert,\netc. In the case of tort litigation associated with property and casualty\ninsurance claims, such as relating to an injury, it is important to know not\nonly about liability, but also about events, accidents, physical conditions,\nand treatments.\n  We hypothesize that a legal deposition consists of various aspects that are\ndiscussed as part of the deponent testimony. Accordingly, we developed an\nontology of aspects in a legal deposition for accident and injury cases. Using\nthat, we have developed a classifier that can identify portions of text for\neach of the aspects of interest. Doing so was complicated by the peculiarities\nof this genre, e.g., that deposition transcripts generally consist of data in\nthe form of question-answer (QA) pairs. Accordingly, our automated system\nstarts with pre-processing, and then transforms the QA pairs into a canonical\nform made up of declarative sentences. Classifying the declarative sentences\nthat are generated, according to the aspect, can then help with downstream\ntasks such as summarization, segmentation, question-answering, and information\nretrieval.\n  Our methods have achieved a classification F1 score of 0.83. Having the\naspects classified with a good accuracy will help in choosing QA pairs that can\nbe used as candidate summary sentences, and to generate an informative summary\nfor legal professionals or insurance claim agents. Our methodology could be\nextended to legal depositions of other kinds, and to aid services like\nsearching.", "published": "2020-09-09 18:00:15", "link": "http://arxiv.org/abs/2009.04485v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regularised Text Logistic Regression: Key Word Detection and Sentiment\n  Classification for Online Reviews", "abstract": "Online customer reviews have become important for managers and executives in\nthe hospitality and catering industry who wish to obtain a comprehensive\nunderstanding of their customers' demands and expectations. We propose a\nRegularized Text Logistic (RTL) regression model to perform text analytics and\nsentiment classification on unstructured text data, which automatically\nidentifies a set of statistically significant and operationally insightful word\nfeatures, and achieves satisfactory predictive classification accuracy. We\napply the RTL model to two online review datasets, Restaurant and Hotel, from\nTripAdvisor. Our results demonstrate satisfactory classification performance\ncompared with alternative classifiers with a highest true positive rate of\n94.9%. Moreover, RTL identifies a small set of word features, corresponding to\n3% for Restaurant and 20% for Hotel, which boosts working efficiency by\nallowing managers to drill down into a much smaller set of important customer\nreviews. We also develop the consistency, sparsity and oracle property of the\nestimator.", "published": "2020-09-09 22:37:53", "link": "http://arxiv.org/abs/2009.04591v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Searching for a Search Method: Benchmarking Search Algorithms for\n  Generating NLP Adversarial Examples", "abstract": "We study the behavior of several black-box search algorithms used for\ngenerating adversarial examples for natural language processing (NLP) tasks. We\nperform a fine-grained analysis of three elements relevant to search: search\nalgorithm, search space, and search budget. When new search algorithms are\nproposed in past work, the attack search space is often modified alongside the\nsearch algorithm. Without ablation studies benchmarking the search algorithm\nchange with the search space held constant, one cannot tell if an increase in\nattack success rate is a result of an improved search algorithm or a less\nrestrictive search space. Additionally, many previous studies fail to properly\nconsider the search algorithms' run-time cost, which is essential for\ndownstream tasks like adversarial training. Our experiments provide a\nreproducible benchmark of search algorithms across a variety of search spaces\nand query budgets to guide future research in adversarial NLP. Based on our\nexperiments, we recommend greedy attacks with word importance ranking when\nunder a time constraint or attacking long inputs, and either beam search or\nparticle swarm optimization otherwise. Code implementation shared via\nhttps://github.com/QData/TextAttack-Search-Benchmark", "published": "2020-09-09 17:04:42", "link": "http://arxiv.org/abs/2009.06368v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forecasting financial markets with semantic network analysis in the\n  COVID-19 crisis", "abstract": "This paper uses a new textual data index for predicting stock market data.\nThe index is applied to a large set of news to evaluate the importance of one\nor more general economic-related keywords appearing in the text. The index\nassesses the importance of the economic-related keywords, based on their\nfrequency of use and semantic network position. We apply it to the Italian\npress and construct indices to predict Italian stock and bond market returns\nand volatilities in a recent sample period, including the COVID-19 crisis. The\nevidence shows that the index captures the different phases of financial time\nseries well. Moreover, results indicate strong evidence of predictability for\nbond market data, both returns and volatilities, short and long maturities, and\nstock market volatility.", "published": "2020-09-09 15:40:56", "link": "http://arxiv.org/abs/2009.04975v4", "categories": ["q-fin.GN", "cs.CL", "cs.SI", "econ.GN", "q-fin.EC", "J.4; H.4.0; I.7; I.2.7"], "primary_category": "q-fin.GN"}
{"title": "1-Dimensional polynomial neural networks for audio signal related\n  problems", "abstract": "In addition to being extremely non-linear, modern problems require millions\nif not billions of parameters to solve or at least to get a good approximation\nof the solution, and neural networks are known to assimilate that complexity by\ndeepening and widening their topology in order to increase the level of\nnon-linearity needed for a better approximation. However, compact topologies\nare always preferred to deeper ones as they offer the advantage of using less\ncomputational units and less parameters. This compacity comes at the price of\nreduced non-linearity and thus, of limited solution search space. We propose\nthe 1-Dimensional Polynomial Neural Network (1DPNN) model that uses automatic\npolynomial kernel estimation for 1-Dimensional Convolutional Neural Networks\n(1DCNNs) and that introduces a high degree of non-linearity from the first\nlayer which can compensate the need for deep and/or wide topologies. We show\nthat this non-linearity enables the model to yield better results with less\ncomputational and spatial complexity than a regular 1DCNN on various\nclassification and regression problems related to audio signals, even though it\nintroduces more computational and spatial complexity on a neuronal level. The\nexperiments were conducted on three publicly available datasets and demonstrate\nthat, on the problems that were tackled, the proposed model can extract more\nrelevant information from the data than a 1DCNN in less time and with less\nmemory.", "published": "2020-09-09 02:29:53", "link": "http://arxiv.org/abs/2009.04077v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multi-modal Attention for Speech Emotion Recognition", "abstract": "Emotion represents an essential aspect of human speech that is manifested in\nspeech prosody. Speech, visual, and textual cues are complementary in human\ncommunication. In this paper, we study a hybrid fusion method, referred to as\nmulti-modal attention network (MMAN) to make use of visual and textual cues in\nspeech emotion recognition. We propose a novel multi-modal attention mechanism,\ncLSTM-MMA, which facilitates the attention across three modalities and\nselectively fuse the information. cLSTM-MMA is fused with other uni-modal\nsub-networks in the late fusion. The experiments show that speech emotion\nrecognition benefits significantly from visual and textual cues, and the\nproposed cLSTM-MMA alone is as competitive as other fusion methods in terms of\naccuracy, but with a much more compact network structure. The proposed hybrid\nnetwork MMAN achieves state-of-the-art performance on IEMOCAP database for\nemotion recognition.", "published": "2020-09-09 05:06:44", "link": "http://arxiv.org/abs/2009.04107v1", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural\n  Networks", "abstract": "This paper addresses the extraction of multiple F0 values from polyphonic and\na cappella vocal performances using convolutional neural networks (CNNs). We\naddress the major challenges of ensemble singing, i.e., all melodic sources are\nvocals and singers sing in harmony. We build upon an existing architecture to\nproduce a pitch salience function of the input signal, where the harmonic\nconstant-Q transform (HCQT) and its associated phase differentials are used as\nan input representation. The pitch salience function is subsequently\nthresholded to obtain a multiple F0 estimation output. For training, we build a\ndataset that comprises several multi-track datasets of vocal quartets with F0\nannotations. This work proposes and evaluates a set of CNNs for this task in\ndiverse scenarios and data configurations, including recordings with additional\nreverb. Our models outperform a state-of-the-art method intended for the same\nmusic genre when evaluated with an increased F0 resolution, as well as a\ngeneral-purpose method for multi-F0 estimation. We conclude with a discussion\non future research directions.", "published": "2020-09-09 09:11:49", "link": "http://arxiv.org/abs/2009.04172v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A dataset and classification model for Malay, Hindi, Tamil and Chinese\n  music", "abstract": "In this paper we present a new dataset, with musical excepts from the three\nmain ethnic groups in Singapore: Chinese, Malay and Indian (both Hindi and\nTamil). We use this new dataset to train different classification models to\ndistinguish the origin of the music in terms of these ethnic groups. The\nclassification models were optimized by exploring the use of different musical\nfeatures as the input. Both high level features, i.e., musically meaningful\nfeatures, as well as low level features, i.e., spectrogram based features, were\nextracted from the audio files so as to optimize the performance of the\ndifferent classification models.", "published": "2020-09-09 06:17:20", "link": "http://arxiv.org/abs/2009.04459v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device\n  Speech Recognition", "abstract": "We introduce VoiceFilter-Lite, a single-channel source separation model that\nruns on the device to preserve only the speech signals from a target user, as\npart of a streaming speech recognition system. Delivering such a model presents\nnumerous challenges: It should improve the performance when the input signal\nconsists of overlapped speech, and must not hurt the speech recognition\nperformance under all other acoustic conditions. Besides, this model must be\ntiny, fast, and perform inference in a streaming fashion, in order to have\nminimal impact on CPU, memory, battery and latency. We propose novel techniques\nto meet these multi-faceted requirements, including using a new asymmetric\nloss, and adopting adaptive runtime suppression strength. We also show that\nsuch a model can be quantized as a 8-bit integer model and run in realtime.", "published": "2020-09-09 14:26:56", "link": "http://arxiv.org/abs/2009.04323v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Hardware Aware Training for Efficient Keyword Spotting on General\n  Purpose and Specialized Hardware", "abstract": "Keyword spotting (KWS) provides a critical user interface for many mobile and\nedge applications, including phones, wearables, and cars. As KWS systems are\ntypically 'always on', maximizing both accuracy and power efficiency are\ncentral to their utility. In this work we use hardware aware training (HAT) to\nbuild new KWS neural networks based on the Legendre Memory Unit (LMU) that\nachieve state-of-the-art (SotA) accuracy and low parameter counts. This allows\nthe neural network to run efficiently on standard hardware (212$\\mu$W). We also\ncharacterize the power requirements of custom designed accelerator hardware\nthat achieves SotA power efficiency of 8.79$\\mu$W, beating general purpose low\npower hardware (a microcontroller) by 24x and special purpose ASICs by 16x.", "published": "2020-09-09 17:06:28", "link": "http://arxiv.org/abs/2009.04465v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
