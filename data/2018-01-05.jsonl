{"title": "A Multi-task Learning Approach for Improving Product Title Compression\n  with User Search Log Data", "abstract": "It is a challenging and practical research problem to obtain effective\ncompression of lengthy product titles for E-commerce. This is particularly\nimportant as more and more users browse mobile E-commerce apps and more\nmerchants make the original product titles redundant and lengthy for Search\nEngine Optimization. Traditional text summarization approaches often require a\nlarge amount of preprocessing costs and do not capture the important issue of\nconversion rate in E-commerce. This paper proposes a novel multi-task learning\napproach for improving product title compression with user search log data. In\nparticular, a pointer network-based sequence-to-sequence approach is utilized\nfor title compression with an attentive mechanism as an extractive method and\nan attentive encoder-decoder approach is utilized for generating user search\nqueries. The encoding parameters (i.e., semantic embedding of original titles)\nare shared among the two tasks and the attention distributions are jointly\noptimized. An extensive set of experiments with both human annotated data and\nonline deployment demonstrate the advantage of the proposed research for both\ncompression qualities and online business values.", "published": "2018-01-05 11:52:44", "link": "http://arxiv.org/abs/1801.01725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching\n  Model", "abstract": "As an alternative to question answering methods based on feature engineering,\ndeep learning approaches such as convolutional neural networks (CNNs) and Long\nShort-Term Memory Models (LSTMs) have recently been proposed for semantic\nmatching of questions and answers. To achieve good results, however, these\nmodels have been combined with additional features such as word overlap or BM25\nscores. Without this combination, these models perform significantly worse than\nmethods based on linguistic feature engineering. In this paper, we propose an\nattention based neural matching model for ranking short answer text. We adopt\nvalue-shared weighting scheme instead of position-shared weighting scheme for\ncombining different matching signals and incorporate question term importance\nlearning using question attention network. Using the popular benchmark TREC QA\ndata, we show that the relatively simple aNMM model can significantly\noutperform other neural network models that have been used for the question\nanswering task, and is competitive with models that are combined with\nadditional features. When aNMM is combined with additional features, it\noutperforms all baselines.", "published": "2018-01-05 06:06:17", "link": "http://arxiv.org/abs/1801.01641v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning Feature Representations for Keyphrase Extraction", "abstract": "In supervised approaches for keyphrase extraction, a candidate phrase is\nencoded with a set of hand-crafted features and machine learning algorithms are\ntrained to discriminate keyphrases from non-keyphrases. Although the\nmanually-designed features have shown to work well in practice, feature\nengineering is a difficult process that requires expert knowledge and normally\ndoes not generalize well. In this paper, we present SurfKE, a feature learning\nframework that exploits the text itself to automatically discover patterns that\nkeyphrases exhibit. Our model represents the document as a graph and\nautomatically learns feature representation of phrases. The proposed model\nobtains remarkable improvements in performance over strong baselines.", "published": "2018-01-05 14:36:31", "link": "http://arxiv.org/abs/1801.01768v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Understanding and Answering Multi-Sentence Recommendation\n  Questions on Tourism", "abstract": "We introduce the first system towards the novel task of answering complex\nmultisentence recommendation questions in the tourism domain. Our solution uses\na pipeline of two modules: question understanding and answering. For question\nunderstanding, we define an SQL-like query language that captures the semantic\nintent of a question; it supports operators like subset, negation, preference\nand similarity, which are often found in recommendation questions. We train and\ncompare traditional CRFs as well as bidirectional LSTM-based models for\nconverting a question to its semantic representation. We extend these models to\na semisupervised setting with partially labeled sequences gathered through\ncrowdsourcing. We find that our best model performs semi-supervised training of\nBiDiLSTM+CRF with hand-designed features and CCM(Chang et al., 2007)\nconstraints. Finally, in an end to end QA system, our answering component\nconverts our question representation into queries fired on underlying knowledge\nsources. Our experiments on two different answer corpora demonstrate that our\nsystem can significantly outperform baselines with up to 20 pt higher accuracy\nand 17 pt higher recall.", "published": "2018-01-05 16:38:05", "link": "http://arxiv.org/abs/1801.01825v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shielding Google's language toxicity model against adversarial attacks", "abstract": "Lack of moderation in online communities enables participants to incur in\npersonal aggression, harassment or cyberbullying, issues that have been\naccentuated by extremist radicalisation in the contemporary post-truth politics\nscenario. This kind of hostility is usually expressed by means of toxic\nlanguage, profanity or abusive statements. Recently Google has developed a\nmachine-learning-based toxicity model in an attempt to assess the hostility of\na comment; unfortunately, it has been suggested that said model can be deceived\nby adversarial attacks that manipulate the text sequence of the comment. In\nthis paper we firstly characterise such adversarial attacks as using\nobfuscation and polarity transformations. The former deceives by corrupting\ntoxic trigger content with typographic edits, whereas the latter deceives by\ngrammatical negation of the toxic content. Then, we propose a two--stage\napproach to counter--attack these anomalies, bulding upon a recently proposed\ntext deobfuscation method and the toxicity scoring model. Lastly, we conducted\nan experiment with approximately 24000 distorted comments, showing how in this\nway it is feasible to restore toxicity of the adversarial variants, while\nincurring roughly on a twofold increase in processing time. Even though novel\nadversary challenges would keep coming up derived from the versatile nature of\nwritten language, we anticipate that techniques combining machine learning and\ntext pattern recognition methods, each one targeting different layers of\nlinguistic features, would be needed to achieve robust detection of toxic\nlanguage, thus fostering aggression--free digital interaction.", "published": "2018-01-05 16:45:59", "link": "http://arxiv.org/abs/1801.01828v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Low-Dimensional Vector Representations for Words, Phrases\n  and Text that are Transparent, Scalable, and produce Similarity Metrics that\n  are Complementary to Neural Embeddings", "abstract": "Neural embeddings are a popular set of methods for representing words,\nphrases or text as a low dimensional vector (typically 50-500 dimensions).\nHowever, it is difficult to interpret these dimensions in a meaningful manner,\nand creating neural embeddings requires extensive training and tuning of\nmultiple parameters and hyperparameters. We present here a simple unsupervised\nmethod for representing words, phrases or text as a low dimensional vector, in\nwhich the meaning and relative importance of dimensions is transparent to\ninspection. We have created a near-comprehensive vector representation of\nwords, and selected bigrams, trigrams and abbreviations, using the set of\ntitles and abstracts in PubMed as a corpus. This vector is used to create\nseveral novel implicit word-word and text-text similarity metrics. The implicit\nword-word similarity metrics correlate well with human judgement of word pair\nsimilarity and relatedness, and outperform or equal all other reported methods\non a variety of biomedical benchmarks, including several implementations of\nneural embeddings trained on PubMed corpora. Our implicit word-word metrics\ncapture different aspects of word-word relatedness than word2vec-based metrics\nand are only partially correlated (rho = ~0.5-0.8 depending on task and\ncorpus). The vector representations of words, bigrams, trigrams, abbreviations,\nand PubMed title+abstracts are all publicly available from\nhttp://arrowsmith.psych.uic.edu for release under CC-BY-NC license. Several\npublic web query interfaces are also available at the same site, including one\nwhich allows the user to specify a given word and view its most closely related\nterms according to direct co-occurrence as well as different implicit\nsimilarity metrics.", "published": "2018-01-05 19:00:04", "link": "http://arxiv.org/abs/1801.01884v2", "categories": ["cs.CL", "cs.IR", "I.2.7; I.7.2"], "primary_category": "cs.CL"}
{"title": "Knowledge-based Word Sense Disambiguation using Topic Models", "abstract": "Word Sense Disambiguation is an open problem in Natural Language Processing\nwhich is particularly challenging and useful in the unsupervised setting where\nall the words in any given text need to be disambiguated without using any\nlabeled data. Typically WSD systems use the sentence or a small window of words\naround the target word as the context for disambiguation because their\ncomputational complexity scales exponentially with the size of the context. In\nthis paper, we leverage the formalism of topic model to design a WSD system\nthat scales linearly with the number of words in the context. As a result, our\nsystem is able to utilize the whole document as the context for a word to be\ndisambiguated. The proposed method is a variant of Latent Dirichlet Allocation\nin which the topic proportions for a document are replaced by synset\nproportions. We further utilize the information in the WordNet by assigning a\nnon-uniform prior to synset distribution over words and a logistic-normal prior\nfor document distribution over synsets. We evaluate the proposed method on\nSenseval-2, Senseval-3, SemEval-2007, SemEval-2013 and SemEval-2015 English\nAll-Word WSD datasets and show that it outperforms the state-of-the-art\nunsupervised knowledge-based WSD system by a significant margin.", "published": "2018-01-05 19:20:24", "link": "http://arxiv.org/abs/1801.01900v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tree based classification of tabla strokes", "abstract": "The paper attempts to validate the effectiveness of tree classifiers to\nclassify tabla strokes especially the ones which are overlapping in nature. It\nuses decision tree, ID3 and random forest as classifiers. A custom made data\nsets of 650 samples of 13 different tabla strokes were used for experimental\npurpose. 31 different features with their mean and variances were extracted for\nclassification. Three data sets consisting of 21361, 18802 and 19543 instances\nrespectively were used for the purpose. Validation has been done using measures\nlike ROC curve and accuracy. The experimental results showed that all the\nclassifiers showing excellent results with random forest outperforming the\nother two. The effectiveness of random forest in classifying strokes which are\noverlapping in nature is done by comparing the known results of that with\nmulti-layer perceptron.", "published": "2018-01-05 11:09:31", "link": "http://arxiv.org/abs/1801.01712v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
