{"title": "Improving Pre-trained Language Model Fine-tuning with Noise Stability\n  Regularization", "abstract": "The advent of large-scale pre-trained language models has contributed greatly\nto the recent progress in natural language processing. Many state-of-the-art\nlanguage models are first trained on a large text corpus and then fine-tuned on\ndownstream tasks. Despite its recent success and wide adoption, fine-tuning a\npre-trained language model often suffers from overfitting, which leads to poor\ngeneralizability due to the extremely high complexity of the model and the\nlimited training samples from downstream tasks. To address this problem, we\npropose a novel and effective fine-tuning framework, named Layerwise Noise\nStability Regularization (LNSR). Specifically, we propose to inject the\nstandard Gaussian noise or In-manifold noise and regularize hidden\nrepresentations of the fine-tuned model. We first provide theoretical analyses\nto support the efficacy of our method. We then demonstrate the advantages of\nthe proposed method over other state-of-the-art algorithms including L2-SP,\nMixout and SMART. While these previous works only verify the effectiveness of\ntheir methods on relatively simple text classification tasks, we also verify\nthe effectiveness of our method on question answering tasks, where the target\nproblem is much more difficult and more training examples are available.\nFurthermore, extensive experimental results indicate that the proposed\nalgorithm can not only enhance the in-domain performance of the language models\nbut also improve the domain generalization performance on out-of-domain data.", "published": "2022-06-12 04:42:49", "link": "http://arxiv.org/abs/2206.05658v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding in social media: An approach to building a chit-chat dialogue\n  model", "abstract": "Building open-domain dialogue systems capable of rich human-like\nconversational ability is one of the fundamental challenges in language\ngeneration. However, even with recent advancements in the field, existing\nopen-domain generative models fail to capture and utilize external knowledge,\nleading to repetitive or generic responses to unseen utterances. Current work\non knowledge-grounded dialogue generation primarily focuses on persona\nincorporation or searching a fact-based structured knowledge source such as\nWikipedia. Our method takes a broader and simpler approach, which aims to\nimprove the raw conversation ability of the system by mimicking the human\nresponse behavior through casual interactions found on social media. Utilizing\na joint retriever-generator setup, the model queries a large set of filtered\ncomment data from Reddit to act as additional context for the seq2seq\ngenerator. Automatic and human evaluations on open-domain dialogue datasets\ndemonstrate the effectiveness of our approach.", "published": "2022-06-12 09:01:57", "link": "http://arxiv.org/abs/2206.05696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer", "abstract": "Pre-trained Language Models (PTLMs) have been shown to perform well on\nnatural language tasks. Many prior works have leveraged structured commonsense\npresent in the form of entities linked through labeled relations in Knowledge\nGraphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static\nmodule which limits coverage since KGs contain finite knowledge. Generative\nmethods train PTLMs on KG triples to improve the scale at which knowledge can\nbe obtained. However, training on symbolic KG entities limits their\napplicability in tasks involving natural language text where they ignore\noverall context. To mitigate this, we propose a CommonSense Contextualizer\n(CoSe-Co) conditioned on sentences as input to make it generically usable in\ntasks for generating knowledge relevant to the overall context of input text.\nTo train CoSe-Co, we propose a novel dataset comprising of sentence and\ncommonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and\ncontain novel entities not present in the underlying KG. We augment generated\nknowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading\nto improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.\nWe also demonstrate its applicability in improving performance of a baseline\nmodel for paraphrase generation task.", "published": "2022-06-12 09:57:32", "link": "http://arxiv.org/abs/2206.05706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Intent Classification", "abstract": "Training accurate intent classifiers requires labeled data, which can be\ncostly to obtain. Data augmentation methods may ameliorate this issue, but the\nquality of the generated data varies significantly across techniques. We study\nthe process of systematically producing pseudo-labeled data given a small seed\nset using a wide variety of data augmentation techniques, including mixing\nmethods together. We find that while certain methods dramatically improve\nqualitative and quantitative performance, other methods have minimal or even\nnegative impact. We also analyze key considerations when implementing data\naugmentation methods in production.", "published": "2022-06-12 16:56:31", "link": "http://arxiv.org/abs/2206.05790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for\n  Simultaneous Speech Translation", "abstract": "Simultaneous speech translation (SimulST) systems aim at generating their\noutput with the lowest possible latency, which is normally computed in terms of\nAverage Lagging (AL). In this paper we highlight that, despite its widespread\nadoption, AL provides underestimated scores for systems that generate longer\npredictions compared to the corresponding references. We also show that this\nproblem has practical relevance, as recent SimulST systems have indeed a\ntendency to over-generate. As a solution, we propose LAAL (Length-Adaptive\nAverage Lagging), a modified version of the metric that takes into account the\nover-generation phenomenon and allows for unbiased evaluation of both\nunder-/over-generating systems.", "published": "2022-06-12 18:00:08", "link": "http://arxiv.org/abs/2206.05807v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline\n  Shared Task", "abstract": "This paper describes the submission of our end-to-end YiTrans speech\ntranslation system for the IWSLT 2022 offline task, which translates from\nEnglish audio to German, Chinese, and Japanese. The YiTrans system is built on\nlarge-scale pre-trained encoder-decoder models. More specifically, we first\ndesign a multi-stage pre-training strategy to build a multi-modality model with\na large amount of labeled and unlabeled data. We then fine-tune the\ncorresponding components of the model for the downstream speech translation\ntasks. Moreover, we make various efforts to improve performance, such as data\nfiltering, data augmentation, speech segmentation, model ensemble, and so on.\nExperimental results show that our YiTrans system obtains a significant\nimprovement than the strong baseline on three translation directions, and it\nachieves +5.2 BLEU improvements over last year's optimal end-to-end system on\ntst2021 English-German. Our final submissions rank first on English-German and\nEnglish-Chinese end-to-end systems in terms of the automatic evaluation metric.\nWe make our code and models publicly available.", "published": "2022-06-12 16:13:01", "link": "http://arxiv.org/abs/2206.05777v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-critiquing models for assisting human evaluators", "abstract": "We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a topic-based\nsummarization task, critiques written by our models help humans find flaws in\nsummaries that they would have otherwise missed. Our models help find naturally\noccurring flaws in both model and human written summaries, and intentional\nflaws in summaries written by humans to be deliberately misleading. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Larger models write more helpful critiques, and on most tasks,\nare better at self-critiquing, despite having harder-to-critique outputs.\nLarger models can also integrate their own self-critiques as feedback, refining\ntheir own summaries into better ones. Finally, we motivate and introduce a\nframework for comparing critiquing ability to generation and discrimination\nability. Our measurements suggest that even large models may still have\nrelevant knowledge they cannot or do not articulate as critiques. These results\nare a proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets, as well as samples from\nour critique assistance experiments.", "published": "2022-06-12 17:40:53", "link": "http://arxiv.org/abs/2206.05802v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"COVID-19 was a FIFA conspiracy #curropt\": An Investigation into the\n  Viral Spread of COVID-19 Misinformation", "abstract": "The outbreak of the infectious and fatal disease COVID-19 has revealed that\npandemics assail public health in two waves: first, from the contagion itself\nand second, from plagues of suspicion and stigma. Now, we have in our hands and\non our phones an outbreak of moral controversy. Modern dependency on social\nmedias has not only facilitated access to the locations of vaccine clinics and\ntesting sites but also-and more frequently-to the convoluted explanations of\nhow \"COVID-19 was a FIFA conspiracy\"[1]. The MIT Media Lab finds that false\nnews \"diffuses significantly farther, faster, deeper, and more broadly than\ntruth, in all categories of information, and by an order of magnitude\"[2]. The\nquestion is, how does the spread of misinformation interact with a physical\nepidemic disease? In this paper, we estimate the extent to which misinformation\nhas influenced the course of the COVID-19 pandemic using natural language\nprocessing models and provide a strategy to combat social media posts that are\nlikely to cause widespread harm.", "published": "2022-06-12 19:41:01", "link": "http://arxiv.org/abs/2207.01483v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "GLIPv2: Unifying Localization and Vision-Language Understanding", "abstract": "We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.", "published": "2022-06-12 20:31:28", "link": "http://arxiv.org/abs/2206.05836v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Don't \"research fast and break things\": On the ethics of Computational\n  Social Science", "abstract": "This article is concerned with setting up practical guardrails within the\nresearch activities and environments of CSS. It aims to provide CSS scholars,\nas well as policymakers and other stakeholders who apply CSS methods, with the\ncritical and constructive means needed to ensure that their practices are\nethical, trustworthy, and responsible. It begins by providing a taxonomy of the\nethical challenges faced by researchers in the field of CSS. These are\nchallenges related to (1) the treatment of research subjects, (2) the impacts\nof CSS research on affected individuals and communities, (3) the quality of CSS\nresearch and to its epistemological status, (4) research integrity, and (5)\nresearch equity. Taking these challenges as a motivation for cultural\ntransformation, it then argues for the end-to-end incorporation of habits of\nresponsible research and innovation (RRI) into CSS practices, focusing on the\nrole that contextual considerations, anticipatory reflection, impact\nassessment, public engagement, and justifiable and well-documented action\nshould play across the research lifecycle. In proposing the inclusion of habits\nof RRI in CSS practices, the chapter lays out several practical steps needed\nfor ethical, trustworthy, and responsible CSS research activities. These\ninclude stakeholder engagement processes, research impact assessments, data\nlifecycle documentation, bias self-assessments, and transparent research\nreporting protocols.", "published": "2022-06-12 09:51:19", "link": "http://arxiv.org/abs/2206.06370v1", "categories": ["cs.CY", "cs.CL", "cs.GT", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
