{"title": "Related Tasks can Share! A Multi-task Framework for Affective language", "abstract": "Expressing the polarity of sentiment as 'positive' and 'negative' usually\nhave limited scope compared with the intensity/degree of polarity. These two\ntasks (i.e. sentiment classification and sentiment intensity prediction) are\nclosely related and may offer assistance to each other during the learning\nprocess. In this paper, we propose to leverage the relatedness of multiple\ntasks in a multi-task learning framework. Our multi-task model is based on\nconvolutional-Gated Recurrent Unit (GRU) framework, which is further assisted\nby a diverse hand-crafted feature set. Evaluation and analysis suggest that\njoint-learning of the related tasks in a multi-task framework can outperform\neach of the individual tasks in the single-task frameworks.", "published": "2020-02-06 08:36:38", "link": "http://arxiv.org/abs/2002.02154v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Citation Data of Czech Apex Courts", "abstract": "In this paper, we introduce the citation data of the Czech apex courts\n(Supreme Court, Supreme Administrative Court and Constitutional Court). This\ndataset was automatically extracted from the corpus of texts of Czech court\ndecisions - CzCDC 1.0. We obtained the citation data by building the natural\nlanguage processing pipeline for extraction of the court decision identifiers.\nThe pipeline included the (i) document segmentation model and the (ii)\nreference recognition model. Furthermore, the dataset was manually processed to\nachieve high-quality citation data as a base for subsequent qualitative and\nquantitative analyses. The dataset will be made available to the general\npublic.", "published": "2020-02-06 12:35:14", "link": "http://arxiv.org/abs/2002.02224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Structure Aware and Context Sensitive Topic Model for\n  Online Discussions", "abstract": "Millions of online discussions are generated everyday on social media\nplatforms. Topic modelling is an efficient way of better understanding large\ntext datasets at scale. Conventional topic models have had limited success in\nonline discussions, and to overcome their limitations, we use the discussion\nthread tree structure and propose a \"popularity\" metric to quantify the number\nof replies to a comment to extend the frequency of word occurrences, and the\n\"transitivity\" concept to characterize topic dependency among nodes in a nested\ndiscussion thread. We build a Conversational Structure Aware Topic Model\n(CSATM) based on popularity and transitivity to infer topics and their\nassignments to comments. Experiments on real forum datasets are used to\ndemonstrate improved performance for topic extraction with six different\nmeasurements of coherence and impressive accuracy for topic assignments.", "published": "2020-02-06 16:57:27", "link": "http://arxiv.org/abs/2002.02353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Neural Machine Translation by Removing the Lexicon from\n  Syntax", "abstract": "The meaning of a natural language utterance is largely determined from its\nsyntax and words. Additionally, there is evidence that humans process an\nutterance by separating knowledge about the lexicon from syntax knowledge.\nTheories from semantics and neuroscience claim that complete word meanings are\nnot encoded in the representation of syntax. In this paper, we propose neural\nunits that can enforce this constraint over an LSTM encoder and decoder. We\ndemonstrate that our model achieves competitive performance across a variety of\ndomains including semantic parsing, syntactic parsing, and English to Mandarin\nChinese translation. In these cases, our model outperforms the standard LSTM\nencoder and decoder architecture on many or all of our metrics. To demonstrate\nthat our model achieves the desired separation between the lexicon and syntax,\nwe analyze its weights and explore its behavior when different neural modules\nare damaged. When damaged, we find that the model displays the knowledge\ndistortions that aphasics are evidenced to have.", "published": "2020-02-06 18:51:16", "link": "http://arxiv.org/abs/2002.08899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves\ninteractions at a car dealership. At the dealership, the car-buyer relays their\nneeds to a sales representative. However, most car-buyers are only have an\nabstract description of the vehicle they need. Therefore, they are only able to\ndescribe their ideal car in \"car-speak\". Car-speak is abstract language that\npertains to a car's physical attributes. In this paper, we define car-speak. We\nalso aim to curate a reasonable data set of car-speak language. Finally, we\ntrain several classifiers in order to classify car-speak.", "published": "2020-02-06 02:10:33", "link": "http://arxiv.org/abs/2002.02070v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Attractive or Faithful? Popularity-Reinforced Learning for Inspired\n  Headline Generation", "abstract": "With the rapid proliferation of online media sources and published news,\nheadlines have become increasingly important for attracting readers to news\narticles, since users may be overwhelmed with the massive information. In this\npaper, we generate inspired headlines that preserve the nature of news articles\nand catch the eye of the reader simultaneously. The task of inspired headline\ngeneration can be viewed as a specific form of Headline Generation (HG) task,\nwith the emphasis on creating an attractive headline from a given news article.\nTo generate inspired headlines, we propose a novel framework called\nPOpularity-Reinforced Learning for inspired Headline Generation (PORL-HG).\nPORL-HG exploits the extractive-abstractive architecture with 1) Popular Topic\nAttention (PTA) for guiding the extractor to select the attractive sentence\nfrom the article and 2) a popularity predictor for guiding the abstractor to\nrewrite the attractive sentence. Moreover, since the sentence selection of the\nextractor is not differentiable, techniques of reinforcement learning (RL) are\nutilized to bridge the gap with rewards obtained from a popularity score\npredictor. Through quantitative and qualitative experiments, we show that the\nproposed PORL-HG significantly outperforms the state-of-the-art headline\ngeneration models in terms of attractiveness evaluated by both human (71.03%)\nand the predictor (at least 27.60%), while the faithfulness of PORL-HG is also\ncomparable to the state-of-the-art generation model.", "published": "2020-02-06 04:37:44", "link": "http://arxiv.org/abs/2002.02095v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "Multilingual acoustic word embedding models for processing zero-resource\n  languages", "abstract": "Acoustic word embeddings are fixed-dimensional representations of\nvariable-length speech segments. In settings where unlabelled speech is the\nonly available resource, such embeddings can be used in \"zero-resource\" speech\nsearch, indexing and discovery systems. Here we propose to train a single\nsupervised embedding model on labelled data from multiple well-resourced\nlanguages and then apply it to unseen zero-resource languages. For this\ntransfer learning approach, we consider two multilingual recurrent neural\nnetwork models: a discriminative classifier trained on the joint vocabularies\nof all training languages, and a correspondence autoencoder trained to\nreconstruct word pairs. We test these using a word discrimination task on six\ntarget zero-resource languages. When trained on seven well-resourced languages,\nboth models perform similarly and outperform unsupervised models trained on the\nzero-resource languages. With just a single training language, the second model\nworks better, but performance depends more on the particular training--testing\nlanguage pair.", "published": "2020-02-06 05:53:41", "link": "http://arxiv.org/abs/2002.02109v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Semantic Noise Cleansing of Categorical Data based on Semantic\n  Infusion", "abstract": "Semantic Noise affects text analytics activities for the domain-specific\nindustries significantly. It impedes the text understanding which holds prime\nimportance in the critical decision making tasks. In this work, we formalize\nsemantic noise as a sequence of terms that do not contribute to the narrative\nof the text. We look beyond the notion of standard statistically-based stop\nwords and consider the semantics of terms to exclude the semantic noise. We\npresent a novel Semantic Infusion technique to associate meta-data with the\ncategorical corpus text and demonstrate its near-lossless nature. Based on this\ntechnique, we propose an unsupervised text-preprocessing framework to filter\nthe semantic noise using the context of the terms. Later we present the\nevaluation results of the proposed framework using a web forum dataset from the\nautomobile-domain.", "published": "2020-02-06 13:11:46", "link": "http://arxiv.org/abs/2002.02238v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Irony Detection in a Multilingual Context", "abstract": "This paper proposes the first multilingual (French, English and Arabic) and\nmulticultural (Indo-European languages vs. less culturally close languages)\nirony detection system. We employ both feature-based models and neural\narchitectures using monolingual word representation. We compare the performance\nof these systems with state-of-the-art systems to identify their capabilities.\nWe show that these monolingual models trained separately on different languages\nusing multilingual word representation or text-based features can open the door\nto irony detection in languages that lack of annotated data for irony.", "published": "2020-02-06 18:23:27", "link": "http://arxiv.org/abs/2002.02427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Embeddings Inherently Recover the Conceptual Organization of the\n  Human Mind", "abstract": "Machine learning is a means to uncover deep patterns from rich sources of\ndata. Here, we find that machine learning can recover the conceptual\norganization of the human mind when applied to the natural language use of\nmillions of people. Utilizing text from billions of webpages, we recover most\nof the concepts contained in English, Dutch, and Japanese, as represented in\nlarge scale Word Association networks. Our results justify machine learning as\na means to probe the human mind, at a depth and scale that has been\nunattainable using self-report and observational methods. Beyond direct\npsychological applications, our methods may prove useful for projects concerned\nwith defining, assessing, relating, or uncovering concepts in any scientific\nfield.", "published": "2020-02-06 23:45:50", "link": "http://arxiv.org/abs/2002.10284v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Source separation with weakly labelled data: An approach to\n  computational auditory scene analysis", "abstract": "Source separation is the task to separate an audio recording into individual\nsound sources. Source separation is fundamental for computational auditory\nscene analysis. Previous work on source separation has focused on separating\nparticular sound classes such as speech and music. Many of previous work\nrequire mixture and clean source pairs for training. In this work, we propose a\nsource separation framework trained with weakly labelled data. Weakly labelled\ndata only contains the tags of an audio clip, without the occurrence time of\nsound events. We first train a sound event detection system with AudioSet. The\ntrained sound event detection system is used to detect segments that are mostly\nlike to contain a target sound event. Then a regression is learnt from a\nmixture of two randomly selected segments to a target segment conditioned on\nthe audio tagging prediction of the target segment. Our proposed system can\nseparate 527 kinds of sound classes from AudioSet within a single system. A\nU-Net is adopted for the separation system and achieves an average SDR of 5.67\ndB over 527 sound classes in AudioSet.", "published": "2020-02-06 02:00:05", "link": "http://arxiv.org/abs/2002.02065v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Multi-channel Speech Recognition using Frequency Aligned Network", "abstract": "Conventional speech enhancement technique such as beamforming has known\nbenefits for far-field speech recognition. Our own work in frequency-domain\nmulti-channel acoustic modeling has shown additional improvements by training a\nspatial filtering layer jointly within an acoustic model. In this paper, we\nfurther develop this idea and use frequency aligned network for robust\nmulti-channel automatic speech recognition (ASR). Unlike an affine layer in the\nfrequency domain, the proposed frequency aligned component prevents one\nfrequency bin influencing other frequency bins. We show that this modification\nnot only reduces the number of parameters in the model but also significantly\nand improves the ASR performance. We investigate effects of frequency aligned\nnetwork through ASR experiments on the real-world far-field data where users\nare interacting with an ASR system in uncontrolled acoustic environments. We\nshow that our multi-channel acoustic model with a frequency aligned network\nshows up to 18% relative reduction in word error rate.", "published": "2020-02-06 21:47:39", "link": "http://arxiv.org/abs/2002.02520v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fully-hierarchical fine-grained prosody modeling for interpretable\n  speech synthesis", "abstract": "This paper proposes a hierarchical, fine-grained and interpretable latent\nvariable model for prosody based on the Tacotron 2 text-to-speech model. It\nachieves multi-resolution modeling of prosody by conditioning finer level\nrepresentations on coarser level ones. Additionally, it imposes hierarchical\nconditioning across all latent dimensions using a conditional variational\nauto-encoder (VAE) with an auto-regressive structure. Evaluation of\nreconstruction performance illustrates that the new structure does not degrade\nthe model while allowing better interpretability. Interpretations of prosody\nattributes are provided together with the comparison between word-level and\nphone-level prosody representations. Moreover, both qualitative and\nquantitative evaluations are used to demonstrate the improvement in the\ndisentanglement of the latent dimensions.", "published": "2020-02-06 12:52:03", "link": "http://arxiv.org/abs/2002.03785v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Generating diverse and natural text-to-speech samples using a quantized\n  fine-grained VAE and auto-regressive prosody prior", "abstract": "Recent neural text-to-speech (TTS) models with fine-grained latent features\nenable precise control of the prosody of synthesized speech. Such models\ntypically incorporate a fine-grained variational autoencoder (VAE) structure,\nextracting latent features at each input token (e.g., phonemes). However,\ngenerating samples with the standard VAE prior often results in unnatural and\ndiscontinuous speech, with dramatic prosodic variation between tokens. This\npaper proposes a sequential prior in a discrete latent space which can generate\nmore naturally sounding samples. This is accomplished by discretizing the\nlatent features using vector quantization (VQ), and separately training an\nautoregressive (AR) prior model over the result. We evaluate the approach using\nlistening tests, objective metrics of automatic speech recognition (ASR)\nperformance, and measurements of prosody attributes. Experimental results show\nthat the proposed model significantly improves the naturalness in random sample\ngeneration. Furthermore, initial experiments demonstrate that randomly sampling\nfrom the proposed model can be used as data augmentation to improve the ASR\nperformance.", "published": "2020-02-06 12:35:50", "link": "http://arxiv.org/abs/2002.03788v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "An initial investigation on optimizing tandem speaker verification and\n  countermeasure systems using reinforcement learning", "abstract": "The spoofing countermeasure (CM) systems in automatic speaker verification\n(ASV) are not typically used in isolation of each other. These systems can be\ncombined, for example, into a cascaded system where CM produces first a\ndecision whether the input is synthetic or bona fide speech. In case the CM\ndecides it is a bona fide sample, then the ASV system will consider it for\nspeaker verification. End users of the system are not interested in the\nperformance of the individual sub-modules, but instead are interested in the\nperformance of the combined system. Such combination can be evaluated with\ntandem detection cost function (t-DCF) measure, yet the individual components\nare trained separately from each other using their own performance metrics. In\nthis work we study training the ASV and CM components together for a better\nt-DCF measure by using reinforcement learning. We demonstrate that such\ntraining procedure indeed is able to improve the performance of the combined\nsystem, and does so with more reliable results than with the standard\nsupervised learning techniques we compare against.", "published": "2020-02-06 15:13:49", "link": "http://arxiv.org/abs/2002.03801v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Continuous Silent Speech Recognition using EEG", "abstract": "In this paper we explore continuous silent speech recognition using\nelectroencephalography (EEG) signals. We implemented a connectionist temporal\nclassification (CTC) automatic speech recognition (ASR) model to translate EEG\nsignals recorded in parallel while subjects were reading English sentences in\ntheir mind without producing any voice to text. Our results demonstrate the\nfeasibility of using EEG signals for performing continuous silent speech\nrecognition. We demonstrate our results for a limited English vocabulary\nconsisting of 30 unique sentences.", "published": "2020-02-06 18:28:45", "link": "http://arxiv.org/abs/2002.03851v7", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Attentional networks for music generation", "abstract": "Realistic music generation has always remained as a challenging problem as it\nmay lack structure or rationality. In this work, we propose a deep learning\nbased music generation method in order to produce old style music particularly\nJAZZ with rehashed melodic structures utilizing a Bi-directional Long Short\nTerm Memory (Bi-LSTM) Neural Network with Attention. Owing to the success in\nmodelling long-term temporal dependencies in sequential data and its success in\ncase of videos, Bi-LSTMs with attention serve as the natural choice and early\nutilization in music generation. We validate in our experiments that Bi-LSTMs\nwith attention are able to preserve the richness and technical nuances of the\nmusic performed.", "published": "2020-02-06 13:26:17", "link": "http://arxiv.org/abs/2002.03854v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
