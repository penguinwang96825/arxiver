{"title": "Supertagging Combinatory Categorial Grammar with Attentive Graph\n  Convolutional Networks", "abstract": "Supertagging is conventionally regarded as an important task for combinatory\ncategorial grammar (CCG) parsing, where effective modeling of contextual\ninformation is highly important to this task. However, existing studies have\nmade limited efforts to leverage contextual features except for applying\npowerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph\nconvolutional networks to enhance neural CCG supertagging through a novel\nsolution of leveraging contextual information. Specifically, we build the graph\nfrom chunks (n-grams) extracted from a lexicon and apply attention over the\ngraph, so that different word pairs from the contexts within and across chunks\nare weighted in the model and facilitate the supertagging accordingly. The\nexperiments performed on the CCGbank demonstrate that our approach outperforms\nall previous studies in terms of both supertagging and parsing. Further\nanalyses illustrate the effectiveness of each component in our approach to\ndiscriminatively learn from word pairs to enhance CCG supertagging.", "published": "2020-10-13 01:58:29", "link": "http://arxiv.org/abs/2010.06115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking Crowdworkers to Write Entailment Examples: The Best of Bad\n  Options", "abstract": "Large-scale natural language inference (NLI) datasets such as SNLI or MNLI\nhave been created by asking crowdworkers to read a premise and write three new\nhypotheses, one for each possible semantic relationships (entailment,\ncontradiction, and neutral). While this protocol has been used to create useful\nbenchmark data, it remains unclear whether the writing-based annotation\nprotocol is optimal for any purpose, since it has not been evaluated directly.\nFurthermore, there is ample evidence that crowdworker writing can introduce\nartifacts in the data. We investigate two alternative protocols which\nautomatically create candidate (premise, hypothesis) pairs for annotators to\nlabel. Using these protocols and a writing-based baseline, we collect several\nnew English NLI datasets of over 3k examples each, each using a fixed amount of\nannotator time, but a varying number of examples to fit that time budget. Our\nexperiments on NLI and transfer learning show negative results: None of the\nalternative protocols outperforms the baseline in evaluations of generalization\nwithin NLI or on transfer to outside target tasks. We conclude that crowdworker\nwriting still the best known option for entailment data, highlighting the need\nfor further data collection work to focus on improving writing-based annotation\nprocesses.", "published": "2020-10-13 02:27:05", "link": "http://arxiv.org/abs/2010.06122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corruption Is Not All Bad: Incorporating Discourse Structure into\n  Pre-training via Corruption for Essay Scoring", "abstract": "Existing approaches for automated essay scoring and document representation\nlearning typically rely on discourse parsers to incorporate discourse structure\ninto text representation. However, the performance of parsers is not always\nadequate, especially when they are used on noisy texts, such as student essays.\nIn this paper, we propose an unsupervised pre-training approach to capture\ndiscourse structure of essays in terms of coherence and cohesion that does not\nrequire any discourse parser or annotation. We introduce several types of\ntoken, sentence and paragraph-level corruption techniques for our proposed\npre-training approach and augment masked language modeling pre-training with\nour pre-training method to leverage both contextualized and discourse\ninformation. Our proposed unsupervised approach achieves new state-of-the-art\nresult on essay Organization scoring task.", "published": "2020-10-13 03:17:34", "link": "http://arxiv.org/abs/2010.06137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating BERT into Parallel Sequence Decoding with Adapters", "abstract": "While large scale pre-trained language models such as BERT have achieved\ngreat success on various natural language understanding tasks, how to\nefficiently and effectively incorporate them into sequence-to-sequence models\nand the corresponding text generation tasks remains a non-trivial problem. In\nthis paper, we propose to address this problem by taking two different BERT\nmodels as the encoder and decoder respectively, and fine-tuning them by\nintroducing simple and lightweight adapter modules, which are inserted between\nBERT layers and tuned on the task-specific dataset. In this way, we obtain a\nflexible and efficient model which is able to jointly leverage the information\ncontained in the source-side and target-side BERT models, while bypassing the\ncatastrophic forgetting problem. Each component in the framework can be\nconsidered as a plug-in unit, making the framework flexible and task agnostic.\nOur framework is based on a parallel sequence decoding algorithm named\nMask-Predict considering the bi-directional and conditional independent nature\nof BERT, and can be adapted to traditional autoregressive decoding easily. We\nconduct extensive experiments on neural machine translation tasks where the\nproposed method consistently outperforms autoregressive baselines while\nreducing the inference latency by half, and achieves $36.49$/$33.57$ BLEU\nscores on IWSLT14 German-English/WMT14 German-English translation. When adapted\nto autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEU\nscores on WMT14 English-German/English-French translation, on par with the\nstate-of-the-art baseline models.", "published": "2020-10-13 03:25:15", "link": "http://arxiv.org/abs/2010.06138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The workweek is the best time to start a family -- A Study of GPT-2\n  Based Claim Generation", "abstract": "Argument generation is a challenging task whose research is timely\nconsidering its potential impact on social media and the dissemination of\ninformation. Here we suggest a pipeline based on GPT-2 for generating coherent\nclaims, and explore the types of claims that it produces, and their veracity,\nusing an array of manual and automatic assessments. In addition, we explore the\ninterplay between this task and the task of Claim Retrieval, showing how they\ncan complement one another.", "published": "2020-10-13 05:22:30", "link": "http://arxiv.org/abs/2010.06185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained\n  Language Models", "abstract": "Language models (LMs) have proven surprisingly successful at capturing\nfactual knowledge by completing cloze-style fill-in-the-blank questions such as\n\"Punta Cana is located in _.\" However, while knowledge is both written and\nqueried in many languages, studies on LMs' factual representation ability have\nalmost invariably been performed on English. To assess factual knowledge\nretrieval in LMs in different languages, we create a multilingual benchmark of\ncloze-style probes for 23 typologically diverse languages. To properly handle\nlanguage variations, we expand probing methods from single- to multi-word\nentities, and develop several decoding algorithms to generate multi-token\npredictions. Extensive experimental results provide insights about how well (or\npoorly) current state-of-the-art LMs perform at this task in languages with\nmore or fewer available resources. We further propose a code-switching-based\nmethod to improve the ability of multilingual LMs to access knowledge, and\nverify its effectiveness on several benchmark languages. Benchmark data and\ncode have been released at https://x-factr.github.io.", "published": "2020-10-13 05:29:56", "link": "http://arxiv.org/abs/2010.06189v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias in Machine Translation with Target Gender\n  Annotations", "abstract": "When translating \"The secretary asked for details.\" to a language with\ngrammatical gender, it might be necessary to determine the gender of the\nsubject \"secretary\". If the sentence does not contain the necessary\ninformation, it is not always possible to disambiguate. In such cases, machine\ntranslation systems select the most common translation option, which often\ncorresponds to the stereotypical translations, thus potentially exacerbating\nprejudice and marginalisation of certain groups and people. We argue that the\ninformation necessary for an adequate translation can not always be deduced\nfrom the sentence being translated or even might depend on external knowledge.\nTherefore, in this work, we propose to decouple the task of acquiring the\nnecessary information from the task of learning to translate correctly when\nsuch information is available. To that end, we present a method for training\nmachine translation systems to use word-level annotations containing\ninformation about subject's gender. To prepare training data, we annotate\nregular source language words with grammatical gender information of the\ncorresponding target language words. Using such data to train machine\ntranslation systems reduces their reliance on gender stereotypes when\ninformation about the subject's gender is available. Our experiments on five\nlanguage pairs show that this allows improving accuracy on the WinoMT test set\nby up to 25.8 percentage points.", "published": "2020-10-13 07:07:59", "link": "http://arxiv.org/abs/2010.06203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KLearn: Background Knowledge Inference from Summarization Data", "abstract": "The goal of text summarization is to compress documents to the relevant\ninformation while excluding background information already known to the\nreceiver. So far, summarization researchers have given considerably more\nattention to relevance than to background knowledge. In contrast, this work\nputs background knowledge in the foreground. Building on the realization that\nthe choices made by human summarizers and annotators contain implicit\ninformation about their background knowledge, we develop and compare techniques\nfor inferring background knowledge from summarization data. Based on this\nframework, we define summary scoring functions that explicitly model background\nknowledge, and show that these scoring functions fit human judgments\nsignificantly better than baselines. We illustrate some of the many potential\napplications of our framework. First, we provide insights into human\ninformation importance priors. Second, we demonstrate that averaging the\nbackground knowledge of multiple, potentially biased annotators or corpora\ngreatly improves summary-scoring performance. Finally, we discuss potential\napplications of our framework beyond summarization.", "published": "2020-10-13 07:42:25", "link": "http://arxiv.org/abs/2010.06213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotationsaurus: A Searchable Directory of Annotation Tools", "abstract": "Manual annotation of textual documents is a necessary task when constructing\nbenchmark corpora for training and evaluating machine learning algorithms. We\ncreated a comprehensive directory of annotation tools that currently includes\n93 tools. We analyzed the tools over a set of 31 features and implemented\nsimple scripts and a Web application that filters the tools based on chosen\ncriteria. We present two use cases using the directory and propose ideas for\nits maintenance. The directory, source codes for scripts, and link to the Web\napplication are available at: https://github.com/mariananeves/annotation-tools", "published": "2020-10-13 09:22:48", "link": "http://arxiv.org/abs/2010.06251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F1 is Not Enough! Models and Evaluation Towards User-Centered\n  Explainable Question Answering", "abstract": "Explainable question answering systems predict an answer together with an\nexplanation showing why the answer has been selected. The goal is to enable\nusers to assess the correctness of the system and understand its reasoning\nprocess. However, we show that current models and evaluation settings have\nshortcomings regarding the coupling of answer and explanation which might cause\nserious issues in user experience. As a remedy, we propose a hierarchical model\nand a new regularization term to strengthen the answer-explanation coupling as\nwell as two evaluation scores to quantify the coupling. We conduct experiments\non the HOTPOTQA benchmark data set and perform a user study. The user study\nshows that our models increase the ability of the users to judge the\ncorrectness of the system and that scores like F1 are not enough to estimate\nthe usefulness of a model in a practical setting with human users. Our scores\nare better aligned with user experience, making them promising candidates for\nmodel selection.", "published": "2020-10-13 10:53:20", "link": "http://arxiv.org/abs/2010.06283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Implicit Discourse Relation Recognition to the PDTB-3", "abstract": "The PDTB-3 contains many more Implicit discourse relations than the previous\nPDTB-2. This is in part because implicit relations have now been annotated\nwithin sentences as well as between them. In addition, some now co-occur with\nexplicit discourse relations, instead of standing on their own. Here we show\nthat while this can complicate the problem of identifying the location of\nimplicit discourse relations, it can in turn simplify the problem of\nidentifying their senses. We present data to support this claim, as well as\nmethods that can serve as a non-trivial baseline for future state-of-the-art\nrecognizers for implicit discourse relations.", "published": "2020-10-13 11:19:42", "link": "http://arxiv.org/abs/2010.06294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Supervised Joint-Event-Extraction with Heterogeneous Information\n  Networks", "abstract": "Joint-event-extraction, which extracts structural information (i.e., entities\nor triggers of events) from unstructured real-world corpora, has attracted more\nand more research attention in natural language processing. Most existing works\ndo not fully address the sparse co-occurrence relationships between entities\nand triggers, which loses this important information and thus deteriorates the\nextraction performance. To mitigate this issue, we first define the\njoint-event-extraction as a sequence-to-sequence labeling task with a tag set\ncomposed of tags of triggers and entities. Then, to incorporate the missing\ninformation in the aforementioned co-occurrence relationships, we propose a\nCross-Supervised Mechanism (CSM) to alternately supervise the extraction of\neither triggers or entities based on the type distribution of each other.\nMoreover, since the connected entities and triggers naturally form a\nheterogeneous information network (HIN), we leverage the latent pattern along\nmeta-paths for a given corpus to further improve the performance of our\nproposed method. To verify the effectiveness of our proposed method, we conduct\nextensive experiments on four real-world datasets as well as compare our method\nwith state-of-the-art methods. Empirical results and analysis show that our\napproach outperforms the state-of-the-art methods in both entity and trigger\nextraction.", "published": "2020-10-13 11:51:17", "link": "http://arxiv.org/abs/2010.06310v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAPT: Contrastive Pre-Training for Learning Denoised Sequence\n  Representations", "abstract": "Pre-trained self-supervised models such as BERT have achieved striking\nsuccess in learning sequence representations, especially for natural language\nprocessing. These models typically corrupt the given sequences with certain\ntypes of noise, such as masking, shuffling, or substitution, and then try to\nrecover the original input. However, such pre-training approaches are prone to\nlearning representations that are covariant with the noise, leading to the\ndiscrepancy between the pre-training and fine-tuning stage. To remedy this, we\npresent ContrAstive Pre-Training (CAPT) to learn noise invariant sequence\nrepresentations. The proposed CAPT encourages the consistency between\nrepresentations of the original sequence and its corrupted version via\nunsupervised instance-wise training signals. In this way, it not only\nalleviates the pretrain-finetune discrepancy induced by the noise of\npre-training, but also aids the pre-trained model in better capturing global\nsemantics of the input via more effective sentence-level supervision. Different\nfrom most prior work that focuses on a particular modality, comprehensive\nempirical evidence on 11 natural language understanding and cross-modal tasks\nillustrates that CAPT is applicable for both language and vision-language\ntasks, and obtains surprisingly consistent improvement, including 0.6\\%\nabsolute gain on GLUE benchmarks and 0.8\\% absolute increment on\n$\\text{NLVR}^2$.", "published": "2020-10-13 13:08:34", "link": "http://arxiv.org/abs/2010.06351v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Tatoeba Translation Challenge -- Realistic Data Sets for Low\n  Resource and Multilingual MT", "abstract": "This paper describes the development of a new benchmark for machine\ntranslation that provides training and test data for thousands of language\npairs covering over 500 languages and tools for creating state-of-the-art\ntranslation models from that collection. The main goal is to trigger the\ndevelopment of open translation tools and models with a much broader coverage\nof the World's languages. Using the package it is possible to work on realistic\nlow-resource scenarios avoiding artificially reduced setups that are common\nwhen demonstrating zero-shot or few-shot learning. For the first time, this\npackage provides a comprehensive collection of diverse data sets in hundreds of\nlanguages with systematic language and script annotation and data splits to\nextend the narrow coverage of existing benchmarks. Together with the data\nrelease, we also provide a growing number of pre-trained baseline models for\nindividual language pairs and selected language groups.", "published": "2020-10-13 13:12:21", "link": "http://arxiv.org/abs/2010.06354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained linguistic evaluation for state-of-the-art Machine\n  Translation", "abstract": "This paper describes a test suite submission providing detailed statistics of\nlinguistic performance for the state-of-the-art German-English systems of the\nFifth Conference of Machine Translation (WMT20). The analysis covers 107\nphenomena organized in 14 categories based on about 5,500 test items, including\na manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan)\nappear to have significantly better test suite accuracy than the others,\nalthough the best system of WMT20 is not significantly better than the one from\nWMT19 in a macro-average. Additionally, we identify some linguistic phenomena\nwhere all systems suffer (such as idioms, resultative predicates and\npluperfect), but we are also able to identify particular weaknesses for\nindividual systems (such as quotation marks, lexical ambiguity and sluicing).\nMost of the systems of WMT19 which submitted new versions this year show\nimprovements.", "published": "2020-10-13 13:14:37", "link": "http://arxiv.org/abs/2010.06359v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Attention Models with Human Visual Attention in Machine\n  Reading Comprehension", "abstract": "While neural networks with attention mechanisms have achieved superior\nperformance on many natural language processing tasks, it remains unclear to\nwhich extent learned attention resembles human visual attention. In this paper,\nwe propose a new method that leverages eye-tracking data to investigate the\nrelationship between human visual attention and neural attention in machine\nreading comprehension. To this end, we introduce a novel 23 participant eye\ntracking dataset - MQA-RC, in which participants read movie plots and answered\npre-defined questions. We compare state of the art networks based on long\nshort-term memory (LSTM), convolutional neural models (CNN) and XLNet\nTransformer architectures. We find that higher similarity to human attention\nand performance significantly correlates to the LSTM and CNN models. However,\nwe show this relationship does not hold true for the XLNet models -- despite\nthe fact that the XLNet performs best on this challenging task. Our results\nsuggest that different architectures seem to learn rather different neural\nattention strategies and similarity of neural to human attention does not\nguarantee best performance.", "published": "2020-10-13 13:51:57", "link": "http://arxiv.org/abs/2010.06396v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuSemShift: a dataset of historical lexical semantic change in Russian", "abstract": "We present RuSemShift, a large-scale manually annotated test set for the task\nof semantic change modeling in Russian for two long-term time period pairs:\nfrom the pre-Soviet through the Soviet times and from the Soviet through the\npost-Soviet times. Target words were annotated by multiple crowd-source\nworkers. The annotation process was organized following the DURel framework and\nwas based on sentence contexts extracted from the Russian National Corpus.\nAdditionally, we report the performance of several distributional approaches on\nRuSemShift, achieving promising results, which at the same time leave room for\nother researchers to improve.", "published": "2020-10-13 14:54:05", "link": "http://arxiv.org/abs/2010.06436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource\n  Language", "abstract": "Low-resource languages such as Filipino suffer from data scarcity which makes\nit challenging to develop NLP applications for Filipino language. The use of\nTransfer Learning (TL) techniques alleviates this problem in low-resource\nsetting. In recent years, transformer-based models are proven to be effective\nin low-resource tasks but faces challenges in accessibility due to its high\ncompute and memory requirements. For this reason, there's a need for a cheaper\nbut effective alternative. This paper has three contributions. First, release a\npre-trained AWD-LSTM language model for Filipino language. Second, benchmark\nAWD-LSTM in the Hate Speech classification task and show that it performs on\npar with transformer-based models. Third, analyze the the performance of\nAWD-LSTM in low-resource setting using degradation test and compare it with\ntransformer-based models.\n  -----\n  Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na\ndatos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer\nLearning (TL) techniques ay malaking tulong para sa low-resource setting o mga\npagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga\ntransformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay\nmataas na compute and memory requirements kaya nangangailangan ng mas mura pero\nepektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una,\nmaglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging\ntuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag\nbenchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang\nnitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang\nperformance ng AWD-LSTM sa low-resource setting gamit ang degradation test at\nikumpara ito sa mga transformer-based models.", "published": "2020-10-13 15:06:07", "link": "http://arxiv.org/abs/2010.06447v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Demographic Representation and Collective Storytelling in the Me Too\n  Twitter Hashtag Activism Movement", "abstract": "The #MeToo movement on Twitter has drawn attention to the pervasive nature of\nsexual harassment and violence. While #MeToo has been praised for providing\nsupport for self-disclosures of harassment or violence and shifting societal\nresponse, it has also been criticized for exemplifying how women of color have\nbeen discounted for their historical contributions to and excluded from\nfeminist movements. Through an analysis of over 600,000 tweets from over\n256,000 unique users, we examine online #MeToo conversations across gender and\nracial/ethnic identities and the topics that each demographic emphasized. We\nfound that tweets authored by white women were overrepresented in the movement\ncompared to other demographics, aligning with criticism of unequal\nrepresentation. We found that intersected identities contributed differing\nnarratives to frame the movement, co-opted the movement to raise visibility in\nparallel ongoing movements, employed the same hashtags both critically and\nsupportively, and revived and created new hashtags in response to pivotal\nmoments. Notably, tweets authored by black women often expressed emotional\nsupport and were critical about differential treatment in the justice system\nand by police. In comparison, tweets authored by white women and men often\nhighlighted sexual harassment and violence by public figures and weaved in more\ngeneral political discussions. We discuss the implications of work for digital\nactivism research and design including suggestions to raise visibility by those\nwho were under-represented in this hashtag activism movement. Content warning:\nthis article discusses issues of sexual harassment and violence.", "published": "2020-10-13 15:25:33", "link": "http://arxiv.org/abs/2010.06472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XL-WiC: A Multilingual Benchmark for Evaluating Semantic\n  Contextualization", "abstract": "The ability to correctly model distinct meanings of a word is crucial for the\neffectiveness of semantic representation techniques. However, most existing\nevaluation benchmarks for assessing this criterion are tied to sense\ninventories (usually WordNet), restricting their usage to a small subset of\nknowledge-based representation techniques. The Word-in-Context dataset (WiC)\naddresses the dependence on sense inventories by reformulating the standard\ndisambiguation task as a binary classification problem; but, it is limited to\nthe English language. We put forward a large multilingual benchmark, XL-WiC,\nfeaturing gold standards in 12 new languages from varied language families and\nwith different degrees of resource availability, opening room for evaluation\nscenarios such as zero-shot cross-lingual transfer. We perform a series of\nexperiments to determine the reliability of the datasets and to set performance\nbaselines for several recent contextualized multilingual models. Experimental\nresults show that even when no tagged instances are available for a target\nlanguage, models trained solely on the English data can attain competitive\nperformance in the task of distinguishing different meanings of a word, even\nfor distant languages. XL-WiC is available at\nhttps://pilehvar.github.io/xlwic/.", "published": "2020-10-13 15:32:00", "link": "http://arxiv.org/abs/2010.06478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing the Identification of Cyberbullying through Participant Roles", "abstract": "Cyberbullying is a prevalent social problem that inflicts detrimental\nconsequences to the health and safety of victims such as psychological\ndistress, anti-social behaviour, and suicide. The automation of cyberbullying\ndetection is a recent but widely researched problem, with current research\nhaving a strong focus on a binary classification of bullying versus\nnon-bullying. This paper proposes a novel approach to enhancing cyberbullying\ndetection through role modeling. We utilise a dataset from ASKfm to perform\nmulti-class classification to detect participant roles (e.g. victim, harasser).\nOur preliminary results demonstrate promising performance including 0.83 and\n0.76 of F1-score for cyberbullying and role classification respectively,\noutperforming baselines.", "published": "2020-10-13 19:13:07", "link": "http://arxiv.org/abs/2010.06640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for Multilingual Numerical Understanding in Transformer-Based\n  Language Models", "abstract": "Natural language numbers are an example of compositional structures, where\nlarger numbers are composed of operations on smaller numbers. Given that\ncompositional reasoning is a key to natural language understanding, we propose\nnovel multilingual probing tasks tested on DistilBERT, XLM, and BERT to\ninvestigate for evidence of compositional reasoning over numerical data in\nvarious natural language number systems. By using both grammaticality judgment\nand value comparison classification tasks in English, Japanese, Danish, and\nFrench, we find evidence that the information encoded in these pretrained\nmodels' embeddings is sufficient for grammaticality judgments but generally not\nfor value comparisons. We analyze possible reasons for this and discuss how our\ntasks could be extended in further studies.", "published": "2020-10-13 19:56:02", "link": "http://arxiv.org/abs/2010.06666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sensitivity of BLANC to human-scored qualities of text summaries", "abstract": "We explore the sensitivity of a document summary quality estimator, BLANC, to\nhuman assessment of qualities for the same summaries. In our human evaluations,\nwe distinguish five summary qualities, defined by how fluent, understandable,\ninformative, compact, and factually correct the summary is. We make the case\nfor optimal BLANC parameters, at which the BLANC sensitivity to almost all of\nsummary qualities is about as good as the sensitivity of a human annotator.", "published": "2020-10-13 22:08:11", "link": "http://arxiv.org/abs/2010.06716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Wrong Answer or a Wrong Question? An Intricate Relationship between\n  Question Reformulation and Answer Selection in Conversational Question\n  Answering", "abstract": "The dependency between an adequate question formulation and correct answer\nselection is a very intriguing but still underexplored area. In this paper, we\nshow that question rewriting (QR) of the conversational context allows to shed\nmore light on this phenomenon and also use it to evaluate robustness of\ndifferent answer selection approaches. We introduce a simple framework that\nenables an automated analysis of the conversational question answering (QA)\nperformance using question rewrites, and present the results of this analysis\non the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover\nsensitivity to question formulation of the popular state-of-the-art models for\nreading comprehension and passage ranking. Our results demonstrate that the\nreading comprehension model is insensitive to question formulation, while the\npassage ranking changes dramatically with a little variation in the input\nquestion. The benefit of QR is that it allows us to pinpoint and group such\ncases automatically. We show how to use this methodology to verify whether QA\nmodels are really learning the task or just finding shortcuts in the dataset,\nand better understand the frequent types of error they make.", "published": "2020-10-13 06:29:51", "link": "http://arxiv.org/abs/2010.06835v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReviewRobot: Explainable Paper Review Generation based on Knowledge\n  Synthesis", "abstract": "To assist human review process, we build a novel ReviewRobot to automatically\nassign a review score and write comments for multiple categories such as\nnovelty and meaningful comparison. A good review needs to be knowledgeable,\nnamely that the comments should be constructive and informative to help improve\nthe paper; and explainable by providing detailed evidence. ReviewRobot achieves\nthese goals via three steps: (1) We perform domain-specific Information\nExtraction to construct a knowledge graph (KG) from the target paper under\nreview, a related work KG from the papers cited by the target paper, and a\nbackground KG from a large collection of previous papers in the domain. (2) By\ncomparing these three KGs, we predict a review score and detailed structured\nknowledge as evidence for each review category. (3) We carefully select and\ngeneralize human review sentences into templates, and apply these templates to\ntransform the review scores and evidence into natural language comments.\nExperimental results show that our review score predictor reaches 71.4%-100%\naccuracy. Human assessment by domain experts shows that 41.7%-70.5% of the\ncomments generated by ReviewRobot are valid and constructive, and better than\nhuman-written ones for 20% of the time. Thus, ReviewRobot can serve as an\nassistant for paper reviewers, program chairs and authors.", "published": "2020-10-13 02:17:58", "link": "http://arxiv.org/abs/2010.06119v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model Selection for Cross-Lingual Transfer", "abstract": "Transformers that are pre-trained on multilingual corpora, such as, mBERT and\nXLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In\nthe zero-shot transfer setting, only English training data is used, and the\nfine-tuned model is evaluated on another target language. While this works\nsurprisingly well, substantial variance has been observed in target language\nperformance between different fine-tuning runs, and in the zero-shot setup, no\ntarget-language development data is available to select among multiple\nfine-tuned models. Prior work has relied on English dev data to select among\nmodels that are fine-tuned with different learning rates, number of steps and\nother hyperparameters, often resulting in suboptimal choices. In this paper, we\nshow that it is possible to select consistently better models when small\namounts of annotated data are available in auxiliary pivot languages. We\npropose a machine learning approach to model selection that uses the fine-tuned\nmodel's own internal representations to predict its cross-lingual capabilities.\nIn extensive experiments we find that this method consistently selects better\nmodels than English validation data across twenty five languages (including\neight low-resource languages), and often achieves results that are comparable\nto model selection using target language development data.", "published": "2020-10-13 02:36:48", "link": "http://arxiv.org/abs/2010.06127v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth\n  Mover's Distance", "abstract": "Pre-trained language models (e.g., BERT) have achieved significant success in\nvarious natural language processing (NLP) tasks. However, high storage and\ncomputational costs obstruct pre-trained language models to be effectively\ndeployed on resource-constrained devices. In this paper, we propose a novel\nBERT distillation method based on many-to-many layer mapping, which allows each\nintermediate student layer to learn from any intermediate teacher layers. In\nthis way, our model can learn from different teacher layers adaptively for\nvarious NLP tasks. %motivated by the intuition that different NLP tasks require\ndifferent levels of linguistic knowledge contained in the intermediate layers\nof BERT. In addition, we leverage Earth Mover's Distance (EMD) to compute the\nminimum cumulative cost that must be paid to transform knowledge from teacher\nnetwork to student network. EMD enables the effective matching for many-to-many\nlayer mapping. %EMD can be applied to network layers with different sizes and\neffectively measures semantic distance between the teacher network and student\nnetwork. Furthermore, we propose a cost attention mechanism to learn the layer\nweights used in EMD automatically, which is supposed to further improve the\nmodel's performance and accelerate convergence time. Extensive experiments on\nGLUE benchmark demonstrate that our model achieves competitive performance\ncompared to strong competitors in terms of both accuracy and model compression.", "published": "2020-10-13 02:53:52", "link": "http://arxiv.org/abs/2010.06133v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Text Generation Evaluation with Batch Centering and Tempered\n  Word Mover Distance", "abstract": "Recent advances in automatic evaluation metrics for text have shown that deep\ncontextualized word representations, such as those generated by BERT encoders,\nare helpful for designing metrics that correlate well with human judgements. At\nthe same time, it has been argued that contextualized word representations\nexhibit sub-optimal statistical properties for encoding the true similarity\nbetween words or sentences. In this paper, we present two techniques for\nimproving encoding representations for similarity metrics: a batch-mean\ncentering strategy that improves statistical properties; and a computationally\nefficient tempered Word Mover Distance, for better fusion of the information in\nthe contextualized word representations. We conduct numerical experiments that\ndemonstrate the robustness of our techniques, reporting results over various\nBERT-backbone learned metrics and achieving state of the art correlation with\nhuman ratings on several benchmarks.", "published": "2020-10-13 03:46:25", "link": "http://arxiv.org/abs/2010.06150v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mathematical Word Problem Generation from Commonsense Knowledge Graph\n  and Equations", "abstract": "There is an increasing interest in the use of mathematical word problem (MWP)\ngeneration in educational assessment. Different from standard natural question\ngeneration, MWP generation needs to maintain the underlying mathematical\noperations between quantities and variables, while at the same time ensuring\nthe relevance between the output and the given topic. To address above problem,\nwe develop an end-to-end neural model to generate diverse MWPs in real-world\nscenarios from commonsense knowledge graph and equations. The proposed model\n(1) learns both representations from edge-enhanced Levi graphs of symbolic\nequations and commonsense knowledge; (2) automatically fuses equation and\ncommonsense knowledge information via a self-planning module when generating\nthe MWPs. Experiments on an educational gold-standard set and a large-scale\ngenerated MWP set show that our approach is superior on the MWP generation\ntask, and it outperforms the SOTA models in terms of both automatic evaluation\nmetrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e.,\nequation relevance, topic relevance, and language coherence. To encourage\nreproducible results, we make our code and MWP dataset public available at\n\\url{https://github.com/tal-ai/MaKE_EMNLP2021}.", "published": "2020-10-13 06:31:53", "link": "http://arxiv.org/abs/2010.06196v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Extractive Text Summarization with Topic-Aware Graph Neural\n  Networks", "abstract": "Text summarization aims to compress a textual document to a short summary\nwhile keeping salient information. Extractive approaches are widely used in\ntext summarization because of their fluency and efficiency. However, most of\nexisting extractive models hardly capture inter-sentence relationships,\nparticularly in long documents. They also often ignore the effect of topical\ninformation on capturing important contents. To address these issues, this\npaper proposes a graph neural network (GNN)-based extractive summarization\nmodel, enabling to capture inter-sentence relationships efficiently via\ngraph-structured document representation. Moreover, our model integrates a\njoint neural topic model (NTM) to discover latent topics, which can provide\ndocument-level features for sentence selection. The experimental results\ndemonstrate that our model not only substantially achieves state-of-the-art\nresults on CNN/DM and NYT datasets but also considerably outperforms existing\napproaches on scientific paper datasets consisting of much longer documents,\nindicating its better robustness in document genres and lengths. Further\ndiscussions show that topical information can help the model preselect salient\ncontents from an entire document, which interprets its effectiveness in long\ndocument summarization.", "published": "2020-10-13 09:30:04", "link": "http://arxiv.org/abs/2010.06253v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting\n  the (Graded) Effect of Context in Word Similarity", "abstract": "This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded\nWord Similarity in Context. The system utilises state-of-the-art contextualised\nword embeddings, which have some task-specific adaptations, including stacked\nembeddings and average embeddings. Overall, the approach achieves good\nevaluation scores across all the languages, while maintaining simplicity.\nFollowing the final rankings, our approach is ranked within the top 5 solutions\nof each language while preserving the 1st position of Finnish subtask 2.", "published": "2020-10-13 10:25:18", "link": "http://arxiv.org/abs/2010.06269v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BRUMS at SemEval-2020 Task 12 : Transformer based Multilingual Offensive\n  Language Identification in Social Media", "abstract": "In this paper, we describe the team \\textit{BRUMS} entry to OffensEval 2:\nMultilingual Offensive Language Identification in Social Media in SemEval-2020.\nThe OffensEval organizers provided participants with annotated datasets\ncontaining posts from social media in Arabic, Danish, English, Greek and\nTurkish. We present a multilingual deep learning model to identify offensive\nlanguage in social media. Overall, the approach achieves acceptable evaluation\nscores, while maintaining flexibility between languages.", "published": "2020-10-13 10:39:14", "link": "http://arxiv.org/abs/2010.06278v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling the Music Genre Perception across Language-Bound Cultures", "abstract": "The music genre perception expressed through human annotations of artists or\nalbums varies significantly across language-bound cultures. These variations\ncannot be modeled as mere translations since we also need to account for\ncultural differences in the music genre perception. In this work, we study the\nfeasibility of obtaining relevant cross-lingual, culture-specific music genre\nannotations based only on language-specific semantic representations, namely\ndistributed concept embeddings and ontologies. Our study, focused on six\nlanguages, shows that unsupervised cross-lingual music genre annotation is\nfeasible with high accuracy, especially when combining both types of\nrepresentations. This approach of studying music genres is the most extensive\nto date and has many implications in musicology and music information\nretrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus\nto benchmark state of the art multilingual pre-trained embedding models.", "published": "2020-10-13 12:20:32", "link": "http://arxiv.org/abs/2010.06325v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aspect-based Document Similarity for Research Papers", "abstract": "Traditional document similarity measures provide a coarse-grained distinction\nbetween similar and dissimilar documents. Typically, they do not consider in\nwhat aspects two documents are similar. This limits the granularity of\napplications like recommender systems that rely on document similarity. In this\npaper, we extend similarity with aspect information by performing a pairwise\ndocument classification task. We evaluate our aspect-based document similarity\nfor research papers. Paper citations indicate the aspect-based similarity,\ni.e., the section title in which a citation occurs acts as a label for the pair\nof citing and cited paper. We apply a series of Transformer models such as\nRoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM\nbaseline. We perform our experiments on two newly constructed datasets of\n172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our\nresults show SciBERT as the best performing system. A qualitative examination\nvalidates our quantitative results. Our findings motivate future research of\naspect-based document similarity and the development of a recommender system\nbased on the evaluated techniques. We make our datasets, code, and trained\nmodels publicly available.", "published": "2020-10-13 13:51:21", "link": "http://arxiv.org/abs/2010.06395v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.", "published": "2020-10-13 15:20:32", "link": "http://arxiv.org/abs/2010.06467v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Autotuning Search Space for Loop Transformations", "abstract": "One of the challenges for optimizing compilers is to predict whether applying\nan optimization will improve its execution speed. Programmers may override the\ncompiler's profitability heuristic using optimization directives such as\npragmas in the source code. Machine learning in the form of autotuning can\nassist users in finding the best optimizations for each platform.\n  In this paper we propose a loop transformation search space that takes the\nform of a tree, in contrast to previous approaches that usually use vector\nspaces to represent loop optimization configurations. We implemented a simple\nautotuner exploring the search space and applied it to a selected set of\nPolyBench kernels. While the autotuner is capable of representing every\npossible sequence of loop transformations and their relations, the results\nmotivate the use of better search strategies such as Monte Carlo tree search to\nfind sophisticated loop transformations such as multilevel tiling.", "published": "2020-10-13 16:26:57", "link": "http://arxiv.org/abs/2010.06521v1", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Controlling the Interaction Between Generation and Inference in\n  Semi-Supervised Variational Autoencoders Using Importance Weighting", "abstract": "Even though Variational Autoencoders (VAEs) are widely used for\nsemi-supervised learning, the reason why they work remains unclear. In fact,\nthe addition of the unsupervised objective is most often vaguely described as a\nregularization. The strength of this regularization is controlled by\ndown-weighting the objective on the unlabeled part of the training set. Through\nan analysis of the objective of semi-supervised VAEs, we observe that they use\nthe posterior of the learned generative model to guide the inference model in\nlearning the partially observed latent variable. We show that given this\nobservation, it is possible to gain finer control on the effect of the\nunsupervised objective on the training procedure. Using importance weighting,\nwe derive two novel objectives that prioritize either one of the partially\nobserved latent variable, or the unobserved latent variable. Experiments on the\nIMDB english sentiment analysis dataset and on the AG News topic classification\ndataset show the improvements brought by our prioritization mechanism and\nexhibit a behavior that is inline with our description of the inner working of\nSemi-Supervised VAEs.", "published": "2020-10-13 17:01:40", "link": "http://arxiv.org/abs/2010.06549v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Does my multimodal model learn cross-modal interactions? It's harder to\n  tell than you might think!", "abstract": "Modeling expressive cross-modal interactions seems crucial in multimodal\ntasks, such as visual question answering. However, sometimes high-performing\nblack-box algorithms turn out to be mostly exploiting unimodal signals in the\ndata. We propose a new diagnostic tool, empirical multimodally-additive\nfunction projection (EMAP), for isolating whether or not cross-modal\ninteractions improve performance for a given model on a given task. This\nfunction projection modifies model predictions so that cross-modal interactions\nare eliminated, isolating the additive, unimodal structure. For seven\nimage+text classification tasks (on each of which we set new state-of-the-art\nbenchmarks), we find that, in many cases, removing cross-modal interactions\nresults in little to no performance degradation. Surprisingly, this holds even\nwhen expressive models, with capacity to consider interactions, otherwise\noutperform less expressive models; thus, performance improvements, even when\npresent, often cannot be attributed to consideration of cross-modal feature\ninteractions. We hence recommend that researchers in multimodal machine\nlearning report the performance not only of unimodal baselines, but also the\nEMAP of their best-performing model.", "published": "2020-10-13 17:45:28", "link": "http://arxiv.org/abs/2010.06572v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Fantastic Features and Where to Find Them: Detecting Cognitive\n  Impairment with a Subsequence Classification Guided Approach", "abstract": "Despite the widely reported success of embedding-based machine learning\nmethods on natural language processing tasks, the use of more easily\ninterpreted engineered features remains common in fields such as cognitive\nimpairment (CI) detection. Manually engineering features from noisy text is\ntime and resource consuming, and can potentially result in features that do not\nenhance model performance. To combat this, we describe a new approach to\nfeature engineering that leverages sequential machine learning models and\ndomain knowledge to predict which features help enhance performance. We provide\na concrete example of this method on a standard data set of CI speech and\ndemonstrate that CI classification accuracy improves by 2.3% over a strong\nbaseline when using features produced by this method. This demonstration\nprovides an ex-ample of how this method can be used to assist classification in\nfields where interpretability is important, such as health care.", "published": "2020-10-13 17:57:18", "link": "http://arxiv.org/abs/2010.06579v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Networks: a Practical Approach", "abstract": "This manuscript provides a short and practical introduction to the topic of\nlanguage networks. This text aims at assisting researchers with no practical\nexperience in text and/or network analysis. We provide a practical tutorial on\nhow to model and characterize texts using network-based features. In this\ntutorial, we also include examples of pre-processing and network\nrepresentations. A brief description of the main tasks allying network science\nand text analysis is also provided. A further development of this text shall\ninclude a practical description of network classification via machine learning\nmethods.", "published": "2020-10-13 21:51:14", "link": "http://arxiv.org/abs/2010.06710v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and\n  Relation Transferring", "abstract": "Taxonomy is not only a fundamental form of knowledge representation, but also\ncrucial to vast knowledge-rich applications, such as question answering and web\nsearch. Most existing taxonomy construction methods extract hypernym-hyponym\nentity pairs to organize a \"universal\" taxonomy. However, these generic\ntaxonomies cannot satisfy user's specific interest in certain areas and\nrelations. Moreover, the nature of instance taxonomy treats each node as a\nsingle word, which has low semantic coverage. In this paper, we propose a\nmethod for seed-guided topical taxonomy construction, which takes a corpus and\na seed taxonomy described by concept names as input, and constructs a more\ncomplete taxonomy based on user's interest, wherein each node is represented by\na cluster of coherent terms. Our framework, CoRel, has two modules to fulfill\nthis goal. A relation transferring module learns and transfers the user's\ninterested relation along multiple paths to expand the seed taxonomy structure\nin width and depth. A concept learning module enriches the semantics of each\nconcept node by jointly embedding the taxonomy and text. Comprehensive\nexperiments conducted on real-world datasets show that Corel generates\nhigh-quality topical taxonomies and outperforms all the baselines\nsignificantly.", "published": "2020-10-13 22:00:31", "link": "http://arxiv.org/abs/2010.06714v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RGCL at SemEval-2020 Task 6: Neural Approaches to Definition Extraction", "abstract": "This paper presents the RGCL team submission to SemEval 2020 Task 6:\nDeftEval, subtasks 1 and 2. The system classifies definitions at the sentence\nand token levels. It utilises state-of-the-art neural network architectures,\nwhich have some task-specific adaptations, including an automatically extended\ntraining set. Overall, the approach achieves acceptable evaluation scores,\nwhile maintaining flexibility in architecture selection.", "published": "2020-10-13 10:48:15", "link": "http://arxiv.org/abs/2010.06281v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Argument Mining: Datasets and Analysis", "abstract": "The growing interest in argument mining and computational argumentation\nbrings with it a plethora of Natural Language Understanding (NLU) tasks and\ncorresponding datasets. However, as with many other NLU tasks, the dominant\nlanguage is English, with resources in other languages being few and far\nbetween. In this work, we explore the potential of transfer learning using the\nmultilingual BERT model to address argument mining tasks in non-English\nlanguages, based on English datasets and the use of machine translation. We\nshow that such methods are well suited for classifying the stance of arguments\nand detecting evidence, but less so for assessing the quality of arguments,\npresumably because quality is harder to preserve under translation. In\naddition, focusing on the translate-train approach, we show how the choice of\nlanguages for translation, and the relations among them, affect the accuracy of\nthe resultant model. Finally, to facilitate evaluation of transfer learning on\nargument mining tasks, we provide a human-generated dataset with more than 10k\narguments in multiple languages, as well as machine translation of the English\ndatasets.", "published": "2020-10-13 14:49:10", "link": "http://arxiv.org/abs/2010.06432v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Urban Outdoor Perception from Geolocated\n  Free-Texts", "abstract": "The automatic extraction of urban perception shared by people on\nlocation-based social networks (LBSNs) is an important multidisciplinary\nresearch goal. One of the reasons is because it facilitates the understanding\nof the intrinsic characteristics of urban areas in a scalable way, helping to\nleverage new services. However, content shared on LBSNs is diverse,\nencompassing several topics, such as politics, sports, culture, religion, and\nurban perceptions, making the task of content extraction regarding a particular\ntopic very challenging. Considering free-text messages shared on LBSNs, we\npropose an automatic and generic approach to extract people's perceptions. For\nthat, our approach explores opinions that are spatial-temporal and semantically\nsimilar. We exemplify our approach in the context of urban outdoor areas in\nChicago, New York City and London. Studying those areas, we found evidence that\nLBSN data brings valuable information about urban regions. To analyze and\nvalidate our outcomes, we conducted a temporal analysis to measure the results'\nrobustness over time. We show that our approach can be helpful to better\nunderstand urban areas considering different perspectives. We also conducted a\ncomparative analysis based on a public dataset, which contains volunteers'\nperceptions regarding urban areas expressed in a controlled experiment. We\nobserve that both results yield a very similar level of agreement.", "published": "2020-10-13 14:59:46", "link": "http://arxiv.org/abs/2010.06444v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "With Little Power Comes Great Responsibility", "abstract": "Despite its importance to experimental design, statistical power (the\nprobability that, given a real effect, an experiment will reject the null\nhypothesis) has largely been ignored by the NLP community. Underpowered\nexperiments make it more difficult to discern the difference between\nstatistical noise and meaningful model improvements, and increase the chances\nof exaggerated findings. By meta-analyzing a set of existing NLP papers and\ndatasets, we characterize typical power for a variety of settings and conclude\nthat underpowered experiments are common in the NLP literature. In particular,\nfor several tasks in the popular GLUE benchmark, small test sets mean that most\nattempted comparisons to state of the art models will not be adequately\npowered. Similarly, based on reasonable assumptions, we find that the most\ntypical experimental design for human rating studies will be underpowered to\ndetect small model differences, of the sort that are frequently studied. For\nmachine translation, we find that typical test sets of 2000 sentences have\napproximately 75% power to detect differences of 1 BLEU point. To improve the\nsituation going forward, we give an overview of best practices for power\nanalysis in NLP and release a series of notebooks to assist with future power\nanalyses.", "published": "2020-10-13 18:00:02", "link": "http://arxiv.org/abs/2010.06595v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer\n  of Scientific Concepts across Text Corpora", "abstract": "What kind of basic research ideas are more likely to get applied in practice?\nThere is a long line of research investigating patterns of knowledge transfer,\nbut it generally focuses on documents as the unit of analysis and follow their\ntransfer into practice for a specific scientific domain. Here we study\ntranslational research at the level of scientific concepts for all scientific\nfields. We do this through text mining and predictive modeling using three\ncorpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28\nmillion clinical trials. We extract scientific concepts (i.e., phrases) from\ncorpora as instantiations of \"research ideas\", create concept-level features as\nmotivated by literature, and then follow the trajectories of over 450,000 new\nconcepts (emerged from 1995-2014) to identify factors that lead only a small\nproportion of these ideas to be used in inventions and drug trials. Results\nfrom our analysis suggest several mechanisms that distinguish which scientific\nconcept will be adopted in practice, and which will not. We also demonstrate\nthat our derived features can be used to explain and predict knowledge transfer\nwith high accuracy. Our work provides greater understanding of knowledge\ntransfer for researchers, practitioners, and government agencies interested in\nencouraging translational research.", "published": "2020-10-13 19:46:59", "link": "http://arxiv.org/abs/2010.06657v1", "categories": ["cs.CY", "cs.CL", "cs.DL"], "primary_category": "cs.CY"}
{"title": "A Multi-Modal Method for Satire Detection using Textual and Visual Cues", "abstract": "Satire is a form of humorous critique, but it is sometimes misinterpreted by\nreaders as legitimate news, which can lead to harmful consequences. We observe\nthat the images used in satirical news articles often contain absurd or\nridiculous content and that image manipulation is used to create fictional\nscenarios. While previous work have studied text-based methods, in this work we\npropose a multi-modal approach based on state-of-the-art visiolinguistic model\nViLBERT. To this end, we create a new dataset consisting of images and\nheadlines of regular and satirical news for the task of satire detection. We\nfine-tune ViLBERT on the dataset and train a convolutional neural network that\nuses an image forensics technique. Evaluation on the dataset shows that our\nproposed multi-modal approach outperforms image-only, text-only, and simple\nfusion baselines.", "published": "2020-10-13 20:08:29", "link": "http://arxiv.org/abs/2010.06671v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Aspect-Based Sentiment Analysis via Joint\n  Aspect-Sentiment Topic Embedding", "abstract": "Aspect-based sentiment analysis of review texts is of great value for\nunderstanding user feedback in a fine-grained manner. It has in general two\nsub-tasks: (i) extracting aspects from each review, and (ii) classifying\naspect-based reviews by sentiment polarity. In this paper, we propose a\nweakly-supervised approach for aspect-based sentiment analysis, which uses only\na few keywords describing each aspect/sentiment without using any labeled\nexamples. Existing methods are either designed only for one of the sub-tasks,\nneglecting the benefit of coupling both, or are based on topic models that may\ncontain overlapping concepts. We propose to first learn <sentiment, aspect>\njoint topic embeddings in the word embedding space by imposing regularizations\nto encourage topic distinctiveness, and then use neural models to generalize\nthe word-level discriminative information by pre-training the classifiers with\nembedding-based predictions and self-training them on unlabeled data. Our\ncomprehensive performance analysis shows that our method generates quality\njoint topics and outperforms the baselines significantly (7.4% and 5.1%\nF1-score gain on average for aspect and sentiment classification respectively)\non benchmark datasets. Our code and data are available at\nhttps://github.com/teapot123/JASen.", "published": "2020-10-13 21:33:24", "link": "http://arxiv.org/abs/2010.06705v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Random Network Distillation as a Diversity Metric for Both Image and\n  Text Generation", "abstract": "Generative models are increasingly able to produce remarkably high quality\nimages and text. The community has developed numerous evaluation metrics for\ncomparing generative models. However, these metrics do not effectively quantify\ndata diversity. We develop a new diversity metric that can readily be applied\nto data, both synthetic and natural, of any type. Our method employs random\nnetwork distillation, a technique introduced in reinforcement learning. We\nvalidate and deploy this metric on both images and text. We further explore\ndiversity in few-shot image generation, a setting which was previously\ndifficult to evaluate.", "published": "2020-10-13 22:03:52", "link": "http://arxiv.org/abs/2010.06715v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Ensemble Distillation for Structured Prediction: Calibrated, Accurate,\n  Fast-Choose Three", "abstract": "Modern neural networks do not always produce well-calibrated predictions,\neven when trained with a proper scoring function such as cross-entropy. In\nclassification settings, simple methods such as isotonic regression or\ntemperature scaling may be used in conjunction with a held-out dataset to\ncalibrate model outputs. However, extending these methods to structured\nprediction is not always straightforward or effective; furthermore, a held-out\ncalibration set may not always be available. In this paper, we study ensemble\ndistillation as a general framework for producing well-calibrated structured\nprediction models while avoiding the prohibitive inference-time cost of\nensembles. We validate this framework on two tasks: named-entity recognition\nand machine translation. We find that, across both tasks, ensemble distillation\nproduces models which retain much of, and occasionally improve upon, the\nperformance and calibration benefits of ensembles, while only requiring a\nsingle model during test-time.", "published": "2020-10-13 22:30:06", "link": "http://arxiv.org/abs/2010.06721v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "\"What Are You Trying to Do?\" Semantic Typing of Event Processes", "abstract": "This paper studies a new cognitively motivated semantic typing task,\nmulti-axis event process typing, that, given an event process, attempts to\ninfer free-form type labels describing (i) the type of action made by the\nprocess and (ii) the type of object the process seeks to affect. This task is\ninspired by computational and cognitive studies of event understanding, which\nsuggest that understanding processes of events is often directed by recognizing\nthe goals, plans or intentions of the protagonist(s). We develop a large\ndataset containing over 60k event processes, featuring ultra fine-grained\ntyping on both the action and object type axes with very large ($10^3\\sim\n10^4$) label vocabularies. We then propose a hybrid learning framework, P2GT,\nwhich addresses the challenging typing problem with indirect supervision from\nglosses1and a joint learning-to-rank framework. As our experiments indicate,\nP2GT supports identifying the intent of processes, as well as the fine semantic\ntype of the affected object. It also demonstrates the capability of handling\nfew-shot cases, and strong generalizability on out-of-domain event processes.", "published": "2020-10-13 22:37:29", "link": "http://arxiv.org/abs/2010.06724v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Constrained Learning for Event-Event Relation Extraction", "abstract": "Understanding natural language involves recognizing how multiple event\nmentions structurally and temporally interact with each other. In this process,\none can induce event complexes that organize multi-granular events with\ntemporal order and membership relations interweaving among them. Due to the\nlack of jointly labeled data for these relational phenomena and the restriction\non the structures they articulate, we propose a joint constrained learning\nframework for modeling event-event relations. Specifically, the framework\nenforces logical constraints within and across multiple temporal and subevent\nrelations by converting these constraints into differentiable learning\nobjectives. We show that our joint constrained learning approach effectively\ncompensates for the lack of jointly labeled data, and outperforms SOTA methods\non benchmarks for both temporal relation extraction and event hierarchy\nconstruction, replacing a commonly used but more expensive global inference\nprocess. We also present a promising case study showing the effectiveness of\nour approach in inducing event complexes on an external corpus.", "published": "2020-10-13 22:45:28", "link": "http://arxiv.org/abs/2010.06727v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring Universal Speech Attributes for Speaker Verification with an\n  Improved Cross-stitch Network", "abstract": "The universal speech attributes for x-vector based speaker verification (SV)\nare addressed in this paper. The manner and place of articulation form the\nfundamental speech attribute unit (SAU), and then new speech attribute (NSA)\nunits for acoustic modeling are generated by tied tri-SAU states. An improved\ncross-stitch network is adopted as a multitask learning (MTL) framework for\nintegrating these universal speech attributes into the x-vector network\ntraining process. Experiments are conducted on common condition 5 (CC5) of the\ncore-core and the 10 s-10 s tests of the NIST SRE10 evaluation set, and the\nproposed algorithm can achieve consistent improvements over the baseline\nx-vector on both these tasks.", "published": "2020-10-13 09:18:22", "link": "http://arxiv.org/abs/2010.06248v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Novel Architectures for Unsupervised Information Bottleneck based\n  Speaker Diarization of Meetings", "abstract": "Speaker diarization is an important problem that is topical, and is\nespecially useful as a preprocessor for conversational speech related\napplications. The objective of this paper is two-fold: (i) segment\ninitialization by uniformly distributing speaker information across the initial\nsegments, and (ii) incorporating speaker discriminative features within the\nunsupervised diarization framework. In the first part of the work, a varying\nlength segment initialization technique for Information Bottleneck (IB) based\nspeaker diarization system using phoneme rate as the side information is\nproposed. This initialization distributes speaker information uniformly across\nthe segments and provides a better starting point for IB based clustering. In\nthe second part of the work, we present a Two-Pass Information Bottleneck\n(TPIB) based speaker diarization system that incorporates speaker\ndiscriminative features during the process of diarization. The TPIB based\nspeaker diarization system has shown improvement over the baseline IB based\nsystem. During the first pass of the TPIB system, a coarse segmentation is\nperformed using IB based clustering. The alignments obtained are used to\ngenerate speaker discriminative features using a shallow feed-forward neural\nnetwork and linear discriminant analysis. The discriminative features obtained\nare used in the second pass to obtain the final speaker boundaries. In the\nfinal part of the paper, variable segment initialization is combined with the\nTPIB framework. This leverages the advantages of better segment initialization\nand speaker discriminative features that results in an additional improvement\nin performance. An evaluation on standard meeting datasets shows that a\nsignificant absolute improvement of 3.9% and 4.7% is obtained on the NIST and\nAMI datasets, respectively.", "published": "2020-10-13 11:44:02", "link": "http://arxiv.org/abs/2010.06304v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end Triplet Loss based Emotion Embedding System for Speech\n  Emotion Recognition", "abstract": "In this paper, an end-to-end neural embedding system based on triplet loss\nand residual learning has been proposed for speech emotion recognition. The\nproposed system learns the embeddings from the emotional information of the\nspeech utterances. The learned embeddings are used to recognize the emotions\nportrayed by given speech samples of various lengths. The proposed system\nimplements Residual Neural Network architecture. It is trained using softmax\npre-training and triplet loss function. The weights between the fully connected\nand embedding layers of the trained network are used to calculate the embedding\nvalues. The embedding representations of various emotions are mapped onto a\nhyperplane, and the angles among them are computed using the cosine similarity.\nThese angles are utilized to classify a new speech sample into its appropriate\nemotion class. The proposed system has demonstrated 91.67% and 64.44% accuracy\nwhile recognizing emotions for RAVDESS and IEMOCAP dataset, respectively.", "published": "2020-10-13 06:56:41", "link": "http://arxiv.org/abs/2010.06200v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound event localization and detection based on crnn using rectangular\n  filters and channel rotation data augmentation", "abstract": "Sound Event Localization and Detection refers to the problem of identifying\nthe presence of independent or temporally-overlapped sound sources, correctly\nidentifying to which sound class it belongs, estimating their spatial\ndirections while they are active. In the last years, neural networks have\nbecome the prevailing method for sound Event Localization and Detection task,\nwith convolutional recurrent neural networks being among the most used systems.\nThis paper presents a system submitted to the Detection and Classification of\nAcoustic Scenes and Events 2020 Challenge Task 3. The algorithm consists of a\nconvolutional recurrent neural network using rectangular filters, specialized\nin recognizing significant spectral features related to the task. In order to\nfurther improve the score and to generalize the system performance to unseen\ndata, the training dataset size has been increased using data augmentation. The\ntechnique used for that is based on channel rotations and reflection on the xy\nplane in the First Order Ambisonic domain, which allows improving Direction of\nArrival labels keeping the physical relationships between channels. Evaluation\nresults on the development dataset show that the proposed system outperforms\nthe baseline results, considerably improving Error Rate and F-score for\nlocation-aware detection.", "published": "2020-10-13 14:30:38", "link": "http://arxiv.org/abs/2010.06422v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A variational autoencoder for music generation controlled by tonal\n  tension", "abstract": "Many of the music generation systems based on neural networks are fully\nautonomous and do not offer control over the generation process. In this\nresearch, we present a controllable music generation system in terms of tonal\ntension. We incorporate two tonal tension measures based on the Spiral Array\nTension theory into a variational autoencoder model. This allows us to control\nthe direction of the tonal tension throughout the generated piece, as well as\nthe overall level of tonal tension. Given a seed musical fragment, stemming\nfrom either the user input or from directly sampling from the latent space, the\nmodel can generate variations of this original seed fragment with altered tonal\ntension. This altered music still resembles the seed music rhythmically, but\nthe pitch of the notes are changed to match the desired tonal tension as\nconditioned by the user.", "published": "2020-10-13 08:37:22", "link": "http://arxiv.org/abs/2010.06230v2", "categories": ["cs.SD", "cs.SC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Data-efficient Modeling for Wake Word Spotting", "abstract": "Wake word (WW) spotting is challenging in far-field not only because of the\ninterference in signal transmission but also the complexity in acoustic\nenvironments. Traditional WW model training requires large amount of in-domain\nWW-specific data with substantial human annotations therefore it is hard to\nbuild WW models without such data. In this paper we present data-efficient\nsolutions to address the challenges in WW modeling, such as domain-mismatch,\nnoisy conditions, limited annotation, etc. Our proposed system is composed of a\nmulti-condition training pipeline with a stratified data augmentation, which\nimproves the model robustness to a variety of predefined acoustic conditions,\ntogether with a semi-supervised learning pipeline to accurately extract the WW\nand confusable examples from untranscribed speech corpus. Starting from only 10\nhours of domain-mismatched WW audio, we are able to enlarge and enrich the\ntraining dataset by 20-100 times to capture the acoustic complexity. Our\nexperiments on real user data show that the proposed solutions can achieve\ncomparable performance of a production-grade model by saving 97\\% of the amount\nof WW-specific data collection and 86\\% of the bandwidth for annotation.", "published": "2020-10-13 19:50:26", "link": "http://arxiv.org/abs/2010.06659v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Front-end Gain Invariant Modeling for Wake Word Spotting", "abstract": "Wake word (WW) spotting is challenging in far-field due to the complexities\nand variations in acoustic conditions and the environmental interference in\nsignal transmission. A suite of carefully designed and optimized audio\nfront-end (AFE) algorithms help mitigate these challenges and provide better\nquality audio signals to the downstream modules such as WW spotter. Since the\nWW model is trained with the AFE-processed audio data, its performance is\nsensitive to AFE variations, such as gain changes. In addition, when deploying\nto new devices, the WW performance is not guaranteed because the AFE is unknown\nto the WW model. To address these issues, we propose a novel approach to use a\nnew feature called $\\Delta$LFBE to decouple the AFE gain variations from the WW\nmodel. We modified the neural network architectures to accommodate the delta\ncomputation, with the feature extraction module unchanged. We evaluate our WW\nmodels using data collected from real household settings and showed the models\nwith the $\\Delta$LFBE is robust to AFE gain changes. Specifically, when AFE\ngain changes up to $\\pm$12dB, the baseline CNN model lost up to relative 19.0%\nin false alarm rate or 34.3% in false reject rate, while the model with\n$\\Delta$LFBE demonstrates no performance loss.", "published": "2020-10-13 20:23:47", "link": "http://arxiv.org/abs/2010.06676v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
