{"title": "MLRIP: Pre-training a military language representation model with\n  informative factual knowledge and professional knowledge base", "abstract": "Incorporating prior knowledge into pre-trained language models has proven to\nbe effective for knowledge-driven NLP tasks, such as entity typing and relation\nextraction. Current pre-training procedures usually inject external knowledge\ninto models by using knowledge masking, knowledge fusion and knowledge\nreplacement. However, factual information contained in the input sentences have\nnot been fully mined, and the external knowledge for injecting have not been\nstrictly checked. As a result, the context information cannot be fully\nexploited and extra noise will be introduced or the amount of knowledge\ninjected is limited. To address these issues, we propose MLRIP, which modifies\nthe knowledge masking strategies proposed by ERNIE-Baidu, and introduce a\ntwo-stage entity replacement strategy. Extensive experiments with comprehensive\nanalyses illustrate the superiority of MLRIP over BERT-based models in military\nknowledge-driven NLP tasks.", "published": "2022-07-28 07:39:30", "link": "http://arxiv.org/abs/2207.13929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interpretability Evaluation Benchmark for Pre-trained Language Models", "abstract": "While pre-trained language models (LMs) have brought great improvements in\nmany NLP tasks, there is increasing attention to explore capabilities of LMs\nand interpret their predictions. However, existing works usually focus only on\na certain capability with some downstream tasks. There is a lack of datasets\nfor directly evaluating the masked word prediction performance and the\ninterpretability of pre-trained LMs. To fill in the gap, we propose a novel\nevaluation benchmark providing with both English and Chinese annotated data. It\ntests LMs abilities in multiple dimensions, i.e., grammar, semantics,\nknowledge, reasoning and computation. In addition, it provides carefully\nannotated token-level rationales that satisfy sufficiency and compactness. It\ncontains perturbed instances for each original instance, so as to use the\nrationale consistency under perturbations as the metric for faithfulness, a\nperspective of interpretability. We conduct experiments on several widely-used\npre-trained LMs. The results show that they perform very poorly on the\ndimensions of knowledge and computation. And their plausibility in all\ndimensions is far from satisfactory, especially when the rationale is short. In\naddition, the pre-trained LMs we evaluated are not robust on syntax-aware data.\nWe will release this evaluation benchmark at \\url{http://xyz}, and hope it can\nfacilitate the research progress of pre-trained LMs.", "published": "2022-07-28 08:28:09", "link": "http://arxiv.org/abs/2207.13948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Architecture Search on Efficient Transformers and Beyond", "abstract": "Recently, numerous efficient Transformers have been proposed to reduce the\nquadratic computational complexity of standard Transformers caused by the\nSoftmax attention. However, most of them simply swap Softmax with an efficient\nattention mechanism without considering the customized architectures specially\nfor the efficient attention. In this paper, we argue that the handcrafted\nvanilla Transformer architectures for Softmax attention may not be suitable for\nefficient Transformers. To address this issue, we propose a new framework to\nfind optimal architectures for efficient Transformers with the neural\narchitecture search (NAS) technique. The proposed method is validated on\npopular machine translation and image classification tasks. We observe that the\noptimal architecture of the efficient Transformer has the reduced computation\ncompared with that of the standard Transformer, but the general accuracy is\nless comparable. It indicates that the Softmax attention and efficient\nattention have their own distinctions but neither of them can simultaneously\nbalance the accuracy and efficiency well. This motivates us to mix the two\ntypes of attention to reduce the performance imbalance. Besides the search\nspaces that commonly used in existing NAS Transformer approaches, we propose a\nnew search space that allows the NAS algorithm to automatically search the\nattention variants along with architectures. Extensive experiments on WMT' 14\nEn-De and CIFAR-10 demonstrate that our searched architecture maintains\ncomparable accuracy to the standard Transformer with notably improved\ncomputational efficiency.", "published": "2022-07-28 08:41:41", "link": "http://arxiv.org/abs/2207.13955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence to sequence pretraining for a less-resourced Slovenian language", "abstract": "Large pretrained language models have recently conquered the area of natural\nlanguage processing. As an alternative to predominant masked language modelling\nintroduced in BERT, the T5 model has introduced a more general training\nobjective, namely sequence to sequence transformation, which includes masked\nlanguage model but more naturally fits text generation tasks such as machine\ntranslation, summarization, question answering, text simplification, dialogue\nsystems, etc. The monolingual variants of T5 models have been limited to\nwell-resourced languages, while the massively multilingual T5 model supports\n101 languages. In contrast, we trained two different sized T5-type sequence to\nsequence models for morphologically rich Slovene language with much less\nresources and analyzed their behavior on 11 tasks. Concerning classification\ntasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa\nmodel but are useful for the generative tasks.", "published": "2022-07-28 10:08:50", "link": "http://arxiv.org/abs/2207.13988v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Causal Effects of Data Statistics on Language Model's\n  `Factual' Predictions", "abstract": "Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.", "published": "2022-07-28 17:36:24", "link": "http://arxiv.org/abs/2207.14251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Training of Language Models to Fill in the Middle", "abstract": "We show that autoregressive language models can learn to infill text after we\napply a straightforward transformation to the dataset, which simply moves a\nspan of text from the middle of a document to its end. While this data\naugmentation has garnered much interest in recent years, we provide extensive\nevidence that training models with a large fraction of data transformed in this\nway does not harm the original left-to-right generative capability, as measured\nby perplexity and sampling evaluations across a wide range of scales. Given the\nusefulness, simplicity, and efficiency of training models to fill-in-the-middle\n(FIM), we suggest that future autoregressive language models be trained with\nFIM by default. To this end, we run a series of ablations on key\nhyperparameters, such as the data transformation frequency, the structure of\nthe transformation, and the method of selecting the infill span. We use these\nablations to prescribe strong default settings and best practices to train FIM\nmodels. We have released our best infilling model trained with best practices\nin our API, and release our infilling benchmarks to aid future research.", "published": "2022-07-28 17:40:47", "link": "http://arxiv.org/abs/2207.14255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient NLP Model Finetuning via Multistage Data Filtering", "abstract": "As model finetuning is central to the modern NLP, we set to maximize its\nefficiency. Motivated by redundancy in training examples and the sheer sizes of\npretrained models, we exploit a key opportunity: training only on important\ndata. To this end, we set to filter training examples in a streaming fashion,\nin tandem with training the target model. Our key techniques are two: (1)\nautomatically determine a training loss threshold for skipping backward\ntraining passes; (2) run a meta predictor for further skipping forward training\npasses. We integrate the above techniques in a holistic, three-stage training\nprocess. On a diverse set of benchmarks, our method reduces the required\ntraining examples by up to 5.3$\\times$ and training time by up to 6.8$\\times$,\nwhile only seeing minor accuracy degradation. Our method is effective even when\ntraining one epoch, where each training example is encountered only once. It is\nsimple to implement and is compatible with the existing finetuning techniques.\nCode is available at: https://github.com/xo28/efficient-\nNLP-multistage-training", "published": "2022-07-28 21:43:31", "link": "http://arxiv.org/abs/2207.14386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Evaluation of Dialog Track at DSTC9", "abstract": "The ultimate goal of dialog research is to develop systems that can be\neffectively used in interactive settings by real users. To this end, we\nintroduced the Interactive Evaluation of Dialog Track at the 9th Dialog System\nTechnology Challenge. This track consisted of two sub-tasks. The first sub-task\ninvolved building knowledge-grounded response generation models. The second\nsub-task aimed to extend dialog models beyond static datasets by assessing them\nin an interactive setting with real users. Our track challenges participants to\ndevelop strong response generation models and explore strategies that extend\nthem to back-and-forth interactions with real users. The progression from\nstatic corpora to interactive evaluation introduces unique challenges and\nfacilitates a more thorough assessment of open-domain dialog systems. This\npaper provides an overview of the track, including the methodology and results.\nFurthermore, it provides insights into how to best evaluate open-domain dialog\nmodels", "published": "2022-07-28 22:54:04", "link": "http://arxiv.org/abs/2207.14403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding\n  Methods", "abstract": "Persona and Knowledge dual context open-domain chat is a novel dialogue\ngeneration task introduced recently. While Persona and Knowledge is each\ninteresting context of open-domain dialogue, the combination of both has not\nbeen well studied. We tackle Persona-Knowledge identification and response\ngeneration tasks in this paper. We design an informed data augmentation\nstrategy that is compatible with neural Q&A retrieval models. With the\naugmented data, we perform permutative Persona-Knowledge evaluation and\nsuccessive Persona search fine-tuning. Furthermore, we perform dialogue\ngeneration with various decoding techniques and illustrate crucial elements. We\nachieve SOTA across official metrics with 93.99% Grounding accuracy average and\n23.62 SacreBLEU score.", "published": "2022-07-28 07:19:08", "link": "http://arxiv.org/abs/2207.13919v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowing Where and What: Unified Word Block Pretraining for Document\n  Understanding", "abstract": "Due to the complex layouts of documents, it is challenging to extract\ninformation for documents. Most previous studies develop multimodal pre-trained\nmodels in a self-supervised way. In this paper, we focus on the embedding\nlearning of word blocks containing text and layout information, and propose\nUTel, a language model with Unified TExt and Layout pre-training. Specifically,\nwe propose two pre-training tasks: Surrounding Word Prediction (SWP) for the\nlayout learning, and Contrastive learning of Word Embeddings (CWE) for\nidentifying different word blocks. Moreover, we replace the commonly used 1D\nposition embedding with a 1D clipped relative position embedding. In this way,\nthe joint training of Masked Layout-Language Modeling (MLLM) and two newly\nproposed tasks enables the interaction between semantic and spatial features in\na unified way. Additionally, the proposed UTel can process arbitrary-length\nsequences by removing the 1D position embedding, while maintaining competitive\nperformance. Extensive experimental results show UTel learns better joint\nrepresentations and achieves superior performance than previous methods on\nvarious downstream tasks, though requiring no image modality. Code is available\nat \\url{https://github.com/taosong2019/UTel}.", "published": "2022-07-28 09:43:06", "link": "http://arxiv.org/abs/2207.13979v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Entity Type Prediction Leveraging Graph Walks and Entity Descriptions", "abstract": "The entity type information in Knowledge Graphs (KGs) such as DBpedia,\nFreebase, etc. is often incomplete due to automated generation or human\ncuration. Entity typing is the task of assigning or inferring the semantic type\nof an entity in a KG. This paper presents \\textit{GRAND}, a novel approach for\nentity typing leveraging different graph walk strategies in RDF2vec together\nwith textual entity descriptions. RDF2vec first generates graph walks and then\nuses a language model to obtain embeddings for each node in the graph. This\nstudy shows that the walk generation strategy and the embedding model have a\nsignificant effect on the performance of the entity typing task. The proposed\napproach outperforms the baseline approaches on the benchmark datasets DBpedia\nand FIGER for entity typing in KGs for both fine-grained and coarse-grained\nclasses. The results show that the combination of order-aware RDF2vec variants\ntogether with the contextual embeddings of the textual entity descriptions\nachieve the best results.", "published": "2022-07-28 13:56:55", "link": "http://arxiv.org/abs/2207.14094v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Claim-Dissector: An Interpretable Fact-Checking System with Joint\n  Re-ranking and Veracity Prediction", "abstract": "We present Claim-Dissector: a novel latent variable model for fact-checking\nand analysis, which given a claim and a set of retrieved evidences jointly\nlearns to identify: (i) the relevant evidences to the given claim, (ii) the\nveracity of the claim. We propose to disentangle the per-evidence relevance\nprobability and its contribution to the final veracity probability in an\ninterpretable way -- the final veracity probability is proportional to a linear\nensemble of per-evidence relevance probabilities. In this way, the individual\ncontributions of evidences towards the final predicted probability can be\nidentified. In per-evidence relevance probability, our model can further\ndistinguish whether each relevant evidence is supporting (S) or refuting (R)\nthe claim. This allows to quantify how much the S/R probability contributes to\nthe final verdict or to detect disagreeing evidence.\n  Despite its interpretable nature, our system achieves results competitive\nwith state-of-the-art on the FEVER dataset, as compared to typical two-stage\nsystem pipelines, while using significantly fewer parameters. It also sets new\nstate-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows\nthat our model can learn fine-grained relevance cues while using coarse-grained\nsupervision, and we demonstrate it in 2 ways. (i) We show that our model can\nachieve competitive sentence recall while using only paragraph-level relevance\nsupervision. (ii) Traversing towards the finest granularity of relevance, we\nshow that our model is capable of identifying relevance at the token level. To\ndo this, we present a new benchmark TLR-FEVER focusing on token-level\ninterpretability -- humans annotate tokens in relevant evidences they\nconsidered essential when making their judgment. Then we measure how similar\nare these annotations to the tokens our model is focusing on.", "published": "2022-07-28 14:30:06", "link": "http://arxiv.org/abs/2207.14116v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Eye Gaze Estimation Model Analysis", "abstract": "We explore techniques for eye gaze estimation using machine learning. Eye\ngaze estimation is a common problem for various behavior analysis and\nhuman-computer interfaces. The purpose of this work is to discuss various model\ntypes for eye gaze estimation and present the results from predicting gaze\ndirection using eye landmarks in unconstrained settings. In unconstrained\nreal-world settings, feature-based and model-based methods are outperformed by\nrecent appearance-based methods due to factors like illumination changes and\nother visual artifacts. We discuss a learning-based method for eye region\nlandmark localization trained exclusively on synthetic data. We discuss how to\nuse detected landmarks as input to iterative model-fitting and lightweight\nlearning-based gaze estimation methods and how to use the model for\nperson-independent and personalized gaze estimations.", "published": "2022-07-28 20:40:03", "link": "http://arxiv.org/abs/2207.14373v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LAD: Language Models as Data for Zero-Shot Dialog", "abstract": "To facilitate zero-shot generalization in taskoriented dialog, this paper\nproposes Language Models as Data (LAD). LAD is a paradigm for creating diverse\nand accurate synthetic data which conveys the necessary structural constraints\nand can be used to train a downstream neural dialog model. LAD leverages GPT-3\nto induce linguistic diversity. LAD achieves significant performance gains in\nzero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and\nnext action prediction (+11 F1). Furthermore, an interactive human evaluation\nshows that training with LAD is competitive with training on human dialogs. LAD\nis open-sourced, with the code and data available at\nhttps://github.com/Shikib/lad.", "published": "2022-07-28 22:10:45", "link": "http://arxiv.org/abs/2207.14393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unit Testing for Concepts in Neural Networks", "abstract": "Many complex problems are naturally understood in terms of symbolic concepts.\nFor example, our concept of \"cat\" is related to our concepts of \"ears\" and\n\"whiskers\" in a non-arbitrary way. Fodor (1998) proposes one theory of\nconcepts, which emphasizes symbolic representations related via constituency\nstructures. Whether neural networks are consistent with such a theory is open\nfor debate. We propose unit tests for evaluating whether a system's behavior is\nconsistent with several key aspects of Fodor's criteria. Using a simple visual\nconcept learning task, we evaluate several modern neural architectures against\nthis specification. We find that models succeed on tests of groundedness,\nmodularlity, and reusability of concepts, but that important questions about\ncausality remain open. Resolving these will require new methods for analyzing\nmodels' internal states.", "published": "2022-07-28 08:49:32", "link": "http://arxiv.org/abs/2208.10244v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PHEMEPlus: Enriching Social Media Rumour Verification with External\n  Evidence", "abstract": "Work on social media rumour verification utilises signals from posts, their\npropagation and users involved. Other lines of work target identifying and\nfact-checking claims based on information from Wikipedia, or trustworthy news\narticles without considering social media context. However works combining the\ninformation from social media with external evidence from the wider web are\nlacking. To facilitate research in this direction, we release a novel dataset,\nPHEMEPlus, an extension of the PHEME benchmark, which contains social media\nconversations as well as relevant external evidence for each rumour. We\ndemonstrate the effectiveness of incorporating such evidence in improving\nrumour verification models. Additionally, as part of the evidence collection,\nwe evaluate various ways of query formulation to identify the most effective\nmethod.", "published": "2022-07-28 09:21:05", "link": "http://arxiv.org/abs/2207.13970v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation", "abstract": "Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.", "published": "2022-07-28 10:44:46", "link": "http://arxiv.org/abs/2207.14000v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "CubeMLP: An MLP-based Model for Multimodal Sentiment Analysis and\n  Depression Estimation", "abstract": "Multimodal sentiment analysis and depression estimation are two important\nresearch topics that aim to predict human mental states using multimodal data.\nPrevious research has focused on developing effective fusion strategies for\nexchanging and integrating mind-related information from different modalities.\nSome MLP-based techniques have recently achieved considerable success in a\nvariety of computer vision tasks. Inspired by this, we explore multimodal\napproaches with a feature-mixing perspective in this study. To this end, we\nintroduce CubeMLP, a multimodal feature processing framework based entirely on\nMLP. CubeMLP consists of three independent MLP units, each of which has two\naffine transformations. CubeMLP accepts all relevant modality features as input\nand mixes them across three axes. After extracting the characteristics using\nCubeMLP, the mixed multimodal features are flattened for task predictions. Our\nexperiments are conducted on sentiment analysis datasets: CMU-MOSI and\nCMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that\nCubeMLP can achieve state-of-the-art performance with a much lower computing\ncost.", "published": "2022-07-28 13:50:55", "link": "http://arxiv.org/abs/2207.14087v3", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Large Language Models and the Reverse Turing Test", "abstract": "Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that are self-supervised and can be adapted with fine\ntuning to a wide range of natural language tasks, each of which previously\nwould have required a separate network model. This is one step closer to the\nextraordinary versatility of human language. GPT-3 and more recently LaMDA can\ncarry on dialogs with humans on many topics after minimal priming with a few\nexamples. However, there has been a wide range of reactions and debate on\nwhether these LLMs understand what they are saying or exhibit signs of\nintelligence. This high variance is exhibited in three interviews with LLMs\nreaching wildly different conclusions. A new possibility was uncovered that\ncould explain this divergence. What appears to be intelligence in LLMs may in\nfact be a mirror that reflects the intelligence of the interviewer, a\nremarkable twist that could be considered a Reverse Turing Test. If so, then by\nstudying interviews we may be learning more about the intelligence and beliefs\nof the interviewer than the intelligence of the LLMs. As LLMs become more\ncapable they may transform the way we interact with machines and how they\ninteract with each other. Increasingly, LLMs are being coupled with\nsensorimotor devices. LLMs can talk the talk, but can they walk the walk? A\nroad map for achieving artificial general autonomy is outlined with seven major\nimprovements inspired by brain systems. LLMs could be used to uncover new\ninsights into brain function by downloading brain data during natural\nbehaviors.", "published": "2022-07-28 21:22:47", "link": "http://arxiv.org/abs/2207.14382v9", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "SDBERT: SparseDistilBERT, a faster and smaller BERT model", "abstract": "In this work we introduce a new transformer architecture called\nSparseDistilBERT (SDBERT), which is a combination of sparse attention and\nknowledge distillantion (KD). We implemented sparse attention mechanism to\nreduce quadratic dependency on input length to linear. In addition to reducing\ncomputational complexity of the model, we used knowledge distillation (KD). We\nwere able to reduce the size of BERT model by 60% while retaining 97%\nperformance and it only took 40% of time to train.", "published": "2022-07-28 07:34:07", "link": "http://arxiv.org/abs/2208.10246v1", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Raising Student Completion Rates with Adaptive Curriculum and Contextual\n  Bandits", "abstract": "We present an adaptive learning Intelligent Tutoring System, which uses\nmodel-based reinforcement learning in the form of contextual bandits to assign\nlearning activities to students. The model is trained on the trajectories of\nthousands of students in order to maximize their exercise completion rates and\ncontinues to learn online, automatically adjusting itself to new activities. A\nrandomized controlled trial with students shows that our model leads to\nsuperior completion rates and significantly improved student engagement when\ncompared to other approaches. Our approach is fully-automated unlocking new\nopportunities for learning experience personalization.", "published": "2022-07-28 10:52:11", "link": "http://arxiv.org/abs/2207.14003v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "I.2.6; I.2.7; K.3.1; K.3.2"], "primary_category": "cs.CL"}
{"title": "Dialogue Enhancement and Listening Effort in Broadcast Audio: A\n  Multimodal Evaluation", "abstract": "Dialogue enhancement (DE) plays a vital role in broadcasting, enabling the\npersonalization of the relative level between foreground speech and background\nmusic and effects. DE has been shown to improve the quality of experience,\nintelligibility, and self-reported listening effort (LE). A physiological\nindicator of LE known from audiology studies is pupil size. The relation\nbetween pupil size and LE is typically studied using artificial sentences and\nbackground noises not encountered in broadcast content. This work evaluates the\neffect of DE on LE in a multimodal manner that includes pupil size (tracked by\na VR headset) and real-world audio excerpts from TV. Under ideal listening\nconditions, 28 normal-hearing participants listened to 30 audio excerpts\npresented in random order and processed by conditions varying the relative\nlevel between foreground and background audio. One of these conditions employed\na recently proposed source separation system to attenuate the background given\nthe original mixture as the sole input. After listening to each excerpt,\nsubjects were asked to repeat the heard sentence and self-report the LE. Mean\npupil dilation and peak pupil dilation were analyzed and compared with the\nself-report and the word recall rate. The multimodal evaluation shows a\nconsistent trend of decreasing LE along with decreasing background level. DE,\nalso when enabled by source separation, significantly reduces the pupil size as\nwell as the self-reported LE. This highlights the benefit of personalization\nfunctionalities at the user's end.", "published": "2022-07-28 17:19:31", "link": "http://arxiv.org/abs/2207.14240v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Utterance-by-utterance overlap-aware neural diarization with Graph-PIT", "abstract": "Recent speaker diarization studies showed that integration of end-to-end\nneural diarization (EEND) and clustering-based diarization is a promising\napproach for achieving state-of-the-art performance on various tasks. Such an\napproach first divides an observed signal into fixed-length segments, then\nperforms {\\it segment-level} local diarization based on an EEND module, and\nmerges the segment-level results via clustering to form a final global\ndiarization result. The segmentation is done to limit the number of speakers in\neach segment since the current EEND cannot handle a large number of speakers.\nIn this paper, we argue that such an approach involving the segmentation has\nseveral issues; for example, it inevitably faces a dilemma that larger segment\nsizes increase both the context available for enhancing the performance and the\nnumber of speakers for the local EEND module to handle. To resolve such a\nproblem, this paper proposes a novel framework that performs diarization\nwithout segmentation. However, it can still handle challenging data containing\nmany speakers and a significant amount of overlapping speech. The proposed\nmethod can take an entire meeting for inference and perform {\\it\nutterance-by-utterance} diarization that clusters utterance activities in terms\nof speakers. To this end, we leverage a neural network training scheme called\nGraph-PIT proposed recently for neural source separation. Experiments with\nsimulated active-meeting-like data and CALLHOME data show the superiority of\nthe proposed approach over the conventional methods.", "published": "2022-07-28 05:49:49", "link": "http://arxiv.org/abs/2207.13888v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Unifying View on Blind Source Separation of Convolutive Mixtures based\n  on Independent Component Analysis", "abstract": "In many daily-life scenarios, acoustic sources recorded in an enclosure can\nonly be observed with other interfering sources. Hence, convolutive Blind\nSource Separation (BSS) is a central problem in audio signal processing.\nMethods based on Independent Component Analysis (ICA) are especially important\nin this field as they require only few and weak assumptions and allow for\nblindness regarding the original source signals and the acoustic propagation\npath. Most of the currently used algorithms belong to one of the following\nthree families: Frequency Domain ICA (FD-ICA), Independent Vector Analysis\n(IVA), and TRIple-N Independent component analysis for CONvolutive mixtures\n(TRINICON). While the relation between ICA, FD-ICA and IVA becomes apparent due\nto their construction, the relation to TRINICON is not well established yet.\nThis paper fills this gap by providing an in-depth treatment of the common\nbuilding blocks of these algorithms and their differences, and thus provides a\ncommon framework for all considered algorithms.", "published": "2022-07-28 07:57:11", "link": "http://arxiv.org/abs/2207.13934v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Extending RNN-T-based speech recognition systems with emotion and\n  language classification", "abstract": "Speech transcription, emotion recognition, and language identification are\nusually considered to be three different tasks. Each one requires a different\nmodel with a different architecture and training process. We propose using a\nrecurrent neural network transducer (RNN-T)-based speech-to-text (STT) system\nas a common component that can be used for emotion recognition and language\nidentification as well as for speech recognition. Our work extends the STT\nsystem for emotion classification through minimal changes, and shows successful\nresults on the IEMOCAP and MELD datasets. In addition, we demonstrate that by\nadding a lightweight component to the RNN-T module, it can also be used for\nlanguage identification. In our evaluations, this new classifier demonstrates\nstate-of-the-art accuracy for the NIST-LRE-07 dataset.", "published": "2022-07-28 09:11:39", "link": "http://arxiv.org/abs/2207.13965v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting Global HRTFs From Scanned Head Geometry Using Deep Learning\n  and Compact Representations", "abstract": "In the growing field of virtual auditory display, personalized head-related\ntransfer functions (HRTFs) play a vital role in establishing an accurate sound\nimage for mixed and augmented reality applications. In this work, we propose an\nHRTF personalization method employing convolutional neural networks (CNN) to\npredict a subject HRTFs for all directions from their scanned head geometry. To\nease the training of the CNN models, we propose novel pre-processing methods\nfor both the head scans and HRTF data to achieve compact representations. For\nthe head scan, we use truncated spherical cap harmonic (SCH) coefficients to\nrepresent the pinna area, which is important in the acoustic scattering\nprocess. For the HRTF data, we use truncated spherical harmonic (SH)\ncoefficients to represent the HRTF magnitudes and onsets. One CNN model is\ntrained to predict the SH coefficients of the HRTF magnitudes from the SCH\ncoefficients of the scanned ear geometry and other anthropometric measurements\nof the head. The other CNN model is trained to predict SH coefficients of the\nHRTF onsets from only the anthropometric measurements of the ear, head, and\ntorso. Combining the magnitude and onset predictions, our method is able to\npredict the complete and global HRTF data. A leave-one-out validation with the\nlog-spectral distortion (LSD) metric is used for objective evaluation. The\nresults show a decent LSD level at both spatial \\& temporal dimensions compared\nto the ground-truth HRTFs and a lower LSD than the boundary element method\n(BEM) simulation of HRTFs that the database provides. The localization\nsimulation results with an auditory model are also consistent with the\nobjective evaluation metrics, showing the localization responses with our\npredicted HRTFs are significantly better than with the BEM-calculated ones.", "published": "2022-07-28 19:13:17", "link": "http://arxiv.org/abs/2207.14352v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning-Based Acoustic Mosquito Detection in Noisy Conditions\n  Using Trainable Kernels and Augmentations", "abstract": "In this paper, we demonstrate a unique recipe to enhance the effectiveness of\naudio machine learning approaches by fusing pre-processing techniques into a\ndeep learning model. Our solution accelerates training and inference\nperformance by optimizing hyper-parameters through training instead of costly\nrandom searches to build a reliable mosquito detector from audio signals. The\nexperiments and the results presented here are part of the MOS C submission of\nthe ACM 2022 challenge. Our results outperform the published baseline by 212%\non the unpublished test set. We believe that this is one of the best real-world\nexamples of building a robust bio-acoustic system that provides reliable\nmosquito detection in noisy conditions.", "published": "2022-07-28 01:05:40", "link": "http://arxiv.org/abs/2207.13843v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.m; I.2.1"], "primary_category": "cs.SD"}
{"title": "EEG2Mel: Reconstructing Sound from Brain Responses to Music", "abstract": "Information retrieval from brain responses to auditory and visual stimuli has\nshown success through classification of song names and image classes presented\nto participants while recording EEG signals. Information retrieval in the form\nof reconstructing auditory stimuli has also shown some success, but here we\nimprove on previous methods by reconstructing music stimuli well enough to be\nperceived and identified independently. Furthermore, deep learning models were\ntrained on time-aligned music stimuli spectrum for each corresponding\none-second window of EEG recording, which greatly reduces feature extraction\nsteps needed when compared to prior studies. The NMED-Tempo and NMED-Hindi\ndatasets of participants passively listening to full length songs were used to\ntrain and validate Convolutional Neural Network (CNN) regressors. The efficacy\nof raw voltage versus power spectrum inputs and linear versus mel spectrogram\noutputs were tested, and all inputs and outputs were converted into 2D images.\nThe quality of reconstructed spectrograms was assessed by training classifiers\nwhich showed 81% accuracy for mel-spectrograms and 72% for linear spectrograms\n(10% chance accuracy). Lastly, reconstructions of auditory music stimuli were\ndiscriminated by listeners at an 85% success rate (50% chance) in a\ntwo-alternative match-to-sample task.", "published": "2022-07-28 01:06:51", "link": "http://arxiv.org/abs/2207.13845v1", "categories": ["cs.SD", "cs.CV", "cs.IR", "eess.AS", "I.4.5; I.4.10"], "primary_category": "cs.SD"}
