{"title": "PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level\n  Emotion Recognition", "abstract": "Utterance-level emotion recognition (ULER) is a significant research topic\nfor understanding human behaviors and developing empathetic chatting machines\nin the artificial intelligence area. Unlike traditional text classification\nproblem, this task is supported by a limited number of datasets, among which\nmost contain inadequate conversations or speeches. Such a data scarcity issue\nlimits the possibility of training larger and more powerful models for this\ntask. Witnessing the success of transfer learning in natural language process\n(NLP), we propose to pre-train a context-dependent encoder (CoDE) for ULER by\nlearning from unlabeled conversation data. Essentially, CoDE is a hierarchical\narchitecture that contains an utterance encoder and a conversation encoder,\nmaking it different from those works that aim to pre-train a universal sentence\nencoder. Also, we propose a new pre-training task named \"conversation\ncompletion\" (CoCo), which attempts to select the correct answer from candidate\nanswers to fill a masked utterance in a question conversation. The CoCo task is\ncarried out on pure movie subtitles so that our CoDE can be pre-trained in an\nunsupervised fashion. Finally, the pre-trained CoDE (PT-CoDE) is fine-tuned for\nULER and boosts the model performance significantly on five datasets.", "published": "2019-10-20 07:12:04", "link": "http://arxiv.org/abs/1910.08916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Byte-Pair Encoding for Text-to-SQL Generation", "abstract": "Neural sequence-to-sequence models provide a competitive approach to the task\nof mapping a question in natural language to an SQL query, also referred to as\ntext-to-SQL generation. The Byte-Pair Encoding algorithm (BPE) has previously\nbeen used to improve machine translation (MT) between natural languages. In\nthis work, we adapt BPE for text-to-SQL generation. As the datasets for this\ntask are rather small compared to MT, we present a novel stopping criterion\nthat prevents overfitting the BPE encoding to the training set. Additionally,\nwe present AST BPE, which is a version of BPE that uses the Abstract Syntax\nTree (AST) of the SQL statement to guide BPE merges and therefore produce BPE\nencodings that generalize better. We improved the accuracy of a strong\nattentive seq2seq baseline on five out of six English text-to-SQL tasks while\nreducing training time by more than 50% on four of them due to the shortened\ntargets. Finally, on two of these tasks we exceeded previously reported\naccuracies.", "published": "2019-10-20 12:32:20", "link": "http://arxiv.org/abs/1910.08962v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda\n  Detection", "abstract": "We present the shared task on Fine-Grained Propaganda Detection, which was\norganized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two\nsubtasks. FLC is a fragment-level task that asks for the identification of\npropagandist text fragments in a news article and also for the prediction of\nthe specific propaganda technique used in each such fragment (18-way\nclassification task). SLC is a sentence-level binary classification task asking\nto detect the sentences that contain propaganda. A total of 12 teams submitted\nsystems for the FLC task, 25 teams did so for the SLC task, and 14 teams\neventually submitted a system description paper. For both subtasks, most\nsystems managed to beat the baseline by a sizable margin. The leaderboard and\nthe data from the competition are available at\nhttp://propaganda.qcri.org/nlp4if-shared-task/.", "published": "2019-10-20 10:53:18", "link": "http://arxiv.org/abs/1910.09982v1", "categories": ["cs.CL", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Privacy- and Utility-Preserving Textual Analysis via Calibrated\n  Multivariate Perturbations", "abstract": "Accurately learning from user data while providing quantifiable privacy\nguarantees provides an opportunity to build better ML models while maintaining\nuser trust. This paper presents a formal approach to carrying out privacy\npreserving text perturbation using the notion of dx-privacy designed to achieve\ngeo-indistinguishability in location data. Our approach applies carefully\ncalibrated noise to vector representation of words in a high dimension space as\ndefined by word embedding models. We present a privacy proof that satisfies\ndx-privacy where the privacy parameter epsilon provides guarantees with respect\nto a distance metric defined by the word embedding space. We demonstrate how\nepsilon can be selected by analyzing plausible deniability statistics backed up\nby large scale analysis on GloVe and fastText embeddings. We conduct privacy\naudit experiments against 2 baseline models and utility experiments on 3\ndatasets to demonstrate the tradeoff between privacy and utility for varying\nvalues of epsilon on different task types. Our results demonstrate practical\nutility (< 2% utility loss for training binary classifiers) while providing\nbetter privacy guarantees than baseline models.", "published": "2019-10-20 05:12:23", "link": "http://arxiv.org/abs/1910.08902v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving Sequence Modeling Ability of Recurrent Neural Networks via\n  Sememes", "abstract": "Sememes, the minimum semantic units of human languages, have been\nsuccessfully utilized in various natural language processing applications.\nHowever, most existing studies exploit sememes in specific tasks and few\nefforts are made to utilize sememes more fundamentally. In this paper, we\npropose to incorporate sememes into recurrent neural networks (RNNs) to improve\ntheir sequence modeling ability, which is beneficial to all kinds of downstream\ntasks. We design three different sememe incorporation methods and employ them\nin typical RNNs including LSTM, GRU and their bidirectional variants. In\nevaluation, we use several benchmark datasets involving PTB and WikiText-2 for\nlanguage modeling, SNLI for natural language inference and another two datasets\nfor sentiment analysis and paraphrase detection. Experimental results show\nevident and consistent improvement of our sememe-incorporated models compared\nwith vanilla RNNs, which proves the effectiveness of our sememe incorporation\nmethods. Moreover, we find the sememe-incorporated models have higher\nrobustness and outperform adversarial training in defending adversarial attack.\nAll the code and data of this work can be obtained at\nhttps://github.com/thunlp/SememeRNN.", "published": "2019-10-20 06:43:21", "link": "http://arxiv.org/abs/1910.08910v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Hierarchical Representations for Preserving Privacy and\n  Utility in Text", "abstract": "Guaranteeing a certain level of user privacy in an arbitrary piece of text is\na challenging issue. However, with this challenge comes the potential of\nunlocking access to vast data stores for training machine learning models and\nsupporting data driven decisions. We address this problem through the lens of\ndx-privacy, a generalization of Differential Privacy to non Hamming distance\nmetrics. In this work, we explore word representations in Hyperbolic space as a\nmeans of preserving privacy in text. We provide a proof satisfying dx-privacy,\nthen we define a probability distribution in Hyperbolic space and describe a\nway to sample from it in high dimensions. Privacy is provided by perturbing\nvector representations of words in high dimensional Hyperbolic space to obtain\na semantic generalization. We conduct a series of experiments to demonstrate\nthe tradeoff between privacy and utility. Our privacy experiments illustrate\nprotections against an authorship attribution algorithm while our utility\nexperiments highlight the minimal impact of our perturbations on several\ndownstream machine learning models. Compared to the Euclidean baseline, we\nobserve > 20x greater guarantees on expected privacy against comparable worst\ncase statistics.", "published": "2019-10-20 07:16:29", "link": "http://arxiv.org/abs/1910.08917v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Predicting the Leading Political Ideology of YouTube Channels Using\n  Acoustic, Textual, and Metadata Information", "abstract": "We address the problem of predicting the leading political ideology, i.e.,\nleft-center-right bias, for YouTube channels of news media. Previous work on\nthe problem has focused exclusively on text and on analysis of the language\nused, topics discussed, sentiment, and the like. In contrast, here we study\nvideos, which yields an interesting multimodal setup. Starting with gold\nannotations about the leading political ideology of major world news media from\nMedia Bias/Fact Check, we searched on YouTube to find their corresponding\nchannels, and we downloaded a recent sample of videos from each channel. We\ncrawled more than 1,000 YouTube hours along with the corresponding subtitles\nand metadata, thus producing a new multimodal dataset. We further developed a\nmultimodal deep-learning architecture for the task. Our analysis shows that the\nuse of acoustic signal helped to improve bias detection by more than 6%\nabsolute over using text and metadata only. We release the dataset to the\nresearch community, hoping to help advance the field of multi-modal political\nbias detection.", "published": "2019-10-20 11:05:05", "link": "http://arxiv.org/abs/1910.08948v1", "categories": ["cs.CL", "cs.IR", "cs.SD", "eess.AS", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Discovering Phonemic Tone Contours", "abstract": "Tone is a prosodic feature used to distinguish words in many languages, some\nof which are endangered and scarcely documented. In this work, we use\nunsupervised representation learning to identify probable clusters of syllables\nthat share the same phonemic tone. Our method extracts the pitch for each\nsyllable, then trains a convolutional autoencoder to learn a low dimensional\nrepresentation for each contour. We then apply the mean shift algorithm to\ncluster tones in high-density regions of the latent space. Furthermore, by\nfeeding the centers of each cluster into the decoder, we produce a prototypical\ncontour that represents each cluster. We apply this method to spoken\nmulti-syllable words in Mandarin Chinese and Cantonese and evaluate how closely\nour clusters match the ground truth tone categories. Finally, we discuss some\ndifficulties with our approach, including contextual tone variation and\nallophony effects.", "published": "2019-10-20 14:18:51", "link": "http://arxiv.org/abs/1910.08987v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep speech inpainting of time-frequency masks", "abstract": "Transient loud intrusions, often occurring in noisy environments, can\ncompletely overpower speech signal and lead to an inevitable loss of\ninformation. While existing algorithms for noise suppression can yield\nimpressive results, their efficacy remains limited for very low signal-to-noise\nratios or when parts of the signal are missing. To address these limitations,\nhere we propose an end-to-end framework for speech inpainting, the\ncontext-based retrieval of missing or severely distorted parts of\ntime-frequency representation of speech. The framework is based on a\nconvolutional U-Net trained via deep feature losses, obtained using speechVGG,\na deep speech feature extractor pre-trained on an auxiliary word classification\ntask. Our evaluation results demonstrate that the proposed framework can\nrecover large portions of missing or distorted time-frequency representation of\nspeech, up to 400 ms and 3.2 kHz in bandwidth. In particular, our approach\nprovided a substantial increase in STOI & PESQ objective metrics of the\ninitially corrupted speech samples. Notably, using deep feature losses to train\nthe framework led to the best results, as compared to conventional approaches.", "published": "2019-10-20 20:24:39", "link": "http://arxiv.org/abs/1910.09058v5", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computer-supported Analysis of Positive Properties, Ultrafilters and\n  Modal Collapse in Variants of G\u00f6del's Ontological Argument", "abstract": "Three variants of Kurt G\\\"odel's ontological argument, proposed by Dana\nScott, C. Anthony Anderson and Melvin Fitting, are encoded and rigorously\nassessed on the computer. In contrast to Scott's version of G\\\"odel's argument\nthe two variants contributed by Anderson and Fitting avoid modal collapse.\nAlthough they appear quite different on a cursory reading they are in fact\nclosely related. This has been revealed in the computer-supported formal\nanalysis presented in this article. Key to our formal analysis is the\nutilization of suitably adapted notions of (modal) ultrafilters, and a careful\ndistinction between extensions and intensions of positive properties.", "published": "2019-10-20 11:54:05", "link": "http://arxiv.org/abs/1910.08955v2", "categories": ["cs.LO", "cs.AI", "cs.CL", "math.GN", "math.LO", "03Axx, 03B15, 03B45, 03B60, 03B80, 68T15, 68T27, 68T30", "F.4.0; F.4.1; I.2.3; I.2.4; J.5; I.1.3"], "primary_category": "cs.LO"}
{"title": "Speech-Based Parameter Estimation of an Asymmetric Vocal Fold\n  Oscillation Model and Its Application in Discriminating Vocal Fold\n  Pathologies", "abstract": "So far, several physical models have been proposed for the study of vocal\nfold oscillations during phonation. The parameters of these models, such as\nvocal fold elasticity, resistance, etc. are traditionally determined through\nthe observation and measurement of the vocal fold vibrations in the larynx.\nSince such direct measurements tend to be the most accurate, the traditional\npractice has been to set the parameter values of these models based on\nmeasurements that are averaged across an ensemble of human subjects. However,\nthe direct measurement process is hard to revise outside of clinical settings.\nIn many cases, especially in pathological ones, the properties of the vocal\nfolds often deviate from their generic values---sometimes asymmetrically\nwherein the characteristics of the two vocal folds differ for the same\nindividual. In such cases, it is desirable to find a more scalable way to\nadjust the model parameters on a case by case basis. In this paper, we present\na novel and alternate way to determine vocal fold model parameters from the\nspeech signal. We focus on an asymmetric model and show that for such models,\ndifferences in estimated parameters can be successfully used to discriminate\nbetween voices that are characteristic of different underlying vocal fold\npathologies.", "published": "2019-10-20 03:36:36", "link": "http://arxiv.org/abs/1910.08886v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition with Dual-Sequence LSTM Architecture", "abstract": "Speech Emotion Recognition (SER) has emerged as a critical component of the\nnext generation human-machine interfacing technologies. In this work, we\npropose a new dual-level model that predicts emotions based on both MFCC\nfeatures and mel-spectrograms produced from raw audio signals. Each utterance\nis preprocessed into MFCC features and two mel-spectrograms at different\ntime-frequency resolutions. A standard LSTM processes the MFCC features, while\na novel LSTM architecture, denoted as Dual-Sequence LSTM (DS-LSTM), processes\nthe two mel-spectrograms simultaneously. The outputs are later averaged to\nproduce a final classification of the utterance. Our proposed model achieves,\non average, a weighted accuracy of 72.7% and an unweighted accuracy of\n73.3%---a 6% improvement over current state-of-the-art unimodal models---and is\ncomparable with multimodal models that leverage textual information as well as\naudio signals.", "published": "2019-10-20 02:04:55", "link": "http://arxiv.org/abs/1910.08874v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musical Instrument Playing Technique Detection Based on FCN: Using\n  Chinese Bowed-Stringed Instrument as an Example", "abstract": "Unlike melody extraction and other aspects of music transcription, research\non playing technique detection is still in its early stages. Compared to\nexisting work mostly focused on playing technique detection for individual\nsingle notes, we propose a general end-to-end method based on Sound Event\nDetection by FCN for musical instrument playing technique detection. In our\ncase, we choose Erhu, a well-known Chinese bowed-stringed instrument, to\nexperiment with our method. Because of the limitation of FCN, we present an\nalgorithm to detect on variable length audio. The effectiveness of the proposed\nframework is tested on a new dataset, its categorization of techniques is\nsimilar to our training dataset. The highest accuracy of our 3 experiments on\nthe new test set is 87.31%. Furthermore, we also evaluate the performance of\nthe proposed framework on 10 real-world studio music (produced by midi) and 7\nreal-world recording samples to address the ability of generalization on our\nmodel.", "published": "2019-10-20 16:50:49", "link": "http://arxiv.org/abs/1910.09021v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
