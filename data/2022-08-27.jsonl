{"title": "Domain-Specific NER via Retrieving Correlated Samples", "abstract": "Successful Machine Learning based Named Entity Recognition models could fail\non texts from some special domains, for instance, Chinese addresses and\ne-commerce titles, where requires adequate background knowledge. Such texts are\nalso difficult for human annotators. In fact, we can obtain some potentially\nhelpful information from correlated texts, which have some common entities, to\nhelp the text understanding. Then, one can easily reason out the correct answer\nby referencing correlated samples. In this paper, we suggest enhancing NER\nmodels with correlated samples. We draw correlated samples by the sparse BM25\nretriever from large-scale in-domain unlabeled data. To explicitly simulate the\nhuman reasoning process, we perform a training-free entity type calibrating by\nmajority voting. To capture correlation features in the training stage, we\nsuggest to model correlated samples by the transformer-based multi-instance\ncross-encoder. Empirical results on datasets of the above two domains show the\nefficacy of our methods.", "published": "2022-08-27 12:25:24", "link": "http://arxiv.org/abs/2208.12995v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Format Transfer Learning Model for Event Argument Extraction via\n  Variational Information Bottleneck", "abstract": "Event argument extraction (EAE) aims to extract arguments with given roles\nfrom texts, which have been widely studied in natural language processing. Most\nprevious works have achieved good performance in specific EAE datasets with\ndedicated neural architectures. Whereas, these architectures are usually\ndifficult to adapt to new datasets/scenarios with various annotation schemas or\nformats. Furthermore, they rely on large-scale labeled data for training, which\nis unavailable due to the high labelling cost in most cases. In this paper, we\npropose a multi-format transfer learning model with variational information\nbottleneck, which makes use of the information especially the common knowledge\nin existing datasets for EAE in new datasets. Specifically, we introduce a\nshared-specific prompt framework to learn both format-shared and\nformat-specific knowledge from datasets with different formats. In order to\nfurther absorb the common knowledge for EAE and eliminate the irrelevant noise,\nwe integrate variational information bottleneck into our architecture to refine\nthe shared representation. We conduct extensive experiments on three benchmark\ndatasets, and obtain new state-of-the-art performance on EAE.", "published": "2022-08-27 13:52:01", "link": "http://arxiv.org/abs/2208.13017v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MDIA: A Benchmark for Multilingual Dialogue Generation in 46 Languages", "abstract": "Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.", "published": "2022-08-27 19:35:20", "link": "http://arxiv.org/abs/2208.13078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying French Document Complexity", "abstract": "Measuring a document's complexity level is an open challenge, particularly\nwhen one is working on a diverse corpus of documents rather than comparing\nseveral documents on a similar topic or working on a language other than\nEnglish. In this paper, we define a methodology to measure the complexity of\nFrench documents, using a new general and diversified corpus of texts, the\n\"French Canadian complexity level corpus\", and a wide range of metrics. We\ncompare different learning algorithms to this task and contrast their\nperformances and their observations on which characteristics of the texts are\nmore significant to their complexity. Our results show that our methodology\ngives a general-purpose measurement of text complexity in French.", "published": "2022-08-27 04:12:54", "link": "http://arxiv.org/abs/2208.12924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Unsupervised Training of Link Grammar Based Language Models", "abstract": "In this short note we explore what is needed for the unsupervised training of\ngraph language models based on link grammars. First, we introduce the\nter-mination tags formalism required to build a language model based on a link\ngrammar formalism of Sleator and Temperley [21] and discuss the influence of\ncontext on the unsupervised learning of link grammars. Second, we pro-pose a\nstatistical link grammar formalism, allowing for statistical language\ngeneration. Third, based on the above formalism, we show that the classical\ndissertation of Yuret [25] on discovery of linguistic relations using lexical\nat-traction ignores contextual properties of the language, and thus the\napproach to unsupervised language learning relying just on bigrams is flawed.\nThis correlates well with the unimpressive results in unsupervised training of\ngraph language models based on bigram approach of Yuret.", "published": "2022-08-27 14:07:24", "link": "http://arxiv.org/abs/2208.13021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Textwash -- automated open-source text anonymisation", "abstract": "The increased use of text data in social science research has benefited from\neasy-to-access data (e.g., Twitter). That trend comes at the cost of research\nrequiring sensitive but hard-to-share data (e.g., interview data, police\nreports, electronic health records). We introduce a solution to that stalemate\nwith the open-source text anonymisation software_Textwash_. This paper presents\nthe empirical evaluation of the tool using the TILD criteria: a technical\nevaluation (how accurate is the tool?), an information loss evaluation (how\nmuch information is lost in the anonymisation process?) and a de-anonymisation\ntest (can humans identify individuals from anonymised text data?). The findings\nsuggest that Textwash performs similar to state-of-the-art entity recognition\nmodels and introduces a negligible information loss of 0.84%. For the\nde-anonymisation test, we tasked humans to identify individuals by name from a\ndataset of crowdsourced person descriptions of very famous, semi-famous and\nnon-existing individuals. The de-anonymisation rate ranged from 1.01-2.01% for\nthe realistic use cases of the tool. We replicated the findings in a second\nstudy and concluded that Textwash succeeds in removing potentially sensitive\ninformation that renders detailed person descriptions practically anonymous.", "published": "2022-08-27 19:55:19", "link": "http://arxiv.org/abs/2208.13081v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Conversion of Legal Agreements into Smart Legal Contracts using NLP", "abstract": "A Smart Legal Contract (SLC) is a specialized digital agreement comprising\nnatural language and computable components. The Accord Project provides an\nopen-source SLC framework containing three main modules: Cicero, Concerto, and\nErgo. Currently, we need lawyers, programmers, and clients to work together\nwith great effort to create a usable SLC using the Accord Project. This paper\nproposes a pipeline to automate the SLC creation process with several Natural\nLanguage Processing (NLP) models to convert law contracts to the Accord\nProject's Concerto model. After evaluating the proposed pipeline, we discovered\nthat our NER pipeline accurately detects CiceroMark from Accord Project\ntemplate text with an accuracy of 0.8. Additionally, our Question Answering\nmethod can extract one-third of the Concerto variables from the template text.\nWe also delve into some limitations and possible future research for the\nproposed pipeline. Finally, we describe a web interface enabling users to build\nSLCs. This interface leverages the proposed pipeline to convert text documents\nto Smart Legal Contracts by using NLP models.", "published": "2022-08-27 06:54:58", "link": "http://arxiv.org/abs/2210.08954v2", "categories": ["cs.CY", "cs.CL", "68T50", "I.7"], "primary_category": "cs.CY"}
{"title": "Target Speaker Voice Activity Detection with Transformers and Its\n  Integration with End-to-End Neural Diarization", "abstract": "This paper describes a speaker diarization model based on target speaker\nvoice activity detection (TS-VAD) using transformers. To overcome the original\nTS-VAD model's drawback of being unable to handle an arbitrary number of\nspeakers, we investigate model architectures that use input tensors with\nvariable-length time and speaker dimensions. Transformer layers are applied to\nthe speaker axis to make the model output insensitive to the order of the\nspeaker profiles provided to the TS-VAD model. Time-wise sequential layers are\ninterspersed between these speaker-wise transformer layers to allow the\ntemporal and cross-speaker correlations of the input speech signal to be\ncaptured. We also extend a diarization model based on end-to-end neural\ndiarization with encoder-decoder based attractors (EEND-EDA) by replacing its\ndot-product-based speaker detection layer with the transformer-based TS-VAD.\nExperimental results on VoxConverse show that using the transformers for the\ncross-speaker modeling reduces the diarization error rate (DER) of TS-VAD by\n11.3%, achieving a new state-of-the-art (SOTA) DER of 4.57%. Also, our extended\nEEND-EDA reduces DER by 6.9% on the CALLHOME dataset relative to the original\nEEND-EDA with a similar model size, achieving a new SOTA DER of 11.18% under a\nwidely used training data setting.", "published": "2022-08-27 21:11:45", "link": "http://arxiv.org/abs/2208.13085v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Minimal Feature Analysis for Isolated Digit Recognition for varying\n  encoding rates in noisy environments", "abstract": "This research work is about recent development made in speech recognition. In\nthis research work, analysis of isolated digit recognition in the presence of\ndifferent bit rates and at different noise levels has been performed. This\nresearch work has been carried using audacity and HTK toolkit. Hidden Markov\nModel (HMM) is the recognition model which was used to perform this experiment.\nThe feature extraction techniques used are Mel Frequency Cepstrum coefficient\n(MFCC), Linear Predictive Coding (LPC), perceptual linear predictive (PLP), mel\nspectrum (MELSPEC), filter bank (FBANK). There were three types of different\nnoise levels which have been considered for testing of data. These include\nrandom noise, fan noise and random noise in real time environment. This was\ndone to analyse the best environment which can used for real time applications.\nFurther, five different types of commonly used bit rates at different sampling\nrates were considered to find out the most optimum bit rate.", "published": "2022-08-27 23:05:06", "link": "http://arxiv.org/abs/2208.13100v1", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy\n  Treatment Strategies with Deep Reinforcement Learning", "abstract": "We propose a recommendation system that suggests treatment strategies to a\ntherapist during the psychotherapy session in real-time. Our system uses a\nturn-level rating mechanism that predicts the therapeutic outcome by computing\na similarity score between the deep embedding of a scoring inventory, and the\ncurrent sentence that the patient is speaking. The system automatically\ntranscribes a continuous audio stream and separates it into turns of the\npatient and of the therapist and perform real-time inference of their\ntherapeutic working alliance. The dialogue pairs along with their computed\nworking alliance as ratings are then fed into a deep reinforcement learning\nrecommendation system where the sessions are treated as users and the topics\nare treated as items. Other than evaluating the empirical advantages of the\ncore components on an existing dataset of psychotherapy sessions, we\ndemonstrate the effectiveness of this system in a web app.", "published": "2022-08-27 19:22:53", "link": "http://arxiv.org/abs/2208.13077v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "SA: Sliding attack for synthetic speech detection with resistance to\n  clipping and self-splicing", "abstract": "Deep neural networks are vulnerable to adversarial examples that mislead\nmodels with imperceptible perturbations. In audio, although adversarial\nexamples have achieved incredible attack success rates on white-box settings\nand black-box settings, most existing adversarial attacks are constrained by\nthe input length. A More practical scenario is that the adversarial examples\nmust be clipped or self-spliced and input into the black-box model. Therefore,\nit is necessary to explore how to improve transferability in different input\nlength settings. In this paper, we take the synthetic speech detection task as\nan example and consider two representative SOTA models. We observe that the\ngradients of fragments with the same sample value are similar in different\nmodels via analyzing the gradients obtained by feeding samples into the model\nafter cropping or self-splicing. Inspired by the above observation, we propose\na new adversarial attack method termed sliding attack. Specifically, we make\neach sampling point aware of gradients at different locations, which can\nsimulate the situation where adversarial examples are input to black-box models\nwith varying input lengths. Therefore, instead of using the current gradient\ndirectly in each iteration of the gradient calculation, we go through the\nfollowing three steps. First, we extract subsegments of different lengths using\nsliding windows. We then augment the subsegments with data from the adjacent\ndomains. Finally, we feed the sub-segments into different models to obtain\naggregate gradients to update adversarial examples. Empirical results\ndemonstrate that our method could significantly improve the transferability of\nadversarial examples after clipping or self-splicing. Besides, our method could\nalso enhance the transferability between models based on different features.", "published": "2022-08-27 18:09:26", "link": "http://arxiv.org/abs/2208.13066v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sub-mW Neuromorphic SNN audio processing applications with Rockpool and\n  Xylo", "abstract": "Spiking Neural Networks (SNNs) provide an efficient computational mechanism\nfor temporal signal processing, especially when coupled with low-power SNN\ninference ASICs. SNNs have been historically difficult to configure, lacking a\ngeneral method for finding solutions for arbitrary tasks. In recent years,\ngradient-descent optimization methods have been applied to SNNs with increasing\nease. SNNs and SNN inference processors therefore offer a good platform for\ncommercial low-power signal processing in energy constrained environments\nwithout cloud dependencies. However, to date these methods have not been\naccessible to ML engineers in industry, requiring graduate-level training to\nsuccessfully configure a single SNN application. Here we demonstrate a\nconvenient high-level pipeline to design, train and deploy arbitrary temporal\nsignal processing applications to sub-mW SNN inference hardware. We apply a new\nstraightforward SNN architecture designed for temporal signal processing, using\na pyramid of synaptic time constants to extract signal features at a range of\ntemporal scales. We demonstrate this architecture on an ambient audio\nclassification task, deployed to the Xylo SNN inference processor in streaming\nmode. Our application achieves high accuracy (98%) and low latency (100ms) at\nlow power (<100$\\mu$W inference power). Our approach makes training and\ndeploying SNN applications available to ML engineers with general NN\nbackgrounds, without requiring specific prior experience with spiking NNs. We\nintend for our approach to make Neuromorphic hardware and SNNs an attractive\nchoice for commercial low-power and edge signal processing applications.", "published": "2022-08-27 11:50:32", "link": "http://arxiv.org/abs/2208.12991v3", "categories": ["cs.NE", "cs.AI", "cs.ET", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
