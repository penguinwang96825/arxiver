{"title": "A Digital Corpus of St. Lawrence Island Yupik", "abstract": "St. Lawrence Island Yupik (ISO 639-3: ess) is an endangered polysynthetic\nlanguage in the Inuit-Yupik language family indigenous to Alaska and Chukotka.\nThis work presents a step-by-step pipeline for the digitization of written\ntexts, and the first publicly available digital corpus for St. Lawrence Island\nYupik, created using that pipeline. This corpus has great potential for future\nlinguistic inquiry and research in NLP. It was also developed for use in Yupik\nlanguage education and revitalization, with a primary goal of enabling easy\naccess to Yupik texts by educators and by members of the Yupik community. A\nsecondary goal is to support development of language technology such as\nspell-checkers, text-completion systems, interactive e-books, and language\nlearning apps for use by the Yupik community.", "published": "2021-01-26 00:14:00", "link": "http://arxiv.org/abs/2101.10496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coloring the Black Box: What Synesthesia Tells Us about Character\n  Embeddings", "abstract": "In contrast to their word- or sentence-level counterparts, character\nembeddings are still poorly understood. We aim at closing this gap with an\nin-depth study of English character embeddings. For this, we use resources from\nresearch on grapheme-color synesthesia -- a neuropsychological phenomenon where\nletters are associated with colors, which give us insight into which characters\nare similar for synesthetes and how characters are organized in color space.\nComparing 10 different character embeddings, we ask: How similar are character\nembeddings to a synesthete's perception of characters? And how similar are\ncharacter embeddings extracted from different models? We find that LSTMs agree\nwith humans more than transformers. Comparing across tasks, grapheme-to-phoneme\nconversion results in the most human-like character embeddings. Finally, ELMo\nembeddings differ from both humans and other models.", "published": "2021-01-26 05:21:58", "link": "http://arxiv.org/abs/2101.10565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representations for Question Answering from Documents with Tables and\n  Text", "abstract": "Tables in Web documents are pervasive and can be directly used to answer many\nof the queries searched on the Web, motivating their integration in question\nanswering. Very often information presented in tables is succinct and hard to\ninterpret with standard language representations. On the other hand, tables\noften appear within textual context, such as an article describing the table.\nUsing the information from an article as additional context can potentially\nenrich table representations. In this work we aim to improve question answering\nfrom tables by refining table representations based on information from\nsurrounding text. We also present an effective method to combine text and\ntable-based predictions for question answering from full documents, obtaining\nsignificant improvements on the Natural Questions dataset.", "published": "2021-01-26 05:52:20", "link": "http://arxiv.org/abs/2101.10573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Syntactically Controlled Paraphrases without Using Annotated\n  Parallel Pairs", "abstract": "Paraphrase generation plays an essential role in natural language process\n(NLP), and it has many downstream applications. However, training supervised\nparaphrase models requires many annotated paraphrase pairs, which are usually\ncostly to obtain. On the other hand, the paraphrases generated by existing\nunsupervised approaches are usually syntactically similar to the source\nsentences and are limited in diversity. In this paper, we demonstrate that it\nis possible to generate syntactically various paraphrases without the need for\nannotated paraphrase pairs. We propose Syntactically controlled Paraphrase\nGenerator (SynPG), an encoder-decoder based model that learns to disentangle\nthe semantics and the syntax of a sentence from a collection of unannotated\ntexts. The disentanglement enables SynPG to control the syntax of output\nparaphrases by manipulating the embedding in the syntactic space. Extensive\nexperiments using automatic metrics and human evaluation show that SynPG\nperforms better syntactic control than unsupervised baselines, while the\nquality of the generated paraphrases is competitive. We also demonstrate that\nthe performance of SynPG is competitive or even better than supervised models\nwhen the unannotated data is large. Finally, we show that the syntactically\ncontrolled paraphrases generated by SynPG can be utilized for data augmentation\nto improve the robustness of NLP models.", "published": "2021-01-26 06:13:52", "link": "http://arxiv.org/abs/2101.10579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low Resource Recognition and Linking of Biomedical Concepts from a Large\n  Ontology", "abstract": "Tools to explore scientific literature are essential for scientists,\nespecially in biomedicine, where about a million new papers are published every\nyear. Many such tools provide users the ability to search for specific entities\n(e.g. proteins, diseases) by tracking their mentions in papers. PubMed, the\nmost well known database of biomedical papers, relies on human curators to add\nthese annotations. This can take several weeks for new papers, and not all\npapers get tagged. Machine learning models have been developed to facilitate\nthe semantic indexing of scientific papers. However their performance on the\nmore comprehensive ontologies of biomedical concepts does not reach the levels\nof typical entity recognition problems studied in NLP. In large part this is\ndue to their low resources, where the ontologies are large, there is a lack of\ndescriptive text defining most entities, and labeled data can only cover a\nsmall portion of the ontology. In this paper, we develop a new model that\novercomes these challenges by (1) generalizing to entities unseen at training\ntime, and (2) incorporating linking predictions into the mention segmentation\ndecisions. Our approach achieves new state-of-the-art results for the UMLS\nontology in both traditional recognition/linking (+8 F1 pts) as well as\nsemantic indexing-based evaluation (+10 F1 pts).", "published": "2021-01-26 06:41:12", "link": "http://arxiv.org/abs/2101.10587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural machine translation, corpus and frugality", "abstract": "In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.", "published": "2021-01-26 09:22:20", "link": "http://arxiv.org/abs/2101.10650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Transitivity in Neural NLI Models through Veridicality", "abstract": "Despite the recent success of deep neural networks in natural language\nprocessing, the extent to which they can demonstrate human-like generalization\ncapacities for natural language understanding remains unclear. We explore this\nissue in the domain of natural language inference (NLI), focusing on the\ntransitivity of inference relations, a fundamental property for systematically\ndrawing inferences. A model capturing transitivity can compose basic inference\npatterns and draw new inferences. We introduce an analysis method using\nsynthetic and naturalistic NLI datasets involving clause-embedding verbs to\nevaluate whether models can perform transitivity inferences composed of\nveridical inferences and arbitrary inference types. We find that current NLI\nmodels do not perform consistently well on transitivity inference tasks,\nsuggesting that they lack the generalization capacity for drawing composite\ninferences from provided training examples. The data and code for our analysis\nare publicly available at https://github.com/verypluming/transitivity.", "published": "2021-01-26 11:18:35", "link": "http://arxiv.org/abs/2101.10713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Deep Generative Models and Multi-lingual Pretraining for\n  Semi-supervised Document Classification", "abstract": "Semi-supervised learning through deep generative models and multi-lingual\npretraining techniques have orchestrated tremendous success across different\nareas of NLP. Nonetheless, their development has happened in isolation, while\nthe combination of both could potentially be effective for tackling\ntask-specific labelled data shortage. To bridge this gap, we combine\nsemi-supervised deep generative models and multi-lingual pretraining to form a\npipeline for document classification task. Compared to strong supervised\nlearning baselines, our semi-supervised classification framework is highly\ncompetitive and outperforms the state-of-the-art counterparts in low-resource\nsettings across several languages.", "published": "2021-01-26 11:26:14", "link": "http://arxiv.org/abs/2101.10717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spark NLP: Natural Language Understanding at Scale", "abstract": "Spark NLP is a Natural Language Processing (NLP) library built on top of\nApache Spark ML. It provides simple, performant and accurate NLP annotations\nfor machine learning pipelines that can scale easily in a distributed\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\nhealthcare organizations as the worlds most widely used NLP library in the\nenterprise.", "published": "2021-01-26 15:11:52", "link": "http://arxiv.org/abs/2101.10848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Can Reflect Syntactic Structure (If You Let It)", "abstract": "Since the popularization of the Transformer as a general-purpose feature\nencoder for NLP, many studies have attempted to decode linguistic structure\nfrom its novel multi-head attention mechanism. However, much of such work\nfocused almost exclusively on English -- a language with rigid word order and a\nlack of inflectional morphology. In this study, we present decoding experiments\nfor multilingual BERT across 18 languages in order to test the generalizability\nof the claim that dependency syntax is reflected in attention patterns. We show\nthat full trees can be decoded above baseline accuracy from single attention\nheads, and that individual relations are often tracked by the same heads across\nlanguages. Furthermore, in an attempt to address recent debates about the\nstatus of attention as an explanatory mechanism, we experiment with fine-tuning\nmBERT on a supervised parsing objective while freezing different series of\nparameters. Interestingly, in steering the objective to learn explicit\nlinguistic structure, we find much of the same structure represented in the\nresulting attention patterns, with interesting differences with respect to\nwhich parameters are frozen.", "published": "2021-01-26 16:49:16", "link": "http://arxiv.org/abs/2101.10927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Laughing at you or with you\": The Role of Sarcasm in Shaping the\n  Disagreement Space", "abstract": "Detecting arguments in online interactions is useful to understand how\nconflicts arise and get resolved. Users often use figurative language, such as\nsarcasm, either as persuasive devices or to attack the opponent by an ad\nhominem argument. To further our understanding of the role of sarcasm in\nshaping the disagreement space, we present a thorough experimental setup using\na corpus annotated with both argumentative moves (agree/disagree) and sarcasm.\nWe exploit joint modeling in terms of (a) applying discrete features that are\nuseful in detecting sarcasm to the task of argumentative relation\nclassification (agree/disagree/none), and (b) multitask learning for\nargumentative relation classification and sarcasm detection using deep learning\narchitectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical\nattention and Transformer-based architectures). We demonstrate that modeling\nsarcasm improves the argumentative relation classification task\n(agree/disagree/none) in all setups.", "published": "2021-01-26 17:19:18", "link": "http://arxiv.org/abs/2101.10952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Approaches to Document-level Machine Translation", "abstract": "Document-level machine translation conditions on surrounding sentences to\nproduce coherent translations. There has been much recent work in this area\nwith the introduction of custom model architectures and decoding algorithms.\nThis paper presents a systematic comparison of selected approaches from the\nliterature on two benchmarks for which document-level phenomena evaluation\nsuites exist. We find that a simple method based purely on back-translating\nmonolingual document-level data performs as well as much more elaborate\nalternatives, both in terms of document-level metrics as well as human\nevaluation.", "published": "2021-01-26 19:21:09", "link": "http://arxiv.org/abs/2101.11040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT", "abstract": "We investigate how Multilingual BERT (mBERT) encodes grammar by examining how\nthe high-order grammatical feature of morphosyntactic alignment (how different\nlanguages define what counts as a \"subject\") is manifested across the embedding\nspaces of different languages. To understand if and how morphosyntactic\nalignment affects contextual embedding spaces, we train classifiers to recover\nthe subjecthood of mBERT embeddings in transitive sentences (which do not\ncontain overt information about morphosyntactic alignment) and then evaluate\nthem zero-shot on intransitive sentences (where subjecthood classification\ndepends on alignment), within and across languages. We find that the resulting\nclassifier distributions reflect the morphosyntactic alignment of their\ntraining languages. Our results demonstrate that mBERT representations are\ninfluenced by high-level grammatical features that are not manifested in any\none input sentence, and that this is robust across languages. Further examining\nthe characteristics that our classifiers rely on, we find that features such as\npassive voice, animacy and case strongly correlate with classification\ndecisions, suggesting that mBERT does not encode subjecthood purely\nsyntactically, but that subjecthood embedding is continuous and dependent on\nsemantic and discourse factors, as is proposed in much of the functional\nlinguistics literature. Together, these results provide insight into how\ngrammatical features manifest in contextual embedding spaces, at a level of\nabstraction not covered by previous work.", "published": "2021-01-26 19:21:59", "link": "http://arxiv.org/abs/2101.11043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "First Align, then Predict: Understanding the Cross-Lingual Ability of\n  Multilingual BERT", "abstract": "Multilingual pretrained language models have demonstrated remarkable\nzero-shot cross-lingual transfer capabilities. Such transfer emerges by\nfine-tuning on a task of interest in one language and evaluating on a distinct\nlanguage, not seen during the fine-tuning. Despite promising results, we still\nlack a proper understanding of the source of this transfer. Using a novel layer\nablation technique and analyses of the model's internal representations, we\nshow that multilingual BERT, a popular multilingual language model, can be\nviewed as the stacking of two sub-networks: a multilingual encoder followed by\na task-specific language-agnostic predictor. While the encoder is crucial for\ncross-lingual transfer and remains mostly unchanged during fine-tuning, the\ntask predictor has little importance on the transfer and can be reinitialized\nduring fine-tuning. We present extensive experiments with three distinct tasks,\nseventeen typologically diverse languages and multiple domains to support our\nhypothesis.", "published": "2021-01-26 22:12:38", "link": "http://arxiv.org/abs/2101.11109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New\n  Approach Using XLM-RoBERTa Alignment", "abstract": "We propose a novel approach for cross-lingual Named Entity Recognition (NER)\nzero-shot transfer using parallel corpora. We built an entity alignment model\non top of XLM-RoBERTa to project the entities detected on the English part of\nthe parallel data to the target language sentences, whose accuracy surpasses\nall previous unsupervised models. With the alignment model we can get\npseudo-labeled NER data set in the target language to train task-specific\nmodel. Unlike using translation methods, this approach benefits from natural\nfluency and nuances in target-language original corpus. We also propose a\nmodified loss function similar to focal loss but assigns weights in the\nopposite direction to further improve the model training on noisy\npseudo-labeled data set. We evaluated this proposed approach over 4 target\nlanguages on benchmark data sets and got competitive F1 scores compared to most\nrecent SOTA models. We also gave extra discussions about the impact of parallel\ncorpus size and domain on the final transfer performance.", "published": "2021-01-26 22:19:52", "link": "http://arxiv.org/abs/2101.11112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition in the Style of Object Detection", "abstract": "In this work, we propose a two-stage method for named entity recognition\n(NER), especially for nested NER. We borrowed the idea from the two-stage\nObject Detection in computer vision and the way how they construct the loss\nfunction. First, a region proposal network generates region candidates and then\na second-stage model discriminates and classifies the entity and makes the\nfinal prediction. We also designed a special loss function for the second-stage\ntraining that predicts the entityness and entity type at the same time. The\nmodel is built on top of pretrained BERT encoders, and we tried both BERT base\nand BERT large models. For experiments, we first applied it to flat NER tasks\nsuch as CoNLL2003 and OntoNotes 5.0 and got comparable results with traditional\nNER models using sequence labeling methodology. We then tested the model on the\nnested named entity recognition task ACE2005 and Genia, and got F1 score of\n85.6$\\%$ and 76.8$\\%$ respectively. In terms of the second-stage training, we\nfound that adding extra randomly selected regions plays an important role in\nimproving the precision. We also did error profiling to better evaluate the\nperformance of the model in different circumstances for potential improvements\nin the future.", "published": "2021-01-26 22:47:05", "link": "http://arxiv.org/abs/2101.11122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLiMP: A Benchmark for Chinese Language Model Evaluation", "abstract": "Linguistically informed analyses of language models (LMs) contribute to the\nunderstanding and improvement of these models. Here, we introduce the corpus of\nChinese linguistic minimal pairs (CLiMP), which can be used to investigate what\nknowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs\n(MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin\nlinguistic phenomena. The MPs are semi-automatically generated, and human\nagreement with the labels in CLiMP is 95.8%. We evaluated 11 different LMs on\nCLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier-noun\nagreement and verb complement selection are the phenomena that models generally\nperform best at. However, models struggle the most with the ba construction,\nbinding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8%\naverage accuracy, while the performances of LSTMs and 5-grams are only\nmoderately above chance level.", "published": "2021-01-26 23:16:29", "link": "http://arxiv.org/abs/2101.11131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Abstractive Summarization of Bengali Text Documents", "abstract": "Abstractive summarization systems generally rely on large collections of\ndocument-summary pairs. However, the performance of abstractive systems remains\na challenge due to the unavailability of parallel data for low-resource\nlanguages like Bengali. To overcome this problem, we propose a graph-based\nunsupervised abstractive summarization system in the single-document setting\nfor Bengali text documents, which requires only a Part-Of-Speech (POS) tagger\nand a pre-trained language model trained on Bengali texts. We also provide a\nhuman-annotated dataset with document-summary pairs to evaluate our abstractive\nmodel and to support the comparison of future abstractive summarization systems\nof the Bengali Language. We conduct experiments on this dataset and compare our\nsystem with several well-established unsupervised extractive summarization\nsystems. Our unsupervised abstractive summarization model outperforms the\nbaselines without being exposed to any human-annotated reference summaries.", "published": "2021-01-26 11:41:28", "link": "http://arxiv.org/abs/2102.04490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Entity Alignment in the Open World: An Unsupervised Approach", "abstract": "Entity alignment (EA) aims to discover the equivalent entities in different\nknowledge graphs (KGs). It is a pivotal step for integrating KGs to increase\nknowledge coverage and quality. Recent years have witnessed a rapid increase of\nEA frameworks. However, state-of-the-art solutions tend to rely on labeled data\nfor model training. Additionally, they work under the closed-domain setting and\ncannot deal with entities that are unmatchable. To address these deficiencies,\nwe offer an unsupervised framework that performs entity alignment in the open\nworld. Specifically, we first mine useful features from the side information of\nKGs. Then, we devise an unmatchable entity prediction module to filter out\nunmatchable entities and produce preliminary alignment results. These\npreliminary results are regarded as the pseudo-labeled data and forwarded to\nthe progressive learning framework to generate structural representations,\nwhich are integrated with the side information to provide a more comprehensive\nview for alignment. Finally, the progressive learning framework gradually\nimproves the quality of structural embeddings and enhances the alignment\nperformance by enriching the pseudo-labeled data with alignment results from\nthe previous round. Our solution does not require labeled data and can\neffectively filter out unmatchable entities. Comprehensive experimental\nevaluations validate its superiority.", "published": "2021-01-26 03:10:24", "link": "http://arxiv.org/abs/2101.10535v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "RESPER: Computationally Modelling Resisting Strategies in Persuasive\n  Conversations", "abstract": "Modelling persuasion strategies as predictors of task outcome has several\nreal-world applications and has received considerable attention from the\ncomputational linguistics community. However, previous research has failed to\naccount for the resisting strategies employed by an individual to foil such\npersuasion attempts. Grounded in prior literature in cognitive and social\npsychology, we propose a generalised framework for identifying resisting\nstrategies in persuasive conversations. We instantiate our framework on two\ndistinct datasets comprising persuasion and negotiation conversations. We also\nleverage a hierarchical sequence-labelling neural architecture to infer the\naforementioned resisting strategies automatically. Our experiments reveal the\nasymmetry of power roles in non-collaborative goal-directed conversations and\nthe benefits accrued from incorporating resisting strategies on the final\nconversation outcome. We also investigate the role of different resisting\nstrategies on the conversation outcome and glean insights that corroborate with\npast findings. We also make the code and the dataset of this work publicly\navailable at https://github.com/americast/resper.", "published": "2021-01-26 03:44:17", "link": "http://arxiv.org/abs/2101.10545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of BERT and ALBERT Sentence Embedding Performance on\n  Downstream NLP Tasks", "abstract": "Contextualized representations from a pre-trained language model are central\nto achieve a high performance on downstream NLP task. The pre-trained BERT and\nA Lite BERT (ALBERT) models can be fine-tuned to give state-ofthe-art results\nin sentence-pair regressions such as semantic textual similarity (STS) and\nnatural language inference (NLI). Although BERT-based models yield the [CLS]\ntoken vector as a reasonable sentence embedding, the search for an optimal\nsentence embedding scheme remains an active research area in computational\nlinguistics. This paper explores on sentence embedding models for BERT and\nALBERT. In particular, we take a modified BERT network with siamese and triplet\nnetwork structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to\ncreate Sentence-ALBERT (SALBERT). We also experiment with an outer CNN\nsentence-embedding network for SBERT and SALBERT. We evaluate performances of\nall sentence-embedding models considered using the STS and NLI datasets. The\nempirical results indicate that our CNN architecture improves ALBERT models\nsubstantially more than BERT models for STS benchmark. Despite significantly\nfewer model parameters, ALBERT sentence embedding is highly competitive to BERT\nin downstream NLP evaluations.", "published": "2021-01-26 09:14:06", "link": "http://arxiv.org/abs/2101.10642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Zero-shot Cross-lingual Transfer in Supervised NLP Tasks", "abstract": "In zero-shot cross-lingual transfer, a supervised NLP task trained on a\ncorpus in one language is directly applicable to another language without any\nadditional training. A source of cross-lingual transfer can be as\nstraightforward as lexical overlap between languages (e.g., use of the same\nscripts, shared subwords) that naturally forces text embeddings to occupy a\nsimilar representation space. Recently introduced cross-lingual language model\n(XLM) pretraining brings out neural parameter sharing in Transformer-style\nnetworks as the most important factor for the transfer. In this paper, we aim\nto validate the hypothetically strong cross-lingual transfer properties induced\nby XLM pretraining. Particularly, we take XLM-RoBERTa (XLMR) in our experiments\nthat extend semantic textual similarity (STS), SQuAD and KorQuAD for machine\nreading comprehension, sentiment analysis, and alignment of sentence embeddings\nunder various cross-lingual settings. Our results indicate that the presence of\ncross-lingual transfer is most pronounced in STS, sentiment analysis the next,\nand MRC the last. That is, the complexity of a downstream task softens the\ndegree of crosslingual transfer. All of our results are empirically observed\nand measured, and we make our code and data publicly available.", "published": "2021-01-26 09:21:25", "link": "http://arxiv.org/abs/2101.10649v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Regulatory Compliance through Doc2Doc Information Retrieval: A case\n  study in EU/UK legislation where text similarity has limitations", "abstract": "Major scandals in corporate history have urged the need for regulatory\ncompliance, where organizations need to ensure that their controls (processes)\ncomply with relevant laws, regulations, and policies. However, keeping track of\nthe constantly changing legislation is difficult, thus organizations are\nincreasingly adopting Regulatory Technology (RegTech) to facilitate the\nprocess. To this end, we introduce regulatory information retrieval (REG-IR),\nan application of document-to-document information retrieval (DOC2DOC IR),\nwhere the query is an entire document making the task more challenging than\ntraditional IR where the queries are short. Furthermore, we compile and release\ntwo datasets based on the relationships between EU directives and UK\nlegislation. We experiment on these datasets using a typical two-step pipeline\napproach comprising a pre-fetcher and a neural re-ranker. Experimenting with\nvarious pre-fetchers from BM25 to k nearest neighbors over representations from\nseveral BERT models, we show that fine-tuning a BERT model on an in-domain\nclassification task produces the best representations for IR. We also show that\nneural re-rankers under-perform due to contradicting supervision, i.e., similar\nquery-document pairs with opposite labels. Thus, they are biased towards the\npre-fetcher's score. Interestingly, applying a date filter further improves the\nperformance, showcasing the importance of the time dimension.", "published": "2021-01-26 11:38:15", "link": "http://arxiv.org/abs/2101.10726v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "I Beg to Differ: A study of constructive disagreement in online\n  conversations", "abstract": "Disagreements are pervasive in human communication. In this paper we\ninvestigate what makes disagreement constructive. To this end, we construct\nWikiDisputes, a corpus of 7 425 Wikipedia Talk page conversations that contain\ncontent disputes, and define the task of predicting whether disagreements will\nbe escalated to mediation by a moderator. We evaluate feature-based models with\nlinguistic markers from previous work, and demonstrate that their performance\nis improved by using features that capture changes in linguistic markers\nthroughout the conversations, as opposed to averaged values. We develop a\nvariety of neural models and show that taking into account the structure of the\nconversation improves predictive accuracy, exceeding that of feature-based\nmodels. We assess our best neural model in terms of both predictive accuracy\nand uncertainty by evaluating its behaviour when it is only exposed to the\nbeginning of the conversation, finding that model accuracy improves and\nuncertainty reduces as models are exposed to more information.", "published": "2021-01-26 16:36:43", "link": "http://arxiv.org/abs/2101.10917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "abstract": "We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.", "published": "2021-01-26 19:18:27", "link": "http://arxiv.org/abs/2101.11038v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-domain Topic Identification of Out-of-domain Utterances using\n  Wikipedia", "abstract": "Users of spoken dialogue systems (SDS) expect high quality interactions\nacross a wide range of diverse topics. However, the implementation of SDS\ncapable of responding to every conceivable user utterance in an informative way\nis a challenging problem. Multi-domain SDS must necessarily identify and deal\nwith out-of-domain (OOD) utterances to generate appropriate responses as users\ndo not always know in advance what domains the SDS can handle. To address this\nproblem, we extend the current state-of-the-art in multi-domain SDS by\nestimating the topic of OOD utterances using external knowledge representation\nfrom Wikipedia. Experimental results on real human-to-human dialogues showed\nthat our approach does not degrade domain prediction performance when compared\nto the base model. But more significantly, our joint training achieves more\naccurate predictions of the nearest Wikipedia article by up to about 30% when\ncompared to the benchmarks.", "published": "2021-01-26 23:46:52", "link": "http://arxiv.org/abs/2101.11134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Medical Segment Coloring of Clinical Notes", "abstract": "This paper proposes a deep learning-based method to identify the segments of\na clinical note corresponding to ICD-9 broad categories which are further\ncolor-coded with respect to 17 ICD-9 categories. The proposed Medical Segment\nColorer (MSC) architecture is a pipeline framework that works in three stages:\n(1) word categorization, (2) phrase allocation, and (3) document\nclassification. MSC uses gated recurrent unit neural networks (GRUs) to map\nfrom an input document to word multi-labels to phrase allocations, and uses\nstatistical median to map phrase allocation to document multi-label. We compute\nvariable length segment coloring from overlapping phrase allocation\nprobabilities. These cross-level bidirectional contextual links identify\nadaptive context and then produce segment coloring. We train and evaluate MSC\nusing the document labeled MIMIC-III clinical notes. Training is conducted\nsolely using document multi-labels without any information on phrases,\nsegments, or words. In addition to coloring a clinical note, MSC generates as\nbyproducts document multi-labeling and word tagging -- creation of ICD9\ncategory keyword lists based on segment coloring. Performance comparison of MSC\nbyproduct document multi-labels versus methods whose purpose is to produce\njustifiable document multi-labels is 64% vs 52.4% micro-average F1-score\nagainst the CAML (CNN attention multi label) method. For evaluation of MSC\nsegment coloring results, medical practitioners independently assigned the\ncolors to broad ICD9 categories given a sample of 40 colored notes and a sample\nof 50 words related to each category based on the word tags. Binary scoring of\nthis evaluation has a median value of 83.3% and mean of 63.7%.", "published": "2021-01-26 09:49:37", "link": "http://arxiv.org/abs/2101.11477v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "On the Evaluation of Vision-and-Language Navigation Instructions", "abstract": "Vision-and-Language Navigation wayfinding agents can be enhanced by\nexploiting automatically generated navigation instructions. However, existing\ninstruction generators have not been comprehensively evaluated, and the\nautomatic evaluation metrics used to develop them have not been validated.\nUsing human wayfinders, we show that these generators perform on par with or\nonly slightly better than a template-based generator and far worse than human\ninstructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are\nineffective for evaluating grounded navigation instructions. To improve\ninstruction evaluation, we propose an instruction-trajectory compatibility\nmodel that operates without reference instructions. Our model shows the highest\ncorrelation with human wayfinding outcomes when scoring individual\ninstructions. For ranking instruction generation systems, if reference\ninstructions are available we recommend using SPICE.", "published": "2021-01-26 01:03:49", "link": "http://arxiv.org/abs/2101.10504v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic\n  Parsing", "abstract": "Being able to parse code-switched (CS) utterances, such as Spanish+English or\nHindi+English, is essential to democratize task-oriented semantic parsing\nsystems for certain locales. In this work, we focus on Spanglish\n(Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances\nalongside their semantic parses. We examine the CS generalizability of various\nCross-lingual (XL) models and exhibit the advantage of pre-trained XL language\nmodels when data for only one language is present. As such, we focus on\nimproving the pre-trained models for the case when only English corpus\nalongside either zero or a few CS training instances are available. We propose\ntwo data augmentation methods for the zero-shot and the few-shot settings:\nfine-tune using translate-and-align and augment using a generation model\nfollowed by match-and-filter. Combining the few-shot setting with the above\nimprovements decreases the initial 30-point accuracy gap between the zero-shot\nand the full-data settings by two thirds.", "published": "2021-01-26 02:40:44", "link": "http://arxiv.org/abs/2101.10524v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Semantic Parsing for New Predicates", "abstract": "In this work, we investigate the problems of semantic parsing in a few-shot\nlearning setting. In this setting, we are provided with utterance-logical form\npairs per new predicate. The state-of-the-art neural semantic parsers achieve\nless than 25% accuracy on benchmark datasets when k= 1. To tackle this problem,\nwe proposed to i) apply a designated meta-learning method to train the model;\nii) regularize attention scores with alignment statistics; iii) apply a\nsmoothing technique in pre-training. As a result, our method consistently\noutperforms all the baselines in both one and two-shot settings.", "published": "2021-01-26 11:08:08", "link": "http://arxiv.org/abs/2101.10708v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summarising Historical Text in Modern Languages", "abstract": "We introduce the task of historical text summarisation, where documents in\nhistorical forms of a language are summarised in the corresponding modern\nlanguage. This is a fundamentally important routine to historians and digital\nhumanities researchers but has never been automated. We compile a high-quality\ngold-standard text summarisation dataset, which consists of historical German\nand Chinese news from hundreds of years ago summarised in modern German or\nChinese. Based on cross-lingual transfer learning techniques, we propose a\nsummarisation model that can be trained even with no cross-lingual (historical\nto modern) parallel data, and further benchmark it against state-of-the-art\nalgorithms. We report automatic and human evaluations that distinguish the\nhistoric to modern language summarisation task from standard cross-lingual\nsummarisation (i.e., modern to modern language), highlight the distinctness and\nvalue of our dataset, and demonstrate that our transfer learning approach\noutperforms standard cross-lingual benchmarks on this task.", "published": "2021-01-26 13:00:07", "link": "http://arxiv.org/abs/2101.10759v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Event-Driven News Stream Clustering using Entity-Aware Contextual\n  Embeddings", "abstract": "We propose a method for online news stream clustering that is a variant of\nthe non-parametric streaming K-means algorithm. Our model uses a combination of\nsparse and dense document representations, aggregates document-cluster\nsimilarity along these multiple representations and makes the clustering\ndecision using a neural classifier. The weighted document-cluster similarity\nmodel is learned using a novel adaptation of the triplet loss into a linear\nclassification objective. We show that the use of a suitable fine-tuning\nobjective and external knowledge in pre-trained transformer models yields\nsignificant improvements in the effectiveness of contextual embeddings for\nclustering. Our model achieves a new state-of-the-art on a standard stream\nclustering dataset of English documents.", "published": "2021-01-26 19:58:30", "link": "http://arxiv.org/abs/2101.11059v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Leveraging End-to-End ASR for Endangered Language Documentation: An\n  Empirical Study on Yolox\u00f3chitl Mixtec", "abstract": "\"Transcription bottlenecks\", created by a shortage of effective human\ntranscribers are one of the main challenges to endangered language (EL)\ndocumentation. Automatic speech recognition (ASR) has been suggested as a tool\nto overcome such bottlenecks. Following this suggestion, we investigated the\neffectiveness for EL documentation of end-to-end ASR, which unlike Hidden\nMarkov Model ASR systems, eschews linguistic resources but is instead more\ndependent on large-data settings. We open source a Yolox\\'ochitl Mixtec EL\ncorpus. First, we review our method in building an end-to-end ASR system in a\nway that would be reproducible by the ASR community. We then propose a novice\ntranscription correction task and demonstrate how ASR systems and novice\ntranscribers can work together to improve EL documentation. We believe this\ncombinatory methodology would mitigate the transcription bottleneck and\ntranscriber shortage that hinders EL documentation.", "published": "2021-01-26 15:35:35", "link": "http://arxiv.org/abs/2101.10877v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised source localization in reverberant environments with\n  deep generative modeling", "abstract": "We propose a semi-supervised approach to acoustic source localization in\nreverberant environments based on deep generative modeling. Localization in\nreverberant environments remains an open challenge. Even with large data\nvolumes, the number of labels available for supervised learning in reverberant\nenvironments is usually small. We address this issue by performing\nsemi-supervised learning (SSL) with convolutional variational autoencoders\n(VAEs) on reverberant speech signals recorded with microphone arrays. The VAE\nis trained to generate the phase of relative transfer functions (RTFs) between\nmicrophones, in parallel with a direction of arrival (DOA) classifier based on\nRTF-phase. These models are trained using both labeled and unlabeled RTF-phase\nsequences. In learning to perform these tasks, the VAE-SSL explicitly learns to\nseparate the physical causes of the RTF-phase (i.e., source location) from\ndistracting signal characteristics such as noise and speech activity. Relative\nto existing semi-supervised localization methods in acoustics, VAE-SSL is\neffectively an end-to-end processing approach which relies on minimal\npreprocessing of RTF-phase features. As far as we are aware, our paper presents\nthe first approach to modeling the physics of acoustic propagation using deep\ngenerative modeling. The VAE-SSL approach is compared with two signal\nprocessing-based approaches, steered response power with phase transform\n(SRP-PHAT) and MUltiple SIgnal Classification (MUSIC), as well as fully\nsupervised CNNs. We find that VAE-SSL can outperform the conventional\napproaches and the CNN in label-limited scenarios. Further, the trained VAE-SSL\nsystem can generate new RTF-phase samples, which shows the VAE-SSL approach\nlearns the physics of the acoustic environment. The generative modeling in\nVAE-SSL thus provides a means of interpreting the learned representations.", "published": "2021-01-26 08:54:38", "link": "http://arxiv.org/abs/2101.10636v2", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "A Case Study of Deep Learning Based Multi-Modal Methods for Predicting\n  the Age-Suitability Rating of Movie Trailers", "abstract": "In this work, we explore different approaches to combine modalities for the\nproblem of automated age-suitability rating of movie trailers. First, we\nintroduce a new dataset containing videos of movie trailers in English\ndownloaded from IMDB and YouTube, along with their corresponding\nage-suitability rating labels. Secondly, we propose a multi-modal deep learning\npipeline addressing the movie trailer age suitability rating problem. This is\nthe first attempt to combine video, audio, and speech information for this\nproblem, and our experimental results show that multi-modal approaches\nsignificantly outperform the best mono and bimodal models in this task.", "published": "2021-01-26 17:15:35", "link": "http://arxiv.org/abs/2101.11704v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.LG"}
