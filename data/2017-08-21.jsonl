{"title": "The Microsoft 2017 Conversational Speech Recognition System", "abstract": "We describe the 2017 version of Microsoft's conversational speech recognition\nsystem, in which we update our 2016 system with recent developments in\nneural-network-based acoustic and language modeling to further advance the\nstate of the art on the Switchboard speech recognition task. The system adds a\nCNN-BLSTM acoustic model to the set of model architectures we combined\npreviously, and includes character-based and dialog session aware LSTM language\nmodels in rescoring. For system combination we adopt a two-stage approach,\nwhereby subsets of acoustic models are first combined at the senone/frame\nlevel, followed by a word-level voting via confusion networks. We also added a\nconfusion network rescoring step after system combination. The resulting system\nyields a 5.1\\% word error rate on the 2000 Switchboard evaluation set.", "published": "2017-08-21 03:17:23", "link": "http://arxiv.org/abs/1708.06073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Information Extraction with Semi-supervised Neural Tagging", "abstract": "This paper addresses the problem of extracting keyphrases from scientific\narticles and categorizing them as corresponding to a task, process, or\nmaterial. We cast the problem as sequence tagging and introduce semi-supervised\nmethods to a neural tagging model, which builds on recent advances in named\nentity recognition. Since annotated training data is scarce in this domain, we\nintroduce a graph-based semi-supervised algorithm together with a data\nselection scheme to leverage unannotated articles. Both inductive and\ntransductive semi-supervised learning strategies outperform state-of-the-art\ninformation extraction performance on the 2017 SemEval Task 10 ScienceIE task.", "published": "2017-08-21 03:33:58", "link": "http://arxiv.org/abs/1708.06075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator", "abstract": "The paper describes experiments on estimating emotion intensity in tweets\nusing a generalized regressor system. The system combines lexical, syntactic\nand pre-trained word embedding features, trains them on general regressors and\nfinally combines the best performing models to create an ensemble. The proposed\nsystem stood 3rd out of 22 systems in the leaderboard of WASSA-2017 Shared Task\non Emotion Intensity.", "published": "2017-08-21 12:30:48", "link": "http://arxiv.org/abs/1708.06185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cold Fusion: Training Seq2Seq Models Together with Language Models", "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks\nwhich involve generating natural language sentences such as machine\ntranslation, image captioning and speech recognition. Performance has further\nbeen improved by leveraging unlabeled data, often in the form of a language\nmodel. In this work, we present the Cold Fusion method, which leverages a\npre-trained language model during training, and show its effectiveness on the\nspeech recognition task. We show that Seq2Seq models with Cold Fusion are able\nto better utilize language information enjoying i) faster convergence and\nbetter generalization, and ii) almost complete transfer to a new domain while\nusing less than 10% of the labeled training data.", "published": "2017-08-21 21:28:07", "link": "http://arxiv.org/abs/1708.06426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Relation Induction in Vector Space Embeddings", "abstract": "Word embeddings have been found to capture a surprisingly rich amount of\nsyntactic and semantic knowledge. However, it is not yet sufficiently\nwell-understood how the relational knowledge that is implicitly encoded in word\nembeddings can be extracted in a reliable way. In this paper, we propose two\nprobabilistic models to address this issue. The first model is based on the\ncommon relations-as-translations view, but is cast in a probabilistic setting.\nOur second model is based on the much weaker assumption that there is a linear\nrelationship between the vector representations of related words. Compared to\nexisting approaches, our models lead to more accurate predictions, and they are\nmore explicit about what can and cannot be extracted from the word embedding.", "published": "2017-08-21 14:52:10", "link": "http://arxiv.org/abs/1708.06266v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Vector Space Model as Cognitive Space for Text Classification", "abstract": "In this era of digitization, knowing the user's sociolect aspects have become\nessential features to build the user specific recommendation systems. These\nsociolect aspects could be found by mining the user's language sharing in the\nform of text in social media and reviews. This paper describes about the\nexperiment that was performed in PAN Author Profiling 2017 shared task. The\nobjective of the task is to find the sociolect aspects of the users from their\ntweets. The sociolect aspects considered in this experiment are user's gender\nand native language information. Here user's tweets written in a different\nlanguage from their native language are represented as Document - Term Matrix\nwith document frequency as the constraint. Further classification is done using\nthe Support Vector Machine by taking gender and native language as target\nclasses. This experiment attains the average accuracy of 73.42% in gender\nprediction and 76.26% in the native language identification task.", "published": "2017-08-21 03:06:07", "link": "http://arxiv.org/abs/1708.06068v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50"], "primary_category": "cs.CL"}
