{"title": "Towards Annotating and Creating Sub-Sentence Summary Highlights", "abstract": "Highlighting is a powerful tool to pick out important content and emphasize.\nCreating summary highlights at the sub-sentence level is particularly\ndesirable, because sub-sentences are more concise than whole sentences. They\nare also better suited than individual words and phrases that can potentially\nlead to disfluent, fragmented summaries. In this paper we seek to generate\nsummary highlights by annotating summary-worthy sub-sentences and teaching\nclassifiers to do the same. We frame the task as jointly selecting important\nsentences and identifying a single most informative textual unit from each\nsentence. This formulation dramatically reduces the task complexity involved in\nsentence compression. Our study provides new benchmarks and baselines for\ngenerating highlights at the sub-sentence level.", "published": "2019-10-17 00:20:11", "link": "http://arxiv.org/abs/1910.07659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge", "abstract": "We introduce a simple yet effective method of integrating contextual\nembeddings with commonsense graph embeddings, dubbed BERT Infused Graphs:\nMatching Over Other embeDdings. First, we introduce a preprocessing method to\nimprove the speed of querying knowledge bases. Then, we develop a method of\ncreating knowledge embeddings from each knowledge base. We introduce a method\nof aligning tokens between two misaligned tokenization methods. Finally, we\ncontribute a method of contextualizing BERT after combining with knowledge base\nembeddings. We also show BERTs tendency to correct lower accuracy question\ntypes. Our model achieves a higher accuracy than BERT, and we score fifth on\nthe official leaderboard of the shared task and score the highest without any\nadditional language model pretraining.", "published": "2019-10-17 05:19:29", "link": "http://arxiv.org/abs/1910.07713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using a KG-Copy Network for Non-Goal Oriented Dialogues", "abstract": "Non-goal oriented, generative dialogue systems lack the ability to generate\nanswers with grounded facts. A knowledge graph can be considered an abstraction\nof the real world consisting of well-grounded facts. This paper addresses the\nproblem of generating well grounded responses by integrating knowledge graphs\ninto the dialogue systems response generation process, in an end-to-end manner.\nA dataset for nongoal oriented dialogues is proposed in this paper in the\ndomain of soccer, conversing on different clubs and national teams along with a\nknowledge graph for each of these teams. A novel neural network architecture is\nalso proposed as a baseline on this dataset, which can integrate knowledge\ngraphs into the response generation process, producing well articulated,\nknowledge grounded responses. Empirical evidence suggests that the proposed\nmodel performs better than other state-of-the-art models for knowledge graph\nintegrated dialogue systems.", "published": "2019-10-17 11:42:58", "link": "http://arxiv.org/abs/1910.07834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topical Keyphrase Extraction with Hierarchical Semantic Networks", "abstract": "Topical keyphrase extraction is used to summarize large collections of text\ndocuments. However, traditional methods cannot properly reflect the intrinsic\nsemantics and relationships of keyphrases because they rely on a simple\nterm-frequency-based process. Consequently, these methods are not effective in\nobtaining significant contextual knowledge. To resolve this, we propose a\ntopical keyphrase extraction method based on a hierarchical semantic network\nand multiple centrality network measures that together reflect the hierarchical\nsemantics of keyphrases. We conduct experiments on real data to examine the\npracticality of the proposed method and to compare its performance with that of\nexisting topical keyphrase extraction methods. The results confirm that the\nproposed method outperforms state-of-the-art topical keyphrase extraction\nmethods in terms of the representativeness of the selected keyphrases for each\ntopic. The proposed method can effectively reflect intrinsic keyphrase\nsemantics and interrelationships.", "published": "2019-10-17 12:09:11", "link": "http://arxiv.org/abs/1910.07848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LibriVoxDeEn: A Corpus for German-to-English Speech Translation and\n  German Speech Recognition", "abstract": "We present a corpus of sentence-aligned triples of German audio, German text,\nand English translation, based on German audiobooks. The speech translation\ndata consist of 110 hours of audio material aligned to over 50k parallel\nsentences. An even larger dataset comprising 547 hours of German speech aligned\nto German text is available for speech recognition. The audio data is read\nspeech and thus low in disfluencies. The quality of audio and sentence\nalignments has been checked by a manual evaluation, showing that speech\nalignment quality is in general very high. The sentence alignment quality is\ncomparable to well-used parallel translation data and can be adjusted by\ncutoffs on the automatic alignment score. To our knowledge, this corpus is to\ndate the largest resource for German speech recognition and for end-to-end\nGerman-to-English speech translation.", "published": "2019-10-17 14:01:57", "link": "http://arxiv.org/abs/1910.07924v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent\n  Variable", "abstract": "Pre-training models have been proved effective for a wide range of natural\nlanguage processing tasks. Inspired by this, we propose a novel dialogue\ngeneration pre-training framework to support various kinds of conversations,\nincluding chit-chat, knowledge grounded dialogues, and conversational question\nanswering. In this framework, we adopt flexible attention mechanisms to fully\nleverage the bi-directional context and the uni-directional characteristic of\nlanguage generation. We also introduce discrete latent variables to tackle the\ninherent one-to-many mapping problem in response generation. Two reciprocal\ntasks of response generation and latent act recognition are designed and\ncarried out simultaneously within a shared network. Comprehensive experiments\non three publicly available datasets verify the effectiveness and superiority\nof the proposed framework.", "published": "2019-10-17 14:09:42", "link": "http://arxiv.org/abs/1910.07931v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Parsing with Polyglot Training and Multi-treebank\n  Learning: A Faroese Case Study", "abstract": "Cross-lingual dependency parsing involves transferring syntactic knowledge\nfrom one language to another. It is a crucial component for inducing dependency\nparsers in low-resource scenarios where no training data for a language exists.\nUsing Faroese as the target language, we compare two approaches using\nannotation projection: first, projecting from multiple monolingual source\nmodels; second, projecting from a single polyglot model which is trained on the\ncombination of all source languages. Furthermore, we reproduce multi-source\nprojection (Tyers et al., 2018), in which dependency trees of multiple sources\nare combined. Finally, we apply multi-treebank modelling to the projected\ntreebanks, in addition to or alternatively to polyglot modelling on the source\nside. We find that polyglot training on the source languages produces an\noverall trend of better results on the target language but the single best\nresult for the target language is obtained by projecting from monolingual\nsource parsing models and then training multi-treebank POS tagging and parsing\nmodels on the target side.", "published": "2019-10-17 14:28:35", "link": "http://arxiv.org/abs/1910.07938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marpa, A practical general parser: the recognizer", "abstract": "The Marpa recognizer is described. Marpa is a practical and fully implemented\nalgorithm for the recognition, parsing and evaluation of context-free grammars.\nThe Marpa recognizer is the first to unite the improvements to Earley's\nalgorithm found in Joop Leo's 1991 paper to those in Aycock and Horspool's 2002\npaper. Marpa tracks the full state of the parse, at it proceeds, in a form\nconvenient for the application. This greatly improves error detection and\nenables event-driven parsing. One such technique is \"Ruby Slippers\" parsing, in\nwhich the input is altered in response to the parser's expectations.", "published": "2019-10-17 19:45:18", "link": "http://arxiv.org/abs/1910.08129v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Authorship Verification in Social Media via Attention-based\n  Similarity Learning", "abstract": "Authorship verification is the task of analyzing the linguistic patterns of\ntwo or more texts to determine whether they were written by the same author or\nnot. The analysis is traditionally performed by experts who consider linguistic\nfeatures, which include spelling mistakes, grammatical inconsistencies, and\nstylistics for example. Machine learning algorithms, on the other hand, can be\ntrained to accomplish the same, but have traditionally relied on so-called\nstylometric features. The disadvantage of such features is that their\nreliability is greatly diminished for short and topically varied social media\ntexts. In this interdisciplinary work, we propose a substantial extension of a\nrecently published hierarchical Siamese neural network approach, with which it\nis feasible to learn neural features and to visualize the decision-making\nprocess. For this purpose, a new large-scale corpus of short Amazon reviews for\ntext comparison research is compiled and we show that the Siamese network\ntopologies outperform state-of-the-art approaches that were built up on\nstylometric features. Our linguistic analysis of the internal attention weights\nof the network shows that the proposed method is indeed able to latch on to\nsome traditional linguistic categories.", "published": "2019-10-17 20:18:23", "link": "http://arxiv.org/abs/1910.08144v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SetExpan: Corpus-Based Set Expansion via Context Feature Selection and\n  Rank Ensemble", "abstract": "Corpus-based set expansion (i.e., finding the \"complete\" set of entities\nbelonging to the same semantic class, based on a given corpus and a tiny set of\nseeds) is a critical task in knowledge discovery. It may facilitate numerous\ndownstream applications, such as information extraction, taxonomy induction,\nquestion answering, and web search. To discover new entities in an expanded\nset, previous approaches either make one-time entity ranking based on\ndistributional similarity, or resort to iterative pattern-based bootstrapping.\nThe core challenge for these methods is how to deal with noisy context features\nderived from free-text corpora, which may lead to entity intrusion and semantic\ndrifting. In this study, we propose a novel framework, SetExpan, which tackles\nthis problem, with two techniques: (1) a context feature selection method that\nselects clean context features for calculating entity-entity distributional\nsimilarity, and (2) a ranking-based unsupervised ensemble method for expanding\nentity set based on denoised context features. Experiments on three datasets\nshow that SetExpan is robust and outperforms previous state-of-the-art methods\nin terms of mean average precision.", "published": "2019-10-17 22:55:29", "link": "http://arxiv.org/abs/1910.08192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Text Representation from BERT: An Empirical Study", "abstract": "We present a systematic investigation of layer-wise BERT activations for\ngeneral-purpose text representations to understand what linguistic information\nthey capture and how transferable they are across different tasks.\nSentence-level embeddings are evaluated against two state-of-the-art models on\ndownstream and probing tasks from SentEval, while passage-level embeddings are\nevaluated on four question-answering (QA) datasets under a learning-to-rank\nproblem setting. Embeddings from the pre-trained BERT model perform poorly in\nsemantic similarity and sentence surface information probing tasks. Fine-tuning\nBERT on natural language inference data greatly improves the quality of the\nembeddings. Combining embeddings from different BERT layers can further boost\nperformance. BERT embeddings outperform BM25 baseline significantly on factoid\nQA datasets at the passage level, but fail to perform better than BM25 on\nnon-factoid datasets. For all QA datasets, there is a gap between\nembedding-based method and in-domain fine-tuned BERT (we report new\nstate-of-the-art results on two datasets), which suggests deep interactions\nbetween question and answer pairs are critical for those hard tasks.", "published": "2019-10-17 15:33:26", "link": "http://arxiv.org/abs/1910.07973v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HiExpan: Task-Guided Taxonomy Construction by Hierarchical Tree\n  Expansion", "abstract": "Taxonomies are of great value to many knowledge-rich applications. As the\nmanual taxonomy curation costs enormous human effects, automatic taxonomy\nconstruction is in great demand. However, most existing automatic taxonomy\nconstruction methods can only build hypernymy taxonomies wherein each edge is\nlimited to expressing the \"is-a\" relation. Such a restriction limits their\napplicability to more diverse real-world tasks where the parent-child may carry\ndifferent relations. In this paper, we aim to construct a task-guided taxonomy\nfrom a domain-specific corpus and allow users to input a \"seed\" taxonomy,\nserving as the task guidance. We propose an expansion-based taxonomy\nconstruction framework, namely HiExpan, which automatically generates key term\nlist from the corpus and iteratively grows the seed taxonomy. Specifically,\nHiExpan views all children under each taxonomy node forming a coherent set and\nbuilds the taxonomy by recursively expanding all these sets. Furthermore,\nHiExpan incorporates a weakly-supervised relation extraction module to extract\nthe initial children of a newly-expanded node and adjusts the taxonomy tree by\noptimizing its global structure. Our experiments on three real datasets from\ndifferent domains demonstrate the effectiveness of HiExpan for building\ntask-guided taxonomies.", "published": "2019-10-17 23:02:34", "link": "http://arxiv.org/abs/1910.08194v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question Classification with Deep Contextualized Transformer", "abstract": "The latest work for Question and Answer problems is to use the Stanford Parse\nTree. We build on prior work and develop a new method to handle the Question\nand Answer problem with the Deep Contextualized Transformer to manage some\naberrant expressions. We also conduct extensive evaluations of the SQuAD and\nSwDA dataset and show significant improvement over QA problem classification of\nindustry needs. We also investigate the impact of different models for the\naccuracy and efficiency of the problem answers. It shows that our new method is\nmore effective for solving QA problems with higher accuracy", "published": "2019-10-17 23:00:22", "link": "http://arxiv.org/abs/1910.10492v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keyphrase Extraction from Disaster-related Tweets", "abstract": "While keyphrase extraction has received considerable attention in recent\nyears, relatively few studies exist on extracting keyphrases from social media\nplatforms such as Twitter, and even fewer for extracting disaster-related\nkeyphrases from such sources. During a disaster, keyphrases can be extremely\nuseful for filtering relevant tweets that can enhance situational awareness.\nPreviously, joint training of two different layers of a stacked Recurrent\nNeural Network for keyword discovery and keyphrase extraction had been shown to\nbe effective in extracting keyphrases from general Twitter data. We improve the\nmodel's performance on both general Twitter data and disaster-related Twitter\ndata by incorporating contextual word embeddings, POS-tags, phonetics, and\nphonological features. Moreover, we discuss the shortcomings of the often used\nF1-measure for evaluating the quality of predicted keyphrases with respect to\nthe ground truth annotations. Instead of the F1-measure, we propose the use of\nembedding-based metrics to better capture the correctness of the predicted\nkeyphrases. In addition, we also present a novel extension of an\nembedding-based metric. The extension allows one to better control the penalty\nfor the difference in the number of ground-truth and predicted keyphrases", "published": "2019-10-17 13:31:45", "link": "http://arxiv.org/abs/1910.07897v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "H-VECTORS: Utterance-level Speaker Embedding Using A Hierarchical\n  Attention Model", "abstract": "In this paper, a hierarchical attention network to generate utterance-level\nembeddings (H-vectors) for speaker identification is proposed. Since different\nparts of an utterance may have different contributions to speaker identities,\nthe use of hierarchical structure aims to learn speaker related information\nlocally and globally. In the proposed approach, frame-level encoder and\nattention are applied on segments of an input utterance and generate individual\nsegment vectors. Then, segment level attention is applied on the segment\nvectors to construct an utterance representation. To evaluate the effectiveness\nof the proposed approach, NIST SRE 2008 Part1 dataset is used for training, and\ntwo datasets, Switchboard Cellular part1 and CallHome American English Speech,\nare used to evaluate the quality of extracted utterance embeddings on speaker\nidentification and verification tasks. In comparison with two baselines,\nX-vector, X-vector+Attention, the obtained results show that H-vectors can\nachieve a significantly better performance. Furthermore, the extracted\nutterance-level embeddings are more discriminative than the two baselines when\nmapped into a 2D space using t-SNE.", "published": "2019-10-17 13:33:41", "link": "http://arxiv.org/abs/1910.07900v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fully Quantized Transformer for Machine Translation", "abstract": "State-of-the-art neural machine translation methods employ massive amounts of\nparameters. Drastically reducing computational costs of such methods without\naffecting performance has been up to this point unsuccessful. To this end, we\npropose FullyQT: an all-inclusive quantization strategy for the Transformer. To\nthe best of our knowledge, we are the first to show that it is possible to\navoid any loss in translation quality with a fully quantized Transformer.\nIndeed, compared to full-precision, our 8-bit models score greater or equal\nBLEU on most tasks. Comparing ourselves to all previously proposed methods, we\nachieve state-of-the-art quantization results.", "published": "2019-10-17 01:29:12", "link": "http://arxiv.org/abs/1910.10485v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-Talker MVDR Beamforming Based on Extended Complex Gaussian Mixture\n  Model", "abstract": "In this letter, we present a novel multi-talker minimum variance\ndistortionless response (MVDR) beamforming as the front-end of an automatic\nspeech recognition (ASR) system in a dinner party scenario. The CHiME-5 dataset\nis selected to evaluate our proposal for overlapping multi-talker scenario with\nsevere noise. A detailed study on beamforming is conducted based on the\nproposed extended complex Gaussian mixture model (CGMM) integrated with various\nspeech separation and speech enhancement masks. Three main changes are made to\nadopt the original CGMM-based MVDR for the multi-talker scenario. First, the\nnumber of Gaussian distributions is extended to 3 with an additional inference\nspeaker model. Second, the mixture coefficients are introduced as a supervisor\nto generate more elaborate masks and avoid the permutation problems. Moreover,\nwe reorganize the MVDR and mask-based speech separation to achieve both noise\nreduction and target speaker extraction. With the official baseline ASR\nback-end, our front-end algorithm gained an absolute WER reduction of 13.87%\ncompared with the baseline front-end.", "published": "2019-10-17 07:51:36", "link": "http://arxiv.org/abs/1910.07753v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end speech enhancement based on discrete cosine transform", "abstract": "Previous speech enhancement methods focus on estimating the short-time\nspectrum of speech signals due to its short-term stability. However, these\nmethods often only estimate the clean magnitude spectrum and reuse the noisy\nphase when resynthesize speech signals, which is unlikely a valid short-time\nFourier transform (STFT). Recently, DNN based speech enhancement methods mainly\njoint estimation of the magnitude and phase spectrum. These methods usually\ngive better performance than magnitude spectrum estimation but need much larger\ncomputation and memory overhead. In this paper, we propose using the Discrete\nCosine Transform (DCT) to reconstruct a valid short-time spectrum. Under the\nU-net structure, we enhance the real spectrogram and finally achieve perfect\nperformance.", "published": "2019-10-17 11:50:02", "link": "http://arxiv.org/abs/1910.07840v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting Multiple Speech Disfluencies using a Deep Residual Network\n  with Bidirectional Long Short-Term Memory", "abstract": "Stuttering is a speech impediment affecting tens of millions of people on an\neveryday basis. Even with its commonality, there is minimal data and research\non the identification and classification of stuttered speech. This paper\ntackles the problem of detection and classification of different forms of\nstutter. As opposed to most existing works that identify stutters with language\nmodels, our work proposes a model that relies solely on acoustic features,\nallowing for identification of several variations of stutter disfluencies\nwithout the need for speech recognition. Our model uses a deep residual network\nand bidirectional long short-term memory layers to classify different types of\nstutters and achieves an average miss rate of 10.03%, outperforming the\nstate-of-the-art by almost 27%", "published": "2019-10-17 21:32:47", "link": "http://arxiv.org/abs/1910.12590v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
