{"title": "On Learning Universal Representations Across Languages", "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual\npre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual\nNLP tasks. However, existing approaches essentially capture the co-occurrence\namong tokens through involving the masked language model (MLM) objective with\ntoken-level cross entropy. In this work, we extend these approaches to learn\nsentence-level representations and show the effectiveness on cross-lingual\nunderstanding and generation. Specifically, we propose a Hierarchical\nContrastive Learning (HiCTL) method to (1) learn universal representations for\nparallel sentences distributed in one or multiple languages and (2) distinguish\nthe semantically-related words from a shared cross-lingual vocabulary for each\nsentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME\nand machine translation. Experimental results show that the HiCTL outperforms\nthe state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME\nbenchmark as well as achieves substantial improvements on both of the\nhigh-resource and low-resource English-to-X translation tasks over strong\nbaselines.", "published": "2020-07-31 10:58:39", "link": "http://arxiv.org/abs/2007.15960v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimulEval: An Evaluation Toolkit for Simultaneous Translation", "abstract": "Simultaneous translation on both text and speech focuses on a real-time and\nlow-latency scenario where the model starts translating before reading the\ncomplete source input. Evaluating simultaneous translation models is more\ncomplex than offline models because the latency is another factor to consider\nin addition to translation quality. The research community, despite its growing\nfocus on novel modeling approaches to simultaneous translation, currently lacks\na universal evaluation procedure. Therefore, we present SimulEval, an\neasy-to-use and general evaluation toolkit for both simultaneous text and\nspeech translation. A server-client scheme is introduced to create a\nsimultaneous translation scenario, where the server sends source input and\nreceives predictions for evaluation and the client executes customized\npolicies. Given a policy, it automatically performs simultaneous decoding and\ncollectively reports several popular latency metrics. We also adapt latency\nmetrics from text simultaneous translation to the speech task. Additionally,\nSimulEval is equipped with a visualization interface to provide better\nunderstanding of the simultaneous decoding process of a system. SimulEval has\nalready been extensively used for the IWSLT 2020 shared task on simultaneous\nspeech translation. Code will be released upon publication.", "published": "2020-07-31 17:44:41", "link": "http://arxiv.org/abs/2007.16193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "abstract": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.", "published": "2020-07-31 00:04:15", "link": "http://arxiv.org/abs/2007.15779v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving NER's Performance with Massive financial corpus", "abstract": "Training large deep neural networks needs massive high quality annotation\ndata, but the time and labor costs are too expensive for small business. We\nstart a company-name recognition task with a small scale and low quality\ntraining data, then using skills to enhanced model training speed and\npredicting performance with minimum labor cost. The methods we use involve\npre-training a lite language model such as Albert-small or Electra-small in\nfinancial corpus, knowledge of distillation and multi-stage learning. The\nresult is that we raised the recall rate by nearly 20 points and get 4 times as\nfast as BERT-CRF model.", "published": "2020-07-31 07:00:34", "link": "http://arxiv.org/abs/2007.15871v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Automatically Generated Phoneme Captions for Images", "abstract": "Image2Speech is the relatively new task of generating a spoken description of\nan image. This paper presents an investigation into the evaluation of this\ntask. For this, first an Image2Speech system was implemented which generates\nimage captions consisting of phoneme sequences. This system outperformed the\noriginal Image2Speech system on the Flickr8k corpus. Subsequently, these\nphoneme captions were converted into sentences of words. The captions were\nrated by human evaluators for their goodness of describing the image. Finally,\nseveral objective metric scores of the results were correlated with these human\nratings. Although BLEU4 does not perfectly correlate with human ratings, it\nobtained the highest correlation among the investigated metrics, and is the\nbest currently existing metric for the Image2Speech task. Current metrics are\nlimited by the fact that they assume their input to be words. A more\nappropriate metric for the Image2Speech task should assume its input to be\nparts of words, i.e. phonemes, instead.", "published": "2020-07-31 09:21:13", "link": "http://arxiv.org/abs/2007.15916v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Robust Benchmarking for Machine Learning of Clinical Entity Extraction", "abstract": "Clinical studies often require understanding elements of a patient's\nnarrative that exist only in free text clinical notes. To transform notes into\nstructured data for downstream use, these elements are commonly extracted and\nnormalized to medical vocabularies. In this work, we audit the performance of\nand indicate areas of improvement for state-of-the-art systems. We find that\nhigh task accuracies for clinical entity normalization systems on the 2019 n2c2\nShared Task are misleading, and underlying performance is still brittle.\nNormalization accuracy is high for common concepts (95.3%), but much lower for\nconcepts unseen in training data (69.3%). We demonstrate that current\napproaches are hindered in part by inconsistencies in medical vocabularies,\nlimitations of existing labeling schemas, and narrow evaluation techniques. We\nreformulate the annotation framework for clinical entity extraction to factor\nin these issues to allow for robust end-to-end system benchmarking. We evaluate\nconcordance of annotations from our new framework between two annotators and\nachieve a Jaccard similarity of 0.73 for entity recognition and an agreement of\n0.83 for entity normalization. We propose a path forward to address the\ndemonstrated need for the creation of a reference standard to spur method\ndevelopment in entity recognition and normalization.", "published": "2020-07-31 15:14:05", "link": "http://arxiv.org/abs/2007.16127v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TweepFake: about Detecting Deepfake Tweets", "abstract": "The recent advances in language modeling significantly improved the\ngenerative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a\npre-trained language model that can autonomously generate coherent, non-trivial\nand human-like text samples. Since then, ever more powerful text generative\nmodels have been developed. Adversaries can exploit these tremendous generative\ncapabilities to enhance social bots that will have the ability to write\nplausible deepfake messages, hoping to contaminate public debate. To prevent\nthis, it is crucial to develop deepfake social media messages detection\nsystems. However, to the best of our knowledge no one has ever addressed the\ndetection of machine-generated texts on social networks like Twitter or\nFacebook. With the aim of helping the research in this detection field, we\ncollected the first dataset of \\real deepfake tweets, TweepFake. It is real in\nthe sense that each deepfake tweet was actually posted on Twitter. We collected\ntweets from a total of 23 bots, imitating 17 human accounts. The bots are based\non various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM,\nGPT-2. We also randomly selected tweets from the humans imitated by the bots to\nhave an overall balanced dataset of 25,572 tweets (half human and half bots\ngenerated). The dataset is publicly available on Kaggle. Lastly, we evaluated\n13 deepfake text detection methods (based on various state-of-the-art\napproaches) to both demonstrate the challenges that Tweepfake poses and create\na solid baseline of detection techniques. We hope that TweepFake can offer the\nopportunity to tackle the deepfake detection on social media messages as well.", "published": "2020-07-31 19:01:13", "link": "http://arxiv.org/abs/2008.00036v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Text Graph Mining with a Prolog-based Dialog Engine", "abstract": "On top of a neural network-based dependency parser and a graph-based natural\nlanguage processing module we design a Prolog-based dialog engine that explores\ninteractively a ranked fact database extracted from a text document.\n  We reorganize dependency graphs to focus on the most relevant content\nelements of a sentence and integrate sentence identifiers as graph nodes.\n  Additionally, after ranking the graph we take advantage of the implicit\nsemantic information that dependency links and WordNet bring in the form of\nsubject-verb-object, is-a and part-of relations.\n  Working on the Prolog facts and their inferred consequences, the dialog\nengine specializes the text graph with respect to a query and reveals\ninteractively the document's most relevant content elements.\n  The open-source code of the integrated system is available at\nhttps://github.com/ptarau/DeepRank .\n  Under consideration in Theory and Practice of Logic Programming (TPLP).", "published": "2020-07-31 03:29:49", "link": "http://arxiv.org/abs/2008.00956v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Writer Identification Using Microblogging Texts for Social Media\n  Forensics", "abstract": "Establishing authorship of online texts is fundamental to combat cybercrimes.\nUnfortunately, text length is limited on some platforms, making the challenge\nharder. We aim at identifying the authorship of Twitter messages limited to 140\ncharacters. We evaluate popular stylometric features, widely used in literary\nanalysis, and specific Twitter features like URLs, hashtags, replies or quotes.\nWe use two databases with 93 and 3957 authors, respectively. We test varying\nsized author sets and varying amounts of training/test texts per author.\nPerformance is further improved by feature combination via automatic selection.\nWith a large number of training Tweets (>500), a good accuracy (Rank-5>80%) is\nachievable with only a few dozens of test Tweets, even with several thousands\nof authors. With smaller sample sizes (10-20 training Tweets), the search space\ncan be diminished by 9-15% while keeping a high chance that the correct author\nis retrieved among the candidates. In such cases, automatic attribution can\nprovide significant time savings to experts in suspect search. For\ncompleteness, we report verification results. With few training/test Tweets,\nthe EER is above 20-25%, which is reduced to < 15% if hundreds of training\nTweets are available. We also quantify the computational complexity and time\npermanence of the employed features.", "published": "2020-07-31 00:23:18", "link": "http://arxiv.org/abs/2008.01533v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural Language Generation: Formulation, Methods, and Evaluation", "abstract": "Recent advances in neural network-based generative modeling have reignited\nthe hopes in having computer systems capable of seamlessly conversing with\nhumans and able to understand natural language. Neural architectures have been\nemployed to generate text excerpts to various degrees of success, in a\nmultitude of contexts and tasks that fulfil various user needs. Notably, high\ncapacity deep learning models trained on large scale datasets demonstrate\nunparalleled abilities to learn patterns in the data even in the lack of\nexplicit supervision signals, opening up a plethora of new possibilities\nregarding producing realistic and coherent texts. While the field of natural\nlanguage generation is evolving rapidly, there are still many open challenges\nto address. In this survey we formally define and categorize the problem of\nnatural language generation. We review particular application tasks that are\ninstantiations of these general formulations, in which generating natural\nlanguage is of practical importance. Next we include a comprehensive outline of\nmethods and neural architectures employed for generating diverse texts.\nNevertheless, there is no standard way to assess the quality of text produced\nby these generative models, which constitutes a serious bottleneck towards the\nprogress of the field. To this end, we also review current approaches to\nevaluating natural language generation systems. We hope this survey will\nprovide an informative overview of formulations, methods, and assessments of\nneural natural language generation.", "published": "2020-07-31 00:08:28", "link": "http://arxiv.org/abs/2007.15780v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Modelling for Source Code with Transformer-XL", "abstract": "It has been found that software, like natural language texts, exhibits\n\"naturalness\", which can be captured by statistical language models. In recent\nyears, neural language models have been proposed to represent the naturalness\nof software through deep learning. In this paper, we conduct an experimental\nevaluation of state-of-the-art neural language models for source code,\nincluding RNN-based models and Transformer-XL based models. Through experiments\non a large-scale Python code corpus, we find that the Transformer-XL model\noutperforms RNN-based models (including LSTM and GRU models) in capturing the\nnaturalness of software, with far less computational cost.", "published": "2020-07-31 02:42:18", "link": "http://arxiv.org/abs/2007.15813v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Explainable Prediction of Text Complexity: The Missing Preliminaries for\n  Text Simplification", "abstract": "Text simplification reduces the language complexity of professional content\nfor accessibility purposes. End-to-end neural network models have been widely\nadopted to directly generate the simplified version of input text, usually\nfunctioning as a blackbox. We show that text simplification can be decomposed\ninto a compact pipeline of tasks to ensure the transparency and explainability\nof the process. The first two steps in this pipeline are often neglected: 1) to\npredict whether a given piece of text needs to be simplified, and 2) if yes, to\nidentify complex parts of the text. The two tasks can be solved separately\nusing either lexical or deep learning methods, or solved jointly. Simply\napplying explainable complexity prediction as a preliminary step, the\nout-of-sample text simplification performance of the state-of-the-art,\nblack-box simplification models can be improved by a large margin.", "published": "2020-07-31 03:33:37", "link": "http://arxiv.org/abs/2007.15823v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Utterance-Wise Meeting Transcription System Using Asynchronous\n  Distributed Microphones", "abstract": "A novel framework for meeting transcription using asynchronous microphones is\nproposed in this paper. It consists of audio synchronization, speaker\ndiarization, utterance-wise speech enhancement using guided source separation,\nautomatic speech recognition, and duplication reduction. Doing speaker\ndiarization before speech enhancement enables the system to deal with\noverlapped speech without considering sampling frequency mismatch between\nmicrophones. Evaluation on our real meeting datasets showed that our framework\nachieved a character error rate (CER) of 28.7 % by using 11 distributed\nmicrophones, while a monaural microphone placed on the center of the table had\na CER of 38.2 %. We also showed that our framework achieved CER of 21.8 %,\nwhich is only 2.1 percentage points higher than the CER in headset\nmicrophone-based transcription.", "published": "2020-07-31 06:50:04", "link": "http://arxiv.org/abs/2007.15868v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Paying Per-label Attention for Multi-label Extraction from Radiology\n  Reports", "abstract": "Training medical image analysis models requires large amounts of expertly\nannotated data which is time-consuming and expensive to obtain. Images are\noften accompanied by free-text radiology reports which are a rich source of\ninformation. In this paper, we tackle the automated extraction of structured\nlabels from head CT reports for imaging of suspected stroke patients, using\ndeep learning. Firstly, we propose a set of 31 labels which correspond to\nradiographic findings (e.g. hyperdensity) and clinical impressions (e.g.\nhaemorrhage) related to neurological abnormalities. Secondly, inspired by\nprevious work, we extend existing state-of-the-art neural network models with a\nlabel-dependent attention mechanism. Using this mechanism and simple synthetic\ndata augmentation, we are able to robustly extract many labels with a single\nmodel, classified according to the radiologist's reporting (positive,\nuncertain, negative). This approach can be used in further research to\neffectively extract many labels from medical text.", "published": "2020-07-31 16:11:09", "link": "http://arxiv.org/abs/2007.16152v3", "categories": ["cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis based Multi-person Multi-criteria Decision Making\n  Methodology using Natural Language Processing and Deep Learning for Smarter\n  Decision Aid. Case study of restaurant choice using TripAdvisor reviews", "abstract": "Decision making models are constrained by taking the expert evaluations with\npre-defined numerical or linguistic terms. We claim that the use of sentiment\nanalysis will allow decision making models to consider expert evaluations in\nnatural language. Accordingly, we propose the Sentiment Analysis based\nMulti-person Multi-criteria Decision Making (SA-MpMcDM) methodology for smarter\ndecision aid, which builds the expert evaluations from their natural language\nreviews, and even from their numerical ratings if they are available. The\nSA-MpMcDM methodology incorporates an end-to-end multi-task deep learning model\nfor aspect based sentiment analysis, named DOC-ABSADeepL model, able to\nidentify the aspect categories mentioned in an expert review, and to distill\ntheir opinions and criteria. The individual evaluations are aggregated via the\nprocedure named criteria weighting through the attention of the experts. We\nevaluate the methodology in a case study of restaurant choice using TripAdvisor\nreviews, hence we build, manually annotate, and release the TripR-2020 dataset\nof restaurant reviews. We analyze the SA-MpMcDM methodology in different\nscenarios using and not using natural language and numerical evaluations. The\nanalysis shows that the combination of both sources of information results in a\nhigher quality preference vector.", "published": "2020-07-31 18:45:52", "link": "http://arxiv.org/abs/2008.00032v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backpropagation through Signal Temporal Logic Specifications: Infusing\n  Logical Structure into Gradient-Based Methods", "abstract": "This paper presents a technique, named STLCG, to compute the quantitative\nsemantics of Signal Temporal Logic (STL) formulas using computation graphs.\nSTLCG provides a platform which enables the incorporation of logical\nspecifications into robotics problems that benefit from gradient-based\nsolutions. Specifically, STL is a powerful and expressive formal language that\ncan specify spatial and temporal properties of signals generated by both\ncontinuous and hybrid systems. The quantitative semantics of STL provide a\nrobustness metric, i.e., how much a signal satisfies or violates an STL\nspecification. In this work, we devise a systematic methodology for translating\nSTL robustness formulas into computation graphs. With this representation, and\nby leveraging off-the-shelf automatic differentiation tools, we are able to\nefficiently backpropagate through STL robustness formulas and hence enable a\nnatural and easy-to-use integration of STL specifications with many\ngradient-based approaches used in robotics. Through a number of examples\nstemming from various robotics applications, we demonstrate that STLCG is\nversatile, computationally efficient, and capable of incorporating human-domain\nknowledge into the problem formulation.", "published": "2020-07-31 22:01:39", "link": "http://arxiv.org/abs/2008.00097v3", "categories": ["eess.SY", "cs.CL", "cs.LO", "cs.SY"], "primary_category": "eess.SY"}
{"title": "An Acoustic Segment Model Based Segment Unit Selection Approach to\n  Acoustic Scene Classification with Partial Utterances", "abstract": "In this paper, we propose a sub-utterance unit selection framework to remove\nacoustic segments in audio recordings that carry little information for\nacoustic scene classification (ASC). Our approach is built upon a universal set\nof acoustic segment units covering the overall acoustic scene space. First,\nthose units are modeled with acoustic segment models (ASMs) used to tokenize\nacoustic scene utterances into sequences of acoustic segment units. Next,\nparalleling the idea of stop words in information retrieval, stop ASMs are\nautomatically detected. Finally, acoustic segments associated with the stop\nASMs are blocked, because of their low indexing power in retrieval of most\nacoustic scenes. In contrast to building scene models with whole utterances,\nthe ASM-removed sub-utterances, i.e., acoustic utterances without stop acoustic\nsegments, are then used as inputs to the AlexNet-L back-end for final\nclassification. On the DCASE 2018 dataset, scene classification accuracy\nincreases from 68%, with whole utterances, to 72.1%, with segment selection.\nThis represents a competitive accuracy without any data augmentation, and/or\nensemble strategy. Moreover, our approach compares favourably to AlexNet-L with\nattention.", "published": "2020-07-31 23:01:53", "link": "http://arxiv.org/abs/2008.00107v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Relational Teacher Student Learning with Neural Label Embedding for\n  Device Adaptation in Acoustic Scene Classification", "abstract": "In this paper, we propose a domain adaptation framework to address the device\nmismatch issue in acoustic scene classification leveraging upon neural label\nembedding (NLE) and relational teacher student learning (RTSL). Taking into\naccount the structural relationships between acoustic scene classes, our\nproposed framework captures such relationships which are intrinsically\ndevice-independent. In the training stage, transferable knowledge is condensed\nin NLE from the source domain. Next in the adaptation stage, a novel RTSL\nstrategy is adopted to learn adapted target models without using paired\nsource-target data often required in conventional teacher student learning. The\nproposed framework is evaluated on the DCASE 2018 Task1b data set. Experimental\nresults based on AlexNet-L deep classification models confirm the effectiveness\nof our proposed approach for mismatch situations. NLE-alone adaptation compares\nfavourably with the conventional device adaptation and teacher student based\nadaptation techniques. NLE with RTSL further improves the classification\naccuracy.", "published": "2020-07-31 23:07:20", "link": "http://arxiv.org/abs/2008.00110v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Study on Effects of Implicit and Explicit Language Model Information\n  for DBLSTM-CTC Based Handwriting Recognition", "abstract": "Deep Bidirectional Long Short-Term Memory (D-BLSTM) with a Connectionist\nTemporal Classification (CTC) output layer has been established as one of the\nstate-of-the-art solutions for handwriting recognition. It is well known that\nthe DBLSTM trained by using a CTC objective function will learn both local\ncharacter image dependency for character modeling and long-range contextual\ndependency for implicit language modeling. In this paper, we study the effects\nof implicit and explicit language model information for DBLSTM-CTC based\nhandwriting recognition by comparing the performance of using or without using\nan explicit language model in decoding. It is observed that even using one\nmillion lines of training sentences to train the DBLSTM, using an explicit\nlanguage model is still helpful. To deal with such a large-scale training\nproblem, a GPU-based training tool has been developed for CTC training of\nDBLSTM by using a mini-batch based epochwise Back Propagation Through Time\n(BPTT) algorithm.", "published": "2020-07-31 08:23:37", "link": "http://arxiv.org/abs/2008.01532v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Model Reduction of Shallow CNN Model for Reliable Deployment of\n  Information Extraction from Medical Reports", "abstract": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the\ninformation extraction from cancer pathology reports. Shallow CNN performs\ncompetitively on this task to other deep learning models including BERT, which\nholds the state-of-the-art for many NLP tasks. The main insight behind this\neccentric phenomenon is that the information extraction from cancer pathology\nreports require only a small number of domain-specific text segments to perform\nthe task, thus making the most of the texts and contexts excessive for the\ntask. Shallow CNN model is well-suited to identify these key short text\nsegments from the labeled training set; however, the identified text segments\nremain obscure to humans. In this study, we fill this gap by developing a model\nreduction tool to make a reliable connection between CNN filters and relevant\ntext segments by discarding the spurious connections. We reduce the complexity\nof shallow CNN representation by approximating it with a linear transformation\nof n-gram presence representation with a non-negativity and sparsity prior on\nthe transformation weights to obtain an interpretable model. Our approach\nbridge the gap between the conventionally perceived trade-off boundary between\naccuracy on the one side and explainability on the other by model reduction.", "published": "2020-07-31 16:41:08", "link": "http://arxiv.org/abs/2008.01572v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Future Vector Enhanced LSTM Language Model for LVCSR", "abstract": "Language models (LM) play an important role in large vocabulary continuous\nspeech recognition (LVCSR). However, traditional language models only predict\nnext single word with given history, while the consecutive predictions on a\nsequence of words are usually demanded and useful in LVCSR. The mismatch\nbetween the single word prediction modeling in trained and the long term\nsequence prediction in read demands may lead to the performance degradation. In\nthis paper, a novel enhanced long short-term memory (LSTM) LM using the future\nvector is proposed. In addition to the given history, the rest of the sequence\nwill be also embedded by future vectors. This future vector can be incorporated\nwith the LSTM LM, so it has the ability to model much longer term sequence\nlevel information. Experiments show that, the proposed new LSTM LM gets a\nbetter result on BLEU scores for long term sequence prediction. For the speech\nrecognition rescoring, although the proposed LSTM LM obtains very slight gains,\nthe new model seems obtain the great complementary with the conventional LSTM\nLM. Rescoring using both the new and conventional LSTM LMs can achieve a very\nlarge improvement on the word error rate.", "published": "2020-07-31 08:38:56", "link": "http://arxiv.org/abs/2008.01832v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality\n  Ratings of Real-World Signals", "abstract": "The real-world capabilities of objective speech quality measures are limited\nsince current measures (1) are developed from simulated data that does not\nadequately model real environments; or they (2) predict objective scores that\nare not always strongly correlated with subjective ratings. Additionally, a\nlarge dataset of real-world signals with listener quality ratings does not\ncurrently exist, which would help facilitate real-world assessment. In this\npaper, we collect and predict the perceptual quality of real-world speech\nsignals that are evaluated by human listeners. We first collect a large quality\nrating dataset by conducting crowdsourced listening studies on two real-world\ncorpora. We further develop a novel approach that predicts human quality\nratings using a pyramid bidirectional long short term memory (pBLSTM) network\nwith an attention mechanism. The results show that the proposed model achieves\nstatistically lower estimation errors than prior assessment approaches, where\nthe predicted scores strongly correlate with human judgments.", "published": "2020-07-31 01:46:06", "link": "http://arxiv.org/abs/2007.15797v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Designing Neural Speaker Embeddings with Meta Learning", "abstract": "Neural speaker embeddings trained using classification objectives have\ndemonstrated state-of-the-art performance in multiple applications. Typically,\nsuch embeddings are trained on an out-of-domain corpus on a single task e.g.,\nspeaker classification, albeit with a large number of classes (speakers). In\nthis work, we reformulate embedding training under the meta-learning paradigm.\nWe redistribute the training corpus as an ensemble of multiple related speaker\nclassification tasks, and learn a representation that generalizes better to\nunseen speakers. First, we develop an open source toolkit to train x-vectors\nthat is matched in performance with pre-trained Kaldi models for speaker\ndiarization and speaker verification applications. We find that different\nbottleneck layers in the architecture variedly favor different applications.\nNext, we use two meta-learning strategies, namely prototypical networks and\nrelation networks, to improve over the x-vector embeddings. Our best performing\nmodel achieves a relative improvement of 12.37% and 7.11% in speaker error on\nthe DIHARD II development corpus and the AMI meeting corpus, respectively. We\nanalyze improvements across different domains in the DIHARD corpus. Notably, on\nthe challenging child speech domain, we study the relation between child age\nand the diarization performance. Further, we show reductions in equal error\nrate for speaker verification on the SITW corpus (7.68%) and the VOiCES\nchallenge corpus (8.78%). We observe that meta-learning particularly offers\nbenefits in challenging acoustic conditions and recording setups encountered in\nthese corpora. Our experiments illustrate the applicability of meta-learning as\na generalized learning paradigm for training deep neural speaker embeddings.", "published": "2020-07-31 17:47:36", "link": "http://arxiv.org/abs/2007.16196v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modular End-to-end Automatic Speech Recognition Framework for\n  Acoustic-to-word Model", "abstract": "End-to-end (E2E) systems have played a more and more important role in\nautomatic speech recognition (ASR) and achieved great performance. However, E2E\nsystems recognize output word sequences directly with the input acoustic\nfeature, which can only be trained on limited acoustic data. The extra text\ndata is widely used to improve the results of traditional artificial neural\nnetwork-hidden Markov model (ANN-HMM) hybrid systems. The involving of extra\ntext data to standard E2E ASR systems may break the E2E property during\ndecoding. In this paper, a novel modular E2E ASR system is proposed. The\nmodular E2E ASR system consists of two parts: an acoustic-to-phoneme (A2P)\nmodel and a phoneme-to-word (P2W) model. The A2P model is trained on acoustic\ndata, while extra data including large scale text data can be used to train the\nP2W model. This additional data enables the modular E2E ASR system to model not\nonly the acoustic part but also the language part. During the decoding phase,\nthe two models will be integrated and act as a standard acoustic-to-word (A2W)\nmodel. In other words, the proposed modular E2E ASR system can be easily\ntrained with extra text data and decoded in the same way as a standard E2E ASR\nsystem. Experimental results on the Switchboard corpus show that the modular\nE2E model achieves better word error rate (WER) than standard A2W models.", "published": "2020-07-31 08:47:10", "link": "http://arxiv.org/abs/2008.00953v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diet deep generative audio models with structured lottery", "abstract": "Deep learning models have provided extremely successful solutions in most\naudio application fields. However, the high accuracy of these models comes at\nthe expense of a tremendous computation cost. This aspect is almost always\noverlooked in evaluating the quality of proposed models. However, models should\nnot be evaluated without taking into account their complexity. This aspect is\nespecially critical in audio applications, which heavily relies on specialized\nembedded hardware with real-time constraints. In this paper, we build on recent\nobservations that deep models are highly overparameterized, by studying the\nlottery ticket hypothesis on deep generative audio models. This hypothesis\nstates that extremely efficient small sub-networks exist in deep models and\nwould provide higher accuracy than larger models if trained in isolation.\nHowever, lottery tickets are found by relying on unstructured masking, which\nmeans that resulting models do not provide any gain in either disk size or\ninference time. Instead, we develop here a method aimed at performing\nstructured trimming. We show that this requires to rely on global selection and\nintroduce a specific criterion based on mutual information. First, we confirm\nthe surprising result that smaller models provide higher accuracy than their\nlarge counterparts. We further show that we can remove up to 95% of the model\nweights without significant degradation in accuracy. Hence, we can obtain very\nlight models for generative audio across popular methods such as Wavenet, SING\nor DDSP, that are up to 100 times smaller with commensurate accuracy. We study\nthe theoretical bounds for embedding these models on Raspberry Pi and Arduino,\nand show that we can obtain generative models on CPU with equivalent quality as\nlarge GPU models. Finally, we discuss the possibility of implementing deep\ngenerative audio models on embedded platforms.", "published": "2020-07-31 16:43:10", "link": "http://arxiv.org/abs/2007.16170v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ultra-light deep MIR by trimming lottery tickets", "abstract": "Current state-of-the-art results in Music Information Retrieval are largely\ndominated by deep learning approaches. These provide unprecedented accuracy\nacross all tasks. However, the consistently overlooked downside of these models\nis their stunningly massive complexity, which seems concomitantly crucial to\ntheir success. In this paper, we address this issue by proposing a model\npruning method based on the lottery ticket hypothesis. We modify the original\napproach to allow for explicitly removing parameters, through structured\ntrimming of entire units, instead of simply masking individual weights. This\nleads to models which are effectively lighter in terms of size, memory and\nnumber of operations. We show that our proposal can remove up to 90% of the\nmodel parameters without loss of accuracy, leading to ultra-light deep MIR\nmodels. We confirm the surprising result that, at smaller compression ratios\n(removing up to 85% of a network), lighter models consistently outperform their\nheavier counterparts. We exhibit these results on a large array of MIR tasks\nincluding audio classification, pitch recognition, chord extraction, drum\ntranscription and onset estimation. The resulting ultra-light deep learning\nmodels for MIR can run on CPU, and can even fit on embedded devices with\nminimal degradation of accuracy.", "published": "2020-07-31 17:30:28", "link": "http://arxiv.org/abs/2007.16187v1", "categories": ["cs.LG", "cs.IR", "cs.MM", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
