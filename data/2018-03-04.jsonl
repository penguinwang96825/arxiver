{"title": "CAESAR: Context Awareness Enabled Summary-Attentive Reader", "abstract": "Comprehending meaning from natural language is a primary objective of Natural\nLanguage Processing (NLP), and text comprehension is the cornerstone for\nachieving this objective upon which all other problems like chat bots, language\ntranslation and others can be achieved. We report a Summary-Attentive Reader we\ndesigned to better emulate the human reading process, along with a\ndictiontary-based solution regarding out-of-vocabulary (OOV) words in the data,\nto generate answer based on machine comprehension of reading passages and\nquestion from the SQuAD benchmark. Our implementation of these features with\ntwo popular models (Match LSTM and Dynamic Coattention) was able to reach close\nto matching the results obtained from humans.", "published": "2018-03-04 11:07:55", "link": "http://arxiv.org/abs/1803.01335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concatenated Power Mean Word Embeddings as Universal Cross-Lingual\n  Sentence Representations", "abstract": "Average word embeddings are a common baseline for more sophisticated sentence\nembedding techniques. However, they typically fall short of the performances of\nmore complex models such as InferSent. Here, we generalize the concept of\naverage word embeddings to power mean word embeddings. We show that the\nconcatenation of different types of power mean word embeddings considerably\ncloses the gap to state-of-the-art methods monolingually and substantially\noutperforms these more complex techniques cross-lingually. In addition, our\nproposed method outperforms different recently proposed baselines such as SIF\nand Sent2Vec by a solid margin, thus constituting a much harder-to-beat\nmonolingual baseline. Our data and code are publicly available.", "published": "2018-03-04 18:42:05", "link": "http://arxiv.org/abs/1803.01400v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep-FSMN for Large Vocabulary Continuous Speech Recognition", "abstract": "In this paper, we present an improved feedforward sequential memory networks\n(FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections\nbetween memory blocks in adjacent layers. These skip connections enable the\ninformation flow across different layers and thus alleviate the gradient\nvanishing problem when building very deep structure. As a result, DFSMN\nsignificantly benefits from these skip connections and deep structure. We have\ncompared the performance of DFSMN to BLSTM both with and without lower frame\nrate (LFR) on several large speech recognition tasks, including English and\nMandarin. Experimental results shown that DFSMN can consistently outperform\nBLSTM with dramatic gain, especially trained with LFR using CD-Phone as\nmodeling units. In the 2000 hours Fisher (FSH) task, the proposed DFSMN can\nachieve a word error rate of 9.4% by purely using the cross-entropy criterion\nand decoding with a 3-gram language model, which achieves a 1.5% absolute\nimprovement compared to the BLSTM. In a 20000 hours Mandarin recognition task,\nthe LFR trained DFSMN can achieve more than 20% relative improvement compared\nto the LFR trained BLSTM. Moreover, we can easily design the lookahead filter\norder of the memory blocks in DFSMN to control the latency for real-time\napplications.", "published": "2018-03-04 11:08:16", "link": "http://arxiv.org/abs/1803.05030v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "abstract": "For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .", "published": "2018-03-04 00:20:29", "link": "http://arxiv.org/abs/1803.01271v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multiple Sound Source Localisation with Steered Response Power Density\n  and Hierarchical Grid Refinement", "abstract": "Estimation of the direction-of-arrival (DOA) of sound sources is an important\nstep in sound field analysis. Rigid spherical microphone arrays allow the\ncalculation of a compact spherical harmonic representation of the sound field.\nA basic method for analysing sound fields recorded using such arrays is steered\nresponse power (SRP) maps wherein the source DOA can be estimated as the\nsteering direction that maximises the output power of a maximally-directive\nbeam. This approach is computationally costly since it requires steering the\nbeam in all possible directions. This paper presents an extension to SRP called\nsteered response power density (SRPD) and an associated, signal-adaptive search\nmethod called hierarchical grid refinement (HiGRID) for reducing the number of\nsteering directions needed for DOA estimation. The proposed method can localise\ncoherent as well as incoherent sources while jointly providing the number of\nprominent sources in the scene. It is shown to be robust to reverberation and\nadditive white noise. An evaluation of the proposed method using simulations\nand real recordings under highly reverberant conditions as well as a comparison\nwith state- of-the-art methods are presented.", "published": "2018-03-04 11:32:21", "link": "http://arxiv.org/abs/1803.01339v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
