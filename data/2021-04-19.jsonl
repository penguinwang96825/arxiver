{"title": "Sentiment Classification in Swahili Language Using Multilingual BERT", "abstract": "The evolution of the Internet has increased the amount of information that is\nexpressed by people on different platforms. This information can be product\nreviews, discussions on forums, or social media platforms. Accessibility of\nthese opinions and peoples feelings open the door to opinion mining and\nsentiment analysis. As language and speech technologies become more advanced,\nmany languages have been used and the best models have been obtained. However,\ndue to linguistic diversity and lack of datasets, African languages have been\nleft behind. In this study, by using the current state-of-the-art model,\nmultilingual BERT, we perform sentiment classification on Swahili datasets. The\ndata was created by extracting and annotating 8.2k reviews and comments on\ndifferent social media platforms and the ISEAR emotion dataset. The data were\nclassified as either positive or negative. The model was fine-tuned and achieve\nthe best accuracy of 87.59%.", "published": "2021-04-19 01:47:00", "link": "http://arxiv.org/abs/2104.09006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Production vs Perception: The Role of Individuality in Usage-Based\n  Grammar Induction", "abstract": "This paper asks whether a distinction between production-based and\nperception-based grammar induction influences either (i) the growth curve of\ngrammars and lexicons or (ii) the similarity between representations learned\nfrom independent sub-sets of a corpus. A production-based model is trained on\nthe usage of a single individual, thus simulating the grammatical knowledge of\na single speaker. A perception-based model is trained on an aggregation of many\nindividuals, thus simulating grammatical generalizations learned from exposure\nto many different speakers. To ensure robustness, the experiments are\nreplicated across two registers of written English, with four additional\nregisters reserved as a control. A set of three computational experiments shows\nthat production-based grammars are significantly different from\nperception-based grammars across all conditions, with a steeper growth curve\nthat can be explained by substantial inter-individual grammatical differences.", "published": "2021-04-19 03:38:47", "link": "http://arxiv.org/abs/2104.09033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Unsupervised Semantic Role Labeling", "abstract": "The task of semantic role labeling (SRL) is dedicated to finding the\npredicate-argument structure. Previous works on SRL are mostly supervised and\ndo not consider the difficulty in labeling each example which can be very\nexpensive and time-consuming. In this paper, we present the first neural\nunsupervised model for SRL. To decompose the task as two argument related\nsubtasks, identification and clustering, we propose a pipeline that\ncorrespondingly consists of two neural modules. First, we train a neural model\non two syntax-aware statistically developed rules. The neural model gets the\nrelevance signal for each token in a sentence, to feed into a BiLSTM, and then\nan adversarial layer for noise-adding and classifying simultaneously, thus\nenabling the model to learn the semantic structure of a sentence. Then we\npropose another neural model for argument role clustering, which is done\nthrough clustering the learned argument embeddings biased towards their\ndependency relations. Experiments on CoNLL-2009 English dataset demonstrate\nthat our model outperforms previous state-of-the-art baseline in terms of\nnon-neural models for argument identification and classification.", "published": "2021-04-19 04:50:16", "link": "http://arxiv.org/abs/2104.09047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Faithfulness in Abstractive Summarization with Contrast\n  Candidate Generation and Selection", "abstract": "Despite significant progress in neural abstractive summarization, recent\nstudies have shown that the current models are prone to generating summaries\nthat are unfaithful to the original context. To address the issue, we study\ncontrast candidate generation and selection as a model-agnostic post-processing\ntechnique to correct the extrinsic hallucinations (i.e. information not present\nin the source text) in unfaithful summaries. We learn a discriminative\ncorrection model by generating alternative candidate summaries where named\nentities and quantities in the generated summary are replaced with ones with\ncompatible semantic types from the source document. This model is then used to\nselect the best candidate as the final output summary. Our experiments and\nanalysis across a number of neural summarization systems show that our proposed\nmethod is effective in identifying and correcting extrinsic hallucinations. We\nanalyze the typical hallucination phenomenon by different types of neural\nsummarization systems, in hope to provide insights for future work on the\ndirection.", "published": "2021-04-19 05:39:24", "link": "http://arxiv.org/abs/2104.09061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always Hope in\n  Transformers", "abstract": "In a world filled with serious challenges like climate change, religious and\npolitical conflicts, global pandemics, terrorism, and racial discrimination, an\ninternet full of hate speech, abusive and offensive content is the last thing\nwe desire for. In this paper, we work to identify and promote positive and\nsupportive content on these platforms. We work with several transformer-based\nmodels to classify social media comments as hope speech or not-hope speech in\nEnglish, Malayalam and Tamil languages. This paper portrays our work for the\nShared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at\nLT-EDI 2021- EACL 2021.", "published": "2021-04-19 06:19:13", "link": "http://arxiv.org/abs/2104.09066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UVCE-IIITT@DravidianLangTech-EACL2021: Tamil Troll Meme Classification:\n  You need to Pay more Attention", "abstract": "Tamil is a Dravidian language that is commonly used and spoken in the\nsouthern part of Asia. In the era of social media, memes have been a fun moment\nin the day-to-day life of people. Here, we try to analyze the true meaning of\nTamil memes by categorizing them as troll and non-troll. We propose an\ningenious model comprising of a transformer-transformer architecture that tries\nto attain state-of-the-art by using attention as its main component. The\ndataset consists of troll and non-troll images with their captions as text. The\ntask is a binary classification task. The objective of the model is to pay more\nattention to the extracted features and to ignore the noise in both images and\ntext.", "published": "2021-04-19 06:57:43", "link": "http://arxiv.org/abs/2104.09081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition", "abstract": "Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.", "published": "2021-04-19 07:54:15", "link": "http://arxiv.org/abs/2104.09106v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No comments: Addressing commentary sections in websites' analyses", "abstract": "Removing or extracting the commentary sections from a series of websites is a\ntedious task, as no standard way to code them is widely adopted. This operation\nis thus very rarely performed. In this paper, we show that these commentary\nsections can induce significant biases in the analyses, especially in the case\nof controversial Highlights $\\bullet$ Commentary sections can induce biases in\nthe analysis of websites' contents $\\bullet$ Analyzing these sections can be\ninteresting per se. $\\bullet$ We illustrate these points using a corpus of\nanti-vaccine websites. $\\bullet$ We provide guidelines to remove or extract\nthese sections.", "published": "2021-04-19 08:10:36", "link": "http://arxiv.org/abs/2104.09113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTi\u0107 -- The Transformer Language Model for Bosnian, Croatian,\n  Montenegrin and Serbian", "abstract": "In this paper we describe a transformer model pre-trained on 8 billion tokens\nof crawled text from the Croatian, Bosnian, Serbian and Montenegrin web\ndomains. We evaluate the transformer model on the tasks of part-of-speech\ntagging, named-entity-recognition, geo-location prediction and commonsense\ncausal reasoning, showing improvements on all tasks over state-of-the-art\nmodels. For commonsense reasoning evaluation, we introduce COPA-HR -- a\ntranslation of the Choice of Plausible Alternatives (COPA) dataset into\nCroatian. The BERTi\\'c model is made available for free usage and further\ntask-specific fine-tuning through HuggingFace.", "published": "2021-04-19 12:30:36", "link": "http://arxiv.org/abs/2104.09243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Structure Guided Transformer for Source Code Summarization", "abstract": "Code summaries help developers comprehend programs and reduce their time to\ninfer the program functionalities during software maintenance. Recent efforts\nresort to deep learning techniques such as sequence-to-sequence models for\ngenerating accurate code summaries, among which Transformer-based approaches\nhave achieved promising performance. However, effectively integrating the code\nstructure information into the Transformer is under-explored in this task\ndomain. In this paper, we propose a novel approach named SG-Trans to\nincorporate code structural properties into Transformer. Specifically, we\ninject the local symbolic information (e.g., code tokens and statements) and\nglobal syntactic structure (e.g., data flow graph) into the self-attention\nmodule of Transformer as inductive bias. To further capture the hierarchical\ncharacteristics of code, the local information and global structure are\ndesigned to distribute in the attention heads of lower layers and high layers\nof Transformer. Extensive evaluation shows the superior performance of SG-Trans\nover the state-of-the-art approaches. Compared with the best-performing\nbaseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a\nmetric widely used for measuring generation quality, respectively on two\nbenchmark datasets.", "published": "2021-04-19 14:26:56", "link": "http://arxiv.org/abs/2104.09340v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for Bridging Inference in Transformer Language Models", "abstract": "We probe pre-trained transformer language models for bridging inference. We\nfirst investigate individual attention heads in BERT and observe that attention\nheads at higher layers prominently focus on bridging relations in-comparison\nwith the lower and middle layers, also, few specific attention heads\nconcentrate consistently on bridging. More importantly, we consider language\nmodels as a whole in our second approach where bridging anaphora resolution is\nformulated as a masked token prediction task (Of-Cloze test). Our formulation\nproduces optimistic results without any fine-tuning, which indicates that\npre-trained language models substantially capture bridging inference. Our\nfurther investigation shows that the distance between anaphor-antecedent and\nthe context provided to language models play an important role in the\ninference.", "published": "2021-04-19 15:42:24", "link": "http://arxiv.org/abs/2104.09400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advanced Long-context End-to-end Speech Recognition Using\n  Context-expanded Transformers", "abstract": "This paper addresses end-to-end automatic speech recognition (ASR) for long\naudio recordings such as lecture and conversational speeches. Most end-to-end\nASR models are designed to recognize independent utterances, but contextual\ninformation (e.g., speaker or topic) over multiple utterances is known to be\nuseful for ASR. In our prior work, we proposed a context-expanded Transformer\nthat accepts multiple consecutive utterances at the same time and predicts an\noutput sequence for the last utterance, achieving 5-15% relative error\nreduction from utterance-based baselines in lecture and conversational ASR\nbenchmarks. Although the results have shown remarkable performance gain, there\nis still potential to further improve the model architecture and the decoding\nprocess. In this paper, we extend our prior work by (1) introducing the\nConformer architecture to further improve the accuracy, (2) accelerating the\ndecoding process with a novel activation recycling technique, and (3) enabling\nstreaming decoding with triggered attention. We demonstrate that the extended\nTransformer provides state-of-the-art end-to-end ASR performance, obtaining a\n17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error\nrates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new\ndecoding method reduces decoding time by more than 50% and further enables\nstreaming ASR with limited accuracy degradation.", "published": "2021-04-19 16:18:00", "link": "http://arxiv.org/abs/2104.09426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELECTRAMed: a new pre-trained language representation model for\n  biomedical NLP", "abstract": "The overwhelming amount of biomedical scientific texts calls for the\ndevelopment of effective language models able to tackle a wide range of\nbiomedical natural language processing (NLP) tasks. The most recent dominant\napproaches are domain-specific models, initialized with general-domain textual\ndata and then trained on a variety of scientific corpora. However, it has been\nobserved that for specialized domains in which large corpora exist, training a\nmodel from scratch with just in-domain knowledge may yield better results.\nMoreover, the increasing focus on the compute costs for pre-training recently\nled to the design of more efficient architectures, such as ELECTRA. In this\npaper, we propose a pre-trained domain-specific language model, called\nELECTRAMed, suited for the biomedical field. The novel approach inherits the\nlearning framework of the general-domain ELECTRA architecture, as well as its\ncomputational advantages. Experiments performed on benchmark datasets for\nseveral biomedical NLP tasks support the usefulness of ELECTRAMed, which sets\nthe novel state-of-the-art result on the BC5CDR corpus for named entity\nrecognition, and provides the best outcome in 2 over the 5 runs of the 7th\nBioASQ-factoid Challange for the question answering task.", "published": "2021-04-19 19:38:34", "link": "http://arxiv.org/abs/2104.09585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Refining Targeted Syntactic Evaluation of Language Models", "abstract": "Targeted syntactic evaluation of subject-verb number agreement in English\n(TSE) evaluates language models' syntactic knowledge using hand-crafted minimal\npairs of sentences that differ only in the main verb's conjugation. The method\nevaluates whether language models rate each grammatical sentence as more likely\nthan its ungrammatical counterpart. We identify two distinct goals for TSE.\nFirst, evaluating the systematicity of a language model's syntactic knowledge:\ngiven a sentence, can it conjugate arbitrary verbs correctly? Second,\nevaluating a model's likely behavior: given a sentence, does the model\nconcentrate its probability mass on correctly conjugated verbs, even if only on\na subset of the possible verbs? We argue that current implementations of TSE do\nnot directly capture either of these goals, and propose new metrics to capture\neach goal separately. Under our metrics, we find that TSE overestimates\nsystematicity of language models, but that models score up to 40% better on\nverbs that they predict are likely in context.", "published": "2021-04-19 20:55:13", "link": "http://arxiv.org/abs/2104.09635v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "\"Don't quote me on that\": Finding Mixtures of Sources in News Articles", "abstract": "Journalists publish statements provided by people, or \\textit{sources} to\ncontextualize current events, help voters make informed decisions, and hold\npowerful individuals accountable. In this work, we construct an ontological\nlabeling system for sources based on each source's \\textit{affiliation} and\n\\textit{role}. We build a probabilistic model to infer these attributes for\nnamed sources and to describe news articles as mixtures of these sources. Our\nmodel outperforms existing mixture modeling and co-clustering approaches and\ncorrectly infers source-type in 80\\% of expert-evaluated trials. Such work can\nfacilitate research in downstream tasks like opinion and argumentation mining,\nrepresenting a first step towards machine-in-the-loop \\textit{computational\njournalism} systems.", "published": "2021-04-19 21:57:11", "link": "http://arxiv.org/abs/2104.09656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When FastText Pays Attention: Efficient Estimation of Word\n  Representations using Constrained Positional Weighting", "abstract": "In 2018, Mikolov et al. introduced the positional language model, which has\ncharacteristics of attention-based neural machine translation models and which\nachieved state-of-the-art performance on the intrinsic word analogy task.\nHowever, the positional model is not practically fast and it has never been\nevaluated on qualitative criteria or extrinsic tasks. We propose a constrained\npositional model, which adapts the sparse attention mechanism from neural\nmachine translation to improve the speed of the positional model. We evaluate\nthe positional and constrained positional models on three novel qualitative\ncriteria and on language modeling. We show that the positional and constrained\npositional models contain interpretable information about the grammatical\nproperties of words and outperform other shallow models on language modeling.\nWe also show that our constrained model outperforms the positional model on\nlanguage modeling and trains twice as fast.", "published": "2021-04-19 23:52:19", "link": "http://arxiv.org/abs/2104.09691v6", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with\n  Assembly Models", "abstract": "This paper describes a system submitted by team BigGreen to LCP 2021 for\npredicting the lexical complexity of English words in a given context. We\nassemble a feature engineering-based model with a deep neural network model\nfounded on BERT. While BERT itself performs competitively, our feature\nengineering-based model helps in extreme cases, eg. separating instances of\neasy and neutral difficulty. Our handcrafted features comprise a breadth of\nlexical, semantic, syntactic, and novel phonological measures. Visualizations\nof BERT attention maps offer insight into potential features that Transformers\nmodels may learn when fine-tuned for lexical complexity prediction. Our\nensembled predictions score reasonably well for the single word subtask, and we\ndemonstrate how they can be harnessed to perform well on the multi word\nexpression subtask too.", "published": "2021-04-19 04:05:50", "link": "http://arxiv.org/abs/2104.09040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scattered Factor Universality -- The Power of the Remainder", "abstract": "Scattered factor (circular) universality was firstly introduced by Barker et\nal. in 2020. A word $w$ is called $k$-universal for some natural number $k$, if\nevery word of length $k$ of $w$'s alphabet occurs as a scattered factor in $w$;\nit is called circular $k$-universal if a conjugate of $w$ is $k$-universal.\nHere, a word $u=u_1\\cdots u_n$ is called a scattered factor of $w$ if $u$ is\nobtained from $w$ by deleting parts of $w$, i.e. there exists (possibly empty)\nwords $v_1,\\dots,v_{n+1}$ with $w=v_1u_1v_2\\cdots v_nu_nv_{n+1}$. In this work,\nwe prove two problems, left open in the aforementioned paper, namely a\ngeneralisation of one of their main theorems to arbitrary alphabets and a\nslight modification of another theorem such that we characterise the circular\nuniversality by the universality. On the way, we present deep insights into the\nbehaviour of the remainder of the so called arch factorisation by Hebrard when\nrepetitions of words are considered.", "published": "2021-04-19 05:45:44", "link": "http://arxiv.org/abs/2104.09063v1", "categories": ["cs.CL", "math.CO", "14J60", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Alexa Conversations: An Extensible Data-driven Approach for Building\n  Task-oriented Dialogue Systems", "abstract": "Traditional goal-oriented dialogue systems rely on various components such as\nnatural language understanding, dialogue state tracking, policy learning and\nresponse generation. Training each component requires annotations which are\nhard to obtain for every new domain, limiting scalability of such systems.\nSimilarly, rule-based dialogue systems require extensive writing and\nmaintenance of rules and do not scale either. End-to-End dialogue systems, on\nthe other hand, do not require module-specific annotations but need a large\namount of data for training. To overcome these problems, in this demo, we\npresent Alexa Conversations, a new approach for building goal-oriented dialogue\nsystems that is scalable, extensible as well as data efficient. The components\nof this system are trained in a data-driven manner, but instead of collecting\nannotated conversations for training, we generate them using a novel dialogue\nsimulator based on a few seed dialogues and specifications of APIs and entities\nprovided by the developer. Our approach provides out-of-the-box support for\nnatural conversational phenomena like entity sharing across turns or users\nchanging their mind during conversation without requiring developers to provide\nany such dialogue flows. We exemplify our approach using a simple pizza\nordering task and showcase its value in reducing the developer burden for\ncreating a robust experience. Finally, we evaluate our system using a typical\nmovie ticket booking task and show that the dialogue simulator is an essential\ncomponent of the system that leads to over $50\\%$ improvement in turn-level\naction signature prediction accuracy.", "published": "2021-04-19 07:09:27", "link": "http://arxiv.org/abs/2104.09088v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection", "abstract": "The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.", "published": "2021-04-19 13:07:52", "link": "http://arxiv.org/abs/2104.09261v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Everything Has a Cause: Leveraging Causal Inference in Legal Text\n  Analysis", "abstract": "Causal inference is the process of capturing cause-effect relationship among\nvariables. Most existing works focus on dealing with structured data, while\nmining causal relationship among factors from unstructured data, like text, has\nbeen less examined, but is of great importance, especially in the legal domain.\n  In this paper, we propose a novel Graph-based Causal Inference (GCI)\nframework, which builds causal graphs from fact descriptions without much human\ninvolvement and enables causal inference to facilitate legal practitioners to\nmake proper decisions. We evaluate the framework on a challenging similar\ncharge disambiguation task. Experimental results show that GCI can capture the\nnuance from fact descriptions among multiple confusing charges and provide\nexplainable discrimination, especially in few-shot settings. We also observe\nthat the causal knowledge contained in GCI can be effectively injected into\npowerful neural networks for better performance and interpretability.", "published": "2021-04-19 16:13:10", "link": "http://arxiv.org/abs/2104.09420v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer", "abstract": "Extracting temporal relations (e.g., before, after, and simultaneous) among\nevents is crucial to natural language understanding. One of the key challenges\nof this problem is that when the events of interest are far away in text, the\ncontext in-between often becomes complicated, making it challenging to resolve\nthe temporal relationship between them. This paper thus proposes a new\nSyntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1)\nexplicitly exploiting the connection between two events based on their\ndependency parsing trees, and (2) automatically locating temporal cues between\ntwo events via a novel syntax-guided attention mechanism. Experiments on two\nbenchmark datasets, MATRES and TB-Dense, show that our approach significantly\noutperforms previous state-of-the-art methods on both end-to-end temporal\nrelation extraction and temporal relation classification; This improvement also\nproves to be robust on the contrast set of MATRES. The code is publicly\navailable at https://github.com/VT-NLP/Syntax-Guided-Graph-Transformer.", "published": "2021-04-19 19:00:45", "link": "http://arxiv.org/abs/2104.09570v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Operationalizing a National Digital Library: The Case for a Norwegian\n  Transformer Model", "abstract": "In this work, we show the process of building a large-scale training set from\ndigital and digitized collections at a national library. The resulting\nBidirectional Encoder Representations from Transformers (BERT)-based language\nmodel for Norwegian outperforms multilingual BERT (mBERT) models in several\ntoken and sequence classification tasks for both Norwegian Bokm{\\aa}l and\nNorwegian Nynorsk. Our model also improves the mBERT performance for other\nlanguages present in the corpus such as English, Swedish, and Danish. For\nlanguages not included in the corpus, the weights degrade moderately while\nkeeping strong multilingual properties. Therefore, we show that building\nhigh-quality models within a memory institution using somewhat noisy optical\ncharacter recognition (OCR) content is feasible, and we hope to pave the way\nfor other memory institutions to follow.", "published": "2021-04-19 20:36:24", "link": "http://arxiv.org/abs/2104.09617v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "NewsEdits: A Dataset of Revision Histories for News Articles (Technical\n  Report: Data Processing)", "abstract": "News article revision histories have the potential to give us novel insights\nacross varied fields of linguistics and social sciences. In this work, we\npresent, to our knowledge, the first publicly available dataset of news article\nrevision histories, or NewsEdits.\n  Our dataset is multilingual; it contains 1,278,804 articles with 4,609,430\nversions from over 22 English- and French-language newspaper sources based in\nthree countries. Across version pairs, we count 10.9 million added sentences;\n8.9 million changed sentences and 6.8 million removed sentences. Within the\nchanged sentences, we derive 72 million atomic edits. NewsEdits is, to our\nknowledge, the largest corpus of revision histories of any domain.", "published": "2021-04-19 21:15:30", "link": "http://arxiv.org/abs/2104.09647v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Modeling \"Newsworthiness\" for Lead-Generation Across Corpora", "abstract": "Journalists obtain \"leads\", or story ideas, by reading large corpora of\ngovernment records: court cases, proposed bills, etc. However, only a small\npercentage of such records are interesting documents. We propose a model of\n\"newsworthiness\" aimed at surfacing interesting documents. We train models on\nautomatically labeled corpora -- published newspaper articles -- to predict\nwhether each article was a front-page article (i.e., \\textbf{newsworthy}) or\nnot (i.e., \\textbf{less newsworthy}). We transfer these models to unlabeled\ncorpora -- court cases, bills, city-council meeting minutes -- to rank\ndocuments in these corpora on \"newsworthiness\". A fine-tuned RoBERTa model\nachieves .93 AUC performance on heldout labeled documents, and .88 AUC on\nexpert-validated unlabeled corpora. We provide interpretation and visualization\nfor our models.", "published": "2021-04-19 21:48:15", "link": "http://arxiv.org/abs/2104.09653v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation Using Link Grammar for General\n  Conversational Intelligence", "abstract": "Many current artificial general intelligence (AGI) and natural language\nprocessing (NLP) architectures do not possess general conversational\nintelligence--that is, they either do not deal with language or are unable to\nconvey knowledge in a form similar to the human language without manual,\nlabor-intensive methods such as template-based customization. In this paper, we\npropose a new technique to automatically generate grammatically valid sentences\nusing the Link Grammar database. This natural language generation method far\noutperforms current state-of-the-art baselines and may serve as the final\ncomponent in a proto-AGI question answering pipeline that understandably\nhandles natural language material.", "published": "2021-04-19 06:16:07", "link": "http://arxiv.org/abs/2105.00830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PyPlutchik: visualising and comparing emotion-annotated corpora", "abstract": "The increasing availability of textual corpora and data fetched from social\nnetworks is fuelling a huge production of works based on the model proposed by\npsychologist Robert Plutchik, often referred simply as the ``Plutchik Wheel''.\nRelated researches range from annotation tasks description to emotions\ndetection tools. Visualisation of such emotions is traditionally carried out\nusing the most popular layouts, as bar plots or tables, which are however\nsub-optimal. The classic representation of the Plutchik's wheel follows the\nprinciples of proximity and opposition between pairs of emotions: spatial\nproximity in this model is also a semantic proximity, as adjacent emotions\nelicit a complex emotion (a primary dyad) when triggered together; spatial\nopposition is a semantic opposition as well, as positive emotions are opposite\nto negative emotions. The most common layouts fail to preserve both features,\nnot to mention the need of visually allowing comparisons between different\ncorpora in a blink of an eye, that is hard with basic design solutions. We\nintroduce PyPlutchik, a Python library specifically designed for the\nvisualisation of Plutchik's emotions in texts or in corpora. PyPlutchik draws\nthe Plutchik's flower with each emotion petal sized after how much that emotion\nis detected or annotated in the corpus, also representing three degrees of\nintensity for each of them. Notably, PyPlutchik allows users to display also\nprimary, secondary, tertiary and opposite dyads in a compact, intuitive way. We\nsubstantiate our claim that PyPlutchik outperforms other classic visualisations\nwhen displaying Plutchik emotions and we showcase a few examples that display\nour library's most compelling features.", "published": "2021-04-19 19:34:44", "link": "http://arxiv.org/abs/2105.04295v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Few-shot Learning for Topic Modeling", "abstract": "Topic models have been successfully used for analyzing text documents.\nHowever, with existing topic models, many documents are required for training.\nIn this paper, we propose a neural network-based few-shot learning method that\ncan learn a topic model from just a few documents. The neural networks in our\nmodel take a small number of documents as inputs, and output topic model\npriors. The proposed method trains the neural networks such that the expected\ntest likelihood is improved when topic model parameters are estimated by\nmaximizing the posterior probability using the priors based on the EM\nalgorithm. Since each step in the EM algorithm is differentiable, the proposed\nmethod can backpropagate the loss through the EM algorithm to train the neural\nnetworks. The expected test likelihood is maximized by a stochastic gradient\ndescent method using a set of multiple text corpora with an episodic training\nframework. In our experiments, we demonstrate that the proposed method achieves\nbetter perplexity than existing methods using three real-world text document\nsets.", "published": "2021-04-19 01:56:48", "link": "http://arxiv.org/abs/2104.09011v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Interpreting intermediate convolutional layers of generative CNNs\n  trained on waveforms", "abstract": "This paper presents a technique to interpret and visualize intermediate\nlayers in generative CNNs trained on raw speech data in an unsupervised manner.\nWe argue that averaging over feature maps after ReLU activation in each\ntranspose convolutional layer yields interpretable time-series data. This\ntechnique allows for acoustic analysis of intermediate layers that parallels\nthe acoustic analysis of human speech data: we can extract F0, intensity,\nduration, formants, and other acoustic properties from intermediate layers in\norder to test where and how CNNs encode various types of information. We\nfurther combine this technique with linear interpolation of a model's latent\nspace to show a causal relationship between individual variables in the latent\nspace and activations in a model's intermediate convolutional layers. In\nparticular, observing the causal effect between linear interpolation and the\nresulting changes in intermediate layers can reveal how individual latent\nvariables get transformed into spikes in activation in intermediate layers. We\ntrain and probe internal representations of two models -- a bare WaveGAN\narchitecture and a ciwGAN extension which forces the Generator to output\ninformative data and results in the emergence of linguistically meaningful\nrepresentations. Interpretation and visualization is performed for three basic\nacoustic properties of speech: periodic vibration (corresponding to vowels),\naperiodic noise vibration (corresponding to fricatives), and silence\n(corresponding to stops). The proposal also allows testing of higher-level\nmorphophonological alternations such as reduplication (copying). In short,\nusing the proposed technique, we can analyze how linguistically meaningful\nunits in speech get encoded in each convolutional layer of a generative neural\nnetwork.", "published": "2021-04-19 17:52:06", "link": "http://arxiv.org/abs/2104.09489v4", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Latent Alignments Improve Autoregressive Machine Translation?", "abstract": "Latent alignment objectives such as CTC and AXE significantly improve\nnon-autoregressive machine translation models. Can they improve autoregressive\nmodels as well? We explore the possibility of training autoregressive machine\ntranslation models with latent alignment objectives, and observe that, in\npractice, this approach results in degenerate models. We provide a theoretical\nexplanation for these empirical results, and prove that latent alignment\nobjectives are incompatible with teacher forcing.", "published": "2021-04-19 18:31:56", "link": "http://arxiv.org/abs/2104.09554v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Communicate with Strangers via Channel Randomisation Methods", "abstract": "We introduce two methods for improving the performance of agents meeting for\nthe first time to accomplish a communicative task. The methods are: (1)\n`message mutation' during the generation of the communication protocol; and (2)\nrandom permutations of the communication channel. These proposals are tested\nusing a simple two-player game involving a `teacher' who generates a\ncommunication protocol and sends a message, and a `student' who interprets the\nmessage. After training multiple agents via self-play we analyse the\nperformance of these agents when they are matched with a stranger, i.e. their\nzero-shot communication performance. We find that both message mutation and\nchannel permutation positively influence performance, and we discuss their\neffects.", "published": "2021-04-19 18:42:48", "link": "http://arxiv.org/abs/2104.09557v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Probing Commonsense Explanation in Dialogue Response Generation", "abstract": "Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.", "published": "2021-04-19 19:10:05", "link": "http://arxiv.org/abs/2104.09574v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Modal Alignment in Vision Language Navigation via\n  Syntactic Information", "abstract": "Vision language navigation is the task that requires an agent to navigate\nthrough a 3D environment based on natural language instructions. One key\nchallenge in this task is to ground instructions with the current visual\ninformation that the agent perceives. Most of the existing work employs soft\nattention over individual words to locate the instruction required for the next\naction. However, different words have different functions in a sentence (e.g.,\nmodifiers convey attributes, verbs convey actions). Syntax information like\ndependencies and phrase structures can aid the agent to locate important parts\nof the instruction. Hence, in this paper, we propose a navigation agent that\nutilizes syntax information derived from a dependency tree to enhance alignment\nbetween the instruction and the current visual scenes. Empirically, our agent\noutperforms the baseline model that does not use syntax information on the\nRoom-to-Room dataset, especially in the unseen environment. Besides, our agent\nachieves the new state-of-the-art on Room-Across-Room dataset, which contains\ninstructions in 3 languages (English, Hindi, and Telugu). We also show that our\nagent is better at aligning instructions with the current visual information\nvia qualitative visualizations. Code and models:\nhttps://github.com/jialuli-luka/SyntaxVLN", "published": "2021-04-19 19:18:41", "link": "http://arxiv.org/abs/2104.09580v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neural Language Models with Distant Supervision to Identify Major\n  Depressive Disorder from Clinical Notes", "abstract": "Major depressive disorder (MDD) is a prevalent psychiatric disorder that is\nassociated with significant healthcare burden worldwide. Phenotyping of MDD can\nhelp early diagnosis and consequently may have significant advantages in\npatient management. In prior research MDD phenotypes have been extracted from\nstructured Electronic Health Records (EHR) or using Electroencephalographic\n(EEG) data with traditional machine learning models to predict MDD phenotypes.\nHowever, MDD phenotypic information is also documented in free-text EHR data,\nsuch as clinical notes. While clinical notes may provide more accurate\nphenotyping information, natural language processing (NLP) algorithms must be\ndeveloped to abstract such information. Recent advancements in NLP resulted in\nstate-of-the-art neural language models, such as Bidirectional Encoder\nRepresentations for Transformers (BERT) model, which is a transformer-based\nmodel that can be pre-trained from a corpus of unsupervised text data and then\nfine-tuned on specific tasks. However, such neural language models have been\nunderutilized in clinical NLP tasks due to the lack of large training datasets.\nIn the literature, researchers have utilized the distant supervision paradigm\nto train machine learning models on clinical text classification tasks to\nmitigate the issue of lacking annotated training data. It is still unknown\nwhether the paradigm is effective for neural language models. In this paper, we\npropose to leverage the neural language models in a distant supervision\nparadigm to identify MDD phenotypes from clinical notes. The experimental\nresults indicate that our proposed approach is effective in identifying MDD\nphenotypes and that the Bio- Clinical BERT, a specific BERT model for clinical\ndata, achieved the best performance in comparison with conventional machine\nlearning models.", "published": "2021-04-19 21:11:41", "link": "http://arxiv.org/abs/2104.09644v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "skweak: Weak Supervision Made Easy for NLP", "abstract": "We present skweak, a versatile, Python-based software toolkit enabling NLP\ndevelopers to apply weak supervision to a wide range of NLP tasks. Weak\nsupervision is an emerging machine learning paradigm based on a simple idea:\ninstead of labelling data points by hand, we use labelling functions derived\nfrom domain knowledge to automatically obtain annotations for a given dataset.\nThe resulting labels are then aggregated with a generative model that estimates\nthe accuracy (and possible confusions) of each labelling function. The skweak\ntoolkit makes it easy to implement a large spectrum of labelling functions\n(such as heuristics, gazetteers, neural models or linguistic constraints) on\ntext data, apply them on a corpus, and aggregate their results in a fully\nunsupervised fashion. skweak is especially designed to facilitate the use of\nweak supervision for NLP tasks such as text classification and sequence\nlabelling. We illustrate the use of skweak for NER and sentiment analysis.\nskweak is released under an open-source license and is available at:\nhttps://github.com/NorskRegnesentral/skweak", "published": "2021-04-19 23:26:51", "link": "http://arxiv.org/abs/2104.09683v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Word2Vec: Leveraging Federated Learning to Encourage\n  Collaborative Representation Learning", "abstract": "Large scale contextual representation models have significantly advanced NLP\nin recent years, understanding the semantics of text to a degree never seen\nbefore. However, they need to process large amounts of data to achieve\nhigh-quality results. Joining and accessing all these data from multiple\nsources can be extremely challenging due to privacy and regulatory reasons.\nFederated Learning can solve these limitations by training models in a\ndistributed fashion, taking advantage of the hardware of the devices that\ngenerate the data. We show the viability of training NLP models, specifically\nWord2Vec, with the Federated Learning protocol. In particular, we focus on a\nscenario in which a small number of organizations each hold a relatively large\ncorpus. The results show that neither the quality of the results nor the\nconvergence time in Federated Word2Vec deteriorates as compared to centralised\nWord2Vec.", "published": "2021-04-19 15:39:02", "link": "http://arxiv.org/abs/2105.00831v1", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Stroke Classification of Tabla Accompaniment in Hindustani\n  Vocal Concert Audio", "abstract": "The tabla is a unique percussion instrument due to the combined harmonic and\npercussive nature of its timbre, and the contrasting harmonic frequency ranges\nof its two drums. This allows a tabla player to uniquely emphasize parts of the\nrhythmic cycle (theka) in order to mark the salient positions. An analysis of\nthe loudness dynamics and timing deviations at various cycle positions is an\nimportant part of musicological studies on the expressivity in tabla\naccompaniment. To achieve this at a corpus-level, and not restrict it to the\nfew recordings that manual annotation can afford, it is helpful to have access\nto an automatic tabla transcription system. Although a few systems have been\nbuilt by training models on labeled tabla strokes, the achieved accuracy does\nnot necessarily carry over to unseen instruments. In this article, we report\nour work towards building an instrument-independent stroke classification\nsystem for accompaniment tabla based on the more easily available tabla solo\naudio tracks. We present acoustic features that capture the distinctive\ncharacteristics of tabla strokes and build an automatic system to predict the\nlabel as one of a reduced, but musicologically motivated, target set of four\nstroke categories. To address the lack of sufficient labeled training data, we\nturn to common data augmentation methods and find the use of pitch-shifting\nbased augmentation to be most promising. We then analyse the important features\nand highlight the problem of their instrument-dependence while motivating the\nuse of more task-specific data augmentation strategies to improve the diversity\nof training data.", "published": "2021-04-19 05:56:46", "link": "http://arxiv.org/abs/2104.09064v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Representation Learning With Path Integral Clustering\n  For Speaker Diarization", "abstract": "Automatic speaker diarization techniques typically involve a two-stage\nprocessing approach where audio segments of fixed duration are converted to\nvector representations in the first stage. This is followed by an unsupervised\nclustering of the representations in the second stage. In most of the prior\napproaches, these two stages are performed in an isolated manner with\nindependent optimization steps. In this paper, we propose a representation\nlearning and clustering algorithm that can be iteratively performed for\nimproved speaker diarization. The representation learning is based on\nprinciples of self-supervised learning while the clustering algorithm is a\ngraph structural method based on path integral clustering (PIC). The\nrepresentation learning step uses the cluster targets from PIC and the\nclustering step is performed on embeddings learned from the self-supervised\ndeep model. This iterative approach is referred to as self-supervised\nclustering (SSC). The diarization experiments are performed on CALLHOME and AMI\nmeeting datasets. In these experiments, we show that the SSC algorithm improves\nsignificantly over the baseline system (relative improvements of 13% and 59% on\nCALLHOME and AMI datasets respectively in terms of diarization error rate\n(DER)). In addition, the DER results reported in this work improve over several\nother recent approaches for speaker diarization.", "published": "2021-04-19 17:13:24", "link": "http://arxiv.org/abs/2104.09456v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fusing information streams in end-to-end audio-visual speech recognition", "abstract": "End-to-end acoustic speech recognition has quickly gained widespread\npopularity and shows promising results in many studies. Specifically the joint\ntransformer/CTC model provides very good performance in many tasks. However,\nunder noisy and distorted conditions, the performance still degrades notably.\nWhile audio-visual speech recognition can significantly improve the recognition\nrate of end-to-end models in such poor conditions, it is not obvious how to\nbest utilize any available information on acoustic and visual signal quality\nand reliability in these models. We thus consider the question of how to\noptimally inform the transformer/CTC model of any time-variant reliability of\nthe acoustic and visual information streams. We propose a new fusion strategy,\nincorporating reliability information in a decision fusion net that considers\nthe temporal effects of the attention mechanism. This approach yields\nsignificant improvements compared to a state-of-the-art baseline model on the\nLip Reading Sentences 2 and 3 (LRS2 and LRS3) corpus. On average, the new\nsystem achieves a relative word error rate reduction of 43% compared to the\naudio-only setup and 31% compared to the audiovisual end-to-end baseline.", "published": "2021-04-19 17:42:07", "link": "http://arxiv.org/abs/2104.09482v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Interdisciplinary Review of Music Performance Analysis", "abstract": "A musical performance renders an acoustic realization of a musical score or\nother representation of a composition. Different performances of the same\ncomposition may vary in terms of performance parameters such as timing or\ndynamics, and these variations may have a major impact on how a listener\nperceives the music. The analysis of music performance has traditionally been a\nperipheral topic for the MIR research community, where often a single audio\nrecording is used as representative of a musical work. This paper surveys the\nfield of Music Performance Analysis (MPA) from several perspectives including\nthe measurement of performance parameters, the relation of those parameters to\nthe actions and intentions of a performer or perceptual effects on a listener,\nand finally the assessment of musical performance. This paper also discusses\nMPA as it relates to MIR, pointing out opportunities for collaboration and\nfuture research in both areas.", "published": "2021-04-19 02:21:49", "link": "http://arxiv.org/abs/2104.09018v1", "categories": ["cs.SD", "cs.DL", "eess.AS", "A.1"], "primary_category": "cs.SD"}
{"title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech\n  Quality Prediction with Crowdsourced Datasets", "abstract": "In this paper, we present an update to the NISQA speech quality prediction\nmodel that is focused on distortions that occur in communication networks. In\ncontrast to the previous version, the model is trained end-to-end and the\ntime-dependency modelling and time-pooling is achieved through a Self-Attention\nmechanism. Besides overall speech quality, the model also predicts the four\nspeech quality dimensions Noisiness, Coloration, Discontinuity, and Loudness,\nand in this way gives more insight into the cause of a quality degradation.\nFurthermore, new datasets with over 13,000 speech files were created for\ntraining and validation of the model. The model was finally tested on a new,\nlive-talking test dataset that contains recordings of real telephone calls.\nOverall, NISQA was trained and evaluated on 81 datasets from different sources\nand showed to provide reliable predictions also for unknown speech samples. The\ncode, model weights, and datasets are open-sourced.", "published": "2021-04-19 17:56:59", "link": "http://arxiv.org/abs/2104.09494v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust parameter design for Wiener-based binaural noise reduction\n  methods in hearing aids", "abstract": "This work presents a method for designing the weighting parameter required by\nWiener-based binaural noise reduction methods. This parameter establishes the\ndesired tradeoff between noise reduction and binaural cue preservation in\nhearing aid applications. The proposed strategy was specially derived for the\npreservation of interaural level difference, interaural time difference and\ninteraural coherence binaural cues. It is defined as a function of the average\ninput noise power at the microphones, providing robustness against the\ninfluence of joint changes in noise and speech power (Lombard effect), as well\nas to signal to noise ratio (SNR) variations. A theoretical framework, based on\nthe mathematical definition of the homogeneity degree, is presented and applied\nto a generic augmented Wiener-based cost function. The theoretical insights\nobtained are supported bycomputational simulations and psychoacoustic\nexperiments using the multichannel Wiener filter with interaural transfer\nfunction preservation technique (MWF-ITF), as a case study. Statistical\nanalysis indicates that the proposed dynamic structure for the weighting\nparameter and the design method of its fixed part provide significant\nrobustness against changes in the original binaural cues of both speech and\nresidual noise, at the cost of a small decrease in the noise reduction\nperformance, as compared to the use of a purely fixed weighting parameter.", "published": "2021-04-19 20:36:08", "link": "http://arxiv.org/abs/2104.09615v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A New Class of Efficient Adaptive Filters for Online Nonlinear Modeling", "abstract": "Nonlinear models are known to provide excellent performance in real-world\napplications that often operate in non-ideal conditions. However, such\napplications often require online processing to be performed with limited\ncomputational resources. To address this problem, we propose a new class of\nefficient nonlinear models for online applications. The proposed algorithms are\nbased on linear-in-the-parameters (LIP) nonlinear filters using functional link\nexpansions. In order to make this class of functional link adaptive filters\n(FLAFs) efficient, we propose low-complexity expansions and frequency-domain\nadaptation of the parameters. Among this family of algorithms, we also define\nthe partitioned-block frequency-domain FLAF, whose implementation is\nparticularly suitable for online nonlinear modeling problems. We assess and\ncompare frequency-domain FLAFs with different expansions providing the best\npossible tradeoff between performance and computational complexity.\nExperimental results prove that the proposed algorithms can be considered as an\nefficient and effective solution for online applications, such as the acoustic\necho cancellation, even in the presence of adverse nonlinear conditions and\nwith limited availability of computational resources.", "published": "2021-04-19 21:07:22", "link": "http://arxiv.org/abs/2104.09641v2", "categories": ["cs.LG", "cs.SD", "cs.SY", "eess.AS", "eess.SP", "eess.SY"], "primary_category": "cs.LG"}
