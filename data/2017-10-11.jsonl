{"title": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning", "abstract": "We show how to predict the basic word-order facts of a novel language given\nonly a corpus of part-of-speech (POS) sequences. We predict how often direct\nobjects follow their verbs, how often adjectives follow their nouns, and in\ngeneral the directionalities of all dependency relations. Such typological\nproperties could be helpful in grammar induction. While such a problem is\nusually regarded as unsupervised learning, our innovation is to treat it as\nsupervised learning, using a large collection of realistic synthetic languages\nas training data. The supervised learner must identify surface features of a\nlanguage's POS sequence (hand-engineered or neural features) that correlate\nwith the language's deeper structure (latent trees). In the experiment, we\nshow: 1) Given a small set of real languages, it helps to add many synthetic\nlanguages to the training data. 2) Our system is robust even when the POS\nsequences include noise. 3) Our system on this task outperforms a grammar\ninduction baseline by a large margin.", "published": "2017-10-11 01:47:21", "link": "http://arxiv.org/abs/1710.03877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decision support from financial disclosures with deep neural networks\n  and transfer learning", "abstract": "Company disclosures greatly aid in the process of financial decision-making;\ntherefore, they are consulted by financial investors and automated traders\nbefore exercising ownership in stocks. While humans are usually able to\ncorrectly interpret the content, the same is rarely true of computerized\ndecision support systems, which struggle with the complexity and ambiguity of\nnatural language. A possible remedy is represented by deep learning, which\novercomes several shortcomings of traditional methods of text mining. For\ninstance, recurrent neural networks, such as long short-term memories, employ\nhierarchical structures, together with a large number of hidden layers, to\nautomatically extract features from ordered sequences of words and capture\nhighly non-linear relationships such as context-dependent meanings. However,\ndeep learning has only recently started to receive traction, possibly because\nits performance is largely untested. Hence, this paper studies the use of deep\nneural networks for financial decision support. We additionally experiment with\ntransfer learning, in which we pre-train the network on a different corpus with\na length of 139.1 million words. Our results reveal a higher directional\naccuracy as compared to traditional machine learning when predicting stock\nprice movements in response to financial disclosures. Our work thereby helps to\nhighlight the business value of deep learning and provides recommendations to\npractitioners and executives.", "published": "2017-10-11 08:22:10", "link": "http://arxiv.org/abs/1710.03954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset", "abstract": "We develop a high-quality multi-turn dialog dataset, DailyDialog, which is\nintriguing in several aspects. The language is human-written and less noisy.\nThe dialogues in the dataset reflect our daily communication way and cover\nvarious topics about our daily life. We also manually label the developed\ndataset with communication intention and emotion information. Then, we evaluate\nexisting approaches on DailyDialog dataset and hope it benefit the research\nfield of dialog systems.", "published": "2017-10-11 08:30:30", "link": "http://arxiv.org/abs/1710.03957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Translation Without Parallel Data", "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have\nrelied on bilingual dictionaries or parallel corpora. Recent studies showed\nthat the need for parallel data supervision can be alleviated with\ncharacter-level information. While these methods showed encouraging results,\nthey are not on par with their supervised counterparts and are limited to pairs\nof languages sharing a common alphabet. In this work, we show that we can build\na bilingual dictionary between two languages without using any parallel\ncorpora, by aligning monolingual word embedding spaces in an unsupervised way.\nWithout using any character information, our model even outperforms existing\nsupervised methods on cross-lingual tasks for some language pairs. Our\nexperiments demonstrate that our method works very well also for distant\nlanguage pairs, like English-Russian or English-Chinese. We finally describe\nexperiments on the English-Esperanto low-resource language pair, on which there\nonly exists a limited amount of parallel data, to show the potential impact of\nour method in fully unsupervised machine translation. Our code, embeddings and\ndictionaries are publicly available.", "published": "2017-10-11 14:24:28", "link": "http://arxiv.org/abs/1710.04087v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bollywood Movie Corpus for Text, Images and Videos", "abstract": "In past few years, several data-sets have been released for text and images.\nWe present an approach to create the data-set for use in detecting and removing\ngender bias from text. We also include a set of challenges we have faced while\ncreating this corpora. In this work, we have worked with movie data from\nWikipedia plots and movie trailers from YouTube. Our Bollywood Movie corpus\ncontains 4000 movies extracted from Wikipedia and 880 trailers extracted from\nYouTube which were released from 1970-2017. The corpus contains csv files with\nthe following data about each movie - Wikipedia title of movie, cast, plot\ntext, co-referenced plot text, soundtrack information, link to movie poster,\ncaption of movie poster, number of males in poster, number of females in\nposter. In addition to that, corresponding to each cast member the following\ndata is available - cast name, cast gender, cast verbs, cast adjectives, cast\nrelations, cast centrality, cast mentions. We present some preliminary results\non the task of bias removal which suggest that the data-set is quite useful for\nperforming such tasks.", "published": "2017-10-11 15:51:13", "link": "http://arxiv.org/abs/1710.04142v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Wembedder: Wikidata entity embedding web service", "abstract": "I present a web service for querying an embedding of entities in the Wikidata\nknowledge graph. The embedding is trained on the Wikidata dump using Gensim's\nWord2Vec implementation and a simple graph walk. A REST API is implemented.\nTogether with the Wikidata API the web service exposes a multilingual resource\nfor over 600'000 Wikidata items and properties.", "published": "2017-10-11 14:56:27", "link": "http://arxiv.org/abs/1710.04099v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "I.2.4; H.3.5"], "primary_category": "stat.ML"}
{"title": "Measurement Context Extraction from Text: Discovering Opportunities and\n  Gaps in Earth Science", "abstract": "We propose Marve, a system for extracting measurement values, units, and\nrelated words from natural language text. Marve uses conditional random fields\n(CRF) to identify measurement values and units, followed by a rule-based system\nto find related entities, descriptors and modifiers within a sentence. Sentence\ntokens are represented by an undirected graphical model, and rules are based on\npart-of-speech and word dependency patterns connecting values and units to\ncontextual words. Marve is unique in its focus on measurement context and early\nexperimentation demonstrates Marve's ability to generate high-precision\nextractions with strong recall. We also discuss Marve's role in refining\nmeasurement requirements for NASA's proposed HyspIRI mission, a hyperspectral\ninfrared imaging satellite that will study the world's ecosystems. In general,\nour work with HyspIRI demonstrates the value of semantic measurement\nextractions in characterizing quantitative discussion contained in large\ncorpuses of natural language text. These extractions accelerate broad,\ncross-cutting research and expose scientists new algorithmic approaches and\nexperimental nuances. They also facilitate identification of scientific\nopportunities enabled by HyspIRI leading to more efficient scientific\ninvestment and research.", "published": "2017-10-11 21:37:07", "link": "http://arxiv.org/abs/1710.04312v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PROSE: Perceptual Risk Optimization for Speech Enhancement", "abstract": "The goal in speech enhancement is to obtain an estimate of clean speech\nstarting from the noisy signal by minimizing a chosen distortion measure, which\nresults in an estimate that depends on the unknown clean signal or its\nstatistics. Since access to such prior knowledge is limited or not possible in\npractice, one has to estimate the clean signal statistics. In this paper, we\ndevelop a new risk minimization framework for speech enhancement, in which, one\noptimizes an unbiased estimate of the distortion/risk instead of the actual\nrisk. The estimated risk is expressed solely as a function of the noisy\nobservations. We consider several perceptually relevant distortion measures and\ndevelop corresponding unbiased estimates under realistic assumptions on the\nnoise distribution and a priori signal-to-noise ratio (SNR). Minimizing the\nrisk estimates gives rise to the corresponding denoisers, which are nonlinear\nfunctions of the a posteriori SNR. Perceptual evaluation of speech quality\n(PESQ), average segmental SNR (SSNR) computations, and listening tests show\nthat the proposed risk optimization approach employing Itakura-Saito and\nweighted hyperbolic cosine distortions gives better performance than the other\ndistortion measures. For SNRs greater than 5 dB, the proposed approach gives\nsuperior denoising performance over the benchmark techniques based on the\nWiener filter, log-MMSE minimization, and Bayesian nonnegative matrix\nfactorization.", "published": "2017-10-11 09:31:38", "link": "http://arxiv.org/abs/1710.03975v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pyroomacoustics: A Python package for audio room simulations and array\n  processing algorithms", "abstract": "We present pyroomacoustics, a software package aimed at the rapid development\nand testing of audio array processing algorithms. The content of the package\ncan be divided into three main components: an intuitive Python object-oriented\ninterface to quickly construct different simulation scenarios involving\nmultiple sound sources and microphones in 2D and 3D rooms; a fast C\nimplementation of the image source model for general polyhedral rooms to\nefficiently generate room impulse responses and simulate the propagation\nbetween sources and receivers; and finally, reference implementations of\npopular algorithms for beamforming, direction finding, and adaptive filtering.\nTogether, they form a package with the potential to speed up the time to market\nof new algorithms by significantly reducing the implementation overhead in the\nperformance evaluation step.", "published": "2017-10-11 17:44:41", "link": "http://arxiv.org/abs/1710.04196v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Concept Classification with Hierarchical Deep Neural Networks", "abstract": "Audio-based multimedia retrieval tasks may identify semantic information in\naudio streams, i.e., audio concepts (such as music, laughter, or a revving\nengine). Conventional Gaussian-Mixture-Models have had some success in\nclassifying a reduced set of audio concepts. However, multi-class\nclassification can benefit from context window analysis and the discriminating\npower of deeper architectures. Although deep learning has shown promise in\nvarious applications such as speech and object recognition, it has not yet met\nthe expectations for other fields such as audio concept classification. This\npaper explores, for the first time, the potential of deep learning in\nclassifying audio concepts on User-Generated Content videos. The proposed\nsystem is comprised of two cascaded neural networks in a hierarchical\nconfiguration to analyze the short- and long-term context information. Our\nsystem outperforms a GMM approach by a relative 54%, a Neural Network by 33%,\nand a Deep Neural Network by 12% on the TRECVID-MED database", "published": "2017-10-11 20:07:28", "link": "http://arxiv.org/abs/1710.04288v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
