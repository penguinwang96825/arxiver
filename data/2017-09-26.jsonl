{"title": "Improving a Multi-Source Neural Machine Translation Model with Corpus\n  Extension for Low-Resource Languages", "abstract": "In machine translation, we often try to collect resources to improve\nperformance. However, most of the language pairs, such as Korean-Arabic and\nKorean-Vietnamese, do not have enough resources to train machine translation\nsystems. In this paper, we propose the use of synthetic methods for extending a\nlow-resource corpus and apply it to a multi-source neural machine translation\nmodel. We showed the improvement of machine translation performance through\ncorpus extension using the synthetic method. We specifically focused on how to\ncreate source sentences that can make better target sentences, including the\nuse of synthetic methods. We found that the corpus extension could also improve\nthe performance of multi-source neural machine translation. We showed the\ncorpus extension and multi-source model to be efficient methods for a\nlow-resource language pair. Furthermore, when both methods were used together,\nwe found better machine translation performance.", "published": "2017-09-26 09:04:29", "link": "http://arxiv.org/abs/1709.08898v2", "categories": ["cs.CL", "68U15, 68M20"], "primary_category": "cs.CL"}
{"title": "Input-to-Output Gate to Improve RNN Language Models", "abstract": "This paper proposes a reinforcing method that refines the output layers of\nexisting Recurrent Neural Network (RNN) language models. We refer to our\nproposed method as Input-to-Output Gate (IOG). IOG has an extremely simple\nstructure, and thus, can be easily combined with any RNN language models. Our\nexperiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG\nconsistently boosts the performance of several different types of current\ntopline RNN language models.", "published": "2017-09-26 09:28:49", "link": "http://arxiv.org/abs/1709.08907v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset Construction via Attention for Aspect Term Extraction with\n  Distant Supervision", "abstract": "Aspect Term Extraction (ATE) detects opinionated aspect terms in sentences or\ntext spans, with the end goal of performing aspect-based sentiment analysis.\nThe small amount of available datasets for supervised ATE and the fact that\nthey cover only a few domains raise the need for exploiting other data sources\nin new and creative ways. Publicly available review corpora contain a plethora\nof opinionated aspect terms and cover a larger domain spectrum. In this paper,\nwe first propose a method for using such review corpora for creating a new\ndataset for ATE. Our method relies on an attention mechanism to select\nsentences that have a high likelihood of containing actual opinionated aspects.\nWe thus improve the quality of the extracted aspects. We then use the\nconstructed dataset to train a model and perform ATE with distant supervision.\nBy evaluating on human annotated datasets, we prove that our method achieves a\nsignificantly improved performance over various unsupervised and supervised\nbaselines. Finally, we prove that sentence selection matters when it comes to\ncreating new datasets for ATE. Specifically, we show that, using a set of\nselected sentences leads to higher ATE performance compared to using the whole\nsentence set.", "published": "2017-09-26 18:54:39", "link": "http://arxiv.org/abs/1709.09220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Disease-Gene Associations using Cross-Document Graph-based\n  Features", "abstract": "In the context of personalized medicine, text mining methods pose an\ninteresting option for identifying disease-gene associations, as they can be\nused to generate novel links between diseases and genes which may complement\nknowledge from structured databases. The most straightforward approach to\nextract such links from text is to rely on a simple assumption postulating an\nassociation between all genes and diseases that co-occur within the same\ndocument. However, this approach (i) tends to yield a number of spurious\nassociations, (ii) does not capture different relevant types of associations,\nand (iii) is incapable of aggregating knowledge that is spread across\ndocuments. Thus, we propose an approach in which disease-gene co-occurrences\nand gene-gene interactions are represented in an RDF graph. A machine\nlearning-based classifier is trained that incorporates features extracted from\nthe graph to separate disease-gene pairs into valid disease-gene associations\nand spurious ones. On the manually curated Genetic Testing Registry, our\napproach yields a 30 points increase in F1 score over a plain co-occurrence\nbaseline.", "published": "2017-09-26 19:59:16", "link": "http://arxiv.org/abs/1709.09239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Explain Non-Standard English Words and Phrases", "abstract": "We describe a data-driven approach for automatically explaining new,\nnon-standard English expressions in a given sentence, building on a large\ndataset that includes 15 years of crowdsourced examples from\nUrbanDictionary.com. Unlike prior studies that focus on matching keywords from\na slang dictionary, we investigate the possibility of learning a neural\nsequence-to-sequence model that generates explanations of unseen non-standard\nEnglish expressions given context. We propose a dual encoder approach---a\nword-level encoder learns the representation of context, and a second\ncharacter-level encoder to learn the hidden representation of the target\nnon-standard expression. Our model can produce reasonable definitions of new\nnon-standard English expressions given their context with certain confidence.", "published": "2017-09-26 20:28:18", "link": "http://arxiv.org/abs/1709.09254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polysemy Detection in Distributed Representation of Word Sense", "abstract": "In this paper, we propose a statistical test to determine whether a given\nword is used as a polysemic word or not. The statistic of the word in this test\nroughly corresponds to the fluctuation in the senses of the neighboring words a\nnd the word itself. Even though the sense of a word corresponds to a single\nvector, we discuss how polysemy of the words affects the position of vectors.\nFinally, we also explain the method to detect this effect.", "published": "2017-09-26 06:51:36", "link": "http://arxiv.org/abs/1709.08858v1", "categories": ["cs.DS", "cs.CL"], "primary_category": "cs.DS"}
{"title": "Tensor Product Generation Networks for Deep NLP Modeling", "abstract": "We present a new approach to the design of deep networks for natural language\nprocessing (NLP), based on the general technique of Tensor Product\nRepresentations (TPRs) for encoding and processing symbol structures in\ndistributed neural networks. A network architecture --- the Tensor Product\nGeneration Network (TPGN) --- is proposed which is capable in principle of\ncarrying out TPR computation, but which uses unconstrained deep learning to\ndesign its internal representations. Instantiated in a model for image-caption\ngeneration, TPGN outperforms LSTM baselines when evaluated on the COCO dataset.\nThe TPR-capable structure enables interpretation of internal representations\nand operations, which prove to contain considerable grammatical content. Our\ncaption-generation model can be interpreted as generating sequences of\ngrammatical categories and retrieving words by their categories from a plan\nencoded as a distributed representation.", "published": "2017-09-26 16:32:20", "link": "http://arxiv.org/abs/1709.09118v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Integration of Japanese Papers Into the DBLP Data Set", "abstract": "If someone is looking for a certain publication in the field of computer\nscience, the searching person is likely to use the DBLP to find the desired\npublication. The DBLP data set is continuously extended with new publications,\nor rather their metadata, for example the names of involved authors, the title\nand the publication date. While the size of the data set is already remarkable,\nspecific areas can still be improved. The DBLP offers a huge collection of\nEnglish papers because most papers concerning computer science are published in\nEnglish. Nevertheless, there are official publications in other languages which\nare supposed to be added to the data set. One kind of these are Japanese\npapers. This diploma thesis will show a way to automatically process\npublication lists of Japanese papers and to make them ready for an import into\nthe DBLP data set. Especially important are the problems along the way of\nprocessing, such as transcription handling and Personal Name Matching with\nJapanese names.", "published": "2017-09-26 16:33:59", "link": "http://arxiv.org/abs/1709.09119v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Lexical Disambiguation in Natural Language Questions (NLQs)", "abstract": "Question processing is a fundamental step in a question answering (QA)\napplication, and its quality impacts the performance of QA application. The\nmajor challenging issue in processing question is how to extract semantic of\nnatural language questions (NLQs). A human language is ambiguous. Ambiguity may\noccur at two levels; lexical and syntactic. In this paper, we propose a new\napproach for resolving lexical ambiguity problem by integrating context\nknowledge and concepts knowledge of a domain, into shallow natural language\nprocessing (SNLP) techniques. Concepts knowledge is modeled using ontology,\nwhile context knowledge is obtained from WordNet, and it is determined based on\nneighborhood words in a question. The approach will be applied to a university\nQA system.", "published": "2017-09-26 20:24:10", "link": "http://arxiv.org/abs/1709.09250v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Object-oriented Neural Programming (OONP) for Document Understanding", "abstract": "We propose Object-oriented Neural Programming (OONP), a framework for\nsemantically parsing documents in specific domains. Basically, OONP reads a\ndocument and parses it into a predesigned object-oriented data structure\n(referred to as ontology in this paper) that reflects the domain-specific\nsemantics of the document. An OONP parser models semantic parsing as a decision\nprocess: a neural net-based Reader sequentially goes through the document, and\nduring the process it builds and updates an intermediate ontology to summarize\nits partial understanding of the text it covers. OONP supports a rich family of\noperations (both symbolic and differentiable) for composing the ontology, and a\nbig variety of forms (both symbolic and differentiable) for representing the\nstate and the document. An OONP parser can be trained with supervision of\ndifferent forms and strength, including supervised learning (SL) ,\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\nsynthetic and real-world document parsing tasks have shown that OONP can learn\nto handle fairly complicated ontology with training data of modest sizes.", "published": "2017-09-26 06:17:35", "link": "http://arxiv.org/abs/1709.08853v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Generating Sentences by Editing Prototypes", "abstract": "We propose a new generative model of sentences that first samples a prototype\nsentence from the training corpus and then edits it into a new sentence.\nCompared to traditional models that generate from scratch either left-to-right\nor by first sampling a latent sentence vector, our prototype-then-edit model\nimproves perplexity on language modeling and generates higher quality outputs\naccording to human evaluation. Furthermore, the model gives rise to a latent\nedit vector that captures interpretable semantics such as sentence similarity\nand sentence-level analogies.", "published": "2017-09-26 08:11:33", "link": "http://arxiv.org/abs/1709.08878v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning a Predictive Model for Music Using PULSE", "abstract": "Predictive models for music are studied by researchers of algorithmic\ncomposition, the cognitive sciences and machine learning. They serve as base\nmodels for composition, can simulate human prediction and provide a\nmultidisciplinary application domain for learning algorithms. A particularly\nwell established and constantly advanced subtask is the prediction of\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\nthe recent feature discovery and learning method PULSE to the realm of symbolic\nmusic modeling. PULSE is comprised of a feature generating operation and\nL1-regularized optimization. These are used to iteratively expand and cull the\nfeature set, effectively exploring feature spaces that are too large for common\nfeature selection approaches. I design a general Python framework for PULSE,\npropose task-optimized feature generating operations and various\nmusic-theoretically motivated features that are evaluated on a standard corpus\nof monophonic folk and chorale melodies. The proposed method significantly\noutperforms comparable state-of-the-art models. I further discuss the free\nparameters of the learning algorithm and analyze the feature composition of the\nlearned models. The models learned by PULSE afford an easy inspection and are\nmusicologically interpreted for the first time.", "published": "2017-09-26 05:47:43", "link": "http://arxiv.org/abs/1709.08842v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
