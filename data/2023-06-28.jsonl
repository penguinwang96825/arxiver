{"title": "Sentence-to-Label Generation Framework for Multi-task Learning of\n  Japanese Sentence Classification and Named Entity Recognition", "abstract": "Information extraction(IE) is a crucial subfield within natural language\nprocessing. In this study, we introduce a Sentence Classification and Named\nEntity Recognition Multi-task (SCNM) approach that combines Sentence\nClassification (SC) and Named Entity Recognition (NER). We develop a\nSentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia\ndataset containing both SC and NER. Using a format converter, we unify input\nformats and employ a generative model to generate SC-labels, NER-labels, and\nassociated text segments. We propose a Constraint Mechanism (CM) to improve\ngenerated format accuracy. Our results show SC accuracy increased by 1.13\npoints and NER by 1.06 points in SCNM compared to standalone tasks, with CM\nraising format accuracy from 63.61 to 100. The findings indicate mutual\nreinforcement effects between SC and NER, and integration enhances both tasks'\nperformance.", "published": "2023-06-28 07:29:44", "link": "http://arxiv.org/abs/2306.15978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Spatial-Temporal Variations of Public Discourse on Social\n  Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy", "abstract": "This paper proposes a methodology for exploring how linguistic behaviour on\nsocial media can be used to explore societal reactions to important events such\nas those that transpired during the SARS CoV2 pandemic. In particular, where\nspatial and temporal aspects of events are important features. Our methodology\nconsists of grounding spatial-temporal categories in tweet usage trends using\ntime-series analysis and clustering. Salient terms in each category were then\nidentified through qualitative comparative analysis based on scaled f-scores\naggregated into hand-coded categories. To exemplify this approach, we conducted\na case study on the first wave of the coronavirus in Italy. We used our\nproposed methodology to explore existing psychological observations which\nclaimed that physical distance from events affects what is communicated about\nthem. We confirmed these findings by showing that the epicentre of the disease\nand peripheral regions correspond to clear time-series clusters and that those\nliving in the epicentre of the SARS CoV2 outbreak were more focused on\nsolidarity and policy than those from more peripheral regions. Furthermore, we\nalso found that temporal categories corresponded closely to policy changes\nduring the handling of the pandemic.", "published": "2023-06-28 08:59:50", "link": "http://arxiv.org/abs/2306.16031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge\n  Graph Enhanced Mixture-of-Experts Large Language Model", "abstract": "AI legal assistants based on Large Language Models (LLMs) can provide\naccessible legal consulting services, but the hallucination problem poses\npotential legal risks. This paper presents Chatlaw, an innovative legal\nassistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system\nto enhance the reliability and accuracy of AI-driven legal services. By\nintegrating knowledge graphs with artificial screening, we construct a\nhigh-quality legal dataset to train the MoE model. This model utilizes\ndifferent experts to address various legal issues, optimizing the accuracy of\nlegal responses. Additionally, Standardized Operating Procedures (SOP), modeled\nafter real law firm workflows, significantly reduce errors and hallucinations\nin legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified\nQualification Exam for Legal Professionals by 7.73% in accuracy and 11 points,\nrespectively, and also surpasses other models in multiple dimensions during\nreal-case consultations, demonstrating our robust capability for legal\nconsultation.", "published": "2023-06-28 10:48:34", "link": "http://arxiv.org/abs/2306.16092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance\n  of Current GPT Models in Biomedical Tasks", "abstract": "We assessed the performance of commercial Large Language Models (LLMs)\nGPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b\nPhase B, which is focused on answer generation, both models demonstrated\ncompetitive abilities with leading systems. Remarkably, they achieved this with\nsimple zero-shot learning, grounded with relevant snippets. Even without\nrelevant snippets, their performance was decent, though not on par with the\nbest systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was\nable to compete with GPT-4 in the grounded Q&A setting on factoid and list\nanswers. In Task 11b Phase A, focusing on retrieval, query expansion through\nzero-shot learning improved performance, but the models fell short compared to\nother systems. The code needed to rerun these experiments is available through\nGitHub.", "published": "2023-06-28 11:24:48", "link": "http://arxiv.org/abs/2306.16108v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SkillNet-X: A Multilingual Multitask Model with Sparsely Activated\n  Skills", "abstract": "Traditional multitask learning methods basically can only exploit common\nknowledge in task- or language-wise, which lose either cross-language or\ncross-task knowledge. This paper proposes a general multilingual multitask\nmodel, named SkillNet-X, which enables a single model to tackle many different\ntasks from different languages. To this end, we define several\nlanguage-specific skills and task-specific skills, each of which corresponds to\na skill module. SkillNet-X sparsely activates parts of the skill modules which\nare relevant either to the target task or the target language. Acting as\nknowledge transit hubs, skill modules are capable of absorbing task-related\nknowledge and language-related knowledge consecutively. Based on Transformer,\nwe modify the multi-head attention layer and the feed forward network layer to\naccommodate skill modules. We evaluate SkillNet-X on eleven natural language\nunderstanding datasets in four languages. Results show that SkillNet-X performs\nbetter than task-specific baselines and two multitask learning baselines (i.e.,\ndense joint model and Mixture-of-Experts model). Furthermore, skill\npre-training further improves the performance of SkillNet-X on almost all\ndatasets. To investigate the generalization of our model, we conduct\nexperiments on two new tasks and find that SkillNet-X significantly outperforms\nbaselines.", "published": "2023-06-28 12:53:30", "link": "http://arxiv.org/abs/2306.16176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models", "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nvarious downstream tasks without requiring fine-tuning, including ChatGPT, a\nchat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having\na lower training proportion compared to English, these models also exhibit\nremarkable capabilities in other languages. In this study, we assess the\nperformance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:\nsentiment analysis, translation, transliteration, paraphrasing, part of speech\ntagging, summarization, and diacritization. Our findings reveal that GPT-4\noutperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an\nextensive analysis of the sentiment analysis task, providing insights into how\nLLMs achieve exceptional results on a challenging dialectal dataset.\nAdditionally, we introduce a new Python interface\nhttps://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks\neffortlessly.", "published": "2023-06-28 15:54:29", "link": "http://arxiv.org/abs/2306.16322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confidence-Calibrated Ensemble Dense Phrase Retrieval", "abstract": "In this paper, we consider the extent to which the transformer-based Dense\nPassage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can\nbe optimized without further pre-training. Our method involves two particular\ninsights: we apply the DPR context encoder at various phrase lengths (e.g.\none-sentence versus five-sentence segments), and we take a\nconfidence-calibrated ensemble prediction over all of these different\nsegmentations. This somewhat exhaustive approach achieves start-of-the-art\nresults on benchmark datasets such as Google NQ and SQuAD. We also apply our\nmethod to domain-specific datasets, and the results suggest how different\ngranularities are optimal for different domains", "published": "2023-06-28 04:40:21", "link": "http://arxiv.org/abs/2306.15917v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022\n  Using Twitter and AI", "abstract": "Twitter is a social media platform bridging most countries and allows\nreal-time news discovery. Since the tweets on Twitter are usually short and\nexpress public feelings, thus provide a source for opinion mining and sentiment\nanalysis for global events. This paper proposed an effective solution, in\nproviding a sentiment on tweets related to the FIFA World Cup. At least 130k\ntweets, as the first in the community, are collected and implemented as a\ndataset to evaluate the performance of the proposed machine learning solution.\nThese tweets are collected with the related hashtags and keywords of the Qatar\nWorld Cup 2022. The Vader algorithm is used in this paper for sentiment\nanalysis. Through the machine learning method and collected Twitter tweets, we\ndiscovered the sentiments and fun facts of several aspects important to the\nperiod before the World Cup. The result shows people are positive to the\nopening of the World Cup.", "published": "2023-06-28 09:29:23", "link": "http://arxiv.org/abs/2306.16049v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Framework for Identifying Depression on Social Media:\n  MentalRiskES@IberLEF 2023", "abstract": "This paper describes our participation in the MentalRiskES task at IberLEF\n2023. The task involved predicting the likelihood of an individual experiencing\ndepression based on their social media activity. The dataset consisted of\nconversations from 175 Telegram users, each labeled according to their evidence\nof suffering from the disorder. We used a combination of traditional machine\nlearning and deep learning techniques to solve four predictive subtasks: binary\nclassification, simple regression, multiclass classification, and multi-output\nregression.\n  We approached this by training a model to solve the multi-output regression\ncase and then transforming the predictions to work for the other three\nsubtasks.\n  We compare the performance of two modeling approaches: fine-tuning a\nBERT-based model directly for the task or using its embeddings as inputs to a\nlinear regressor, with the latter yielding better results. The code to\nreproduce our results can be found at:\nhttps://github.com/simonsanvil/EarlyDepression-MentalRiskES", "published": "2023-06-28 11:53:07", "link": "http://arxiv.org/abs/2306.16125v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation", "abstract": "Incorporating external graph knowledge into neural chatbot models has been\nproven effective for enhancing dialogue generation. However, in conventional\ngraph neural networks (GNNs), message passing on a graph is independent from\ntext, resulting in the graph representation hidden space differing from that of\nthe text. This training regime of existing models therefore leads to a semantic\ngap between graph knowledge and text. In this study, we propose a novel\nframework for knowledge graph enhanced dialogue generation. We dynamically\nconstruct a multi-hop knowledge graph with pseudo nodes to involve the language\nmodel in feature aggregation within the graph at all steps. To avoid the\nsemantic biases caused by learning on vanilla subgraphs, the proposed framework\napplies hierarchical graph attention to aggregate graph features on pseudo\nnodes and then attains a global feature. Therefore, the framework can better\nutilise the heterogeneous features from both the post and external graph\nknowledge. Extensive experiments demonstrate that our framework outperforms\nstate-of-the-art (SOTA) baselines on dialogue generation. Further analysis also\nshows that our representation learning framework can fill the semantic gap by\ncoagulating representations of both text and graph knowledge. Moreover, the\nlanguage model also learns how to better select knowledge triples for a more\ninformative response via exploiting subgraph patterns within our feature\naggregation process. Our code and resources are available at\nhttps://github.com/tangg555/SaBART.", "published": "2023-06-28 13:21:00", "link": "http://arxiv.org/abs/2306.16195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI\n  Collaboration for Large Language Models", "abstract": "Holistically measuring societal biases of large language models is crucial\nfor detecting and reducing ethical risks in highly capable AI models. In this\nwork, we present a Chinese Bias Benchmark dataset that consists of over 100K\nquestions jointly constructed by human experts and generative language models,\ncovering stereotypes and societal biases in 14 social dimensions related to\nChinese culture and values. The curation process contains 4 essential steps:\nbias identification via extensive literature review, ambiguous context\ngeneration, AI-assisted disambiguous context generation, snd manual review \\&\nrecomposition. The testing instances in the dataset are automatically derived\nfrom 3K+ high-quality templates manually authored with stringent quality\ncontrol. The dataset exhibits wide coverage and high diversity. Extensive\nexperiments demonstrate the effectiveness of the dataset in detecting model\nbias, with all 10 publicly available Chinese large language models exhibiting\nstrong bias in certain categories. Additionally, we observe from our\nexperiments that fine-tuned models could, to a certain extent, heed\ninstructions and avoid generating outputs that are morally harmful in some\ntypes, in the way of \"moral self-correction\". Our dataset and results are\npublicly available at\n\\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},\noffering debiasing research opportunities to a widened community.", "published": "2023-06-28 14:14:44", "link": "http://arxiv.org/abs/2306.16244v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion Analysis of Tweets Banning Education in Afghanistan", "abstract": "This paper introduces the first emotion annotated dataset for the Dari\nvariant of Persian spoken in Afghanistan. The LetHerLearn dataset contains\n7,600 tweets posted in reaction to the Taliban ban of women rights to education\nin 2022 and has been manually annotated according to Ekman emotion categories.\nWe here detail the data collection and annotation process, present relevant\ndataset statistics as well as initial experiments on the resulting dataset,\nbenchmarking a number of different neural architectures for the task of Dari\nemotion classification.", "published": "2023-06-28 14:50:49", "link": "http://arxiv.org/abs/2306.16268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Adversarial Multi-Task Learning Method for Chinese Text Correction\n  with Semantic Detection", "abstract": "Text correction, especially the semantic correction of more widely used\nscenes, is strongly required to improve, for the fluency and writing efficiency\nof the text. An adversarial multi-task learning method is proposed to enhance\nthe modeling and detection ability of character polysemy in Chinese sentence\ncontext. Wherein, two models, the masked language model and scoring language\nmodel, are introduced as a pair of not only coupled but also adversarial\nlearning tasks. Moreover, the Monte Carlo tree search strategy and a policy\nnetwork are introduced to accomplish the efficient Chinese text correction task\nwith semantic detection. The experiments are executed on three datasets and\nfive comparable methods, and the experimental results show that our method can\nobtain good performance in Chinese text correction task for better semantic\nrationality.", "published": "2023-06-28 15:46:00", "link": "http://arxiv.org/abs/2306.16313v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Measuring the Representation of Subjective Global Opinions in\n  Language Models", "abstract": "Large language models (LLMs) may not equitably represent diverse global\nperspectives on societal issues. In this paper, we develop a quantitative\nframework to evaluate whose opinions model-generated responses are more similar\nto. We first build a dataset, GlobalOpinionQA, comprised of questions and\nanswers from cross-national surveys designed to capture diverse opinions on\nglobal issues across different countries. Next, we define a metric that\nquantifies the similarity between LLM-generated survey responses and human\nresponses, conditioned on country. With our framework, we run three experiments\non an LLM trained to be helpful, honest, and harmless with Constitutional AI.\nBy default, LLM responses tend to be more similar to the opinions of certain\npopulations, such as those from the USA, and some European and South American\ncountries, highlighting the potential for biases. When we prompt the model to\nconsider a particular country's perspective, responses shift to be more similar\nto the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes. When we translate GlobalOpinionQA questions to a target language,\nthe model's responses do not necessarily become the most similar to the\nopinions of speakers of those languages. We release our dataset for others to\nuse and build on. Our data is at\nhttps://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide\nan interactive visualization at https://llmglobalvalues.anthropic.com.", "published": "2023-06-28 17:31:53", "link": "http://arxiv.org/abs/2306.16388v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Language Models That Can See: Computer Vision Through the LENS\n  of Natural Language", "abstract": "We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.", "published": "2023-06-28 17:57:10", "link": "http://arxiv.org/abs/2306.16410v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Pareto Optimal Learning for Estimating Large Language Model Errors", "abstract": "Large Language Models (LLMs) have shown impressive abilities in many\napplications. When a concrete and precise answer is desired, it is important to\nhave a quantitative estimation of the potential error rate. However, this can\nbe challenging due to the text-in-text-out nature of generative models. We\npresent a method based on Pareto optimization that generates a risk score to\nestimate the probability of error in an LLM response by integrating multiple\nsources of information. We prove theoretically that the error estimator\noptimized in our framework aligns with the LLM and the information sources in\nan Pareto optimal manner. Experimental results show that the risk scores\nestimated by our method are well correlated with the true LLM error rate, thus\nfacilitating error correction. By dynamically combining with prompting\nstrategies such as self-verification and information retrieval, we demonstrate\nthe proposed method can be utilized to increase the performance of an LLM,\nsurpassing state-of-the-art task specific models.", "published": "2023-06-28 21:11:15", "link": "http://arxiv.org/abs/2306.16564v4", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Large Language Model as Attributed Training Data Generator: A Tale of\n  Diversity and Bias", "abstract": "Large language models (LLMs) have been recently leveraged as training data\ngenerators for various natural language processing (NLP) tasks. While previous\nresearch has explored different approaches to training models using generated\ndata, they generally rely on simple class-conditional prompts, which may limit\nthe diversity of the generated data and inherit systematic biases of LLM. Thus,\nwe investigate training data generation with diversely attributed prompts\n(e.g., specifying attributes like length and style), which have the potential\nto yield diverse and attributed generated data. Our investigation focuses on\ndatasets with high cardinality and diverse domains, wherein we demonstrate that\nattributed prompts outperform simple class-conditional prompts in terms of the\nresulting model's performance. Additionally, we present a comprehensive\nempirical study on data generation encompassing vital aspects like bias,\ndiversity, and efficiency, and highlight three key observations: firstly,\nsynthetic datasets generated by simple prompts exhibit significant biases, such\nas regional bias; secondly, attribute diversity plays a pivotal role in\nenhancing model performance; lastly, attributed prompts achieve the performance\nof simple class-conditional prompts while utilizing only 5\\% of the querying\ncost of ChatGPT associated with the latter. The data and code are available on\n\\url{https://github.com/yueyu1030/AttrPrompt}.", "published": "2023-06-28 03:31:31", "link": "http://arxiv.org/abs/2306.15895v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Most Language Models can be Poets too: An AI Writing Assistant and\n  Constrained Text Generation Studio", "abstract": "Despite rapid advancement in the field of Constrained Natural Language\nGeneration, little time has been spent on exploring the potential of language\nmodels which have had their vocabularies lexically, semantically, and/or\nphonetically constrained. We find that most language models generate compelling\ntext even under significant constraints. We present a simple and universally\napplicable technique for modifying the output of a language model by\ncompositionally applying filter functions to the language models vocabulary\nbefore a unit of text is generated. This approach is plug-and-play and requires\nno modification to the model. To showcase the value of this technique, we\npresent an easy to use AI writing assistant called Constrained Text Generation\nStudio (CTGS). CTGS allows users to generate or choose from text with any\ncombination of a wide variety of constraints, such as banning a particular\nletter, forcing the generated words to have a certain number of syllables,\nand/or forcing the words to be partial anagrams of another word. We introduce a\nnovel dataset of prose that omits the letter e. We show that our method results\nin strictly superior performance compared to fine-tuning alone on this dataset.\nWe also present a Huggingface space web-app presenting this technique called\nGadsby. The code is available to the public here:\nhttps://github.com/Hellisotherpeople/Constrained-Text-Generation-Studio", "published": "2023-06-28 05:10:51", "link": "http://arxiv.org/abs/2306.15926v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "You Can Generate It Again: Data-to-text Generation with Verification and\n  Correction Prompting", "abstract": "Despite significant advancements in existing models, generating text\ndescriptions from structured data input, known as data-to-text generation,\nremains a challenging task. In this paper, we propose a novel approach that\ngoes beyond traditional one-shot generation methods by introducing a multi-step\nprocess consisting of generation, verification, and correction stages. Our\napproach, VCP(Verification and Correction Prompting), begins with the model\ngenerating an initial output. We then proceed to verify the correctness of\ndifferent aspects of the generated text. The observations from the verification\nstep are converted into a specialized error-indication prompt, which instructs\nthe model to regenerate the output while considering the identified errors. To\nenhance the model's correction ability, we have developed a carefully designed\ntraining procedure. This procedure enables the model to incorporate feedback\nfrom the error-indication prompt, resulting in improved output generation.\nThrough experimental results, we demonstrate that our approach effectively\nreduces slot error rates while maintaining the overall quality of the generated\ntext.", "published": "2023-06-28 05:34:25", "link": "http://arxiv.org/abs/2306.15933v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Streamlining Social Media Information Retrieval for COVID-19 Research\n  with Deep Learning", "abstract": "Objective: Social media-based public health research is crucial for epidemic\nsurveillance, but most studies identify relevant corpora with keyword-matching.\nThis study develops a system to streamline the process of curating colloquial\nmedical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial\nsymptom dictionary from COVID-19-related tweets as proof of concept. Methods:\nCOVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The\npipeline includes three modules: a named entity recognition module to detect\nsymptoms in tweets; an entity normalization module to aggregate detected\nentities; and a mapping module that iteratively maps entities to Unified\nMedical Language System concepts. A random 500 entity sample were drawn from\nthe final dictionary for accuracy validation. Additionally, we conducted a\nsymptom frequency distribution analysis to compare our dictionary to a\npre-defined lexicon from previous research. Results: We identified 498,480\nunique symptom entity expressions from the tweets. Pre-processing reduces the\nnumber to 18,226. The final dictionary contains 38,175 unique expressions of\nsymptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom\ndistribution analysis found that our dictionary detects more symptoms and is\neffective at identifying psychiatric disorders like anxiety and depression,\noften missed by pre-defined lexicons. Conclusions: This study advances public\nhealth research by implementing a novel, systematic pipeline for curating\nsymptom lexicons from social media data. The final lexicon's high accuracy,\nvalidated by medical professionals, underscores the potential of this\nmethodology to reliably interpret and categorize vast amounts of unstructured\nsocial media data into actionable medical insights across diverse linguistic\nand regional landscapes.", "published": "2023-06-28 08:20:35", "link": "http://arxiv.org/abs/2306.16001v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models for Zero-Shot Domain Adaptation in\n  Speech Recognition", "abstract": "The integration of Language Models (LMs) has proven to be an effective way to\naddress domain shifts in speech recognition. However, these approaches usually\nrequire a significant amount of target domain text data for the training of\nLMs. Different from these methods, in this work, with only a domain-specific\ntext prompt, we propose two zero-shot ASR domain adaptation methods using\nLLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two\nways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR\nsystem with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an\nencoder-decoder based ASR system. Experiments show that, with only one domain\nprompt, both methods can effectively reduce word error rates (WER) on\nout-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep\nLLM-fusion has the advantage of better recall of entity and out-of-vocabulary\nwords.", "published": "2023-06-28 08:29:00", "link": "http://arxiv.org/abs/2306.16007v1", "categories": ["cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Accelerating Transducers through Adjacent Token Merging", "abstract": "Recent end-to-end automatic speech recognition (ASR) systems often utilize a\nTransformer-based acoustic encoder that generates embedding at a high frame\nrate. However, this design is inefficient, particularly for long speech signals\ndue to the quadratic computation of self-attention. To address this, we propose\na new method, Adjacent Token Merging (A-ToMe), which gradually combines\nadjacent tokens with high similarity scores between their key values. In this\nway, the total time step could be reduced, and the inference of both the\nencoder and joint network is accelerated. Experiments on LibriSpeech show that\nour method can reduce 57% of tokens and improve the inference speed on GPU by\n70% without any notable loss of accuracy. Additionally, we demonstrate that\nA-ToMe is also an effective solution to reduce tokens in long-form ASR, where\nthe input speech consists of multiple utterances.", "published": "2023-06-28 08:33:13", "link": "http://arxiv.org/abs/2306.16009v1", "categories": ["cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Long-term Conversation Analysis: Exploring Utility and Privacy", "abstract": "The analysis of conversations recorded in everyday life requires privacy\nprotection. In this contribution, we explore a privacy-preserving feature\nextraction method based on input feature dimension reduction, spectral\nsmoothing and the low-cost speaker anonymization technique based on McAdams\ncoefficient. We assess the utility of the feature extraction methods with a\nvoice activity detection and a speaker diarization system, while privacy\nprotection is determined with a speech recognition and a speaker verification\nmodel. We show that the combination of McAdams coefficient and spectral\nsmoothing maintains the utility while improving privacy.", "published": "2023-06-28 10:10:57", "link": "http://arxiv.org/abs/2306.16071v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative User-Experience Research for Developing Domain-specific\n  Natural Language Processing Applications", "abstract": "User experience (UX) is a part of human-computer interaction (HCI) research\nand focuses on increasing intuitiveness, transparency, simplicity, and trust\nfor the system users. Most UX research for machine learning (ML) or natural\nlanguage processing (NLP) focuses on a data-driven methodology. It engages\ndomain users mainly for usability evaluation. Moreover, more typical UX methods\ntailor the systems towards user usability, unlike learning about the user needs\nfirst. This paper proposes a new methodology for integrating generative UX\nresearch into developing domain NLP applications. Generative UX research\nemploys domain users at the initial stages of prototype development, i.e.,\nideation and concept evaluation, and the last stage for evaluating system\nusefulness and user utility. The methodology emerged from and is evaluated on a\ncase study about the full-cycle prototype development of a domain-specific\nsemantic search for daily operations in the process industry. A key finding of\nour case study is that involving domain experts increases their interest and\ntrust in the final NLP application. The combined UX+NLP research of the\nproposed method efficiently considers data- and user-driven opportunities and\nconstraints, which can be crucial for developing NLP applications.", "published": "2023-06-28 12:17:45", "link": "http://arxiv.org/abs/2306.16143v5", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Inferring the Goals of Communicating Agents from Actions and\n  Instructions", "abstract": "When humans cooperate, they frequently coordinate their activity through both\nverbal communication and non-verbal actions, using this information to infer a\nshared goal and plan. How can we model this inferential ability? In this paper,\nwe introduce a model of a cooperative team where one agent, the principal, may\ncommunicate natural language instructions about their shared plan to another\nagent, the assistant, using GPT-3 as a likelihood function for instruction\nutterances. We then show how a third person observer can infer the team's goal\nvia multi-modal Bayesian inverse planning from actions and instructions,\ncomputing the posterior distribution over goals under the assumption that\nagents will act and communicate rationally to achieve them. We evaluate this\napproach by comparing it with human goal inferences in a multi-agent gridworld,\nfinding that our model's inferences closely correlate with human judgments (R =\n0.96). When compared to inference from actions alone, we also find that\ninstructions lead to more rapid and less uncertain goal inference, highlighting\nthe importance of verbal communication for cooperative agents.", "published": "2023-06-28 13:43:46", "link": "http://arxiv.org/abs/2306.16207v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Leveraging GPT-4 for Food Effect Summarization to Enhance\n  Product-Specific Guidance Development via Iterative Prompting", "abstract": "Food effect summarization from New Drug Application (NDA) is an essential\ncomponent of product-specific guidance (PSG) development and assessment.\nHowever, manual summarization of food effect from extensive drug application\nreview documents is time-consuming, which arouses a need to develop automated\nmethods. Recent advances in large language models (LLMs) such as ChatGPT and\nGPT-4, have demonstrated great potential in improving the effectiveness of\nautomated text summarization, but its ability regarding the accuracy in\nsummarizing food effect for PSG assessment remains unclear. In this study, we\nintroduce a simple yet effective approach, iterative prompting, which allows\none to interact with ChatGPT or GPT-4 more effectively and efficiently through\nmulti-turn interaction. Specifically, we propose a three-turn iterative\nprompting approach to food effect summarization in which the keyword-focused\nand length-controlled prompts are respectively provided in consecutive turns to\nrefine the quality of the generated summary. We conduct a series of extensive\nevaluations, ranging from automated metrics to FDA professionals and even\nevaluation by GPT-4, on 100 NDA review documents selected over the past five\nyears. We observe that the summary quality is progressively improved throughout\nthe process. Moreover, we find that GPT-4 performs better than ChatGPT, as\nevaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).\nImportantly, all the FDA professionals unanimously rated that 85% of the\nsummaries generated by GPT-4 are factually consistent with the golden reference\nsummary, a finding further supported by GPT-4 rating of 72% consistency. These\nresults strongly suggest a great potential for GPT-4 to draft food effect\nsummaries that could be reviewed by FDA professionals, thereby improving the\nefficiency of PSG assessment cycle and promoting the generic drug product\ndevelopment.", "published": "2023-06-28 14:55:13", "link": "http://arxiv.org/abs/2306.16275v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation Learning via Variational Bayesian Networks", "abstract": "We present Variational Bayesian Network (VBN) - a novel Bayesian entity\nrepresentation learning model that utilizes hierarchical and relational side\ninformation and is particularly useful for modeling entities in the\n``long-tail'', where the data is scarce. VBN provides better modeling for\nlong-tail entities via two complementary mechanisms: First, VBN employs\ninformative hierarchical priors that enable information propagation between\nentities sharing common ancestors. Additionally, VBN models explicit relations\nbetween entities that enforce complementary structure and consistency, guiding\nthe learned representations towards a more meaningful arrangement in space.\nSecond, VBN represents entities by densities (rather than vectors), hence\nmodeling uncertainty that plays a complementary role in coping with data\nscarcity. Finally, we propose a scalable Variational Bayes optimization\nalgorithm that enables fast approximate Bayesian inference. We evaluate the\neffectiveness of VBN on linguistic, recommendations, and medical inference\ntasks. Our findings show that VBN outperforms other existing methods across\nmultiple datasets, and especially in the long-tail.", "published": "2023-06-28 16:00:45", "link": "http://arxiv.org/abs/2306.16326v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-Site Clinical Federated Learning using Recursive and Attentive\n  Models and NVFlare", "abstract": "The prodigious growth of digital health data has precipitated a mounting\ninterest in harnessing machine learning methodologies, such as natural language\nprocessing (NLP), to scrutinize medical records, clinical notes, and other\ntext-based health information. Although NLP techniques have exhibited\nsubstantial potential in augmenting patient care and informing clinical\ndecision-making, data privacy and adherence to regulations persist as critical\nconcerns. Federated learning (FL) emerges as a viable solution, empowering\nmultiple organizations to train machine learning models collaboratively without\ndisseminating raw data. This paper proffers a pragmatic approach to medical NLP\nby amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA.\nWe introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based\nmodel and Bidirectional Encoder Representations from Transformers (BERT), which\nhave demonstrated exceptional performance in comprehending context and\nsemantics within medical data. This paper encompasses the development of an\nintegrated framework that addresses data privacy and regulatory compliance\nchallenges while maintaining elevated accuracy and performance, incorporating\nBERT pretraining, and comprehensively substantiating the efficacy of the\nproposed approach.", "published": "2023-06-28 17:00:32", "link": "http://arxiv.org/abs/2306.16367v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual\n  Question Answering", "abstract": "This paper studies a category of visual question answering tasks, in which\naccessing external knowledge is necessary for answering the questions. This\ncategory is called outside-knowledge visual question answering (OK-VQA). A\nmajor step in developing OK-VQA systems is to retrieve relevant documents for\nthe given multi-modal query. Current state-of-the-art asymmetric dense\nretrieval model for this task uses an architecture with a multi-modal query\nencoder and a uni-modal document encoder. Such an architecture requires a large\namount of training data for effective performance. We propose an automatic data\ngeneration pipeline for pre-training passage retrieval models for OK-VQA tasks.\nThe proposed approach leads to 26.9% Precision@5 improvements compared to the\ncurrent state-of-the-art asymmetric architecture. Additionally, the proposed\npre-training approach exhibits a good ability in zero-shot retrieval scenarios.", "published": "2023-06-28 18:06:40", "link": "http://arxiv.org/abs/2306.16478v1", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "ICSVR: Investigating Compositional and Syntactic Understanding in Video\n  Retrieval Models", "abstract": "Video retrieval (VR) involves retrieving the ground truth video from the\nvideo database given a text caption or vice-versa. The two important components\nof compositionality: objects & attributes and actions are joined using correct\nsyntax to form a proper text query. These components (objects & attributes,\nactions and syntax) each play an important role to help distinguish among\nvideos and retrieve the correct ground truth video. However, it is unclear what\nis the effect of these components on the video retrieval performance. We\ntherefore, conduct a systematic study to evaluate the compositional and\nsyntactic understanding of video retrieval models on standard benchmarks such\nas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video\nretrieval models: (i) which are pre-trained on video-text pairs and fine-tuned\non downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)\n(ii) which adapt pre-trained image-text representations like CLIP for video\nretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that\nactions and syntax play a minor role compared to objects & attributes in video\nunderstanding. Moreover, video retrieval models that use pre-trained image-text\nrepresentations (CLIP) have better syntactic and compositional understanding as\ncompared to models pre-trained on video-text data. The code is available at\nhttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR", "published": "2023-06-28 20:06:36", "link": "http://arxiv.org/abs/2306.16533v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Efficient Sparse Inference Software Accelerator for Transformer-based\n  Language Models on CPUs", "abstract": "In recent years, Transformer-based language models have become the standard\napproach for natural language processing tasks. However, stringent throughput\nand latency requirements in industrial applications are limiting their\nadoption. To mitigate the gap, model compression techniques such as structured\npruning are being used to improve inference efficiency. However, most existing\nneural network inference runtimes lack adequate support for structured\nsparsity. In this paper, we propose an efficient sparse deep learning inference\nsoftware stack for Transformer-based language models where the weights are\npruned with constant block size. Our sparse software accelerator leverages\nIntel Deep Learning Boost to maximize the performance of sparse matrix - dense\nmatrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel\noutperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an\norder of magnitude on a wide range of GEMM shapes under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up\nto 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library\nwidely used in industry. We apply our sparse accelerator on widely-used\nTransformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over\nNeural Magic's Deepsparse under same configurations on Xeon on Amazon Web\nServices under proxy production latency constraints. We also compare our\nsolution with two framework-based inference solutions, ONNX Runtime and\nPyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over\nPyTorch on Xeon under the latency constraints. All the source code is publicly\navailable on Github: https://github.com/intel/intel-extension-for-transformers.", "published": "2023-06-28 23:55:51", "link": "http://arxiv.org/abs/2306.16601v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Exploitability of Instruction Tuning", "abstract": "Instruction tuning is an effective technique to align large language models\n(LLMs) with human intents. In this work, we investigate how an adversary can\nexploit instruction tuning by injecting specific instruction-following examples\ninto the training data that intentionally changes the model's behavior. For\nexample, an adversary can achieve content injection by injecting training\nexamples that mention target content and eliciting such behavior from\ndownstream models. To achieve this goal, we propose \\textit{AutoPoison}, an\nautomated data poisoning pipeline. It naturally and coherently incorporates\nversatile attack goals into poisoned data with the help of an oracle LLM. We\nshowcase two example attacks: content injection and over-refusal attacks, each\naiming to induce a specific exploitable behavior. We quantify and benchmark the\nstrength and the stealthiness of our data poisoning scheme. Our results show\nthat AutoPoison allows an adversary to change a model's behavior by poisoning\nonly a small fraction of data while maintaining a high level of stealthiness in\nthe poisoned examples. We hope our work sheds light on how data quality affects\nthe behavior of instruction-tuned models and raises awareness of the importance\nof data quality for responsible deployments of LLMs. Code is available at\n\\url{https://github.com/azshue/AutoPoison}.", "published": "2023-06-28 17:54:04", "link": "http://arxiv.org/abs/2306.17194v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep\n  Learning", "abstract": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. In order to accelerate progress towards\nunderstudied modalities and tasks while ensuring real-world robustness, we\nrelease MultiZoo, a public toolkit consisting of standardized implementations\nof > 20 core multimodal algorithms and MultiBench, a large-scale benchmark\nspanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.\nTogether, these provide an automated end-to-end machine learning pipeline that\nsimplifies and standardizes data loading, experimental setup, and model\nevaluation. To enable holistic evaluation, we offer a comprehensive methodology\nto assess (1) generalization, (2) time and space complexity, and (3) modality\nrobustness. MultiBench paves the way towards a better understanding of the\ncapabilities and limitations of multimodal models, while ensuring ease of use,\naccessibility, and reproducibility. Our toolkits are publicly available, will\nbe regularly updated, and welcome inputs from the community.", "published": "2023-06-28 17:59:10", "link": "http://arxiv.org/abs/2306.16413v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Graph neural networks for sound source localization on distributed\n  microphone networks", "abstract": "Distributed Microphone Arrays (DMAs) present many challenges with respect to\ncentralized microphone arrays. An important requirement of applications on\nthese arrays is handling a variable number of input channels. We consider the\nuse of Graph Neural Networks (GNNs) as a solution to this challenge. We present\na localization method using the Relation Network GNN, which we show shares many\nsimilarities to classical signal processing algorithms for Sound Source\nLocalization (SSL). We apply our method for the task of SSL and validate it\nexperimentally using an unseen number of microphones. We test different feature\nextractors and show that our approach significantly outperforms classical\nbaselines.", "published": "2023-06-28 10:27:53", "link": "http://arxiv.org/abs/2306.16081v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data", "abstract": "We propose UnitSpeech, a speaker-adaptive speech synthesis method that\nfine-tunes a diffusion-based text-to-speech (TTS) model using minimal\nuntranscribed data. To achieve this, we use the self-supervised unit\nrepresentation as a pseudo transcript and integrate the unit encoder into the\npre-trained TTS model. We train the unit encoder to provide speech content to\nthe diffusion-based decoder and then fine-tune the decoder for speaker\nadaptation to the reference speaker using a single $<$unit, speech$>$ pair.\nUnitSpeech performs speech synthesis tasks such as TTS and voice conversion\n(VC) in a personalized manner without requiring model re-training for each\ntask. UnitSpeech achieves comparable and superior results on personalized TTS\nand any-to-any VC tasks compared to previous baselines. Our model also shows\nwidespread adaptive performance on real-world data and other tasks that use a\nunit sequence as input.", "published": "2023-06-28 10:30:39", "link": "http://arxiv.org/abs/2306.16083v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Focus on the Sound around You: Monaural Target Speaker Extraction via\n  Distance and Speaker Information", "abstract": "Previously, Target Speaker Extraction (TSE) has yielded outstanding\nperformance in certain application scenarios for speech enhancement and source\nseparation. However, obtaining auxiliary speaker-related information is still\nchallenging in noisy environments with significant reverberation. inspired by\nthe recently proposed distance-based sound separation, we propose the near\nsound (NS) extractor, which leverages distance information for TSE to reliably\nextract speaker information without requiring previous speaker enrolment,\ncalled speaker embedding self-enrollment (SESE). Full- & sub-band modeling is\nintroduced to enhance our NS-Extractor's adaptability towards environments with\nsignificant reverberation. Experimental results on several cross-datasets\ndemonstrate the effectiveness of our improvements and the excellent performance\nof our proposed NS-Extractor in different application scenarios.", "published": "2023-06-28 14:09:46", "link": "http://arxiv.org/abs/2306.16241v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale\n  Interfusion and Conditional Speaker Modulation", "abstract": "The previous SpEx+ has yielded outstanding performance in speaker extraction\nand attracted much attention. However, it still encounters inadequate\nutilization of multi-scale information and speaker embedding. To this end, this\npaper proposes a new effective speaker extraction system with multi-scale\ninterfusion and conditional speaker modulation (ConSM), which is called\nMC-SpEx. First of all, we design the weight-share multi-scale fusers\n(ScaleFusers) for efficiently leveraging multi-scale information as well as\nensuring consistency of the model's feature space. Then, to consider different\nscale information while generating masks, the multi-scale interactive mask\ngenerator (ScaleInterMG) is presented. Moreover, we introduce ConSM module to\nfully exploit speaker embedding in the speech extractor. Experimental results\non the Libri2Mix dataset demonstrate the effectiveness of our improvements and\nthe state-of-the-art performance of our proposed MC-SpEx.", "published": "2023-06-28 14:21:06", "link": "http://arxiv.org/abs/2306.16250v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cascaded encoders for fine-tuning ASR models on overlapped speech", "abstract": "Multi-talker speech recognition (MT-ASR) has been shown to improve ASR\nperformance on speech containing overlapping utterances from more than one\nspeaker. Multi-talker models have typically been trained from scratch using\nsimulated or actual overlapping speech datasets. On the other hand, the trend\nin ASR has been to train foundation models using massive datasets collected\nfrom a wide variety of task domains. Given the scale of these models and their\nability to generalize well across a variety of domains, it makes sense to\nconsider scenarios where a foundation model is augmented with multi-talker\ncapability. This paper presents an MT-ASR model formed by combining a\nwell-trained foundation model with a multi-talker mask model in a cascaded\nRNN-T encoder configuration. Experimental results show that the cascade\nconfiguration provides improved WER on overlapping speech utterances with\nrespect to a baseline multi-talker model without sacrificing performance\nachievable by the foundation model on non-overlapping utterances.", "published": "2023-06-28 17:44:30", "link": "http://arxiv.org/abs/2306.16398v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice\n  Conversion", "abstract": "Deep speech classification has achieved tremendous success and greatly\npromoted the emergence of many real-world applications. However, backdoor\nattacks present a new security threat to it, particularly with untrustworthy\nthird-party platforms, as pre-defined triggers set by the attacker can activate\nthe backdoor. Most of the triggers in existing speech backdoor attacks are\nsample-agnostic, and even if the triggers are designed to be unnoticeable, they\ncan still be audible. This work explores a backdoor attack that utilizes\nsample-specific triggers based on voice conversion. Specifically, we adopt a\npre-trained voice conversion model to generate the trigger, ensuring that the\npoisoned samples does not introduce any additional audible noise. Extensive\nexperiments on two speech classification tasks demonstrate the effectiveness of\nour attack. Furthermore, we analyzed the specific scenarios that activated the\nproposed backdoor and verified its resistance against fine-tuning.", "published": "2023-06-28 02:19:31", "link": "http://arxiv.org/abs/2306.15875v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhanced Neural Beamformer with Spatial Information for Target Speech\n  Extraction", "abstract": "Recently, deep learning-based beamforming algorithms have shown promising\nperformance in target speech extraction tasks. However, most systems do not\nfully utilize spatial information. In this paper, we propose a target speech\nextraction network that utilizes spatial information to enhance the performance\nof neural beamformer. To achieve this, we first use the UNet-TCN structure to\nmodel input features and improve the estimation accuracy of the speech\npre-separation module by avoiding information loss caused by direct\ndimensionality reduction in other models. Furthermore, we introduce a\nmulti-head cross-attention mechanism that enhances the neural beamformer's\nperception of spatial information by making full use of the spatial information\nreceived by the array. Experimental results demonstrate that our approach,\nwhich incorporates a more reasonable target mask estimation network and a\nspatial information-based cross-attention mechanism into the neural beamformer,\neffectively improves speech separation performance.", "published": "2023-06-28 06:03:10", "link": "http://arxiv.org/abs/2306.15942v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-driven Talking Face Synthesis by Reprogramming Audio-driven Models", "abstract": "In this paper, we present a method for reprogramming pre-trained audio-driven\ntalking face synthesis models to operate in a text-driven manner. Consequently,\nwe can easily generate face videos that articulate the provided textual\nsentences, eliminating the necessity of recording speech for each inference, as\nrequired in the audio-driven model. To this end, we propose to embed the input\ntext into the learned audio latent space of the pre-trained audio-driven model,\nwhile preserving the face synthesis capability of the original pre-trained\nmodel. Specifically, we devise a Text-to-Audio Embedding Module (TAEM) which\nmaps a given text input into the audio latent space by modeling pronunciation\nand duration characteristics. Furthermore, to consider the speaker\ncharacteristics in audio while using text inputs, TAEM is designed to accept a\nvisual speaker embedding. The visual speaker embedding is derived from a single\ntarget face image and enables improved mapping of input text to the learned\naudio latent space by incorporating the speaker characteristics inherent in the\naudio. The main advantages of the proposed framework are that 1) it can be\napplied to diverse audio-driven talking face synthesis models and 2) we can\ngenerate talking face videos with either text inputs or audio inputs with high\nflexibility.", "published": "2023-06-28 08:22:53", "link": "http://arxiv.org/abs/2306.16003v2", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via\n  Adversarial Ultrasound", "abstract": "Automatic Speaker Recognition Systems (SRSs) have been widely used in voice\napplications for personal identification and access control. A typical SRS\nconsists of three stages, i.e., training, enrollment, and recognition. Previous\nwork has revealed that SRSs can be bypassed by backdoor attacks at the training\nstage or by adversarial example attacks at the recognition stage. In this\npaper, we propose Tuner, a new type of backdoor attack against the enrollment\nstage of SRS via adversarial ultrasound modulation, which is inaudible,\nsynchronization-free, content-independent, and black-box. Our key idea is to\nfirst inject the backdoor into the SRS with modulated ultrasound when a\nlegitimate user initiates the enrollment, and afterward, the polluted SRS will\ngrant access to both the legitimate user and the adversary with high\nconfidence. Our attack faces a major challenge of unpredictable user\narticulation at the enrollment stage. To overcome this challenge, we generate\nthe ultrasonic backdoor by augmenting the optimization process with random\nspeech content, vocalizing time, and volume of the user. Furthermore, to\nachieve real-world robustness, we improve the ultrasonic signal over\ntraditional methods using sparse frequency points, pre-compensation, and\nsingle-sideband (SSB) modulation. We extensively evaluate Tuner on two common\ndatasets and seven representative SRS models, as well as its robustness against\nseven kinds of defenses. Results show that our attack can successfully bypass\nspeaker recognition systems while remaining effective to various speakers,\nspeech content, etc. To mitigate this newly discovered threat, we also provide\ndiscussions on potential countermeasures, limitations, and future works of this\nnew threat.", "published": "2023-06-28 08:50:35", "link": "http://arxiv.org/abs/2306.16022v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Primate Sounds Classification using Binary Presorting for Deep\n  Learning", "abstract": "In the field of wildlife observation and conservation, approaches involving\nmachine learning on audio recordings are becoming increasingly popular.\nUnfortunately, available datasets from this field of research are often not\noptimal learning material; Samples can be weakly labeled, of different lengths\nor come with a poor signal-to-noise ratio. In this work, we introduce a\ngeneralized approach that first relabels subsegments of MEL spectrogram\nrepresentations, to achieve higher performances on the actual multi-class\nclassification tasks. For both the binary pre-sorting and the classification,\nwe make use of convolutional neural networks (CNN) and various\ndata-augmentation techniques. We showcase the results of this approach on the\nchallenging \\textit{ComparE 2021} dataset, with the task of classifying between\ndifferent primate species sounds, and report significantly higher Accuracy and\nUAR scores in contrast to comparatively equipped model baselines.", "published": "2023-06-28 09:35:09", "link": "http://arxiv.org/abs/2306.16054v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Two-Stage Voice Anonymization for Enhanced Privacy", "abstract": "In recent years, the need for privacy preservation when manipulating or\nstoring personal data, including speech , has become a major issue. In this\npaper, we present a system addressing the speaker-level anonymization problem.\nWe propose and evaluate a two-stage anonymization pipeline exploiting a\nstate-of-the-art anonymization model described in the Voice Privacy Challenge\n2022 in combination with a zero-shot voice conversion architecture able to\ncapture speaker characteristics from a few seconds of speech. We show this\narchitecture can lead to strong privacy preservation while preserving pitch\ninformation. Finally, we propose a new compressed metric to evaluate\nanonymization systems in privacy scenarios with different constraints on\nprivacy and utility.", "published": "2023-06-28 10:08:21", "link": "http://arxiv.org/abs/2306.16069v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech", "abstract": "State-of-the-art speech synthesis models try to get as close as possible to\nthe human voice. Hence, modelling emotions is an essential part of\nText-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the\nstarting point and proposed a series of modifications for synthesizing\nemotional speech. According to automatic and human evaluation, our model,\nEmoSpeech, surpasses existing models regarding both MOS score and emotion\nrecognition accuracy in generated speech. We provided a detailed ablation study\nfor every extension to FastSpeech2 architecture that forms EmoSpeech. The\nuneven distribution of emotions in the text is crucial for better, synthesized\nspeech and intonation perception. Our model includes a conditioning mechanism\nthat effectively handles this issue by allowing emotions to contribute to each\nphone with varying intensity levels. The human assessment indicates that\nproposed modifications generate audio with higher MOS and emotional\nexpressiveness.", "published": "2023-06-28 19:34:16", "link": "http://arxiv.org/abs/2307.00024v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
