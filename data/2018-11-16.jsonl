{"title": "Mining Entity Synonyms with Efficient Neural Set Generation", "abstract": "Mining entity synonym sets (i.e., sets of terms referring to the same entity)\nis an important task for many entity-leveraging applications. Previous work\neither rank terms based on their similarity to a given query term, or treats\nthe problem as a two-phase task (i.e., detecting synonymy pairs, followed by\norganizing these pairs into synonym sets). However, these approaches fail to\nmodel the holistic semantics of a set and suffer from the error propagation\nissue. Here we propose a new framework, named SynSetMine, that efficiently\ngenerates entity synonym sets from a given vocabulary, using example sets from\nexternal knowledge bases as distant supervision. SynSetMine consists of two\nnovel modules: (1) a set-instance classifier that jointly learns how to\nrepresent a permutation invariant synonym set and whether to include a new\ninstance (i.e., a term) into the set, and (2) a set generation algorithm that\nenumerates the vocabulary only once and applies the learned set-instance\nclassifier to detect all entity synonym sets in it. Experiments on three real\ndatasets from different domains demonstrate both effectiveness and efficiency\nof SynSetMine for mining entity synonym sets.", "published": "2018-11-16 21:08:50", "link": "http://arxiv.org/abs/1811.07032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Sentiment Induction to Understand Variation in Gendered Online\n  Communities", "abstract": "We analyze gendered communities defined in three different ways: text, users,\nand sentiment. Differences across these representations reveal facets of\ncommunities' distinctive identities, such as social group, topic, and\nattitudes. Two communities may have high text similarity but not user\nsimilarity or vice versa, and word usage also does not vary according to a\nclearcut, binary perspective of gender. Community-specific sentiment lexicons\ndemonstrate that sentiment can be a useful indicator of words' social meaning\nand community values, especially in the context of discussion content and user\ndemographics. Our results show that social platforms such as Reddit are active\nsettings for different constructions of gender.", "published": "2018-11-16 23:17:33", "link": "http://arxiv.org/abs/1811.07061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beam Search Decoding using Manner of Articulation Detection Knowledge\n  Derived from Connectionist Temporal Classification", "abstract": "Manner of articulation detection using deep neural networks require a priori\nknowledge of the attribute discriminative features or the decent phoneme\nalignments. However generating an appropriate phoneme alignment is complex and\nits performance depends on the choice of optimal number of senones, Gaussians,\netc. In the first part of our work, we exploit the manner of articulation\ndetection using connectionist temporal classification (CTC) which doesn't need\nany phoneme alignment. Later we modify the state-of-the-art character based\nposteriors generated by CTC using the manner of articulation CTC detector. Beam\nsearch decoding is performed on the modified posteriors and it's impact on open\nsource datasets such as AN4 and LibriSpeech is observed.", "published": "2018-11-16 04:40:03", "link": "http://arxiv.org/abs/1811.07720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Compositionality-Sensitivity of NLI Models", "abstract": "Success in natural language inference (NLI) should require a model to\nunderstand both lexical and compositional semantics. However, through\nadversarial evaluation, we find that several state-of-the-art models with\ndiverse architectures are over-relying on the former and fail to use the\nlatter. Further, this compositionality unawareness is not reflected via\nstandard evaluation on current datasets. We show that removing RNNs in existing\nmodels or shuffling input words during training does not induce large\nperformance loss despite the explicit removal of compositional information.\nTherefore, we propose a compositionality-sensitivity testing setup that\nanalyzes models on natural examples from existing datasets that cannot be\nsolved via lexical features alone (i.e., on which a bag-of-words model gives a\nhigh probability to one wrong label), hence revealing the models' actual\ncompositionality awareness. We show that this setup not only highlights the\nlimited compositional ability of current NLI models, but also differentiates\nmodel performance based on design, e.g., separating shallow bag-of-words models\nfrom deeper, linguistically-grounded tree-based models. Our evaluation setup is\nan important analysis tool: complementing currently existing adversarial and\nlinguistically driven diagnostic evaluations, and exposing opportunities for\nfuture work on evaluating models' compositional understanding.", "published": "2018-11-16 21:12:24", "link": "http://arxiv.org/abs/1811.07033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Fact Extraction and Verification with Neural Semantic Matching\n  Networks", "abstract": "The increasing concern with misinformation has stimulated research efforts on\nautomatic fact checking. The recently-released FEVER dataset introduced a\nbenchmark fact-verification task in which a system is asked to verify a claim\nusing evidential sentences from Wikipedia documents. In this paper, we present\na connected system consisting of three homogeneous neural semantic matching\nmodels that conduct document retrieval, sentence selection, and claim\nverification jointly for fact extraction and verification. For evidence\nretrieval (document retrieval and sentence selection), unlike traditional\nvector space IR models in which queries and sources are matched in some\npre-designed term vector space, we develop neural models to perform deep\nsemantic matching from raw textual input, assuming no intermediate term\nrepresentation and no access to structured external knowledge bases. We also\nshow that Pageview frequency can also help improve the performance of evidence\nretrieval results, that later can be matched by using our neural semantic\nmatching network. For claim verification, unlike previous approaches that\nsimply feed upstream retrieved evidence and the claim to a natural language\ninference (NLI) model, we further enhance the NLI model by providing it with\ninternal semantic relatedness scores (hence integrating it with the evidence\nretrieval modules) and ontological WordNet features. Experiments on the FEVER\ndataset indicate that (1) our neural semantic matching method outperforms\npopular TF-IDF and encoder models, by significant margins on all evidence\nretrieval metrics, (2) the additional relatedness score and WordNet features\nimprove the NLI model via better semantic awareness, and (3) by formalizing all\nthree subtasks as a similar semantic matching problem and improving on all\nthree stages, the complete model is able to achieve the state-of-the-art\nresults on the FEVER test set.", "published": "2018-11-16 21:37:59", "link": "http://arxiv.org/abs/1811.07039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Paper Summary Generation from Visual and Textual Information", "abstract": "Due to the recent boom in artificial intelligence (AI) research, including\ncomputer vision (CV), it has become impossible for researchers in these fields\nto keep up with the exponentially increasing number of manuscripts. In response\nto this situation, this paper proposes the paper summary generation (PSG) task\nusing a simple but effective method to automatically generate an academic paper\nsummary from raw PDF data. We realized PSG by combination of vision-based\nsupervised components detector and language-based unsupervised important\nsentence extractor, which is applicable for a trained format of manuscripts. We\nshow the quantitative evaluation of ability of simple vision-based components\nextraction, and the qualitative evaluation that our system can extract both\nvisual item and sentence that are helpful for understanding. After processing\nvia our PSG, the 979 manuscripts accepted by the Conference on Computer Vision\nand Pattern Recognition (CVPR) 2018 are available. It is believed that the\nproposed method will provide a better way for researchers to stay caught with\nimportant academic papers.", "published": "2018-11-16 17:52:25", "link": "http://arxiv.org/abs/1811.06943v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Investigating the Effects of Word Substitution Errors on Sentence\n  Embeddings", "abstract": "A key initial step in several natural language processing (NLP) tasks\ninvolves embedding phrases of text to vectors of real numbers that preserve\nsemantic meaning. To that end, several methods have been recently proposed with\nimpressive results on semantic similarity tasks. However, all of these\napproaches assume that perfect transcripts are available when generating the\nembeddings. While this is a reasonable assumption for analysis of written text,\nit is limiting for analysis of transcribed text. In this paper we investigate\nthe effects of word substitution errors, such as those coming from automatic\nspeech recognition errors (ASR), on several state-of-the-art sentence embedding\nmethods. To do this, we propose a new simulator that allows the experimenter to\ninduce ASR-plausible word substitution errors in a corpus at a desired word\nerror rate. We use this simulator to evaluate the robustness of several\nsentence embedding methods. Our results show that pre-trained neural sentence\nencoders are both robust to ASR errors and perform well on textual similarity\ntasks after errors are introduced. Meanwhile, unweighted averages of word\nvectors perform well with perfect transcriptions, but their performance\ndegrades rapidly on textual similarity tasks for text with word substitution\nerrors.", "published": "2018-11-16 20:25:23", "link": "http://arxiv.org/abs/1811.07021v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Voice Controlled E-Commerce Web Application", "abstract": "Automatic voice-controlled systems have changed the way humans interact with\na computer. Voice or speech recognition systems allow a user to make a\nhands-free request to the computer, which in turn processes the request and\nserves the user with appropriate responses. After years of research and\ndevelopments in machine learning and artificial intelligence, today\nvoice-controlled technologies have become more efficient and are widely applied\nin many domains to enable and improve human-to-human and human-to-computer\ninteractions. The state-of-the-art e-commerce applications with the help of web\ntechnologies offer interactive and user-friendly interfaces. However, there are\nsome instances where people, especially with visual disabilities, are not able\nto fully experience the serviceability of such applications. A voice-controlled\nsystem embedded in a web application can enhance user experience and can\nprovide voice as a means to control the functionality of e-commerce websites.\nIn this paper, we propose a taxonomy of speech recognition systems (SRS) and\npresent a voice-controlled commodity purchase e-commerce application using IBM\nWatson speech-to-text to demonstrate its usability. The prototype can be\nextended to other application scenarios such as government service kiosks and\nenable analytics of the converted text data for scenarios such as medical\ndiagnosis at the clinics.", "published": "2018-11-16 01:35:09", "link": "http://arxiv.org/abs/1811.09688v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "stat.ML", "68T10"], "primary_category": "cs.CY"}
{"title": "Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment\n  Classification", "abstract": "Aspect-level sentiment classification (ASC) aims at identifying sentiment\npolarities towards aspects in a sentence, where the aspect can behave as a\ngeneral Aspect Category (AC) or a specific Aspect Term (AT). However, due to\nthe especially expensive and labor-intensive labeling, existing public corpora\nin AT-level are all relatively small. Meanwhile, most of the previous methods\nrely on complicated structures with given scarce data, which largely limits the\nefficacy of the neural models. In this paper, we exploit a new direction named\ncoarse-to-fine task transfer, which aims to leverage knowledge learned from a\nrich-resource source domain of the coarse-grained AC task, which is more easily\naccessible, to improve the learning in a low-resource target domain of the\nfine-grained AT task. To resolve both the aspect granularity inconsistency and\nfeature mismatch between domains, we propose a Multi-Granularity Alignment\nNetwork (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary\ntask can help the AC task modeling at the same fine-grained level with the AT\ntask. To alleviate the feature false alignment, a contrastive feature alignment\nmethod is adopted to align aspect-specific feature representations\nsemantically. In addition, a large-scale multi-domain dataset for the AC task\nis provided. Empirically, extensive experiments demonstrate the effectiveness\nof the MGAN.", "published": "2018-11-16 07:09:30", "link": "http://arxiv.org/abs/1811.10999v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Building and Evaluation of a Real Room Impulse Response Dataset", "abstract": "This paper presents BUT ReverbDB - a dataset of real room impulse responses\n(RIR), background noises and re-transmitted speech data. The retransmitted data\nincludes LibriSpeech test-clean, 2000 HUB5 English evaluation and part of 2010\nNIST Speaker Recognition Evaluation datasets. We provide a detailed description\nof RIR collection (hardware, software, post-processing) that can serve as a\n\"cook-book\" for similar efforts. We also validate BUT ReverbDB in two sets of\nautomatic speech recognition (ASR) experiments and draw conclusions for\naugmenting ASR training data with real and artificially generated RIRs. We show\nthat a limited number of real RIRs, carefully selected to match the target\nenvironment, provide results comparable to a large number of artificially\ngenerated RIRs, and that both sets can be combined to achieve the best ASR\nresults. The dataset is distributed for free under a non-restrictive license\nand it currently contains data from 8 rooms, which is growing. The distribution\npackage also contains a Kaldi-based recipe for augmenting publicly available\nAMI close-talk meeting data and test the results on an AMI single distant\nmicrophone set, allowing it to reproduce our experiments.", "published": "2018-11-16 13:18:14", "link": "http://arxiv.org/abs/1811.06795v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Generating Albums with SampleRNN to Imitate Metal, Rock, and Punk Bands", "abstract": "This early example of neural synthesis is a proof-of-concept for how machine\nlearning can drive new types of music software. Creating music can be as simple\nas specifying a set of music influences on which a model trains. We demonstrate\na method for generating albums that imitate bands in experimental music genres\npreviously unrealized by traditional synthesis techniques (e.g. additive,\nsubtractive, FM, granular, concatenative). Raw audio is generated\nautoregressively in the time-domain using an unconditional SampleRNN. We create\nsix albums this way. Artwork and song titles are also generated using materials\nfrom the original artists' back catalog as training data. We try a\nfully-automated method and a human-curated method. We discuss its potential for\nmachine-assisted production.", "published": "2018-11-16 00:04:16", "link": "http://arxiv.org/abs/1811.06633v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Black Metal and Math Rock: Beyond Bach, Beethoven, and\n  Beatles", "abstract": "We use a modified SampleRNN architecture to generate music in modern genres\nsuch as black metal and math rock. Unlike MIDI and symbolic models, SampleRNN\ngenerates raw audio in the time domain. This requirement becomes increasingly\nimportant in modern music styles where timbre and space are used\ncompositionally. Long developmental compositions with rapid transitions between\nsections are possible by increasing the depth of the network beyond the number\nused for speech datasets. We are delighted by the unique characteristic\nartifacts of neural synthesis.", "published": "2018-11-16 00:54:05", "link": "http://arxiv.org/abs/1811.06639v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direction of Arrival Estimation of Wide-band Signals with Planar\n  Microphone Arrays", "abstract": "An approach to the estimation of the Direction of Arrival (DOA) of wide-band\nsignals with a planar microphone array is presented. Our algorithm estimates an\nunambiguous DOA using a single planar array in which the microphones are placed\nfairly close together and the sound source is expected to be in the far field.\nThe algorithm uses the ambiguous DOA estimates obtained from microphone pairs\nin the array to determine an unambiguous DOA estimate for the array as a whole.\nThe required pair-wise DOAs may be calculated using Time Delay Estimations\n(TDEs), which may in turn be calculated using cross-correlation, making the\nalgorithm suitable for wide-band signals. No a priori knowledge of the true\nSound Source Location (SSL) is required. Simulations show that the algorithm is\nrobust against noise in the input data. An average ratio of approximately 3:1\nexists between the input DOA errors and the output DOA error. Field tests with\na moving sound source provided DOA estimates with standard deviations between\n20.4 and 15.2 degrees.", "published": "2018-11-16 11:22:00", "link": "http://arxiv.org/abs/1811.06756v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundSignaling: Realtime, Stylistic Modification of a Personal Music\n  Corpus for Information Delivery", "abstract": "Drawing inspiration from the notion of cognitive incongruence associated with\nStroop's famous experiment, from musical principles, and from the observation\nthat music consumption on an individual basis is becoming increasingly\nubiquitous, we present the SoundSignaling system -- a software platform\ndesigned to make real-time, stylistically relevant modifications to a personal\ncorpus of music as a means of conveying information or notifications. In this\nwork, we discuss in detail the system's technical implementation and its\nmotivation from a musical perspective, and validate these design choices\nthrough a crowd-sourced signal identification experiment consisting of 200\nindependent tasks performed by 50 online participants. We then qualitatively\ndiscuss the potential implications of such a system from the standpoint of\nswitch cost, cognitive load, and listening behavior by considering the\nanecdotal outcomes of a small-scale, in-the-wild experiment consisting of over\n180 hours of usage from 6 participants. Through this work, we suggest a\nre-evaluation of the age-old paradigm of binary audio notifications in favor of\na system designed to operate upon the relatively unexplored medium of a user's\nmusical preferences.", "published": "2018-11-16 15:32:28", "link": "http://arxiv.org/abs/1811.06859v1", "categories": ["eess.AS", "cs.IR"], "primary_category": "eess.AS"}
{"title": "Exploring Tradeoffs in Models for Low-latency Speech Enhancement", "abstract": "We explore a variety of neural networks configurations for one- and\ntwo-channel spectrogram-mask-based speech enhancement. Our best model improves\non previous state-of-the-art performance on the CHiME2 speech enhancement task\nby 0.4 decibels in signal-to-distortion ratio (SDR). We examine trade-offs such\nas non-causal look-ahead, computation, and parameter count versus enhancement\nperformance and find that zero-look-ahead models can achieve, on average,\nwithin 0.03 dB SDR of our best bidirectional model. Further, we find that 200\nmilliseconds of look-ahead is sufficient to achieve equivalent performance to\nour best bidirectional model.", "published": "2018-11-16 20:49:15", "link": "http://arxiv.org/abs/1811.07030v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semi-supervised multichannel speech enhancement with variational\n  autoencoders and non-negative matrix factorization", "abstract": "In this paper we address speaker-independent multichannel speech enhancement\nin unknown noisy environments. Our work is based on a well-established\nmultichannel local Gaussian modeling framework. We propose to use a neural\nnetwork for modeling the speech spectro-temporal content. The parameters of\nthis supervised model are learned using the framework of variational\nautoencoders. The noisy recording environment is supposed to be unknown, so the\nnoise spectro-temporal modeling remains unsupervised and is based on\nnon-negative matrix factorization (NMF). We develop a Monte Carlo\nexpectation-maximization algorithm and we experimentally show that the proposed\napproach outperforms its NMF-based counterpart, where speech is modeled using\nsupervised NMF.", "published": "2018-11-16 09:11:07", "link": "http://arxiv.org/abs/1811.06713v3", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Stochastic Adaptive Neural Architecture Search for Keyword Spotting", "abstract": "The problem of keyword spotting i.e. identifying keywords in a real-time\naudio stream is mainly solved by applying a neural network over successive\nsliding windows. Due to the difficulty of the task, baseline models are usually\nlarge, resulting in a high computational cost and energy consumption level. We\npropose a new method called SANAS (Stochastic Adaptive Neural Architecture\nSearch) which is able to adapt the architecture of the neural network\non-the-fly at inference time such that small architectures will be used when\nthe stream is easy to process (silence, low noise, ...) and bigger networks\nwill be used when the task becomes more difficult. We show that this adaptive\nmodel can be learned end-to-end by optimizing a trade-off between the\nprediction performance and the average computational cost per unit of time.\nExperiments on the Speech Commands dataset show that this approach leads to a\nhigh recognition level while being much faster (and/or energy saving) than\nclassical approaches where the network architecture is static.", "published": "2018-11-16 11:08:26", "link": "http://arxiv.org/abs/1811.06753v1", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Using recurrences in time and frequency within U-net architecture for\n  speech enhancement", "abstract": "When designing fully-convolutional neural network, there is a trade-off\nbetween receptive field size, number of parameters and spatial resolution of\nfeatures in deeper layers of the network. In this work we present a novel\nnetwork design based on combination of many convolutional and recurrent layers\nthat solves these dilemmas. We compare our solution with U-nets based models\nknown from the literature and other baseline models on speech enhancement task.\nWe test our solution on TIMIT speech utterances combined with noise segments\nextracted from NOISEX-92 database and show clear advantage of proposed solution\nin terms of SDR (signal-to-distortion ratio), SIR (signal-to-interference\nratio) and STOI (spectro-temporal objective intelligibility) metrics compared\nto the current state-of-the-art.", "published": "2018-11-16 13:58:42", "link": "http://arxiv.org/abs/1811.06805v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "John, the semi-conductor : a tool for comprovisation", "abstract": "This article presents \"John\", an open-source software designed to help\ncollective free improvisation. It provides generated screen-scores running on\ndistributed, reactive web-browsers. The musicians can then concurrently edit\nthe scores in their own browser. John is used by ONE, a septet playing\nimprovised electro-acoustic music with digital musical instruments (DMI). One\nof the original features of John is that its design takes care of leaving the\nmusician's attention as free as possible. Firstly, a quick review of the\ncontext of screen-based scores will help situate this research in the history\nof contemporary music notation. Then I will trace back how improvisation\nsessions led to John's particular \"notational perspective\". A brief description\nof the software will precede a discussion about the various aspects guiding its\ndesign.", "published": "2018-11-16 15:31:12", "link": "http://arxiv.org/abs/1811.06858v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Protecting Voice Controlled Systems Using Sound Source Identification\n  Based on Acoustic Cues", "abstract": "Over the last few years, a rapidly increasing number of Internet-of-Things\n(IoT) systems that adopt voice as the primary user input have emerged. These\nsystems have been shown to be vulnerable to various types of voice spoofing\nattacks. Existing defense techniques can usually only protect from a specific\ntype of attack or require an additional authentication step that involves\nanother device. Such defense strategies are either not strong enough or lower\nthe usability of the system. Based on the fact that legitimate voice commands\nshould only come from humans rather than a playback device, we propose a novel\ndefense strategy that is able to detect the sound source of a voice command\nbased on its acoustic features. The proposed defense strategy does not require\nany information other than the voice command itself and can protect a system\nfrom multiple types of spoofing attacks. Our proof-of-concept experiments\nverify the feasibility and effectiveness of this defense strategy.", "published": "2018-11-16 20:13:25", "link": "http://arxiv.org/abs/1811.07018v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
