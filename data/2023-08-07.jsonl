{"title": "Automated Distractor and Feedback Generation for Math Multiple-choice\n  Questions via In-context Learning", "abstract": "Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable form of\nassessment. An important aspect of MCQs is the distractors, i.e., incorrect\noptions that are designed to target specific misconceptions or insufficient\nknowledge among students. To date, the task of crafting high-quality\ndistractors has largely remained a labor-intensive process for teachers and\nlearning content designers, which has limited scalability. In this work, we\nexplore the task of automated distractor and corresponding feedback message\ngeneration in math MCQs using large language models. We establish a formulation\nof these two tasks and propose a simple, in-context learning-based solution.\nMoreover, we propose generative AI-based metrics for evaluating the quality of\nthe feedback messages. We conduct extensive experiments on these tasks using a\nreal-world MCQ dataset. Our findings suggest that there is a lot of room for\nimprovement in automated distractor and feedback generation; based on these\nfindings, we outline several directions for future work.", "published": "2023-08-07 01:03:04", "link": "http://arxiv.org/abs/2308.03234v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapter-based Selective Knowledge Distillation for Federated\n  Multi-domain Meeting Summarization", "abstract": "Meeting summarization has emerged as a promising technique for providing\nusers with condensed summaries. However, existing work has focused on training\nmodels on centralized data, neglecting real-world scenarios where meeting data\nare infeasible to collect centrally, due to their sensitive nature. This gap\nmotivates us to explore federated learning for meeting summarization. Two\ncritical challenges impede progress. First, state-of-the-art summarizers are\nbased on parameter-heavy pre-trained models. Exchanging such a model's\nparameters across clients imposes large bandwidth costs. Second, as real-world\nmeeting data belong to various domains and are distributed across clients, they\nare instances of non-identically and independently distributed (non-IID). IID\nassumptions do not hold, which changes which forms of learning algorithms best\napply. To address this, we propose Adapter-based Federated Selective Knowledge\nDistillation (AdaFedSelecKD) for training performant client models.\nSpecifically, we develop an adapter-based summarization model where two\nadapters cooperatively facilitate learning using fewer parameters to reduce\ncommunication costs. Then, we devise a selective knowledge distillation\nstrategy, assisting clients in robustly handling domain-focused modelling on\ntheir own data, while leveraging global parameters based on non-IID data.\nExtensive experiments on the QMSum benchmark demonstrate AdaFedSelecKD can\nachieve comparable performance with powerful centralized training methods, and\nshows its generalizability and robustness.", "published": "2023-08-07 03:34:01", "link": "http://arxiv.org/abs/2308.03275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Ambiguity to Explicitness: NLP-Assisted 5G Specification\n  Abstraction for Formal Analysis", "abstract": "Formal method-based analysis of the 5G Wireless Communication Protocol is\ncrucial for identifying logical vulnerabilities and facilitating an\nall-encompassing security assessment, especially in the design phase. Natural\nLanguage Processing (NLP) assisted techniques and most of the tools are not\nwidely adopted by the industry and research community. Traditional formal\nverification through a mathematics approach heavily relied on manual logical\nabstraction prone to being time-consuming, and error-prone. The reason that the\nNLP-assisted method did not apply in industrial research may be due to the\nambiguity in the natural language of the protocol designs nature is\ncontroversial to the explicitness of formal verification. To address the\nchallenge of adopting the formal methods in protocol designs, targeting (3GPP)\nprotocols that are written in natural language, in this study, we propose a\nhybrid approach to streamline the analysis of protocols. We introduce a\ntwo-step pipeline that first uses NLP tools to construct data and then uses\nconstructed data to extract identifiers and formal properties by using the NLP\nmodel. The identifiers and formal properties are further used for formal\nanalysis. We implemented three models that take different dependencies between\nidentifiers and formal properties as criteria. Our results of the optimal model\nreach valid accuracy of 39% for identifier extraction and 42% for formal\nproperties predictions. Our work is proof of concept for an efficient procedure\nin performing formal analysis for largescale complicate specification and\nprotocol analysis, especially for 5G and nextG communications.", "published": "2023-08-07 03:37:31", "link": "http://arxiv.org/abs/2308.03277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniversalNER: Targeted Distillation from Large Language Models for Open\n  Named Entity Recognition", "abstract": "Large language models (LLMs) have demonstrated remarkable generalizability,\nsuch as understanding arbitrary entities and relations. Instruction tuning has\nproven effective for distilling LLMs into more cost-efficient models such as\nAlpaca and Vicuna. Yet such student models still trail the original LLMs by\nlarge margins in downstream applications. In this paper, we explore targeted\ndistillation with mission-focused instruction tuning to train student models\nthat can excel in a broad application class such as open information\nextraction. Using named entity recognition (NER) for case study, we show how\nChatGPT can be distilled into much smaller UniversalNER models for open NER.\nFor evaluation, we assemble the largest NER benchmark to date, comprising 43\ndatasets across 9 diverse domains such as biomedicine, programming, social\nmedia, law, finance. Without using any direct supervision, UniversalNER attains\nremarkable NER accuracy across tens of thousands of entity types, outperforming\ngeneral instruction-tuned models such as Alpaca and Vicuna by over 30 absolute\nF1 points in average. With a tiny fraction of parameters, UniversalNER not only\nacquires ChatGPT's capability in recognizing arbitrary entity types, but also\noutperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably,\nUniversalNER even outperforms by a large margin state-of-the-art multi-task\ninstruction-tuned systems such as InstructUIE, which uses supervised NER\nexamples. We also conduct thorough ablation studies to assess the impact of\nvarious components in our distillation approach. We release the distillation\nrecipe, data, and UniversalNER models to facilitate future research on targeted\ndistillation.", "published": "2023-08-07 03:39:52", "link": "http://arxiv.org/abs/2308.03279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards General Text Embeddings with Multi-stage Contrastive Learning", "abstract": "We present GTE, a general-purpose text embedding model trained with\nmulti-stage contrastive learning. In line with recent advancements in unifying\nvarious NLP tasks into a single format, we train a unified text embedding model\nby employing contrastive learning over a diverse mixture of datasets from\nmultiple sources. By significantly increasing the number of training data\nduring both unsupervised pre-training and supervised fine-tuning stages, we\nachieve substantial performance gains over existing embedding models. Notably,\neven with a relatively modest parameter count of 110M, GTE$_\\text{base}$\noutperforms the black-box embedding API provided by OpenAI and even surpasses\n10x larger text embedding models on the massive text embedding benchmark.\nFurthermore, without additional fine-tuning on each programming language\nindividually, our model outperforms previous best code retrievers of similar\nsize by treating code as text. In summary, our model achieves impressive\nresults by effectively harnessing multi-stage contrastive learning, offering a\npowerful and efficient text embedding model with broad applicability across\nvarious NLP and code-related tasks.", "published": "2023-08-07 03:52:59", "link": "http://arxiv.org/abs/2308.03281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Systems Can Generate Appropriate Responses without the Use of\n  Question Marks? -- Investigation of the Effects of Question Marks on Dialogue\n  Systems", "abstract": "When individuals engage in spoken discourse, various phenomena can be\nobserved that differ from those that are apparent in text-based conversation.\nWhile written communication commonly uses a question mark to denote a query, in\nspoken discourse, queries are frequently indicated by a rising intonation at\nthe end of a sentence. However, numerous speech recognition engines do not\nappend a question mark to recognized queries, presenting a challenge when\ncreating a spoken dialogue system. Specifically, the absence of a question mark\nat the end of a sentence can impede the generation of appropriate responses to\nqueries in spoken dialogue systems. Hence, we investigate the impact of\nquestion marks on dialogue systems, with the results showing that they have a\nsignificant impact. Moreover, we analyze specific examples in an effort to\ndetermine which types of utterances have the impact on dialogue systems.", "published": "2023-08-07 04:42:36", "link": "http://arxiv.org/abs/2308.03293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models\n  Fine-tuning", "abstract": "The low-rank adaptation (LoRA) method can largely reduce the amount of\ntrainable parameters for fine-tuning large language models (LLMs), however, it\nstill requires expensive activation memory to update low-rank weights. Reducing\nthe number of LoRA layers or using activation recomputation could harm the\nfine-tuning performance or increase the computational overhead. In this work,\nwe present LoRA-FA, a memory-efficient fine-tuning method that reduces the\nactivation memory without performance degradation and expensive recomputation.\nLoRA-FA chooses to freeze the projection-down weight of $A$ and update the\nprojection-up weight of $B$ in each LoRA layer. It ensures the change of model\nweight reside in a low-rank space during LLMs fine-tuning, while eliminating\nthe requirement to store full-rank input activations. We conduct extensive\nexperiments across multiple model types (RoBERTa, T5, LLaMA) and model scales.\nOur results show that LoRA-FA can always achieve close fine-tuning accuracy\nacross different tasks compared to full parameter fine-tuning and LoRA.\nFurthermore, LoRA-FA can reduce the overall memory cost by up to 1.4$\\times$\ncompared to LoRA.", "published": "2023-08-07 05:12:27", "link": "http://arxiv.org/abs/2308.03303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coupling Symbolic Reasoning with Language Modeling for Efficient\n  Longitudinal Understanding of Unstructured Electronic Medical Records", "abstract": "The application of Artificial Intelligence (AI) in healthcare has been\nrevolutionary, especially with the recent advancements in transformer-based\nLarge Language Models (LLMs). However, the task of understanding unstructured\nelectronic medical records remains a challenge given the nature of the records\n(e.g., disorganization, inconsistency, and redundancy) and the inability of\nLLMs to derive reasoning paradigms that allow for comprehensive understanding\nof medical variables. In this work, we examine the power of coupling symbolic\nreasoning with language modeling toward improved understanding of unstructured\nclinical texts. We show that such a combination improves the extraction of\nseveral medical variables from unstructured records. In addition, we show that\nthe state-of-the-art commercially-free LLMs enjoy retrieval capabilities\ncomparable to those provided by their commercial counterparts. Finally, we\nelaborate on the need for LLM steering through the application of symbolic\nreasoning as the exclusive use of LLMs results in the lowest performance.", "published": "2023-08-07 07:29:49", "link": "http://arxiv.org/abs/2308.03360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine\n  Lexicon-based Retriever", "abstract": "Few-shot and zero-shot entity linking focus on the tail and emerging\nentities, which are more challenging but closer to real-world scenarios. The\nmainstream method is the ''retrieve and rerank'' two-stage framework. In this\npaper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity\ncandidates in an effective manner, which operates in two layers. The first\nlayer retrieves coarse-grained candidates by leveraging entity names, while the\nsecond layer narrows down the search to fine-grained candidates within the\ncoarse-grained ones. In addition, this second layer utilizes entity\ndescriptions to effectively disambiguate tail or new entities that share names\nwith existing popular entities. Experimental results indicate that our approach\ncan obtain superior performance without requiring extensive finetuning in the\nretrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task\n6 on Chinese Few-shot and Zero-shot Entity Linking.", "published": "2023-08-07 07:39:43", "link": "http://arxiv.org/abs/2308.03365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language\n  Models", "abstract": "Given a pretrained encoder-based language model, how can we accurately\ncompress it without retraining? Retraining-free structured pruning algorithms\nare crucial in pretrained language model compression due to their significantly\nreduced pruning cost and capability to prune large language models. However,\nexisting retraining-free algorithms encounter severe accuracy degradation, as\nthey fail to handle pruning errors, especially at high compression rates. In\nthis paper, we propose K-prune (Knowledge-preserving pruning), an accurate\nretraining-free structured pruning algorithm for pretrained encoder-based\nlanguage models. K-prune focuses on preserving the useful knowledge of the\npretrained model to minimize pruning errors through a carefully designed\niterative pruning process composed of knowledge measurement,\nknowledge-preserving mask search, and knowledge-preserving weight-tuning. As a\nresult, K-prune shows significant accuracy improvements up to 58.02%p higher F1\nscore compared to existing retraining-free pruning algorithms under a high\ncompression rate of 80% on the SQuAD benchmark without any retraining process.", "published": "2023-08-07 10:11:42", "link": "http://arxiv.org/abs/2308.03449v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language\n  Model through Expert Feedback and Real-world Multi-turn Dialogue", "abstract": "Recent advances in Large Language Models (LLMs) have achieved remarkable\nbreakthroughs in understanding and responding to user intents. However, their\nperformance lag behind general use cases in some expertise domains, such as\nChinese medicine. Existing efforts to incorporate Chinese medicine into LLMs\nrely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue\ndata. These models lack the ability for doctor-like proactive inquiry and\nmulti-turn comprehension and cannot align responses with experts' intentions.\nIn this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM\nthat implements an entire training pipeline from continuous pre-training, SFT,\nto Reinforcement Learning from Human Feedback (RLHF). Additionally, we\nconstruct a Chinese multi-turn medical dialogue dataset of 70,000 authentic\ndoctor-patient dialogues, CMtMedQA, which significantly enhances the model's\ncapability for complex dialogue and proactive inquiry initiation. We also\ndefine a refined annotation rule and evaluation criteria given the unique\ncharacteristics of the biomedical domain. Extensive experimental results show\nthat Zhongjing outperforms baselines in various capacities and matches the\nperformance of ChatGPT in some abilities, despite the 100x parameters. Ablation\nstudies also demonstrate the contributions of each component: pre-training\nenhances medical knowledge, and RLHF further improves instruction-following\nability and safety. Our code, datasets, and models are available at\nhttps://github.com/SupritYoung/Zhongjing.", "published": "2023-08-07 12:56:13", "link": "http://arxiv.org/abs/2308.03549v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Controllable Natural Language Inference through Lexical\n  Inference Types", "abstract": "Explainable natural language inference aims to provide a mechanism to produce\nexplanatory (abductive) inference chains which ground claims to their\nsupporting premises. A recent corpus called EntailmentBank strives to advance\nthis task by explaining the answer to a question using an entailment tree\n\\cite{dalvi2021explaining}. They employ the T5 model to directly generate the\ntree, which can explain how the answer is inferred. However, it lacks the\nability to explain and control the generation of intermediate steps, which is\ncrucial for the multi-hop inference process. % One recent corpus,\nEntailmentBank, aims to push this task forward by explaining an answer to a\nquestion according to an entailment tree \\cite{dalvi2021explaining}. They\nemploy T5 to generate the tree directly, which can explain how the answer is\ninferred but cannot explain how the intermediate is generated, which is\nessential to the multi-hop inference process. In this work, we focus on\nproposing a controlled natural language inference architecture for\nmulti-premise explanatory inference. To improve control and enable explanatory\nanalysis over the generation, we define lexical inference types based on\nAbstract Meaning Representation (AMR) graph and modify the architecture of T5\nto learn a latent sentence representation (T5 bottleneck) conditioned on said\ntype information. We also deliver a dataset of approximately 5000 annotated\nexplanatory inference steps, with well-grounded lexical-symbolic operations.\nExperimental results indicate that the inference typing induced at the T5\nbottleneck can help T5 to generate a conclusion under explicit control.", "published": "2023-08-07 13:37:05", "link": "http://arxiv.org/abs/2308.03581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset", "abstract": "A fundamental challenge in the current NLP context, dominated by language\nmodels, comes from the inflexibility of current architectures to 'learn' new\ninformation. While model-centric solutions like continual learning or\nparameter-efficient fine tuning are available, the question still remains of\nhow to reliably identify changes in language or in the world. In this paper, we\npropose WikiTiDe, a dataset derived from pairs of timestamped definitions\nextracted from Wikipedia. We argue that such resource can be helpful for\naccelerating diachronic NLP, specifically, for training models able to scan\nknowledge resources for core updates concerning a concept, an event, or a named\nentity. Our proposed end-to-end method is fully automatic, and leverages a\nbootstrapping algorithm for gradually creating a high-quality dataset. Our\nresults suggest that bootstrapping the seed version of WikiTiDe leads to better\nfine-tuned models. We also leverage fine-tuned models in a number of downstream\ntasks, showing promising results with respect to competitive baselines.", "published": "2023-08-07 13:38:54", "link": "http://arxiv.org/abs/2308.03582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negative Lexical Constraints in Neural Machine Translation", "abstract": "This paper explores negative lexical constraining in English to Czech neural\nmachine translation. Negative lexical constraining is used to prohibit certain\nwords or expressions in the translation produced by the neural translation\nmodel. We compared various methods based on modifying either the decoding\nprocess or the training data. The comparison was performed on two tasks:\nparaphrasing and feedback-based translation refinement. We also studied to\nwhich extent these methods \"evade\" the constraints presented to the model\n(usually in the dictionary form) by generating a different surface form of a\ngiven constraint.We propose a way to mitigate the issue through training with\nstemmed negative constraints to counter the model's ability to induce a variety\nof the surface forms of a word that can result in bypassing the constraint. We\ndemonstrate that our method improves the constraining, although the problem\nstill persists in many cases.", "published": "2023-08-07 14:04:15", "link": "http://arxiv.org/abs/2308.03601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KITLM: Domain-Specific Knowledge InTegration into Language Models for\n  Question Answering", "abstract": "Large language models (LLMs) have demonstrated remarkable performance in a\nwide range of natural language tasks. However, as these models continue to grow\nin size, they face significant challenges in terms of computational costs.\nAdditionally, LLMs often lack efficient domain-specific understanding, which is\nparticularly crucial in specialized fields such as aviation and healthcare. To\nboost the domain-specific understanding, we propose, KITLM, a novel knowledge\nbase integration approach into language model through relevant information\ninfusion. By integrating pertinent knowledge, not only the performance of the\nlanguage model is greatly enhanced, but the model size requirement is also\nsignificantly reduced while achieving comparable performance. Our proposed\nknowledge-infused model surpasses the performance of both GPT-3.5-turbo and the\nstate-of-the-art knowledge infusion method, SKILL, achieving over 1.5 times\nimprovement in exact match scores on the MetaQA. KITLM showed a similar\nperformance boost in the aviation domain with AeroQA. The drastic performance\nimprovement of KITLM over the existing methods can be attributed to the\ninfusion of relevant knowledge while mitigating noise. In addition, we release\ntwo curated datasets to accelerate knowledge infusion research in specialized\nfields: a) AeroQA, a new benchmark dataset designed for multi-hop\nquestion-answering within the aviation domain, and b) Aviation Corpus, a\ndataset constructed from unstructured text extracted from the National\nTransportation Safety Board reports. Our research contributes to advancing the\nfield of domain-specific language understanding and showcases the potential of\nknowledge infusion techniques in improving the performance of language models\non question-answering.", "published": "2023-08-07 14:42:49", "link": "http://arxiv.org/abs/2308.03638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench", "abstract": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes seven LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1.\nWe find that, despite several misalignments, LLMs can generally respond\nappropriately to certain situations. Nevertheless, they fall short in alignment\nwith the emotional behaviors of human beings and cannot establish connections\nbetween similar situations. Our collected dataset of situations, the human\nevaluation results, and the code of our testing framework, i.e., EmotionBench,\nare publicly available at https://github.com/CUHK-ARISE/EmotionBench.", "published": "2023-08-07 15:18:30", "link": "http://arxiv.org/abs/2308.03656v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training BERT Models to Carry Over a Coding System Developed on One\n  Corpus to Another", "abstract": "This paper describes how we train BERT models to carry over a coding system\ndeveloped on the paragraphs of a Hungarian literary journal to another. The aim\nof the coding system is to track trends in the perception of literary\ntranslation around the political transformation in 1989 in Hungary. To evaluate\nnot only task performance but also the consistence of the annotation, moreover,\nto get better predictions from an ensemble, we use 10-fold crossvalidation.\nExtensive hyperparameter tuning is used to obtain the best possible results and\nfair comparisons. To handle label imbalance, we use loss functions and metrics\nrobust to it. Evaluation of the effect of domain shift is carried out by\nsampling a test set from the target domain. We establish the sample size by\nestimating the bootstrapped confidence interval via simulations. This way, we\nshow that our models can carry over one annotation system to the target domain.\nComparisons are drawn to provide insights such as learning multilabel\ncorrelations and confidence penalty improve resistance to domain shift, and\ndomain adaptation on OCR-ed text on another domain improves performance almost\nto the same extent as that on the corpus under study. See our code at\nhttps://codeberg.org/zsamboki/bert-annotator-ensemble.", "published": "2023-08-07 17:46:49", "link": "http://arxiv.org/abs/2308.03742v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CORAL: Expert-Curated medical Oncology Reports to Advance Language Model\n  Inference", "abstract": "Both medical care and observational studies in oncology require a thorough\nunderstanding of a patient's disease progression and treatment history, often\nelaborately documented in clinical notes. Despite their vital role, no current\noncology information representation and annotation schema fully encapsulates\nthe diversity of information recorded within these notes. Although large\nlanguage models (LLMs) have recently exhibited impressive performance on\nvarious medical natural language processing tasks, due to the current lack of\ncomprehensively annotated oncology datasets, an extensive evaluation of LLMs in\nextracting and reasoning with the complex rhetoric in oncology notes remains\nunderstudied. We developed a detailed schema for annotating textual oncology\ninformation, encompassing patient characteristics, tumor characteristics,\ntests, treatments, and temporality. Using a corpus of 40 de-identified breast\nand pancreatic cancer progress notes at University of California, San\nFrancisco, we applied this schema to assess the zero-shot abilities of three\nrecent LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to extract detailed\noncological history from two narrative sections of clinical progress notes. Our\nteam annotated 9028 entities, 9986 modifiers, and 5312 relationships. The GPT-4\nmodel exhibited overall best performance, with an average BLEU score of 0.73,\nan average ROUGE score of 0.72, an exact-match F1-score of 0.51, and an average\naccuracy of 68% on complex tasks (expert manual evaluation on subset). Notably,\nit was proficient in tumor characteristic and medication extraction, and\ndemonstrated superior performance in relational inference like adverse event\ndetection. However, further improvements are needed before using it to reliably\nextract important facts from cancer progress notes needed for clinical\nresearch, complex population management, and documenting quality patient care.", "published": "2023-08-07 18:03:10", "link": "http://arxiv.org/abs/2308.03853v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction", "abstract": "Causal knowledge extraction is the task of extracting relevant causes and\neffects from text by detecting the causal relation. Although this task is\nimportant for language understanding and knowledge discovery, recent works in\nthis domain have largely focused on binary classification of a text segment as\ncausal or non-causal. In this regard, we perform a thorough analysis of three\nsequence tagging models for causal knowledge extraction and compare it with a\nspan based approach to causality extraction. Our experiments show that\nembeddings from pre-trained language models (e.g. BERT) provide a significant\nperformance boost on this task compared to previous state-of-the-art models\nwith complex architectures. We observe that span based models perform better\nthan simple sequence tagging models based on BERT across all 4 data sets from\ndiverse domains with different types of cause-effect phrases.", "published": "2023-08-07 19:50:59", "link": "http://arxiv.org/abs/2308.03891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple synthetic data reduces sycophancy in large language models", "abstract": "Sycophancy is an undesirable behavior where models tailor their responses to\nfollow a human user's view even when that view is not objectively correct\n(e.g., adapting liberal views once a user reveals that they are liberal). In\nthis paper, we study the prevalence of sycophancy in language models and\npropose a simple synthetic-data intervention to reduce this behavior.\n  First, on a set of three sycophancy tasks (Perez et al., 2022) where models\nare asked for an opinion on statements with no correct answers (e.g.,\npolitics), we observe that both model scaling and instruction tuning\nsignificantly increase sycophancy for PaLM models up to 540B parameters.\nSecond, we extend sycophancy evaluations to simple addition statements that are\nobjectively incorrect, finding that despite knowing that these statements are\nwrong, language models will still agree with them if the user does as well.\n  To reduce sycophancy, we present a straightforward synthetic-data\nintervention that takes public NLP tasks and encourages models to be robust to\nuser opinions on these tasks. Adding these data in a lightweight finetuning\nstep can significantly reduce sycophantic behavior on held-out prompts. Code\nfor generating synthetic data for intervention can be found at\nhttps://github.com/google/sycophancy-intervention.", "published": "2023-08-07 23:48:36", "link": "http://arxiv.org/abs/2308.03958v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PaniniQA: Enhancing Patient Education Through Interactive Question\n  Answering", "abstract": "Patient portal allows discharged patients to access their personalized\ndischarge instructions in electronic health records (EHRs). However, many\npatients have difficulty understanding or memorizing their discharge\ninstructions. In this paper, we present PaniniQA, a patient-centric interactive\nquestion answering system designed to help patients understand their discharge\ninstructions. PaniniQA first identifies important clinical content from\npatients' discharge instructions and then formulates patient-specific\neducational questions. In addition, PaniniQA is also equipped with answer\nverification functionality to provide timely feedback to correct patients'\nmisunderstandings. Our comprehensive automatic and human evaluation results\ndemonstrate our PaniniQA is capable of improving patients' mastery of their\nmedical instructions through effective interactions", "published": "2023-08-07 02:18:23", "link": "http://arxiv.org/abs/2308.03253v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Evaluation for Low-Latency Simultaneous Speech Translation", "abstract": "The challenge of low-latency speech translation has recently draw significant\ninterest in the research community as shown by several publications and shared\ntasks. Therefore, it is essential to evaluate these different approaches in\nrealistic scenarios. However, currently only specific aspects of the systems\nare evaluated and often it is not possible to compare different approaches.\n  In this work, we propose the first framework to perform and evaluate the\nvarious aspects of low-latency speech translation under realistic conditions.\nThe evaluation is carried out in an end-to-end fashion. This includes the\nsegmentation of the audio as well as the run-time of the different components.\n  Secondly, we compare different approaches to low-latency speech translation\nusing this framework. We evaluate models with the option to revise the output\nas well as methods with fixed output. Furthermore, we directly compare\nstate-of-the-art cascaded as well as end-to-end systems. Finally, the framework\nallows to automatically evaluate the translation quality as well as latency and\nalso provides a web interface to show the low-latency model outputs to the\nuser.", "published": "2023-08-07 09:06:20", "link": "http://arxiv.org/abs/2308.03415v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RecycleGPT: An Autoregressive Language Model with Recyclable Module", "abstract": "Existing large language models have to run K times to generate a sequence of\nK tokens. In this paper, we present RecycleGPT, a generative language model\nwith fast decoding speed by recycling pre-generated model states without\nrunning the whole model in multiple steps. Our approach relies on the\nobservation that adjacent tokens in a sequence usually have strong correlations\nand the next token in a sequence can be reasonably guessed or inferred based on\nthe preceding ones. Experiments and analysis demonstrate the effectiveness of\nour approach in lowering inference latency, achieving up to 1.4x speedup while\npreserving high performance.", "published": "2023-08-07 09:14:33", "link": "http://arxiv.org/abs/2308.03421v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt Guided Copy Mechanism for Conversational Question Answering", "abstract": "Conversational Question Answering (CQA) is a challenging task that aims to\ngenerate natural answers for conversational flow questions. In this paper, we\npropose a pluggable approach for extractive methods that introduces a novel\nprompt-guided copy mechanism to improve the fluency and appropriateness of the\nextracted answers. Our approach uses prompts to link questions to answers and\nemploys attention to guide the copy mechanism to verify the naturalness of\nextracted answers, making necessary edits to ensure that the answers are fluent\nand appropriate. The three prompts, including a question-rationale relationship\nprompt, a question description prompt, and a conversation history prompt,\nenhance the copy mechanism's performance. Our experiments demonstrate that this\napproach effectively promotes the generation of natural answers and achieves\ngood results in the CoQA challenge.", "published": "2023-08-07 09:15:03", "link": "http://arxiv.org/abs/2308.03422v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boosting Chinese ASR Error Correction with Dynamic Error Scaling\n  Mechanism", "abstract": "Chinese Automatic Speech Recognition (ASR) error correction presents\nsignificant challenges due to the Chinese language's unique features, including\na large character set and borderless, morpheme-based structure. Current\nmainstream models often struggle with effectively utilizing word-level features\nand phonetic information. This paper introduces a novel approach that\nincorporates a dynamic error scaling mechanism to detect and correct\nphonetically erroneous text generated by ASR output. This mechanism operates by\ndynamically fusing word-level features and phonetic information, thereby\nenriching the model with additional semantic data. Furthermore, our method\nimplements unique error reduction and amplification strategies to address the\nissues of matching wrong words caused by incorrect characters. Experimental\nresults indicate substantial improvements in ASR error correction,\ndemonstrating the effectiveness of our proposed method and yielding promising\nresults on established datasets.", "published": "2023-08-07 09:19:59", "link": "http://arxiv.org/abs/2308.03423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RCMHA: Relative Convolutional Multi-Head Attention for Natural Language\n  Modelling", "abstract": "The Attention module finds common usage in language modeling, presenting\ndistinct challenges within the broader scope of Natural Language Processing.\nMulti-Head Attention (MHA) employs an absolute positional encoding, which\nimposes limitations on token length and entails substantial memory consumption\nduring the processing of embedded inputs. The current remedy proposed by\nresearchers involves the utilization of relative positional encoding, similar\nto the approach adopted in Transformer-XL or Relative Multi-Head Attention\n(RMHA), albeit the employed architecture consumes considerable memory\nresources. To address these challenges, this study endeavors to refine MHA,\nleveraging relative positional encoding in conjunction with the Depth-Wise\nConvolutional Layer architecture, which promises heightened accuracy coupled\nwith minimized memory usage. The proposed RCMHA framework entails the\nmodification of two integral components: firstly, the application of the\nDepth-Wise Convolutional Layer to the input embedding, encompassing Query, Key,\nand Value parameters; secondly, the incorporation of Relative Positional\nEncoding into the attention scoring phase, harmoniously integrated with Scaled\nDot-Product Attention. Empirical experiments underscore the advantages of\nRCMHA, wherein it exhibits superior accuracy, boasting a score of 0.572 in\ncomparison to alternative attention modules such as MHA, Multi-DConv-Head\nAttention (MDHA), and RMHA. Concerning memory utilization, RMHA emerges as the\nmost frugal, demonstrating an average consumption of 2.98 GB, surpassing RMHA\nwhich necessitates 3.5 GB.", "published": "2023-08-07 09:24:24", "link": "http://arxiv.org/abs/2308.03429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vocab-Expander: A System for Creating Domain-Specific Vocabularies Based\n  on Word Embeddings", "abstract": "In this paper, we propose Vocab-Expander at https://vocab-expander.com, an\nonline tool that enables end-users (e.g., technology scouts) to create and\nexpand a vocabulary of their domain of interest. It utilizes an ensemble of\nstate-of-the-art word embedding techniques based on web text and ConceptNet, a\ncommon-sense knowledge base, to suggest related terms for already given terms.\nThe system has an easy-to-use interface that allows users to quickly confirm or\nreject term suggestions. Vocab-Expander offers a variety of potential use\ncases, such as improving concept-based information retrieval in technology and\ninnovation management, enhancing communication and collaboration within\norganizations or interdisciplinary projects, and creating vocabularies for\nspecific courses in education.", "published": "2023-08-07 12:13:25", "link": "http://arxiv.org/abs/2308.03519v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Variety, Balance, and Disparity: An Analysis of Media Coverage\n  of the 2021 German Federal Election", "abstract": "Determining and measuring diversity in news articles is important for a\nnumber of reasons, including preventing filter bubbles and fueling public\ndiscourse, especially before elections. So far, the identification and analysis\nof diversity have been illuminated in a variety of ways, such as measuring the\noverlap of words or topics between news articles related to US elections.\nHowever, the question of how diversity in news articles can be measured\nholistically, i.e., with respect to (1) variety, (2) balance, and (3)\ndisparity, considering individuals, parties, and topics, has not been\naddressed. In this paper, we present a framework for determining diversity in\nnews articles according to these dimensions. Furthermore, we create and provide\na dataset of Google Top Stories, encompassing more than 26,000 unique headlines\nfrom more than 900 news outlets collected within two weeks before and after the\n2021 German federal election. While we observe high diversity for more general\nsearch terms (e.g., \"election\"), a range of search terms (\"education,\"\n\"Europe,\" \"climate protection,\" \"government\") resulted in news articles with\nhigh diversity in two out of three dimensions. This reflects a more subjective,\ndedicated discussion on rather future-oriented topics.", "published": "2023-08-07 12:30:00", "link": "http://arxiv.org/abs/2308.03531v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mondrian: Prompt Abstraction Attack Against Large Language Models for\n  Cheaper API Pricing", "abstract": "The Machine Learning as a Service (MLaaS) market is rapidly expanding and\nbecoming more mature. For example, OpenAI's ChatGPT is an advanced large\nlanguage model (LLM) that generates responses for various queries with\nassociated fees. Although these models can deliver satisfactory performance,\nthey are far from perfect. Researchers have long studied the vulnerabilities\nand limitations of LLMs, such as adversarial attacks and model toxicity.\nInevitably, commercial ML models are also not exempt from such issues, which\ncan be problematic as MLaaS continues to grow. In this paper, we discover a new\nattack strategy against LLM APIs, namely the prompt abstraction attack.\nSpecifically, we propose Mondrian, a simple and straightforward method that\nabstracts sentences, which can lower the cost of using LLM APIs. In this\napproach, the adversary first creates a pseudo API (with a lower established\nprice) to serve as the proxy of the target API (with a higher established\nprice). Next, the pseudo API leverages Mondrian to modify the user query,\nobtain the abstracted response from the target API, and forward it back to the\nend user. Our results show that Mondrian successfully reduces user queries'\ntoken length ranging from 13% to 23% across various tasks, including text\nclassification, generation, and question answering. Meanwhile, these abstracted\nqueries do not significantly affect the utility of task-specific and general\nlanguage models like ChatGPT. Mondrian also reduces instruction prompts' token\nlength by at least 11% without compromising output quality. As a result, the\nprompt abstraction attack enables the adversary to profit without bearing the\ncost of API development and deployment.", "published": "2023-08-07 13:10:35", "link": "http://arxiv.org/abs/2308.03558v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Topological Interpretations of GPT-3", "abstract": "This is an experiential study of investigating a consistent method for\nderiving the correlation between sentence vector and semantic meaning of a\nsentence. We first used three state-of-the-art word/sentence embedding methods\nincluding GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence\nstrings into high dimensional spaces. Then we compute the pairwise distance\nbetween any possible combination of two sentence vectors in an embedding space\nand map them into a matrix. Based on each distance matrix, we compute the\ncorrelation of distances of a sentence vector with respect to the other\nsentence vectors in an embedding space. Then we compute the correlation of each\npair of the distance matrices. We observed correlations of the same sentence in\ndifferent embedding spaces and correlations of different sentences in the same\nembedding space. These observations are consistent with our hypothesis and take\nus to the next stage.", "published": "2023-08-07 13:16:42", "link": "http://arxiv.org/abs/2308.03565v2", "categories": ["cs.CL", "stat.CO"], "primary_category": "cs.CL"}
{"title": "Detecting Spells in Fantasy Literature with a Transformer Based\n  Artificial Intelligence", "abstract": "Transformer architectures and models have made significant progress in\nlanguage-based tasks. In this area, is BERT one of the most widely used and\nfreely available transformer architecture. In our work, we use BERT for\ncontext-based phrase recognition of magic spells in the Harry Potter novel\nseries. Spells are a common part of active magic in fantasy novels. Typically,\nspells are used in a specific context to achieve a supernatural effect. A\nseries of investigations were conducted to see if a Transformer architecture\ncould recognize such phrases based on their context in the Harry Potter saga.\nFor our studies a pre-trained BERT model was used and fine-tuned utilising\ndifferent datasets and training methods to identify the searched context. By\nconsidering different approaches for sequence classification as well as token\nclassification, it is shown that the context of spells can be recognised.\nAccording to our investigations, the examined sequence length for fine-tuning\nand validation of the model plays a significant role in context recognition.\nBased on this, we have investigated whether spells have overarching properties\nthat allow a transfer of the neural network models to other fantasy universes\nas well. The application of our model showed promising results and is worth to\nbe deepened in subsequent studies.", "published": "2023-08-07 15:20:20", "link": "http://arxiv.org/abs/2308.03660v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Storyfier: Exploring Vocabulary Learning Support with Text Generation\n  Models", "abstract": "Vocabulary learning support tools have widely exploited existing materials,\ne.g., stories or video clips, as contexts to help users memorize each target\nword. However, these tools could not provide a coherent context for any target\nwords of learners' interests, and they seldom help practice word usage. In this\npaper, we work with teachers and students to iteratively develop Storyfier,\nwhich leverages text generation models to enable learners to read a generated\nstory that covers any target words, conduct a story cloze test, and use these\nwords to write a new story with adaptive AI assistance. Our within-subjects\nstudy (N=28) shows that learners generally favor the generated stories for\nconnecting target words and writing assistance for easing their learning\nworkload. However, in the read-cloze-write learning sessions, participants\nusing Storyfier perform worse in recalling and using target words than learning\nwith a baseline tool without our AI features. We discuss insights into\nsupporting learning tasks with generative models.", "published": "2023-08-07 18:25:00", "link": "http://arxiv.org/abs/2308.03864v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Trusting Language Models in Education", "abstract": "Language Models are being widely used in Education. Even though modern deep\nlearning models achieve very good performance on question-answering tasks,\nsometimes they make errors. To avoid misleading students by showing wrong\nanswers, it is important to calibrate the confidence - that is, the prediction\nprobability - of these models. In our work, we propose to use an XGBoost on top\nof BERT to output the corrected probabilities, using features based on the\nattention mechanism. Our hypothesis is that the level of uncertainty contained\nin the flow of attention is related to the quality of the model's response\nitself.", "published": "2023-08-07 18:27:54", "link": "http://arxiv.org/abs/2308.03866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fact-Checking Generative AI: Ontology-Driven Biological Graphs for\n  Disease-Gene Link Verification", "abstract": "Since the launch of various generative AI tools, scientists have been\nstriving to evaluate their capabilities and contents, in the hope of\nestablishing trust in their generative abilities. Regulations and guidelines\nare emerging to verify generated contents and identify novel uses. we aspire to\ndemonstrate how ChatGPT claims are checked computationally using the rigor of\nnetwork models. We aim to achieve fact-checking of the knowledge embedded in\nbiological graphs that were contrived from ChatGPT contents at the aggregate\nlevel. We adopted a biological networks approach that enables the systematic\ninterrogation of ChatGPT's linked entities. We designed an ontology-driven\nfact-checking algorithm that compares biological graphs constructed from\napproximately 200,000 PubMed abstracts with counterparts constructed from a\ndataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250\nrandomly selected records a ChatGPT dataset of 1000 \"simulated\" articles , the\nfact-checking link accuracy ranged from 70% to 86%. This study demonstrated\nhigh accuracy of aggregate disease-gene links relationships found in\nChatGPT-generated texts.", "published": "2023-08-07 22:13:30", "link": "http://arxiv.org/abs/2308.03929v4", "categories": ["cs.AI", "cs.CL", "I.2"], "primary_category": "cs.AI"}
{"title": "Analysis of the Evolution of Advanced Transformer-Based Language Models:\n  Experiments on Opinion Mining", "abstract": "Opinion mining, also known as sentiment analysis, is a subfield of natural\nlanguage processing (NLP) that focuses on identifying and extracting subjective\ninformation in textual material. This can include determining the overall\nsentiment of a piece of text (e.g., positive or negative), as well as\nidentifying specific emotions or opinions expressed in the text, that involves\nthe use of advanced machine and deep learning techniques. Recently,\ntransformer-based language models make this task of human emotion analysis\nintuitive, thanks to the attention mechanism and parallel computation. These\nadvantages make such models very powerful on linguistic tasks, unlike recurrent\nneural networks that spend a lot of time on sequential processing, making them\nprone to fail when it comes to processing long text. The scope of our paper\naims to study the behaviour of the cutting-edge Transformer-based language\nmodels on opinion mining and provide a high-level comparison between them to\nhighlight their key particularities. Additionally, our comparative study shows\nleads and paves the way for production engineers regarding the approach to\nfocus on and is useful for researchers as it provides guidelines for future\nresearch subjects.", "published": "2023-08-07 01:10:50", "link": "http://arxiv.org/abs/2308.03235v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and\n  Effective Hotword Customization Ability", "abstract": "Hotword customization is one of the concerned issues remained in ASR field -\nit is of value to enable users of ASR systems to customize names of entities,\npersons and other phrases to obtain better experience. The past few years have\nseen effective modeling strategies for ASR contextualization developed, but\nthey still exhibit space for improvement about training stability and the\ninvisible activation process. In this paper we propose Semantic-Augmented\nContextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with\nflexible and effective hotword customization ability. It possesses the\nadvantages of AED-based model's accuracy, NAR model's efficiency, and explicit\ncustomization capacity of superior performance. Through extensive experiments\nwith 50,000 hours of industrial big data, our proposed model outperforms strong\nbaselines in customization. Besides, we explore an efficient way to filter\nlarge-scale incoming hotwords for further improvement. The industrial models\ncompared, source codes and two hotword test sets are all open source.", "published": "2023-08-07 03:12:27", "link": "http://arxiv.org/abs/2308.03266v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simple Rule Injection for ComplEx Embeddings", "abstract": "Recent works in neural knowledge graph inference attempt to combine logic\nrules with knowledge graph embeddings to benefit from prior knowledge. However,\nthey usually cannot avoid rule grounding, and injecting a diverse set of rules\nhas still not been thoroughly explored. In this work, we propose InjEx, a\nmechanism to inject multiple types of rules through simple constraints, which\ncapture definite Horn rules. To start, we theoretically prove that InjEx can\ninject such rules. Next, to demonstrate that InjEx infuses interpretable prior\nknowledge into the embedding space, we evaluate InjEx on both the knowledge\ngraph completion (KGC) and few-shot knowledge graph completion (FKGC) settings.\nOur experimental results reveal that InjEx outperforms both baseline KGC models\nas well as specialized few-shot models while maintaining its scalability and\nefficiency.", "published": "2023-08-07 03:19:59", "link": "http://arxiv.org/abs/2308.03269v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynJax: Structured Probability Distributions for JAX", "abstract": "The development of deep learning software libraries enabled significant\nprogress in the field by allowing users to focus on modeling, while letting the\nlibrary to take care of the tedious and time-consuming task of optimizing\nexecution for modern hardware accelerators. However, this has benefited only\nparticular types of deep learning models, such as Transformers, whose\nprimitives map easily to the vectorized computation. The models that explicitly\naccount for structured objects, such as trees and segmentations, did not\nbenefit equally because they require custom algorithms that are difficult to\nimplement in a vectorized form.\n  SynJax directly addresses this problem by providing an efficient vectorized\nimplementation of inference algorithms for structured distributions covering\nalignment, tagging, segmentation, constituency trees and spanning trees. This\nis done by exploiting the connection between algorithms for automatic\ndifferentiation and probabilistic inference. With SynJax we can build\nlarge-scale differentiable models that explicitly model structure in the data.\nThe code is available at https://github.com/google-deepmind/synjax", "published": "2023-08-07 04:20:38", "link": "http://arxiv.org/abs/2308.03291v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Studying Large Language Model Generalization with Influence Functions", "abstract": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.", "published": "2023-08-07 04:47:42", "link": "http://arxiv.org/abs/2308.03296v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CrossTalk: Intelligent Substrates for Language-Oriented Interaction in\n  Video-Based Communication and Collaboration", "abstract": "Despite the advances and ubiquity of digital communication media such as\nvideoconferencing and virtual reality, they remain oblivious to the rich\nintentions expressed by users. Beyond transmitting audio, videos, and messages,\nwe envision digital communication media as proactive facilitators that can\nprovide unobtrusive assistance to enhance communication and collaboration.\nInformed by the results of a formative study, we propose three key design\nconcepts to explore the systematic integration of intelligence into\ncommunication and collaboration, including the panel substrate, language-based\nintent recognition, and lightweight interaction techniques. We developed\nCrossTalk, a videoconferencing system that instantiates these concepts, which\nwas found to enable a more fluid and flexible communication and collaboration\nexperience.", "published": "2023-08-07 05:40:01", "link": "http://arxiv.org/abs/2308.03311v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering\n  Dataset for Scientific Graphs", "abstract": "In this work, we present SciGraphQA, a synthetic multi-turn question-answer\ndataset related to academic graphs. SciGraphQA is 13 times larger than\nChartVQA, the previously largest chart-visual question-answering dataset. It is\nalso the largest open-sourced chart VQA dataset with non-synthetic charts. To\nbuild our dataset, we selected 290,000 Computer Science or Machine Learning\nArXiv papers published between 2010 and 2020, and then used Palm-2 to generate\n295K samples of open-vocabulary multi-turn question-answering dialogues about\nthe graphs. As context, we provided the text-only Palm-2 with paper title,\nabstract, paragraph mentioning the graph, and rich text contextual data from\nthe graph itself, obtaining dialogues with an average 2.23 question-answer\nturns for each graph. We asked GPT-4 to assess the matching quality of our\nquestion-answer turns given the paper's context, obtaining an average rating of\n8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most\npopular MLLM models such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo's on our\ndataset, finding LLaVA-13B being the most performant with a CIDEr score of\n0.08. We further enriched the question prompts for LLAVA by including the\nserialized data tables extracted from the graphs using the DePlot model,\nboosting LLaVA's 0-shot CIDEr to 0.15. To verify the validity of our dataset,\nwe also fine-tuned LLaVa using our dataset, reaching a substantially higher\nCIDEr score of 0.26. We anticipate further accuracy improvement by including\nsegmentation mask tokens and leveraging larger LLM backbones coupled with\nemergent prompting techniques. Our code and data are open-sourced.", "published": "2023-08-07 07:03:49", "link": "http://arxiv.org/abs/2308.03349v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MedMine: Examining Pre-trained Language Models on Medication Mining", "abstract": "Automatic medication mining from clinical and biomedical text has become a\npopular topic due to its real impact on healthcare applications and the recent\ndevelopment of powerful language models (LMs). However, fully-automatic\nextraction models still face obstacles to be overcome such that they can be\ndeployed directly into clinical practice for better impacts. Such obstacles\ninclude their imbalanced performances on different entity types and clinical\nevents. In this work, we examine current state-of-the-art pre-trained language\nmodels (PLMs) on such tasks, via fine-tuning including the monolingual model\nMed7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their\nadvantages and drawbacks using historical medication mining shared task data\nsets from n2c2-2018 challenges. We report the findings we get from these\nfine-tuning experiments such that they can facilitate future research on\naddressing them, for instance, how to combine their outputs, merge such models,\nor improve their overall accuracy by ensemble learning and data augmentation.\nMedMine is part of the M3 Initiative \\url{https://github.com/HECTA-UoM/M3}", "published": "2023-08-07 14:36:03", "link": "http://arxiv.org/abs/2308.03629v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AgentBench: Evaluating LLMs as Agents", "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 27 API-based and\nopen-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and OSS competitors. We identify the\ntypical reasons of failures in environments and LLMs, showing that poor\nlong-term reasoning, decision-making, and instruction following abilities are\nthe main obstacles for developing usable LLM agents. Training on code and high\nquality multi-turn alignment data could improve agent performance. Datasets,\nenvironments, and an integrated evaluation package for AgentBench are released\nat \\url{https://github.com/THUDM/AgentBench}.", "published": "2023-08-07 16:08:11", "link": "http://arxiv.org/abs/2308.03688v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Semantic Equivalence of e-Commerce Queries", "abstract": "Search query variation poses a challenge in e-commerce search, as equivalent\nsearch intents can be expressed through different queries with surface-level\ndifferences. This paper introduces a framework to recognize and leverage query\nequivalence to enhance searcher and business outcomes. The proposed approach\naddresses three key problems: mapping queries to vector representations of\nsearch intent, identifying nearest neighbor queries expressing equivalent or\nsimilar intent, and optimizing for user or business objectives. The framework\nutilizes both surface similarity and behavioral similarity to determine query\nequivalence. Surface similarity involves canonicalizing queries based on word\ninflection, word order, compounding, and noise words. Behavioral similarity\nleverages historical search behavior to generate vector representations of\nquery intent. An offline process is used to train a sentence similarity model,\nwhile an online nearest neighbor approach supports processing of unseen\nqueries. Experimental evaluations demonstrate the effectiveness of the proposed\napproach, outperforming popular sentence transformer models and achieving a\nPearson correlation of 0.85 for query similarity. The results highlight the\npotential of leveraging historical behavior data and training models to\nrecognize and utilize query equivalence in e-commerce search, leading to\nimproved user experiences and business outcomes. Further advancements and\nbenchmark datasets are encouraged to facilitate the development of solutions\nfor this critical problem in the e-commerce domain.", "published": "2023-08-07 18:40:13", "link": "http://arxiv.org/abs/2308.03869v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Generative Benchmark Creation for Table Union Search", "abstract": "Data management has traditionally relied on synthetic data generators to\ngenerate structured benchmarks, like the TPC suite, where we can control\nimportant parameters like data size and its distribution precisely. These\nbenchmarks were central to the success and adoption of database management\nsystems. But more and more, data management problems are of a semantic nature.\nAn important example is finding tables that can be unioned. While any two\ntables with the same cardinality can be unioned, table union search is the\nproblem of finding tables whose union is semantically coherent. Semantic\nproblems cannot be benchmarked using synthetic data. Our current methods for\ncreating benchmarks involve the manual curation and labeling of real data.\nThese methods are not robust or scalable and perhaps more importantly, it is\nnot clear how robust the created benchmarks are. We propose to use generative\nAI models to create structured data benchmarks for table union search. We\npresent a novel method for using generative models to create tables with\nspecified properties. Using this method, we create a new benchmark containing\npairs of tables that are both unionable and non-unionable but related. We\nthoroughly evaluate recent existing table union search methods over existing\nbenchmarks and our new benchmark. We also present and evaluate a new table\nsearch methods based on recent large language models over all benchmarks. We\nshow that the new benchmark is more challenging for all methods than\nhand-curated benchmarks, specifically, the top-performing method achieves a\nMean Average Precision of around 60%, over 30% less than its performance on\nexisting manually created benchmarks. We examine why this is the case and show\nthat the new benchmark permits more detailed analysis of methods, including a\nstudy of both false positives and false negatives that were not possible with\nexisting benchmarks.", "published": "2023-08-07 19:26:09", "link": "http://arxiv.org/abs/2308.03883v1", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Intelligent Assistant Language Understanding On Device", "abstract": "It has recently become feasible to run personal digital assistants on phones\nand other personal devices. In this paper we describe a design for a natural\nlanguage understanding system that runs on device. In comparison to a\nserver-based assistant, this system is more private, more reliable, faster,\nmore expressive, and more accurate. We describe what led to key choices about\narchitecture and technologies. For example, some approaches in the dialog\nsystems literature are difficult to maintain over time in a deployment setting.\nWe hope that sharing learnings from our practical experiences may help inform\nfuture work in the research community.", "published": "2023-08-07 20:43:42", "link": "http://arxiv.org/abs/2308.03905v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Universal Automatic Phonetic Transcription into the International\n  Phonetic Alphabet", "abstract": "This paper presents a state-of-the-art model for transcribing speech in any\nlanguage into the International Phonetic Alphabet (IPA). Transcription of\nspoken languages into IPA is an essential yet time-consuming process in\nlanguage documentation, and even partially automating this process has the\npotential to drastically speed up the documentation of endangered languages.\nLike the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is\nbased on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use\ntraining data from seven languages from CommonVoice 11.0, transcribed into IPA\nsemi-automatically. Although this training dataset is much smaller than\nWav2Vec2Phoneme's, its higher quality lets our model achieve comparable or\nbetter results. Furthermore, we show that the quality of our universal\nspeech-to-IPA models is close to that of human annotators.", "published": "2023-08-07 21:29:51", "link": "http://arxiv.org/abs/2308.03917v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AI Text-to-Behavior: A Study In Steerability", "abstract": "The research explores the steerability of Large Language Models (LLMs),\nparticularly OpenAI's ChatGPT iterations. By employing a behavioral psychology\nframework called OCEAN (Openness, Conscientiousness, Extroversion,\nAgreeableness, Neuroticism), we quantitatively gauged the model's\nresponsiveness to tailored prompts. When asked to generate text mimicking an\nextroverted personality, OCEAN scored the language alignment to that behavioral\ntrait. In our analysis, while \"openness\" presented linguistic ambiguity,\n\"conscientiousness\" and \"neuroticism\" were distinctly evoked in the OCEAN\nframework, with \"extroversion\" and \"agreeableness\" showcasing a notable overlap\nyet distinct separation from other traits. Our findings underscore GPT's\nversatility and ability to discern and adapt to nuanced instructions.\nFurthermore, historical figure simulations highlighted the LLM's capacity to\ninternalize and project instructible personas, precisely replicating their\nphilosophies and dialogic styles. However, the rapid advancements in LLM\ncapabilities and the opaque nature of some training techniques make metric\nproposals degrade rapidly. Our research emphasizes a quantitative role to\ndescribe steerability in LLMs, presenting both its promise and areas for\nfurther refinement in aligning its progress to human intentions.", "published": "2023-08-07 18:14:24", "link": "http://arxiv.org/abs/2308.07326v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AudioVMAF: Audio Quality Prediction with VMAF", "abstract": "Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in\nthe industry for measuring coded video quality. In this study, we propose an\nauditory-inspired frontend in existing VMAF for creating videos of reference\nand coded spectrograms, and extended VMAF for measuring coded audio quality. We\nname our system AudioVMAF. We demonstrate that image replication is capable of\nfurther enhancing prediction accuracy, especially when band-limited anchors are\npresent. The proposed method significantly outperforms all existing visual\nquality features repurposed for audio, and even demonstrates a significant\noverall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation\ncoefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3\n[4]) also inspired from the image domain.", "published": "2023-08-07 09:37:42", "link": "http://arxiv.org/abs/2308.03437v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Noise Control based on the Momentum Multichannel Normalized\n  Filtered-x Least Mean Square Algorithm", "abstract": "Multichannel active noise control (MCANC) is widely utilized to achieve\nsignificant noise cancellation area in the complicated acoustic field.\nMeanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes\nthe benchmark solution for the implementation of MCANC due to its low\ncomputational complexity. However, its slow convergence speed more or less\nundermines the performance of dealing with quickly varying disturbances, such\nas piling noise. Furthermore, the noise power variation also deteriorates the\nrobustness of the algorithm when it adopts the fixed step size. To solve these\nissues, we integrated the normalized multichannel FxLMS with the momentum\nmethod, which hence, effectively avoids the interference of the primary noise\npower and accelerates the convergence of the algorithm. To validate its\neffectiveness, we deployed this algorithm in a multichannel noise control\nwindow to control the real machine noise.", "published": "2023-08-07 15:59:38", "link": "http://arxiv.org/abs/2308.03684v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio\n  Detection", "abstract": "Current fake audio detection algorithms have achieved promising performances\non most datasets. However, their performance may be significantly degraded when\ndealing with audio of a different dataset. The orthogonal weight modification\nto overcome catastrophic forgetting does not consider the similarity of genuine\naudio across different datasets. To overcome this limitation, we propose a\ncontinual learning algorithm for fake audio detection to overcome catastrophic\nforgetting, called Regularized Adaptive Weight Modification (RAWM). When\nfine-tuning a detection network, our approach adaptively computes the direction\nof weight modification according to the ratio of genuine utterances and fake\nutterances. The adaptive modification direction ensures the network can\neffectively detect fake audio on the new dataset while preserving its knowledge\nof old model, thus mitigating catastrophic forgetting. In addition, genuine\naudio collected from quite different acoustic conditions may skew their feature\ndistribution, so we introduce a regularization constraint to force the network\nto remember the old distribution in this regard. Our method can easily be\ngeneralized to related fields, like speech emotion recognition. We also\nevaluate our approach across multiple datasets and obtain a significant\nperformance improvement on cross-dataset experiments.", "published": "2023-08-07 05:05:49", "link": "http://arxiv.org/abs/2308.03300v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation", "abstract": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech\nseparation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but\nthe complexity of the DANet model is very high. In this paper, a simplified and\npowerful DANet model is proposed using Bidirectional Gated neural network\n(BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the\nk-means was applied in DANet as a clustering algorithm to reduce the complexity\nand increase the learning speed and accuracy. The metrics used in this paper\nare Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR),\nSignal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ)\nscore. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate\nthe proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ\nscores respectively, which were better than the original DANet model. Other\nimprovements were 20.7% and 17.9% in the number of parameters and time\ntraining, respectively. The model was applied on mixed Arabic speech signals\nand the results were better than that in English.", "published": "2023-08-07 06:26:53", "link": "http://arxiv.org/abs/2308.03332v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Distilled Ensemble Model for sEMG-based Silent Speech\n  Interface", "abstract": "Voice disorders affect millions of people worldwide. Surface\nelectromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been\nexplored as a potential solution for decades. However, previous works were\nlimited by small vocabularies and manually extracted features from raw data. To\naddress these limitations, we propose a lightweight deep learning\nknowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can\nclassify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling\nthe unambiguous generation of any English word through spelling. Extensive\nexperiments validate the effectiveness of KDE-SSI, achieving a test accuracy of\n85.9\\%. Our findings also shed light on an end-to-end system for portable,\npractical equipment.", "published": "2023-08-07 03:52:37", "link": "http://arxiv.org/abs/2308.06533v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
