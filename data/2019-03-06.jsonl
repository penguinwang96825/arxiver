{"title": "Bidirectional Attentive Memory Networks for Question Answering over\n  Knowledge Bases", "abstract": "When answering natural language questions over knowledge bases (KBs),\ndifferent question components and KB aspects play different roles. However,\nmost existing embedding-based methods for knowledge base question answering\n(KBQA) ignore the subtle inter-relationships between the question and the KB\n(e.g., entity types, relation paths and context). In this work, we propose to\ndirectly model the two-way flow of interactions between the questions and the\nKB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring\nno external resources and only very few hand-crafted features, on the\nWebQuestions benchmark, our method significantly outperforms existing\ninformation-retrieval based methods, and remains competitive with\n(hand-crafted) semantic parsing based methods. Also, since we use attention\nmechanisms, our method offers better interpretability compared to other\nbaselines.", "published": "2019-03-06 06:09:02", "link": "http://arxiv.org/abs/1903.02188v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dixit: Interactive Visual Storytelling via Term Manipulation", "abstract": "In this paper, we introduce Dixit, an interactive visual storytelling system\nthat the user interacts with iteratively to compose a short story for a photo\nsequence. The user initiates the process by uploading a sequence of photos.\nDixit first extracts text terms from each photo which describe the objects\n(e.g., boy, bike) or actions (e.g., sleep) in the photo, and then allows the\nuser to add new terms or remove existing terms. Dixit then generates a short\nstory based on these terms. Behind the scenes, Dixit uses an LSTM-based model\ntrained on image caption data and FrameNet to distill terms from each image and\nutilizes a transformer decoder to compose a context-coherent story. Users\nchange images or terms iteratively with Dixit to create the most ideal story.\nDixit also allows users to manually edit and rate stories. The proposed\nprocedure opens up possibilities for interpretable and controllable visual\nstorytelling, allowing users to understand the story formation rationale and to\nintervene in the generation process.", "published": "2019-03-06 08:08:01", "link": "http://arxiv.org/abs/1903.02230v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KBQA: Learning Question Answering over QA Corpora and Knowledge Bases", "abstract": "Question answering (QA) has become a popular way for humans to access\nbillion-scale knowledge bases. Unlike web search, QA over a knowledge base\ngives out accurate and concise results, provided that natural language\nquestions can be understood and mapped precisely to structured queries over the\nknowledge base. The challenge, however, is that a human can ask one question in\nmany different ways. Previous approaches have natural limits due to their\nrepresentations: rule based approaches only understand a small set of \"canned\"\nquestions, while keyword based or synonym based approaches cannot fully\nunderstand the questions. In this paper, we design a new kind of question\nrepresentation: templates, over a billion scale knowledge base and a million\nscale QA corpora. For example, for questions about a city's population, we\nlearn templates such as What's the population of $city?, How many people are\nthere in $city?. We learned 27 million templates for 2782 intents. Based on\nthese templates, our QA system KBQA effectively supports binary factoid\nquestions, as well as complex questions which are composed of a series of\nbinary factoid questions. Furthermore, we expand predicates in RDF knowledge\nbase, which boosts the coverage of knowledge base by 57 times. Our QA system\nbeats all other state-of-art works on both effectiveness and efficiency over\nQALD benchmarks.", "published": "2019-03-06 14:38:28", "link": "http://arxiv.org/abs/1903.02419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Embedding Alignment for Lifelong Relation Extraction", "abstract": "Conventional approaches to relation extraction usually require a fixed set of\npre-defined relations. Such requirement is hard to meet in many real\napplications, especially when new data and relations are emerging incessantly\nand it is computationally expensive to store all data and re-train the whole\nmodel every time new data and relations come in. We formulate such a\nchallenging problem as lifelong relation extraction and investigate\nmemory-efficient incremental learning methods without catastrophically\nforgetting knowledge learned from previous tasks. We first investigate a\nmodified version of the stochastic gradient methods with a replay memory, which\nsurprisingly outperforms recent state-of-the-art lifelong learning methods. We\nfurther propose to improve this approach to alleviate the forgetting problem by\nanchoring the sentence embedding space. Specifically, we utilize an explicit\nalignment model to mitigate the sentence embedding distortion of the learned\nmodel when training on new data and new relations. Experiment results on\nmultiple benchmarks show that our proposed method significantly outperforms the\nstate-of-the-art lifelong learning approaches.", "published": "2019-03-06 19:22:24", "link": "http://arxiv.org/abs/1903.02588v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained\n  Entity Typing", "abstract": "Existing entity typing systems usually exploit the type hierarchy provided by\nknowledge base (KB) schema to model label correlations and thus improve the\noverall performance. Such techniques, however, are not directly applicable to\nmore open and practical scenarios where the type set is not restricted by KB\nschema and includes a vast number of free-form types. To model the underly-ing\nlabel correlations without access to manually annotated label structures, we\nintroduce a novel label-relational inductive bias, represented by a graph\npropagation layer that effectively encodes both global label co-occurrence\nstatistics and word-level similarities.On a large dataset with over 10,000\nfree-form types, the graph-enhanced model equipped with an attention-based\nmatching module is able to achieve a much higher recall score while maintaining\na high-level precision. Specifically, it achieves a 15.3% relative F1\nimprovement and also less inconsistency in the outputs. We further show that a\nsimple modification of our proposed graph layer can also improve the\nperformance on a conventional and widely-tested dataset that only includes\nKB-schema types.", "published": "2019-03-06 19:42:19", "link": "http://arxiv.org/abs/1903.02591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Instance Learning for End-to-End Knowledge Base Question Answering", "abstract": "End-to-end training has been a popular approach for knowledge base question\nanswering (KBQA). However, real world applications often contain answers of\nvaried quality for users' questions. It is not appropriate to treat all\navailable answers of a user question equally.\n  This paper proposes a novel approach based on multiple instance learning to\naddress the problem of noisy answers by exploring consensus among answers to\nthe same question in training end-to-end KBQA models. In particular, the QA\npairs are organized into bags with dynamic instance selection and different\noptions of instance weighting. Curriculum learning is utilized to select\ninstance bags during training. On the public CQA dataset, the new method\nsignificantly improves both entity accuracy and the Rouge-L score over a\nstate-of-the-art end-to-end KBQA baseline.", "published": "2019-03-06 23:14:49", "link": "http://arxiv.org/abs/1903.02652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA", "abstract": "We present the SemEval 2019 shared task on UCCA parsing in English, German\nand French, and discuss the participating systems and results. UCCA is a\ncross-linguistically applicable framework for semantic representation, which\nbuilds on extensive typological work and supports rapid annotation. UCCA poses\na challenge for existing parsing techniques, as it exhibits reentrancy\n(resulting in DAG structures), discontinuous structures and non-terminal nodes\ncorresponding to complex semantic units. The shared task has yielded\nimprovements over the state-of-the-art baseline in all languages and settings.\nFull results can be found in the task's website\n\\url{https://competitions.codalab.org/competitions/19160}.", "published": "2019-03-06 16:55:58", "link": "http://arxiv.org/abs/1903.02953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negative Training for Neural Dialogue Response Generation", "abstract": "Although deep learning models have brought tremendous advancements to the\nfield of open-domain dialogue response generation, recent research results have\nrevealed that the trained models have undesirable generation behaviors, such as\nmalicious responses and generic (boring) responses. In this work, we propose a\nframework named \"Negative Training\" to minimize such behaviors. Given a trained\nmodel, the framework will first find generated samples that exhibit the\nundesirable behavior, and then use them to feed negative training signals for\nfine-tuning the model. Our experiments show that negative training can\nsignificantly reduce the hit rate of malicious responses, or discourage\nfrequent responses and improve response diversity.", "published": "2019-03-06 01:37:51", "link": "http://arxiv.org/abs/1903.02134v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SNU_IDS at SemEval-2019 Task 3: Addressing Training-Test Class\n  Distribution Mismatch in Conversational Classification", "abstract": "We present several techniques to tackle the mismatch in class distributions\nbetween training and test data in the Contextual Emotion Detection task of\nSemEval 2019, by extending the existing methods for class imbalance problem.\nReducing the distance between the distribution of prediction and ground truth,\nthey consistently show positive effects on the performance. Also we propose a\nnovel neural architecture which utilizes representation of overall context as\nwell as of each utterance. The combination of the methods and the models\nachieved micro F1 score of about 0.766 on the final evaluation.", "published": "2019-03-06 03:53:19", "link": "http://arxiv.org/abs/1903.02163v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Character-Level Approach to the Text Normalization Problem Based on a\n  New Causal Encoder", "abstract": "Text normalization is a ubiquitous process that appears as the first step of\nmany Natural Language Processing problems. However, previous Deep Learning\napproaches have suffered from so-called silly errors, which are undetectable on\nunsupervised frameworks, making those models unsuitable for deployment. In this\nwork, we make use of an attention-based encoder-decoder architecture that\novercomes these undetectable errors by using a fine-grained character-level\napproach rather than a word-level one. Furthermore, our new general-purpose\nencoder based on causal convolutions, called Causal Feature Extractor (CFE), is\nintroduced and compared to other common encoders. The experimental results show\nthe feasibility of this encoder, which leverages the attention mechanisms the\nmost and obtains better results in terms of accuracy, number of parameters and\nconvergence time. While our method results in a slightly worse initial accuracy\n(92.74%), errors can be automatically detected and, thus, more readily solved,\nobtaining a more robust model for deployment. Furthermore, there is still\nplenty of room for future improvements that will push even further these\nadvantages.", "published": "2019-03-06 22:48:21", "link": "http://arxiv.org/abs/1903.02642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Persona-Aware Tips Generation", "abstract": "Tips, as a compacted and concise form of reviews, were paid less attention by\nresearchers. In this paper, we investigate the task of tips generation by\nconsidering the `persona' information which captures the intrinsic language\nstyle of the users or the different characteristics of the product items. In\norder to exploit the persona information, we propose a framework based on\nadversarial variational auto-encoders (aVAE) for persona modeling from the\nhistorical tips and reviews of users and items. The latent variables from aVAE\nare regarded as persona embeddings. Besides representing persona using the\nlatent embeddings, we design a persona memory for storing the persona related\nwords for users and items. Pointer Network is used to retrieve persona wordings\nfrom the memory when generating tips. Moreover, the persona embeddings are used\nas latent factors by a rating prediction component to predict the sentiment of\na user over an item. Finally, the persona embeddings and the sentiment\ninformation are incorporated into a recurrent neural networks based tips\ngeneration component. Extensive experimental results are reported and discussed\nto elaborate the peculiarities of our framework.", "published": "2019-03-06 03:36:29", "link": "http://arxiv.org/abs/1903.02156v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence", "abstract": "This volume represents the accepted submissions from the AAAI-2019 Workshop\non Games and Simulations for Artificial Intelligence held on January 29, 2019\nin Honolulu, Hawaii, USA. https://www.gamesim.ai", "published": "2019-03-06 04:49:07", "link": "http://arxiv.org/abs/1903.02172v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language\n  Navigation", "abstract": "We present the Frontier Aware Search with backTracking (FAST) Navigator, a\ngeneral framework for action decoding, that achieves state-of-the-art results\non the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson\net. al. (2018). Given a natural language instruction and photo-realistic image\nviews of a previously unseen environment, the agent was tasked with navigating\nfrom source to target location as quickly as possible. While all current\napproaches make local action decisions or score entire trajectories using beam\nsearch, ours balances local and global signals when exploring an unobserved\nenvironment. Importantly, this lets us act greedily but use global signals to\nbacktrack when necessary. Applying FAST framework to existing state-of-the-art\nmodels achieved a 17% relative gain, an absolute 6% gain on Success rate\nweighted by Path Length (SPL).", "published": "2019-03-06 18:54:55", "link": "http://arxiv.org/abs/1903.02547v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.NE", "cs.RO"], "primary_category": "cs.CL"}
