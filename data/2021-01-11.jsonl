{"title": "AT-BERT: Adversarial Training BERT for Acronym Identification Winning\n  Solution for SDU@AAAI-21", "abstract": "Acronym identification focuses on finding the acronyms and the phrases that\nhave been abbreviated, which is crucial for scientific document understanding\ntasks. However, the limited size of manually annotated datasets hinders further\nimprovement for the problem. Recent breakthroughs of language models\npre-trained on large corpora clearly show that unsupervised pre-training can\nvastly improve the performance of downstream tasks. In this paper, we present\nan Adversarial Training BERT method named AT-BERT, our winning solution to\nacronym identification task for Scientific Document Understanding (SDU)\nChallenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to\ncapture better semantic representation. Then we incorporate the FGM adversarial\ntraining strategy into the fine-tuning of BERT, which makes the model more\nrobust and generalized. Furthermore, an ensemble mechanism is devised to\ninvolve the representations learned from multiple BERT variants. Assembling all\nthese components together, the experimental results on the SciAI dataset show\nthat our proposed approach outperforms all other competitive state-of-the-art\nmethods.", "published": "2021-01-11 05:02:34", "link": "http://arxiv.org/abs/2101.03700v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constraint 2021: Machine Learning Models for COVID-19 Fake News\n  Detection Shared Task", "abstract": "In this system paper we present our contribution to the Constraint 2021\nCOVID-19 Fake News Detection Shared Task, which poses the challenge of\nclassifying COVID-19 related social media posts as either fake or real. In our\nsystem, we address this challenge by applying classical machine learning\nalgorithms together with several linguistic features, such as n-grams,\nreadability, emotional tone and punctuation. In terms of pre-processing, we\nexperiment with various steps like stop word removal, stemming/lemmatization,\nlink removal and more. We find our best performing system to be based on a\nlinear SVM, which obtains a weighted average F1 score of 95.19% on test data,\nwhich lands a place in the middle of the leaderboard (place 80 of 167).", "published": "2021-01-11 05:57:32", "link": "http://arxiv.org/abs/2101.03717v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural\n  Machine Translation", "abstract": "Neural Machine Translation model is a sequence-to-sequence converter based on\nneural networks. Existing models use recurrent neural networks to construct\nboth the encoder and decoder modules. In alternative research, the recurrent\nnetworks were substituted by convolutional neural networks for capturing the\nsyntactic structure in the input sentence and decreasing the processing time.\nWe incorporate the goodness of both approaches by proposing a\nconvolutional-recurrent encoder for capturing the context information as well\nas the sequential information from the source sentence. Word embedding and\nposition embedding of the source sentence is performed prior to the\nconvolutional encoding layer which is basically a n-gram feature extractor\ncapturing phrase-level context information. The rectified output of the\nconvolutional encoding layer is added to the original embedding vector, and the\nsum is normalized by layer normalization. The normalized output is given as a\nsequential input to the recurrent encoding layer that captures the temporal\ninformation in the sequence. For the decoder, we use the attention-based\nrecurrent neural network. Translation task on the German-English dataset\nverifies the efficacy of the proposed approach from the higher BLEU scores\nachieved as compared to the state of the art.", "published": "2021-01-11 17:03:52", "link": "http://arxiv.org/abs/2101.04030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph\n  Transformer", "abstract": "A biomedical relation statement is commonly expressed in multiple sentences\nand consists of many concepts, including gene, disease, chemical, and mutation.\nTo automatically extract information from biomedical literature, existing\nbiomedical text-mining approaches typically formulate the problem as a\ncross-sentence n-ary relation-extraction task that detects relations among n\nentities across multiple sentences, and use either a graph neural network (GNN)\nwith long short-term memory (LSTM) or an attention mechanism. Recently,\nTransformer has been shown to outperform LSTM on many natural language\nprocessing (NLP) tasks. In this work, we propose a novel architecture that\ncombines Bidirectional Encoder Representations from Transformers with Graph\nTransformer (BERT-GT), through integrating a neighbor-attention mechanism into\nthe BERT architecture. Unlike the original Transformer architecture, which\nutilizes the whole sentence(s) to calculate the attention of the current token,\nthe neighbor-attention mechanism in our method calculates its attention\nutilizing only its neighbor tokens. Thus, each token can pay attention to its\nneighbor information with little noise. We show that this is critically\nimportant when the text is very long, as in cross-sentence or abstract-level\nrelation-extraction tasks. Our benchmarking results show improvements of 5.44%\nand 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and\nchemical-protein relation datasets, suggesting BERT-GT is a robust approach\nthat is applicable to other biomedical relation extraction tasks or datasets.", "published": "2021-01-11 19:34:55", "link": "http://arxiv.org/abs/2101.04158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering Word Embeddings with Self-Organizing Maps. Application on\n  LaRoSeDa -- A Large Romanian Sentiment Data Set", "abstract": "Romanian is one of the understudied languages in computational linguistics,\nwith few resources available for the development of natural language processing\ntools. In this paper, we introduce LaRoSeDa, a Large Romanian Sentiment Data\nSet, which is composed of 15,000 positive and negative reviews collected from\none of the largest Romanian e-commerce platforms. We employ two sentiment\nclassification methods as baselines for our new data set, one based on\nlow-level features (character n-grams) and one based on high-level features\n(bag-of-word-embeddings generated by clustering word embeddings with k-means).\nAs an additional contribution, we replace the k-means clustering algorithm with\nself-organizing maps (SOMs), obtaining better results because the generated\nclusters of word embeddings are closer to the Zipf's law distribution, which is\nknown to govern natural language. We also demonstrate the generalization\ncapacity of using SOMs for the clustering of word embeddings on another\nrecently-introduced Romanian data set, for text categorization by topic.", "published": "2021-01-11 21:19:22", "link": "http://arxiv.org/abs/2101.04197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Unlikelihood Training: Improving Neural Text Generation with\n  Reinforcement Learning", "abstract": "Likelihood training and maximization-based decoding result in dull and\nrepetitive generated texts even when using powerful language models (Holtzman\net al., 2019). Adding a loss function for regularization was shown to improve\ntext generation output by helping avoid unwanted properties, such as\ncontradiction or repetition (Li at al., 2020). In this work, we propose\nfine-tuning a language model by using policy gradient reinforcement learning,\ndirectly optimizing for better generation. We apply this approach to minimizing\nrepetition in generated text, and show that, when combined with unlikelihood\ntraining (Welleck et al., 2020), our method further reduces repetition without\nimpacting the language model quality. We also evaluate other methods for\nimproving generation at training and decoding time, and compare them using\nvarious metrics aimed at control for better text generation output.", "published": "2021-01-11 23:10:01", "link": "http://arxiv.org/abs/2101.04229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A More Efficient Chinese Named Entity Recognition base on BERT and\n  Syntactic Analysis", "abstract": "We propose a new Named entity recognition (NER) method to effectively make\nuse of the results of Part-of-speech (POS) tagging, Chinese word segmentation\n(CWS) and parsing while avoiding NER error caused by POS tagging error. This\npaper first uses Stanford natural language process (NLP) tool to annotate\nlarge-scale untagged data so as to reduce the dependence on the tagged data;\nthen a new NLP model, g-BERT model, is designed to compress Bidirectional\nEncoder Representations from Transformers (BERT) model in order to reduce\ncalculation quantity; finally, the model is evaluated based on Chinese NER\ndataset. The experimental results show that the calculation quantity in g-BERT\nmodel is reduced by 60% and performance improves by 2% with Test F1 to 96.5\ncompared with that in BERT model.", "published": "2021-01-11 15:33:39", "link": "http://arxiv.org/abs/2101.11423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-hop Knowledge Base Question Answering by Learning\n  Intermediate Supervision Signals", "abstract": "Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer\nentities that are multiple hops away in the Knowledge Base (KB) from the\nentities in the question. A major challenge is the lack of supervision signals\nat intermediate steps. Therefore, multi-hop KBQA algorithms can only receive\nthe feedback from the final answer, which makes the learning unstable or\nineffective.\n  To address this challenge, we propose a novel teacher-student approach for\nthe multi-hop KBQA task. In our approach, the student network aims to find the\ncorrect answer to the query, while the teacher network tries to learn\nintermediate supervision signals for improving the reasoning capacity of the\nstudent network. The major novelty lies in the design of the teacher network,\nwhere we utilize both forward and backward reasoning to enhance the learning of\nintermediate entity distributions. By considering bidirectional reasoning, the\nteacher network can produce more reliable intermediate supervision signals,\nwhich can alleviate the issue of spurious reasoning. Extensive experiments on\nthree benchmark datasets have demonstrated the effectiveness of our approach on\nthe KBQA task. The code to reproduce our analysis is available at\nhttps://github.com/RichardHGL/WSDM2021_NSM.", "published": "2021-01-11 07:49:50", "link": "http://arxiv.org/abs/2101.03737v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain\n  Detection", "abstract": "Real-life applications, heavily relying on machine learning, such as dialog\nsystems, demand out-of-domain detection methods. Intent classification models\nshould be equipped with a mechanism to distinguish seen intents from unseen\nones so that the dialog agent is capable of rejecting the latter and avoiding\nundesired behavior. However, despite increasing attention paid to the task, the\nbest practices for out-of-domain intent detection have not yet been fully\nestablished.\n  This paper conducts a thorough comparison of out-of-domain intent detection\nmethods. We prioritize the methods, not requiring access to out-of-domain data\nduring training, gathering of which is extremely time- and labor-consuming due\nto lexical and stylistic variation of user utterances. We evaluate multiple\ncontextual encoders and methods, proven to be efficient, on three standard\ndatasets for intent classification, expanded with out-of-domain utterances. Our\nmain findings show that fine-tuning Transformer-based encoders on in-domain\ndata leads to superior results. Mahalanobis distance, together with utterance\nrepresentations, derived from Transformer-based encoders, outperforms other\nmethods by a wide margin and establishes new state-of-the-art results for all\ndatasets.\n  The broader analysis shows that the reason for success lies in the fact that\nthe fine-tuned Transformer is capable of constructing homogeneous\nrepresentations of in-domain utterances, revealing geometrical disparity to out\nof domain utterances. In turn, the Mahalanobis distance captures this disparity\neasily.\n  The code is available in our GitHub repo:\nhttps://github.com/huawei-noah/noah-research/tree/master/Maha_OOD .", "published": "2021-01-11 09:10:58", "link": "http://arxiv.org/abs/2101.03778v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model Generalization on COVID-19 Fake News Detection", "abstract": "Amid the pandemic COVID-19, the world is facing unprecedented infodemic with\nthe proliferation of both fake and real information. Considering the\nproblematic consequences that the COVID-19 fake-news have brought, the\nscientific community has put effort to tackle it. To contribute to this fight\nagainst the infodemic, we aim to achieve a robust model for the COVID-19\nfake-news detection task proposed at CONSTRAINT 2021 (FakeNews-19) by taking\ntwo separate approaches: 1) fine-tuning transformers based language models with\nrobust loss functions and 2) removing harmful training instances through\ninfluence calculation. We further evaluate the robustness of our models by\nevaluating on different COVID-19 misinformation test set (Tweets-19) to\nunderstand model generalization ability. With the first approach, we achieve\n98.13% for weighted F1 score (W-F1) for the shared task, whereas 38.18% W-F1 on\nthe Tweets-19 highest. On the contrary, by performing influence data cleansing,\nour model with 99% cleansing percentage can achieve 54.33% W-F1 score on\nTweets-19 with a trade-off. By evaluating our models on two COVID-19 fake-news\ntest sets, we suggest the importance of model generalization ability in this\ntask to step forward to tackle the COVID-19 fake-news problem in online social\nmedia platforms.", "published": "2021-01-11 12:23:41", "link": "http://arxiv.org/abs/2101.03841v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identification of COVID-19 related Fake News via Neural Stacking", "abstract": "Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}", "published": "2021-01-11 15:52:37", "link": "http://arxiv.org/abs/2101.03988v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automating the Compilation of Potential Core-Outcomes for Clinical\n  Trials", "abstract": "Due to increased access to clinical trial outcomes and analysis, researchers\nand scientists are able to iterate or improve upon relevant approaches more\neffectively. However, the metrics and related results of clinical trials\ntypically do not follow any standardization in their reports, making it more\ndifficult for researchers to parse the results of different trials. The\nobjective of this paper is to describe an automated method utilizing natural\nlanguage processing in order to describe the probable core outcomes of clinical\ntrials, in order to alleviate the issues around disparate clinical trial\noutcomes. As the nature of this process is domain specific, BioBERT was\nemployed in order to conduct a multi-class entity normalization task. In\naddition to BioBERT, an unsupervised feature-based approach making use of only\nthe encoder output embedding representations for the outcomes and labels was\nutilized. Finally, cosine similarity was calculated across the vectors to\nobtain the semantic similarity. This method was able to both harness the\ndomain-specific context of each of the tokens from the learned embeddings of\nthe BioBERT model as well as a more stable metric of sentence similarity. Some\ncommon outcomes identified using the Jaccard similarity in each of the\nclassifications were compiled, and while some are untenable, a pipeline for\nwhich this automation process could be conducted was established.", "published": "2021-01-11 18:14:49", "link": "http://arxiv.org/abs/2101.04076v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of Deep Learning Models for Hostility Detection in Hindi Text", "abstract": "The social media platform is a convenient medium to express personal thoughts\nand share useful information. It is fast, concise, and has the ability to reach\nmillions. It is an effective place to archive thoughts, share artistic content,\nreceive feedback, promote products, etc. Despite having numerous advantages\nthese platforms have given a boost to hostile posts. Hate speech and derogatory\nremarks are being posted for personal satisfaction or political gain. The\nhostile posts can have a bullying effect rendering the entire platform\nexperience hostile. Therefore detection of hostile posts is important to\nmaintain social media hygiene. The problem is more pronounced languages like\nHindi which are low in resources. In this work, we present approaches for\nhostile text detection in the Hindi language. The proposed approaches are\nevaluated on the Constraint@AAAI 2021 Hindi hostility detection dataset. The\ndataset consists of hostile and non-hostile texts collected from social media\nplatforms. The hostile posts are further segregated into overlapping classes of\nfake, offensive, hate, and defamation. We evaluate a host of deep learning\napproaches based on CNN, LSTM, and BERT for this multi-label classification\nproblem. The pre-trained Hindi fast text word embeddings by IndicNLP and\nFacebook are used in conjunction with CNN and LSTM models. Two variations of\npre-trained multilingual transformer language models mBERT and IndicBERT are\nused. We show that the performance of BERT based models is best. Moreover, CNN\nand LSTM models also perform competitively with BERT based models.", "published": "2021-01-11 19:10:57", "link": "http://arxiv.org/abs/2101.04144v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DBTagger: Multi-Task Learning for Keyword Mapping in NLIDBs Using\n  Bi-Directional Recurrent Neural Networks", "abstract": "Translating Natural Language Queries (NLQs) to Structured Query Language\n(SQL) in interfaces deployed in relational databases is a challenging task,\nwhich has been widely studied in database community recently. Conventional rule\nbased systems utilize series of solutions as a pipeline to deal with each step\nof this task, namely stop word filtering, tokenization, stemming/lemmatization,\nparsing, tagging, and translation. Recent works have mostly focused on the\ntranslation step overlooking the earlier steps by using ad-hoc solutions. In\nthe pipeline, one of the most critical and challenging problems is keyword\nmapping; constructing a mapping between tokens in the query and relational\ndatabase elements (tables, attributes, values, etc.). We define the keyword\nmapping problem as a sequence tagging problem, and propose a novel deep\nlearning based supervised approach that utilizes POS tags of NLQs. Our proposed\napproach, called \\textit{DBTagger} (DataBase Tagger), is an end-to-end and\nschema independent solution, which makes it practical for various relational\ndatabases. We evaluate our approach on eight different datasets, and report new\nstate-of-the-art accuracy results, $92.4\\%$ on the average. Our results also\nindicate that DBTagger is faster than its counterparts up to $10000$ times and\nscalable for bigger databases.", "published": "2021-01-11 22:54:39", "link": "http://arxiv.org/abs/2101.04226v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Explain and Predict, and then Predict Again", "abstract": "A desirable property of learning systems is to be both effective and\ninterpretable. Towards this goal, recent models have been proposed that first\ngenerate an extractive explanation from the input text and then generate a\nprediction on just the explanation called explain-then-predict models. These\nmodels primarily consider the task input as a supervision signal in learning an\nextractive explanation and do not effectively integrate rationales data as an\nadditional inductive bias to improve task performance. We propose a novel yet\nsimple approach ExPred, that uses multi-task learning in the explanation\ngeneration phase effectively trading-off explanation and prediction losses. And\nthen we use another prediction network on just the extracted explanations for\noptimizing the task performance. We conduct an extensive evaluation of our\napproach on three diverse language datasets -- fact verification, sentiment\nclassification, and QA -- and find that we substantially outperform existing\napproaches.", "published": "2021-01-11 19:36:52", "link": "http://arxiv.org/abs/2101.04109v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.m; I.2.7"], "primary_category": "cs.CL"}
