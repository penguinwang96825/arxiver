{"title": "Unleashing the True Potential of Sequence-to-Sequence Models for\n  Sequence Tagging and Structure Parsing", "abstract": "Sequence-to-Sequence (S2S) models have achieved remarkable success on various\ntext generation tasks. However, learning complex structures with S2S models\nremains challenging as external neural modules and additional lexicons are\noften supplemented to predict non-textual outputs. We present a systematic\nstudy of S2S modeling using contained decoding on four core tasks:\npart-of-speech tagging, named entity recognition, constituency and dependency\nparsing, to develop efficient exploitation methods costing zero extra\nparameters. In particular, 3 lexically diverse linearization schemas and\ncorresponding constrained decoding methods are designed and evaluated.\nExperiments show that although more lexicalized schemas yield longer output\nsequences that require heavier training, their sequences being closer to\nnatural language makes them easier to learn. Moreover, S2S models using our\nconstrained decoding outperform other S2S approaches using external resources.\nOur best models perform better than or comparably to the state-of-the-art for\nall 4 tasks, lighting a promise for S2S models to generate non-sequential\nstructures.", "published": "2023-02-05 01:37:26", "link": "http://arxiv.org/abs/2302.02275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TempEL: Linking Dynamically Evolving and Newly Emerging Entities", "abstract": "In our continuously evolving world, entities change over time and new,\npreviously non-existing or unknown, entities appear. We study how this\nevolutionary scenario impacts the performance on a well established entity\nlinking (EL) task. For that study, we introduce TempEL, an entity linking\ndataset that consists of time-stratified English Wikipedia snapshots from 2013\nto 2022, from which we collect both anchor mentions of entities, and these\ntarget entities' descriptions. By capturing such temporal aspects, our newly\nintroduced TempEL resource contrasts with currently existing entity linking\ndatasets, which are composed of fixed mentions linked to a single static\nversion of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA).\nIndeed, for each of our collected temporal snapshots, TempEL contains links to\nentities that are continual, i.e., occur in all of the years, as well as\ncompletely new entities that appear for the first time at some point. Thus, we\nenable to quantify the performance of current state-of-the-art EL models for:\n(i) entities that are subject to changes over time in their Knowledge Base\ndescriptions as well as their mentions' contexts, and (ii) newly created\nentities that were previously non-existing (e.g., at the time the EL model was\ntrained). Our experimental results show that in terms of temporal performance\ndegradation, (i) continual entities suffer a decrease of up to 3.1% EL\naccuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This\nhighlights the challenge of the introduced TempEL dataset and opens new\nresearch prospects in the area of time-evolving entity disambiguation.", "published": "2023-02-05 22:34:36", "link": "http://arxiv.org/abs/2302.02500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FineDeb: A Debiasing Framework for Language Models", "abstract": "As language models are increasingly included in human-facing machine learning\ntools, bias against demographic subgroups has gained attention. We propose\nFineDeb, a two-phase debiasing framework for language models that starts with\ncontextual debiasing of embeddings learned by pretrained language models. The\nmodel is then fine-tuned on a language modeling objective. Our results show\nthat FineDeb offers stronger debiasing in comparison to other methods which\noften result in models as biased as the original language model. Our framework\nis generalizable for demographics with multiple classes, and we demonstrate its\neffectiveness through extensive experiments and comparisons with state of the\nart techniques. We release our code and data on GitHub.", "published": "2023-02-05 18:35:21", "link": "http://arxiv.org/abs/2302.02453v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Nationality Bias in Text Generation", "abstract": "Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.", "published": "2023-02-05 19:15:33", "link": "http://arxiv.org/abs/2302.02463v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Siamese Network for Few-Shot Text Classification", "abstract": "Few-shot learning has been used to tackle the problem of label scarcity in\ntext classification, of which meta-learning based methods have shown to be\neffective, such as the prototypical networks (PROTO). Despite the success of\nPROTO, there still exist three main problems: (1) ignore the randomness of the\nsampled support sets when computing prototype vectors; (2) disregard the\nimportance of labeled samples; (3) construct meta-tasks in a purely random\nmanner. In this paper, we propose a Meta-Learning Siamese Network, namely,\nMeta-SN, to address these issues. Specifically, instead of computing prototype\nvectors from the sampled support sets, Meta-SN utilizes external knowledge\n(e.g. class names and descriptive texts) for class labels, which is encoded as\nthe low-dimensional embeddings of prototype vectors. In addition, Meta-SN\npresents a novel sampling strategy for constructing meta-tasks, which gives\nhigher sampling probabilities to hard-to-classify samples. Extensive\nexperiments are conducted on six benchmark datasets to show the clear\nsuperiority of Meta-SN over other state-of-the-art models. For reproducibility,\nall the datasets and codes are provided at https://github.com/hccngu/Meta-SN.", "published": "2023-02-05 07:41:17", "link": "http://arxiv.org/abs/2302.03507v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Semantic Approach to Negation Detection and Word Disambiguation with\n  Natural Language Processing", "abstract": "This study aims to demonstrate the methods for detecting negations in a\nsentence by uniquely evaluating the lexical structure of the text via\nword-sense disambiguation. The proposed framework examines all the unique\nfeatures in the various expressions within a text to resolve the contextual\nusage of all tokens and decipher the effect of negation on sentiment analysis.\nThe application of popular expression detectors skips this important step,\nthereby neglecting the root words caught in the web of negation and making text\nclassification difficult for machine learning and sentiment analysis. This\nstudy adopts the Natural Language Processing (NLP) approach to discover and\nantonimize words that were negated for better accuracy in text classification\nusing a knowledge base provided by an NLP library called WordHoard. Early\nresults show that our initial analysis improved on traditional sentiment\nanalysis, which sometimes neglects negations or assigns an inverse polarity\nscore. The SentiWordNet analyzer was improved by 35%, the Vader analyzer by 20%\nand the TextBlob by 6%.", "published": "2023-02-05 03:58:45", "link": "http://arxiv.org/abs/2302.02291v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.5.1; I.7.1; I.7.2"], "primary_category": "cs.CL"}
{"title": "deep learning of segment-level feature representation for speech emotion\n  recognition in conversations", "abstract": "Accurately detecting emotions in conversation is a necessary yet challenging\ntask due to the complexity of emotions and dynamics in dialogues. The emotional\nstate of a speaker can be influenced by many different factors, such as\ninterlocutor stimulus, dialogue scene, and topic. In this work, we propose a\nconversational speech emotion recognition method to deal with capturing\nattentive contextual dependency and speaker-sensitive interactions. First, we\nuse a pretrained VGGish model to extract segment-based audio representation in\nindividual utterances. Second, an attentive bi-directional gated recurrent unit\n(GRU) models contextual-sensitive information and explores intra- and\ninter-speaker dependencies jointly in a dynamic manner. The experiments\nconducted on the standard conversational dataset MELD demonstrate the\neffectiveness of the proposed method when compared against state-of the-art\nmethods.", "published": "2023-02-05 16:15:46", "link": "http://arxiv.org/abs/2302.02419v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hatemongers ride on echo chambers to escalate hate speech diffusion", "abstract": "Recent years have witnessed a swelling rise of hateful and abusive content\nover online social networks. While detection and moderation of hate speech have\nbeen the early go-to countermeasures, the solution requires a deeper\nexploration of the dynamics of hate generation and propagation. We analyze more\nthan 32 million posts from over 6.8 million users across three popular online\nsocial networks to investigate the interrelations between hateful behavior,\ninformation dissemination, and polarised organization mediated by echo\nchambers. We find that hatemongers play a more crucial role in governing the\nspread of information compared to singled-out hateful content. This observation\nholds for both the growth of information cascades as well as the conglomeration\nof hateful actors. Dissection of the core-wise distribution of these networks\npoints towards the fact that hateful users acquire a more well-connected\nposition in the social network and often flock together to build up information\ncascades. We observe that this cohesion is far from mere organized behavior;\ninstead, in these networks, hatemongers dominate the echo chambers -- groups of\nusers actively align themselves to specific ideological positions. The observed\ndominance of hateful users to inflate information cascades is primarily via\nuser interactions amplified within these echo chambers. We conclude our study\nwith a cautionary note that popularity-based recommendation of content is\nsusceptible to be exploited by hatemongers given their potential to escalate\ncontent popularity via echo-chambered interactions.", "published": "2023-02-05 20:30:48", "link": "http://arxiv.org/abs/2302.02479v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "MAC: A unified framework boosting low resource automatic speech\n  recognition", "abstract": "We propose a unified framework for low resource automatic speech recognition\ntasks named meta audio concatenation (MAC). It is easy to implement and can be\ncarried out in extremely low resource environments. Mathematically, we give a\nclear description of MAC framework from the perspective of bayesian sampling.\nIn this framework, we leverage a novel concatenative synthesis text-to-speech\nsystem to boost the low resource ASR task. By the concatenative synthesis\ntext-to-speech system, we can integrate language pronunciation rules and adjust\nthe TTS process. Furthermore, we propose a broad notion of meta audio set to\nmeet the modeling needs of different languages and different scenes when using\nthe system. Extensive experiments have demonstrated the great effectiveness of\nMAC on low resource ASR tasks. For CTC greedy search, CTC prefix, attention,\nand attention rescoring decode mode in Cantonese ASR task, Taiwanese ASR task,\nand Japanese ASR task the MAC method can reduce the CER by more than 15\\%.\nFurthermore, in the ASR task, MAC beats wav2vec2 (with fine-tuning) on common\nvoice datasets of Cantonese and gets really competitive results on common voice\ndatasets of Taiwanese and Japanese. Among them, it is worth mentioning that we\nachieve a \\textbf{10.9\\%} character error rate (CER) on the common voice\nCantonese ASR task, bringing about \\textbf{30\\%} relative improvement compared\nto the wav2vec2 (with fine-tuning).", "published": "2023-02-05 09:49:18", "link": "http://arxiv.org/abs/2302.03498v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Data Augmentation for Code Generation Tasks", "abstract": "Advances in natural language processing, such as transfer learning from\npre-trained language models, have impacted how models are trained for\nprogramming language tasks too. Previous research primarily explored code\npre-training and expanded it through multi-modality and multi-tasking, yet the\ndata for downstream tasks remain modest in size. Focusing on data utilization\nfor downstream tasks, we propose and adapt augmentation methods that yield\nconsistent improvements in code translation and summarization by up to 6.9% and\n7.5% respectively. Further analysis suggests that our methods work orthogonally\nand show benefits in output code style and numeric consistency. We also discuss\ntest data imperfections.", "published": "2023-02-05 14:30:32", "link": "http://arxiv.org/abs/2302.03499v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "cross-modal fusion techniques for utterance-level emotion recognition\n  from text and speech", "abstract": "Multimodal emotion recognition (MER) is a fundamental complex research\nproblem due to the uncertainty of human emotional expression and the\nheterogeneity gap between different modalities. Audio and text modalities are\nparticularly important for a human participant in understanding emotions.\nAlthough many successful attempts have been designed multimodal representations\nfor MER, there still exist multiple challenges to be addressed: 1) bridging the\nheterogeneity gap between multimodal features and model inter- and intra-modal\ninteractions of multiple modalities; 2) effectively and efficiently modelling\nthe contextual dynamics in the conversation sequence. In this paper, we propose\nCross-Modal RoBERTa (CM-RoBERTa) model for emotion detection from spoken audio\nand corresponding transcripts. As the core unit of the CM-RoBERTa, parallel\nself- and cross- attention is designed to dynamically capture inter- and\nintra-modal interactions of audio and text. Specially, the mid-level fusion and\nresidual module are employed to model long-term contextual dependencies and\nlearn modality-specific patterns. We evaluate the approach on the MELD dataset\nand the experimental results show the proposed approach achieves the\nstate-of-art performance on the dataset.", "published": "2023-02-05 18:16:12", "link": "http://arxiv.org/abs/2302.02447v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
