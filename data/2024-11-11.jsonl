{"title": "An Empirical Implementation of the Shadow Riskless Rate", "abstract": "We address the problem of asset pricing in a market where there is no risky\nasset. Previous work developed a theoretical model for a shadow riskless rate\n(SRR) for such a market in terms of the drift component of the state-price\ndeflator for that asset universe. Assuming asset prices are modeled by\ncorrelated geometric Brownian motion, in this work we develop a computational\napproach to estimate the SRR from empirical datasets. The approach employs:\nprincipal component analysis to model the effects of the individual Brownian\nmotions; singular value decomposition to capture the abrupt changes in\ncondition number of the linear system whose solution provides the SRR values;\nand a regularization to control the rate of change of the condition number.\nAmong other uses (e.g., for option pricing, developing a term structure of\ninterest rate), the SRR can be employed as an investment discriminator between\nasset classes. We apply the computational procedure to markets consisting of\ngroups of stocks, varying asset type and number. The theoretical and\ncomputational analysis provides not only the drift, but also the total\nvolatility of the state-price deflator. We investigate the time trajectory of\nthese two descriptive components of the state-price deflator for the empirical\ndatasets.", "published": "2024-11-11 22:47:35", "link": "http://arxiv.org/abs/2411.07421v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Reverse Prompt Engineering", "abstract": "We explore a new language model inversion problem under strict black-box,\nzero-shot, and limited data conditions. We propose a novel training-free\nframework that reconstructs prompts using only a limited number of text outputs\nfrom a language model. Existing methods rely on the availability of a large\nnumber of outputs for both training and inference, an assumption that is\nunrealistic in the real world, and they can sometimes produce garbled text. In\ncontrast, our approach, which relies on limited resources, consistently yields\ncoherent and semantically meaningful prompts. Our framework leverages a large\nlanguage model together with an optimization process inspired by the genetic\nalgorithm to effectively recover prompts. Experimental results on several\ndatasets derived from public sources indicate that our approach achieves\nhigh-quality prompt recovery and generates prompts more semantically and\nfunctionally aligned with the originals than current state-of-the-art methods.\nAdditionally, use-case studies introduced demonstrate the method's strong\npotential for generating high-quality text data on perturbed prompts.", "published": "2024-11-11 05:58:48", "link": "http://arxiv.org/abs/2411.06729v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persuasion with Large Language Models: a Survey", "abstract": "The rapid rise of Large Language Models (LLMs) has created new disruptive\npossibilities for persuasive communication, by enabling fully-automated\npersonalized and interactive content generation at an unprecedented scale. In\nthis paper, we survey the research field of LLM-based persuasion that has\nemerged as a result. We begin by exploring the different modes in which LLM\nSystems are used to influence human attitudes and behaviors. In areas such as\npolitics, marketing, public health, e-commerce, and charitable giving, such LLM\nSystems have already achieved human-level or even super-human persuasiveness.\nWe identify key factors influencing their effectiveness, such as the manner of\npersonalization and whether the content is labelled as AI-generated. We also\nsummarize the experimental designs that have been used to evaluate progress.\nOur survey suggests that the current and future potential of LLM-based\npersuasion poses profound ethical and societal risks, including the spread of\nmisinformation, the magnification of biases, and the invasion of privacy. These\nrisks underscore the urgent need for ethical guidelines and updated regulatory\nframeworks to avoid the widespread deployment of irresponsible and harmful LLM\nSystems.", "published": "2024-11-11 10:05:52", "link": "http://arxiv.org/abs/2411.06837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Multi-Task Learning Architecture for Hate Detection Leveraging\n  User-Based Information", "abstract": "Hate speech, offensive language, aggression, racism, sexism, and other\nabusive language are common phenomena in social media. There is a need for\nArtificial Intelligence(AI)based intervention which can filter hate content at\nscale. Most existing hate speech detection solutions have utilized the features\nby treating each post as an isolated input instance for the classification.\nThis paper addresses this issue by introducing a unique model that improves\nhate speech identification for the English language by utilising intra-user and\ninter-user-based information. The experiment is conducted over single-task\nlearning (STL) and multi-task learning (MTL) paradigms that use deep neural\nnetworks, such as convolutional neural networks (CNN), gated recurrent unit\n(GRU), bidirectional encoder representations from the transformer (BERT), and A\nLite BERT (ALBERT). We use three benchmark datasets and conclude that combining\ncertain user features with textual features gives significant improvements in\nmacro-F1 and weighted-F1.", "published": "2024-11-11 10:37:11", "link": "http://arxiv.org/abs/2411.06855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models", "abstract": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.", "published": "2024-11-11 12:54:22", "link": "http://arxiv.org/abs/2411.06946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios", "abstract": "As Large Language Models (LLMs) evolve in natural language processing (NLP),\ntheir ability to stably follow instructions in long-context inputs has become\ncritical for real-world applications. However, existing benchmarks seldom focus\non instruction-following in long-context scenarios or stability on different\ninputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed\nto evaluate LLMs' instruction-following capabilities and stability across long\ncontexts. LIFBench comprises three long-context scenarios and eleven diverse\ntasks, featuring 2,766 instructions generated through an automated expansion\nmethod across three dimensions: length, expression, and variables. For\nevaluation, we propose LIFEval, a rubric-based assessment method that enables\nprecise, automated scoring of complex LLM responses without reliance on\nLLM-assisted assessments or human judgment. This method allows for a\ncomprehensive analysis of model performance and stability from multiple\nperspectives. We conduct detailed experiments on 20 prominent LLMs across six\nlength intervals. Our work contributes LIFBench and LIFEval as robust tools for\nassessing LLM performance in complex and long-context settings, offering\nvaluable insights to guide future advancements in LLM development.", "published": "2024-11-11 14:43:51", "link": "http://arxiv.org/abs/2411.07037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer verbatim in-context retrieval across time and scale", "abstract": "To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.", "published": "2024-11-11 15:50:01", "link": "http://arxiv.org/abs/2411.07075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications.", "published": "2024-11-11 16:51:39", "link": "http://arxiv.org/abs/2411.07122v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Many-Shot In-Context Learning for Long-Context Evaluation", "abstract": "Many-shot in-context learning (ICL) has emerged as a unique setup to both\nutilize and test the ability of large language models to handle long context.\nThis paper delves into long-context language model (LCLM) evaluation through\nmany-shot ICL. We first ask: what types of ICL tasks benefit from additional\ndemonstrations, and how effective are they in evaluating LCLMs? We find that\nclassification and summarization tasks show performance improvements with\nadditional demonstrations, while translation and reasoning tasks do not exhibit\nclear trends. Next, we investigate the extent to which different tasks\nnecessitate retrieval versus global context understanding. We develop metrics\nto categorize ICL tasks into two groups: (i) similar-sample learning (SSL):\ntasks where retrieval of the most similar examples is sufficient for good\nperformance, and (ii) all-sample learning (ASL): tasks that necessitate a\ndeeper comprehension of all examples in the prompt. Lastly, we introduce a new\nmany-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both\nfronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while\nstate-of-the-art models demonstrate good performance up to 64k tokens in SSL\ntasks, many models experience significant performance drops at only 16k tokens\nin ASL tasks.", "published": "2024-11-11 17:00:59", "link": "http://arxiv.org/abs/2411.07130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language\n  Models", "abstract": "New LLM evaluation benchmarks are important to align with the rapid\ndevelopment of Large Language Models (LLMs). In this work, we present Chinese\nSimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality\nability of language models to answer short questions, and Chinese SimpleQA\nmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,\nEasy-to-evaluate). Specifically, first, we focus on the Chinese language over 6\nmajor topics with 99 diverse subtopics. Second, we conduct a comprehensive\nquality control process to achieve high-quality questions and answers, where\nthe reference answers are static and cannot be changed over time. Third,\nfollowing SimpleQA, the questions and answers are very short, and the grading\nprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we\nperform a comprehensive evaluation on the factuality abilities of existing\nLLMs. Finally, we hope that Chinese SimpleQA could guide the developers to\nbetter understand the Chinese factuality abilities of their models and\nfacilitate the growth of foundation models.", "published": "2024-11-11 17:10:56", "link": "http://arxiv.org/abs/2411.07140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text\n  Embeddings Must Adapt", "abstract": "Financial documents are filled with specialized terminology, arcane jargon,\nand curious acronyms that pose challenges for general-purpose text embeddings.\nYet, few text embeddings specialized for finance have been reported in the\nliterature, perhaps in part due to a lack of public datasets and benchmarks. We\npresent BAM embeddings, a set of text embeddings finetuned on a carefully\nconstructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of\ndomain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a\nheld-out test set, vs. only 39.2% for the best general-purpose text embedding\nfrom OpenAI. Further, BAM embeddings increase question answering accuracy by 8%\non FinanceBench and show increased sensitivity to the finance-specific elements\nthat are found in detailed, forward-looking and company and date-specific\nqueries. To support further research we describe our approach in detail,\nquantify the importance of hard negative mining and dataset scale.", "published": "2024-11-11 17:13:28", "link": "http://arxiv.org/abs/2411.07142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Primer on Word Embeddings: AI Techniques for Text Analysis in Social\n  Work", "abstract": "Word embeddings represent a transformative technology for analyzing text data\nin social work research, offering sophisticated tools for understanding case\nnotes, policy documents, research literature, and other text-based materials.\nThis methodological paper introduces word embeddings to social work\nresearchers, explaining how these mathematical representations capture meaning\nand relationships in text data more effectively than traditional keyword-based\napproaches. We discuss fundamental concepts, technical foundations, and\npractical applications, including semantic search, clustering, and retrieval\naugmented generation. The paper demonstrates how embeddings can enhance\nresearch workflows through concrete examples from social work practice, such as\nanalyzing case notes for housing instability patterns and comparing social work\nlicensing examinations across languages. While highlighting the potential of\nembeddings for advancing social work research, we acknowledge limitations\nincluding information loss, training data constraints, and potential biases. We\nconclude that successfully implementing embedding technologies in social work\nrequires developing domain-specific models, creating accessible tools, and\nestablishing best practices aligned with social work's ethical principles. This\nintegration can enhance our ability to analyze complex patterns in text data\nwhile supporting more effective services and interventions.", "published": "2024-11-11 17:33:51", "link": "http://arxiv.org/abs/2411.07156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Memorization of Factoids in Language Models", "abstract": "As new knowledge rapidly accumulates, language models (LMs) with pretrained\nknowledge quickly become obsolete. A common approach to updating LMs is\nfine-tuning them directly on new knowledge. However, recent studies have shown\nthat fine-tuning for memorization may be ineffective in storing knowledge or\nmay exacerbate hallucinations. In this work, we introduce a setting we call\ncontinual memorization, where a model must memorize and retain a set of\nfactoids through multiple stages of fine-tuning on subsequent datasets. We\ncharacterized the forgetting patterns through extensive experiments and show\nthat LMs widely suffer from forgetting, especially when needing to memorize\nfactoids in the second stage. We posit that forgetting can be alleviated by\nmodifying training dynamics: (1) protecting the memorization process when\nlearning factoids or (2) reducing interference from subsequent training stages.\nIntriguingly, we find that mixing randomly generated word sequences or generic\ndata sampled from pretraining corpora at different training stages effectively\nmitigates forgetting REMIX: Random and Generic Data Mixing). REMIX can recover\nperformance from severe forgetting, outperforming replay methods and other\ncontinual learning baselines. We analyze how REMIX influences the learning\nprocess and find that robust memorization follows a distinct pattern: the model\nstores factoids in earlier layers than usual and diversifies the layers that\nretain them, which results in easier recall and manipulate of the learned\nfactoids.", "published": "2024-11-11 17:56:15", "link": "http://arxiv.org/abs/2411.07175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Evaluations: Taking the Guesswork Out of Language Model\n  Evaluations", "abstract": "Language model users often issue queries that lack specification, where the\ncontext under which a query was issued -- such as the user's identity, the\nquery's intent, and the criteria for a response to be useful -- is not\nexplicit. For instance, a good response to a subjective query like \"What book\nshould I read next?\" would depend on the user's preferences, and a good\nresponse to an open-ended query like \"How do antibiotics work against\nbacteria?\" would depend on the user's expertise. This makes evaluation of\nresponses to such queries an ill-posed task, as evaluators may make arbitrary\njudgments about the response quality. To remedy this, we present contextualized\nevaluations, a protocol that synthetically constructs context surrounding an\nunderspecified query and provides it during evaluation. We find that the\npresence of context can 1) alter conclusions drawn from evaluation, even\nflipping win rates between model pairs, 2) nudge evaluators to make fewer\njudgments based on surface-level criteria, like style, and 3) provide new\ninsights about model behavior across diverse contexts. Specifically, our\nprocedure uncovers an implicit bias towards WEIRD contexts in models' \"default\"\nresponses and we find that models are not equally sensitive to following\ndifferent contexts, even when they are provided in prompts.", "published": "2024-11-11 18:58:38", "link": "http://arxiv.org/abs/2411.07237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model", "abstract": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies.", "published": "2024-11-11 18:58:46", "link": "http://arxiv.org/abs/2411.07238v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and\n  Semantic Robustness of Language Models", "abstract": "Set theory is foundational to mathematics and, when sets are finite, to\nreasoning about the world. An intelligent system should perform set operations\nconsistently, regardless of superficial variations in the operands. Initially\ndesigned for semantically-oriented NLP tasks, large language models (LLMs) are\nnow being evaluated on algorithmic tasks. Because sets are comprised of\narbitrary symbols (e.g. numbers, words), they provide an opportunity to test,\nsystematically, the invariance of LLMs' algorithmic abilities under simple\nlexical or semantic variations. To this end, we present the SetLexSem\nChallenge, a synthetic benchmark that evaluates the performance of LLMs on set\noperations. SetLexSem assesses the robustness of LLMs' instruction-following\nabilities under various conditions, focusing on the set operations and the\nnature and construction of the set members. Evaluating seven LLMs with\nSetLexSem, we find that they exhibit poor robustness to variation in both\noperation and operands. We show -- via the framework's systematic sampling of\nset members along lexical and semantic dimensions -- that LLMs are not only not\nrobust to variation along these dimensions but demonstrate unique failure modes\nin particular, easy-to-create semantic groupings of \"deceptive\" sets. We find\nthat rigorously measuring language model robustness to variation in frequency\nand length is challenging and present an analysis that measures them\nindependently. The code for reproducing the results of this paper, and for\ngenerating the SetLexSem Challenge dataset, is available at\n\\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge}.", "published": "2024-11-11 19:55:24", "link": "http://arxiv.org/abs/2411.07336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-head Span-based Detector for AI-generated Fragments in Scientific\n  Papers", "abstract": "This paper describes a system designed to distinguish between AI-generated\nand human-written scientific excerpts in the DAGPap24 competition hosted within\nthe Fourth Workshop on Scientific Document Processing. In this competition the\ntask is to find artificially generated token-level text fragments in documents\nof a scientific domain. Our work focuses on the use of a multi-task learning\narchitecture with two heads. The application of this approach is justified by\nthe specificity of the task, where class spans are continuous over several\nhundred characters. We considered different encoder variations to obtain a\nstate vector for each token in the sequence, as well as a variation in\nsplitting fragments into tokens to further feed into the input of a\ntransform-based encoder. This approach allows us to achieve a 9% quality\nimprovement relative to the baseline solution score on the development set\n(from 0.86 to 0.95) using the average macro F1-score, as well as a score of\n0.96 on a closed test part of the dataset from the competition.", "published": "2024-11-11 20:05:22", "link": "http://arxiv.org/abs/2411.07343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement --\n  LLaMA3.1 and GPT-4o for Complete Abstract Adaptation", "abstract": "This report is the system description of the MaLei team (Manchester and\nLeiden) for the shared task Plain Language Adaptation of Biomedical Abstracts\n(PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated\nwith TREC2024 (33rd Text REtrieval Conference\nhttps://ir.nist.gov/evalbase/conf/trec-2024). This report contains two sections\ncorresponding to the two sub-tasks in PLABA-2024. In task one (term\nreplacement), we applied fine-tuned ReBERTa-Base models to identify and\nclassify the difficult terms, jargon, and acronyms in the biomedical abstracts\nand reported the F1 score (Task 1A and 1B). In task two (complete abstract\nadaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot\nprompts to complete the abstract adaptation and reported the scores in BLEU,\nSARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024\non Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd\nand 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores\nacross the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed\nmodel achieved the highest Completeness score for Task 2. We share our source\ncodes, fine-tuned models, and related resources at\nhttps://github.com/HECTA-UoM/PLABA2024", "published": "2024-11-11 21:32:06", "link": "http://arxiv.org/abs/2411.07381v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Optimal Search and Retrieval for RAG", "abstract": "Retrieval-augmented generation (RAG) is a promising method for addressing\nsome of the memory-related challenges associated with Large Language Models\n(LLMs). Two separate systems form the RAG pipeline, the retriever and the\nreader, and the impact of each on downstream task performance is not\nwell-understood. Here, we work towards the goal of understanding how retrievers\ncan be optimized for RAG pipelines for common tasks such as Question Answering\n(QA). We conduct experiments focused on the relationship between retrieval and\nRAG performance on QA and attributed QA and unveil a number of insights useful\nto practitioners developing high-performance RAG pipelines. For example,\nlowering search accuracy has minor implications for RAG performance while\npotentially increasing retrieval speed and memory efficiency.", "published": "2024-11-11 22:06:51", "link": "http://arxiv.org/abs/2411.07396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Generative AI and Multi-Agents to Provide Automatic Feedback", "abstract": "This study investigates the use of generative AI and multi-agent systems to\nprovide automatic feedback in educational contexts, particularly for student\nconstructed responses in science assessments. The research addresses a key gap\nin the field by exploring how multi-agent systems, called AutoFeedback, can\nimprove the quality of GenAI-generated feedback, overcoming known issues such\nas over-praise and over-inference that are common in single-agent large\nlanguage models (LLMs). The study developed a multi-agent system consisting of\ntwo AI agents: one for generating feedback and another for validating and\nrefining it. The system was tested on a dataset of 240 student responses, and\nits performance was compared to that of a single-agent LLM. Results showed that\nAutoFeedback significantly reduced the occurrence of over-praise and\nover-inference errors, providing more accurate and pedagogically sound\nfeedback. The findings suggest that multi-agent systems can offer a more\nreliable solution for generating automated feedback in educational settings,\nhighlighting their potential for scalable and personalized learning support.\nThese results have important implications for educators and researchers seeking\nto leverage AI in formative assessments, offering a pathway to more effective\nfeedback mechanisms that enhance student learning outcomes.", "published": "2024-11-11 22:27:36", "link": "http://arxiv.org/abs/2411.07407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Untangling Hate Speech Definitions: A Semantic Componential Analysis\n  Across Cultures and Domains", "abstract": "Hate speech relies heavily on cultural influences, leading to varying\nindividual interpretations. For that reason, we propose a Semantic Componential\nAnalysis (SCA) framework for a cross-cultural and cross-domain analysis of hate\nspeech definitions. We create the first dataset of definitions derived from\nfive domains: online dictionaries, research papers, Wikipedia articles,\nlegislation, and online platforms, which are later analyzed into semantic\ncomponents. Our analysis reveals that the components differ from definition to\ndefinition, yet many domains borrow definitions from one another without taking\ninto account the target culture. We conduct zero-shot model experiments using\nour proposed dataset, employing three popular open-sourced LLMs to understand\nthe impact of different definitions on hate speech detection. Our findings\nindicate that LLMs are sensitive to definitions: responses for hate speech\ndetection change according to the complexity of definitions used in the prompt.", "published": "2024-11-11 22:44:29", "link": "http://arxiv.org/abs/2411.07417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Importance of Homophones Spelling Correction Model for Khmer\n  Authors", "abstract": "Homophones present a significant challenge to authors in any languages due to\ntheir similarities of pronunciations but different meanings and spellings. This\nissue is particularly pronounced in the Khmer language, rich in homophones due\nto its complex structure and extensive character set. This research aims to\naddress the difficulties faced by Khmer authors when using homophones in their\nwriting and proposes potential solutions based on an extensive literature\nreview and survey analysis. A survey of 108 Khmer native speakers, including\nstudents, employees, and professionals, revealed that many frequently encounter\nchallenges with homophones in their writing, often struggling to choose the\ncorrect word based on context. The survey also highlighted the absence of\neffective tools to address homophone errors in Khmer, which complicates the\nwriting process. Additionally, a review of existing studies on spelling\ncorrection in other languages, such as English, Azerbaijani, and Bangla,\nidentified a lack of research focused specifically on homophones, particularly\nin the Khmer language. In summary, this research highlights the necessity for a\nspecialized tool to address Khmer homophone errors. By bridging current gaps in\nresearch and available resources, such a tool would enhance the confidence and\naccuracy of Khmer authors in their writing, thereby contributing to the\nenrichment and preservation of the language. Continued efforts in this domain\nare essential for ensuring that Khmer can leverage advancements in technology\nand linguistics effectively.", "published": "2024-11-11 10:07:03", "link": "http://arxiv.org/abs/2411.10477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Editing for LLMs4Code: How Far are We?", "abstract": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs.", "published": "2024-11-11 00:18:54", "link": "http://arxiv.org/abs/2411.06638v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Explore the Reasoning Capability of LLMs in the Chess Testbed", "abstract": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models.", "published": "2024-11-11 01:42:56", "link": "http://arxiv.org/abs/2411.06655v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridge: A Unified Framework to Knowledge Graph Completion via Language\n  Models and Knowledge Representation", "abstract": "Knowledge graph completion (KGC) is a task of inferring missing triples based\non existing Knowledge Graphs (KGs). Both structural and semantic information\nare vital for successful KGC. However, existing methods only use either the\nstructural knowledge from the KG embeddings or the semantic information from\npre-trained language models (PLMs), leading to suboptimal model performance.\nMoreover, since PLMs are not trained on KGs, directly using PLMs to encode\ntriples may be inappropriate. To overcome these limitations, we propose a novel\nframework called Bridge, which jointly encodes structural and semantic\ninformation of KGs. Specifically, we strategically encode entities and\nrelations separately by PLMs to better utilize the semantic knowledge of PLMs\nand enable structured representation learning via a structural learning\nprinciple. Furthermore, to bridge the gap between KGs and PLMs, we employ a\nself-supervised representation learning method called BYOL to fine-tune PLMs\nwith two different views of a triple. Unlike BYOL, which uses augmentation\nmethods to create two semantically similar views of the same image, potentially\naltering the semantic information. We strategically separate the triple into\ntwo parts to create different views, thus avoiding semantic alteration.\nExperiments demonstrate that Bridge outperforms the SOTA models on three\nbenchmark datasets.", "published": "2024-11-11 01:59:04", "link": "http://arxiv.org/abs/2411.06660v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Should Baby Models Read? Exploring Sample-Efficient Data\n  Composition on Model Performance", "abstract": "We explore the impact of pre-training data composition on the performance of\nsmall language models in a sample-efficient setting. Using datasets limited to\n10 million words, we evaluate several dataset sources, including child-directed\nspeech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and\na mix of these (Mix) across different model sizes ranging from 18 million to\n705 million parameters. Our experiments show that smaller models (e.g.,\nGPT2-97M, GPT2-705M, Llama-360M) perform better when trained on more complex\nand rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories\ndatasets underperformed across all model sizes. These findings suggest that the\noptimal dataset for sample efficient training depends on the model size, and\nthat neither child-directed speech nor simplified stories are optimal for\nlanguage models of all sizes. We highlight the importance of considering both\ndataset composition and model capacity for effective sample efficient language\nmodel training.", "published": "2024-11-11 02:37:21", "link": "http://arxiv.org/abs/2411.06672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning", "abstract": "Fine-tuning pre-trained models for downstream tasks is a widely adopted\ntechnique known for its adaptability and reliability across various domains.\nDespite its conceptual simplicity, fine-tuning entails several troublesome\nengineering choices, such as selecting hyperparameters and determining\ncheckpoints from an optimization trajectory. To tackle the difficulty of\nchoosing the best model, one effective solution is model fusion, which combines\nmultiple models in a parameter space. However, we observe a large discrepancy\nbetween loss and metric landscapes during the fine-tuning of pre-trained\nlanguage models. Building on this observation, we introduce a novel model\nfusion technique that optimizes both the desired metric and loss through\nmulti-objective Bayesian optimization. In addition, to effectively select\nhyperparameters, we establish a two-stage procedure by integrating Bayesian\noptimization processes into our framework. Experiments across various\ndownstream tasks show considerable performance improvements using our Bayesian\noptimization-guided method.", "published": "2024-11-11 04:36:58", "link": "http://arxiv.org/abs/2411.06710v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of\n  Quantization on Model Alignment", "abstract": "With the introduction of the transformers architecture, LLMs have\nrevolutionized the NLP field with ever more powerful models. Nevertheless,\ntheir development came up with several challenges. The exponential growth in\ncomputational power and reasoning capabilities of language models has\nheightened concerns about their security. As models become more powerful,\nensuring their safety has become a crucial focus in research. This paper aims\nto address gaps in the current literature on jailbreaking techniques and the\nevaluation of LLM vulnerabilities. Our contributions include the creation of a\nnovel dataset designed to assess the harmfulness of model outputs across\nmultiple harm levels, as well as a focus on fine-grained harm-level analysis.\nUsing this framework, we provide a comprehensive benchmark of state-of-the-art\njailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.\nAdditionally, we examine how quantization techniques, such as AWQ and GPTQ,\ninfluence the alignment and robustness of models, revealing trade-offs between\nenhanced robustness with regards to transfer attacks and potential increases in\nvulnerability on direct ones. This study aims to demonstrate the influence of\nharmful input queries on the complexity of jailbreaking techniques, as well as\nto deepen our understanding of LLM vulnerabilities and improve methods for\nassessing model robustness when confronted with harmful content, particularly\nin the context of compression strategies.", "published": "2024-11-11 10:02:49", "link": "http://arxiv.org/abs/2411.06835v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface.", "published": "2024-11-11 10:36:04", "link": "http://arxiv.org/abs/2411.06852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EVQAScore: A Fine-grained Metric for Video Question Answering Data\n  Quality Evaluation", "abstract": "Video question-answering (QA) is a core task in video understanding.\nEvaluating the quality of video QA and video caption data quality for training\nvideo large language models (VideoLLMs) is an essential challenge. Although\nvarious methods have been proposed for assessing video caption quality, there\nremains a lack of dedicated evaluation methods for Video QA. To address this\ngap, we introduce EVQAScore, a reference-free method that leverages keyword\nextraction to assess both video caption and video QA data quality.\nAdditionally, we incorporate frame sampling and rescaling techniques to enhance\nthe efficiency and robustness of our evaluation, this enables our score to\nevaluate the quality of extremely long videos. Our approach achieves\nstate-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for\nSpearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on\nthe VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using\nEVQAScore for data selection, we achieved SOTA results with only 12.5\\% of the\noriginal data volume, outperforming the previous SOTA method PAC-S and 100\\% of\ndata.", "published": "2024-11-11 12:11:36", "link": "http://arxiv.org/abs/2411.06908v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual\n  Alignment with Human Smell Experiences", "abstract": "Aligning AI with human intent is important, yet perceptual alignment-how AI\ninterprets what we see, hear, or smell-remains underexplored. This work focuses\non olfaction, human smell experiences. We conducted a user study with 40\nparticipants to investigate how well AI can interpret human descriptions of\nscents. Participants performed \"sniff and describe\" interactive tasks, with our\ndesigned AI system attempting to guess what scent the participants were\nexperiencing based on their descriptions. These tasks evaluated the Large\nLanguage Model's (LLMs) contextual understanding and representation of scent\nrelationships within its internal states - high-dimensional embedding space.\nBoth quantitative and qualitative methods were used to evaluate the AI system's\nperformance. Results indicated limited perceptual alignment, with biases\ntowards certain scents, like lemon and peppermint, and continued failing to\nidentify others, like rosemary. We discuss these findings in light of human-AI\nalignment advancements, highlighting the limitations and opportunities for\nenhancing HCI systems with multisensory experience integration.", "published": "2024-11-11 12:56:52", "link": "http://arxiv.org/abs/2411.06950v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Backpropagation of the Wave Network", "abstract": "This paper provides an in-depth analysis of Wave Network, a novel token\nrepresentation method derived from the Wave Network, designed to capture both\nglobal and local semantics of input text through wave-inspired complex vectors.\nIn complex vector token representation, each token is represented with a\nmagnitude component, capturing the global semantics of the entire input text,\nand a phase component, encoding the relationships between individual tokens and\nthe global semantics. Building on prior research that demonstrated the\neffectiveness of wave-like operations, such as interference and modulation,\nduring forward propagation, this study investigates the convergence behavior,\nbackpropagation characteristics, and embedding independence within the\nToken2Wave framework. A detailed computational complexity analysis shows that\nToken2Wave can significantly reduce video memory usage and training time\ncompared to BERT. Gradient comparisons for the [CLS] token, total input text,\nand classifier parameters further highlight Token2Wave's unique\ncharacteristics. This research offers new insights into wave-based token\nrepresentations, demonstrating their potential to enable efficient and\ncomputationally friendly language model architectures.", "published": "2024-11-11 13:48:01", "link": "http://arxiv.org/abs/2411.06989v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph\n  Link Prediction", "abstract": "Beyond-triple fact representations including hyper-relational facts with\nauxiliary key-value pairs, temporal facts with additional timestamps, and\nnested facts implying relationships between facts, are gaining significant\nattention. However, existing link prediction models are usually designed for\none specific type of facts, making it difficult to generalize to other fact\nrepresentations. To overcome this limitation, we propose a Unified Hierarchical\nRepresentation learning framework (UniHR) for unified knowledge graph link\nprediction. It consists of a unified Hierarchical Data Representation (HiDR)\nmodule and a unified Hierarchical Structure Learning (HiSL) module as graph\nencoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested\nfactual KGs into triple-based representations. Then HiSL incorporates\nintra-fact and inter-fact message passing, focusing on enhancing the semantic\ninformation within individual facts and enriching the structural information\nbetween facts. Experimental results across 7 datasets from 3 types of KGs\ndemonstrate that our UniHR outperforms baselines designed for one specific kind\nof KG, indicating strong generalization capability of HiDR form and the\neffectiveness of HiSL module. Code and data are available at\nhttps://github.com/Lza12a/UniHR.", "published": "2024-11-11 14:22:42", "link": "http://arxiv.org/abs/2411.07019v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models", "abstract": "The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://anonymous.4open.science/r/PARSING-4817/.", "published": "2024-11-11 15:46:07", "link": "http://arxiv.org/abs/2411.07070v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Neural Networks as Recognizers of Formal Languages", "abstract": "Characterizing the computational power of neural network architectures in\nterms of formal language theory remains a crucial line of research, as it\ndescribes lower and upper bounds on the reasoning capabilities of modern AI.\nHowever, when empirically testing these bounds, existing work often leaves a\ndiscrepancy between experiments and the formal claims they are meant to\nsupport. The problem is that formal language theory pertains specifically to\nrecognizers: machines that receive a string as input and classify whether it\nbelongs to a language. On the other hand, it is common instead to evaluate\nlanguage models on proxy tasks, e.g., language modeling or sequence-to-sequence\ntransduction, that are similar in only an informal sense to the underlying\ntheory. We correct this mismatch by training and evaluating neural networks\ndirectly as binary classifiers of strings, using a general method that can be\napplied to a wide variety of languages. As part of this, we extend an algorithm\nrecently proposed by Sn{\\ae}bjarnarson et al. (2025) for efficient\nlength-controlled sampling of strings from regular languages. We provide\nresults on a variety of languages across the Chomsky hierarchy for three neural\narchitectures: a simple RNN, an LSTM, and a causally-masked transformer. We\nfind that the RNN and LSTM often outperform the transformer, and that auxiliary\ntraining objectives such as language modeling can help, although no single\nobjective uniformly improves performance across languages and architectures.\nOur contributions will facilitate theoretically sound empirical testing of\nlanguage recognition claims in future work. We have released our datasets as a\nbenchmark called FLaRe (Formal Language Recognition), along with our code.", "published": "2024-11-11 16:33:25", "link": "http://arxiv.org/abs/2411.07107v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking LLMs' Judgments with No Gold Standard", "abstract": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset.", "published": "2024-11-11 16:58:36", "link": "http://arxiv.org/abs/2411.07127v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning", "abstract": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.", "published": "2024-11-11 17:06:48", "link": "http://arxiv.org/abs/2411.07133v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals", "abstract": "Task-Oriented Dialogue (TOD) systems assist users in completing tasks through\nnatural language interactions, often relying on a single-layered workflow\nstructure for slot-filling in public tasks, such as hotel bookings. However, in\nenterprise environments, which involve rich domain-specific knowledge, TOD\nsystems face challenges due to task complexity and the lack of standardized\ndocumentation. In this work, we introduce HierTOD, an enterprise TOD system\ndriven by hierarchical goals and can support composite workflows. By focusing\non goal-driven interactions, our system serves a more proactive role,\nfacilitating mixed-initiative dialogue and improving task completion. Equipped\nwith components for natural language understanding, composite goal retriever,\ndialogue management, and response generation, backed by a well-organized data\nservice with domain knowledge base and retrieval engine, HierTOD delivers\nefficient task assistance. Furthermore, our system implementation unifies two\nTOD paradigms: slot-filling for information collection and step-by-step\nguidance for task execution. Our human study demonstrates the effectiveness and\nhelpfulness of HierTOD in performing both paradigms.", "published": "2024-11-11 17:28:19", "link": "http://arxiv.org/abs/2411.07152v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Super Weight in Large Language Models", "abstract": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.", "published": "2024-11-11 18:05:48", "link": "http://arxiv.org/abs/2411.07191v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TreeCoders: Trees of Transformers", "abstract": "In this paper, we introduce TreeCoders, a novel family of transformer trees.\nWe moved away from traditional linear transformers to complete k-ary trees.\nTransformer blocks serve as nodes, and generic classifiers learn to select the\nbest child and route the sequence of tokens to a specific leaf. The selectors,\nmoved outside the transformer blocks, allow for the use of a variety of\narchitecture without further modifications. Furthermore, our proposed\narchitecture supports sparse node activation due to the logarithmic complexity\nof a tree search. We validate our idea by testing a series of decoder-only tree\ntransformers, achieving competitive results across a diverse range of language\ndatasets. Our study demonstrates that the proposed tree transformer model\noutperforms a size-equivalent linear transformer model 76\\% of the time over a\nwide range of tree architectures. Furthermore, our proposed model naturally\nlends itself to distributed implementation.", "published": "2024-11-11 18:40:04", "link": "http://arxiv.org/abs/2411.07218v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on\n  Pre-trained Language Models", "abstract": "With the widespread of digital environments, reliable authentication and\ncontinuous access control has become crucial. It can minimize cyber attacks and\nprevent frauds, specially those associated with identity theft. A particular\ninterest lies on keystroke dynamics (KD), which refers to the task of\nrecognizing individuals' identity based on their unique typing style. In this\nwork, we propose the use of pre-trained language models (PLMs) to recognize\nsuch patterns. Although PLMs have shown high performance on multiple NLP\nbenchmarks, the use of these models on specific tasks requires customization.\nBERT and RoBERTa, for instance, rely on subword tokenization, and they cannot\nbe directly applied to KD, which requires temporal-character information to\nrecognize users. Recent character-aware PLMs are able to process both subwords\nand character-level information and can be an alternative solution.\nNotwithstanding, they are still not suitable to be directly fine-tuned for KD\nas they are not optimized to account for user's temporal typing information\n(e.g., hold time and flight time). To overcome this limitation, we propose\nTempCharBERT, an architecture that incorporates temporal-character information\nin the embedding layer of CharBERT. This allows modeling keystroke dynamics for\nthe purpose of user identification and authentication. Our results show a\nsignificant improvement with this customization. We also showed the feasibility\nof training TempCharBERT on a federated learning settings in order to foster\ndata privacy.", "published": "2024-11-11 18:44:17", "link": "http://arxiv.org/abs/2411.07224v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts", "abstract": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath", "published": "2024-11-11 18:59:02", "link": "http://arxiv.org/abs/2411.07240v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Isochrony-Controlled Speech-to-Text Translation: A study on translating\n  from Sino-Tibetan to Indo-European Languages", "abstract": "End-to-end speech translation (ST), which translates source language speech\ndirectly into target language text, has garnered significant attention in\nrecent years. Many ST applications require strict length control to ensure that\nthe translation duration matches the length of the source audio, including both\nspeech and pause segments. Previous methods often controlled the number of\nwords or characters generated by the Machine Translation model to approximate\nthe source sentence's length without considering the isochrony of pauses and\nspeech segments, as duration can vary between languages. To address this, we\npresent improvements to the duration alignment component of our\nsequence-to-sequence ST model. Our method controls translation length by\npredicting the duration of speech and pauses in conjunction with the\ntranslation process. This is achieved by providing timing information to the\ndecoder, ensuring it tracks the remaining duration for speech and pauses while\ngenerating the translation. The evaluation on the Zh-En test set of CoVoST 2,\ndemonstrates that the proposed Isochrony-Controlled ST achieves 0.92 speech\noverlap and 8.9 BLEU, which has only a 1.4 BLEU drop compared to the ST\nbaseline.", "published": "2024-11-11 21:39:21", "link": "http://arxiv.org/abs/2411.07387v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Controllable Context Sensitivity and the Knob Behind It", "abstract": "When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior.", "published": "2024-11-11 22:22:21", "link": "http://arxiv.org/abs/2411.07404v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Scaling Laws with Statistical and Approximation Theory for\n  Transformer Neural Networks on Intrinsically Low-dimensional Data", "abstract": "When training deep neural networks, a model's generalization error is often\nobserved to follow a power scaling law dependent both on the model size and the\ndata size. Perhaps the best known example of such scaling laws are for\ntransformer-based large language models, where networks with billions of\nparameters are trained on trillions of tokens of text. Yet, despite sustained\nwidespread interest, a rigorous understanding of why transformer scaling laws\nexist is still missing. To answer this question, we establish novel statistical\nestimation and mathematical approximation theories for transformers when the\ninput data are concentrated on a low-dimensional manifold. Our theory predicts\na power law between the generalization error and both the training data size\nand the network size for transformers, where the power depends on the intrinsic\ndimension $d$ of the training data. Notably, the constructed model architecture\nis shallow, requiring only logarithmic depth in $d$. By leveraging\nlow-dimensional data structures under a manifold hypothesis, we are able to\nexplain transformer scaling laws in a way which respects the data geometry.\nMoreover, we test our theory with empirical observation by training LLMs on\nnatural language datasets. We find the observed empirical data scaling laws\nclosely agree with our theoretical predictions. Taken together, these results\nrigorously show the intrinsic dimension of data to be a crucial quantity\naffecting transformer scaling laws in both theory and practice.", "published": "2024-11-11 01:05:28", "link": "http://arxiv.org/abs/2411.06646v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Renaissance: Investigating the Pretraining of Vision-Language Encoders", "abstract": "In the past several years there has been an explosion of available models for\nvision-language tasks. Unfortunately, the literature still leaves open a number\nof questions related to best practices in designing and training such models.\nIn this paper we seek to answer several questions related to the pretraining of\nvision-language encoders through meta-analysis. In our first set of\nexperiments, we show that we can save significant compute at no cost to\ndownstream performance, by freezing large parts of vision-language models\nduring pretraining. In our second set of experiments we examine the effect of\nbasing a VL transformer on a vision model versus a text model. Additionally, we\nintroduce a VL modeling platform called Renaissance that we use to conduct all\nof the experiments. This program offers a great deal of flexibility in\ncreating, training and evaluating transformer encoders for VL modeling. The\nsource code for Renaissance can be found at\nhttps://github.com/bsu-slim/renaissance.", "published": "2024-11-11 01:44:54", "link": "http://arxiv.org/abs/2411.06657v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing", "abstract": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger.", "published": "2024-11-11 07:47:20", "link": "http://arxiv.org/abs/2411.06767v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-scale moral machine experiment on large language models", "abstract": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 52 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making.", "published": "2024-11-11 08:36:49", "link": "http://arxiv.org/abs/2411.06790v2", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "LA4SR: illuminating the dark proteome with generative AI", "abstract": "AI language models (LMs) show promise for biological sequence analysis. We\nre-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba,\nranging from 70M to 12B parameters) for microbial sequence classification. The\nmodels achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the\nrecall of BLASTP. They effectively classified the algal dark proteome -\nuncharacterized proteins comprising about 65% of total proteins - validated on\nnew data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger\n(>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2%\nof available data, rapidly achieving strong generalization capacity. High\naccuracy was achieved when training data had intact or scrambled terminal\ninformation, demonstrating robust generalization to incomplete sequences.\nFinally, we provide custom AI explainability software tools for attributing\namino acid patterns to AI generative processes and interpret their outputs in\nevolutionary and biophysical contexts.", "published": "2024-11-11 08:51:18", "link": "http://arxiv.org/abs/2411.06798v2", "categories": ["q-bio.GN", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "q-bio.GN"}
{"title": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant", "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses.", "published": "2024-11-11 09:03:52", "link": "http://arxiv.org/abs/2411.06805v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-NEO: Parameter Efficient Knowledge Distillation for Large Language\n  Models", "abstract": "Knowledge distillation (KD) has been a predominant method for compressing\nLarge Language Models (LLMs). In this paper, we first revisit KD and Low-Rank\nAdaption (LoRA) and demonstrate that they follow the same paradigm. Inspired by\nthis observation, we propose a parameter-efficient knowledge distillation\nmethod, LLM-NEO, which integrates LoRA into KD to improve the efficiency of\nknowledge transfer. After that, we summarize some valuable guidelines for the\nhyperparameters in LLM-NEO. Experimental results on compressing Llama 2 and\nLlama 3.2 show that LLM-NEO outperforms various baselines. Further analysis\ndemonstrates the robustness of the proposed LLM-NEO on variants of LoRA. The\ncode and trained models are available at\n[Github](https://github.com/yang3121099/LLM-Neo).", "published": "2024-11-11 10:07:51", "link": "http://arxiv.org/abs/2411.06839v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs", "abstract": "This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations.", "published": "2024-11-11 10:34:36", "link": "http://arxiv.org/abs/2411.06850v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense\n  Question Answering", "abstract": "Commonsense question answering is a crucial task that requires machines to\nemploy reasoning according to commonsense. Previous studies predominantly\nemploy an extracting-and-modeling paradigm to harness the information in KG,\nwhich first extracts relevant subgraphs based on pre-defined rules and then\nproceeds to design various strategies aiming to improve the representations and\nfusion of the extracted structural knowledge. Despite their effectiveness,\nthere are still two challenges. On one hand, subgraphs extracted by rule-based\nmethods may have the potential to overlook critical nodes and result in\nuncontrollable subgraph size. On the other hand, the misalignment between graph\nand text modalities undermines the effectiveness of knowledge fusion,\nultimately impacting the task performance. To deal with the problems above, we\npropose a novel framework: \\textbf{S}ubgraph R\\textbf{E}trieval Enhanced by\nGra\\textbf{P}h-\\textbf{T}ext \\textbf{A}lignment, named \\textbf{SEPTA}. Firstly,\nwe transform the knowledge graph into a database of subgraph vectors and\npropose a BFS-style subgraph sampling strategy to avoid information loss,\nleveraging the analogy between BFS and the message-passing mechanism. In\naddition, we propose a bidirectional contrastive learning approach for\ngraph-text alignment, which effectively enhances both subgraph retrieval and\nknowledge fusion. Finally, all the retrieved information is combined for\nreasoning in the prediction module. Extensive experiments on five datasets\ndemonstrate the effectiveness and robustness of our framework.", "published": "2024-11-11 10:57:31", "link": "http://arxiv.org/abs/2411.06866v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "LongSafety: Enhance Safety for Long-Context LLMs", "abstract": "Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios.", "published": "2024-11-11 11:57:37", "link": "http://arxiv.org/abs/2411.06899v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-class Decoding of Attended Speaker Direction Using\n  Electroencephalogram and Audio Spatial Spectrum", "abstract": "Decoding the directional focus of an attended speaker from listeners'\nelectroencephalogram (EEG) signals is essential for developing brain-computer\ninterfaces to improve the quality of life for individuals with hearing\nimpairment. Previous works have concentrated on binary directional focus\ndecoding, i.e., determining whether the attended speaker is on the left or\nright side of the listener. However, a more precise decoding of the exact\ndirection of the attended speaker is necessary for effective speech processing.\nAdditionally, audio spatial information has not been effectively leveraged,\nresulting in suboptimal decoding results. In this paper, it is found that on\nthe recently presented dataset with 14-class directional focus, models relying\nexclusively on EEG inputs exhibit significantly lower accuracy when decoding\nthe directional focus in both leave-one-subject-out and leave-one-trial-out\nscenarios. By integrating audio spatial spectra with EEG features, the decoding\naccuracy can be effectively improved. The CNN, LSM-CNN, and Deformer models are\nemployed to decode the directional focus from listeners' EEG signals and audio\nspatial spectra. The proposed Sp-EEG-Deformer model achieves notable 14-class\ndecoding accuracies of 55.35% and 57.19% in leave-one-subject-out and\nleave-one-trial-out scenarios with a decision window of 1 second, respectively.\nExperiment results indicate increased decoding accuracy as the number of\nalternative directions reduces. These findings suggest the efficacy of our\nproposed dual modal directional focus decoding strategy.", "published": "2024-11-11 12:32:26", "link": "http://arxiv.org/abs/2411.06928v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Minion: A Technology Probe for Resolving Value Conflicts through\n  Expert-Driven and User-Driven Strategies in AI Companion Applications", "abstract": "AI companions based on large language models can role-play and converse very\nnaturally. When value conflicts arise between the AI companion and the user, it\nmay offend or upset the user. Yet, little research has examined such conflicts.\nWe first conducted a formative study that analyzed 151 user complaints about\nconflicts with AI companions, providing design implications for our study.\nBased on these, we created Minion, a technology probe to help users resolve\nhuman-AI value conflicts. Minion applies a user-empowerment intervention method\nthat provides suggestions by combining expert-driven and user-driven conflict\nresolution strategies. We conducted a technology probe study, creating 40 value\nconflict scenarios on Character.AI and Talkie. 22 participants completed 274\ntasks and successfully resolved conflicts 94.16% of the time. We summarize user\nresponses, preferences, and needs in resolving value conflicts, and propose\ndesign implications to reduce conflicts and empower users to resolve them more\neffectively.", "published": "2024-11-11 14:49:43", "link": "http://arxiv.org/abs/2411.07042v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training", "abstract": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.", "published": "2024-11-11 15:30:16", "link": "http://arxiv.org/abs/2411.07066v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Universal Response and Emergence of Induction in LLMs", "abstract": "While induction is considered a key mechanism for in-context learning in\nLLMs, understanding its precise circuit decomposition beyond toy models remains\nelusive. Here, we study the emergence of induction behavior within LLMs by\nprobing their response to weak single-token perturbations of the residual\nstream. We find that LLMs exhibit a robust, universal regime in which their\nresponse remains scale-invariant under changes in perturbation strength,\nthereby allowing us to quantify the build-up of token correlations throughout\nthe model. By applying our method, we observe signatures of induction behavior\nwithin the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across\nall models, we find that these induction signatures gradually emerge within\nintermediate layers and identify the relevant model sections composing this\nbehavior. Our results provide insights into the collective interplay of\ncomponents within LLMs and serve as a benchmark for large-scale circuit\nanalysis.", "published": "2024-11-11 15:47:15", "link": "http://arxiv.org/abs/2411.07071v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt", "abstract": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin.", "published": "2024-11-11 16:37:40", "link": "http://arxiv.org/abs/2411.07111v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "More Expressive Attention with Negative Weights", "abstract": "We propose a novel attention mechanism, named Cog Attention, that enables\nattention weights to be negative for enhanced expressiveness, which stems from\ntwo key factors: (1) Cog Attention enhances parameter flexibility. For example,\nunlike traditional softmax attention heads that use a static output-value (OV)\nmatrix to delete or copy inputs that the heads attend to, Cog Attention\nnaturally learns to use the sign of dynamic query-key (QK) inner products to\nrepresent these operations. This enables Cog Attention to perform multiple\noperations simultaneously within a single head. Meanwhile, Cog Attention's OV\nmatrix can focus more on refinement or modification. (2) Cog Attention enhances\nthe model's robustness against representational collapse by preventing the\n``over-squashing'' of earlier tokens into later positions. We develop\nTransformer-like models which use Cog Attention as attention modules, including\ndecoder-only models at various scales for language modeling and U-ViT diffusion\nmodels for image generation. Experiments show that models using Cog Attention\nexhibit superior performance compared to those employing traditional softmax\nattention modules. Our approach suggests a promising research direction for\nrethinking and breaking the entrenched constraints of traditional softmax\nattention, such as the requirement for non-negative weights.", "published": "2024-11-11 17:56:28", "link": "http://arxiv.org/abs/2411.07176v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gumbel Counterfactual Generation From Language Models", "abstract": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.", "published": "2024-11-11 17:57:30", "link": "http://arxiv.org/abs/2411.07180v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Surprising Effectiveness of Test-Time Training for Few-Shot Learning", "abstract": "Language models (LMs) have shown impressive performance on tasks within their\ntraining distribution, but often struggle with structurally novel tasks even\nwhen given a small number of in-context task examples. We investigate the\neffectiveness of test-time training (TTT) -- temporarily updating model\nparameters during inference using a loss derived from input data -- as a\nmechanism for improving LMs' reasoning and few-shot learning capabilities. On\nthe Abstraction and Reasoning Corpus (ARC), performing TTT with in-context\nexamples yields up to $6\\times$ higher accuracy compared to fine-tuned\nbaselines -- reaching $53.0\\%$ on the public validation set with an\n8B-parameter LM and $61.9\\%$ when ensembled with program-synthesis methods,\nmatching average human performance. On BIG-Bench Hard (BBH), TTT on in-context\nexamples surpasses standard few-shot prompting in the $10$-shot setting by\n$7.3$ percentage points ($50.5\\%$ to $57.8\\%$). Our findings highlight the\nlimitations of in-context learning for novel tasks and demonstrate the\npotential of test-time training to enhance language model adaptability.", "published": "2024-11-11 18:59:45", "link": "http://arxiv.org/abs/2411.07279v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Richer Output for Richer Countries: Uncovering Geographical Disparities\n  in Generated Stories and Travel Recommendations", "abstract": "While a large body of work inspects language models for biases concerning\ngender, race, occupation and religion, biases of geographical nature are\nrelatively less explored. Some recent studies benchmark the degree to which\nlarge language models encode geospatial knowledge. However, the impact of the\nencoded geographical knowledge (or lack thereof) on real-world applications has\nnot been documented. In this work, we examine large language models for two\ncommon scenarios that require geographical knowledge: (a) travel\nrecommendations and (b) geo-anchored story generation. Specifically, we study\nfive popular language models, and across about $100$K travel requests, and\n$200$K story generations, we observe that travel recommendations corresponding\nto poorer countries are less unique with fewer location references, and stories\nfrom these regions more often convey emotions of hardship and sadness compared\nto those from wealthier nations.", "published": "2024-11-11 19:25:25", "link": "http://arxiv.org/abs/2411.07320v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical\n  Concern-related App Reviews", "abstract": "With the increasing proliferation of mobile applications in our everyday\nexperiences, the concerns surrounding ethics have surged significantly. Users\ngenerally communicate their feedback, report issues, and suggest new\nfunctionalities in application (app) reviews, frequently emphasizing safety,\nprivacy, and accountability concerns. Incorporating these reviews is essential\nto developing successful products. However, app reviews related to ethical\nconcerns generally use domain-specific language and are expressed using a more\nvaried vocabulary. Thus making automated ethical concern-related app review\nextraction a challenging and time-consuming effort.\n  This study proposes a novel Natural Language Processing (NLP) based approach\nthat combines Natural Language Inference (NLI), which provides a deep\ncomprehension of language nuances, and a decoder-only (LLaMA-like) Large\nLanguage Model (LLM) to extract ethical concern-related app reviews at scale.\nUtilizing 43,647 app reviews from the mental health domain, the proposed\nmethodology 1) Evaluates four NLI models to extract potential privacy reviews\nand compares the results of domain-specific privacy hypotheses with generic\nprivacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to\nprivacy concerns; and 3) Uses the best NLI and LLM models further to extract\nnew privacy reviews from the dataset. Results show that the\nDeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses\nyields the best performance, and Llama3.1-8B-Instruct LLM performs best in the\nclassification of app reviews. Then, using NLI+LLM, an additional 1,008 new\nprivacy-related reviews were extracted that were not identified through the\nkeyword-based approach in previous research, thus demonstrating the\neffectiveness of the proposed approach.", "published": "2024-11-11 22:08:48", "link": "http://arxiv.org/abs/2411.07398v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech\n  Recognition under Realistic Single-Channel Conditions", "abstract": "We propose a single-channel Deep Cascade Fusion of Diarization and Separation\n(DCF-DS) framework for back-end automatic speech recognition (ASR), combining\nneural speaker diarization (NSD) and speech separation (SS). First, we\nsequentially integrate the NSD and SS modules within a joint training\nframework, enabling the separation module to leverage speaker time boundaries\nfrom the diarization module effectively. Then, to complement DCF-DS training,\nwe introduce a window-level decoding scheme that allows the DCF-DS framework to\nhandle the sparse data convergence instability (SDCI) problem. We also explore\nusing an NSD system trained on real datasets to provide more accurate speaker\nboundaries. Additionally, we incorporate an optional multi-input multi-output\nspeech enhancement module (MIMO-SE) within the DCF-DS framework, which offers\nfurther performance gains. Finally, we enhance diarization results by\nre-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the\nDCF-DS method, we achieved first place in the realistic single-channel track of\nthe CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open\nLibriCSS dataset, achieving a new state-of-the-art single-channel speech\nrecognition performance.", "published": "2024-11-11 02:23:08", "link": "http://arxiv.org/abs/2411.06667v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Wavehax: Aliasing-Free Neural Waveform Synthesis Based on 2D Convolution\n  and Harmonic Prior for Reliable Complex Spectrogram Estimation", "abstract": "Neural vocoders often struggle with aliasing in latent feature spaces, caused\nby time-domain nonlinear operations and resampling layers. Aliasing folds\nhigh-frequency components into the low-frequency range, making aliased and\noriginal frequency components indistinguishable and introducing two practical\nissues. First, aliasing complicates the waveform generation process, as the\nsubsequent layers must address these aliasing effects, increasing the\ncomputational complexity. Second, it limits extrapolation performance,\nparticularly in handling high fundamental frequencies, which degrades the\nperceptual quality of generated speech waveforms. This paper demonstrates that\n1) time-domain nonlinear operations inevitably introduce aliasing but provide a\nstrong inductive bias for harmonic generation, and 2) time-frequency-domain\nprocessing can achieve aliasing-free waveform synthesis but lacks the inductive\nbias for effective harmonic generation. Building on this insight, we propose\nWavehax, an aliasing-free neural WAVEform generator that integrates 2D\nconvolution and a HArmonic prior for reliable Complex Spectrogram estimation.\nExperimental results show that Wavehax achieves speech quality comparable to\nexisting high-fidelity neural vocoders and exhibits exceptional robustness in\nscenarios requiring high fundamental frequency extrapolation, where aliasing\neffects become typically severe. Moreover, Wavehax requires less than 5% of the\nmultiply-accumulate operations and model parameters compared to HiFi-GAN V1,\nwhile achieving over four times faster CPU inference speed.", "published": "2024-11-11 09:03:58", "link": "http://arxiv.org/abs/2411.06807v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for\n  Speech Recognition", "abstract": "Selective state space models (SSMs) represented by Mamba have demonstrated\ntheir computational efficiency and promising outcomes in various tasks,\nincluding automatic speech recognition (ASR). Mamba has been applied to ASR\ntask with the attention-based encoder-decoder framework, where the\ncross-attention mechanism between encoder and decoder remains. This paper\nexplores the capability of Mamba as the decoder-only architecture in ASR task.\nOur MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder\nthat takes speech tokens as a condition and predicts text tokens in an\nautoregressive manner. To enhance MADEON, we further propose speech prefixing\nthat performs bidirectional processing on speech tokens, which enriches the\ncontextual information in the hidden states. Our experiments show that MADEON\nsignificantly outperforms a non-selective SSM. The combination of speech\nprefixing and the recently proposed Mamba-2 yields comparable performance to\nTransformer-based models on large datasets.", "published": "2024-11-11 13:17:24", "link": "http://arxiv.org/abs/2411.06968v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AEROMamba: An efficient architecture for audio super-resolution using\n  generative adversarial networks and state space models", "abstract": "Audio super-resolution aims to enhance low-resolution signals by creating\nhigh-frequency content. In this work, we modify the architecture of AERO (a\nstate-of-the-art system for this task) for music super-resolution.\nSPecifically, we replace its original Attention and LSTM layers with Mamba, a\nState Space Model (SSM), across all network layers. Mamba is capable of\neffectively substituting the mentioned modules, as it offers a mechanism\nsimilar to that of Attention while also functioning as a recurrent network.\nWith the proposed AEROMamba, training requires 2-4x less GPU memory, since\nMamba exploits the convolutional formulation and leverages GPU memory\nhierarchy. Additionally, during inference, Mamba operates in constant memory\ndue to recurrence, avoiding memory growth associated with Attention. This\nresults in a 14x speed improvement using 5x less GPU. Subjective listening\ntests (0 to 100 scale) show that the proposed model surpasses the AERO model.\nIn the MUSDB dataset, degraded signals scored 38.22, while AERO and AEROMamba\nscored 60.03 and 66.74, respectively. For the PianoEval dataset, scores were\n72.92 for degraded signals, 76.89 for AERO, and 84.41 for AEROMamba.", "published": "2024-11-11 21:07:56", "link": "http://arxiv.org/abs/2411.07364v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics", "abstract": "Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model.", "published": "2024-11-11 18:01:45", "link": "http://arxiv.org/abs/2411.07186v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Just Label the Repeats for In-The-Wild Audio-to-Score Alignment", "abstract": "We propose an efficient workflow for high-quality offline alignment of\nin-the-wild performance audio and corresponding sheet music scans (images).\nRecent work on audio-to-score alignment extends dynamic time warping (DTW) to\nbe theoretically able to handle jumps in sheet music induced by repeat\nsigns-this method requires no human annotations, but we show that it often\nyields low-quality alignments. As an alternative, we propose a workflow and\ninterface that allows users to quickly annotate jumps (by clicking on repeat\nsigns), requiring a small amount of human supervision but yielding much higher\nquality alignments on average. Additionally, we refine audio and score feature\nrepresentations to improve alignment quality by: (1) integrating measure\ndetection into the score feature representation, and (2) using raw onset\nprediction probabilities from a music transcription model instead of piano\nroll. We propose an evaluation protocol for audio-to-score alignment that\ncomputes the distance between the estimated and ground truth alignment in units\nof measures. Under this evaluation, we find that our proposed jump annotation\nworkflow and improved feature representations together improve alignment\naccuracy by 150% relative to prior work (33% to 82%).", "published": "2024-11-11 23:05:02", "link": "http://arxiv.org/abs/2411.07428v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Discovery Dialogue Generation Using Human Intent Analysis and\n  Large Language Models", "abstract": "A conversational music retrieval system can help users discover music that\nmatches their preferences through dialogue. To achieve this, a conversational\nmusic retrieval system should seamlessly engage in multi-turn conversation by\n1) understanding user queries and 2) responding with natural language and\nretrieved music. A straightforward solution would be a data-driven approach\nutilizing such conversation logs. However, few datasets are available for the\nresearch and are limited in terms of volume and quality. In this paper, we\npresent a data generation framework for rich music discovery dialogue using a\nlarge language model (LLM) and user intents, system actions, and musical\nattributes. This is done by i) dialogue intent analysis using grounded theory,\nii) generating attribute sequences via cascading database filtering, and iii)\ngenerating utterances using large language models. By applying this framework\nto the Million Song dataset, we create LP-MusicDialog, a Large Language Model\nbased Pseudo Music Dialogue dataset, containing over 288k music conversations\nusing more than 319k music items. Our evaluation shows that the synthetic\ndataset is competitive with an existing, small human dialogue dataset in terms\nof dialogue consistency, item relevance, and naturalness. Furthermore, using\nthe dataset, we train a conversational music retrieval model and show promising\nresults.", "published": "2024-11-11 23:40:45", "link": "http://arxiv.org/abs/2411.07439v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
