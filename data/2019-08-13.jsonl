{"title": "Understanding Spatial Language in Radiology: Representation Framework,\n  Annotation, and Spatial Relation Extraction from Chest X-ray Reports using\n  Deep Learning", "abstract": "We define a representation framework for extracting spatial information from\nradiology reports (Rad-SpRL). We annotated a total of 2000 chest X-ray reports\nwith 4 spatial roles corresponding to the common radiology entities. Our focus\nis on extracting detailed information of a radiologist's interpretation\ncontaining a radiographic finding, its anatomical location, corresponding\nprobable diagnoses, as well as associated hedging terms. For this, we propose a\ndeep learning-based natural language processing (NLP) method involving both\nword and character-level encodings. Specifically, we utilize a bidirectional\nlong short-term memory (Bi-LSTM) conditional random field (CRF) model for\nextracting the spatial roles. The model achieved average F1 measures of 90.28\nand 94.61 for extracting the Trajector and Landmark roles respectively whereas\nthe performance was moderate for Diagnosis and Hedge roles with average F1 of\n71.47 and 73.27 respectively. The corpus will soon be made available upon\nrequest.", "published": "2019-08-13 04:44:22", "link": "http://arxiv.org/abs/1908.04485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Relation Knowledge into Commonsense Reading Comprehension\n  with Multi-task Learning", "abstract": "This paper focuses on how to take advantage of external relational knowledge\nto improve machine reading comprehension (MRC) with multi-task learning. Most\nof the traditional methods in MRC assume that the knowledge used to get the\ncorrect answer generally exists in the given documents. However, in real-world\ntask, part of knowledge may not be mentioned and machines should be equipped\nwith the ability to leverage external knowledge. In this paper, we integrate\nrelational knowledge into MRC model for commonsense reasoning. Specifically,\nbased on a pre-trained language model (LM). We design two auxiliary\nrelation-aware tasks to predict if there exists any commonsense relation and\nwhat is the relation type between two words, in order to better model the\ninteractions between document and candidate answer option. We conduct\nexperiments on two multi-choice benchmark datasets: the SemEval-2018 Task 11\nand the Cloze Story Test. The experimental results demonstrate the\neffectiveness of the proposed method, which achieves superior performance\ncompared with the comparable baselines on both datasets.", "published": "2019-08-13 08:28:05", "link": "http://arxiv.org/abs/1908.04530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Offensive Language and Hate Speech Detection for Danish", "abstract": "The presence of offensive language on social media platforms and the\nimplications this poses is becoming a major concern in modern society. Given\nthe enormous amount of content created every day, automatic methods are\nrequired to detect and deal with this type of content. Until now, most of the\nresearch has focused on solving the problem for the English language, while the\nproblem is multilingual.\n  We construct a Danish dataset containing user-generated comments from\n\\textit{Reddit} and \\textit{Facebook}. It contains user generated comments from\nvarious social media platforms, and to our knowledge, it is the first of its\nkind. Our dataset is annotated to capture various types and target of offensive\nlanguage. We develop four automatic classification systems, each designed to\nwork for both the English and the Danish language. In the detection of\noffensive language in English, the best performing system achieves a macro\naveraged F1-score of $0.74$, and the best performing system for Danish achieves\na macro averaged F1-score of $0.70$. In the detection of whether or not an\noffensive post is targeted, the best performing system for English achieves a\nmacro averaged F1-score of $0.62$, while the best performing system for Danish\nachieves a macro averaged F1-score of $0.73$. Finally, in the detection of the\ntarget type in a targeted offensive post, the best performing system for\nEnglish achieves a macro averaged F1-score of $0.56$, and the best performing\nsystem for Danish achieves a macro averaged F1-score of $0.63$.\n  Our work for both the English and the Danish language captures the type and\ntargets of offensive language, and present automatic methods for detecting\ndifferent kinds of offensive language such as hate speech and cyberbullying.", "published": "2019-08-13 08:29:48", "link": "http://arxiv.org/abs/1908.04531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EASSE: Easier Automatic Sentence Simplification Evaluation", "abstract": "We introduce EASSE, a Python package aiming to facilitate and standardise\nautomatic evaluation and comparison of Sentence Simplification (SS) systems.\nEASSE provides a single access point to a broad range of evaluation resources:\nstandard automatic metrics for assessing SS outputs (e.g. SARI), word-level\naccuracy scores for certain simplification transformations,\nreference-independent quality estimation features (e.g. compression ratio), and\nstandard test data for SS evaluation (e.g. TurkCorpus). Finally, EASSE\ngenerates easy-to-visualise reports on the various metrics and features above\nand on how a particular SS output fares against reference simplifications.\nThrough experiments, we show that these functionalities allow for better\ncomparison and understanding of the performance of SS systems.", "published": "2019-08-13 10:17:11", "link": "http://arxiv.org/abs/1908.04567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep\n  Language Understanding", "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized\nversion RoBERTa), has attracted a lot of attention in natural language\nunderstanding (NLU), and achieved state-of-the-art accuracy in various NLU\ntasks, such as sentiment classification, natural language inference, semantic\ntextual similarity and question answering. Inspired by the linearization\nexploration work of Elman [8], we extend BERT to a new model, StructBERT, by\nincorporating language structures into pre-training. Specifically, we pre-train\nStructBERT with two auxiliary tasks to make the most of the sequential order of\nwords and sentences, which leverage language structures at the word and\nsentence levels, respectively. As a result, the new model is adapted to\ndifferent levels of language understanding required by downstream tasks. The\nStructBERT with structural pre-training gives surprisingly good empirical\nresults on a variety of downstream tasks, including pushing the\nstate-of-the-art on the GLUE benchmark to 89.0 (outperforming all published\nmodels), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on\nSNLI to 91.7.", "published": "2019-08-13 11:12:58", "link": "http://arxiv.org/abs/1908.04577v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention is not not Explanation", "abstract": "Attention mechanisms play a central role in NLP systems, especially within\nrecurrent neural network (RNN) models. Recently, there has been increasing\ninterest in whether or not the intermediate representations offered by these\nmodules may be used to explain the reasoning for a model's prediction, and\nconsequently reach insights regarding the model's decision-making process. A\nrecent paper claims that `Attention is not Explanation' (Jain and Wallace,\n2019). We challenge many of the assumptions underlying this work, arguing that\nsuch a claim depends on one's definition of explanation, and that testing it\nneeds to take into account all elements of the model, using a rigorous\nexperimental design. We propose four alternative tests to determine\nwhen/whether attention can be used as explanation: a simple uniform-weights\nbaseline; a variance calibration based on multiple random seed runs; a\ndiagnostic framework using frozen weights from pretrained models; and an\nend-to-end adversarial attention training protocol. Each allows for meaningful\ninterpretation of attention mechanisms in RNN models. We show that even when\nreliable adversarial distributions can be found, they don't perform well on the\nsimple diagnostic, indicating that prior work does not disprove the usefulness\nof attention mechanisms for explainability.", "published": "2019-08-13 13:15:04", "link": "http://arxiv.org/abs/1908.04626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Playing log(N)-Questions over Sentences", "abstract": "We propose a two-agent game wherein a questioner must be able to conjure\ndiscerning questions between sentences, incorporate responses from an answerer,\nand keep track of a hypothesis state. The questioner must be able to understand\nthe information required to make its final guess, while also being able to\nreason over the game's text environment based on the answerer's responses. We\nexperiment with an end-to-end model where both agents can learn simultaneously\nto play the game, showing that simultaneously achieving high game accuracy and\nproducing meaningful questions can be a difficult trade-off.", "published": "2019-08-13 14:31:05", "link": "http://arxiv.org/abs/1908.04660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Noisy Lexical Constraints", "abstract": "Lexically constrained decoding for machine translation has shown to be\nbeneficial in previous studies. Unfortunately, constraints provided by users\nmay contain mistakes in real-world situations. It is still an open question\nthat how to manipulate these noisy constraints in such practical scenarios. We\npresent a novel framework that treats constraints as external memories. In this\nsoft manner, a mistaken constraint can be corrected. Experiments demonstrate\nthat our approach can achieve substantial BLEU gains in handling noisy\nconstraints. These results motivate us to apply the proposed approach on a new\nscenario where constraints are generated without the help of users. Experiments\nshow that our approach can indeed improve the translation quality with the\nautomatically generated constraints.", "published": "2019-08-13 14:32:56", "link": "http://arxiv.org/abs/1908.04664v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Generalization in Coreference Resolution via Adversarial\n  Training", "abstract": "In order for coreference resolution systems to be useful in practice, they\nmust be able to generalize to new text. In this work, we demonstrate that the\nperformance of the state-of-the-art system decreases when the names of PER and\nGPE named entities in the CoNLL dataset are changed to names that do not occur\nin the training set. We use the technique of adversarial gradient-based\ntraining to retrain the state-of-the-art system and demonstrate that the\nretrained system achieves higher performance on the CoNLL dataset (both with\nand without the change of named entities) and the GAP dataset.", "published": "2019-08-13 16:39:48", "link": "http://arxiv.org/abs/1908.04728v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Reasoning over Knowledge Graphs", "abstract": "The ability to reason over learned knowledge is an innate ability for humans\nand humans can easily master new reasoning rules with only a few\ndemonstrations. While most existing studies on knowledge graph (KG) reasoning\nassume enough training examples, we study the challenging and practical problem\nof few-shot knowledge graph reasoning under the paradigm of meta-learning. We\npropose a new meta learning framework that effectively utilizes the\ntask-specific meta information such as local graph neighbors and reasoning\npaths in KGs. Specifically, we design a meta-encoder that encodes the meta\ninformation into task-specific initialization parameters for different tasks.\nThis allows our reasoning module to have diverse starting points when learning\nto reason over different relations, which is expected to better fit the target\ntask. On two few-shot knowledge base completion benchmarks, we show that the\naugmented task-specific meta-encoder yields much better initial point than MAML\nand outperforms several few-shot learning baselines.", "published": "2019-08-13 22:21:25", "link": "http://arxiv.org/abs/1908.04877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical\n  Sequence Labeling Tasks", "abstract": "Biomedical Named Entity Recognition (NER) is a challenging problem in\nbiomedical information processing due to the widespread ambiguity of out of\ncontext terms and extensive lexical variations. Performance on bioNER\nbenchmarks continues to improve due to advances like BERT, GPT, and XLNet.\nFLAIR (1) is an alternative embedding model which is less computationally\nintensive than the others mentioned. We test FLAIR and its pretrained PubMed\nembeddings (which we term BioFLAIR) on a variety of bio NER tasks and compare\nthose with results from BERT-type networks. We also investigate the effects of\na small amount of additional pretraining on PubMed content, and of combining\nFLAIR and ELMO models. We find that with the provided embeddings, FLAIR\nperforms on-par with the BERT networks - even establishing a new state of the\nart on one benchmark. Additional pretraining did not provide a clear benefit,\nalthough this might change with even more pretraining being done. Stacking the\nFLAIR embeddings with others typically does provide a boost in the benchmark\nresults.", "published": "2019-08-13 13:55:48", "link": "http://arxiv.org/abs/1908.05760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tackling Online Abuse: A Survey of Automated Abuse Detection Methods", "abstract": "Abuse on the Internet represents an important societal problem of our time.\nMillions of Internet users face harassment, racism, personal attacks, and other\ntypes of abuse on online platforms. The psychological effects of such abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abuse detection\nin the field of natural language processing (NLP). In this paper, we present a\ncomprehensive survey of the methods that have been proposed to date, thus\nproviding a platform for further development of this area. We describe the\nexisting datasets and review the computational approaches to abuse detection,\nanalyzing their strengths and limitations. We discuss the main trends that\nemerge, highlight the challenges that remain, outline possible solutions, and\npropose guidelines for ethics and explainability", "published": "2019-08-13 10:59:06", "link": "http://arxiv.org/abs/1908.06024v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting To Know You: User Attribute Extraction from Dialogues", "abstract": "User attributes provide rich and useful information for user understanding,\nyet structured and easy-to-use attributes are often sparsely populated. In this\npaper, we leverage dialogues with conversational agents, which contain strong\nsuggestions of user information, to automatically extract user attributes.\nSince no existing dataset is available for this purpose, we apply distant\nsupervision to train our proposed two-stage attribute extractor, which\nsurpasses several retrieval and generation baselines on human evaluation.\nMeanwhile, we discuss potential applications (e.g., personalized recommendation\nand dialogue systems) of such extracted user attributes, and point out current\nlimitations to cast light on future work.", "published": "2019-08-13 13:03:58", "link": "http://arxiv.org/abs/1908.04621v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-grained Information Status Classification Using Discourse\n  Context-Aware Self-Attention", "abstract": "Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the\nproblem as a subtask of learning fine-grained information status (IS). However,\nthese systems heavily depend on many hand-crafted linguistic features. In this\npaper, we propose a discourse context-aware self-attention neural network model\nfor fine-grained IS classification. On the ISNotes corpus (Markert et al.,\n2012), our model with the contextually-encoded word representations (BERT)\n(Devlin et al., 2018) achieves new state-of-the-art performances on\nfine-grained IS classification, obtaining a 4.1% absolute overall accuracy\nimprovement compared to Hou et al. (2013a). More importantly, we also show an\nimprovement of 3.9% F1 for bridging anaphora recognition without using any\ncomplex hand-crafted semantic features designed for capturing the bridging\nphenomenon.", "published": "2019-08-13 17:20:51", "link": "http://arxiv.org/abs/1908.04755v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learn How to Cook a New Recipe in a New House: Using Map\n  Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families\n  of Text-Based Adventure Games", "abstract": "We consider the task of learning to play families of text-based computer\nadventure games, i.e., fully textual environments with a common theme (e.g.\ncooking) and goal (e.g. prepare a meal from a recipe) but with different\nspecifics; new instances of such games are relatively straightforward for\nhumans to master after a brief exposure to the genre but have been curiously\ndifficult for computer agents to learn. We find that the deep Q-learning\nstrategies that have been successfully leveraged for superhuman performance in\nsingle-instance action video games can be applied to learn families of text\nvideo games when adopting simple strategies that correlate with human-like\nlearning behavior. Specifically, we build agents that learn to tackle simple\nscenarios before more complex ones using curriculum learning, that familiarize\nthemselves in an unfamiliar environment by navigating before acting, and that\nexplore uncertain environments more thoroughly using contextual multi-armed\nbandit decision policies. We demonstrate improved task completion rates over\nreasonable baselines when evaluating on never-before-seen games of that theme.", "published": "2019-08-13 17:48:10", "link": "http://arxiv.org/abs/1908.04777v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Effective Domain Adaptive Post-Training Method for BERT in Response\n  Selection", "abstract": "We focus on multi-turn response selection in a retrieval-based dialog system.\nIn this paper, we utilize the powerful pre-trained language model\nBi-directional Encoder Representations from Transformer (BERT) for a multi-turn\ndialog system and propose a highly effective post-training method on\ndomain-specific corpus. Although BERT is easily adopted to various NLP tasks\nand outperforms previous baselines of each task, it still has limitations if a\ntask corpus is too focused on a certain domain. Post-training on\ndomain-specific corpus (e.g., Ubuntu Corpus) helps the model to train\ncontextualized representations and words that do not appear in general corpus\n(e.g., English Wikipedia). Experimental results show that our approach achieves\nnew state-of-the-art on two response selection benchmarks (i.e., Ubuntu Corpus\nV1, Advising Corpus) performance improvement by 5.9% and 6% on R@1.", "published": "2019-08-13 18:24:29", "link": "http://arxiv.org/abs/1908.04812v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entertaining and Opinionated but Too Controlling: A Large-Scale User\n  Study of an Open Domain Alexa Prize System", "abstract": "Conversational systems typically focus on functional tasks such as scheduling\nappointments or creating todo lists. Instead we design and evaluate SlugBot\n(SB), one of 8 semifinalists in the 2018 AlexaPrize, whose goal is to support\ncasual open-domain social inter-action. This novel application requires both\nbroad topic coverage and engaging interactive skills. We developed a new\ntechnical approach to meet this demanding situation by crowd-sourcing novel\ncontent and introducing playful conversational strategies based on storytelling\nand games. We collected over 10,000 conversations during August 2018 as part of\nthe Alexa Prize competition. We also conducted an in-lab follow-up qualitative\nevaluation. Over-all users found SB moderately engaging; conversations averaged\n3.6 minutes and involved 26 user turns. However, users reacted very differently\nto different conversation subtypes. Storytelling and games were evaluated\npositively; these were seen as entertaining with predictable interactive\nstructure. They also led users to impute personality and intelligence to SB. In\ncontrast, search and general Chit-Chat induced coverage problems; here users\nfound it hard to infer what topics SB could understand, with these\nconversations seen as being too system-driven. Theoretical and design\nimplications suggest a move away from conversational systems that simply\nprovide factual information. Future systems should be designed to have their\nown opinions with personal stories to share, and SB provides an example of how\nwe might achieve this.", "published": "2019-08-13 19:10:36", "link": "http://arxiv.org/abs/1908.04832v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Building a Massive Corpus for Named Entity Recognition using Free Open\n  Data Sources", "abstract": "With the recent progress in machine learning, boosted by techniques such as\ndeep learning, many tasks can be successfully solved once a large enough\ndataset is available for training. Nonetheless, human-annotated datasets are\noften expensive to produce, especially when labels are fine-grained, as is the\ncase of Named Entity Recognition (NER), a task that operates with labels on a\nword-level.\n  In this paper, we propose a method to automatically generate labeled datasets\nfor NER from public data sources by exploiting links and structured data from\nDBpedia and Wikipedia. Due to the massive size of these data sources, the\nresulting dataset -- SESAME Available at https://sesame-pt.github.io -- is\ncomposed of millions of labeled sentences. We detail the method to generate the\ndataset, report relevant statistics, and design a baseline using a neural\nnetwork, showing that our dataset helps building better NER predictors.", "published": "2019-08-13 03:47:03", "link": "http://arxiv.org/abs/1908.05758v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Multi-Speaker Speech Recognition using Speaker Embeddings and\n  Transfer Learning", "abstract": "This paper presents our latest investigation on end-to-end automatic speech\nrecognition (ASR) for overlapped speech. We propose to train an end-to-end\nsystem conditioned on speaker embeddings and further improved by transfer\nlearning from clean speech. This proposed framework does not require any\nparallel non-overlapped speech materials and is independent of the number of\nspeakers. Our experimental results on overlapped speech datasets show that\njoint conditioning on speaker embeddings and transfer learning significantly\nimproves the ASR performance.", "published": "2019-08-13 16:56:41", "link": "http://arxiv.org/abs/1908.04737v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "IMS-Speech: A Speech to Text Tool", "abstract": "We present the IMS-Speech, a web based tool for German and English speech\ntranscription aiming to facilitate research in various disciplines which\nrequire accesses to lexical information in spoken language materials. This tool\nis based on modern open source software stack, advanced speech recognition\nmethods and public data resources and is freely available for academic\nresearchers. The utilized models are built to be generic in order to provide\ntranscriptions of competitive accuracy on a diverse set of tasks and\nconditions.", "published": "2019-08-13 17:03:24", "link": "http://arxiv.org/abs/1908.04743v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Variational Fusion for Multimodal Sentiment Analysis", "abstract": "Multimodal fusion is considered a key step in multimodal tasks such as\nsentiment analysis, emotion detection, question answering, and others. Most of\nthe recent work on multimodal fusion does not guarantee the fidelity of the\nmultimodal representation with respect to the unimodal representations. In this\npaper, we propose a variational autoencoder-based approach for modality fusion\nthat minimizes information loss between unimodal and multimodal\nrepresentations. We empirically show that this method outperforms the\nstate-of-the-art methods by a significant margin on several popular datasets.", "published": "2019-08-13 13:39:19", "link": "http://arxiv.org/abs/1908.06008v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RTF-steered binaural MVDR beamforming incorporating multiple external\n  microphones", "abstract": "The binaural minimum-variance distortionless-response (BMVDR) beamformer is a\nwell-known noise reduction algorithm that can be steered using the relative\ntransfer function (RTF) vector of the desired speech source. Exploiting the\navailability of an external microphone that is spatially separated from the\nhead-mounted microphones, an efficient method has been recently proposed to\nestimate the RTF vector in a diffuse noise field. When multiple external\nmicrophones are available, different RTF vector estimates can be obtained by\nusing this method for each external microphone. In this paper, we propose\nseveral procedures to combine these RTF vector estimates, either by selecting\nthe estimate corresponding to the highest input SNR, by averaging the estimates\nor by combining the estimates in order to maximize the output SNR of the BMVDR\nbeamformer. Experimental results for a moving speaker and diffuse noise in a\nreverberant environment show that the output SNR-maximizing combination yields\nthe largest binaural SNR improvement and also outperforms the state-of-the art\ncovariance whitening method.", "published": "2019-08-13 20:37:12", "link": "http://arxiv.org/abs/1908.04848v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Estimating & Mitigating the Impact of Acoustic Environments on\n  Machine-to-Machine Signalling", "abstract": "The advance of technology for transmitting Data-over-Sound in various IoT and\ntelecommunication applications has led to the concept of machine-to-machine\nover-the-air acoustic signalling. Reverberation can have a detrimental effect\non such machine-to-machine signals while decoding. Various methods have been\nstudied to combat the effects of reverberation in speech and audio signals, but\nit is not clear how well they generalise to other sound types. We look at\nextending these models to facilitate machine-to-machine acoustic signalling.\nThis research investigates dereverberation techniques to shortlist a\nsingle-channel reverberation suppression method through a pilot test. In order\nto apply the chosen dereverberation method a novel method of estimating\nacoustic parameters governing reverberation is proposed. The performance of the\nfinal algorithm is evaluated on quality metrics as well as the performance of a\nreal machine-to-machine decoder. We demonstrate a dramatic reduction in error\nrate for both audible and ultrasonic signals.", "published": "2019-08-13 14:45:27", "link": "http://arxiv.org/abs/1908.04672v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
