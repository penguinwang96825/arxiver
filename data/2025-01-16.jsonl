{"title": "Convergence of a Deep BSDE solver with jumps", "abstract": "We study the error arising in the numerical approximation of FBSDEs and\nrelated PIDEs by means of a deep learning-based method. Our results focus on\ndecoupled FBSDEs with jumps and extend the seminal work of HAn and Long (2020)\nanalyzing the numerical error of the deep BSDE solver proposed in E et al.\n(2017). We provide a priori and a posteriori error estimates for the finite and\ninfinite activity case.", "published": "2025-01-16 18:21:14", "link": "http://arxiv.org/abs/2501.09727v1", "categories": ["math.PR", "cs.NA", "math.NA", "math.OC", "q-fin.CP", "q-fin.PR", "60H35, 65C30, 65N75"], "primary_category": "math.PR"}
{"title": "ADAGE: A generic two-layer framework for adaptive agent based modelling", "abstract": "Agent-based models (ABMs) are valuable for modelling complex, potentially\nout-of-equilibria scenarios. However, ABMs have long suffered from the Lucas\ncritique, stating that agent behaviour should adapt to environmental changes.\nFurthermore, the environment itself often adapts to these behavioural changes,\ncreating a complex bi-level adaptation problem. Recent progress integrating\nmulti-agent reinforcement learning into ABMs introduces adaptive agent\nbehaviour, beginning to address the first part of this critique, however, the\napproaches are still relatively ad hoc, lacking a general formulation, and\nfurthermore, do not tackle the second aspect of simultaneously adapting\nenvironmental level characteristics in addition to the agent behaviours. In\nthis work, we develop a generic two-layer framework for ADaptive AGEnt based\nmodelling (ADAGE) for addressing these problems. This framework formalises the\nbi-level problem as a Stackelberg game with conditional behavioural policies,\nproviding a consolidated framework for adaptive agent-based modelling based on\nsolving a coupled set of non-linear equations. We demonstrate how this generic\napproach encapsulates several common (previously viewed as distinct) ABM tasks,\nsuch as policy design, calibration, scenario generation, and robust behavioural\nlearning under one unified framework. We provide example simulations on\nmultiple complex economic and financial environments, showing the strength of\nthe novel framework under these canonical settings, addressing long-standing\ncritiques of traditional ABMs.", "published": "2025-01-16 09:58:24", "link": "http://arxiv.org/abs/2501.09429v1", "categories": ["cs.MA", "cs.AI", "cs.LG", "econ.GN", "q-fin.CP", "q-fin.EC"], "primary_category": "cs.MA"}
{"title": "Optimal Execution among $N$ Traders with Transient Price Impact", "abstract": "We study $N$-player optimal execution games in an Obizhaeva--Wang model of\ntransient price impact. When the game is regularized by an instantaneous cost\non the trading rate, a unique equilibrium exists and we derive its closed form.\nWhereas without regularization, there is no equilibrium. We prove that\nexistence is restored if (and only if) a very particular, time-dependent cost\non block trades is added to the model. In that case, the equilibrium is\nparticularly tractable. We show that this equilibrium is the limit of the\nregularized equilibria as the instantaneous cost parameter $\\varepsilon$ tends\nto zero. Moreover, we explain the seemingly ad-hoc block cost as the limit of\nthe equilibrium instantaneous costs. Notably, in contrast to the single-player\nproblem, the optimal instantaneous costs do not vanish in the limit\n$\\varepsilon\\to0$. We use this tractable equilibrium to study the cost of\nliquidating in the presence of predators and the cost of anarchy. Our results\nalso give a new interpretation to the erratic behaviors previously observed in\ndiscrete-time trading games with transient price impact.", "published": "2025-01-16 16:27:52", "link": "http://arxiv.org/abs/2501.09638v1", "categories": ["q-fin.TR", "q-fin.MF", "91A06, 91A15, 91G10"], "primary_category": "q-fin.TR"}
{"title": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading", "abstract": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.", "published": "2025-01-16 16:25:30", "link": "http://arxiv.org/abs/2501.09636v2", "categories": ["cs.LG", "q-fin.TR"], "primary_category": "cs.LG"}
{"title": "Agent-Based Simulation of a Perpetual Futures Market", "abstract": "I introduce an agent-based model of a Perpetual Futures market with\nheterogeneous agents trading via a central limit order book. Perpetual Futures\n(henceforth Perps) are financial derivatives introduced by the economist Robert\nShiller, designed to peg their price to that of the underlying Spot market.\nThis paper extends the limit order book model of Chiarella et al. (2002) by\ntaking their agent and orderbook parameters, designed for a simple stock\nexchange, and applying it to the more complex environment of a Perp market with\nlong and short traders who exhibit both positional and basis-trading behaviors.\nI find that despite the simplicity of the agent behavior, the simulation is\nable to reproduce the most salient feature of a Perp market, the pegging of the\nPerp price to the underlying Spot price. In contrast to fundamental simulations\nof stock markets which aim to reproduce empirically observed stylized facts\nsuch as the leptokurtosis and heteroscedasticity of returns, volatility\nclustering and others, in derivatives markets many of these features are\nprovided exogenously by the underlying Spot price signal. This is especially\ntrue of Perps since the derivative is designed to mimic the price of the Spot\nmarket. Therefore, this paper will focus exclusively on analyzing how market\nand agent parameters such as order lifetime, trading horizon and spread affect\nthe premiums at which Perps trade with respect to the underlying Spot market. I\nshow that this simulation provides a simple and robust environment for\nexploring the dynamics of Perpetual Futures markets and their microstructure in\nthis regard. Lastly, I explore the ability of the model to reproduce the\neffects of biasing long traders to trade positionally and short traders to\nbasis-trade, which was the original intention behind the market design, and is\na tendency observed empirically in real Perp markets.", "published": "2025-01-16 09:19:08", "link": "http://arxiv.org/abs/2501.09404v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training", "abstract": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.", "published": "2025-01-16 00:19:19", "link": "http://arxiv.org/abs/2501.09213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Short Text Classification with Multi-Source Information\n  Exploration and Dual-Level Contrastive Learning", "abstract": "Short text classification, as a research subtopic in natural language\nprocessing, is more challenging due to its semantic sparsity and insufficient\nlabeled samples in practical scenarios. We propose a novel model named\nMI-DELIGHT for short text classification in this work. Specifically, it first\nperforms multi-source information (i.e., statistical information, linguistic\ninformation, and factual information) exploration to alleviate the sparsity\nissues. Then, the graph learning approach is adopted to learn the\nrepresentation of short texts, which are presented in graph forms. Moreover, we\nintroduce a dual-level (i.e., instance-level and cluster-level) contrastive\nlearning auxiliary task to effectively capture different-grained contrastive\ninformation within massive unlabeled data. Meanwhile, previous models merely\nperform the main task and auxiliary tasks in parallel, without considering the\nrelationship among tasks. Therefore, we introduce a hierarchical architecture\nto explicitly model the correlations between tasks. We conduct extensive\nexperiments across various benchmark datasets, demonstrating that MI-DELIGHT\nsignificantly surpasses previous competitive models. It even outperforms\npopular large language models on several datasets.", "published": "2025-01-16 00:26:15", "link": "http://arxiv.org/abs/2501.09214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Graph Contrastive Learning Framework for Short Text\n  Classification", "abstract": "Short text classification has gained significant attention in the information\nage due to its prevalence and real-world applications. Recent advancements in\ngraph learning combined with contrastive learning have shown promising results\nin addressing the challenges of semantic sparsity and limited labeled data in\nshort text classification. However, existing models have certain limitations.\nThey rely on explicit data augmentation techniques to generate contrastive\nviews, resulting in semantic corruption and noise. Additionally, these models\nonly focus on learning the intrinsic consistency between the generated views,\nneglecting valuable discriminative information from other potential views. To\naddress these issues, we propose a Simple graph contrastive learning framework\nfor Short Text Classification (SimSTC). Our approach involves performing graph\nlearning on multiple text-related component graphs to obtain multi-view text\nembeddings. Subsequently, we directly apply contrastive learning on these\nembeddings. Notably, our method eliminates the need for data augmentation\noperations to generate contrastive views while still leveraging the benefits of\nmulti-view contrastive learning. Despite its simplicity, our model achieves\noutstanding performance, surpassing large language models on various datasets.", "published": "2025-01-16 00:35:56", "link": "http://arxiv.org/abs/2501.09219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithm for Semantic Network Generation from Texts of Low Resource\n  Languages Such as Kiswahili", "abstract": "Processing low-resource languages, such as Kiswahili, using machine learning\nis difficult due to lack of adequate training data. However, such low-resource\nlanguages are still important for human communication and are already in daily\nuse and users need practical machine processing tasks such as summarization,\ndisambiguation and even question answering (QA). One method of processing such\nlanguages, while bypassing the need for training data, is the use semantic\nnetworks. Some low resource languages, such as Kiswahili, are of the\nsubject-verb-object (SVO) structure, and similarly semantic networks are a\ntriple of subject-predicate-object, hence SVO parts of speech tags can map into\na semantic network triple. An algorithm to process raw natural language text\nand map it into a semantic network is therefore necessary and desirable in\nstructuring low resource languages texts. This algorithm tested on the\nKiswahili QA task with upto 78.6% exact match.", "published": "2025-01-16 06:51:32", "link": "http://arxiv.org/abs/2501.09326v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "mGeNTE: A Multilingual Resource for Gender-Neutral Language and\n  Translation", "abstract": "Gender-neutral language reflects societal and linguistic shifts towards\ngreater inclusivity by avoiding the implication that one gender is the norm\nover others. This is particularly relevant for grammatical gender languages,\nwhich heavily encode the gender of terms for human referents and over-relies on\nmasculine forms, even when gender is unspecified or irrelevant. Language\ntechnologies are known to mirror these inequalities, being affected by a male\nbias and perpetuating stereotypical associations when translating into\nlanguages with extensive gendered morphology. In such cases, gender-neutral\nlanguage can help avoid undue binary assumptions. However, despite its\nimportance for creating fairer multi- and cross-lingual technologies, inclusive\nlanguage research remains scarce and insufficiently supported in current\nresources. To address this gap, we present the multilingual mGeNTe dataset.\nDerived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the\noriginal corpus to include the English-Italian/German/Spanish language pairs.\nSince each language pair is English-aligned with gendered and neutral sentences\nin the target languages, mGeNTE enables research in both automatic\nGender-Neutral Translation (GNT) and language modelling for three grammatical\ngender languages.", "published": "2025-01-16 09:35:15", "link": "http://arxiv.org/abs/2501.09409v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling", "abstract": "Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services.", "published": "2025-01-16 09:57:12", "link": "http://arxiv.org/abs/2501.09426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Graph-Based Dependency Parsing with Arc Vectorization and\n  Attention-Based Refinement", "abstract": "We propose a novel architecture for graph-based dependency parsing that\nexplicitly constructs vectors, from which both arcs and labels are scored. Our\nmethod addresses key limitations of the standard two-pipeline approach by\nunifying arc scoring and labeling into a single network, reducing scalability\nissues caused by the information bottleneck and lack of parameter sharing.\nAdditionally, our architecture overcomes limited arc interactions with\ntransformer layers to efficiently simulate higher-order dependencies.\nExperiments on PTB and UD show that our model outperforms state-of-the-art\nparsers in both accuracy and efficiency.", "published": "2025-01-16 10:26:17", "link": "http://arxiv.org/abs/2501.09451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient\n  Simulators", "abstract": "Recently, large language models have shown great potential to transform\nonline medical consultation. Despite this, most research targets improving\ndiagnostic accuracy with ample information, often overlooking the inquiry\nphase. Some studies try to evaluate or refine doctor models by using\nprompt-engineered patient agents. However, prompt engineering alone falls short\nin accurately simulating real patients. We need to explore new paradigms for\npatient simulation. Furthermore, the relationship between inquiry and diagnosis\nremains unexplored. This paper extracts dialogue strategies from real\ndoctor-patient conversations to guide the training of a patient simulator. Our\nsimulator shows higher anthropomorphism and lower hallucination rates, using\ndynamic dialogue strategies. This innovation offers a more accurate evaluation\nof diagnostic models and generates realistic synthetic data. We conduct\nextensive experiments on the relationship between inquiry and diagnosis,\nshowing they adhere to Liebig's law: poor inquiry limits diagnosis\neffectiveness, regardless of diagnostic skill, and vice versa. The experiments\nalso reveal substantial differences in inquiry performance among models. To\ndelve into this phenomenon, the inquiry process is categorized into four\ndistinct types. Analyzing the distribution of inquiries across these types\nhelps explain the performance differences. The weights of our patient simulator\nare available https://github.com/PatientSimulator/PatientSimulator.", "published": "2025-01-16 11:41:14", "link": "http://arxiv.org/abs/2501.09484v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Continuous Semantic Shifts with Diachronic Word Similarity\n  Matrices", "abstract": "The meanings and relationships of words shift over time. This phenomenon is\nreferred to as semantic shift. Research focused on understanding how semantic\nshifts occur over multiple time periods is essential for gaining a detailed\nunderstanding of semantic shifts. However, detecting change points only between\nadjacent time periods is insufficient for analyzing detailed semantic shifts,\nand using BERT-based methods to examine word sense proportions incurs a high\ncomputational cost. To address those issues, we propose a simple yet intuitive\nframework for how semantic shifts occur over multiple time periods by\nleveraging a similarity matrix between the embeddings of the same word through\ntime. We compute a diachronic word similarity matrix using fast and lightweight\nword embeddings across arbitrary time periods, making it deeper to analyze\ncontinuous semantic shifts. Additionally, by clustering the similarity matrices\nfor different words, we can categorize words that exhibit similar behavior of\nsemantic shift in an unsupervised manner.", "published": "2025-01-16 13:42:09", "link": "http://arxiv.org/abs/2501.09538v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylomech: Unveiling Authorship via Computational Stylometry in English\n  and Romanized Sinhala", "abstract": "With the advent of Web 2.0, the development in social technology coupled with\nglobal communication systematically brought positive and negative impacts to\nsociety. Copyright claims and Author identification are deemed crucial as there\nhas been a considerable amount of increase in content violation owing to the\nlack of proper ethics in society. The Author's attribution in both English and\nRomanized Sinhala became a major requirement in the last few decades. As an\narea largely unexplored, particularly within the context of Romanized Sinhala,\nthe research contributes significantly to the field of computational\nlinguistics. The proposed author attribution system offers a unique approach,\nallowing for the comparison of only two sets of text: suspect author and\nanonymous text, a departure from traditional methodologies which often rely on\nlarger corpora. This work focuses on using the numerical representation of\nvarious pairs of the same and different authors allowing for, the model to\ntrain on these representations as opposed to text, this allows for it to apply\nto a multitude of authors and contexts, given that the suspected author text,\nand the anonymous text are of reasonable quality. By expanding the scope of\nauthorship attribution to encompass diverse linguistic contexts, the work\ncontributes to fostering trust and accountability in digital communication,\nespecially in Sri Lanka. This research presents a pioneering approach to author\nattribution in both English and Romanized Sinhala, addressing a critical need\nfor content verification and intellectual property rights enforcement in the\ndigital age.", "published": "2025-01-16 14:26:48", "link": "http://arxiv.org/abs/2501.09561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs", "abstract": "The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection.", "published": "2025-01-16 15:24:41", "link": "http://arxiv.org/abs/2501.09604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation of Foundation LLMs for e-Commerce", "abstract": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.", "published": "2025-01-16 17:58:32", "link": "http://arxiv.org/abs/2501.09706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Insights from 12 Machine Learning Models in Extracting\n  Economic Ideology from Political Text", "abstract": "This study conducts a systematic assessment of the capabilities of 12 machine\nlearning models and model variations in detecting economic ideology. As an\nevaluation benchmark, I use manifesto data spanning six elections in the United\nKingdom and pre-annotated by expert and crowd coders. The analysis assesses the\nperformance of several generative, fine-tuned, and zero-shot models at the\ngranular and aggregate levels. The results show that generative models such as\nGPT-4o and Gemini 1.5 Flash consistently outperform other models against all\nbenchmarks. However, they pose issues of accessibility and resource\navailability. Fine-tuning yielded competitive performance and offers a reliable\nalternative through domain-specific optimization. But its dependency on\ntraining data severely limits scalability. Zero-shot models consistently face\ndifficulties with identifying signals of economic ideology, often resulting in\nnegative associations with human coding. Using general knowledge for the\ndomain-specific task of ideology scaling proved to be unreliable. Other key\nfindings include considerable within-party variation, fine-tuning benefiting\nfrom larger training data, and zero-shot's sensitivity to prompt content. The\nassessments include the strengths and limitations of each model and derive\nbest-practices for automated analyses of political content.", "published": "2025-01-16 18:06:22", "link": "http://arxiv.org/abs/2501.09719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Twitter Social Network Centered on\n  Cryptocurrencies Using Machine Learning", "abstract": "Cryptocurrency is a digital currency that uses blockchain technology with\nsecure encryption. Due to the decentralization of these currencies, traditional\nmonetary systems and the capital market of each they, can influence a society.\nTherefore, due to the importance of the issue, the need to understand public\nopinion and analyze people's opinions in this regard increases. To understand\nthe opinions and views of people about different topics, you can take help from\nsocial networks because they are a rich source of opinions. The Twitter social\nnetwork is one of the main platforms where users discuss various topics,\ntherefore, in the shortest time and with the lowest cost, the opinion of the\ncommunity can be measured on this social network. Twitter Sentiment Analysis\n(TSA) is a field that analyzes the sentiment expressed in tweets. Considering\nthat most of TSA's research efforts on cryptocurrencies are focused on English\nlanguage, the purpose of this paper is to investigate the opinions of Iranian\nusers on the Twitter social network about cryptocurrencies and provide the best\nmodel for classifying tweets based on sentiment. In the case of automatic\nanalysis of tweets, managers and officials in the field of economy can gain\nknowledge from the general public's point of view about this issue and use the\ninformation obtained in order to properly manage this phenomenon. For this\npurpose, in this paper, in order to build emotion classification models,\nnatural language processing techniques such as bag of words (BOW) and FastText\nfor text vectorization and classical machine learning algorithms including KNN,\nSVM and Adaboost learning methods Deep including LSTM and BERT model were used\nfor classification, and finally BERT linguistic model had the best accuracy\nwith 83.50%.", "published": "2025-01-16 16:15:52", "link": "http://arxiv.org/abs/2501.09777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qwen it detect machine-generated text?", "abstract": "This paper describes the approach of the Unibuc - NLP team in tackling the\nColing 2025 GenAI Workshop, Task 1: Binary Multilingual Machine-Generated Text\nDetection. We explored both masked language models and causal models. For\nSubtask A, our best model achieved first-place out of 36 teams when looking at\nF1 Micro (Auxiliary Score) of 0.8333, and second-place when looking at F1 Macro\n(Main Score) of 0.8301", "published": "2025-01-16 19:59:16", "link": "http://arxiv.org/abs/2501.09813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perspective Transition of Large Language Models for Solving Subjective\n  Tasks", "abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.", "published": "2025-01-16 03:30:47", "link": "http://arxiv.org/abs/2501.09265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive\n  Vision-Language Learning", "abstract": "Few-shot learning in medical image classification presents a significant\nchallenge due to the limited availability of annotated data and the complex\nnature of medical imagery. In this work, we propose Adaptive Vision-Language\nFine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework\nthat leverages the capabilities of Large Vision-Language Models (LVLMs) for\nmedical image analysis. HiCA introduces a two-stage fine-tuning strategy,\ncombining domain-specific pretraining and hierarchical contrastive learning to\nalign visual and textual representations at multiple levels. We evaluate our\napproach on two benchmark datasets, Chest X-ray and Breast Ultrasound,\nachieving state-of-the-art performance in both few-shot and zero-shot settings.\nFurther analyses demonstrate the robustness, generalizability, and\ninterpretability of our method, with substantial improvements in performance\ncompared to existing baselines. Our work highlights the potential of\nhierarchical contrastive strategies in adapting LVLMs to the unique challenges\nof medical imaging tasks.", "published": "2025-01-16 05:01:30", "link": "http://arxiv.org/abs/2501.09294v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ChartInsighter: An Approach for Mitigating Hallucination in Time-series\n  Chart Summary Generation with A Benchmark Dataset", "abstract": "Effective chart summary can significantly reduce the time and effort decision\nmakers spend interpreting charts, enabling precise and efficient communication\nof data insights. Previous studies have faced challenges in generating accurate\nand semantically rich summaries of time-series data charts. In this paper, we\nidentify summary elements and common hallucination types in the generation of\ntime-series chart summaries, which serve as our guidelines for automatic\ngeneration. We introduce ChartInsighter, which automatically generates chart\nsummaries of time-series data, effectively reducing hallucinations in chart\nsummary generation. Specifically, we assign multiple agents to generate the\ninitial chart summary and collaborate iteratively, during which they invoke\nexternal data analysis modules to extract insights and compile them into a\ncoherent summary. Additionally, we implement a self-consistency test method to\nvalidate and correct our summary. We create a high-quality benchmark of charts\nand summaries, with hallucination types annotated on a sentence-by-sentence\nbasis, facilitating the evaluation of the effectiveness of reducing\nhallucinations. Our evaluations using our benchmark show that our method\nsurpasses state-of-the-art models, and that our summary hallucination rate is\nthe lowest, which effectively reduces various hallucinations and improves\nsummary quality. The benchmark is available at\nhttps://github.com/wangfen01/ChartInsighter.", "published": "2025-01-16 08:03:32", "link": "http://arxiv.org/abs/2501.09349v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval", "abstract": "Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search.", "published": "2025-01-16 08:52:50", "link": "http://arxiv.org/abs/2501.09384v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Vision-Language Models Do Not Understand Negation", "abstract": "Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and 79k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 40%\nboost in accuracy on multiple-choice questions with negated captions.", "published": "2025-01-16 09:55:42", "link": "http://arxiv.org/abs/2501.09425v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PIER: A Novel Metric for Evaluating What Matters in Code-Switching", "abstract": "Code-switching, the alternation of languages within a single discourse,\npresents a significant challenge for Automatic Speech Recognition. Despite the\nunique nature of the task, performance is commonly measured with established\nmetrics such as Word-Error-Rate (WER). However, in this paper, we question\nwhether these general metrics accurately assess performance on code-switching.\nSpecifically, using both Connectionist-Temporal-Classification and\nEncoder-Decoder models, we show fine-tuning on non-code-switched data from both\nmatrix and embedded language improves classical metrics on code-switching test\nsets, although actual code-switched words worsen (as expected). Therefore, we\npropose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only\non specific words of interest. We instantiate PIER on code-switched utterances\nand show that this more accurately describes the code-switching performance,\nshowing huge room for improvement in future work. This focused evaluation\nallows for a more precise assessment of model performance, particularly in\nchallenging aspects such as inter-word and intra-word code-switching.", "published": "2025-01-16 12:57:33", "link": "http://arxiv.org/abs/2501.09512v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data", "abstract": "We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription.", "published": "2025-01-16 13:16:37", "link": "http://arxiv.org/abs/2501.09521v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Confidence Estimation for Error Detection in Text-to-SQL Systems", "abstract": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.", "published": "2025-01-16 13:23:07", "link": "http://arxiv.org/abs/2501.09527v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating\n  Large Language Models", "abstract": "The recent rise in the popularity of large language models has spurred the\ndevelopment of extensive code datasets needed to train them. This has left\nlimited code available for collection and use in the downstream investigation\nof specific behaviors, or evaluation of large language models without suffering\nfrom data contamination. To address this problem, we release The Heap, a large\nmultilingual dataset covering 57 programming languages that has been\ndeduplicated with respect to other open datasets of code, enabling researchers\nto conduct fair evaluations of large language models without significant data\ncleaning overhead.", "published": "2025-01-16 16:48:41", "link": "http://arxiv.org/abs/2501.09653v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models", "abstract": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.", "published": "2025-01-16 17:37:58", "link": "http://arxiv.org/abs/2501.09686v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Attention based Bidirectional GRU hybrid model for inappropriate content\n  detection in Urdu language", "abstract": "With the increased use of the internet and social networks for online\ndiscussions, the spread of toxic and inappropriate content on social networking\nsites has also increased. Several studies have been conducted in different\nlanguages. However, there is less work done for South Asian languages for\ninappropriate content identification using deep learning techniques. In Urdu\nlanguage, the spellings are not unique, and people write different common\nspellings for the same word, while mixing it other languages, like English in\nthe text makes it more challenging, and limited research work is available to\nprocess such language with the finest algorithms. The use of attention layer\nwith a deep learning model can help handling the long-term dependencies and\nincrease its efficiency . To explore the effects of the attention layer, this\nstudy proposes attention-based Bidirectional GRU hybrid model for identifying\ninappropriate content in Urdu Unicode text language. Four different baseline\ndeep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the\nperformance of the proposed model. The results of these models were compared\nbased on evaluation metrics, dataset size, and impact of the word embedding\nlayer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our\nproposed model BiGRU-A outperformed all other baseline models by yielding 84\\%\naccuracy without using pre-trained word2Vec layer. From our experiments, we\nhave established that the attention layer improves the model's efficiency, and\npre-trained word2Vec embedding does not work well with an inappropriate content\ndataset.", "published": "2025-01-16 18:10:37", "link": "http://arxiv.org/abs/2501.09722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models", "abstract": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).", "published": "2025-01-16 18:57:20", "link": "http://arxiv.org/abs/2501.09749v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs)\n  More Self-Confident Even When They Are Wrong", "abstract": "One of the most widely used methods to evaluate LLMs are Multiple Choice\nQuestion (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on\nalmost any topic at scale as the results can be processed automatically. To\nhelp the LLM answer, a few examples called few shots can be included in the\nprompt. Moreover, the LLM can be asked to answer the question directly with the\nselected option or to first provide the reasoning and then the selected answer,\nwhich is known as chain of thought. In addition to checking whether the\nselected answer is correct, the evaluation can look at the LLM-estimated\nprobability of its response as an indication of the confidence of the LLM in\nthe response. In this paper, we study how the LLM confidence in its answer\ndepends on whether the model has been asked to answer directly or to provide\nthe reasoning before answering. The results of the evaluation of questions on a\nwide range of topics in seven different models show that LLMs are more\nconfident in their answers when they provide reasoning before the answer. This\noccurs regardless of whether the selected answer is correct. Our hypothesis is\nthat this behavior is due to the reasoning that modifies the probability of the\nselected answer, as the LLM predicts the answer based on the input question and\nthe reasoning that supports the selection made. Therefore, LLM estimated\nprobabilities seem to have intrinsic limitations that should be understood in\norder to use them in evaluation procedures. Interestingly, the same behavior\nhas been observed in humans, for whom explaining an answer increases confidence\nin its correctness.", "published": "2025-01-16 10:27:51", "link": "http://arxiv.org/abs/2501.09775v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computing Optimization-Based Prompt Injections Against Closed-Weights\n  Models By Misusing a Fine-Tuning API", "abstract": "We surface a new threat to closed-weight Large Language Models (LLMs) that\nenables an attacker to compute optimization-based prompt injections.\nSpecifically, we characterize how an attacker can leverage the loss-like\ninformation returned from the remote fine-tuning interface to guide the search\nfor adversarial prompts. The fine-tuning interface is hosted by an LLM vendor\nand allows developers to fine-tune LLMs for their tasks, thus providing\nutility, but also exposes enough information for an attacker to compute\nadversarial prompts. Through an experimental analysis, we characterize the\nloss-like values returned by the Gemini fine-tuning API and demonstrate that\nthey provide a useful signal for discrete optimization of adversarial prompts\nusing a greedy search algorithm. Using the PurpleLlama prompt injection\nbenchmark, we demonstrate attack success rates between 65% and 82% on Google's\nGemini family of LLMs. These attacks exploit the classic utility-security\ntradeoff - the fine-tuning interface provides a useful feature for developers\nbut also exposes the LLMs to powerful attacks.", "published": "2025-01-16 19:01:25", "link": "http://arxiv.org/abs/2501.09798v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Conversational Text Extraction with Large Language Models Using\n  Retrieval-Augmented Systems", "abstract": "This study introduces a system leveraging Large Language Models (LLMs) to\nextract text and enhance user interaction with PDF documents via a\nconversational interface. Utilizing Retrieval-Augmented Generation (RAG), the\nsystem provides informative responses to user inquiries while highlighting\nrelevant passages within the PDF. Upon user upload, the system processes the\nPDF, employing sentence embeddings to create a document-specific vector store.\nThis vector store enables efficient retrieval of pertinent sections in response\nto user queries. The LLM then engages in a conversational exchange, using the\nretrieved information to extract text and generate comprehensive, contextually\naware answers. While our approach demonstrates competitive ROUGE values\ncompared to existing state-of-the-art techniques for text extraction and\nsummarization, we acknowledge that further qualitative evaluation is necessary\nto fully assess its effectiveness in real-world applications. The proposed\nsystem gives competitive ROUGE values as compared to existing state-of-the-art\ntechniques for text extraction and summarization, thus offering a valuable tool\nfor researchers, students, and anyone seeking to efficiently extract knowledge\nand gain insights from documents through an intuitive question-answering\ninterface.", "published": "2025-01-16 19:12:25", "link": "http://arxiv.org/abs/2501.09801v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs", "abstract": "This paper investigates the challenges of developing large language models\n(LLMs) proficient in both multilingual understanding and medical knowledge. We\ndemonstrate that simply translating medical data does not guarantee strong\nperformance on clinical tasks in the target language. Our experiments reveal\nthat the optimal language mix in training data varies significantly across\ndifferent medical tasks. We find that larger models with carefully calibrated\nlanguage ratios achieve superior performance on native-language clinical tasks.\nFurthermore, our results suggest that relying solely on fine-tuning may not be\nthe most effective approach for incorporating new language knowledge into LLMs.\nInstead, data and computationally intensive pretraining methods may still be\nnecessary to achieve optimal performance in multilingual medical settings.\nThese findings provide valuable guidance for building effective and inclusive\nmedical AI systems for diverse linguistic communities.", "published": "2025-01-16 20:24:56", "link": "http://arxiv.org/abs/2501.09825v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimization Strategies for Enhancing Resource Efficiency in\n  Transformers & Large Language Models", "abstract": "Advancements in Natural Language Processing are heavily reliant on the\nTransformer architecture, whose improvements come at substantial resource costs\ndue to ever-growing model sizes. This study explores optimization techniques,\nincluding Quantization, Knowledge Distillation, and Pruning, focusing on energy\nand computational efficiency while retaining performance. Among standalone\nmethods, 4-bit Quantization significantly reduces energy use with minimal\naccuracy loss. Hybrid approaches, like NVIDIA's Minitron approach combining KD\nand Structured Pruning, further demonstrate promising trade-offs between size\nreduction and accuracy retention. A novel optimization equation is introduced,\noffering a flexible framework for comparing various methods. Through the\ninvestigation of these compression methods, we provide valuable insights for\ndeveloping more sustainable and efficient LLMs, shining a light on the\noften-ignored concern of energy efficiency.", "published": "2025-01-16 08:54:44", "link": "http://arxiv.org/abs/2502.00046v1", "categories": ["cs.LG", "cs.CL", "68T50"], "primary_category": "cs.LG"}
{"title": "Foundations of Large Language Models", "abstract": "This is a book about large language models. As indicated by the title, it\nprimarily focuses on foundational concepts rather than comprehensive coverage\nof all cutting-edge technologies. The book is structured into four main\nchapters, each exploring a key area: pre-training, generative models, prompting\ntechniques, and alignment methods. It is intended for college students,\nprofessionals, and practitioners in natural language processing and related\nfields, and can serve as a reference for anyone interested in large language\nmodels.", "published": "2025-01-16 01:03:56", "link": "http://arxiv.org/abs/2501.09223v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Delayed Fusion: Integrating Large Language Models into First-Pass\n  Decoding in End-to-end Speech Recognition", "abstract": "This paper presents an efficient decoding approach for end-to-end automatic\nspeech recognition (E2E-ASR) with large language models (LLMs). Although\nshallow fusion is the most common approach to incorporate language models into\nE2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference\nis computationally costly. (2) There may be a vocabulary mismatch between the\nASR model and the LLM. To resolve this mismatch, we need to retrain the ASR\nmodel and/or the LLM, which is at best time-consuming and in many cases not\nfeasible. We propose \"delayed fusion,\" which applies LLM scores to ASR\nhypotheses with a delay during decoding and enables easier use of pre-trained\nLLMs in ASR tasks. This method can reduce not only the number of hypotheses\nscored by the LLM but also the number of LLM inference calls. It also allows\nre-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different\ntokenizations. We demonstrate that delayed fusion provides improved decoding\nspeed and accuracy compared to shallow fusion and N-best rescoring using the\nLibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.", "published": "2025-01-16 03:01:50", "link": "http://arxiv.org/abs/2501.09258v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation", "abstract": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.", "published": "2025-01-16 04:56:33", "link": "http://arxiv.org/abs/2501.09292v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; H.3.3; I.5.5"], "primary_category": "cs.CL"}
{"title": "Understanding Mental Health Content on Social Media and Its Effect\n  Towards Suicidal Ideation", "abstract": "This review underscores the critical need for effective strategies to\nidentify and support individuals with suicidal ideation, exploiting\ntechnological innovations in ML and DL to further suicide prevention efforts.\nThe study details the application of these technologies in analyzing vast\namounts of unstructured social media data to detect linguistic patterns,\nkeywords, phrases, tones, and contextual cues associated with suicidal\nthoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural\nnetworks, and their effectiveness in interpreting complex data patterns and\nemotional nuances within text data. The review discusses the potential of these\ntechnologies to serve as a life-saving tool by identifying at-risk individuals\nthrough their digital traces. Furthermore, it evaluates the real-world\neffectiveness, limitations, and ethical considerations of employing these\ntechnologies for suicide prevention, stressing the importance of responsible\ndevelopment and usage. The study aims to fill critical knowledge gaps by\nanalyzing recent studies, methodologies, tools, and techniques in this field.\nIt highlights the importance of synthesizing current literature to inform\npractical tools and suicide prevention efforts, guiding innovation in reliable,\nethical systems for early intervention. This research synthesis evaluates the\nintersection of technology and mental health, advocating for the ethical and\nresponsible application of ML, DL, and NLP to offer life-saving potential\nworldwide while addressing challenges like generalizability, biases, privacy,\nand the need for further research to ensure these technologies do not\nexacerbate existing inequities and harms.", "published": "2025-01-16 05:46:27", "link": "http://arxiv.org/abs/2501.09309v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Study of In-Context-Learning-Based Text-to-SQL Errors", "abstract": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.", "published": "2025-01-16 05:54:59", "link": "http://arxiv.org/abs/2501.09310v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Shape-Based Single Object Classification Using Ensemble Method\n  Classifiers", "abstract": "Nowadays, more and more images are available. Annotation and retrieval of the\nimages pose classification problems, where each class is defined as the group\nof database images labelled with a common semantic label. Various systems have\nbeen proposed for content-based retrieval, as well as for image classification\nand indexing. In this paper, a hierarchical classification framework has been\nproposed for bridging the semantic gap effectively and achieving multi-category\nimage classification. A well known pre-processing and post-processing method\nwas used and applied to three problems; image segmentation, object\nidentification and image classification. The method was applied to classify\nsingle object images from Amazon and Google datasets. The classification was\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\nBagging and Vote. The estimated classification accuracies ranged from 20% to\n99% (using 10-fold cross validation). The Bagging classifier presents the best\nperformance, followed by the Random Forest classifier.", "published": "2025-01-16 05:58:32", "link": "http://arxiv.org/abs/2501.09311v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy", "abstract": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.", "published": "2025-01-16 09:59:45", "link": "http://arxiv.org/abs/2501.09431v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Solving the Unsolvable: Translating Case Law in Hong Kong", "abstract": "This paper addresses the challenges translating case law under Hong Kong's\nbilingual legal system. It highlights the initial success of translating all\nwritten statutes into Chinese before the 1997 handover, a task mandated by the\nBasic Law. The effort involved significant collaboration among legal,\nlinguistic, and translation experts, resulting in a comprehensive and\nculturally appropriate bilingual legal system. However, translating case law\nremains a significant challenge due to the sheer volume and continuous growth\nof judicial decisions. The paper critiques the governments and judiciarys\nsporadic and uncoordinated efforts to translate case law, contrasting it with\nthe thorough approach previously taken for statute translation. Although the\ngovernment acknowledges the importance of legal bilingualism, it lacks a\nsustainable strategy for translating case law. The Judiciarys position that\ntranslating all judgments is unnecessary, unrealistic, and not cost-effectiveis\nanalyzed and critiqued for its impact on legal transparency and public trust. A\nproposed solution involves leveraging machine translation technology through a\nhuman-machine interactive translation platform, which undergoes two major\ntransitions. Initially based on a neural model, the platform transitions to\nusing a large language model for improved translation accuracy. Furthermore, it\nevolves from a single-agent system to a multi-agent system, incorporating\nTranslator, Annotator, and Proofreader agents. This multi-agent approach,\nsupported by a grant, aims to facilitate efficient, high-quality translation of\njudicial judgments by integrating advanced artificial intelligence and\ncontinuous feedback mechanisms, thus better meeting the needs of a bilingual\nlegal system.", "published": "2025-01-16 10:17:58", "link": "http://arxiv.org/abs/2501.09444v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding", "abstract": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.", "published": "2025-01-16 16:37:33", "link": "http://arxiv.org/abs/2501.09645v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models", "abstract": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.", "published": "2025-01-16 18:55:38", "link": "http://arxiv.org/abs/2501.09745v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller\n  Models", "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging\nnatural language process problem yet highly desirable in many real-life\napplications. Existing CoT knowledge distillation methods often suffer from\noverly conservative memorization in smaller LLMs, leading to low generalization\nconfidence. As fully preserving the CoT ability of teacher model is impossible,\nwe hypothesize that adversarial CoT fine-tuning is crucial for developing\nsmaller LLM with robust CoT generalization. To this end, we propose\n\\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled\nfine-tuning framework that integrates diverse CoT domains. Specifically, PRADA\npioneers two CoT improvements in smaller LLM: (1) Recovering the\ndomain-invariant feature insight which typically lost during distillation with\ndomain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT\nprompt engineering by employing domain-adversarial approaches. We theoretically\ndemonstrate the effectiveness of our approach and empirically show that it\nsignificantly outperforms the state of the arts in a wide range of tasks.\nMoreover, our empirical findings reveal that the smaller LLM, when leveraging\nPRADA, aligns closely with domain knowledge, thereby improving the\nexplainability of our approach.", "published": "2025-01-16 19:23:11", "link": "http://arxiv.org/abs/2501.09804v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking", "abstract": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles.", "published": "2025-01-16 18:58:06", "link": "http://arxiv.org/abs/2501.09751v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic\n  Recognition", "abstract": "Contrastive Language-Audio Pretraining (CLAP) models have demonstrated\nunprecedented performance in various acoustic signal recognition tasks.\nFiber-optic-based acoustic recognition is one of the most important downstream\ntasks and plays a significant role in environmental sensing. Adapting CLAP for\nfiber-optic acoustic recognition has become an active research area. As a\nnon-conventional acoustic sensor, fiber-optic acoustic recognition presents a\nchallenging, domain-specific, low-shot deployment environment with significant\ndomain shifts due to unique frequency response and noise characteristics. To\naddress these challenges, we propose a support-based adaptation method, CLAP-S,\nwhich linearly interpolates a CLAP Adapter with the Support Set, leveraging\nboth implicit knowledge through fine-tuning and explicit knowledge retrieved\nfrom memory for cross-domain generalization. Experimental results show that our\nmethod delivers competitive performance on both laboratory-recorded fiber-optic\nESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our\nresearch also provides valuable insights for other downstream acoustic\nrecognition tasks. The code and gunshot-firework dataset are available at\nhttps://github.com/Jingchensun/clap-s.", "published": "2025-01-16 23:22:17", "link": "http://arxiv.org/abs/2501.09877v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Tessellated Linear Model for Age Prediction from Voice", "abstract": "Voice biometric tasks, such as age estimation require modeling the often\ncomplex relationship between voice features and the biometric variable. While\ndeep learning models can handle such complexity, they typically require large\namounts of accurately labeled data to perform well. Such data are often scarce\nfor biometric tasks such as voice-based age prediction. On the other hand,\nsimpler models like linear regression can work with smaller datasets but often\nfail to generalize to the underlying non-linear patterns present in the data.\nIn this paper we propose the Tessellated Linear Model (TLM), a piecewise linear\napproach that combines the simplicity of linear models with the capacity of\nnon-linear functions. TLM tessellates the feature space into convex regions and\nfits a linear model within each region. We optimize the tessellation and the\nlinear models using a hierarchical greedy partitioning. We evaluated TLM on the\nTIMIT dataset on the task of age prediction from voice, where it outperformed\nstate-of-the-art deep learning models.", "published": "2025-01-16 01:28:45", "link": "http://arxiv.org/abs/2501.09229v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport", "abstract": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.", "published": "2025-01-16 04:53:29", "link": "http://arxiv.org/abs/2501.09291v2", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Multimodal Marvels of Deep Learning in Medical Diagnosis: A\n  Comprehensive Review of COVID-19 Detection", "abstract": "This study presents a comprehensive review of the potential of multimodal\ndeep learning (DL) in medical diagnosis, using COVID-19 as a case example.\nMotivated by the success of artificial intelligence applications during the\nCOVID-19 pandemic, this research aims to uncover the capabilities of DL in\ndisease screening, prediction, and classification, and to derive insights that\nenhance the resilience, sustainability, and inclusiveness of science,\ntechnology, and innovation systems. Adopting a systematic approach, we\ninvestigate the fundamental methodologies, data sources, preprocessing steps,\nand challenges encountered in various studies and implementations. We explore\nthe architecture of deep learning models, emphasising their data-specific\nstructures and underlying algorithms. Subsequently, we compare different deep\nlearning strategies utilised in COVID-19 analysis, evaluating them based on\nmethodology, data, performance, and prerequisites for future research. By\nexamining diverse data types and diagnostic modalities, this research\ncontributes to scientific understanding and knowledge of the multimodal\napplication of DL and its effectiveness in diagnosis. We have implemented and\nanalysed 11 deep learning models using COVID-19 image, text, and speech (ie,\ncough) data. Our analysis revealed that the MobileNet model achieved the\nhighest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data\n(i.e., cough). However, the BiGRU model demonstrated superior performance in\nCOVID-19 text classification with an accuracy of 99.89%. The broader\nimplications of this research suggest potential benefits for other domains and\ndisciplines that could leverage deep learning techniques for image, text, and\nspeech analysis.", "published": "2025-01-16 12:38:49", "link": "http://arxiv.org/abs/2501.09506v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.LG"}
{"title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification\n  in IoT Environments", "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments.", "published": "2025-01-16 09:06:10", "link": "http://arxiv.org/abs/2501.09394v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.PF", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric Learning with Progressive Self-Distillation for Audio-Visual\n  Embedding Learning", "abstract": "Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t", "published": "2025-01-16 15:32:41", "link": "http://arxiv.org/abs/2501.09608v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
