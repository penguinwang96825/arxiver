{"title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "abstract": "Recent QA with logical reasoning questions requires passage-level relations\namong the sentences. However, current approaches still focus on sentence-level\nrelations interacting among tokens. In this work, we explore aggregating\npassage-level clues for solving logical reasoning QA by using discourse-based\ninformation. We propose a discourse-aware graph network (DAGN) that reasons\nrelying on the discourse structure of the texts. The model encodes discourse\ninformation as a graph with elementary discourse units (EDUs) and discourse\nrelations, and learns the discourse-aware features via a graph network for\ndownstream QA tasks. Experiments are conducted on two logical reasoning QA\ndatasets, ReClor and LogiQA, and our proposed DAGN achieves competitive\nresults. The source code is available at https://github.com/Eleanor-H/DAGN.", "published": "2021-03-26 09:41:56", "link": "http://arxiv.org/abs/2103.14349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NL-EDIT: Correcting semantic parse errors through natural language\n  interaction", "abstract": "We study semantic parsing in an interactive setting in which users correct\nerrors with natural language feedback. We present NL-EDIT, a model for\ninterpreting natural language feedback in the interaction context to generate a\nsequence of edits that can be applied to the initial parse to correct its\nerrors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL\nparsers by up to 20% with only one turn of correction. We analyze the\nlimitations of the model and discuss directions for improvement and evaluation.\nThe code and datasets used in this paper are publicly available at\nhttp://aka.ms/NLEdit.", "published": "2021-03-26 15:45:46", "link": "http://arxiv.org/abs/2103.14540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Document Embedding via Contrastive Augmentation", "abstract": "We present a contrasting learning approach with data augmentation techniques\nto learn document representations in an unsupervised manner. Inspired by recent\ncontrastive self-supervised learning algorithms used for image and NLP\npretraining, we hypothesize that high-quality document embedding should be\ninvariant to diverse paraphrases that preserve the semantics of the original\ndocument. With different backbones and contrastive learning frameworks, our\nstudy reveals the enormous benefits of contrastive augmentation for document\nrepresentation learning with two additional insights: 1) including data\naugmentation in a contrastive way can substantially improve the embedding\nquality in unsupervised document representation learning, and 2) in general,\nstochastic augmentations generated by simple word-level manipulation work much\nbetter than sentence-level and document-level ones. We plug our method into a\nclassifier and compare it with a broad range of baseline methods on six\nbenchmark datasets. Our method can decrease the classification error rate by up\nto 6.4% over the SOTA approaches on the document classification task, matching\nor even surpassing fully-supervised methods.", "published": "2021-03-26 15:48:52", "link": "http://arxiv.org/abs/2103.14542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correcting Automated and Manual Speech Transcription Errors using Warped\n  Language Models", "abstract": "Masked language models have revolutionized natural language processing\nsystems in the past few years. A recently introduced generalization of masked\nlanguage models called warped language models are trained to be more robust to\nthe types of errors that appear in automatic or manual transcriptions of spoken\nlanguage by exposing the language model to the same types of errors during\ntraining. In this work we propose a novel approach that takes advantage of the\nrobustness of warped language models to transcription noise for correcting\ntranscriptions of spoken language. We show that our proposed approach is able\nto achieve up to 10% reduction in word error rates of both automatic and manual\ntranscriptions of spoken language.", "published": "2021-03-26 16:43:23", "link": "http://arxiv.org/abs/2103.14580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiGCN: Label-interpretable Graph Convolutional Networks for Multi-label\n  Text Classification", "abstract": "Multi-label text classification (MLTC) is an attractive and challenging task\nin natural language processing (NLP). Compared with single-label text\nclassification, MLTC has a wider range of applications in practice. In this\npaper, we propose a label-interpretable graph convolutional network model to\nsolve the MLTC problem by modeling tokens and labels as nodes in a\nheterogeneous graph. In this way, we are able to take into account multiple\nrelationships including token-level relationships. Besides, the model allows\nbetter interpretability for predicted labels as the token-label edges are\nexposed. We evaluate our method on four real-world datasets and it achieves\ncompetitive scores against selected baseline methods. Specifically, this model\nachieves a gain of 0.14 on the F1 score in the small label set MLTC, and 0.07\nin the large label set scenario.", "published": "2021-03-26 17:33:31", "link": "http://arxiv.org/abs/2103.14620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Embedding-based Joint Sentiment-Topic Model for Short Texts", "abstract": "Short text is a popular avenue of sharing feedback, opinions and reviews on\nsocial media, e-commerce platforms, etc. Many companies need to extract\nmeaningful information (which may include thematic content as well as semantic\npolarity) out of such short texts to understand users' behaviour. However,\nobtaining high quality sentiment-associated and human interpretable themes\nstill remains a challenge for short texts. In this paper we develop ELJST, an\nembedding enhanced generative joint sentiment-topic model that can discover\nmore coherent and diverse topics from short texts. It uses Markov Random Field\nRegularizer that can be seen as a generalisation of skip-gram based models.\nFurther, it can leverage higher-order semantic information appearing in word\nembedding, such as self-attention weights in graphical models. Our results show\nan average improvement of 10% in topic coherence and 5% in topic\ndiversification over baselines. Finally, ELJST helps understand users'\nbehaviour at more granular levels which can be explained. All these can bring\nsignificant values to the service and healthcare industries often dealing with\ncustomers.", "published": "2021-03-26 11:41:21", "link": "http://arxiv.org/abs/2103.14410v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Functorial Language Models", "abstract": "We introduce functorial language models: a principled way to compute\nprobability distributions over word sequences given a monoidal functor from\ngrammar to meaning. This yields a method for training categorical compositional\ndistributional (DisCoCat) models on raw text data. We provide a\nproof-of-concept implementation in DisCoPy, the Python toolbox for monoidal\ncategories.", "published": "2021-03-26 11:41:52", "link": "http://arxiv.org/abs/2103.14411v1", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "Incorporating Connections Beyond Knowledge Embeddings: A Plug-and-Play\n  Module to Enhance Commonsense Reasoning in Machine Reading Comprehension", "abstract": "Conventional Machine Reading Comprehension (MRC) has been well-addressed by\npattern matching, but the ability of commonsense reasoning remains a gap\nbetween humans and machines. Previous methods tackle this problem by enriching\nword representations via pre-trained Knowledge Graph Embeddings (KGE). However,\nthey make limited use of a large number of connections between nodes in\nKnowledge Graphs (KG), which could be pivotal cues to build the commonsense\nreasoning chains. In this paper, we propose a Plug-and-play module to\nIncorporatE Connection information for commonsEnse Reasoning (PIECER). Beyond\nenriching word representations with knowledge embeddings, PIECER constructs a\njoint query-passage graph to explicitly guide commonsense reasoning by the\nknowledge-oriented connections between words. Further, PIECER has high\ngeneralizability since it can be plugged into suitable positions in any MRC\nmodel. Experimental results on ReCoRD, a large-scale public MRC dataset\nrequiring commonsense reasoning, show that PIECER introduces stable performance\nimprovements for four representative base MRC models, especially in\nlow-resource settings.", "published": "2021-03-26 12:55:19", "link": "http://arxiv.org/abs/2103.14443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation in Natural Language Processing: A Novel Text\n  Generation Approach for Long and Short Text Classifiers", "abstract": "In many cases of machine learning, research suggests that the development of\ntraining data might have a higher relevance than the choice and modelling of\nclassifiers themselves. Thus, data augmentation methods have been developed to\nimprove classifiers by artificially created training data. In NLP, there is the\nchallenge of establishing universal rules for text transformations which\nprovide new linguistic patterns. In this paper, we present and evaluate a text\ngeneration method suitable to increase the performance of classifiers for long\nand short texts. We achieved promising improvements when evaluating short as\nwell as long text tasks with the enhancement by our text generation method.\nEspecially with regard to small data analytics, additive accuracy gains of up\nto 15.53% and 3.56% are achieved within a constructed low data regime, compared\nto the no augmentation baseline and another data augmentation technique. As the\ncurrent track of these constructed regimes is not universally applicable, we\nalso show major improvements in several real world low data tasks (up to +4.84\nF1-score). Since we are evaluating the method from many perspectives (in total\n11 datasets), we also observe situations where the method might not be\nsuitable. We discuss implications and patterns for the successful application\nof our approach on different types of datasets.", "published": "2021-03-26 13:16:07", "link": "http://arxiv.org/abs/2103.14453v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers", "abstract": "We investigate how sentence-level transformers can be modified into effective\nsequence labelers at the token level without any direct supervision. Existing\napproaches to zero-shot sequence labeling do not perform well when applied on\ntransformer-based architectures. As transformers contain multiple layers of\nmulti-head self-attention, information in the sentence gets distributed between\nmany tokens, negatively affecting zero-shot token-level performance. We find\nthat a soft attention module which explicitly encourages sharpness of attention\nweights can significantly outperform existing methods.", "published": "2021-03-26 13:35:43", "link": "http://arxiv.org/abs/2103.14465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Automated Multiple-Choice Question Generation Using Natural Language\n  Processing Techniques", "abstract": "Automatic multiple-choice question generation (MCQG) is a useful yet\nchallenging task in Natural Language Processing (NLP). It is the task of\nautomatic generation of correct and relevant questions from textual data.\nDespite its usefulness, manually creating sizeable, meaningful and relevant\nquestions is a time-consuming and challenging task for teachers. In this paper,\nwe present an NLP-based system for automatic MCQG for Computer-Based Testing\nExamination (CBTE).We used NLP technique to extract keywords that are important\nwords in a given lesson material. To validate that the system is not perverse,\nfive lesson materials were used to check the effectiveness and efficiency of\nthe system. The manually extracted keywords by the teacher were compared to the\nauto-generated keywords and the result shows that the system was capable of\nextracting keywords from lesson materials in setting examinable questions. This\noutcome is presented in a user-friendly interface for easy accessibility.", "published": "2021-03-26 22:39:59", "link": "http://arxiv.org/abs/2103.14757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TOUR: Dynamic Topic and Sentiment Analysis of User Reviews for Assisting\n  App Release", "abstract": "App reviews deliver user opinions and emerging issues (e.g., new bugs) about\nthe app releases. Due to the dynamic nature of app reviews, topics and\nsentiment of the reviews would change along with app release versions. Although\nseveral studies have focused on summarizing user opinions by analyzing user\nsentiment towards app features, no practical tool is released. The large\nquantity of reviews and noise words also necessitates an automated tool for\nmonitoring user reviews. In this paper, we introduce TOUR for dynamic TOpic and\nsentiment analysis of User Reviews. TOUR is able to (i) detect and summarize\nemerging app issues over app versions, (ii) identify user sentiment towards app\nfeatures, and (iii) prioritize important user reviews for facilitating\ndevelopers' examination. The core techniques of TOUR include the online topic\nmodeling approach and sentiment prediction strategy. TOUR provides entries for\ndevelopers to customize the hyper-parameters and the results are presented in\nan interactive way. We evaluate TOUR by conducting a developer survey that\ninvolves 15 developers, and all of them confirm the practical usefulness of the\nrecommended feature changes by TOUR.", "published": "2021-03-26 08:44:55", "link": "http://arxiv.org/abs/2103.15774v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Systematic Mapping Protocol: Reasoning Algorithms on Feature Model", "abstract": "Context: The importance of the feature modeling for the software product\nlines considering the modeling and management of the variability. Objective:\nDefine a protocol to conduct a systematic mapping study to summarize and\nsynthesize the evidence on reasoning algorithms for feature modeling. Method:\nApplication the protocol to conduct a systematic mapping study according the\nguidelines of K. Petersen. Results: A validated protocol to conduct a\nsystematic mapping study. Conclusions: Initial findings show that a more\ndetailed review for the different reasoning algorithms for feature modeling is\nneeded.", "published": "2021-03-26 23:34:55", "link": "http://arxiv.org/abs/2103.16325v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "BART based semantic correction for Mandarin automatic speech recognition\n  system", "abstract": "Although automatic speech recognition (ASR) systems achieved significantly\nimprovements in recent years, spoken language recognition error occurs which\ncan be easily spotted by human beings. Various language modeling techniques\nhave been developed on post recognition tasks like semantic correction. In this\npaper, we propose a Transformer based semantic correction method with\npretrained BART initialization, Experiments on 10000 hours Mandarin speech\ndataset show that character error rate (CER) can be effectively reduced by\n21.7% relatively compared to our baseline ASR system. Expert evaluation\ndemonstrates that actual improvement of our model surpasses what CER indicates.", "published": "2021-03-26 06:41:16", "link": "http://arxiv.org/abs/2104.05507v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improve GAN-based Neural Vocoder using Pointwise Relativistic\n  LeastSquare GAN", "abstract": "GAN-based neural vocoders, such as Parallel WaveGAN and MelGAN have attracted\ngreat interest due to their lightweight and parallel structures, enabling them\nto generate high fidelity waveform in a real-time manner. In this paper,\ninspired by Relativistic GAN, we introduce a novel variant of the LSGAN\nframework under the context of waveform synthesis, named Pointwise Relativistic\nLSGAN (PRLSGAN). In this approach, we take the truism score distribution into\nconsideration and combine the original MSE loss with the proposed pointwise\nrelative discrepancy loss to increase the difficulty of the generator to fool\nthe discriminator, leading to improved generation quality. Moreover, PRLSGAN is\na general-purposed framework that can be combined with any GAN-based neural\nvocoder to enhance its generation quality. Experiments have shown a consistent\nperformance boost based on Parallel WaveGAN and MelGAN, demonstrating the\neffectiveness and strong generalization ability of our proposed PRLSGAN neural\nvocoders.", "published": "2021-03-26 03:35:22", "link": "http://arxiv.org/abs/2103.14245v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mutually-Constrained Monotonic Multihead Attention for Online ASR", "abstract": "Despite the feature of real-time decoding, Monotonic Multihead Attention\n(MMA) shows comparable performance to the state-of-the-art offline methods in\nmachine translation and automatic speech recognition (ASR) tasks. However, the\nlatency of MMA is still a major issue in ASR and should be combined with a\ntechnique that can reduce the test latency at inference time, such as\nhead-synchronous beam search decoding, which forces all non-activated heads to\nactivate after a small fixed delay from the first head activation. In this\npaper, we remove the discrepancy between training and test phases by\nconsidering, in the training of MMA, the interactions across multiple heads\nthat will occur in the test time. Specifically, we derive the expected\nalignments from monotonic attention by considering the boundaries of other\nheads and reflect them in the learning process. We validate our proposed method\non the two standard benchmark datasets for ASR and show that our approach, MMA\nwith the mutually-constrained heads from the training stage, provides better\nperformance than baselines.", "published": "2021-03-26 07:33:25", "link": "http://arxiv.org/abs/2103.14302v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Theory of Stochastic Automata", "abstract": "The theory of discrete stochastic systems has been initiated by the work of\nShannon and von Neumann. While Shannon has considered memory-less communication\nchannels and their generalization by introducing states, von Neumann has\nstudied the synthesis of reliable systems from unreliable components. The\nfundamental work of Rabin and Scott about deterministic finite-state automata\nhas led to two generalizations. First, the generalization of transition\nfunctions to conditional distributions studied by Carlyle and Starke. This in\nturn has led to a generalization of time-discrete Markov chains in which the\nchains are governed by more than one transition probability matrix. Second, the\ngeneralization of regular sets by introducing stochastic automata as described\nby Rabin. Stochastic automata are well-investigated. This report provides a\nshort introduction to stochastic automata based on the valuable book of Claus.\nThis includes the basic topics of the theory of stochastic automata:\nequivalence, minimization, reduction, covering, observability, and determinism.\nThen stochastic versions of Mealy and Moore automata are studied and finally\nstochastic language acceptors are considered as a generalization of\nnondeterministic finite-state acceptors.", "published": "2021-03-26 12:05:42", "link": "http://arxiv.org/abs/2103.14423v1", "categories": ["cs.FL", "cs.CL", "math.PR", "68Q70, 68Q87, 20M35"], "primary_category": "cs.FL"}
{"title": "Continual Speaker Adaptation for Text-to-Speech Synthesis", "abstract": "Training a multi-speaker Text-to-Speech (TTS) model from scratch is\ncomputationally expensive and adding new speakers to the dataset requires the\nmodel to be re-trained. The naive solution of sequential fine-tuning of a model\nfor new speakers can lead to poor performance of older speakers. This\nphenomenon is known as catastrophic forgetting. In this paper, we look at TTS\nmodeling from a continual learning perspective, where the goal is to add new\nspeakers without forgetting previous speakers. Therefore, we first propose an\nexperimental setup and show that serial fine-tuning for new speakers can cause\nthe forgetting of the earlier speakers. Then we exploit two well-known\ntechniques for continual learning, namely experience replay and weight\nregularization. We reveal how one can mitigate the effect of degradation in\nspeech synthesis diversity in sequential training of new speakers using these\nmethods. Finally, we present a simple extension to experience replay to improve\nthe results in extreme setups where we have access to very small buffers.", "published": "2021-03-26 15:14:20", "link": "http://arxiv.org/abs/2103.14512v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging pre-trained representations to improve access to\n  untranscribed speech from endangered languages", "abstract": "Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.", "published": "2021-03-26 16:44:08", "link": "http://arxiv.org/abs/2103.14583v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dodrio: Exploring Transformer Models with Interactive Visualization", "abstract": "Why do large pre-trained transformer-based models perform so well across a\nwide variety of NLP tasks? Recent research suggests the key may lie in\nmulti-headed attention mechanism's ability to learn and represent linguistic\ninformation. Understanding how these models represent both syntactic and\nsemantic knowledge is vital to investigate why they succeed and fail, what they\nhave learned, and how they can improve. We present Dodrio, an open-source\ninteractive visualization tool to help NLP researchers and practitioners\nanalyze attention mechanisms in transformer-based models with linguistic\nknowledge. Dodrio tightly integrates an overview that summarizes the roles of\ndifferent attention heads, and detailed views that help users compare attention\nweights with the syntactic structure and semantic information in the input\ntext. To facilitate the visual comparison of attention weights and linguistic\nknowledge, Dodrio applies different graph visualization techniques to represent\nattention weights scalable to longer input text. Case studies highlight how\nDodrio provides insights into understanding the attention mechanism in\ntransformer-based models. Dodrio is available at\nhttps://poloclub.github.io/dodrio/.", "published": "2021-03-26 17:39:37", "link": "http://arxiv.org/abs/2103.14625v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Construction of a Large-scale Japanese ASR Corpus on TV Recordings", "abstract": "This paper presents a new large-scale Japanese speech corpus for training\nautomatic speech recognition (ASR) systems. This corpus contains over 2,000\nhours of speech with transcripts built on Japanese TV recordings and their\nsubtitles. We develop herein an iterative workflow to extract matching audio\nand subtitle segments from TV recordings based on a conventional method for\nlightly-supervised audio-to-text alignment. We evaluate a model trained with\nour corpus using an evaluation dataset built on Japanese TEDx presentation\nvideos and confirm that the performance is better than that trained with the\nCorpus of Spontaneous Japanese (CSJ). The experiment results show the\nusefulness of our corpus for training ASR systems. This corpus is made public\nfor the research community along with Kaldi scripts for training the models\nreported in this paper.", "published": "2021-03-26 21:14:12", "link": "http://arxiv.org/abs/2103.14736v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Three dimensional higher-order raypath separation in a shallow-water\n  waveguide", "abstract": "Separating raypaths in a multipath shallow-water environment is a challenge\nproblem due to the interferences between them and colored noise existing in\nocean environment, especially for two raypaths arrive close to each other.\nThus, in this paper, a three dimensional (3D) higher-order raypath separation\nin an array to array configuration is proposed. Performance tests using\nsimulation data in a multipath environment, real data obtained in an ultrasonic\nwaveguide and ocean shallow-water data, respectively, illustrate that the\nproposed algorithm achieves a higher resolution and a stronger robustness\ncomparing to the existing algorithms.", "published": "2021-03-26 01:49:08", "link": "http://arxiv.org/abs/2103.14206v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Subspace-based compressive sensing algorithm for raypath separation in a\n  shallow-water waveguide", "abstract": "Compressive sensing (CS) has been applied to estimate the direction of\narrival (DOA) in underwater acoustics. However, the key problem needed to be\nresolved in a {multipath} propagation environment is to suppress the\ninterferences between the raypaths. Thus, in this paper, {a subspace-based\ncompressive sensing algorithm that formulates the statistic information of the\nsignal subspace in a CS framework is proposed.} The experiment results show\nthat (1) the proposed algorithm enables the separation of raypaths that arrive\nclosely at the {receiver} array and (2) the existing algorithms fail,\nespecially in a low signal-to-noise ratio (SNR) environment.", "published": "2021-03-26 02:55:28", "link": "http://arxiv.org/abs/2103.14236v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CNN-based Discriminative Training for Domain Compensation in Acoustic\n  Event Detection with Frame-wise Classifier", "abstract": "Domain mismatch is a noteworthy issue in acoustic event detection tasks, as\nthe target domain data is difficult to access in most real applications. In\nthis study, we propose a novel CNN-based discriminative training framework as a\ndomain compensation method to handle this issue. It uses a parallel CNN-based\ndiscriminator to learn a pair of high-level intermediate acoustic\nrepresentations. Together with a binary discriminative loss, the discriminators\nare forced to maximally exploit the discrimination of heterogeneous acoustic\ninformation in each audio clip with target events, which results in a robust\npaired representations that can well discriminate the target events and\nbackground/domain variations separately. Moreover, to better learn the\ntransient characteristics of target events, a frame-wise classifier is designed\nto perform the final classification. In addition, a two-stage training with the\nCNN-based discriminator initialization is further proposed to enhance the\nsystem training. All experiments are performed on the DCASE 2018 Task3\ndatasets. Results show that our proposal significantly outperforms the official\nbaseline on cross-domain conditions in AUC by relative $1.8-12.1$% without any\nperformance degradation on in-domain evaluation conditions.", "published": "2021-03-26 07:17:22", "link": "http://arxiv.org/abs/2103.14297v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with\n  Differentiable Duration Modeling", "abstract": "This paper introduces Parallel Tacotron 2, a non-autoregressive neural\ntext-to-speech model with a fully differentiable duration model which does not\nrequire supervised duration signals. The duration model is based on a novel\nattention mechanism and an iterative reconstruction loss based on Soft Dynamic\nTime Warping, this model can learn token-frame alignments as well as token\ndurations automatically. Experimental results show that Parallel Tacotron 2\noutperforms baselines in subjective naturalness in several diverse multi\nspeaker evaluations. Its duration control capability is also demonstrated.", "published": "2021-03-26 16:40:28", "link": "http://arxiv.org/abs/2103.14574v7", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis", "abstract": "Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This work generates an IR from a single image, which can then be\napplied to other signals using convolution, simulating the reverberant\ncharacteristics of the space shown in the image. Recording these IRs is both\ntime-intensive and expensive, and often infeasible for inaccessible locations.\nWe use an end-to-end neural network architecture to generate plausible audio\nimpulse responses from single images of acoustic environments. We evaluate our\nmethod both by comparisons to ground truth data and by human expert evaluation.\nWe demonstrate our approach by generating plausible impulse responses from\ndiverse settings and formats including well known places, musical halls, rooms\nin paintings, images from animations and computer games, synthetic environments\ngenerated from text, panoramic images, and video conference backgrounds.", "published": "2021-03-26 01:25:58", "link": "http://arxiv.org/abs/2103.14201v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modeling the Compatibility of Stem Tracks to Generate Music Mashups", "abstract": "A music mashup combines audio elements from two or more songs to create a new\nwork. To reduce the time and effort required to make them, researchers have\ndeveloped algorithms that predict the compatibility of audio elements. Prior\nwork has focused on mixing unaltered excerpts, but advances in source\nseparation enable the creation of mashups from isolated stems (e.g., vocals,\ndrums, bass, etc.). In this work, we take advantage of separated stems not just\nfor creating mashups, but for training a model that predicts the mutual\ncompatibility of groups of excerpts, using self-supervised and semi-supervised\nmethods. Specifically, we first produce a random mashup creation pipeline that\ncombines stem tracks obtained via source separation, with key and tempo\nautomatically adjusted to match, since these are prerequisites for high-quality\nmashups. To train a model to predict compatibility, we use stem tracks obtained\nfrom the same song as positive examples, and random combinations of stems with\nkey and/or tempo unadjusted as negative examples. To improve the model and use\nmore data, we also train on \"average\" examples: random combinations with\nmatching key and tempo, where we treat them as unlabeled data as their true\ncompatibility is unknown. To determine whether the combined signal or the set\nof stem signals is more indicative of the quality of the result, we experiment\non two model architectures and train them using semi-supervised learning\ntechnique. Finally, we conduct objective and subjective evaluations of the\nsystem, comparing them to a standard rule-based system.", "published": "2021-03-26 01:51:11", "link": "http://arxiv.org/abs/2103.14208v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Supervised Chorus Detection for Popular Music Using Convolutional Neural\n  Network and Multi-task Learning", "abstract": "This paper presents a novel supervised approach to detecting the chorus\nsegments in popular music. Traditional approaches to this task are mostly\nunsupervised, with pipelines designed to target some quality that is assumed to\ndefine \"chorusness,\" which usually means seeking the loudest or most frequently\nrepeated sections. We propose to use a convolutional neural network with a\nmulti-task learning objective, which simultaneously fits two temporal\nactivation curves: one indicating \"chorusness\" as a function of time, and the\nother the location of the boundaries. We also propose a post-processing method\nthat jointly takes into account the chorus and boundary predictions to produce\nbinary output. In experiments using three datasets, we compare our system to a\nset of public implementations of other segmentation and chorus-detection\nalgorithms, and find our approach performs significantly better.", "published": "2021-03-26 04:32:08", "link": "http://arxiv.org/abs/2103.14253v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Guided Training: A Simple Method for Single-channel Speaker Separation", "abstract": "Deep learning has shown a great potential for speech separation, especially\nfor speech and non-speech separation. However, it encounters permutation\nproblem for multi-speaker separation where both target and interference are\nspeech. Permutation Invariant training (PIT) was proposed to solve this problem\nby permuting the order of the multiple speakers. Another way is to use an\nanchor speech, a short speech of the target speaker, to model the speaker\nidentity. In this paper, we propose a simple strategy to train a long\nshort-term memory (LSTM) model to solve the permutation problem in speaker\nseparation. Specifically, we insert a short speech of target speaker at the\nbeginning of a mixture as guide information. So, the first appearing speaker is\ndefined as the target. Due to the powerful capability on sequence modeling,\nLSTM can use its memory cells to track and separate target speech from\ninterfering speech. Experimental results show that the proposed training\nstrategy is effective for speaker separation.", "published": "2021-03-26 08:46:50", "link": "http://arxiv.org/abs/2103.14330v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization", "abstract": "Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.", "published": "2021-03-26 17:09:06", "link": "http://arxiv.org/abs/2103.14602v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cyclic Defense GAN Against Speech Adversarial Attacks", "abstract": "This paper proposes a new defense approach for counteracting state-of-the-art\nwhite and black-box adversarial attack algorithms. Our approach fits into the\nimplicit reactive defense algorithm category since it does not directly\nmanipulate the potentially malicious input signals. Instead, it reconstructs a\nsimilar signal with a synthesized spectrogram using a cyclic generative\nadversarial network. This cyclic framework helps to yield a stable generative\nmodel. Finally, we feed the reconstructed signal into the speech-to-text model\nfor transcription. The conducted experiments on targeted and non-targeted\nadversarial attacks developed for attacking DeepSpeech, Kaldi, and Lingvo\nmodels demonstrate the proposed defense's effectiveness in adverse scenarios.", "published": "2021-03-26 20:09:46", "link": "http://arxiv.org/abs/2103.14717v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
