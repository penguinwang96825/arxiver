{"title": "Polarity and Intensity: the Two Aspects of Sentiment Analysis", "abstract": "Current multimodal sentiment analysis frames sentiment score prediction as a\ngeneral Machine Learning task. However, what the sentiment score actually\nrepresents has often been overlooked. As a measurement of opinions and\naffective states, a sentiment score generally consists of two aspects: polarity\nand intensity. We decompose sentiment scores into these two aspects and study\nhow they are conveyed through individual modalities and combined multimodal\nmodels in a naturalistic monologue setting. In particular, we build unimodal\nand multimodal multi-task learning models with sentiment score prediction as\nthe main task and polarity and/or intensity classification as the auxiliary\ntasks. Our experiments show that sentiment analysis benefits from multi-task\nlearning, and individual modalities differ when conveying the polarity and\nintensity aspects of sentiment.", "published": "2018-07-04 07:18:36", "link": "http://arxiv.org/abs/1807.01466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automation of Sense-type Identification of Verbs in\n  OntoSenseNet(Telugu)", "abstract": "In this paper, we discuss the enrichment of a manually developed resource of\nTelugu lexicon, OntoSenseNet. OntoSenseNet is a ontological sense annotated\nlexicon that marks each verb of Telugu with a primary and a secondary sense.\nThe area of research is relatively recent but has a large scope of development.\nWe provide an introductory work to enrich the OntoSenseNet to promote further\nresearch in Telugu. Classifiers are adopted to learn the sense relevant\nfeatures of the words in the resource and also to automate the tagging of\nsense-types for verbs. We perform a comparative analysis of different\nclassifiers applied on OntoSenseNet. The results of the experiment prove that\nautomated enrichment of the resource is effective using SVM classifiers and\nAdaboost ensemble.", "published": "2018-07-04 16:54:05", "link": "http://arxiv.org/abs/1807.01677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BCSAT : A Benchmark Corpus for Sentiment Analysis in Telugu Using\n  Word-level Annotations", "abstract": "The presented work aims at generating a systematically annotated corpus that\ncan support the enhancement of sentiment analysis tasks in Telugu using\nword-level sentiment annotations. From OntoSenseNet, we extracted 11,000\nadjectives, 253 adverbs, 8483 verbs and sentiment annotation is being done by\nlanguage experts. We discuss the methodology followed for the polarity\nannotations and validate the developed resource. This work aims at developing a\nbenchmark corpus, as an extension to SentiWordNet, and baseline accuracy for a\nmodel where lexeme annotations are applied for sentiment predictions. The\nfundamental aim of this paper is to validate and study the possibility of\nutilizing machine learning algorithms, word-level sentiment annotations in the\ntask of automated sentiment identification. Furthermore, accuracy is improved\nby annotating the bi-grams extracted from the target corpus.", "published": "2018-07-04 16:56:50", "link": "http://arxiv.org/abs/1807.01679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Mandarin and Cantonese F0 Contours with Decision Trees and\n  BLSTMs", "abstract": "This paper models the fundamental frequency contours on both Mandarin and\nCantonese speech with decision trees and DNNs (deep neural networks). Different\nkinds of f0 representations and model architectures are tested for decision\ntrees and DNNs. A new model called Additive-BLSTM (additive bidirectional long\nshort term memory) that predicts a base f0 contour and a residual f0 contour\nwith two BLSTMs is proposed. With respect to objective measures of RMSE and\ncorrelation, applying tone-dependent trees together with sample normalization\nand delta feature regularization within decision tree framework performs best.\nWhile the new Additive-BLSTM model with delta feature regularization performs\neven better. Subjective listening tests on both Mandarin and Cantonese\ncomparing Random Forest model (multiple decision trees) and the Additive-BLSTM\nmodel were also held and confirmed the advantage of the new model according to\nthe listeners' preference.", "published": "2018-07-04 17:04:14", "link": "http://arxiv.org/abs/1807.01682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Convolutional Neural Network for Aspect Sentiment Classification", "abstract": "With the development of the Internet, natural language processing (NLP), in\nwhich sentiment analysis is an important task, became vital in information\nprocessing.Sentiment analysis includes aspect sentiment classification. Aspect\nsentiment can provide complete and in-depth results with increased attention on\naspect-level. Different context words in a sentence influence the sentiment\npolarity of a sentence variably, and polarity varies based on the different\naspects in a sentence. Take the sentence, 'I bought a new camera. The picture\nquality is amazing but the battery life is too short.'as an example. If the\naspect is picture quality, then the expected sentiment polarity is 'positive',\nif the battery life aspect is considered, then the sentiment polarity should be\n'negative'; therefore, aspect is important to consider when we explore aspect\nsentiment in the sentence. Recurrent neural network (RNN) is regarded as a good\nmodel to deal with natural language processing, and RNNs has get good\nperformance on aspect sentiment classification including Target-Dependent LSTM\n(TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM,\nAEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment\nclassification utilizing convolutional neural network, but there is little\nliterature on aspect sentiment classification using convolutional neural\nnetwork. In our paper, we develop attention-based input layers in which aspect\ninformation is considered by input layer. We then incorporate attention-based\ninput layers into convolutional neural network (CNN) to introduce context words\ninformation. In our experiment, incorporating aspect information into CNN\nimproves the latter's aspect sentiment classification performance without using\nsyntactic parser or external sentiment lexicons in a benchmark dataset from\nTwitter but get better performance compared with other models.", "published": "2018-07-04 09:07:34", "link": "http://arxiv.org/abs/1807.01704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Transition-based Non-projective Dependency Parsing", "abstract": "Shi, Huang, and Lee (2017) obtained state-of-the-art results for English and\nChinese dependency parsing by combining dynamic-programming implementations of\ntransition-based dependency parsers with a minimal set of bidirectional LSTM\nfeatures. However, their results were limited to projective parsing. In this\npaper, we extend their approach to support non-projectivity by providing the\nfirst practical implementation of the MH_4 algorithm, an $O(n^4)$ mildly\nnonprojective dynamic-programming parser with very high coverage on\nnon-projective treebanks. To make MH_4 compatible with minimal transition-based\nfeature sets, we introduce a transition-based interpretation of it in which\nparser items are mapped to sequences of transitions. We thus obtain the first\nimplementation of global decoding for non-projective transition-based parsing,\nand demonstrate empirically that it is more effective than its projective\ncounterpart in parsing a number of highly non-projective languages", "published": "2018-07-04 19:09:40", "link": "http://arxiv.org/abs/1807.01745v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Formal Ontology-Based Classification of Lexemes and its Applications", "abstract": "The paper describes the enrichment of OntoSenseNet - a verb-centric lexical\nresource for Indian Languages. A major contribution of this work is\npreservation of an authentic Telugu dictionary by developing a computational\nversion of the same. It is important because native speakers can better\nannotate the sense-types when both the word and its meaning are in Telugu.\nHence efforts are made to develop the aforementioned Telugu dictionary and\nannotations are done manually. The manually annotated gold standard corpus\nconsists 8483 verbs, 253 adverbs and 1673 adjectives. Annotations are done by\nnative speakers according to defined annotation guidelines. In this paper, we\nprovide an overview of the annotation procedure and present the validation of\nthe developed resource through inter-annotator agreement. Additional words from\nTelugu WordNet are added to our resource and are crowd-sourced for annotation.\nThe statistics are compared with the sense-annotated lexicon, our resource for\nmore insights.", "published": "2018-07-04 17:05:09", "link": "http://arxiv.org/abs/1807.01996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction", "abstract": "In this paper, we introduce the \\textbf{C}hinese \\textbf{AI} and \\textbf{L}aw\nchallenge dataset (CAIL2018), the first large-scale Chinese legal dataset for\njudgment prediction. \\dataset contains more than $2.6$ million criminal cases\npublished by the Supreme People's Court of China, which are several times\nlarger than other datasets in existing works on judgment prediction. Moreover,\nthe annotations of judgment results are more detailed and rich. It consists of\napplicable law articles, charges, and prison terms, which are expected to be\ninferred according to the fact descriptions of cases. For comparison, we\nimplement several conventional text classification baselines for judgment\nprediction and experimental results show that it is still a challenge for\ncurrent models to predict the judgment results of legal cases, especially on\nprison terms. To help the researchers make improvements on legal judgment\nprediction, both \\dataset and baselines will be released after the CAIL\ncompetition\\footnote{http://cail.cipsc.org.cn/}.", "published": "2018-07-04 02:09:06", "link": "http://arxiv.org/abs/1807.02478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Data Augmentation for Dialogue Language\n  Understanding", "abstract": "In this paper, we study the problem of data augmentation for language\nunderstanding in task-oriented dialogue system. In contrast to previous work\nwhich augments an utterance without considering its relation with other\nutterances, we propose a sequence-to-sequence generation based data\naugmentation framework that leverages one utterance's same semantic\nalternatives in the training data. A novel diversity rank is incorporated into\nthe utterance representation to make the model produce diverse utterances and\nthese diversely augmented utterances help to improve the language understanding\nmodule. Experimental results on the Airline Travel Information System dataset\nand a newly created semantic frame annotation on Stanford Multi-turn,\nMultidomain Dialogue Dataset show that our framework achieves significant\nimprovements of 6.38 and 10.04 F-scores respectively when only a training set\nof hundreds utterances is represented. Case studies also confirm that our\nmethod generates diverse utterances.", "published": "2018-07-04 13:07:53", "link": "http://arxiv.org/abs/1807.01554v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seq2RDF: An end-to-end application for deriving Triples from Natural\n  Language Text", "abstract": "We present an end-to-end approach that takes unstructured textual input and\ngenerates structured output compliant with a given vocabulary. Inspired by\nrecent successes in neural machine translation, we treat the triples within a\ngiven knowledge graph as an independent graph language and propose an\nencoder-decoder framework with an attention mechanism that leverages knowledge\ngraph embeddings. Our model learns the mapping from natural language text to\ntriple representation in the form of subject-predicate-object using the\nselected knowledge graph vocabulary. Experiments on three different data sets\nshow that we achieve competitive F1-Measures over the baselines using our\nsimple yet effective approach. A demo video is included.", "published": "2018-07-04 20:13:31", "link": "http://arxiv.org/abs/1807.01763v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Encoding Spatial Relations from Natural Language", "abstract": "Natural language processing has made significant inroads into learning the\nsemantics of words through distributional approaches, however representations\nlearnt via these methods fail to capture certain kinds of information implicit\nin the real world. In particular, spatial relations are encoded in a way that\nis inconsistent with human spatial reasoning and lacking invariance to\nviewpoint changes. We present a system capable of capturing the semantics of\nspatial relations such as behind, left of, etc from natural language. Our key\ncontributions are a novel multi-modal objective based on generating images of\nscenes from their textual descriptions, and a new dataset on which to train it.\nWe demonstrate that internal representations are robust to meaning preserving\ntransformations of descriptions (paraphrase invariance), while viewpoint\ninvariance is an emergent property of the system.", "published": "2018-07-04 16:38:49", "link": "http://arxiv.org/abs/1807.01670v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the role of L1 in automatic pronunciation evaluation of L2\n  speech", "abstract": "Automatic pronunciation evaluation plays an important role in pronunciation\ntraining and second language education. This field draws heavily on concepts\nfrom automatic speech recognition (ASR) to quantify how close the pronunciation\nof non-native speech is to native-like pronunciation. However, it is known that\nthe formation of accent is related to pronunciation patterns of both the target\nlanguage (L2) and the speaker's first language (L1). In this paper, we propose\nto use two native speech acoustic models, one trained on L2 speech and the\nother trained on L1 speech. We develop two sets of measurements that can be\nextracted from two acoustic models given accented speech. A new utterance-level\nfeature extraction scheme is used to convert these measurements into a\nfixed-dimension vector which is used as an input to a statistical model to\npredict the accentedness of a speaker. On a data set consisting of speakers\nfrom 4 different L1 backgrounds, we show that the proposed system yields\nimproved correlation with human evaluators compared to systems only using the\nL2 acoustic model.", "published": "2018-07-04 18:44:43", "link": "http://arxiv.org/abs/1807.01738v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
