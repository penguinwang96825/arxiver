{"title": "Towards the Rapid Development of a Natural Language Understanding Module", "abstract": "When developing a conversational agent, there is often an urgent need to have\na prototype available in order to test the application with real users. A\nWizard of Oz is a possibility, but sometimes the agent should be simply\ndeployed in the environment where it will be used. Here, the agent should be\nable to capture as many interactions as possible and to understand how people\nreact to failure. In this paper, we focus on the rapid development of a natural\nlanguage understanding module by non experts. Our approach follows the learning\nparadigm and sees the process of understanding natural language as a\nclassification problem. We test our module with a conversational agent that\nanswers questions in the art domain. Moreover, we show how our approach can be\nused by a natural language interface to a cinema database.", "published": "2013-02-06 14:17:55", "link": "http://arxiv.org/abs/1302.1380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S\u00e9mantique des d\u00e9terminants dans un cadre richement typ\u00e9", "abstract": "The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.", "published": "2013-02-06 16:26:49", "link": "http://arxiv.org/abs/1302.1422v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Access for Speech Understanding using Minimum Message Length\n  Encoding", "abstract": "The Lexical Access Problem consists of determining the intended sequence of\nwords corresponding to an input sequence of phonemes (basic speech sounds) that\ncome from a low-level phoneme recognizer. In this paper we present an\ninformation-theoretic approach based on the Minimum Message Length Criterion\nfor solving the Lexical Access Problem. We model sentences using phoneme\nrealizations seen in training, and word and part-of-speech information obtained\nfrom text corpora. We show results on multiple-speaker, continuous, read speech\nand discuss a heuristic using equivalence classes of similar sounding words\nwhich speeds up the recognition process without significant deterioration in\nrecognition accuracy.", "published": "2013-02-06 15:59:24", "link": "http://arxiv.org/abs/1302.1572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic text summarization based on latent semantic analysis to enhance\n  arabic documents clustering", "abstract": "Arabic Documents Clustering is an important task for obtaining good results\nwith the traditional Information Retrieval (IR) systems especially with the\nrapid growth of the number of online documents present in Arabic language.\nDocuments clustering aim to automatically group similar documents in one\ncluster using different similarity/distance measures. This task is often\naffected by the documents length, useful information on the documents is often\naccompanied by a large amount of noise, and therefore it is necessary to\neliminate this noise while keeping useful information to boost the performance\nof Documents clustering. In this paper, we propose to evaluate the impact of\ntext summarization using the Latent Semantic Analysis Model on Arabic Documents\nClustering in order to solve problems cited above, using five\nsimilarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard\nCoefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler\nDivergence, for two times: without and with stemming. Our experimental results\nindicate that our proposed approach effectively solves the problems of noisy\ninformation and documents length, and thus significantly improve the clustering\nperformance.", "published": "2013-02-06 23:24:37", "link": "http://arxiv.org/abs/1302.1612v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
