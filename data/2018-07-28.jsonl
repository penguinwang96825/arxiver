{"title": "Back-Translation-Style Data Augmentation for End-to-End ASR", "abstract": "In this paper we propose a novel data augmentation method for attention-based\nend-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of\ntext which is not paired with speech signals. Inspired by the back-translation\ntechnique proposed in the field of machine translation, we build a neural\ntext-to-encoder model which predicts a sequence of hidden states extracted by a\npre-trained E2E-ASR encoder from a sequence of characters. By using hidden\nstates as a target instead of acoustic features, it is possible to achieve\nfaster attention learning and reduce computational cost, thanks to sub-sampling\nin E2E-ASR encoder, also the use of the hidden states can avoid to model\nspeaker dependencies unlike acoustic features. After training, the\ntext-to-encoder model generates the hidden states from a large amount of\nunpaired text, then E2E-ASR decoder is retrained using the generated hidden\nstates as additional training data. Experimental evaluation using LibriSpeech\ndataset demonstrates that our proposed method achieves improvement of ASR\nperformance and reduces the number of unknown words without the need for paired\ndata.", "published": "2018-07-28 05:32:11", "link": "http://arxiv.org/abs/1807.10893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acoustic and Textual Data Augmentation for Improved ASR of\n  Code-Switching Speech", "abstract": "In this paper, we describe several techniques for improving the acoustic and\nlanguage model of an automatic speech recognition (ASR) system operating on\ncode-switching (CS) speech. We focus on the recognition of Frisian-Dutch radio\nbroadcasts where one of the mixed languages, namely Frisian, is an\nunder-resourced language. In previous work, we have proposed several automatic\ntranscription strategies for CS speech to increase the amount of available\ntraining speech data. In this work, we explore how the acoustic modeling (AM)\ncan benefit from monolingual speech data belonging to the high-resourced mixed\nlanguage. For this purpose, we train state-of-the-art AMs, which were\nineffective due to lack of training data, on a significantly increased amount\nof CS speech and monolingual Dutch speech. Moreover, we improve the language\nmodel (LM) by creating code-switching text, which is in practice almost\nnon-existent, by (1) generating text using recurrent LMs trained on the\ntranscriptions of the training CS speech data, (2) adding the transcriptions of\nthe automatically transcribed CS speech data and (3) translating Dutch text\nextracted from the transcriptions of a large Dutch speech corpora. We report\nsignificantly improved CS ASR performance due to the increase in the acoustic\nand textual training data.", "published": "2018-07-28 14:59:52", "link": "http://arxiv.org/abs/1807.10945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Articulatory Features for ASR of Pathological Speech", "abstract": "In this work, we investigate the joint use of articulatory and acoustic\nfeatures for automatic speech recognition (ASR) of pathological speech. Despite\nlong-lasting efforts to build speaker- and text-independent ASR systems for\npeople with dysarthria, the performance of state-of-the-art systems is still\nconsiderably lower on this type of speech than on normal speech. The most\nprominent reason for the inferior performance is the high variability in\npathological speech that is characterized by the spectrotemporal deviations\ncaused by articulatory impairments due to various etiologies. To cope with this\nhigh variation, we propose to use speech representations which utilize\narticulatory information together with the acoustic properties. A designated\nacoustic model, namely a fused-feature-map convolutional neural network (fCNN),\nwhich performs frequency convolution on acoustic features and time convolution\non articulatory features is trained and tested on a Dutch and a Flemish\npathological speech corpus. The ASR performance of fCNN-based ASR system using\njoint features is compared to other neural network architectures such\nconventional CNNs and time-frequency convolutional networks (TFCNNs) in several\ntraining scenarios.", "published": "2018-07-28 15:04:53", "link": "http://arxiv.org/abs/1807.10948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Unified Code-Switching ASR System for South African Languages", "abstract": "We present our first efforts towards building a single multilingual automatic\nspeech recognition (ASR) system that can process code-switching (CS) speech in\nfive languages spoken within the same population. This contrasts with related\nprior work which focuses on the recognition of CS speech in bilingual\nscenarios. Recently, we have compiled a small five-language corpus of South\nAfrican soap opera speech which contains examples of CS between 5 languages\noccurring in various contexts such as using English as the matrix language and\nswitching to other indigenous languages. The ASR system presented in this work\nis trained on 4 corpora containing English-isiZulu, English-isiXhosa,\nEnglish-Setswana and English-Sesotho CS speech. The interpolation of multiple\nlanguage models trained on these language pairs enables the ASR system to\nhypothesize mixed word sequences from these 5 languages. We evaluate various\nstate-of-the-art acoustic models trained on this 5-lingual training data and\nreport ASR accuracy and language recognition performance on the development and\ntest sets of the South African multilingual soap opera corpus.", "published": "2018-07-28 15:09:55", "link": "http://arxiv.org/abs/1807.10949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Switching Detection with Data-Augmented Acoustic and Language\n  Models", "abstract": "In this paper, we investigate the code-switching detection performance of a\ncode-switching (CS) automatic speech recognition (ASR) system with\ndata-augmented acoustic and language models. We focus on the recognition of\nFrisian-Dutch radio broadcasts where one of the mixed languages, namely\nFrisian, is under-resourced. Recently, we have explored how the acoustic\nmodeling (AM) can benefit from monolingual speech data belonging to the\nhigh-resourced mixed language. For this purpose, we have trained\nstate-of-the-art AMs on a significantly increased amount of CS speech by\napplying automatic transcription and monolingual Dutch speech. Moreover, we\nhave improved the language model (LM) by creating CS text in various ways\nincluding text generation using recurrent LMs trained on existing CS text.\nMotivated by the significantly improved CS ASR performance, we delve into the\nCS detection performance of the same ASR system in this work by reporting CS\ndetection accuracies together with a detailed detection error analysis.", "published": "2018-07-28 15:16:33", "link": "http://arxiv.org/abs/1808.00521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology-Grounded Topic Modeling for Climate Science Research", "abstract": "In scientific disciplines where research findings have a strong impact on\nsociety, reducing the amount of time it takes to understand, synthesize and\nexploit the research is invaluable. Topic modeling is an effective technique\nfor summarizing a collection of documents to find the main themes among them\nand to classify other documents that have a similar mixture of co-occurring\nwords. We show how grounding a topic model with an ontology, extracted from a\nglossary of important domain phrases, improves the topics generated and makes\nthem easier to understand. We apply and evaluate this method to the climate\nscience domain. The result improves the topics generated and supports faster\nresearch understanding, discovery of social networks among researchers, and\nautomatic ontology generation.", "published": "2018-07-28 18:26:28", "link": "http://arxiv.org/abs/1807.10965v2", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Domain Robust Feature Extraction for Rapid Low Resource ASR Development", "abstract": "Developing a practical speech recognizer for a low resource language is\nchallenging, not only because of the (potentially unknown) properties of the\nlanguage, but also because test data may not be from the same domain as the\navailable training data. In this paper, we focus on the latter challenge, i.e.\ndomain mismatch, for systems trained using a sequence-based criterion. We\ndemonstrate the effectiveness of using a pre-trained English recognizer, which\nis robust to such mismatched conditions, as a domain normalizing feature\nextractor on a low resource language. In our example, we use Turkish\nConversational Speech and Broadcast News data. This enables rapid development\nof speech recognizers for new languages which can easily adapt to any domain.\nTesting in various cross-domain scenarios, we achieve relative improvements of\naround 25% in phoneme error rate, with improvements being around 50% for some\ndomains.", "published": "2018-07-28 23:54:59", "link": "http://arxiv.org/abs/1807.10984v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Analysing Shortcomings of Statistical Parametric Speech Synthesis", "abstract": "Output from statistical parametric speech synthesis (SPSS) remains noticeably\nworse than natural speech recordings in terms of quality, naturalness, speaker\nsimilarity, and intelligibility in noise. There are many hypotheses regarding\nthe origins of these shortcomings, but these hypotheses are often kept vague\nand presented without empirical evidence that could confirm and quantify how a\nspecific shortcoming contributes to imperfections in the synthesised speech.\nThroughout speech synthesis literature, surprisingly little work is dedicated\ntowards identifying the perceptually most important problems in speech\nsynthesis, even though such knowledge would be of great value for creating\nbetter SPSS systems.\n  In this book chapter, we analyse some of the shortcomings of SPSS. In\nparticular, we discuss issues with vocoding and present a general methodology\nfor quantifying the effect of any of the many assumptions and design choices\nthat hold SPSS back. The methodology is accompanied by an example that\ncarefully measures and compares the severity of perceptual limitations imposed\nby vocoding as well as other factors such as the statistical model and its use.", "published": "2018-07-28 14:40:59", "link": "http://arxiv.org/abs/1807.10941v1", "categories": ["eess.AS", "cs.SD", "I.2.7; H.5.5"], "primary_category": "eess.AS"}
