{"title": "When Does Unsupervised Machine Translation Work?", "abstract": "Despite the reported success of unsupervised machine translation (MT), the\nfield has yet to examine the conditions under which these methods succeed, and\nwhere they fail. We conduct an extensive empirical evaluation of unsupervised\nMT using dissimilar language pairs, dissimilar domains, diverse datasets, and\nauthentic low-resource languages. We find that performance rapidly deteriorates\nwhen source and target corpora are from different domains, and that random word\nembedding initialization can dramatically affect downstream translation\nperformance. We additionally find that unsupervised MT performance declines\nwhen source and target languages use different scripts, and observe very poor\nperformance on authentic low-resource language pairs. We advocate for extensive\nempirical evaluation of unsupervised MT systems to highlight failure points and\nencourage continued research on the most promising paradigms.", "published": "2020-04-12 00:57:47", "link": "http://arxiv.org/abs/2004.05516v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Text Representations as Meta Learning", "abstract": "Pre-training text representations has recently been shown to significantly\nimprove the state-of-the-art in many natural language processing tasks. The\ncentral goal of pre-training is to learn text representations that are useful\nfor subsequent tasks. However, existing approaches are optimized by minimizing\na proxy objective, such as the negative log likelihood of language modeling. In\nthis work, we introduce a learning algorithm which directly optimizes model's\nability to learn text representations for effective learning of downstream\ntasks. We show that there is an intrinsic connection between multi-task\npre-training and model-agnostic meta-learning with a sequence of meta-train\nsteps. The standard multi-task learning objective adopted in BERT is a special\ncase of our learning algorithm where the depth of meta-train is zero. We study\nthe problem in two settings: unsupervised pre-training and supervised\npre-training with different pre-training objects to verify the generality of\nour approach.Experimental results show that our algorithm brings improvements\nand learns better initializations for a variety of downstream tasks.", "published": "2020-04-12 09:05:47", "link": "http://arxiv.org/abs/2004.05568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR Parsing via Graph-Sequence Iterative Inference", "abstract": "We propose a new end-to-end model that treats AMR parsing as a series of dual\ndecisions on the input sequence and the incrementally constructed graph. At\neach time step, our model performs multiple rounds of attention, reasoning, and\ncomposition that aim to answer two critical questions: (1) which part of the\ninput \\textit{sequence} to abstract; and (2) where in the output \\textit{graph}\nto construct the new concept. We show that the answers to these two questions\nare mutually causalities. We design a model based on iterative inference that\nhelps achieve better answers in both perspectives, leading to greatly improved\nparsing accuracy. Our experimental results significantly outperform all\npreviously reported \\textsc{Smatch} scores by large margins. Remarkably,\nwithout the help of any large-scale pre-trained language model (e.g., BERT),\nour model already surpasses previous state-of-the-art using BERT. With the help\nof BERT, we can push the state-of-the-art results to 80.2\\% on LDC2017T10 (AMR\n2.0) and 75.4\\% on LDC2014T12 (AMR 1.0).", "published": "2020-04-12 09:15:21", "link": "http://arxiv.org/abs/2004.05572v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models", "abstract": "Deep and large pre-trained language models are the state-of-the-art for\nvarious natural language processing tasks. However, the huge size of these\nmodels could be a deterrent to use them in practice. Some recent and concurrent\nworks use knowledge distillation to compress these huge models into shallow\nones. In this work we study knowledge distillation with a focus on\nmulti-lingual Named Entity Recognition (NER). In particular, we study several\ndistillation strategies and propose a stage-wise optimization scheme leveraging\nteacher internal representations that is agnostic of teacher architecture and\nshow that it outperforms strategies employed in prior works. Additionally, we\ninvestigate the role of several factors like the amount of unlabeled data,\nannotation resources, model architecture and inference latency to name a few.\nWe show that our approach leads to massive compression of MBERT-like teacher\nmodels by upto 35x in terms of parameters and 51x in terms of latency for batch\ninference while retaining 95% of its F1-score for NER over 41 languages.", "published": "2020-04-12 19:49:27", "link": "http://arxiv.org/abs/2004.05686v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Question Answering Models through Text Generation", "abstract": "Large pre-trained language models (LMs) have been shown to perform\nsurprisingly well when fine-tuned on tasks that require commonsense and world\nknowledge. However, in end-to-end architectures, it is difficult to explain\nwhat is the knowledge in the LM that allows it to make a correct prediction. In\nthis work, we propose a model for multi-choice question answering, where a\nLM-based generator generates a textual hypothesis that is later used by a\nclassifier to answer the question. The hypothesis provides a window into the\ninformation used by the fine-tuned LM that can be inspected by humans. A key\nchallenge in this setup is how to constrain the model to generate hypotheses\nthat are meaningful to humans. We tackle this by (a) joint training with a\nsimple similarity classifier that encourages meaningful hypotheses, and (b) by\nadding loss functions that encourage natural text without repetitions. We show\non several tasks that our model reaches performance that is comparable to\nend-to-end architectures, while producing hypotheses that elucidate the\nknowledge used by the LM for answering the question.", "published": "2020-04-12 09:06:46", "link": "http://arxiv.org/abs/2004.05569v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Grounding Methods for VQA are Working for the Wrong Reasons!", "abstract": "Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.", "published": "2020-04-12 21:45:23", "link": "http://arxiv.org/abs/2004.05704v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification", "abstract": "Much progress has been made recently on text classification with methods\nbased on neural networks. In particular, models using attention mechanism such\nas BERT have shown to have the capability of capturing the contextual\ninformation within a sentence or document. However, their ability of capturing\nthe global information about the vocabulary of a language is more limited. This\nlatter is the strength of Graph Convolutional Networks (GCN). In this paper, we\npropose VGCN-BERT model which combines the capability of BERT with a Vocabulary\nGraph Convolutional Network (VGCN). Local information and global information\ninteract through different layers of BERT, allowing them to influence mutually\nand to build together a final representation for classification. In our\nexperiments on several text classification datasets, our approach outperforms\nBERT and GCN alone, and achieve higher effectiveness than that reported in\nprevious studies.", "published": "2020-04-12 22:02:33", "link": "http://arxiv.org/abs/2004.05707v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "A hybrid classical-quantum workflow for natural language processing", "abstract": "Natural language processing (NLP) problems are ubiquitous in classical\ncomputing, where they often require significant computational resources to\ninfer sentence meanings. With the appearance of quantum computing hardware and\nsimulators, it is worth developing methods to examine such problems on these\nplatforms. In this manuscript we demonstrate the use of quantum computing\nmodels to perform NLP tasks, where we represent corpus meanings, and perform\ncomparisons between sentences of a given structure. We develop a hybrid\nworkflow for representing small and large scale corpus data sets to be encoded,\nprocessed, and decoded using a quantum circuit model. In addition, we provide\nour results showing the efficacy of the method, and release our developed\ntoolkit as an open software suite.", "published": "2020-04-12 12:19:17", "link": "http://arxiv.org/abs/2004.06800v1", "categories": ["quant-ph", "cs.CL", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Bayesian Hierarchical Words Representation Learning", "abstract": "This paper presents the Bayesian Hierarchical Words Representation (BHWR)\nlearning algorithm. BHWR facilitates Variational Bayes word representation\nlearning combined with semantic taxonomy modeling via hierarchical priors. By\npropagating relevant information between related words, BHWR utilizes the\ntaxonomy to improve the quality of such representations. Evaluation of several\nlinguistic datasets demonstrates the advantages of BHWR over suitable\nalternatives that facilitate Bayesian modeling with or without semantic priors.\nFinally, we further show that BHWR produces better representations for rare\nwords.", "published": "2020-04-12 13:39:52", "link": "http://arxiv.org/abs/2004.07126v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
