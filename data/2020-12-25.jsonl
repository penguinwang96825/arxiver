{"title": "Fine-grained Emotion and Intent Learning in Movie Dialogues", "abstract": "We propose a novel large-scale emotional dialogue dataset, consisting of 1M\ndialogues retrieved from the OpenSubtitles corpus and annotated with 32\nemotions and 9 empathetic response intents using a BERT-based fine-grained\ndialogue emotion classifier. This work explains the complex pipeline used to\npreprocess movie subtitles and select good movie dialogues to annotate. We also\ndescribe the semi-supervised learning process followed to train a fine-grained\nemotion classifier to annotate these dialogues. Despite the large set of\nlabels, our dialogue emotion classifier achieved an accuracy of $65\\%$ and was\nused to annotate 1M emotional movie dialogues from OpenSubtitles. This scale of\nemotional dialogue classification has never been attempted before, both in\nterms of dataset size and fine-grained emotion and intent categories.\nVisualization techniques used to analyze the quality of the resultant dataset\nsuggest that it conforms to the patterns of human social interaction.", "published": "2020-12-25 20:29:56", "link": "http://arxiv.org/abs/2012.13624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Universal Continuous Knowledge Base", "abstract": "In artificial intelligence (AI), knowledge is the information required by an\nintelligent system to accomplish tasks. While traditional knowledge bases use\ndiscrete, symbolic representations, detecting knowledge encoded in the\ncontinuous representations learned from data has received increasing attention\nrecently. In this work, we propose a method for building a continuous knowledge\nbase (CKB) that can store knowledge imported from multiple, diverse neural\nnetworks. The key idea of our approach is to define an interface for each\nneural network and cast knowledge transferring as a function simulation\nproblem. Experiments on text classification show promising results: the CKB\nimports knowledge from a single model and then exports the knowledge to a new\nmodel, achieving comparable performance with the original model. More\ninteresting, we import the knowledge from multiple models to the knowledge\nbase, from which the fused knowledge is exported back to a single model,\nachieving a higher accuracy than the original model. With the CKB, it is also\neasy to achieve knowledge distillation and transfer learning. Our work opens\nthe door to building a universal continuous knowledge base to collect, store,\nand organize all continuous knowledge encoded in various neural networks\ntrained for different AI tasks.", "published": "2020-12-25 12:27:44", "link": "http://arxiv.org/abs/2012.13568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Temperature for Language Modeling", "abstract": "Temperature scaling has been widely used as an effective approach to control\nthe smoothness of a distribution, which helps the model performance in various\ntasks. Current practices to apply temperature scaling assume either a fixed, or\na manually-crafted dynamically changing schedule. However, our studies indicate\nthat the individual optimal trajectory for each class can change with the\ncontext. To this end, we propose contextual temperature, a generalized approach\nthat learns an optimal temperature trajectory for each vocabulary over the\ncontext. Experimental results confirm that the proposed method significantly\nimproves state-of-the-art language models, achieving a perplexity of 55.31 and\n62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth\nanalyses show that the behaviour of the learned temperature schedules varies\ndramatically by vocabulary, and that the optimal schedules help in controlling\nthe uncertainties. These evidences further justify the need for the proposed\nmethod and its advantages over fixed temperature schedules.", "published": "2020-12-25 13:50:03", "link": "http://arxiv.org/abs/2012.13575v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification", "abstract": "Given a natural language statement, how to verify its veracity against a\nlarge-scale textual knowledge source like Wikipedia? Most existing neural\nmodels make predictions without giving clues about which part of a false claim\ngoes wrong. In this paper, we propose LOREN, an approach for interpretable fact\nverification. We decompose the verification of the whole claim at phrase-level,\nwhere the veracity of the phrases serves as explanations and can be aggregated\ninto the final verdict according to logical rules. The key insight of LOREN is\nto represent claim phrase veracity as three-valued latent variables, which are\nregularized by aggregation logical rules. The final claim verification is based\non all latent variables. Thus, LOREN enjoys the additional benefit of\ninterpretability -- it is easy to explain how it reaches certain results with\nclaim phrase veracity. Experiments on a public fact verification benchmark show\nthat LOREN is competitive against previous approaches while enjoying the merit\nof faithful and accurate interpretability. The resources of LOREN are available\nat: https://github.com/jiangjiechen/LOREN.", "published": "2020-12-25 13:57:04", "link": "http://arxiv.org/abs/2012.13577v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An analytic physically motivated model of the mammalian cochlea", "abstract": "We develop an analytic model of the mammalian cochlea. We use a mixed\nphysical-phenomenological approach by utilizing existing work on the physics of\nclassical box-representations of the cochlea, and behavior of recent\ndata-derived wavenumber estimates. Spatial variation is incorporated through a\nsingle independent variable that combines space and frequency. We arrive at\nclosed-form expressions for the organ of Corti velocity, its impedance, the\npressure difference across the organ of Corti, and its wavenumber. We perform\nmodel tests using real and imaginary parts of chinchilla data from multiple\nlocations and for multiple variables. The model also predicts impedances that\nare qualitatively consistent with current literature. For implementation, the\nmodel can leverage existing efforts for both filter bank and filter cascade\nmodels that target improved algorithmic or analog circuit efficiencies. The\nsimplicity of the cochlear model, its small number of model constants, its\nability to capture the variation of tuning, its closed-form expressions for\nphysically-interrelated variables, and the form of these expressions that\nallows for easily determining one variable from another make the model\nappropriate for analytic and digital auditory filter implementations as\ndiscussed here, as well as for extracting macromechanical insights regarding\nhow the cochlea works.", "published": "2020-12-25 00:26:22", "link": "http://arxiv.org/abs/2012.15750v1", "categories": ["q-bio.TO", "cs.SD", "eess.AS", "eess.SP", "q-bio.NC"], "primary_category": "q-bio.TO"}
