{"title": "ajdmom: A Python Package for Deriving Moment Formulas of Affine Jump Diffusion Processes", "abstract": "We introduce ajdmom, a Python package designed for automatically deriving\nmoment formulae for the well-established affine jump diffusion processes with\nstate-independent jump intensities. ajdmom can produce explicit closed-form\nexpressions for conditional and unconditional moments of any order,\nsignificantly enhancing the usability of these models. Additionally, ajdmom can\ncompute partial derivatives of these moments with respect to the model\nparameters, offering a valuable tool for sensitivity analysis. The package's\nmodular architecture makes it easy for adaptation and extension by researchers.\najdmom is open-source and readily available for installation from GitHub or the\nPython package index (PyPI).", "published": "2024-11-10 14:55:10", "link": "http://arxiv.org/abs/2411.06484v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Optimal Execution with Reinforcement Learning", "abstract": "This study investigates the development of an optimal execution strategy\nthrough reinforcement learning, aiming to determine the most effective approach\nfor traders to buy and sell inventory within a limited time frame. Our proposed\nmodel leverages input features derived from the current state of the limit\norder book.\n  To simulate this environment and overcome the limitations associated with\nrelying on historical data, we utilize the multi-agent market simulator ABIDES,\nwhich provides a diverse range of depth levels within the limit order book.\n  We present a custom MDP formulation followed by the results of our\nmethodology and benchmark the performance against standard execution\nstrategies. Our findings suggest that the reinforcement learning-based approach\ndemonstrates significant potential.", "published": "2024-11-10 08:21:03", "link": "http://arxiv.org/abs/2411.06389v1", "categories": ["q-fin.TR", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical\n  Prediction?", "abstract": "Large Language Models (LLMs) hold great promise to revolutionize current\nclinical systems for their superior capacities on medical text processing tasks\nand medical licensing exams. Meanwhile, traditional ML models such as SVM and\nXGBoost have still been mainly adopted in clinical prediction tasks. An\nemerging question is Can LLMs beat traditional ML models in clinical\nprediction? Thus, we build a new benchmark ClinicalBench to comprehensively\nstudy the clinical predictive modeling capacities of both general-purpose and\nmedical LLMs, and compare them with traditional ML models. ClinicalBench\nembraces three common clinical prediction tasks, two databases, 14\ngeneral-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through\nextensive empirical investigation, we discover that both general-purpose and\nmedical LLMs, even with different model scales, diverse prompting or\nfine-tuning strategies, still cannot beat traditional ML models in clinical\nprediction yet, shedding light on their potential deficiency in clinical\nreasoning and decision-making. We call for caution when practitioners adopt\nLLMs in clinical applications. ClinicalBench can be utilized to bridge the gap\nbetween LLMs' development for healthcare and real-world clinical practice.", "published": "2024-11-10 14:07:43", "link": "http://arxiv.org/abs/2411.06469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CULL-MT: Compression Using Language and Layer pruning for Machine\n  Translation", "abstract": "Multilingual machine translation models often outperform traditional\nbilingual models by leveraging translation knowledge transfer. Recent\nadvancements have led to these models supporting hundreds of languages and\nachieving state-of-the-art results across various translation directions.\nHowever, as these models grow larger, their inference operations become\nincreasingly costly. In many use cases, there is no need to support such a wide\nrange of language pairs, as translation is typically needed in only a few\nselected directions. In this paper, we present CULL-MT, a compression method\nfor machine translation models based on structural layer pruning and selected\nlanguage directions. Our approach identifies and prunes unimportant layers\nusing a greedy strategy, then mitigates the impact by applying knowledge\ndistillation from the original model along with parameter-efficient\nfine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models.\nIn a multi-way translation scenario (Persian, French, and German to English),\nwe find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned\nwith only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive,\nwith a 2.0 spBLEU drop after pruning 5 layers.", "published": "2024-11-10 16:05:11", "link": "http://arxiv.org/abs/2411.06506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube\n  Comments on Movie-Drama using Transformers: Insights from Interpretability\n  Tool", "abstract": "In recent years, YouTube has become the leading platform for Bangla movies\nand dramas, where viewers express their opinions in comments that convey their\nsentiments about the content. However, not all comments are relevant for\nsentiment analysis, necessitating a filtering mechanism. We propose a system\nthat first assesses the relevance of comments and then analyzes the sentiment\nof those deemed relevant. We introduce a dataset of 14,000 manually collected\nand preprocessed comments, annotated for relevance (relevant or irrelevant) and\nsentiment (positive or negative). Eight transformer models, including\nBanglaBERT, were used for classification tasks, with BanglaBERT achieving the\nhighest accuracy (83.99% for relevance detection and 93.3% for sentiment\nanalysis). The study also integrates LIME to interpret model decisions,\nenhancing transparency.", "published": "2024-11-10 18:04:41", "link": "http://arxiv.org/abs/2411.06548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The KIPARLA Forest treebank of spoken Italian: an overview of initial\n  design choices", "abstract": "The paper presents an overview of initial design choices discussed towards\nthe creation of a treebank for the Italian KIParla corpus", "published": "2024-11-10 18:32:24", "link": "http://arxiv.org/abs/2411.06554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Vocabulary Compression for Low-Compute Environments", "abstract": "We present a method to compress the final linear layer of language models,\nreducing memory usage by up to 3.4x without significant performance loss. By\ngrouping tokens based on Byte Pair Encoding (BPE) merges, we prevent\nmaterialization of the memory-intensive logits tensor. Evaluations on the\nTinyStories dataset show that our method performs on par with GPT-Neo and GPT2\nwhile significantly improving throughput by up to 3x, making it suitable for\nlow-compute environments.", "published": "2024-11-10 06:39:10", "link": "http://arxiv.org/abs/2411.06371v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small\n  Language Models", "abstract": "As large language models (LLMs) grow and develop, so do their data demands.\nThis is especially true for multilingual LLMs, where the scarcity of\nhigh-quality and readily available data online has led to a multitude of\nsynthetic dataset generation approaches. A key technique in this space is\nmachine translation (MT), where high-quality English text is adapted to a\ntarget, comparatively low-resource language. This report introduces\nFineWeb-Edu-Ar, a machine-translated version of the exceedingly popular\n(deduplicated) FineWeb-Edu dataset from HuggingFace. To the best of our\nknowledge, FineWeb-Edu-Ar is the largest publicly available machine-translated\nArabic dataset out there, with its size of 202B tokens of an Arabic-trained\ntokenizer.", "published": "2024-11-10 09:29:51", "link": "http://arxiv.org/abs/2411.06402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Toxic Neurons: A Mechanistic Analysis of DPO for Toxicity\n  Reduction", "abstract": "Safety fine-tuning algorithms are widely used to reduce harmful outputs in\nlanguage models, but how they achieve this remain unclear. Studying the Direct\nPreference Optimization (DPO) algorithm for toxicity reduction, current\nexplanations claim that DPO achieves this by dampening the activations of toxic\nMLP neurons. However, through activation patching, we show that this\nexplanation is incomplete. Projections onto a toxicity probe's direction show\nthat only 4.9% of toxicity reduction comes from dampened toxic neurons.\nInstead, DPO reduces toxicity through distributed activation shifts across a\nmajority of neurons, progressively shifting MLP layer outputs away from\ntoxicity. These shifts accumulate across four neuron groups: two reducing\ntoxicity and two promoting anti-toxicity. Activation patching validates the\ncumulative roles of these groups, where patching all identified groups\neffectively replicates DPO's effects. These findings illustrate DPO's\nmechanism: it reduces toxicity by accumulating small activation shifts across\nmany neurons throughout the layers. Our findings provide new mechanistic\ninsights into how safety fine-tuning reduces harmful outputs in language\nmodels.", "published": "2024-11-10 11:07:34", "link": "http://arxiv.org/abs/2411.06424v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Conditional [MASK] Discrete Diffusion Language Model", "abstract": "Although auto-regressive models excel in natural language processing, they\noften struggle to generate diverse text and provide limited controllability.\nNon-auto-regressive methods could be an alternative but often produce\ndegenerate outputs and exhibit shortcomings in conditional generation. To\naddress these challenges, we propose Diffusion-EAGS, a novel framework that\nintegrates conditional masked language models into diffusion language models\nthrough the theoretical lens of a conditional Markov Random Field. In doing so,\nwe propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling\nto counterbalance each model's shortcomings. Experimental results show that\nDiffusion-EAGS outperforms baselines and achieves the best quality-diversity\ntradeoff, demonstrating its effectiveness in non-autoregressive text\ngeneration.", "published": "2024-11-10 11:49:36", "link": "http://arxiv.org/abs/2411.06438v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce\n  Hallucination and to Improve Reproducibility in Scientific Text Generation\n  Using Stochastic Optimisation Techniques", "abstract": "Large Language Models (LLMs) are increasingly adopted for complex scientific\ntext generation tasks, yet they often suffer from limitations in accuracy,\nconsistency, and hallucination control. This thesis introduces a\nParameter-Efficient Fine-Tuning (PEFT) approach tailored for GPT-like models,\naiming to mitigate hallucinations and enhance reproducibility, particularly in\nthe computational domain of mass spectrometry. We implemented Low-Rank\nAdaptation (LoRA) adapters to refine GPT-2, termed MS-GPT, using a specialized\ncorpus of mass spectrometry literature. Through novel evaluation methods\napplied to LLMs, including BLEU, ROUGE, and Perplexity scores, the fine-tuned\nMS-GPT model demonstrated superior text coherence and reproducibility compared\nto the baseline GPT-2, confirmed through statistical analysis with the Wilcoxon\nrank-sum test. Further, we propose a reproducibility metric based on cosine\nsimilarity of model outputs under controlled prompts, showcasing MS-GPT's\nenhanced stability. This research highlights PEFT's potential to optimize LLMs\nfor scientific contexts, reducing computational costs while improving model\nreliability.", "published": "2024-11-10 12:28:09", "link": "http://arxiv.org/abs/2411.06445v1", "categories": ["cs.CL", "cs.AI", "68T50, 91B82", "I.2.7; G.3"], "primary_category": "cs.CL"}
{"title": "VocalTweets: Investigating Social Media Offensive Language Among\n  Nigerian Musicians", "abstract": "Musicians frequently use social media to express their opinions, but they\noften convey different messages in their music compared to their posts online.\nSome utilize these platforms to abuse their colleagues, while others use it to\nshow support for political candidates or engage in activism, as seen during the\n#EndSars protest. There are extensive research done on offensive language\ndetection on social media, the usage of offensive language by musicians has\nreceived limited attention. In this study, we introduce VocalTweets, a\ncode-switched and multilingual dataset comprising tweets from 12 prominent\nNigerian musicians, labeled with a binary classification method as Normal or\nOffensive. We trained a model using HuggingFace's base-Twitter-RoBERTa,\nachieving an F1 score of 74.5. Additionally, we conducted cross-corpus\nexperiments with the OLID dataset to evaluate the generalizability of our\ndataset.", "published": "2024-11-10 14:31:36", "link": "http://arxiv.org/abs/2411.06477v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Learning for Preserving Patient Privacy: A Framework for\n  Synthesizing Realistic Patient Portal Messages", "abstract": "Since the COVID-19 pandemic, clinicians have seen a large and sustained\ninflux in patient portal messages, significantly contributing to clinician\nburnout. To the best of our knowledge, there are no large-scale public patient\nportal messages corpora researchers can use to build tools to optimize\nclinician portal workflows. Informed by our ongoing work with a regional\nhospital, this study introduces an LLM-powered framework for configurable and\nrealistic patient portal message generation. Our approach leverages few-shot\ngrounded text generation, requiring only a small number of de-identified\npatient portal messages to help LLMs better match the true style and tone of\nreal data. Clinical experts in our team deem this framework as HIPAA-friendly,\nunlike existing privacy-preserving approaches to synthetic text generation\nwhich cannot guarantee all sensitive attributes will be protected. Through\nextensive quantitative and human evaluation, we show that our framework\nproduces data of higher quality than comparable generation methods as well as\nall related datasets. We believe this work provides a path forward for (i) the\nrelease of large-scale synthetic patient message datasets that are\nstylistically similar to ground-truth samples and (ii) HIPAA-friendly data\ngeneration which requires minimal human de-identification efforts.", "published": "2024-11-10 18:06:55", "link": "http://arxiv.org/abs/2411.06549v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive\n  Qualitative Coding Results", "abstract": "Inductive qualitative methods have been a mainstay of education research for\ndecades, yet it takes much time and effort to conduct rigorously. Recent\nadvances in artificial intelligence, particularly with generative AI (GAI),\nhave led to initial success in generating inductive coding results. Like human\ncoders, GAI tools rely on instructions to work, and how to instruct it may\nmatter. To understand how ML/GAI approaches could contribute to qualitative\ncoding processes, this study applied two known and two theory-informed novel\napproaches to an online community dataset and evaluated the resulting coding\nresults. Our findings show significant discrepancies between ML/GAI approaches\nand demonstrate the advantage of our approaches, which introduce human coding\nprocesses into GAI prompts.", "published": "2024-11-10 00:23:55", "link": "http://arxiv.org/abs/2411.06316v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Self-Training Meets Consistency: Improving LLMs' Reasoning with\n  Consistency-Driven Rationale Evaluation", "abstract": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.", "published": "2024-11-10 08:11:05", "link": "http://arxiv.org/abs/2411.06387v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CausalStock: Deep End-to-end Causal Discovery for News-driven Stock\n  Movement Prediction", "abstract": "There are two issues in news-driven multi-stock movement prediction tasks\nthat are not well solved in the existing works. On the one hand, \"relation\ndiscovery\" is a pivotal part when leveraging the price information of other\nstocks to achieve accurate stock movement prediction. Given that stock\nrelations are often unidirectional, such as the \"supplier-consumer\"\nrelationship, causal relations are more appropriate to capture the impact\nbetween stocks. On the other hand, there is substantial noise existing in the\nnews data leading to extracting effective information with difficulty. With\nthese two issues in mind, we propose a novel framework called CausalStock for\nnews-driven multi-stock movement prediction, which discovers the temporal\ncausal relations between stocks. We design a lag-dependent temporal causal\ndiscovery mechanism to model the temporal causal graph distribution. Then a\nFunctional Causal Model is employed to encapsulate the discovered causal\nrelations and predict the stock movements. Additionally, we propose a Denoised\nNews Encoder by taking advantage of the excellent text evaluation ability of\nlarge language models (LLMs) to extract useful information from massive news\ndata. The experiment results show that CausalStock outperforms the strong\nbaselines for both news-driven multi-stock movement prediction and multi-stock\nmovement prediction tasks on six real-world datasets collected from the US,\nChina, Japan, and UK markets. Moreover, getting benefit from the causal\nrelations, CausalStock could offer a clear prediction mechanism with good\nexplainability.", "published": "2024-11-10 08:24:03", "link": "http://arxiv.org/abs/2411.06391v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains", "abstract": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.", "published": "2024-11-10 11:08:28", "link": "http://arxiv.org/abs/2411.06426v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "CTC-Assisted LLM-Based Contextual ASR", "abstract": "Contextual ASR or hotword customization holds substantial practical value.\nDespite the impressive performance of current end-to-end (E2E) automatic speech\nrecognition (ASR) systems, they often face challenges in accurately recognizing\nrare words. Typical E2E contextual ASR models commonly feature complex\narchitectures and decoding mechanisms, limited in performance and susceptible\nto interference from distractor words. With large language model (LLM)-based\nASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based\nContextual ASR model with an efficient filtering algorithm. By using coarse CTC\ndecoding results to filter potential relevant hotwords and incorporating them\ninto LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and\n2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on\nrecognizing rare long-tail words, demonstrating significant improvements\ncompared to the baseline LLM-based ASR model, and substantially surpassing\nother related work. More remarkably, with the help of the large language model\nand proposed filtering algorithm, our contextual ASR model still performs well\nwith 2000 biasing words.", "published": "2024-11-10 11:47:50", "link": "http://arxiv.org/abs/2411.06437v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Epistemic Integrity in Large Language Models", "abstract": "Large language models are increasingly relied upon as sources of information,\nbut their propensity for generating false or misleading statements with high\nconfidence poses risks for users and society. In this paper, we confront the\ncritical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's\nlinguistic assertiveness fails to reflect its true internal certainty. We\nintroduce a new human-labeled dataset and a novel method for measuring the\nlinguistic assertiveness of Large Language Models (LLMs) which cuts error rates\nby over 50% relative to previous benchmarks. Validated across multiple\ndatasets, our method reveals a stark misalignment between how confidently\nmodels linguistically present information and their actual accuracy. Further\nhuman evaluations confirm the severity of this miscalibration. This evidence\nunderscores the urgent risk of the overstated certainty LLMs hold which may\nmislead users on a massive scale. Our framework provides a crucial step forward\nin diagnosing this miscalibration, offering a path towards correcting it and\nmore trustworthy AI across domains.", "published": "2024-11-10 17:10:13", "link": "http://arxiv.org/abs/2411.06528v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Probabilistic Consensus through Ensemble Validation: A Framework for LLM\n  Reliability", "abstract": "Large Language Models (LLMs) have shown significant advances in text\ngeneration but often lack the reliability needed for autonomous deployment in\nhigh-stakes domains like healthcare, law, and finance. Existing approaches rely\non external knowledge or human oversight, limiting scalability. We introduce a\nnovel framework that repurposes ensemble methods for content validation through\nmodel consensus. In tests across 78 complex cases requiring factual accuracy\nand causal consistency, our framework improved precision from 73.1% to 93.9%\nwith two models (95% CI: 83.5%-97.9%) and to 95.6% with three models (95% CI:\n85.2%-98.8%). Statistical analysis indicates strong inter-model agreement\n($\\kappa$ > 0.76) while preserving sufficient independence to catch errors\nthrough disagreement. We outline a clear pathway to further enhance precision\nwith additional validators and refinements. Although the current approach is\nconstrained by multiple-choice format requirements and processing latency, it\noffers immediate value for enabling reliable autonomous AI systems in critical\napplications.", "published": "2024-11-10 17:32:16", "link": "http://arxiv.org/abs/2411.06535v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CriticAL: Critic Automation with Language Models", "abstract": "Understanding the world through models is a fundamental goal of scientific\nresearch. While large language model (LLM) based approaches show promise in\nautomating scientific discovery, they often overlook the importance of\ncriticizing scientific models. Criticizing models deepens scientific\nunderstanding and drives the development of more accurate models. Automating\nmodel criticism is difficult because it traditionally requires a human expert\nto define how to compare a model with data and evaluate if the discrepancies\nare significant--both rely heavily on understanding the modeling assumptions\nand domain. Although LLM-based critic approaches are appealing, they introduce\nnew challenges: LLMs might hallucinate the critiques themselves. Motivated by\nthis, we introduce CriticAL (Critic Automation with Language Models). CriticAL\nuses LLMs to generate summary statistics that capture discrepancies between\nmodel predictions and data, and applies hypothesis tests to evaluate their\nsignificance. We can view CriticAL as a verifier that validates models and\ntheir critiques by embedding them in a hypothesis testing framework. In\nexperiments, we evaluate CriticAL across key quantitative and qualitative\ndimensions. In settings where we synthesize discrepancies between models and\ndatasets, CriticAL reliably generates correct critiques without hallucinating\nincorrect ones. We show that both human and LLM judges consistently prefer\nCriticAL's critiques over alternative approaches in terms of transparency and\nactionability. Finally, we show that CriticAL's critiques enable an LLM\nscientist to improve upon human-designed models on real-world datasets.", "published": "2024-11-10 20:41:35", "link": "http://arxiv.org/abs/2411.06590v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Debatts: Zero-Shot Debating Text-to-Speech Synthesis", "abstract": "In debating, rebuttal is one of the most critical stages, where a speaker\naddresses the arguments presented by the opposing side. During this process,\nthe speaker synthesizes their own persuasive articulation given the context\nfrom the opposing side. This work proposes a novel zero-shot text-to-speech\nsynthesis system for rebuttal, namely Debatts. Debatts takes two speech\nprompts, one from the opposing side (i.e. opponent) and one from the speaker.\nThe prompt from the opponent is supposed to provide debating style prosody, and\nthe prompt from the speaker provides identity information. In particular, we\npretrain the Debatts system from in-the-wild dataset, and integrate an\nadditional reference encoder to take debating prompt for style. In addition, we\nalso create a debating dataset to develop Debatts. In this setting, Debatts can\ngenerate a debating-style speech in rebuttal for any voices. Experimental\nresults confirm the effectiveness of the proposed system in comparison with the\nclassic zero-shot TTS systems.", "published": "2024-11-10 17:42:06", "link": "http://arxiv.org/abs/2411.06540v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PSELDNets: Pre-trained Neural Networks on Large-scale Synthetic Datasets\n  for Sound Event Localization and Detection", "abstract": "Sound event localization and detection (SELD) has seen substantial\nadvancements through learning-based methods. These systems, typically trained\nfrom scratch on specific datasets, have shown considerable generalization\ncapabilities. Recently, deep neural networks trained on large-scale datasets\nhave achieved remarkable success in the sound event classification (SEC) field,\nprompting an open question of whether these advancements can be extended to\ndevelop general-purpose SELD models. In this paper, leveraging the power of\npre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on\nlarge-scale synthetic datasets. These synthetic datasets, generated by\nconvolving sound events with simulated spatial room impulse responses (SRIRs),\ncontain 1,167 hours of audio clips with an ontology of 170 sound classes. These\nPSELDNets are transferred to downstream SELD tasks. When we adapt PSELDNets to\nspecific scenarios, particularly in low-resource data cases, we introduce a\ndata-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on a\nsynthetic-test-set using collected SRIRs from TAU Spatial Room Impulse Response\nDatabase (TAU-SRIR DB) and achieve satisfactory performance. We also conduct\nour experiments to validate the transferability of PSELDNets to three publicly\navailable datasets and our own collected audio recordings. Results demonstrate\nthat PSELDNets surpass state-of-the-art systems across all publicly available\ndatasets. Given the need for direction-of-arrival estimation, SELD generally\nrelies on sufficient multi-channel audio clips. However, incorporating the\nAdapterBit, PSELDNets show more efficient adaptability to various tasks using\nminimal multi-channel or even just monophonic audio clips, outperforming the\ntraditional fine-tuning approaches.", "published": "2024-11-10 09:20:48", "link": "http://arxiv.org/abs/2411.06399v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diff-MSTC: A Mixing Style Transfer Prototype for Cubase", "abstract": "In our demo, participants are invited to explore the Diff-MSTC prototype,\nwhich integrates the Diff-MST model into Steinberg's digital audio workstation\n(DAW), Cubase. Diff-MST, a deep learning model for mixing style transfer,\nforecasts mixing console parameters for tracks using a reference song. The\nsystem processes up to 20 raw tracks along with a reference song to predict\nmixing console parameters that can be used to create an initial mix. Users have\nthe option to manually adjust these parameters further for greater control. In\ncontrast to earlier deep learning systems that are limited to research ideas,\nDiff-MSTC is a first-of-its-kind prototype integrated into a DAW. This\nintegration facilitates mixing decisions on multitracks and lets users input\ncontext through a reference song, followed by fine-tuning of audio effects in a\ntraditional manner.", "published": "2024-11-10 19:43:31", "link": "http://arxiv.org/abs/2411.06576v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
