{"title": "Variational Inference for Logical Inference", "abstract": "Functional Distributional Semantics is a framework that aims to learn, from\ntext, semantic representations which can be interpreted in terms of truth. Here\nwe make two contributions to this framework. The first is to show how a type of\nlogical inference can be performed by evaluating conditional probabilities. The\nsecond is to make these calculations tractable by means of a variational\napproximation. This approximation also enables faster convergence during\ntraining, allowing us to close the gap with state-of-the-art vector space\nmodels when evaluating on semantic similarity. We demonstrate promising\nperformance on two tasks.", "published": "2017-09-01 09:55:44", "link": "http://arxiv.org/abs/1709.00224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Composition via Probabilistic Model Theory", "abstract": "Semantic composition remains an open problem for vector space models of\nsemantics. In this paper, we explain how the probabilistic graphical model used\nin the framework of Functional Distributional Semantics can be interpreted as a\nprobabilistic version of model theory. Building on this, we explain how various\nsemantic phenomena can be recast in terms of conditional probabilities in the\ngraphical model. This connection between formal semantics and machine learning\nis helpful in both directions: it gives us an explicit mechanism for modelling\ncontext-dependent meanings (a challenge for formal semantics), and also gives\nus well-motivated techniques for composing distributed representations (a\nchallenge for distributional semantics). We present results on two datasets\nthat go beyond word similarity, showing how these semantically-motivated\ntechniques improve on the performance of vector models.", "published": "2017-09-01 10:01:52", "link": "http://arxiv.org/abs/1709.00226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arc-Standard Spinal Parsing with Stack-LSTMs", "abstract": "We present a neural transition-based parser for spinal trees, a dependency\nrepresentation of constituent trees. The parser uses Stack-LSTMs that compose\nconstituent nodes with dependency-based derivations. In experiments, we show\nthat this model adapts to different styles of dependency relations, but this\nchoice has little effect for predicting constituent structure, suggesting that\nLSTMs induce useful states by themselves.", "published": "2017-09-01 21:38:28", "link": "http://arxiv.org/abs/1709.00489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-by-example Spoken Term Detection using Attention-based Multi-hop\n  Networks", "abstract": "Retrieving spoken content with spoken queries, or query-by- example spoken\nterm detection (STD), is attractive because it makes possible the matching of\nsignals directly on the acoustic level without transcribing them into text.\nHere, we propose an end-to-end query-by-example STD model based on an\nattention-based multi-hop network, whose input is a spoken query and an audio\nsegment containing several utterances; the output states whether the audio\nsegment includes the query. The model can be trained in either a supervised\nscenario using labeled data, or in an unsupervised fashion. In the supervised\nscenario, we find that the attention mechanism and multiple hops improve\nperformance, and that the attention weights indicate the time span of the\ndetected terms. In the unsupervised setting, the model mimics the behavior of\nthe existing query-by-example STD system, yielding performance comparable to\nthe existing system but with a lower search time complexity.", "published": "2017-09-01 14:56:53", "link": "http://arxiv.org/abs/1709.00354v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Learning what to read: Focused machine reading", "abstract": "Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents.", "published": "2017-09-01 04:09:42", "link": "http://arxiv.org/abs/1709.00149v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.6; I.2.7"], "primary_category": "cs.AI"}
{"title": "Order-Planning Neural Text Generation From Structured Data", "abstract": "Generating texts from structured data (e.g., a table) is important for\nvarious natural language processing tasks such as question answering and dialog\nsystems. In recent studies, researchers use neural language models and\nencoder-decoder frameworks for table-to-text generation. However, these neural\nnetwork-based approaches do not model the order of contents during text\ngeneration. When a human writes a summary based on a given table, he or she\nwould probably consider the content order before wording. In a biography, for\nexample, the nationality of a person is typically mentioned before occupation\nin a biography. In this paper, we propose an order-planning text generation\nmodel to capture the relationship between different fields and use such\nrelationship to make the generated text more fluent and smooth. We conducted\nexperiments on the WikiBio dataset and achieve significantly higher performance\nthan previous methods in terms of BLEU, ROUGE, and NIST scores.", "published": "2017-09-01 04:46:10", "link": "http://arxiv.org/abs/1709.00155v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Making \"fetch\" happen: The influence of social and linguistic context on\n  nonstandard word growth and decline", "abstract": "In an online community, new words come and go: today's \"haha\" may be replaced\nby tomorrow's \"lol.\" Changes in online writing are usually studied as a social\nprocess, with innovations diffusing through a network of individuals in a\nspeech community. But unlike other types of innovation, language change is\nshaped and constrained by the system in which it takes part. To investigate the\nlinks between social and structural factors in language change, we undertake a\nlarge-scale analysis of nonstandard word growth in the online community Reddit.\nWe find that dissemination across many linguistic contexts is a sign of growth:\nwords that appear in more linguistic contexts grow faster and survive longer.\nWe also find that social dissemination likely plays a less important role in\nexplaining word growth and decline than previously hypothesized.", "published": "2017-09-01 14:38:21", "link": "http://arxiv.org/abs/1709.00345v4", "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional\n  Neural Networks", "abstract": "Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus only on addressing audio information. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. We also propose a multi-task learning\nframework for reconstructing audio and visual signals at the output layer.\nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual\nencoder-decoder network, in which audio and visual data are first processed\nusing individual CNNs, and then fused into a joint network to generate enhanced\nspeech (the primary task) and reconstructed images (the secondary task) at the\noutput layer. The model is trained in an end-to-end manner, and parameters are\njointly learned through back-propagation. We evaluate enhanced speech using\nfive instrumental criteria. Results show that the AVDCNN model yields a notably\nsuperior performance compared with an audio-only CNN-based SE model and two\nconventional SE approaches, confirming the effectiveness of integrating visual\ninformation into the SE process. In addition, the AVDCNN model also outperforms\nan existing audio-visual SE model, confirming its capability of effectively\ncombining audio and visual information in SE.", "published": "2017-09-01 14:17:53", "link": "http://arxiv.org/abs/1709.00944v5", "categories": ["cs.SD", "cs.MM", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
