{"title": "Conversational Transfer Learning for Emotion Recognition", "abstract": "Recognizing emotions in conversations is a challenging task due to the\npresence of contextual dependencies governed by self- and inter-personal\ninfluences. Recent approaches have focused on modeling these dependencies\nprimarily via supervised learning. However, purely supervised strategies demand\nlarge amounts of annotated data, which is lacking in most of the available\ncorpora in this task. To tackle this challenge, we look at transfer learning\napproaches as a viable alternative. Given the large amount of available\nconversational data, we investigate whether generative conversational models\ncan be leveraged to transfer affective knowledge for detecting emotions in\ncontext. We propose an approach, TL-ERC, where we pre-train a hierarchical\ndialogue model on multi-turn conversations (source) and then transfer its\nparameters to a conversational emotion classifier (target). In addition to the\npopular practice of using pre-trained sentence encoders, our approach also\nincorporates recurrent parameters that model inter-sentential context across\nthe whole conversation. Based on this idea, we perform several experiments\nacross multiple datasets and find improvement in performance and robustness\nagainst limited training data. TL-ERC also achieves better validation\nperformances in significantly fewer epochs. Overall, we infer that knowledge\nacquired from dialogue generators can indeed help recognize emotions in\nconversations.", "published": "2019-10-11 05:39:08", "link": "http://arxiv.org/abs/1910.04980v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Group, Extract and Aggregate: Summarizing a Large Amount of Finance News\n  for Forex Movement Prediction", "abstract": "Incorporating related text information has proven successful in stock market\nprediction. However, it is a huge challenge to utilize texts in the enormous\nforex (foreign currency exchange) market because the associated texts are too\nredundant. In this work, we propose a BERT-based Hierarchical Aggregation Model\nto summarize a large amount of finance news to predict forex movement. We\nfirstly group news from different aspects: time, topic and category. Then we\nextract the most crucial news in each group by the SOTA extractive\nsummarization method. Finally, we conduct interaction between the news and the\ntrade data with attention to predict the forex movement. The experimental\nresults show that the category based method performs best among three grouping\nmethods and outperforms all the baselines. Besides, we study the influence of\nessential news attributes (category and region) by statistical analysis and\nsummarize the influence patterns for different currency pairs.", "published": "2019-10-11 08:56:52", "link": "http://arxiv.org/abs/1910.05032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Conversational Question Answering over a\n  Large-Scale Knowledge Base", "abstract": "We consider the problem of conversational question answering over a\nlarge-scale knowledge base. To handle huge entity vocabulary of a large-scale\nknowledge base, recent neural semantic parsing based approaches usually\ndecompose the task into several subtasks and then solve them sequentially,\nwhich leads to following issues: 1) errors in earlier subtasks will be\npropagated and negatively affect downstream ones; and 2) each subtask cannot\nnaturally share supervision signals with others. To tackle these issues, we\npropose an innovative multi-task learning framework where a pointer-equipped\nsemantic parsing model is designed to resolve coreference in conversations, and\nnaturally empower joint learning with a novel type-aware entity detection\nmodel. The proposed framework thus enables shared supervisions and alleviates\nthe effect of error propagation. Experiments on a large-scale conversational\nquestion answering dataset containing 1.6M question answering pairs over 12.8M\nentities show that the proposed framework improves overall F1 score from 67% to\n79% compared with previous state-of-the-art work.", "published": "2019-10-11 10:40:42", "link": "http://arxiv.org/abs/1910.05069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does Language Influence Documentation Workflow? Unsupervised Word\n  Discovery Using Translations in Multiple Languages", "abstract": "For language documentation initiatives, transcription is an expensive\nresource: one minute of audio is estimated to take one hour and a half on\naverage of a linguist's work (Austin and Sallabank, 2013). Recently, collecting\naligned translations in well-resourced languages became a popular solution for\nensuring posterior interpretability of the recordings (Adda et al. 2016). In\nthis paper we investigate language-related impact in automatic approaches for\ncomputational language documentation. We translate the bilingual Mboshi-French\nparallel corpus (Godard et al. 2017) into four other languages, and we perform\nbilingual-rooted unsupervised word discovery. Our results hint towards an\nimpact of the well-resourced language in the quality of the output. However, by\ncombining the information learned by different bilingual models, we are only\nable to marginally increase the quality of the segmentation.", "published": "2019-10-11 13:04:31", "link": "http://arxiv.org/abs/1910.05154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Generation for Czech: Data and Baselines", "abstract": "We present the first dataset targeted at end-to-end NLG in Czech in the\nrestaurant domain, along with several strong baseline models using the\nsequence-to-sequence approach. While non-English NLG is under-explored in\ngeneral, Czech, as a morphologically rich language, makes the task even harder:\nSince Czech requires inflecting named entities, delexicalization or copy\nmechanisms do not work out-of-the-box and lexicalizing the generated outputs is\nnon-trivial.\n  In our experiments, we present two different approaches to this this problem:\n(1) using a neural language model to select the correct inflected form while\nlexicalizing, (2) a two-step generation setup: our sequence-to-sequence model\ngenerates an interleaved sequence of lemmas and morphological tags, which are\nthen inflected by a morphological generator.", "published": "2019-10-11 16:43:05", "link": "http://arxiv.org/abs/1910.05298v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatic segmentation of texts into units of meaning for reading\n  assistance", "abstract": "The emergence of the digital book is a major step forward in providing access\nto reading, and therefore often to the common culture and the labour market. By\nallowing the enrichment of texts with cognitive crutches, EPub 3 compatible\naccessibility formats such as FROG have proven their effectiveness in\nalleviating but also reducing dyslexic disorders. In this paper, we show how\nArtificial Intelligence and particularly Transfer Learning with Google BERT can\nautomate the division into units of meaning, and thus facilitate the creation\nof enriched digital books at a moderate cost.", "published": "2019-10-11 07:54:38", "link": "http://arxiv.org/abs/1910.05014v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.7; K.3.2"], "primary_category": "cs.CL"}
{"title": "BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual\n  Reading Comprehension on Novels", "abstract": "This paper presents BiPaR, a bilingual parallel novel-style machine reading\ncomprehension (MRC) dataset, developed to support multilingual and\ncross-lingual reading comprehension. The biggest difference between BiPaR and\nexisting reading comprehension datasets is that each triple (Passage, Question,\nAnswer) in BiPaR is written parallelly in two languages. We collect 3,667\nbilingual parallel paragraphs from Chinese and English novels, from which we\nconstruct 14,668 parallel question-answer pairs via crowdsourced workers\nfollowing a strict quality control procedure. We analyze BiPaR in depth and\nfind that BiPaR offers good diversification in prefixes of questions, answer\ntypes and relationships between questions and passages. We also observe that\nanswering questions of novels requires reading comprehension skills of\ncoreference resolution, multi-sentence reasoning, and understanding of implicit\ncausality, etc. With BiPaR, we build monolingual, multilingual, and\ncross-lingual MRC baseline models. Even for the relatively simple monolingual\nMRC on this dataset, experiments show that a strong BERT baseline is over 30\npoints behind human in terms of both EM and F1 score, indicating that BiPaR\nprovides a challenging testbed for monolingual, multilingual and cross-lingual\nMRC on novels. The dataset is available at https://multinlp.github.io/BiPaR/.", "published": "2019-10-11 09:16:29", "link": "http://arxiv.org/abs/1910.05040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keyphrase Generation: A Multi-Aspect Survey", "abstract": "Extractive keyphrase generation research has been around since the nineties,\nbut the more advanced abstractive approach based on the encoder-decoder\nframework and sequence-to-sequence learning has been explored only recently. In\nfact, more than a dozen of abstractive methods have been proposed in the last\nthree years, producing meaningful keyphrases and achieving state-of-the-art\nscores. In this survey, we examine various aspects of the extractive keyphrase\ngeneration methods and focus mostly on the more recent abstractive methods that\nare based on neural networks. We pay particular attention to the mechanisms\nthat have driven the perfection of the later. A huge collection of scientific\narticle metadata and the corresponding keyphrases is created and released for\nthe research community. We also present various keyphrase generation and text\nsummarization research patterns and trends of the last two decades.", "published": "2019-10-11 10:03:46", "link": "http://arxiv.org/abs/1910.05059v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in\n  Transformers Models", "abstract": "Large language models can produce powerful contextual representations that\nlead to improvements across many NLP tasks. Since these models are typically\nguided by a sequence of learned self attention mechanisms and may comprise\nundesired inductive biases, it is paramount to be able to explore what the\nattention has learned. While static analyses of these models lead to targeted\ninsights, interactive tools are more dynamic and can help humans better gain an\nintuition for the model-internal reasoning process. We present exBERT, an\ninteractive tool named after the popular BERT language model, that provides\ninsights into the meaning of the contextual representations by matching a\nhuman-specified input to similar contexts in a large annotated dataset. By\naggregating the annotations of the matching similar contexts, exBERT helps\nintuitively explain what each attention-head has learned.", "published": "2019-10-11 16:10:55", "link": "http://arxiv.org/abs/1910.05276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Analogy-Preserving Sentence Embeddings for Answer Selection", "abstract": "Answer selection aims at identifying the correct answer for a given question\nfrom a set of potentially correct answers. Contrary to previous works, which\ntypically focus on the semantic similarity between a question and its answer,\nour hypothesis is that question-answer pairs are often in analogical relation\nto each other. Using analogical inference as our use case, we propose a\nframework and a neural network architecture for learning dedicated sentence\nembeddings that preserve analogical properties in the semantic space. We\nevaluate the proposed method on benchmark datasets for answer selection and\ndemonstrate that our sentence embeddings indeed capture analogical properties\nbetter than conventional embeddings, and that analogy-based question answering\noutperforms a comparable similarity-based technique.", "published": "2019-10-11 17:22:19", "link": "http://arxiv.org/abs/1910.05315v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model-based Interactive Semantic Parsing: A Unified Framework and A\n  Text-to-SQL Case Study", "abstract": "As a promising paradigm, interactive semantic parsing has shown to improve\nboth semantic parsing accuracy and user confidence in the results. In this\npaper, we propose a new, unified formulation of the interactive semantic\nparsing problem, where the goal is to design a model-based intelligent agent.\nThe agent maintains its own state as the current predicted semantic parse,\ndecides whether and where human intervention is needed, and generates a\nclarification question in natural language. A key part of the agent is a world\nmodel: it takes a percept (either an initial question or subsequent feedback\nfrom the user) and transitions to a new state. We then propose a simple yet\nremarkably effective instantiation of our framework, demonstrated on two\ntext-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base\nsemantic parsers. Compared to an existing interactive semantic parsing approach\nthat treats the base parser as a black box, our approach solicits less user\nfeedback but yields higher run-time accuracy.", "published": "2019-10-11 19:56:47", "link": "http://arxiv.org/abs/1910.05389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Invariant Representations of Social Media Users", "abstract": "The evolution of social media users' behavior over time complicates\nuser-level comparison tasks such as verification, classification, clustering,\nand ranking. As a result, na\\\"ive approaches may fail to generalize to new\nusers or even to future observations of previously known users. In this paper,\nwe propose a novel procedure to learn a mapping from short episodes of user\nactivity on social media to a vector space in which the distance between points\ncaptures the similarity of the corresponding users' invariant features. We fit\nthe model by optimizing a surrogate metric learning objective over a large\ncorpus of unlabeled social media content. Once learned, the mapping may be\napplied to users not seen at training time and enables efficient comparisons of\nusers in the resulting vector space. We present a comprehensive evaluation to\nvalidate the benefits of the proposed approach using data from Reddit, Twitter,\nand Wikipedia.", "published": "2019-10-11 05:37:11", "link": "http://arxiv.org/abs/1910.04979v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Query-by-example on-device keyword spotting", "abstract": "A keyword spotting (KWS) system determines the existence of, usually\npredefined, keyword in a continuous speech stream. This paper presents a\nquery-by-example on-device KWS system which is user-specific. The proposed\nsystem consists of two main steps: query enrollment and testing. In query\nenrollment step, phonetic posteriors are output by a small-footprint automatic\nspeech recognition model based on connectionist temporal classification. Using\nthe phonetic-level posteriorgram, hypothesis graph of finite-state transducer\n(FST) is built, thus can enroll any keywords thus avoiding an out-of-vocabulary\nproblem. In testing, a log-likelihood is scored for input audio using the FST.\nWe propose a threshold prediction method while using the user-specific keyword\nhypothesis only. The system generates query-specific negatives by rearranging\neach query utterance in waveform. The threshold is decided based on the\nenrollment queries and generated negatives. We tested two keywords in English,\nand the proposed work shows promising performance while preserving simplicity.", "published": "2019-10-11 13:28:03", "link": "http://arxiv.org/abs/1910.05171v3", "categories": ["cs.LG", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Emergence of Compositional Languages for Numeric Concepts Through\n  Iterated Learning in Neural Agents", "abstract": "Since first introduced, computer simulation has been an increasingly\nimportant tool in evolutionary linguistics. Recently, with the development of\ndeep learning techniques, research in grounded language learning has also\nstarted to focus on facilitating the emergence of compositional languages\nwithout pre-defined elementary linguistic knowledge. In this work, we explore\nthe emergence of compositional languages for numeric concepts in multi-agent\ncommunication systems. We demonstrate that compositional language for encoding\nnumeric concepts can emerge through iterated learning in populations of deep\nneural network agents. However, language properties greatly depend on the input\nrepresentations given to agents. We found that compositional languages only\nemerge if they require less iterations to be fully learnt than other\nnon-degenerate languages for agents on a given input representation.", "published": "2019-10-11 16:34:01", "link": "http://arxiv.org/abs/1910.05291v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box\n  Attacks on Speech Recognition and Voice Identification Systems", "abstract": "Automatic speech recognition and voice identification systems are being\ndeployed in a wide array of applications, from providing control mechanisms to\ndevices lacking traditional interfaces, to the automatic transcription of\nconversations and authentication of users. Many of these applications have\nsignificant security and privacy considerations. We develop attacks that force\nmistranscription and misidentification in state of the art systems, with\nminimal impact on human comprehension. Processing pipelines for modern systems\nare comprised of signal preprocessing and feature extraction steps, whose\noutput is fed to a machine-learned model. Prior work has focused on the models,\nusing white-box knowledge to tailor model-specific attacks. We focus on the\npipeline stages before the models, which (unlike the models) are quite similar\nacross systems. As such, our attacks are black-box and transferable, and\ndemonstrably achieve mistranscription and misidentification rates as high as\n100% by modifying only a few frames of audio. We perform a study via Amazon\nMechanical Turk demonstrating that there is no statistically significant\ndifference between human perception of regular and perturbed audio. Our\nfindings suggest that models may learn aspects of speech that are generally not\nperceived by human subjects, but that are crucial for model accuracy. We also\nfind that certain English language phonemes (in particular, vowels) are\nsignificantly more susceptible to our attack. We show that the attacks are\neffective when mounted over cellular networks, where signals are subject to\ndegradation due to transcoding, jitter, and packet loss.", "published": "2019-10-11 15:54:40", "link": "http://arxiv.org/abs/1910.05262v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
