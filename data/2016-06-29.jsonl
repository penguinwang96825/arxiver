{"title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "abstract": "We present a transition-based parser that jointly produces syntactic and\nsemantic dependencies. It learns a representation of the entire algorithm\nstate, using stack long short-term memories. Our greedy inference algorithm has\nlinear time, including feature extraction. On the CoNLL 2008--9 English shared\ntasks, we obtain the best published parsing performance among models that\njointly learn syntax and semantics.", "published": "2016-06-29 05:01:56", "link": "http://arxiv.org/abs/1606.08954v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Distributional Semantics Approach to Implicit Language Learning", "abstract": "In the present paper we show that distributional information is particularly\nimportant when considering concept availability under implicit language\nlearning conditions. Based on results from different behavioural experiments we\nargue that the implicit learnability of semantic regularities depends on the\ndegree to which the relevant concept is reflected in language use. In our\nsimulations, we train a Vector-Space model on either an English or a Chinese\ncorpus and then feed the resulting representations to a feed-forward neural\nnetwork. The task of the neural network was to find a mapping between the word\nrepresentations and the novel words. Using datasets from four behavioural\nexperiments, which used different semantic manipulations, we were able to\nobtain learning patterns very similar to those obtained by humans.", "published": "2016-06-29 12:08:51", "link": "http://arxiv.org/abs/1606.09058v1", "categories": ["cs.CL", "cs.LG", "I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "The rotating normal form of braids is regular", "abstract": "Defined on Birman-Ko-Lee monoids, the rotating normal form has strong\nconnections with the Dehornoy's braid ordering. It can be seen as a process for\nselecting between all the representative words of a Birman-Ko-Lee braid a\nparticular one, called rotating word. In this paper we construct, for all n 2,\na finite-state automaton which recognizes rotating words on n strands, proving\nthat the rotating normal form is regular. As a consequence we obtain the\nregularity of a $\\sigma$-definite normal form defined on the whole braid group.", "published": "2016-06-29 06:37:09", "link": "http://arxiv.org/abs/1606.08970v4", "categories": ["math.GR", "cs.CL", "cs.FL"], "primary_category": "math.GR"}
{"title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme\n  Recognition for Low Latency Processing", "abstract": "We present a systematic analysis on the performance of a phonetic recogniser\nwhen the window of input features is not symmetric with respect to the current\nframe. The recogniser is based on Context Dependent Deep Neural Networks\n(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the\nlatency of the system by reducing the number of future feature frames required\nto estimate the current output. Our tests performed on the TIMIT database show\nthat the performance does not degrade when the input window is shifted up to 5\nframes in the past compared to common practice (no future frame). This\ncorresponds to improving the latency by 50 ms in our settings. Our tests also\nshow that the best results are not obtained with the symmetric window commonly\nemployed, but with an asymmetric window with eight past and two future context\nframes, although this observation should be confirmed on other data sets. The\nreduction in latency suggested by our results is critical for specific\napplications such as real-time lip synchronisation for tele-presence, but may\nalso be beneficial in general applications to improve the lag in human-machine\nspoken interaction.", "published": "2016-06-29 15:51:44", "link": "http://arxiv.org/abs/1606.09163v1", "categories": ["cs.CL", "cs.CV", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Penambahan emosi menggunakan metode manipulasi prosodi untuk sistem text\n  to speech bahasa Indonesia", "abstract": "Adding an emotions using prosody manipulation method for Indonesian text to\nspeech system. Text To Speech (TTS) is a system that can convert text in one\nlanguage into speech, accordance with the reading of the text in the language\nused. The focus of this research is a natural sounding concept, the make\n\"humanize\" for the pronunciation of voice synthesis system Text To Speech.\nHumans have emotions / intonation that may affect the sound produced. The main\nrequirement for the system used Text To Speech in this research is eSpeak, the\ndatabase MBROLA using id1, Human Speech Corpus database from a website that\nsummarizes the words with the highest frequency (Most Common Words) used in a\ncountry. And there are 3 types of emotional / intonation designed base. There\nis a happy, angry and sad emotion. Method for develop the emotional filter is\nmanipulate the relevant features of prosody (especially pitch and duration\nvalue) using a predetermined rate factor that has been established by analyzing\nthe differences between the standard output Text To Speech and voice recording\nwith emotional prosody / a particular intonation. The test results for the\nperception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for\nangry emotion and 98.75 % for sad emotions. For perception test system carried\nby intelligibility and naturalness test. Intelligibility test for the accuracy\nof sound with the original sentence is 93.3%, and for clarity rate for each\nsentence is 62.8%. For naturalness, accuracy emotional election amounted to\n75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions.\n  -----\n  Text To Speech (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam\nformat suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa\nyang digunakan.", "published": "2016-06-29 19:06:48", "link": "http://arxiv.org/abs/1606.09222v1", "categories": ["cs.SD", "cs.CL", "cs.RO", "I.2.7; H.5.2"], "primary_category": "cs.SD"}
{"title": "Learning Concept Taxonomies from Multi-modal Data", "abstract": "We study the problem of automatically building hypernym taxonomies from\ntextual and visual data. Previous works in taxonomy induction generally ignore\nthe increasingly prominent visual data, which encode important perceptual\nsemantics. Instead, we propose a probabilistic model for taxonomy induction by\njointly leveraging text and images. To avoid hand-crafted feature engineering,\nwe design end-to-end features based on distributed representations of images\nand words. The model is discriminatively trained given a small set of existing\nontologies and is capable of building full taxonomies from scratch for a\ncollection of unseen conceptual label items with associated images. We evaluate\nour model and features on the WordNet hierarchies, where our system outperforms\nprevious approaches by a large gap.", "published": "2016-06-29 19:52:53", "link": "http://arxiv.org/abs/1606.09239v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compression of Neural Machine Translation Models via Pruning", "abstract": "Neural Machine Translation (NMT), like many other deep learning domains,\ntypically suffers from over-parameterization, resulting in large storage sizes.\nThis paper examines three simple magnitude-based pruning schemes to compress\nNMT models, namely class-blind, class-uniform, and class-distribution, which\ndiffer in terms of how pruning thresholds are computed for the different\nclasses of weights in the NMT architecture. We demonstrate the efficacy of\nweight pruning as a compression technique for a state-of-the-art NMT system. We\nshow that an NMT model with over 200 million parameters can be pruned by 40%\nwith very little performance loss as measured on the WMT'14 English-German\ntranslation task. This sheds light on the distribution of redundancy in the NMT\narchitecture. Our main result is that with retraining, we can recover and even\nsurpass the original performance with an 80%-pruned model.", "published": "2016-06-29 20:36:23", "link": "http://arxiv.org/abs/1606.09274v1", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
