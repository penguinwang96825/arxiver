{"title": "Improving Entity Linking through Semantic Reinforced Entity Embeddings", "abstract": "Entity embeddings, which represent different aspects of each entity with a\nsingle vector like word embeddings, are a key component of neural entity\nlinking models. Existing entity embeddings are learned from canonical Wikipedia\narticles and local contexts surrounding target entities. Such entity embeddings\nare effective, but too distinctive for linking models to learn contextual\ncommonality. We propose a simple yet effective method, FGS2EE, to inject\nfine-grained semantic information into entity embeddings to reduce the\ndistinctiveness and facilitate the learning of contextual commonality. FGS2EE\nfirst uses the embeddings of semantic type words to generate semantic\nembeddings, and then combines them with existing entity embeddings through\nlinear aggregation. Extensive experiments show the effectiveness of such\nembeddings. Based on our entity embeddings, we achieved new sate-of-the-art\nperformance on entity linking.", "published": "2021-06-16 00:27:56", "link": "http://arxiv.org/abs/2106.08495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alternated Training with Synthetic and Authentic Data for Neural Machine\n  Translation", "abstract": "While synthetic bilingual corpora have demonstrated their effectiveness in\nlow-resource neural machine translation (NMT), adding more synthetic data often\ndeteriorates translation performance. In this work, we propose alternated\ntraining with synthetic and authentic data for NMT. The basic idea is to\nalternate synthetic and authentic corpora iteratively during training. Compared\nwith previous work, we introduce authentic data as guidance to prevent the\ntraining of NMT models from being disturbed by noisy synthetic data.\nExperiments on Chinese-English and German-English translation tasks show that\nour approach improves the performance over several strong baselines. We\nvisualize the BLEU landscape to further investigate the role of authentic and\nsynthetic data during alternated training. From the visualization, we find that\nauthentic data helps to direct the NMT model parameters towards points with\nhigher BLEU scores and leads to consistent translation performance improvement.", "published": "2021-06-16 07:13:16", "link": "http://arxiv.org/abs/2106.08582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Discourse to Narrative: Knowledge Projection for Event Relation\n  Extraction", "abstract": "Current event-centric knowledge graphs highly rely on explicit connectives to\nmine relations between events. Unfortunately, due to the sparsity of\nconnectives, these methods severely undermine the coverage of EventKGs. The\nlack of high-quality labelled corpora further exacerbates that problem. In this\npaper, we propose a knowledge projection paradigm for event relation\nextraction: projecting discourse knowledge to narratives by exploiting the\ncommonalities between them. Specifically, we propose Multi-tier Knowledge\nProjection Network (MKPNet), which can leverage multi-tier discourse knowledge\neffectively for event relation extraction. In this way, the labelled data\nrequirement is significantly reduced, and implicit event relations can be\neffectively extracted. Intrinsic experimental results show that MKPNet achieves\nthe new state-of-the-art performance, and extrinsic experimental results verify\nthe value of the extracted event relations.", "published": "2021-06-16 08:35:29", "link": "http://arxiv.org/abs/2106.08629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic sentence similarity: size does not always matter", "abstract": "This study addresses the question whether visually grounded speech\nrecognition (VGS) models learn to capture sentence semantics without access to\nany prior linguistic knowledge. We produce synthetic and natural spoken\nversions of a well known semantic textual similarity database and show that our\nVGS model produces embeddings that correlate well with human semantic\nsimilarity judgements. Our results show that a model trained on a small\nimage-caption database outperforms two models trained on much larger databases,\nindicating that database size is not all that matters. We also investigate the\nimportance of having multiple captions per image and find that this is indeed\nhelpful even if the total number of images is lower, suggesting that\nparaphrasing is a valuable learning signal. While the general trend in the\nfield is to create ever larger datasets to train models on, our findings\nindicate other characteristics of the database can just as important important.", "published": "2021-06-16 09:22:58", "link": "http://arxiv.org/abs/2106.08648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eider: Empowering Document-level Relation Extraction with Efficient\n  Evidence Extraction and Inference-stage Fusion", "abstract": "Document-level relation extraction (DocRE) aims to extract semantic relations\namong entity pairs in a document. Typical DocRE methods blindly take the full\ndocument as input, while a subset of the sentences in the document, noted as\nthe evidence, are often sufficient for humans to predict the relation of an\nentity pair. In this paper, we propose an evidence-enhanced framework, Eider,\nthat empowers DocRE by efficiently extracting evidence and effectively fusing\nthe extracted evidence in inference. We first jointly train an RE model with a\nlightweight evidence extraction model, which is efficient in both memory and\nruntime. Empirically, even training the evidence model on silver labels\nconstructed by our heuristic rules can lead to better RE performance. We\nfurther design a simple yet effective inference process that makes RE\npredictions on both extracted evidence and the full document, then fuses the\npredictions through a blending layer. This allows Eider to focus on important\nsentences while still having access to the complete information in the\ndocument. Extensive experiments show that Eider outperforms state-of-the-art\nmethods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).", "published": "2021-06-16 09:43:16", "link": "http://arxiv.org/abs/2106.08657v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the proper role of linguistically-oriented deep net analysis in\n  linguistic theorizing", "abstract": "A lively research field has recently emerged that uses experimental methods\nto probe the linguistic behavior of modern deep networks. While work in this\ntradition often reports intriguing results about the grammatical skills of deep\nnets, it is not clear what their implications for linguistic theorizing should\nbe. As a consequence, linguistically-oriented deep net analysis has had very\nlittle impact on linguistics at large. In this chapter, I suggest that deep\nnetworks should be treated as theories making explicit predictions about the\nacceptability of linguistic utterances. I argue that, if we overcome some\nobstacles standing in the way of seriously pursuing this idea, we will gain a\npowerful new theoretical tool, complementary to mainstream algebraic\napproaches.", "published": "2021-06-16 10:57:24", "link": "http://arxiv.org/abs/2106.08694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State\n  Tracking", "abstract": "Dialogue State Tracking (DST), which is the process of inferring user goals\nby estimating belief states given the dialogue history, plays a critical role\nin task-oriented dialogue systems. A coreference phenomenon observed in\nmulti-turn conversations is not addressed by existing DST models, leading to\nsub-optimal performances. In this paper, we propose Coreference Dialogue State\nTracker (CDST) that explicitly models the coreference feature. In particular,\nat each turn, the proposed model jointly predicts the coreferred domain-slot\npair and extracts the coreference values from the dialogue context.\nExperimental results on MultiWOZ 2.1 dataset show that the proposed model\nachieves the state-of-the-art joint goal accuracy of 56.47%.", "published": "2021-06-16 11:47:29", "link": "http://arxiv.org/abs/2106.08723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-independent User Simulation with Transformers for Task-oriented\n  Dialogue Systems", "abstract": "Dialogue policy optimisation via reinforcement learning requires a large\nnumber of training interactions, which makes learning with real users time\nconsuming and expensive. Many set-ups therefore rely on a user simulator\ninstead of humans. These user simulators have their own problems. While\nhand-coded, rule-based user simulators have been shown to be sufficient in\nsmall, simple domains, for complex domains the number of rules quickly becomes\nintractable. State-of-the-art data-driven user simulators, on the other hand,\nare still domain-dependent. This means that adaptation to each new domain\nrequires redesigning and retraining. In this work, we propose a\ndomain-independent transformer-based user simulator (TUS). The structure of our\nTUS is not tied to a specific domain, enabling domain generalisation and\nlearning of cross-domain user behaviour from data. We compare TUS with the\nstate of the art using automatic as well as human evaluations. TUS can compete\nwith rule-based user simulators on pre-defined domains and is able to\ngeneralise to unseen domains in a zero-shot fashion.", "published": "2021-06-16 14:56:04", "link": "http://arxiv.org/abs/2106.08838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine\n  Translation", "abstract": "Policy gradient algorithms have found wide adoption in NLP, but have recently\nbecome subject to criticism, doubting their suitability for NMT. Choshen et al.\n(2020) identify multiple weaknesses and suspect that their success is\ndetermined by the shape of output distributions rather than the reward. In this\npaper, we revisit these claims and study them under a wider range of\nconfigurations. Our experiments on in-domain and cross-domain adaptation reveal\nthe importance of exploration and reward scaling, and provide empirical\ncounter-evidence to these claims.", "published": "2021-06-16 16:54:49", "link": "http://arxiv.org/abs/2106.08942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specializing Multilingual Language Models: An Empirical Study", "abstract": "Pretrained multilingual language models have become a common tool in\ntransferring NLP capabilities to low-resource languages, often with\nadaptations. In this work, we study the performance, extensibility, and\ninteraction of two such adaptations: vocabulary augmentation and script\ntransliteration. Our evaluations on part-of-speech tagging, universal\ndependency parsing, and named entity recognition in nine diverse low-resource\nlanguages uphold the viability of these approaches while raising new questions\naround how to optimally adapt multilingual models to low-resource settings.", "published": "2021-06-16 18:13:55", "link": "http://arxiv.org/abs/2106.09063v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference-Aware Dialogue Summarization", "abstract": "Summarizing conversations via neural approaches has been gaining research\ntraction lately, yet it is still challenging to obtain practical solutions.\nExamples of such challenges include unstructured information exchange in\ndialogues, informal interactions between speakers, and dynamic role changes of\nspeakers as the dialogue evolves. Many of such challenges result in complex\ncoreference links. Therefore, in this work, we investigate different approaches\nto explicitly incorporate coreference information in neural abstractive\ndialogue summarization models to tackle the aforementioned challenges.\nExperimental results show that the proposed approaches achieve state-of-the-art\nperformance, implying it is useful to utilize coreference information in\ndialogue summarization. Evaluation results on factual correctness suggest such\ncoreference-aware models are better at tracing the information flow among\ninterlocutors and associating accurate status/actions with the corresponding\ninterlocutors and person mentions.", "published": "2021-06-16 05:18:50", "link": "http://arxiv.org/abs/2106.08556v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discrete Auto-regressive Variational Attention Models for Text Modeling", "abstract": "Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts.", "published": "2021-06-16 06:36:26", "link": "http://arxiv.org/abs/2106.08571v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative\n  Training", "abstract": "Out-of-scope intent detection is of practical importance in task-oriented\ndialogue systems. Since the distribution of outlier utterances is arbitrary and\nunknown in the training stage, existing methods commonly rely on strong\nassumptions on data distribution such as mixture of Gaussians to make\ninference, resulting in either complex multi-step training procedures or\nhand-crafted rules such as confidence threshold selection for outlier\ndetection. In this paper, we propose a simple yet effective method to train an\nout-of-scope intent classifier in a fully end-to-end manner by simulating the\ntest scenario in training, which requires no assumption on data distribution\nand no additional post-processing or threshold setting. Specifically, we\nconstruct a set of pseudo outliers in the training stage, by generating\nsynthetic outliers using inliner features via self-supervision and sampling\nout-of-scope sentences from easily available open-domain datasets. The pseudo\noutliers are used to train a discriminative classifier that can be directly\napplied to and generalize well on the test task. We evaluate our method\nextensively on four benchmark dialogue datasets and observe significant\nimprovements over state-of-the-art approaches. Our code has been released at\nhttps://github.com/liam0949/DCLOOS.", "published": "2021-06-16 08:17:18", "link": "http://arxiv.org/abs/2106.08616v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEOVER: Sentence-level Emotion Orientation Vector based Conversation\n  Emotion Recognition Model", "abstract": "For the task of conversation emotion recognition, recent works focus on\nspeaker relationship modeling but ignore the role of utterance's emotional\ntendency.In this paper, we propose a new expression paradigm of sentence-level\nemotion orientation vector to model the potential correlation of emotions\nbetween sentence vectors. Based on it, we design an emotion recognition model,\nwhich extracts the sentence-level emotion orientation vectors from the language\nmodel and jointly learns from the dialogue sentiment analysis model and\nextracted sentence-level emotion orientation vectors to identify the speaker's\nemotional orientation during the conversation. We conduct experiments on two\nbenchmark datasets and compare them with the five baseline models.The\nexperimental results show that our model has better performance on all data\nsets.", "published": "2021-06-16 13:44:03", "link": "http://arxiv.org/abs/2106.08785v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the long-term learning ability of LSTM LMs", "abstract": "We inspect the long-term learning ability of Long Short-Term Memory language\nmodels (LSTM LMs) by evaluating a contextual extension based on the Continuous\nBag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and\nby analyzing its performance. We evaluate on text and speech. Sentence-level\nmodels using the long-term contextual module perform comparably to vanilla\ndiscourse-level LSTM LMs. On the other hand, the extension does not provide\ngains for discourse-level models. These findings indicate that discourse-level\nLSTM LMs already rely on contextual information to perform long-term learning.", "published": "2021-06-16 16:34:37", "link": "http://arxiv.org/abs/2106.08927v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Disentangling Online Chats with DAG-Structured LSTMs", "abstract": "Many modern messaging systems allow fast and synchronous textual\ncommunication among many users. The resulting sequence of messages hides a more\ncomplicated structure in which independent sub-conversations are interwoven\nwith one another. This poses a challenge for any task aiming to understand the\ncontent of the chat logs or gather information from them. The ability to\ndisentangle these conversations is then tantamount to the success of many\ndownstream tasks such as summarization and question answering. Structured\ninformation accompanying the text such as user turn, user mentions, timestamps,\nis used as a cue by the participants themselves who need to follow the\nconversation and has been shown to be important for disentanglement. DAG-LSTMs,\na generalization of Tree-LSTMs that can handle directed acyclic dependencies,\nare a natural way to incorporate such information and its non-sequential\nnature. In this paper, we apply DAG-LSTMs to the conversation disentanglement\ntask. We perform our experiments on the Ubuntu IRC dataset. We show that the\nnovel model we propose achieves state of the art status on the task of\nrecovering reply-to relations and it is competitive on other disentanglement\nmetrics.", "published": "2021-06-16 18:00:00", "link": "http://arxiv.org/abs/2106.09024v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Construction of Evaluation Suites for Natural Language\n  Generation Datasets", "abstract": "Machine learning approaches applied to NLP are often evaluated by summarizing\ntheir performance in a single number, for example accuracy. Since most test\nsets are constructed as an i.i.d. sample from the overall data, this approach\noverly simplifies the complexity of language and encourages overfitting to the\nhead of the data distribution. As such, rare language phenomena or text about\nunderrepresented groups are not equally included in the evaluation. To\nencourage more in-depth model analyses, researchers have proposed the use of\nmultiple test sets, also called challenge sets, that assess specific\ncapabilities of a model. In this paper, we develop a framework based on this\nidea which is able to generate controlled perturbations and identify subsets in\ntext-to-scalar, text-to-text, or data-to-text settings. By applying this\nframework to the GEM generation benchmark, we propose an evaluation suite made\nof 80 challenge sets, demonstrate the kinds of analyses that it enables and\nshed light onto the limits of current generation models.", "published": "2021-06-16 18:20:58", "link": "http://arxiv.org/abs/2106.09069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Image-Language Transformers for Verb Understanding", "abstract": "Multimodal image-language transformers have achieved impressive results on a\nvariety of tasks that rely on fine-tuning (e.g., visual question answering and\nimage retrieval). We are interested in shedding light on the quality of their\npretrained representations -- in particular, if these models can distinguish\ndifferent types of verbs or if they rely solely on nouns in a given sentence.\nTo do so, we collect a dataset of image-sentence pairs (in English) consisting\nof 421 verbs that are either visual or commonly found in the pretraining data\n(i.e., the Conceptual Captions dataset). We use this dataset to evaluate\npretrained image-language transformers and find that they fail more in\nsituations that require verb understanding compared to other parts of speech.\nWe also investigate what category of verbs are particularly challenging.", "published": "2021-06-16 21:36:36", "link": "http://arxiv.org/abs/2106.09141v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Sentiment Progression based Searching and Indexing of Literary Textual\n  Artefacts", "abstract": "Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nhttp://www.cs.cmu.edu/~dbamman/booksummaries.html with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.", "published": "2021-06-16 20:49:51", "link": "http://arxiv.org/abs/2106.13767v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Topic Classification on Spoken Documents Using Deep Acoustic and\n  Linguistic Features", "abstract": "Topic classification systems on spoken documents usually consist of two\nmodules: an automatic speech recognition (ASR) module to convert speech into\ntext and a text topic classification (TTC) module to predict the topic class\nfrom the decoded text. In this paper, instead of using the ASR transcripts, the\nfusion of deep acoustic and linguistic features is used for topic\nclassification on spoken documents. More specifically, a conventional CTC-based\nacoustic model (AM) using phonemes as output units is first trained, and the\noutputs of the layer before the linear phoneme classifier in the trained AM are\nused as the deep acoustic features of spoken documents. Furthermore, these deep\nacoustic features are fed to a phoneme-to-word (P2W) module to obtain deep\nlinguistic features. Finally, a local multi-head attention module is proposed\nto fuse these two types of deep features for topic classification. Experiments\nconducted on a subset selected from Switchboard corpus show that our proposed\nframework outperforms the conventional ASR+TTC systems and achieves a 3.13%\nimprovement in ACC.", "published": "2021-06-16 08:54:31", "link": "http://arxiv.org/abs/2106.08637v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving the expressiveness of neural vocoding with non-affine\n  Normalizing Flows", "abstract": "This paper proposes a general enhancement to the Normalizing Flows (NF) used\nin neural vocoding. As a case study, we improve expressive speech vocoding with\na revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine\ntransformation of PW to the more expressive invertible non-affine function. The\ngreater expressiveness of the improved PW leads to better-perceived signal\nquality and naturalness in the waveform reconstruction and text-to-speech (TTS)\ntasks. We evaluate the model across different speaking styles on a\nmulti-speaker, multi-lingual dataset. In the waveform reconstruction task, the\nproposed model closes the naturalness and signal quality gap from the original\nPW to recordings by $10\\%$, and from other state-of-the-art neural vocoding\nsystems by more than $60\\%$. We also demonstrate improvements in objective\nmetrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy\nreduced by $3\\%$ and $6\\unicode{x2030}$ comparing to the affine PW.\nFurthermore, we extend the probability density distillation procedure proposed\nby the original PW paper, so that it works with any non-affine invertible and\ndifferentiable function.", "published": "2021-06-16 09:25:18", "link": "http://arxiv.org/abs/2106.08649v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluating Gender Bias in Hindi-English Machine Translation", "abstract": "With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.", "published": "2021-06-16 10:35:51", "link": "http://arxiv.org/abs/2106.08680v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An\n  Empirical Study", "abstract": "Several variants of deep neural networks have been successfully employed for\nbuilding parametric models that project variable-duration spoken word segments\nonto fixed-size vector representations, or acoustic word embeddings (AWEs).\nHowever, it remains unclear to what degree we can rely on the distance in the\nemerging AWE space as an estimate of word-form similarity. In this paper, we\nask: does the distance in the acoustic embedding space correlate with\nphonological dissimilarity? To answer this question, we empirically investigate\nthe performance of supervised approaches for AWEs with different neural\narchitectures and learning objectives. We train AWE models in controlled\nsettings for two languages (German and Czech) and evaluate the embeddings on\ntwo tasks: word discrimination and phonological similarity. Our experiments\nshow that (1) the distance in the embedding space in the best cases only\nmoderately correlates with phonological distance, and (2) improving the\nperformance on the word discrimination task does not necessarily yield models\nthat better reflect word phonological similarity. Our findings highlight the\nnecessity to rethink the current intrinsic evaluations for AWEs.", "published": "2021-06-16 10:47:56", "link": "http://arxiv.org/abs/2106.08686v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Alzheimer's Disease Detection from Spontaneous Speech through Combining\n  Linguistic Complexity and (Dis)Fluency Features with Pretrained Language\n  Models", "abstract": "In this paper, we combined linguistic complexity and (dis)fluency features\nwith pretrained language models for the task of Alzheimer's disease detection\nof the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous\nSpeech) challenge. An accuracy of 83.1% was achieved on the test set, which\namounts to an improvement of 4.23% over the baseline model. Our best-performing\nmodel that integrated component models using a stacking ensemble technique\nperformed equally well on cross-validation and test data, indicating that it is\nrobust against overfitting.", "published": "2021-06-16 10:50:18", "link": "http://arxiv.org/abs/2106.08689v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based\n  Knowledge Graph Alignment System", "abstract": "Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces. The demo is available at\nhttps://prasemap.qizhy.com.", "published": "2021-06-16 14:06:09", "link": "http://arxiv.org/abs/2106.08801v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment\n  Analysis Methods", "abstract": "Opinion and sentiment analysis is a vital task to characterize subjective\ninformation in social media posts. In this paper, we present a comprehensive\nexperimental evaluation and comparison with six state-of-the-art methods, from\nwhich we have re-implemented one of them. In addition, we investigate different\ntextual and visual feature embeddings that cover different aspects of the\ncontent, as well as the recently introduced multimodal CLIP embeddings.\nExperimental results are presented for two different publicly available\nbenchmark datasets of tweets and corresponding images. In contrast to the\nevaluation methodology of previous work, we introduce a reproducible and fair\nevaluation scheme to make results comparable. Finally, we conduct an error\nanalysis to outline the limitations of the methods and possibilities for the\nfuture work.", "published": "2021-06-16 14:44:48", "link": "http://arxiv.org/abs/2106.08829v1", "categories": ["cs.SI", "cs.CL", "cs.CV"], "primary_category": "cs.SI"}
{"title": "Grounding Spatio-Temporal Language with Transformers", "abstract": "Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.", "published": "2021-06-16 15:28:22", "link": "http://arxiv.org/abs/2106.08858v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Attention-Based Keyword Localisation in Speech using Visual Grounding", "abstract": "Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it is possible\nto train a model that can detect whether a particular text keyword occurs in\nspeech utterances or not. Here we investigate whether visually grounded speech\nmodels can also do keyword localisation: predicting where, within an utterance,\na given textual keyword occurs without any explicit text-based or alignment\nsupervision. We specifically consider whether incorporating attention into a\nconvolutional model is beneficial for localisation. Although absolute\nlocalisation performance with visually supervised models is still modest\n(compared to using unordered bag-of-word text labels for supervision), we show\nthat attention provides a large gain in performance over previous visually\ngrounded models. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating the\nword 'backstroke' for the query keyword 'swimming'.", "published": "2021-06-16 15:29:11", "link": "http://arxiv.org/abs/2106.08859v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "$C^3$: Compositional Counterfactual Contrastive Learning for\n  Video-grounded Dialogues", "abstract": "Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.", "published": "2021-06-16 16:05:27", "link": "http://arxiv.org/abs/2106.08914v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Collaborative Training of Acoustic Encoders for Speech Recognition", "abstract": "On-device speech recognition requires training models of different sizes for\ndeploying on devices with various computational budgets. When building such\ndifferent models, we can benefit from training them jointly to take advantage\nof the knowledge shared between them. Joint training is also efficient since it\nreduces the redundancy in the training procedure's data handling operations. We\npropose a method for collaboratively training acoustic encoders of different\nsizes for speech recognition. We use a sequence transducer setup where\ndifferent acoustic encoders share a common predictor and joiner modules. The\nacoustic encoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer loss. We\nperform experiments using the LibriSpeech corpus and demonstrate that the\ncollaboratively trained acoustic encoders can provide up to a 11% relative\nimprovement in the word error rate on both the test partitions.", "published": "2021-06-16 17:05:47", "link": "http://arxiv.org/abs/2106.08960v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly\n  Labeled Data", "abstract": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.", "published": "2021-06-16 17:18:14", "link": "http://arxiv.org/abs/2106.08977v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Spoken Language Understanding for Generalized Voice\n  Assistants", "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous work in\nthis area has focused on targeted tasks in fixed domains, where the output\nsemantic structure is assumed a priori and the input speech is of limited\ncomplexity. In this work we present our approach to developing an E2E model for\ngeneralized SLU in commercial voice assistants (VAs). We propose a fully\ndifferentiable, transformer-based, hierarchical system that can be pretrained\nat both the ASR and NLU levels. This is then fine-tuned on both transcription\nand semantic classification losses to handle a diverse set of intent and\nargument combinations. This leads to an SLU system that achieves significant\nimprovements over baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark on\nthe popular Fluent Speech Commands dataset. We further evaluate our model on a\nhard test set, exclusively containing slot arguments unseen in training, and\ndemonstrate a nearly 20% improvement, showing the efficacy of our approach in\ntruly demanding VA scenarios.", "published": "2021-06-16 17:56:47", "link": "http://arxiv.org/abs/2106.09009v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to\n  Improve Task-oriented Conversational Modeling", "abstract": "Most prior work on task-oriented dialogue systems are restricted to limited\ncoverage of domain APIs. However, users oftentimes have requests that are out\nof the scope of these APIs. This work focuses on responding to these\nbeyond-API-coverage user turns by incorporating external, unstructured\nknowledge sources. Our approach works in a pipelined manner with\nknowledge-seeking turn detection, knowledge selection, and response generation\nin sequence. We introduce novel data augmentation methods for the first two\nsteps and demonstrate that the use of information extracted from dialogue\ncontext improves the knowledge selection and end-to-end performances. Through\nexperiments, we achieve state-of-the-art performance for both automatic and\nhuman evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the\neffectiveness of our contributions.", "published": "2021-06-16 23:31:42", "link": "http://arxiv.org/abs/2106.09174v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Algorithm to Compilation Co-design: An Integrated View of Neural Network\n  Sparsity", "abstract": "Reducing computation cost, inference latency, and memory footprint of neural\nnetworks are frequently cited as research motivations for pruning and sparsity.\nHowever, operationalizing those benefits and understanding the end-to-end\neffect of algorithm design and regularization on the runtime execution is not\noften examined in depth.\n  Here we apply structured and unstructured pruning to attention weights of\ntransformer blocks of the BERT language model, while also expanding block\nsparse representation (BSR) operations in the TVM compiler. Integration of BSR\noperations enables the TVM runtime execution to leverage structured pattern\nsparsity induced by model regularization.\n  This integrated view of pruning algorithms enables us to study relationships\nbetween modeling decisions and their direct impact on sparsity-enhanced\nexecution. Our main findings are: 1) we validate that performance benefits of\nstructured sparsity block regularization must be enabled by the BSR\naugmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x\nspeedup relative to standard TVM compilation (without expanded BSR support). 2)\nfor BERT attention weights, the end-to-end optimal block sparsity shape in this\nCPU inference context is not a square block (as in \\cite{gray2017gpu}) but\nrather a linear 32x1 block 3) the relationship between performance and block\nsize / shape is is suggestive of how model regularization parameters interact\nwith task scheduler optimizations resulting in the observed end-to-end\nperformance.", "published": "2021-06-16 15:13:26", "link": "http://arxiv.org/abs/2106.08846v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "WSRGlow: A Glow-based Waveform Generative Model for Audio\n  Super-Resolution", "abstract": "Audio super-resolution is the task of constructing a high-resolution (HR)\naudio from a low-resolution (LR) audio by adding the missing band. Previous\nmethods based on convolutional neural networks and mean squared error training\nobjective have relatively low performance, while adversarial generative models\nare difficult to train and tune. Recently, normalizing flow has attracted a lot\nof attention for its high performance, simple training and fast inference. In\nthis paper, we propose WSRGlow, a Glow-based waveform generative model to\nperform audio super-resolution. Specifically, 1) we integrate WaveNet and Glow\nto directly maximize the exact likelihood of the target HR audio conditioned on\nLR information; and 2) to exploit the audio information from low-resolution\naudio, we propose an LR audio encoder and an STFT encoder, which encode the LR\ninformation from the time domain and frequency domain respectively. The\nexperimental results show that the proposed model is easier to train and\noutperforms the previous works in terms of both objective and perceptual\nquality. WSRGlow is also the first model to produce 48kHz waveforms from 12kHz\nLR audio.", "published": "2021-06-16 01:37:34", "link": "http://arxiv.org/abs/2106.08507v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of Consonant Errors in Disordered Speech Based on\n  Consonant-vowel Segment Embedding", "abstract": "Speech sound disorder (SSD) refers to a type of developmental disorder in\nyoung children who encounter persistent difficulties in producing certain\nspeech sounds at the expected age. Consonant errors are the major indicator of\nSSD in clinical assessment. Previous studies on automatic assessment of SSD\nrevealed that detection of speech errors concerning short and transitory\nconsonants is less satisfactory. This paper investigates a neural network based\napproach to detecting consonant errors in disordered speech using\nconsonant-vowel (CV) diphone segment in comparison to using consonant monophone\nsegment. The underlying assumption is that the vowel part of a CV segment\ncarries important information of co-articulation from the consonant. Speech\nembeddings are extracted from CV segments by a recurrent neural network model.\nThe similarity scores between the embeddings of the test segment and the\nreference segments are computed to determine if the test segment is the\nexpected consonant or not. Experimental results show that using CV segments\nachieves improved performance on detecting speech errors concerning those\n\"difficult\" consonants reported in the previous studies.", "published": "2021-06-16 03:25:28", "link": "http://arxiv.org/abs/2106.08536v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and\n  Conditional Speaker Chain", "abstract": "Non-autoregressive (NAR) models have achieved a large inference computation\nreduction and comparable results with autoregressive (AR) models on various\nsequence to sequence tasks. However, there has been limited research aiming to\nexplore the NAR approaches on sequence to multi-sequence problems, like\nmulti-speaker automatic speech recognition (ASR). In this study, we extend our\nproposed conditional chain model to NAR multi-speaker ASR. Specifically, the\noutput of each speaker is inferred one-by-one using both the input mixture\nspeech and previously-estimated conditional speaker features. In each step, a\nNAR connectionist temporal classification (CTC) encoder is used to perform\nparallel computation. With this design, the total inference steps will be\nrestricted to the number of mixed speakers. Besides, we also adopt the\nConformer and incorporate an intermediate CTC loss to improve the performance.\nExperiments on WSJ0-Mix and LibriMix corpora show that our model outperforms\nother NAR models with only a slight increase of latency, achieving WERs of\n22.3% and 24.9%, respectively. Moreover, by including the data of variable\nnumbers of speakers, our model can even better than the PIT-Conformer AR model\nwith only 1/7 latency, obtaining WERs of 19.9% and 34.3% on WSJ0-2mix and\nWSJ0-3mix sets. All of our codes are publicly available at\nhttps://github.com/pengchengguo/espnet/tree/conditional-multispk.", "published": "2021-06-16 07:41:14", "link": "http://arxiv.org/abs/2106.08595v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DCCRN+: Channel-wise Subband DCCRN with SNR Estimation for Speech\n  Enhancement", "abstract": "Deep complex convolution recurrent network (DCCRN), which extends CRN with\ncomplex structure, has achieved superior performance in MOS evaluation in\nInterspeech 2020 deep noise suppression challenge (DNS2020). This paper further\nextends DCCRN with the following significant revisions. We first extend the\nmodel to sub-band processing where the bands are split and merged by learnable\nneural network filters instead of engineered FIR filters, leading to a faster\nnoise suppressor trained in an end-to-end manner. Then the LSTM is further\nsubstituted with a complex TF-LSTM to better model temporal dependencies along\nboth time and frequency axes. Moreover, instead of simply concatenating the\noutput of each encoder layer to the input of the corresponding decoder layer,\nwe use convolution blocks to first aggregate essential information from the\nencoder output before feeding it to the decoder layers. We specifically\nformulate the decoder with an extra a priori SNR estimation module to maintain\ngood speech quality while removing noise. Finally a post-processing module is\nadopted to further suppress the unnatural residual noise. The new model, named\nDCCRN+, has surpassed the original DCCRN as well as several competitive models\nin terms of PESQ and DNSMOS, and has achieved superior performance in the new\nInterspeech 2021 DNS challenge", "published": "2021-06-16 10:16:35", "link": "http://arxiv.org/abs/2106.08672v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enriching Source Style Transfer in Recognition-Synthesis based\n  Non-Parallel Voice Conversion", "abstract": "Current voice conversion (VC) methods can successfully convert timbre of the\naudio. As modeling source audio's prosody effectively is a challenging task,\nthere are still limitations of transferring source style to the converted\nspeech. This study proposes a source style transfer method based on\nrecognition-synthesis framework. Previously in speech generation task, prosody\ncan be modeled explicitly with prosodic features or implicitly with a latent\nprosody extractor. In this paper, taking advantages of both, we model the\nprosody in a hybrid manner, which effectively combines explicit and implicit\nmethods in a proposed prosody module. Specifically, prosodic features are used\nto explicit model prosody, while VAE and reference encoder are used to\nimplicitly model prosody, which take Mel spectrum and bottleneck feature as\ninput respectively. Furthermore, adversarial training is introduced to remove\nspeaker-related information from the VAE outputs, avoiding leaking source\nspeaker information while transferring style. Finally, we use a modified\nself-attention based encoder to extract sentential context from bottleneck\nfeatures, which also implicitly aggregates the prosodic aspects of source\nspeech from the layered representations. Experiments show that our approach is\nsuperior to the baseline and a competitive system in terms of style transfer;\nmeanwhile, the speech quality and speaker similarity are well maintained.", "published": "2021-06-16 12:34:47", "link": "http://arxiv.org/abs/2106.08741v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Flow-Based Neural Network for Time Domain Speech Enhancement", "abstract": "Speech enhancement involves the distinction of a target speech signal from an\nintrusive background. Although generative approaches using Variational\nAutoencoders or Generative Adversarial Networks (GANs) have increasingly been\nused in recent years, normalizing flow (NF) based systems are still scarse,\ndespite their success in related fields. Thus, in this paper we propose a NF\nframework to directly model the enhancement process by density estimation of\nclean speech utterances conditioned on their noisy counterpart. The WaveGlow\nmodel from speech synthesis is adapted to enable direct enhancement of noisy\nutterances in time domain. In addition, we demonstrate that nonlinear input\ncompanding benefits the model performance by equalizing the distribution of\ninput samples. Experimental evaluation on a publicly available dataset shows\ncomparable results to current state-of-the-art GAN-based approaches, while\nsurpassing the chosen baselines using objective evaluation metrics.", "published": "2021-06-16 17:56:40", "link": "http://arxiv.org/abs/2106.09008v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Hands-on Comparison of DNNs for Dialog Separation Using Transfer\n  Learning from Music Source Separation", "abstract": "This paper describes a hands-on comparison on using state-of-the-art music\nsource separation deep neural networks (DNNs) before and after task-specific\nfine-tuning for separating speech content from non-speech content in broadcast\naudio (i.e., dialog separation). The music separation models are selected as\nthey share the number of channels (2) and sampling rate (44.1 kHz or higher)\nwith the considered broadcast content, and vocals separation in music is\nconsidered as a parallel for dialog separation in the target application\ndomain. These similarities are assumed to enable transfer learning between the\ntasks. Three models pre-trained on music (Open-Unmix, Spleeter, and\nConv-TasNet) are considered in the experiments, and fine-tuned with real\nbroadcast data. The performance of the models is evaluated before and after\nfine-tuning with computational evaluation metrics (SI-SIRi, SI-SDRi, 2f-model),\nas well as with a listening test simulating an application where the non-speech\nsignal is partially attenuated, e.g., for better speech intelligibility. The\nevaluations include two reference systems specifically developed for dialog\nseparation. The results indicate that pre-trained music source separation\nmodels can be used for dialog separation to some degree, and that they benefit\nfrom the fine-tuning, reaching a performance close to task-specific solutions.", "published": "2021-06-16 19:35:34", "link": "http://arxiv.org/abs/2106.09093v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Global Rhythm Style Transfer Without Text Transcriptions", "abstract": "Prosody plays an important role in characterizing the style of a speaker or\nan emotion, but most non-parallel voice or emotion style transfer algorithms do\nnot convert any prosody information. Two major components of prosody are pitch\nand rhythm. Disentangling the prosody information, particularly the rhythm\ncomponent, from the speech is challenging because it involves breaking the\nsynchrony between the input speech and the disentangled speech representation.\nAs a result, most existing prosody style transfer algorithms would need to rely\non some form of text transcriptions to identify the content information, which\nconfines their application to high-resource languages only. Recently,\nSpeechSplit has made sizeable progress towards unsupervised prosody style\ntransfer, but it is unable to extract high-level global prosody style in an\nunsupervised manner. In this paper, we propose AutoPST, which can disentangle\nglobal prosody style from speech without relying on any text transcriptions.\nAutoPST is an Autoencoder-based Prosody Style Transfer framework with a\nthorough rhythm removal module guided by the self-expressive representation\nlearning. Experiments on different style transfer tasks show that AutoPST can\neffectively convert prosody that correctly reflects the styles of the target\ndomains.", "published": "2021-06-16 02:21:00", "link": "http://arxiv.org/abs/2106.08519v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Drum-Aware Ensemble Architecture for Improved Joint Musical Beat and\n  Downbeat Tracking", "abstract": "This paper presents a novel system architecture that integrates blind source\nseparation with joint beat and downbeat tracking in musical audio signals. The\nsource separation module segregates the percussive and non-percussive\ncomponents of the input signal, over which beat and downbeat tracking are\nperformed separately and then the results are aggregated with a learnable\nfusion mechanism. This way, the system can adaptively determine how much the\ntracking result for an input signal should depend on the input's percussive or\nnon-percussive components. Evaluation on four testing sets that feature\ndifferent levels of presence of drum sounds shows that the new architecture\nconsistently outperforms the widely-adopted baseline architecture that does not\nemploy source separation.", "published": "2021-06-16 10:47:44", "link": "http://arxiv.org/abs/2106.08685v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Source Separation-based Data Augmentation for Improved Joint Beat and\n  Downbeat Tracking", "abstract": "Due to advances in deep learning, the performance of automatic beat and\ndownbeat tracking in musical audio signals has seen great improvement in recent\nyears. In training such deep learning based models, data augmentation has been\nfound an important technique. However, existing data augmentation methods for\nthis task mainly target at balancing the distribution of the training data with\nrespect to their tempo. In this paper, we investigate another approach for data\naugmentation, to account for the composition of the training data in terms of\nthe percussive and non-percussive sound sources. Specifically, we propose to\nemploy a blind drum separation model to segregate the drum and non-drum sounds\nfrom each training audio signal, filtering out training signals that are\ndrumless, and then use the obtained drum and non-drum stems to augment the\ntraining data. We report experiments on four completely unseen test sets,\nvalidating the effectiveness of the proposed method, and accordingly the\nimportance of drum sound composition in the training data for beat and downbeat\ntracking.", "published": "2021-06-16 11:09:05", "link": "http://arxiv.org/abs/2106.08703v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant\n  Environments", "abstract": "Voice Conversion (VC) is a technique that aims to transform the\nnon-linguistic information of a source utterance to change the perceived\nidentity of the speaker. While there is a rich literature on VC, most proposed\nmethods are trained and evaluated on clean speech recordings. However, many\nacoustic environments are noisy and reverberant, severely restricting the\napplicability of popular VC methods to such scenarios. To address this\nlimitation, we propose Voicy, a new VC framework particularly tailored for\nnoisy speech. Our method, which is inspired by the de-noising auto-encoders\nframework, is comprised of four encoders (speaker, content, phonetic and\nacoustic-ASR) and one decoder. Importantly, Voicy is capable of performing\nnon-parallel zero-shot VC, an important requirement for any VC system that\nneeds to work on speakers not seen during training. We have validated our\napproach using a noisy reverberant version of the LibriSpeech dataset.\nExperimental results show that Voicy outperforms other tested VC techniques in\nterms of naturalness and target speaker similarity in noisy reverberant\nenvironments.", "published": "2021-06-16 15:47:06", "link": "http://arxiv.org/abs/2106.08873v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition", "abstract": "Pseudo-labeling (PL) has been shown to be effective in semi-supervised\nautomatic speech recognition (ASR), where a base model is self-trained with\npseudo-labels generated from unlabeled data. While PL can be further improved\nby iteratively updating pseudo-labels as the model evolves, most of the\nprevious approaches involve inefficient retraining of the model or intricate\ncontrol of the label update. We present momentum pseudo-labeling (MPL), a\nsimple yet effective strategy for semi-supervised ASR. MPL consists of a pair\nof online and offline models that interact and learn from each other, inspired\nby the mean teacher method. The online model is trained to predict\npseudo-labels generated on the fly by the offline model. The offline model\nmaintains a momentum-based moving average of the online model. MPL is performed\nin a single training process and the interaction between the two models\neffectively helps them reinforce each other to improve the ASR performance. We\napply MPL to an end-to-end ASR model based on the connectionist temporal\nclassification. The experimental results demonstrate that MPL effectively\nimproves over the base model and is scalable to different semi-supervised\nscenarios with varying amounts of data or domain mismatch.", "published": "2021-06-16 16:24:55", "link": "http://arxiv.org/abs/2106.08922v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiRA: Learning Visual Speech Representations from Audio through\n  Self-supervision", "abstract": "The large amount of audiovisual content being shared online today has drawn\nsubstantial attention to the prospect of audiovisual self-supervised learning.\nRecent works have focused on each of these modalities separately, while others\nhave attempted to model both simultaneously in a cross-modal fashion. However,\ncomparatively little attention has been given to leveraging one modality as a\ntraining objective to learn from the other. In this work, we propose Learning\nvisual speech Representations from Audio via self-supervision (LiRA).\nSpecifically, we train a ResNet+Conformer model to predict acoustic features\nfrom unlabelled visual speech. We find that this pre-trained model can be\nleveraged towards word-level and sentence-level lip-reading through feature\nextraction and fine-tuning experiments. We show that our approach significantly\noutperforms other self-supervised methods on the Lip Reading in the Wild (LRW)\ndataset and achieves state-of-the-art performance on Lip Reading Sentences 2\n(LRS2) using only a fraction of the total labelled data.", "published": "2021-06-16 23:20:06", "link": "http://arxiv.org/abs/2106.09171v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in\n  Real-Time MRI", "abstract": "Speech sounds of spoken language are obtained by varying configuration of the\narticulators surrounding the vocal tract. They contain abundant information\nthat can be utilized to better understand the underlying mechanism of human\nspeech production. We propose a novel deep neural network-based learning\nframework that understands acoustic information in the variable-length sequence\nof vocal tract shaping during speech production, captured by real-time magnetic\nresonance imaging (rtMRI), and translate it into text. The proposed framework\ncomprises of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. On the\nUSC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better\ncompared to the existing models. To the best of our knowledge, this is the\nfirst study that demonstrates the recognition of entire spoken sentence based\non an individual's articulatory motions captured by rtMRI video. We also\nperformed an analysis of variations in the geometry of articulation in each\nsub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard\npalate, labial constriction region) with respect to different emotions and\ngenders. Results suggest that each sub-regions distortion is affected by both\nemotion and gender.", "published": "2021-06-16 11:20:02", "link": "http://arxiv.org/abs/2106.08706v1", "categories": ["eess.IV", "cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.AS", "I.4.9; I.2.10"], "primary_category": "eess.IV"}
