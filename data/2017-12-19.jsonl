{"title": "Subword and Crossword Units for CTC Acoustic Models", "abstract": "This paper proposes a novel approach to create an unit set for CTC based\nspeech recognition systems. By using Byte Pair Encoding we learn an unit set of\nan arbitrary size on a given training text. In contrast to using characters or\nwords as units this allows us to find a good trade-off between the size of our\nunit set and the available training data. We evaluate both Crossword units,\nthat may span multiple word, and Subword units. By combining this approach with\ndecoding methods using a separate language model we are able to achieve state\nof the art results for grapheme based CTC systems.", "published": "2017-12-19 10:29:28", "link": "http://arxiv.org/abs/1712.06855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analogy Mining for Specific Design Needs", "abstract": "Finding analogical inspirations in distant domains is a powerful way of\nsolving problems. However, as the number of inspirations that could be matched\nand the dimensions on which that matching could occur grow, it becomes\nchallenging for designers to find inspirations relevant to their needs.\nFurthermore, designers are often interested in exploring specific aspects of a\nproduct-- for example, one designer might be interested in improving the\nbrewing capability of an outdoor coffee maker, while another might wish to\noptimize for portability. In this paper we introduce a novel system for\ntargeting analogical search for specific needs. Specifically, we contribute a\nnovel analogical search engine for expressing and abstracting specific design\nneeds that returns more distant yet relevant inspirations than alternate\napproaches.", "published": "2017-12-19 11:47:53", "link": "http://arxiv.org/abs/1712.06880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Mapping Using Structural Similarities in Monolingual\n  Embeddings", "abstract": "Most existing methods for automatic bilingual dictionary induction rely on\nprior alignments between the source and target languages, such as parallel\ncorpora or seed dictionaries. For many language pairs, such supervised\nalignments are not readily available. We propose an unsupervised approach for\nlearning a bilingual dictionary for a pair of languages given their\nindependently-learned monolingual word embeddings. The proposed method exploits\nlocal and global structures in monolingual vector spaces to align them such\nthat similar words are mapped to each other. We show empirically that the\nperformance of bilingual correspondents learned using our proposed unsupervised\nmethod is comparable to that of using supervised bilingual correspondents from\na seed dictionary.", "published": "2017-12-19 14:52:47", "link": "http://arxiv.org/abs/1712.06961v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentive Memory Networks: Efficient Machine Reading for Conversational\n  Search", "abstract": "Recent advances in conversational systems have changed the search paradigm.\nTraditionally, a user poses a query to a search engine that returns an answer\nbased on its index, possibly leveraging external knowledge bases and\nconditioning the response on earlier interactions in the search session. In a\nnatural conversation, there is an additional source of information to take into\naccount: utterances produced earlier in a conversation can also be referred to\nand a conversational IR system has to keep track of information conveyed by the\nuser during the conversation, even if it is implicit.\n  We argue that the process of building a representation of the conversation\ncan be framed as a machine reading task, where an automated system is presented\nwith a number of statements about which it should answer questions. The\nquestions should be answered solely by referring to the statements provided,\nwithout consulting external knowledge. The time is right for the information\nretrieval community to embrace this task, both as a stand-alone task and\nintegrated in a broader conversational search setting.\n  In this paper, we focus on machine reading as a stand-alone task and present\nthe Attentive Memory Network (AMN), an end-to-end trainable machine reading\nalgorithm. Its key contribution is in efficiency, achieved by having an\nhierarchical input encoder, iterating over the input only once. Speed is an\nimportant requirement in the setting of conversational search, as gaps between\nconversational turns have a detrimental effect on naturalness. On 20 datasets\ncommonly used for evaluating machine reading algorithms we show that the AMN\nachieves performance comparable to the state-of-the-art models, while using\nconsiderably fewer computations.", "published": "2017-12-19 21:43:59", "link": "http://arxiv.org/abs/1712.07229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HotFlip: White-Box Adversarial Examples for Text Classification", "abstract": "We propose an efficient method to generate white-box adversarial examples to\ntrick a character-level neural classifier. We find that only a few\nmanipulations are needed to greatly decrease the accuracy. Our method relies on\nan atomic flip operation, which swaps one token for another, based on the\ngradients of the one-hot input vectors. Due to efficiency of our method, we can\nperform adversarial training which makes the model more robust to attacks at\ntest time. With the use of a few semantics-preserving constraints, we\ndemonstrate that HotFlip can be adapted to attack a word-level classifier as\nwell.", "published": "2017-12-19 02:15:19", "link": "http://arxiv.org/abs/1712.06751v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case\n  Study", "abstract": "Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram\nfeatures when learning from textual data. They are also compatible with the use\nof word embeddings so that word similarities can be accounted for. While the\noriginal any-gram kernels are implemented on top of tree kernels, we propose a\nnew approach which is independent of tree kernels and is more efficient. We\nalso propose a more effective way to make use of word embeddings than the\noriginal any-gram formulation. When applied to the task of sentiment\nclassification, our new formulation achieves significantly better performance.", "published": "2017-12-19 15:47:00", "link": "http://arxiv.org/abs/1712.07004v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The NarrativeQA Reading Comprehension Challenge", "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requires\nintegrating information and reasoning about events, entities, and their\nrelations across a full document. Question answering is conventionally used to\nassess RC ability, in both artificial agents and children learning to read.\nHowever, existing RC datasets and tasks are dominated by questions that can be\nsolved by selecting answers using superficial information (e.g., local context\nsimilarity or global term frequency); they thus fail to test for the essential\nintegrative aspect of RC. To encourage progress on deeper comprehension of\nlanguage, we present a new dataset and set of tasks in which the reader must\nanswer questions about stories by reading entire books or movie scripts. These\ntasks are designed so that successfully answering their questions requires\nunderstanding the underlying narrative rather than relying on shallow pattern\nmatching or salience. We show that although humans solve the tasks easily,\nstandard RC models struggle on the tasks presented here. We provide an analysis\nof the dataset and the challenges it presents.", "published": "2017-12-19 16:48:05", "link": "http://arxiv.org/abs/1712.07040v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Improving End-to-End Speech Recognition with Policy Learning", "abstract": "Connectionist temporal classification (CTC) is widely used for maximum\nlikelihood learning in end-to-end speech recognition models. However, there is\nusually a disparity between the negative maximum likelihood and the performance\nmetric used in speech recognition, e.g., word error rate (WER). This results in\na mismatch between the objective function and metric during training. We show\nthat the above problem can be mitigated by jointly training with maximum\nlikelihood and policy gradient. In particular, with policy learning we are able\nto directly optimize on the (otherwise non-differentiable) performance metric.\nWe show that joint training improves relative performance by 4% to 13% for our\nend-to-end model as compared to the same model learned through maximum\nlikelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and\n5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.", "published": "2017-12-19 18:39:50", "link": "http://arxiv.org/abs/1712.07101v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved Regularization Techniques for End-to-End Speech Recognition", "abstract": "Regularization is important for end-to-end speech models, since the models\nare highly flexible and easy to overfit. Data augmentation and dropout has been\nimportant for improving end-to-end models in other domains. However, they are\nrelatively under explored for end-to-end speech models. Therefore, we\ninvestigate the effectiveness of both methods for end-to-end trainable, deep\nspeech recognition models. We augment audio data through random perturbations\nof tempo, pitch, volume, temporal alignment, and adding random noise.We further\ninvestigate the effect of dropout when applied to the inputs of all layers of\nthe network. We show that the combination of data augmentation and dropout give\na relative performance improvement on both Wall Street Journal (WSJ) and\nLibriSpeech dataset of over 20%. Our model performance is also competitive with\nother end-to-end speech models on both datasets.", "published": "2017-12-19 18:45:25", "link": "http://arxiv.org/abs/1712.07108v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cognitive Database: A Step towards Endowing Relational Databases with\n  Artificial Intelligence Capabilities", "abstract": "We propose Cognitive Databases, an approach for transparently enabling\nArtificial Intelligence (AI) capabilities in relational databases. A novel\naspect of our design is to first view the structured data source as meaningful\nunstructured text, and then use the text to build an unsupervised neural\nnetwork model using a Natural Language Processing (NLP) technique called word\nembedding. This model captures the hidden inter-/intra-column relationships\nbetween database tokens of different types. For each database token, the model\nincludes a vector that encodes contextual semantic relationships. We seamlessly\nintegrate the word embedding model into existing SQL query infrastructure and\nuse it to enable a new class of SQL-based analytics queries called cognitive\nintelligence (CI) queries. CI queries use the model vectors to enable complex\nqueries such as semantic matching, inductive reasoning queries such as\nanalogies, predictive queries using entities not present in a database, and,\nmore generally, using knowledge from external sources. We demonstrate unique\ncapabilities of Cognitive Databases using an Apache Spark based prototype to\nexecute inductive reasoning CI queries over a multi-modal database containing\ntext and images. We believe our first-of-a-kind system exemplifies using AI\nfunctionality to endow relational databases with capabilities that were\npreviously very hard to realize in practice.", "published": "2017-12-19 20:49:26", "link": "http://arxiv.org/abs/1712.07199v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.DB"}
{"title": "Joint model-based recognition and localization of overlapped acoustic\n  events using a set of distributed small microphone arrays", "abstract": "In the analysis of acoustic scenes, often the occurring sounds have to be\ndetected in time, recognized, and localized in space. Usually, each of these\ntasks is done separately. In this paper, a model-based approach to jointly\ncarry them out for the case of multiple simultaneous sources is presented and\ntested. The recognized event classes and their respective room positions are\nobtained with a single system that maximizes the combination of a large set of\nscores, each one resulting from a different acoustic event model and a\ndifferent beamformer output signal, which comes from one of several\narbitrarily-located small microphone arrays. By using a two-step method, the\nexperimental work for a specific scenario consisting of meeting-room acoustic\nevents, either isolated or overlapped with speech, is reported. Tests carried\nout with two datasets show the advantage of the proposed approach with respect\nto some usual techniques, and that the inclusion of estimated priors brings a\nfurther performance improvement.", "published": "2017-12-19 17:30:19", "link": "http://arxiv.org/abs/1712.07065v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio to Body Dynamics", "abstract": "We present a method that gets as input an audio of violin or piano playing,\nand outputs a video of skeleton predictions which are further used to animate\nan avatar. The key idea is to create an animation of an avatar that moves their\nhands similarly to how a pianist or violinist would do, just from audio. Aiming\nfor a fully detailed correct arms and fingers motion is a goal, however, it's\nnot clear if body movement can be predicted from music at all. In this paper,\nwe present the first result that shows that natural body dynamics can be\npredicted at all. We built an LSTM network that is trained on violin and piano\nrecital videos uploaded to the Internet. The predicted points are applied onto\na rigged avatar to create the animation.", "published": "2017-12-19 23:45:00", "link": "http://arxiv.org/abs/1712.09382v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
