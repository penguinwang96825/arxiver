{"title": "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine\n  Translation", "abstract": "Training of neural machine translation (NMT) models usually uses mini-batches\nfor efficiency purposes. During the mini-batched training process, it is\nnecessary to pad shorter sentences in a mini-batch to be equal in length to the\nlongest sentence therein for efficient computation. Previous work has noted\nthat sorting the corpus based on the sentence length before making mini-batches\nreduces the amount of padding and increases the processing speed. However,\ndespite the fact that mini-batch creation is an essential step in NMT training,\nwidely used NMT toolkits implement disparate strategies for doing so, which\nhave not been empirically validated or compared. This work investigates\nmini-batch creation strategies with experiments over two different datasets.\nOur results suggest that the choice of a mini-batch creation strategy has a\nlarge effect on NMT training and some length-based sorting strategies do not\nalways work well compared with simple shuffling.", "published": "2017-06-19 02:38:01", "link": "http://arxiv.org/abs/1706.05765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling for Classification of Clinical Reports", "abstract": "Electronic health records (EHRs) contain important clinical information about\npatients. Efficient and effective use of this information could supplement or\neven replace manual chart review as a means of studying and improving the\nquality and safety of healthcare delivery. However, some of these clinical data\nare in the form of free text and require pre-processing before use in automated\nsystems. A common free text data source is radiology reports, typically\ndictated by radiologists to explain their interpretations. We sought to\ndemonstrate machine learning classification of computed tomography (CT) imaging\nreports into binary outcomes, i.e. positive and negative for fracture, using\nregular text classification and classifiers based on topic modeling. Topic\nmodeling provides interpretable themes (topic distributions) in reports, a\nrepresentation that is more compact than the commonly used bag-of-words\nrepresentation and can be processed faster than raw text in subsequent\nautomated processes. We demonstrate new classifiers based on this topic\nmodeling representation of the reports. Aggregate topic classifier (ATC) and\nconfidence-based topic classifier (CTC) use a single topic that is determined\nfrom the training dataset based on different measures to classify the reports\non the test dataset. Alternatively, similarity-based topic classifier (STC)\nmeasures the similarity between the reports' topic distributions to determine\nthe predicted class. Our proposed topic modeling-based classifier systems are\nshown to be competitive with existing text classification techniques and\nprovides an efficient and interpretable representation.", "published": "2017-06-19 21:04:22", "link": "http://arxiv.org/abs/1706.06177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sub-domain Modelling for Dialogue Management with Hierarchical\n  Reinforcement Learning", "abstract": "Human conversation is inherently complex, often spanning many different\ntopics/domains. This makes policy learning for dialogue systems very\nchallenging. Standard flat reinforcement learning methods do not provide an\nefficient framework for modelling such dialogues. In this paper, we focus on\nthe under-explored problem of multi-domain dialogue management. First, we\npropose a new method for hierarchical reinforcement learning using the option\nframework. Next, we show that the proposed architecture learns faster and\narrives at a better policy than the existing flat ones do. Moreover, we show\nhow pretrained policies can be adapted to more complex systems with an\nadditional set of new actions. In doing that, we show that our approach has the\npotential to facilitate policy optimisation for more sophisticated multi-domain\ndialogue systems.", "published": "2017-06-19 23:15:22", "link": "http://arxiv.org/abs/1706.06210v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with\n  Reduced Overfitting", "abstract": "We propose a simple yet effective technique for neural network learning. The\nforward propagation is computed as usual. In back propagation, only a small\nsubset of the full gradient is computed to update the model parameters. The\ngradient vectors are sparsified in such a way that only the top-$k$ elements\n(in terms of magnitude) are kept. As a result, only $k$ rows or columns\n(depending on the layout) of the weight matrix are modified, leading to a\nlinear reduction ($k$ divided by the vector dimension) in the computational\ncost. Surprisingly, experimental results demonstrate that we can update only\n1-4% of the weights at each back propagation pass. This does not result in a\nlarger number of training iterations. More interestingly, the accuracy of the\nresulting models is actually improved rather than degraded, and a detailed\nanalysis is given. The code is available at https://github.com/lancopku/meProp", "published": "2017-06-19 22:36:33", "link": "http://arxiv.org/abs/1706.06197v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
