{"title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of\n  Gender Bias", "abstract": "Common methods for interpreting neural models in natural language processing\ntypically examine either their structure or their behavior, but not both. We\npropose a methodology grounded in the theory of causal mediation analysis for\ninterpreting which parts of a model are causally implicated in its behavior. It\nenables us to analyze the mechanisms by which information flows from input to\noutput through various model components, known as mediators. We apply this\nmethodology to analyze gender bias in pre-trained Transformer language models.\nWe study the role of individual neurons and attention heads in mediating gender\nbias across three datasets designed to gauge a model's sensitivity to gender\nbias. Our mediation analysis reveals that gender bias effects are (i) sparse,\nconcentrated in a small part of the network; (ii) synergistic, amplified or\nrepressed by different components; and (iii) decomposable into effects flowing\ndirectly from the input and indirectly through the mediators.", "published": "2020-04-26 01:53:03", "link": "http://arxiv.org/abs/2004.12265v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GLUECoS : An Evaluation Benchmark for Code-Switched NLP", "abstract": "Code-switching is the use of more than one language in the same conversation\nor utterance. Recently, multilingual contextual embedding models, trained on\nmultiple monolingual corpora, have shown promising results on cross-lingual and\nmultilingual tasks. We present an evaluation benchmark, GLUECoS, for\ncode-switched languages, that spans several NLP tasks in English-Hindi and\nEnglish-Spanish. Specifically, our evaluation benchmark includes Language\nIdentification from text, POS tagging, Named Entity Recognition, Sentiment\nAnalysis, Question Answering and a new task for code-switching, Natural\nLanguage Inference. We present results on all these tasks using cross-lingual\nword embedding models and multilingual models. In addition, we fine-tune\nmultilingual models on artificially generated code-switched data. Although\nmultilingual models perform significantly better than cross-lingual models, our\nresults show that in most tasks, across both language pairs, multilingual\nmodels fine-tuned on code-switched data perform best, showing that multilingual\nmodels can be further optimized for code-switching tasks.", "published": "2020-04-26 13:28:34", "link": "http://arxiv.org/abs/2004.12376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Graph Neural Networks for Extractive Document\n  Summarization", "abstract": "As a crucial step in extractive document summarization, learning\ncross-sentence relations has been explored by a plethora of approaches. An\nintuitive way is to put them in the graph-based neural network, which has a\nmore complex structure for capturing inter-sentence relationships. In this\npaper, we present a heterogeneous graph-based neural network for extractive\nsummarization (HeterSumGraph), which contains semantic nodes of different\ngranularity levels apart from sentences. These additional nodes act as the\nintermediary between sentences and enrich the cross-sentence relations.\nBesides, our graph structure is flexible in natural extension from a\nsingle-document setting to multi-document via introducing document nodes. To\nour knowledge, we are the first one to introduce different types of nodes into\ngraph-based neural networks for extractive document summarization and perform a\ncomprehensive qualitative analysis to investigate their benefits. The code will\nbe released on Github", "published": "2020-04-26 14:38:11", "link": "http://arxiv.org/abs/2004.12393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masking as an Efficient Alternative to Finetuning for Pretrained\n  Language Models", "abstract": "We present an efficient method of utilizing pretrained language models, where\nwe learn selective binary masks for pretrained weights in lieu of modifying\nthem through finetuning. Extensive evaluations of masking BERT and RoBERTa on a\nseries of NLP tasks show that our masking scheme yields performance comparable\nto finetuning, yet has a much smaller memory footprint when several tasks need\nto be inferred simultaneously. Through intrinsic evaluations, we show that\nrepresentations computed by masked language models encode information necessary\nfor solving downstream tasks. Analyzing the loss landscape, we show that\nmasking and finetuning produce models that reside in minima that can be\nconnected by a line segment with nearly constant test accuracy. This confirms\nthat masking can be utilized as an efficient alternative to finetuning.", "published": "2020-04-26 15:03:47", "link": "http://arxiv.org/abs/2004.12406v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Multimodal Response Generation with Exemplar Augmentation and\n  Curriculum Optimization", "abstract": "Recently, variational auto-encoder (VAE) based approaches have made\nimpressive progress on improving the diversity of generated responses. However,\nthese methods usually suffer the cost of decreased relevance accompanied by\ndiversity improvements. In this paper, we propose a novel multimodal response\ngeneration framework with exemplar augmentation and curriculum optimization to\nenhance relevance and diversity of generated responses. First, unlike existing\nVAE-based models that usually approximate a simple Gaussian posterior\ndistribution, we present a Gaussian mixture posterior distribution (i.e,\nmultimodal) to further boost response diversity, which helps capture complex\nsemantics of responses. Then, to ensure that relevance does not decrease while\ndiversity increases, we fully exploit similar examples (exemplars) retrieved\nfrom the training data into posterior distribution modeling to augment response\nrelevance. Furthermore, to facilitate the convergence of Gaussian mixture prior\nand posterior distributions, we devise a curriculum optimization strategy to\nprogressively train the model under multiple training criteria from easy to\nhard. Experimental results on widely used SwitchBoard and DailyDialog datasets\ndemonstrate that our model achieves significant improvements compared to strong\nbaselines in terms of diversity and relevance.", "published": "2020-04-26 16:29:06", "link": "http://arxiv.org/abs/2004.12429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on\n  Unlabeled Data in Target Language", "abstract": "To better tackle the named entity recognition (NER) problem on languages with\nlittle/no labeled data, cross-lingual NER must effectively leverage knowledge\nlearned from source languages with rich labeled data. Previous works on\ncross-lingual NER are mostly based on label projection with pairwise texts or\ndirect model transfer. However, such methods either are not applicable if the\nlabeled data in the source languages is unavailable, or do not leverage\ninformation contained in unlabeled data in the target language. In this paper,\nwe propose a teacher-student learning method to address such limitations, where\nNER models in the source languages are used as teachers to train a student\nmodel on unlabeled data in the target language. The proposed method works for\nboth single-source and multi-source cross-lingual NER. For the latter, we\nfurther propose a similarity measuring method to better weight the supervision\nfrom different teacher models. Extensive experiments for 3 target languages on\nbenchmark datasets well demonstrate that our method outperforms existing\nstate-of-the-art methods for both single-source and multi-source cross-lingual\nNER.", "published": "2020-04-26 17:22:09", "link": "http://arxiv.org/abs/2004.12440v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Neural System for Tagging, Parsing and Lematization", "abstract": "This paper describes the ICS PAS system which took part in CoNLL 2018 shared\ntask on Multilingual Parsing from Raw Text to Universal Dependencies. The\nsystem consists of jointly trained tagger, lemmatizer, and dependency parser\nwhich are based on features extracted by a biLSTM network. The system uses both\nfully connected and dilated convolutional neural architectures. The novelty of\nour approach is the use of an additional loss function, which reduces the\nnumber of cycles in the predicted dependency graphs, and the use of\nself-training to increase the system performance. The proposed system, i.e. ICS\nPAS (Warszawa), ranked 3th/4th in the official evaluation obtaining the\nfollowing overall results: 73.02 (LAS), 60.25 (MLAS) and 64.44 (BLEX).", "published": "2020-04-26 18:29:31", "link": "http://arxiv.org/abs/2004.12450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experiments with LVT and FRE for Transformer model", "abstract": "In this paper, we experiment with Large Vocabulary Trick and Feature-rich\nencoding applied to the Transformer model for Text Summarization. We could not\nachieve better results, than the analogous RNN-based sequence-to-sequence\nmodel, so we tried more models to find out, what improves the results and what\ndeteriorates them.", "published": "2020-04-26 22:47:29", "link": "http://arxiv.org/abs/2004.12495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PTPARL-D: Annotated Corpus of 44 years of Portuguese Parliament debates", "abstract": "In a representative democracy, some decide in the name of the rest, and these\nelected officials are commonly gathered in public assemblies, such as\nparliaments, where they discuss policies, legislate, and vote on fundamental\ninitiatives. A core aspect of such democratic processes are the plenary\ndebates, where important public discussions take place. Many parliaments around\nthe world are increasingly keeping the transcripts of such debates, and other\nparliamentary data, in digital formats accessible to the public, increasing\ntransparency and accountability. Furthermore, some parliaments are bringing old\npaper transcripts to semi-structured digital formats. However, these records\nare often only provided as raw text or even as images, with little to no\nannotation, and inconsistent formats, making them difficult to analyze and\nstudy, reducing both transparency and public reach. Here, we present PTPARL-D,\nan annotated corpus of debates in the Portuguese Parliament, from 1976 to 2019,\ncovering the entire period of Portuguese democracy.", "published": "2020-04-26 23:22:41", "link": "http://arxiv.org/abs/2004.12502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Discourse Relations in Language Generation from GPT-2", "abstract": "Recent advances in NLP have been attributed to the emergence of large-scale\npre-trained language models. GPT-2, in particular, is suited for generation\ntasks given its left-to-right language modeling objective, yet the linguistic\nquality of its generated text has largely remain unexplored. Our work takes a\nstep in understanding GPT-2's outputs in terms of discourse coherence. We\nperform a comprehensive study on the validity of explicit discourse relations\nin GPT-2's outputs under both organic generation and fine-tuned scenarios.\nResults show GPT-2 does not always generate text containing valid discourse\nrelations; nevertheless, its text is more aligned with human expectation in the\nfine-tuned scenario. We propose a decoupled strategy to mitigate these problems\nand highlight the importance of explicitly modeling discourse information.", "published": "2020-04-26 23:29:27", "link": "http://arxiv.org/abs/2004.12506v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification of Cuisines from Sequentially Structured Recipes", "abstract": "Cultures across the world are distinguished by the idiosyncratic patterns in\ntheir cuisines. These cuisines are characterized in terms of their\nsubstructures such as ingredients, cooking processes and utensils. A complex\nfusion of these substructures intrinsic to a region defines the identity of a\ncuisine. Accurate classification of cuisines based on their culinary features\nis an outstanding problem and has hitherto been attempted to solve by\naccounting for ingredients of a recipe as features. Previous studies have\nattempted cuisine classification by using unstructured recipes without\naccounting for details of cooking techniques. In reality, the cooking\nprocesses/techniques and their order are highly significant for the recipe's\nstructure and hence for its classification. In this article, we have\nimplemented a range of classification techniques by accounting for this\ninformation on the RecipeDB dataset containing sequential data on recipes. The\nstate-of-the-art RoBERTa model presented the highest accuracy of 73.30% among a\nrange of classification models from Logistic Regression and Naive Bayes to\nLSTMs and Transformers.", "published": "2020-04-26 05:40:36", "link": "http://arxiv.org/abs/2004.14165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpellGCN: Incorporating Phonological and Visual Similarities into\n  Language Models for Chinese Spelling Check", "abstract": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors\nin Chinese natural language. Existing methods have made attempts to incorporate\nthe similarity knowledge between Chinese characters. However, they take the\nsimilarity knowledge as either an external input resource or just heuristic\nrules. This paper proposes to incorporate phonological and visual similarity\nknowledge into language models for CSC via a specialized graph convolutional\nnetwork (SpellGCN). The model builds a graph over the characters, and SpellGCN\nis learned to map this graph into a set of inter-dependent character\nclassifiers. These classifiers are applied to the representations extracted by\nanother network, such as BERT, enabling the whole network to be end-to-end\ntrainable. Experiments (The dataset and all code for this paper are available\nat https://github.com/ACL2020SpellGCN/SpellGCN) are conducted on three\nhuman-annotated datasets. Our method achieves superior performance against\nprevious models by a large margin.", "published": "2020-04-26 03:34:06", "link": "http://arxiv.org/abs/2004.14166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical\n  Encoder for Long-Form Document Matching", "abstract": "Many natural language processing and information retrieval problems can be\nformalized as the task of semantic matching. Existing work in this area has\nbeen largely focused on matching between short texts (e.g., question\nanswering), or between a short and a long text (e.g., ad-hoc retrieval).\nSemantic matching between long-form documents, which has many important\napplications like news recommendation, related article recommendation and\ndocument clustering, is relatively less explored and needs more research\neffort. In recent years, self-attention based models like Transformers and BERT\nhave achieved state-of-the-art performance in the task of text matching. These\nmodels, however, are still limited to short text like a few sentences or one\nparagraph due to the quadratic computational complexity of self-attention with\nrespect to input text length. In this paper, we address the issue by proposing\nthe Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for\nlong-form document matching. Our model contains several innovations to adapt\nself-attention models for longer text input. In order to better capture\nsentence level semantic relations within a document, we pre-train the model\nwith a novel masked sentence block language modeling task in addition to the\nmasked word language modeling task used by BERT. Our experimental results on\nseveral benchmark datasets for long-form document matching show that our\nproposed SMITH model outperforms the previous state-of-the-art models including\nhierarchical attention, multi-depth attention-based hierarchical recurrent\nneural network, and BERT. Comparing to BERT based baselines, our model is able\nto increase maximum input text length from 512 to 2048. We will open source a\nWikipedia based benchmark dataset, code and a pre-trained checkpoint to\naccelerate future research on long-form document matching.", "published": "2020-04-26 07:04:08", "link": "http://arxiv.org/abs/2004.12297v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Dual Learning for Semi-Supervised Natural Language Understanding", "abstract": "Natural language understanding (NLU) converts sentences into structured\nsemantic forms. The paucity of annotated training samples is still a\nfundamental challenge of NLU. To solve this data sparsity problem, previous\nwork based on semi-supervised learning mainly focuses on exploiting unlabeled\nsentences. In this work, we introduce a dual task of NLU, semantic-to-sentence\ngeneration (SSG), and propose a new framework for semi-supervised NLU with the\ncorresponding dual model. The framework is composed of dual pseudo-labeling and\ndual learning method, which enables an NLU model to make full use of data\n(labeled and unlabeled) through a closed-loop of the primal and dual tasks. By\nincorporating the dual task, the framework can exploit pure semantic forms as\nwell as unlabeled sentences, and further improve the NLU and SSG models\niteratively in the closed-loop. The proposed approaches are evaluated on two\npublic datasets (ATIS and SNIPS). Experiments in the semi-supervised setting\nshow that our methods can outperform various baselines significantly, and\nextensive ablation studies are conducted to verify the effectiveness of our\nframework. Finally, our method can also achieve the state-of-the-art\nperformance on the two datasets in the supervised setting. Our code is\navailable at \\url{https://github.com/rhythmcao/slu-dual-learning.git}.", "published": "2020-04-26 07:17:48", "link": "http://arxiv.org/abs/2004.12299v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenge Closed-book Science Exam: A Meta-learning Based Question\n  Answering System", "abstract": "Prior work in standardized science exams requires support from large text\ncorpus, such as targeted science corpus fromWikipedia or SimpleWikipedia.\nHowever, retrieving knowledge from the large corpus is time-consuming and\nquestions embedded in complex semantic representation may interfere with\nretrieval. Inspired by the dual process theory in cognitive science, we propose\na MetaQA framework, where system 1 is an intuitive meta-classifier and system 2\nis a reasoning module. Specifically, our method based on meta-learning method\nand large language model BERT, which can efficiently solve science problems by\nlearning from related example questions without relying on external knowledge\nbases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the\nexperimental results show that meta-classifier yields considerable\nclassification performance on emerging question types. The information provided\nby meta-classifier significantly improves the accuracy of reasoning module from\n46.6% to 64.2%, which has a competitive advantage over retrieval-based QA\nmethods.", "published": "2020-04-26 07:43:30", "link": "http://arxiv.org/abs/2004.12303v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Detecting fake news for the new coronavirus by reasoning on the Covid-19\n  ontology", "abstract": "In the context of the Covid-19 pandemic, many were quick to spread deceptive\ninformation. I investigate here how reasoning in Description Logics (DLs) can\ndetect inconsistencies between trusted medical sources and not trusted ones.\nThe not-trusted information comes in natural language (e.g. \"Covid-19 affects\nonly the elderly\"). To automatically convert into DLs, I used the FRED\nconverter. Reasoning in Description Logics is then performed with the Racer\ntool.", "published": "2020-04-26 09:34:32", "link": "http://arxiv.org/abs/2004.12330v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty\n  with Bernstein Bounds", "abstract": "Most NLP datasets are not annotated with protected attributes such as gender,\nmaking it difficult to measure classification bias using standard measures of\nfairness (e.g., equal opportunity). However, manually annotating a large\ndataset with a protected attribute is slow and expensive. Instead of annotating\nall the examples, can we annotate a subset of them and use that sample to\nestimate the bias? While it is possible to do so, the smaller this annotated\nsample is, the less certain we are that the estimate is close to the true bias.\nIn this work, we propose using Bernstein bounds to represent this uncertainty\nabout the bias estimate as a confidence interval. We provide empirical evidence\nthat a 95% confidence interval derived this way consistently bounds the true\nbias. In quantifying this uncertainty, our method, which we call\nBernstein-bounded unfairness, helps prevent classifiers from being deemed\nbiased or unbiased when there is insufficient evidence to make either claim.\nOur findings suggest that the datasets currently used to measure specific\nbiases are too small to conclusively identify bias except in the most egregious\ncases. For example, consider a co-reference resolution system that is 5% more\naccurate on gender-stereotypical sentences -- to claim it is biased with 95%\nconfidence, we need a bias-specific dataset that is 3.8 times larger than\nWinoBias, the largest available.", "published": "2020-04-26 09:45:45", "link": "http://arxiv.org/abs/2004.12332v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis aims to determine the sentiment polarity\ntowards a specific aspect in online reviews. Most recent efforts adopt\nattention-based neural network models to implicitly connect aspects with\nopinion words. However, due to the complexity of language and the existence of\nmultiple aspects in a single sentence, these models often confuse the\nconnections. In this paper, we address this problem by means of effective\nencoding of syntax information. Firstly, we define a unified aspect-oriented\ndependency tree structure rooted at a target aspect by reshaping and pruning an\nordinary dependency parse tree. Then, we propose a relational graph attention\nnetwork (R-GAT) to encode the new tree structure for sentiment prediction.\nExtensive experiments are conducted on the SemEval 2014 and Twitter datasets,\nand the experimental results confirm that the connections between aspects and\nopinion words can be better established with our approach, and the performance\nof the graph attention network (GAT) is significantly improved as a\nconsequence.", "published": "2020-04-26 12:21:04", "link": "http://arxiv.org/abs/2004.12362v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Dialogue Acts and Response Co-Generation", "abstract": "Generating fluent and informative responses is of critical importance for\ntask-oriented dialogue systems. Existing pipeline approaches generally predict\nmultiple dialogue acts first and use them to assist response generation. There\nare at least two shortcomings with such approaches. First, the inherent\nstructures of multi-domain dialogue acts are neglected. Second, the semantic\nassociations between acts and responses are not taken into account for response\ngeneration. To address these issues, we propose a neural co-generation model\nthat generates dialogue acts and responses concurrently. Unlike those pipeline\napproaches, our act generation module preserves the semantic structures of\nmulti-domain dialogue acts and our response generation module dynamically\nattends to different acts as needed. We train the two modules jointly using an\nuncertainty loss to adjust their task weights adaptively. Extensive experiments\nare conducted on the large-scale MultiWOZ dataset and the results show that our\nmodel achieves very favorable improvement over several state-of-the-art models\nin both automatic and human evaluations.", "published": "2020-04-26 12:21:17", "link": "http://arxiv.org/abs/2004.12363v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Show, Describe and Conclude: On Exploiting the Structure Information of\n  Chest X-Ray Reports", "abstract": "Chest X-Ray (CXR) images are commonly used for clinical screening and\ndiagnosis. Automatically writing reports for these images can considerably\nlighten the workload of radiologists for summarizing descriptive findings and\nconclusive impressions. The complex structures between and within sections of\nthe reports pose a great challenge to the automatic report generation.\nSpecifically, the section Impression is a diagnostic summarization over the\nsection Findings; and the appearance of normality dominates each section over\nthat of abnormality. Existing studies rarely explore and consider this\nfundamental structure information. In this work, we propose a novel framework\nthat exploits the structure information between and within report sections for\ngenerating CXR imaging reports. First, we propose a two-stage strategy that\nexplicitly models the relationship between Findings and Impression. Second, we\ndesign a novel cooperative multi-agent system that implicitly captures the\nimbalanced distribution between abnormality and normality. Experiments on two\nCXR report datasets show that our method achieves state-of-the-art performance\nin terms of various evaluation metrics. Our results expose that the proposed\napproach is able to generate high-quality medical reports through integrating\nthe structure information.", "published": "2020-04-26 02:29:20", "link": "http://arxiv.org/abs/2004.12274v2", "categories": ["cs.CL", "cs.CV", "eess.IV"], "primary_category": "cs.CL"}
{"title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification,\n  Question Answering and Summarization", "abstract": "Recently, large-scale datasets have vastly facilitated the development in\nnearly all domains of Natural Language Processing. However, there is currently\nno cross-task dataset in NLP, which hinders the development of multi-task\nlearning. We propose MATINF, the first jointly labeled large-scale dataset for\nclassification, question answering and summarization. MATINF contains 1.07\nmillion question-answer pairs with human-labeled categories and user-generated\nquestion descriptions. Based on such rich information, MATINF is applicable for\nthree major NLP tasks, including classification, question answering, and\nsummarization. We benchmark existing methods and a novel multi-task baseline\nover MATINF to inspire further research. Our comprehensive comparison and\nexperiments over MATINF and other datasets demonstrate the merits held by\nMATINF.", "published": "2020-04-26 07:43:15", "link": "http://arxiv.org/abs/2004.12302v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Methods for Computing Legal Document Similarity: A Comparative Study", "abstract": "Computing similarity between two legal documents is an important and\nchallenging task in the domain of Legal Information Retrieval. Finding similar\nlegal documents has many applications in downstream tasks, including prior-case\nretrieval, recommendation of legal articles, and so on. Prior works have\nproposed two broad ways of measuring similarity between legal documents -\nanalyzing the precedent citation network, and measuring similarity based on\ntextual content similarity measures. But there has not been a comprehensive\ncomparison of these existing methods on a common platform. In this paper, we\nperform the first systematic analysis of the existing methods. In addition, we\nexplore two promising new similarity computation methods - one text-based and\nthe other based on network embeddings, which have not been considered till now.", "published": "2020-04-26 08:26:04", "link": "http://arxiv.org/abs/2004.12307v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Towards Persona-Based Empathetic Conversational Models", "abstract": "Empathetic conversational models have been shown to improve user satisfaction\nand task outcomes in numerous domains. In Psychology, persona has been shown to\nbe highly correlated to personality, which in turn influences empathy. In\naddition, our empirical analysis also suggests that persona plays an important\nrole in empathetic conversations. To this end, we propose a new task towards\npersona-based empathetic conversations and present the first empirical study on\nthe impact of persona on empathetic responding. Specifically, we first present\na novel large-scale multi-domain dataset for persona-based empathetic\nconversations. We then propose CoBERT, an efficient BERT-based response\nselection model that obtains the state-of-the-art performance on our dataset.\nFinally, we conduct extensive experiments to investigate the impact of persona\non empathetic responding. Notably, our results show that persona improves\nempathetic responding more when CoBERT is trained on empathetic conversations\nthan non-empathetic ones, establishing an empirical link between persona and\nempathy in human conversations.", "published": "2020-04-26 08:51:01", "link": "http://arxiv.org/abs/2004.12316v7", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural Topic Modeling with Bidirectional Adversarial Training", "abstract": "Recent years have witnessed a surge of interests of using neural topic models\nfor automatic topic extraction from text, since they avoid the complicated\nmathematical derivations for model inference as in traditional topic models\nsuch as Latent Dirichlet Allocation (LDA). However, these models either\ntypically assume improper prior (e.g. Gaussian or Logistic Normal) over latent\ntopic space or could not infer topic distribution for a given document. To\naddress these limitations, we propose a neural topic modeling approach, called\nBidirectional Adversarial Topic (BAT) model, which represents the first attempt\nof applying bidirectional adversarial training for neural topic modeling. The\nproposed BAT builds a two-way projection between the document-topic\ndistribution and the document-word distribution. It uses a generator to capture\nthe semantic patterns from texts and an encoder for topic inference.\nFurthermore, to incorporate word relatedness information, the Bidirectional\nAdversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To\nverify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are\nused in our experiments. The experimental results show that BAT and\nGaussian-BAT obtain more coherent topics, outperforming several competitive\nbaselines. Moreover, when performing text clustering based on the extracted\ntopics, our models outperform all the baselines, with more significant\nimprovements achieved by Gaussian-BAT where an increase of near 6\\% is observed\nin accuracy.", "published": "2020-04-26 09:41:17", "link": "http://arxiv.org/abs/2004.12331v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Choppy: Cut Transformer For Ranked List Truncation", "abstract": "Work in information retrieval has traditionally focused on ranking and\nrelevance: given a query, return some number of results ordered by relevance to\nthe user. However, the problem of determining how many results to return, i.e.\nhow to optimally truncate the ranked result list, has received less attention\ndespite being of critical importance in a range of applications. Such\ntruncation is a balancing act between the overall relevance, or usefulness of\nthe results, with the user cost of processing more results. In this work, we\npropose Choppy, an assumption-free model based on the widely successful\nTransformer architecture, to the ranked list truncation problem. Needing\nnothing more than the relevance scores of the results, the model uses a\npowerful multi-head attention mechanism to directly optimize any user-defined\nIR metric. We show Choppy improves upon recent state-of-the-art methods.", "published": "2020-04-26 00:52:49", "link": "http://arxiv.org/abs/2004.13012v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Research on Modeling Units of Transformer Transducer for Mandarin Speech\n  Recognition", "abstract": "Modeling unit and model architecture are two key factors of Recurrent Neural\nNetwork Transducer (RNN-T) in end-to-end speech recognition. To improve the\nperformance of RNN-T for Mandarin speech recognition task, a novel transformer\ntransducer with the combination architecture of self-attention transformer and\nRNN is proposed. And then the choice of different modeling units for\ntransformer transducer is explored. In addition, we present a new mix-bandwidth\ntraining method to obtain a general model that is able to accurately recognize\nMandarin speech with different sampling rates simultaneously. All of our\nexperiments are conducted on about 12,000 hours of Mandarin speech with\nsampling rate in 8kHz and 16kHz. Experimental results show that Mandarin\ntransformer transducer using syllable with tone achieves the best performance.\nIt yields an average of 14.4% and 44.1% relative Word Error Rate (WER)\nreduction when compared with the models using syllable initial/final with tone\nand Chinese character, respectively. Also, it outperforms the model based on\nsyllable initial/final with tone with an average of 13.5% relative Character\nError Rate (CER) reduction.", "published": "2020-04-26 05:12:52", "link": "http://arxiv.org/abs/2004.13522v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MICK: A Meta-Learning Framework for Few-shot Relation Classification\n  with Small Training Data", "abstract": "Few-shot relation classification seeks to classify incoming query instances\nafter meeting only few support instances. This ability is gained by training\nwith large amount of in-domain annotated data. In this paper, we tackle an even\nharder problem by further limiting the amount of data available at training\ntime. We propose a few-shot learning framework for relation classification,\nwhich is particularly powerful when the training data is very small. In this\nframework, models not only strive to classify query instances, but also seek\nunderlying knowledge about the support instances to obtain better instance\nrepresentations. The framework also includes a method for aggregating\ncross-domain knowledge into models by open-source task enrichment.\nAdditionally, we construct a brand new dataset: the TinyRel-CM dataset, a\nfew-shot relation classification dataset in health domain with purposely small\ntraining data and challenging relation classes. Experimental results\ndemonstrate that our framework brings performance gains for most underlying\nclassification models, outperforms the state-of-the-art results given small\ntraining data, and achieves competitive results with sufficiently large\ntraining data.", "published": "2020-04-26 06:23:38", "link": "http://arxiv.org/abs/2004.14164v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Detect Language of Transliterated Texts", "abstract": "Informal transliteration from other languages to English is prevalent in\nsocial media threads, instant messaging, and discussion forums. Without\nidentifying the language of such transliterated text, users who do not speak\nthat language cannot understand its content using translation tools. We propose\na Language Identification (LID) system, with an approach for feature\nextraction, which can detect the language of transliterated texts reasonably\nwell even with limited training data and computational resources. We tokenize\nthe words into phonetic syllables and use a simple Long Short-term Memory\n(LSTM) network architecture to detect the language of transliterated texts.\nWith intensive experiments, we show that the tokenization of transliterated\nwords as phonetic syllables effectively represents their causal sound patterns.\nPhonetic syllable tokenization, therefore, makes it easier for even simpler\nmodel architectures to learn the characteristic patterns to identify any\nlanguage.", "published": "2020-04-26 10:28:02", "link": "http://arxiv.org/abs/2004.13521v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Enabling Fast and Universal Audio Adversarial Attack Using Generative\n  Model", "abstract": "Recently, the vulnerability of DNN-based audio systems to adversarial attacks\nhas obtained the increasing attention. However, the existing audio adversarial\nattacks allow the adversary to possess the entire user's audio input as well as\ngranting sufficient time budget to generate the adversarial perturbations.\nThese idealized assumptions, however, makes the existing audio adversarial\nattacks mostly impossible to be launched in a timely fashion in practice (e.g.,\nplaying unnoticeable adversarial perturbations along with user's streaming\ninput). To overcome these limitations, in this paper we propose fast audio\nadversarial perturbation generator (FAPG), which uses generative model to\ngenerate adversarial perturbations for the audio input in a single forward\npass, thereby drastically improving the perturbation generation speed. Built on\nthe top of FAPG, we further propose universal audio adversarial perturbation\ngenerator (UAPG), a scheme crafting universal adversarial perturbation that can\nbe imposed on arbitrary benign audio input to cause misclassification.\nExtensive experiments show that our proposed FAPG can achieve up to 167X\nspeedup over the state-of-the-art audio adversarial attack methods. Also our\nproposed UAPG can generate universal adversarial perturbation that achieves\nmuch better attack performance than the state-of-the-art solutions.", "published": "2020-04-26 00:51:54", "link": "http://arxiv.org/abs/2004.12261v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
