{"title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases\n  in Word Embeddings But do not Remove Them", "abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It was\nshown that word embeddings derived from text corpora reflect gender biases in\nsociety. This phenomenon is pervasive and consistent across different word\nembedding models, causing serious concern. Several recent works tackle this\nproblem, and propose methods for significantly reducing this gender bias in\nword embeddings, demonstrating convincing results. However, we argue that this\nremoval is superficial. While the bias is indeed substantially reduced\naccording to the provided bias definition, the actual effect is mostly hiding\nthe bias, not removing it. The gender bias information is still reflected in\nthe distances between \"gender-neutralized\" words in the debiased embeddings,\nand can be recovered from them. We present a series of experiments to support\nthis claim, for two debiasing methods. We conclude that existing bias removal\ntechniques are insufficient, and should not be trusted for providing\ngender-neutral modeling.", "published": "2019-03-09 19:56:47", "link": "http://arxiv.org/abs/1903.03862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutual Clustering on Comparative Texts via Heterogeneous Information\n  Networks", "abstract": "Currently, many intelligence systems contain the texts from multi-sources,\ne.g., bulletin board system (BBS) posts, tweets and news. These texts can be\n``comparative'' since they may be semantically correlated and thus provide us\nwith different perspectives toward the same topics or events. To better\norganize the multi-sourced texts and obtain more comprehensive knowledge, we\npropose to study the novel problem of Mutual Clustering on Comparative Texts\n(MCCT), which aims to cluster the comparative texts simultaneously and\ncollaboratively. The MCCT problem is difficult to address because 1)\ncomparative texts usually present different data formats and structures and\nthus they are hard to organize, and 2) there lacks an effective method to\nconnect the semantically correlated comparative texts to facilitate clustering\nthem in an unified way. To this aim, in this paper we propose a Heterogeneous\nInformation Network-based Text clustering framework HINT. HINT first models\nmulti-sourced texts (e.g. news and tweets) as heterogeneous information\nnetworks by introducing the shared ``anchor texts'' to connect the comparative\ntexts. Next, two similarity matrices based on HINT as well as a transition\nmatrix for cross-text-source knowledge transfer are constructed. Comparative\ntexts clustering are then conducted by utilizing the constructed matrices.\nFinally, a mutual clustering algorithm is also proposed to further unify the\nseparate clustering results of the comparative texts by introducing a\nclustering consistency constraint. We conduct extensive experimental on three\ntweets-news datasets, and the results demonstrate the effectiveness and\nrobustness of the proposed method in addressing the MCCT problem.", "published": "2019-03-09 08:24:15", "link": "http://arxiv.org/abs/1903.03762v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Logic Rules Powered Knowledge Graph Embedding", "abstract": "Large scale knowledge graph embedding has attracted much attention from both\nacademia and industry in the field of Artificial Intelligence. However, most\nexisting methods concentrate solely on fact triples contained in the given\nknowledge graph. Inspired by the fact that logic rules can provide a flexible\nand declarative language for expressing rich background knowledge, it is\nnatural to integrate logic rules into knowledge graph embedding, to transfer\nhuman knowledge to entity and relation embedding, and strengthen the learning\nprocess. In this paper, we propose a novel logic rule-enhanced method which can\nbe easily integrated with any translation based knowledge graph embedding\nmodel, such as TransE . We first introduce a method to automatically mine the\nlogic rules and corresponding confidences from the triples. And then, to put\nboth triples and mined logic rules within the same semantic space, all triples\nin the knowledge graph are represented as first-order logic. Finally, we define\nseveral operations on the first-order logic and minimize a global loss over\nboth of the mined logic rules and the transformed first-order logics. We\nconduct extensive experiments for link prediction and triple classification on\nthree datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced\nmethod can significantly improve the performance of several baselines. The\nhighlight of our model is that the filtered Hits@1, which is a pivotal\nevaluation in the knowledge inference task, has a significant improvement (up\nto 700% improvement).", "published": "2019-03-09 10:01:12", "link": "http://arxiv.org/abs/1903.03772v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
