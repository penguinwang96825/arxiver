{"title": "Multi-turn Inference Matching Network for Natural Language Inference", "abstract": "Natural Language Inference (NLI) is a fundamental and challenging task in\nNatural Language Processing (NLP). Most existing methods only apply one-pass\ninference process on a mixed matching feature, which is a concatenation of\ndifferent matching features between a premise and a hypothesis. In this paper,\nwe propose a new model called Multi-turn Inference Matching Network (MIMN) to\nperform multi-turn inference on different matching features. In each turn, the\nmodel focuses on one particular matching feature instead of the mixed matching\nfeature. To enhance the interaction between different matching features, a\nmemory component is employed to store the history inference information. The\ninference of each turn is performed on the current matching feature and the\nmemory. We conduct experiments on three different NLI datasets. The\nexperimental results show that our model outperforms or achieves the\nstate-of-the-art performance on all the three datasets.", "published": "2019-01-08 09:48:41", "link": "http://arxiv.org/abs/1901.02222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEMN: Distilled-Exposition Enhanced Matching Network for Story\n  Comprehension", "abstract": "This paper proposes a Distilled-Exposition Enhanced Matching Network (DEMN)\nfor story-cloze test, which is still a challenging task in story comprehension.\nWe divide a complete story into three narrative segments: an\n\\textit{exposition}, a \\textit{climax}, and an \\textit{ending}. The model\nconsists of three modules: input module, matching module, and distillation\nmodule. The input module provides semantic representations for the three\nsegments and then feeds them into the other two modules. The matching module\ncollects interaction features between the ending and the climax. The\ndistillation module distills the crucial semantic information in the exposition\nand infuses it into the matching module in two different ways. We evaluate our\nsingle and ensemble model on ROCStories Corpus \\cite{Mostafazadeh2016ACA},\nachieving an accuracy of 80.1\\% and 81.2\\% on the test set respectively. The\nexperimental results demonstrate that our DEMN model achieves a\nstate-of-the-art performance.", "published": "2019-01-08 11:06:42", "link": "http://arxiv.org/abs/1901.02252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Fusion Network for Commonsense Reading Comprehension", "abstract": "Commonsense Reading Comprehension (CRC) is a significantly challenging task,\naiming at choosing the right answer for the question referring to a narrative\npassage, which may require commonsense knowledge inference. Most of the\nexisting approaches only fuse the interaction information of choice, passage,\nand question in a simple combination manner from a \\emph{union} perspective,\nwhich lacks the comparison information on a deeper level. Instead, we propose a\nMulti-Perspective Fusion Network (MPFN), extending the single fusion method\nwith multiple perspectives by introducing the \\emph{difference} and\n\\emph{similarity} fusion\\deleted{along with the \\emph{union}}. More\ncomprehensive and accurate information can be captured through the three types\nof fusion. We design several groups of experiments on MCScript dataset\n\\cite{Ostermann:LREC18:MCScript} to evaluate the effectiveness of the three\ntypes of fusion respectively. From the experimental results, we can conclude\nthat the difference fusion is comparable with union fusion, and the similarity\nfusion needs to be activated by the union fusion. The experimental result also\nshows that our MPFN model achieves the state-of-the-art with an accuracy of\n83.52\\% on the official test set.", "published": "2019-01-08 11:15:07", "link": "http://arxiv.org/abs/1901.02257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-style Generative Reading Comprehension", "abstract": "This study tackles generative reading comprehension (RC), which consists of\nanswering questions based on textual evidence and natural language generation\n(NLG). We propose a multi-style abstractive summarization model for question\nanswering, called Masque. The proposed model has two key characteristics.\nFirst, unlike most studies on RC that have focused on extracting an answer span\nfrom the provided passages, our model instead focuses on generating a summary\nfrom the question and multiple passages. This serves to cover various answer\nstyles required for real-world applications. Second, whereas previous studies\nbuilt a specific model for each answer style because of the difficulty of\nacquiring one general model, our approach learns multi-style answers within a\nmodel to improve the NLG capability for all styles involved. This also enables\nour model to give an answer in the target style. Experiments show that our\nmodel achieves state-of-the-art performance on the Q&A task and the Q&A + NLG\ntask of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the\ntransfer of the style-independent NLG capability to the target style is the key\nto its success.", "published": "2019-01-08 11:27:58", "link": "http://arxiv.org/abs/1901.02262v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing\n  Support Systems", "abstract": "Scientific writing is difficult. It is even harder for those for whom English\nis a second language (ESL learners). Scholars around the world spend a\nsignificant amount of time and resources proofreading their work before\nsubmitting it for review or publication.\n  In this paper we present a novel machine learning based application for\nproper word choice task. Proper word choice is a generalization the lexical\nsubstitution (LS) and grammatical error correction (GEC) tasks. We demonstrate\nand evaluate the usefulness of applying bidirectional Long Short Term Memory\n(LSTM) tagger, for this task. While state-of-the-art grammatical error\ncorrection uses error-specific classifiers and machine translation methods, we\ndemonstrate an unsupervised method that is based solely on a high quality text\ncorpus and does not require manually annotated data. We use a bidirectional\nRecurrent Neural Network (RNN) with LSTM for learning the proper word choice\nbased on a word's sentential context. We demonstrate and evaluate our\napplication on both a domain-specific (scientific), writing task and a\ngeneral-purpose writing task. We show that our domain-specific and\ngeneral-purpose models outperform state-of-the-art general context learning. As\nan additional contribution of this research, we also share our code,\npre-trained models, and a new ESL learner test set with the research community.", "published": "2019-01-08 19:59:33", "link": "http://arxiv.org/abs/1901.02490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Team Papelo: Transformer Networks at FEVER", "abstract": "We develop a system for the FEVER fact extraction and verification challenge\nthat uses a high precision entailment classifier based on transformer networks\npretrained with language modeling, to classify a broad set of potential\nevidence. The precision of the entailment classifier allows us to enhance\nrecall by considering every statement from several articles to decide upon each\nclaim. We include not only the articles best matching the claim text by TFIDF\nscore, but read additional articles whose titles match named entities and\ncapitalized expressions occurring in the claim text. The entailment module\nevaluates potential evidence one statement at a time, together with the title\nof the page the evidence came from (providing a hint about possible pronoun\nantecedents). In preliminary evaluation, the system achieves .5736 FEVER score,\n.6108 label accuracy, and .6485 evidence F1 on the FEVER shared task test set.", "published": "2019-01-08 21:57:30", "link": "http://arxiv.org/abs/1901.02534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deconstructing Word Embeddings", "abstract": "A review of Word Embedding Models through a deconstructive approach reveals\ntheir several shortcomings and inconsistencies. These include instability of\nthe vector representations, a distorted analogical reasoning, geometric\nincompatibility with linguistic features, and the inconsistencies in the corpus\ndata. A new theoretical embedding model, Derridian Embedding, is proposed in\nthis paper. Contemporary embedding models are evaluated qualitatively in terms\nof how adequate they are in relation to the capabilities of a Derridian\nEmbedding.", "published": "2019-01-08 06:44:40", "link": "http://arxiv.org/abs/1902.00551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Possibilities and Limitations of Multi-hop Reasoning Under\n  Linguistic Imperfections", "abstract": "Systems for language understanding have become remarkably strong at\novercoming linguistic imperfections in tasks involving phrase matching or\nsimple reasoning. Yet, their accuracy drops dramatically as the number of\nreasoning steps increases. We present the first formal framework to study such\nempirical observations. It allows one to quantify the amount and effect of\nambiguity, redundancy, incompleteness, and inaccuracy that the use of language\nintroduces when representing a hidden conceptual space. The idea is to consider\ntwo interrelated spaces: a conceptual meaning space that is unambiguous and\ncomplete but hidden, and a linguistic space that captures a noisy grounding of\nthe meaning space in the words of a language---the level at which all systems,\nwhether neural or symbolic, operate. Applying this framework to a special class\nof multi-hop reasoning, namely the connectivity problem in graphs of\nrelationships between concepts, we derive rigorous intuitions and impossibility\nresults even under this simplified setting. For instance, if a query requires a\nmoderately large (logarithmic) number of hops in the meaning graph, no\nreasoning system operating over a noisy graph grounded in language is likely to\ncorrectly answer it. This highlights a fundamental barrier that extends to a\nbroader class of reasoning problems and systems, and suggests an alternative\npath forward: focusing on aligning the two spaces via richer representations,\nbefore investing in reasoning with many hops.", "published": "2019-01-08 21:19:34", "link": "http://arxiv.org/abs/1901.02522v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supervised Transfer Learning for Product Information Question Answering", "abstract": "Popular e-commerce websites such as Amazon offer community question answering\nsystems for users to pose product related questions and experienced customers\nmay provide answers voluntarily. In this paper, we show that the large volume\nof existing community question answering data can be beneficial when building a\nsystem for answering questions related to product facts and specifications. Our\nexperimental results demonstrate that the performance of a model for answering\nquestions related to products listed in the Home Depot website can be improved\nby a large margin via a simple transfer learning technique from an existing\nlarge-scale Amazon community question answering dataset. Transfer learning can\nresult in an increase of about 10% in accuracy in the experimental setting\nwhere we restrict the size of the data of the target task used for training. As\nan application of this work, we integrate the best performing model trained in\nthis work into a mobile-based shopping assistant and show its usefulness.", "published": "2019-01-08 22:24:59", "link": "http://arxiv.org/abs/1901.02539v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Computational Register Analysis and Synthesis", "abstract": "The study of register in computational language research has historically\nbeen divided into register analysis, seeking to determine the registerial\ncharacter of a text or corpus, and register synthesis, seeking to generate a\ntext in a desired register. This article surveys the different approaches to\nthese disparate tasks. Register synthesis has tended to use more theoretically\narticulated notions of register and genre than analysis work, which often seeks\nto categorize on the basis of intuitive and somewhat incoherent notions of\nprelabeled 'text types'. I argue that an integration of computational register\nanalysis and synthesis will benefit register studies as a whole, by enabling a\nnew large-scale research program in register studies. It will enable\ncomprehensive global mapping of functional language varieties in multiple\nlanguages, including the relationships between them. Furthermore, computational\nmethods together with high coverage systematically collected and analyzed data\nwill thus enable rigorous empirical validation and refinement of different\ntheories of register, which will have also implications for our understanding\nof linguistic variation in general.", "published": "2019-01-08 22:35:03", "link": "http://arxiv.org/abs/1901.02543v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Audio Captcha Recognition Using RastaPLP Features by SVM", "abstract": "Nowadays, CAPTCHAs are computer generated tests that human can pass but\ncurrent computer systems can not. They have common usage in various web\nservices in order to be able to detect a human from computer programs\nautonomously. In this way, owners can protect their web services from bots. In\naddition to visual CAPTCHAs which consist of distorted images, mostly test\nimages, that a user must write some description about that image, there are a\nsignificant amount of audio CAPTCHAs as well. Briefly, audio CAPTCHAs are sound\nfiles which consist of human sound under heavy noise where the speaker\npronounces a bunch of digits consecutively. Generally, in those sound files,\nthere are some periodic and non-periodic noises to get difficult to recognize\nthem with a program but not for a human listener. We gathered numerous randomly\ncollected audio file to train and then test them using our SVM algorithm to be\nable to extract digits out of each conversation.", "published": "2019-01-08 04:44:37", "link": "http://arxiv.org/abs/1901.02153v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML", "68T10"], "primary_category": "cs.LG"}
{"title": "Presence-absence estimation in audio recordings of tropical frog\n  communities", "abstract": "One non-invasive way to study frog communities is by analyzing long-term\nsamples of acoustic material containing calls. This immense task has been\noptimized by the development of Machine Learning tools to extract ecological\ninformation. We explored a likelihood-ratio audio detector based on Gaussian\nmixture model classification of 10 frog species, and applied it to estimate\npresence-absence in audio recordings from an actual amphibian monitoring\nperformed at Yasun\\'i National Park in the Ecuadorian Amazonia. A modified\nfilter-bank was used to extract 20 cepstral features that model the spectral\ncontent of frog calls. Experiments were carried out to investigate the\nhyperparameters and the minimum frog-call time needed to train an accurate GMM\nclassifier. With 64 Gaussians and 12 seconds of training time, the classifier\nachieved an average weighted error rate of 0.9% on the 10-fold cross-validation\nfor nine species classification, as compared to 3% with MFCC and 1.8% with PLP\nfeatures. For testing, 10 GMMs were trained using all the available\ntraining-validation dataset to study 23.5 hours in 141, 10-minute long samples\nof unidentified real-world audio recorded at two frog communities in 2001 with\nanalog equipment. To evaluate automatic presence-absence estimation, we\ncharacterized the audio samples with 10 binary variables each corresponding to\na frog species, and manually labeled a sub-set of 18 samples using headphones.\nA recall of 87.5% and precision of 100% with average accuracy of 96.66%\nsuggests good generalization ability of the algorithm, and provides evidence of\nthe validity of this approach to study real-world audio recorded in a tropical\nacoustic environment. Finally, we applied the algorithm to the available\ncorpus, and show its potentiality to gain insights into the temporal\nreproductive behavior of frogs.", "published": "2019-01-08 20:08:42", "link": "http://arxiv.org/abs/1901.02495v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
