{"title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "abstract": "This work compares concept models for cross-language retrieval: First, we\nadapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.\nExperiments with different weighting schemes show that a weighting method\nfavoring documents of similar length in both language sides gives best results.\nConsidering that both monolingual and multilingual Latent Dirichlet Allocation\n(LDA) behave alike when applied for such documents, we use a training corpus\nbuilt on Wikipedia where all documents are length-normalized and obtain\nimprovements over previously reported scores for LDA. Another focus of our work\nis on model combination. For this end we include Explicit Semantic Analysis\n(ESA) in the experiments. We observe that ESA is not competitive with LDA in a\nquery based retrieval task on CLEF 2000 data. The combination of machine\ntranslation with concept models increased performance by 21.1% map in\ncomparison to machine translation alone. Machine translation relies on parallel\ncorpora, which may not be available for many language pairs. We further explore\nhow much cross-lingual information can be carried over by a specific\ninformation source in Wikipedia, namely linked text. The best results are\nobtained using a language modeling approach, entirely without information from\nparallel corpora. The need for smoothing raises interesting questions on\nsoundness and efficiency. Link models capture only a certain kind of\ninformation and suggest weighting schemes to emphasize particular words. For a\ncombined model, another interesting question is therefore how to integrate\ndifferent weighting schemes. Using a very simple combination scheme, we obtain\nresults that compare favorably to previously reported results on the CLEF 2000\ndataset.", "published": "2014-01-10 08:50:54", "link": "http://arxiv.org/abs/1401.2258v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
