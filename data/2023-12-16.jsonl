{"title": "Continuous Prompt Generation from Linear Combination of Discrete Prompt\n  Embeddings", "abstract": "The wayward quality of continuous prompts stresses the importance of their\ninterpretability as unexpected and unpredictable behaviors appear following\ntraining, especially in the context of large language models automating\npeople-sensitive tasks such as resume screening. In this paper we present a\nnovel method of constructing continuous prompts via discrete prompt embeddings\nand evaluate improvements to continuous prompt interpretability and inference\naccuracy. For a set of manually designed discrete prompts $\\mathcal{D}$, which\nwe tokenize and embed each into tensor form, we train a model to predict the\nweights such that the linear combinations of those prompts correspond to higher\nperformance on natural language understanding tasks.", "published": "2023-12-16 05:02:06", "link": "http://arxiv.org/abs/2312.10323v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation", "abstract": "Recently, natural language generation (NLG) evaluation has shifted from a\nsingle-aspect to a multi-aspect paradigm, allowing for a more accurate\nassessment. Large language models (LLMs) achieve superior performance on\nvarious NLG evaluation tasks. However, current work often employs the LLM to\nindependently evaluate different aspects, which largely ignores the rich\ncorrelation between various aspects. To fill this research gap, in this work,\nwe propose an NLG evaluation metric called CoAScore. Powered by LLMs, the\nCoAScore utilizes multi-aspect knowledge through a CoA\n(\\textbf{C}hain-\\textbf{o}f-\\textbf{A}spects) prompting framework when\nassessing the quality of a certain aspect. Specifically, for a given aspect to\nevaluate, we first prompt the LLM to generate a chain of aspects that are\nrelevant to the target aspect and could be useful for the evaluation. We then\ncollect evaluation scores for each generated aspect, and finally, leverage the\nknowledge of these aspects to improve the evaluation of the target aspect. We\nevaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog\nresponse generation, etc) and nine aspects (e.g., overall quality, relevance,\ncoherence, etc). Our experimental findings highlight that, in comparison to\nindividual aspect evaluation, CoAScore exhibits a higher correlation with human\njudgments. This improvement significantly outperforms existing unsupervised\nevaluation metrics, whether for assessing overall quality or other aspects. We\nalso conducted extensive ablation studies to validate the effectiveness of the\nthree stages within the CoAScore framework and conducted case studies to show\nhow the LLM performs in these stages. Our code and scripts are available.", "published": "2023-12-16 06:57:20", "link": "http://arxiv.org/abs/2312.10355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via\n  Prompt Learning", "abstract": "Automatic psychological counseling requires mass of professional knowledge\nthat can be found in online counseling forums. Motivated by this, we propose\nK-ESConv, a novel prompt learning based knowledge injection method for\nemotional support dialogue system, transferring forum knowledge to response\ngeneration. We evaluate our model on an emotional support dataset ESConv, where\nthe model retrieves and incorporates knowledge from external professional\nemotional Q\\&A forum. Experiment results show that the proposed method\noutperforms existing baselines on both automatic evaluation and human\nevaluation, which shows that our approach significantly improves the\ncorrelation and diversity of responses and provides more comfort and better\nsuggestion for the seeker.", "published": "2023-12-16 08:10:10", "link": "http://arxiv.org/abs/2312.10371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Soft Contrastive Learning-based Prompt Model for Few-shot Sentiment\n  Analysis", "abstract": "Few-shot text classification has attracted great interest in both academia\nand industry due to the lack of labeled data in many fields. Different from\ngeneral text classification (e.g., topic classification), few-shot sentiment\nclassification is more challenging because the semantic distances among the\nclasses are more subtle. For instance, the semantic distances between the\nsentiment labels in a positive or negative polarity (e.g., ``love\" and ``joy\",\n``remorse\" and ``sadness\") are close, while the distances are large for the\nsentiment labels in two opposite polarities (e.g., ``love\" and ``sadness\"). To\naddress this problem, we propose a Soft Contrastive learning-based Prompt\n(\\texttt{SCP}) model for few-shot sentiment analysis. First, we design a\nsentiment-aware chain of thought prompt module to guide the model to predict\nthe sentiment from coarse grain to fine grain via a series of intermediate\nreasoning steps. Then, we propose a soft contrastive learning algorithm to take\nthe correlation of the labels into account. A series of experiments on several\nsentiment analysis datasets show the great advantages of \\texttt{SCP} by\ncomparing it with SOTA baselines (e.g., ChatGPT).", "published": "2023-12-16 15:17:28", "link": "http://arxiv.org/abs/2312.10479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature\n  Engineering Strategies for Arabic Dialect Identification", "abstract": "In this paper, we conduct an in-depth analysis of several key factors\ninfluencing the performance of Arabic Dialect Identification NADI'2023, with a\nspecific focus on the first subtask involving country-level dialect\nidentification. Our investigation encompasses the effects of surface\npreprocessing, morphological preprocessing, FastText vector model, and the\nweighted concatenation of TF-IDF features. For classification purposes, we\nemploy the Linear Support Vector Classification (LSVC) model. During the\nevaluation phase, our system demonstrates noteworthy results, achieving an F1\nscore of 62.51%. This achievement closely aligns with the average F1 scores\nattained by other systems submitted for the first subtask, which stands at\n72.91%.", "published": "2023-12-16 20:23:53", "link": "http://arxiv.org/abs/2312.10536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One-Shot Learning as Instruction Data Prospector for Large Language\n  Models", "abstract": "Contemporary practices in instruction tuning often hinge on enlarging data\nscaling without a clear strategy for ensuring data quality, inadvertently\nintroducing noise that may compromise model performance. To address this\nchallenge, we introduce \\textsc{Nuggets}, a novel and efficient methodology\nthat leverages one-shot learning to discern and select high-quality instruction\ndata from extensive datasets. \\textsc{Nuggets} assesses the potential of\nindividual instruction examples to act as effective one-shot learning\ninstances, thereby identifying those that can significantly improve performance\nacross diverse tasks. \\textsc{Nuggets} utilizes a scoring system based on the\nimpact of candidate examples on the perplexity of a diverse anchor set,\nfacilitating the selection of the most advantageous data for instruction\ntuning. Through comprehensive evaluations on two benchmarks, including MT-Bench\nand Alpaca-Eval, we show that instruction tuning with the top 1\\% of examples\ncurated by \\textsc{Nuggets} substantially outperforms conventional methods\nemploying the entire dataset.", "published": "2023-12-16 03:33:12", "link": "http://arxiv.org/abs/2312.10302v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?", "abstract": "Judging the equivalence between two SQL queries is a fundamental problem with\nmany practical applications in data management and SQL generation (i.e.,\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\nresearch community has reasoned about SQL equivalence for decades, it poses\nconsiderable difficulties and no complete solutions exist. Recently, Large\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\nquestion answering and solving mathematics challenges. In this paper, we study\nif LLMs can be used to determine the equivalence between SQL queries under two\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\nassist LLMs in generating high quality responses, we present two prompting\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\nquery on a simple database instance and then explore if a counterexample exists\nby modifying the database. The latter technique is used to evaluate the relaxed\nequivalence in which it asks LLMs to explain the queries and then compare if\nthey contain significant logical differences. Our experiments demonstrate using\nour techniques, LLMs is a promising tool to help data engineers in writing\nsemantically equivalent SQL queries, however challenges still persist, and is a\nbetter metric for evaluating SQL generation than the popular execution\naccuracy.", "published": "2023-12-16 05:01:23", "link": "http://arxiv.org/abs/2312.10321v4", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Perturbation-Invariant Adversarial Training for Neural Ranking Models:\n  Improving the Effectiveness-Robustness Trade-Off", "abstract": "Neural ranking models (NRMs) have shown great success in information\nretrieval (IR). But their predictions can easily be manipulated using\nadversarial examples, which are crafted by adding imperceptible perturbations\nto legitimate documents. This vulnerability raises significant concerns about\ntheir reliability and hinders the widespread deployment of NRMs. By\nincorporating adversarial examples into training data, adversarial training has\nbecome the de facto defense approach to adversarial attacks against NRMs.\nHowever, this defense mechanism is subject to a trade-off between effectiveness\nand adversarial robustness. In this study, we establish theoretical guarantees\nregarding the effectiveness-robustness trade-off in NRMs. We decompose the\nrobust ranking error into two components, i.e., a natural ranking error for\neffectiveness evaluation and a boundary ranking error for assessing adversarial\nrobustness. Then, we define the perturbation invariance of a ranking model and\nprove it to be a differentiable upper bound on the boundary ranking error for\nattainable computation. Informed by our theoretical analysis, we design a novel\n\\emph{perturbation-invariant adversarial training} (PIAT) method for ranking\nmodels to achieve a better effectiveness-robustness trade-off. We design a\nregularized surrogate loss, in which one term encourages the effectiveness to\nbe maximized while the regularization term encourages the output to be smooth,\nso as to improve adversarial robustness. Experimental results on several\nranking models demonstrate the superiority of PITA compared to existing\nadversarial defenses.", "published": "2023-12-16 05:38:39", "link": "http://arxiv.org/abs/2312.10329v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CONCSS: Contrastive-based Context Comprehension for Dialogue-appropriate\n  Prosody in Conversational Speech Synthesis", "abstract": "Conversational speech synthesis (CSS) incorporates historical dialogue as\nsupplementary information with the aim of generating speech that has\ndialogue-appropriate prosody. While previous methods have already delved into\nenhancing context comprehension, context representation still lacks effective\nrepresentation capabilities and context-sensitive discriminability. In this\npaper, we introduce a contrastive learning-based CSS framework, CONCSS. Within\nthis framework, we define an innovative pretext task specific to CSS that\nenables the model to perform self-supervised learning on unlabeled\nconversational datasets to boost the model's context understanding.\nAdditionally, we introduce a sampling strategy for negative sample augmentation\nto enhance context vectors' discriminability. This is the first attempt to\nintegrate contrastive learning into CSS. We conduct ablation studies on\ndifferent contrastive learning strategies and comprehensive experiments in\ncomparison with prior CSS systems. Results demonstrate that the synthesized\nspeech from our proposed method exhibits more contextually appropriate and\nsensitive prosody.", "published": "2023-12-16 07:05:16", "link": "http://arxiv.org/abs/2312.10358v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "From Dialogue to Diagram: Task and Relationship Extraction from Natural\n  Language for Accelerated Business Process Prototyping", "abstract": "The automatic transformation of verbose, natural language descriptions into\nstructured process models remains a challenge of significant complexity - This\npaper introduces a contemporary solution, where central to our approach, is the\nuse of dependency parsing and Named Entity Recognition (NER) for extracting key\nelements from textual descriptions. Additionally, we utilize\nSubject-Verb-Object (SVO) constructs for identifying action relationships and\nintegrate semantic analysis tools, including WordNet, for enriched contextual\nunderstanding. A novel aspect of our system is the application of neural\ncoreference resolution, integrated with the SpaCy framework, enhancing the\nprecision of entity linkage and anaphoric references. Furthermore, the system\nadeptly handles data transformation and visualization, converting extracted\ninformation into BPMN (Business Process Model and Notation) diagrams. This\nmethodology not only streamlines the process of capturing and representing\nbusiness workflows but also significantly reduces the manual effort and\npotential for error inherent in traditional modeling approaches.", "published": "2023-12-16 12:35:28", "link": "http://arxiv.org/abs/2312.10432v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Debiasing Multimodal Sarcasm Detection with Contrastive Learning", "abstract": "Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.", "published": "2023-12-16 16:14:50", "link": "http://arxiv.org/abs/2312.10493v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Cross-Linguistic Offensive Language Detection: BERT-Based Analysis of\n  Bengali, Assamese, & Bodo Conversational Hateful Content from Social Media", "abstract": "In today's age, social media reigns as the paramount communication platform,\nproviding individuals with the avenue to express their conjectures,\nintellectual propositions, and reflections. Unfortunately, this freedom often\ncomes with a downside as it facilitates the widespread proliferation of hate\nspeech and offensive content, leaving a deleterious impact on our world. Thus,\nit becomes essential to discern and eradicate such offensive material from the\nrealm of social media. This article delves into the comprehensive results and\nkey revelations from the HASOC-2023 offensive language identification result.\nThe primary emphasis is placed on the meticulous detection of hate speech\nwithin the linguistic domains of Bengali, Assamese, and Bodo, forming the\nframework for Task 4: Annihilate Hates. In this work, we used BERT models,\nincluding XML-Roberta, L3-cube, IndicBERT, BenglaBERT, and BanglaHateBERT. The\nresearch outcomes were promising and showed that XML-Roberta-lagre performed\nbetter than monolingual models in most cases. Our team 'TeamBD' achieved rank\n3rd for Task 4 - Assamese, & 5th for Bengali.", "published": "2023-12-16 19:59:07", "link": "http://arxiv.org/abs/2312.10528v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization\n  in Healthcare", "abstract": "In the era of modern healthcare, swiftly generating medical question\nsummaries is crucial for informed and timely patient care. Despite the\nincreasing complexity and volume of medical data, existing studies have focused\nsolely on text-based summarization, neglecting the integration of visual\ninformation. Recognizing the untapped potential of combining textual queries\nwith visual representations of medical conditions, we introduce the Multimodal\nMedical Question Summarization (MMQS) Dataset. This dataset, a major\ncontribution to our work, pairs medical queries with visual aids, facilitating\na richer and more nuanced understanding of patient needs. We also propose a\nframework, utilizing the power of Contrastive Language Image Pretraining(CLIP)\nand Large Language Models(LLMs), consisting of four modules that identify\nmedical disorders, generate relevant context, filter medical concepts, and\ncraft visually aware summaries. Our comprehensive framework harnesses the power\nof CLIP, a multimodal foundation model, and various general-purpose LLMs,\ncomprising four main modules: the medical disorder identification module, the\nrelevant context generation module, the context filtration module for\ndistilling relevant medical concepts and knowledge, and finally, a\ngeneral-purpose LLM to generate visually aware medical question summaries.\nLeveraging our MMQS dataset, we showcase how visual cues from images enhance\nthe generation of medically nuanced summaries. This multimodal approach not\nonly enhances the decision-making process in healthcare but also fosters a more\nnuanced understanding of patient queries, laying the groundwork for future\nresearch in personalized and responsive medical care", "published": "2023-12-16 03:02:05", "link": "http://arxiv.org/abs/2312.11541v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "When Parameter-efficient Tuning Meets General-purpose Vision-language\n  Models", "abstract": "Instruction tuning has shown promising potential for developing\ngeneral-purpose AI capabilities by using large-scale pre-trained models and\nboosts growing research to integrate multimodal information for creative\napplications. However, existing works still face two main limitations: the high\ntraining costs and heavy computing resource dependence of full model\nfine-tuning, and the lack of semantic information in instructions, which\nhinders multimodal alignment. Addressing these challenges, this paper proposes\na novel approach to utilize Parameter-Efficient Tuning for generAl-purpose\nvision-Language models, namely PETAL. PETAL revolutionizes the training process\nby requiring only 0.5% of the total parameters, achieved through a unique mode\napproximation technique, which significantly reduces the training costs and\nreliance on heavy computing resources. Furthermore, PETAL enhances the semantic\ndepth of instructions in two innovative ways: 1) by introducing adaptive\ninstruction mixture-of-experts(MOEs), and 2) by fortifying the score-based\nlinkage between parameter-efficient tuning and mutual information. Our\nextensive experiments across five multimodal downstream benchmarks reveal that\nPETAL not only outperforms current state-of-the-art methods in most scenarios\nbut also surpasses full fine-tuning models in effectiveness. Additionally, our\napproach demonstrates remarkable advantages in few-shot settings, backed by\ncomprehensive visualization analyses. Our source code is available at:\nhttps://github. com/melonking32/PETAL.", "published": "2023-12-16 17:13:08", "link": "http://arxiv.org/abs/2312.12458v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Collect and Connect Data Leaves to Feature Concepts: Interactive Graph\n  Generation Toward Well-being", "abstract": "Feature concepts and data leaves have been invented using datasets to foster\ncreative thoughts for creating well-being in daily life. The idea, simply put,\nis to attach selected and collected data leaves that are summaries of event\nflows to be discovered from corresponding datasets, on the target feature\nconcept representing the well-being aimed. A graph of existing or expected\ndatasets to be attached to a feature concept is generated semi-automatically.\nRather than sheer automated generative AI, our work addresses the process of\ngenerative artificial and natural intelligence to create the basis for data use\nand reuse.", "published": "2023-12-16 08:23:12", "link": "http://arxiv.org/abs/2312.10375v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Resolving Crash Bugs via Large Language Models: An Empirical Study", "abstract": "Crash bugs cause unexpected program behaviors or even termination, requiring\nhigh-priority resolution. However, manually resolving crash bugs is challenging\nand labor-intensive, and researchers have proposed various techniques for their\nautomated localization and repair. ChatGPT, a recent large language model\n(LLM), has garnered significant attention due to its exceptional performance\nacross various domains. This work performs the first investigation into\nChatGPT's capability in resolve real-world crash bugs, focusing on its\neffectiveness in both localizing and repairing code-related and\nenvironment-related crash bugs. Specifically, we initially assess ChatGPT's\nfundamental ability to resolve crash bugs with basic prompts in a single\niteration. We observe that ChatGPT performs better at resolving code-related\ncrash bugs compared to environment-related ones, and its primary challenge in\nresolution lies in inaccurate localization. Additionally, we explore ChatGPT's\npotential with various advanced prompts. Furthermore, by stimulating ChatGPT's\nself-planning, it methodically investigates each potential crash-causing\nenvironmental factor through proactive inquiry, ultimately identifying the root\ncause of the crash. Based on our findings, we propose IntDiagSolver, an\ninteraction methodology designed to facilitate precise crash bug resolution\nthrough continuous interaction with LLMs. Evaluating IntDiagSolver on multiple\nLLMs reveals consistent enhancement in the accuracy of crash bug resolution,\nincluding ChatGPT, Claude, and CodeLlama.", "published": "2023-12-16 13:41:04", "link": "http://arxiv.org/abs/2312.10448v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "RIGHT: Retrieval-augmented Generation for Mainstream Hashtag\n  Recommendation", "abstract": "Automatic mainstream hashtag recommendation aims to accurately provide users\nwith concise and popular topical hashtags before publication. Generally,\nmainstream hashtag recommendation faces challenges in the comprehensive\ndifficulty of newly posted tweets in response to new topics, and the accurate\nidentification of mainstream hashtags beyond semantic correctness. However,\nprevious retrieval-based methods based on a fixed predefined mainstream hashtag\nlist excel in producing mainstream hashtags, but fail to understand the\nconstant flow of up-to-date information. Conversely, generation-based methods\ndemonstrate a superior ability to comprehend newly posted tweets, but their\ncapacity is constrained to identifying mainstream hashtags without additional\nfeatures. Inspired by the recent success of the retrieval-augmented technique,\nin this work, we attempt to adopt this framework to combine the advantages of\nboth approaches. Meantime, with the help of the generator component, we could\nrethink how to further improve the quality of the retriever component at a low\ncost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag\nRecommender (RIGHT), which consists of three components: 1) a retriever seeks\nrelevant hashtags from the entire tweet-hashtags set; 2) a selector enhances\nmainstream identification by introducing global signals; and 3) a generator\nincorporates input tweets and selected hashtags to directly generate the\ndesired hashtags. The experimental results show that our method achieves\nsignificant improvements over state-of-the-art baselines. Moreover, RIGHT can\nbe easily integrated into large language models, improving the performance of\nChatGPT by more than 10%.", "published": "2023-12-16 14:47:03", "link": "http://arxiv.org/abs/2312.10466v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Paloma: A Benchmark for Evaluating Language Model Fit", "abstract": "Evaluations of language models (LMs) commonly report perplexity on monolithic\ndata held out from training. Implicitly or explicitly, this data is composed of\ndomains--varying distributions of language. We introduce Perplexity Analysis\nfor Language Model Assessment (Paloma), a benchmark to measure LM fit to 546\nEnglish and code domains, instead of assuming perplexity on one distribution\nextrapolates to others. We include two new datasets of the top 100 subreddits\n(e.g., r/depression on Reddit) and programming languages (e.g., Java on\nGitHub), both sources common in contemporary LMs. With our benchmark, we\nrelease 6 baseline 1B LMs carefully controlled to provide fair comparisons\nabout which pretraining corpus is best and code for others to apply those\ncontrols to their own experiments. Our case studies demonstrate how the\nfine-grained results from Paloma surface findings such as that models\npretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to\nmany domains or that loss is dominated by the most frequently occurring strings\nin the vocabulary.", "published": "2023-12-16 19:12:45", "link": "http://arxiv.org/abs/2312.10523v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SECap: Speech Emotion Captioning with Large Language Model", "abstract": "Speech emotions are crucial in human communication and are extensively used\nin fields like speech synthesis and natural language understanding. Most prior\nstudies, such as speech emotion recognition, have categorized speech emotions\ninto a fixed set of classes. Yet, emotions expressed in human speech are often\ncomplex, and categorizing them into predefined groups can be insufficient to\nadequately represent speech emotions. On the contrary, describing speech\nemotions directly by means of natural language may be a more effective\napproach. Regrettably, there are not many studies available that have focused\non this direction. Therefore, this paper proposes a speech emotion captioning\nframework named SECap, aiming at effectively describing speech emotions using\nnatural language. Owing to the impressive capabilities of large language models\nin language comprehension and text generation, SECap employs LLaMA as the text\ndecoder to allow the production of coherent speech emotion captions. In\naddition, SECap leverages HuBERT as the audio encoder to extract general speech\nfeatures and Q-Former as the Bridge-Net to provide LLaMA with emotion-related\nspeech features. To accomplish this, Q-Former utilizes mutual information\nlearning to disentangle emotion-related speech features and speech contents,\nwhile implementing contrastive learning to extract more emotion-related speech\nfeatures. The results of objective and subjective evaluations demonstrate that:\n1) the SECap framework outperforms the HTSAT-BART baseline in all objective\nevaluations; 2) SECap can generate high-quality speech emotion captions that\nattain performance on par with human annotators in subjective mean opinion\nscore tests.", "published": "2023-12-16 08:33:10", "link": "http://arxiv.org/abs/2312.10381v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Disentangled Representation Learning for Robust Target\n  Speech Extraction", "abstract": "Speech signals are inherently complex as they encompass both global acoustic\ncharacteristics and local semantic information. However, in the task of target\nspeech extraction, certain elements of global and local semantic information in\nthe reference speech, which are irrelevant to speaker identity, can lead to\nspeaker confusion within the speech extraction network. To overcome this\nchallenge, we propose a self-supervised disentangled representation learning\nmethod. Our approach tackles this issue through a two-phase process, utilizing\na reference speech encoding network and a global information disentanglement\nnetwork to gradually disentangle the speaker identity information from other\nirrelevant factors. We exclusively employ the disentangled speaker identity\ninformation to guide the speech extraction network. Moreover, we introduce the\nadaptive modulation Transformer to ensure that the acoustic representation of\nthe mixed signal remains undisturbed by the speaker embeddings. This component\nincorporates speaker embeddings as conditional information, facilitating\nnatural and efficient guidance for the speech extraction network. Experimental\nresults substantiate the effectiveness of our meticulously crafted approach,\nshowcasing a substantial reduction in the likelihood of speaker confusion.", "published": "2023-12-16 03:48:24", "link": "http://arxiv.org/abs/2312.10305v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MusER: Musical Element-Based Regularization for Generating Symbolic\n  Music with Emotion", "abstract": "Generating music with emotion is an important task in automatic music\ngeneration, in which emotion is evoked through a variety of musical elements\n(such as pitch and duration) that change over time and collaborate with each\nother. However, prior research on deep learning-based emotional music\ngeneration has rarely explored the contribution of different musical elements\nto emotions, let alone the deliberate manipulation of these elements to alter\nthe emotion of music, which is not conducive to fine-grained element-level\ncontrol over emotions. To address this gap, we present a novel approach\nemploying musical element-based regularization in the latent space to\ndisentangle distinct elements, investigate their roles in distinguishing\nemotions, and further manipulate elements to alter musical emotions.\nSpecifically, we propose a novel VQ-VAE-based model named MusER. MusER\nincorporates a regularization loss to enforce the correspondence between the\nmusical element sequences and the specific dimensions of latent variable\nsequences, providing a new solution for disentangling discrete sequences.\nTaking advantage of the disentangled latent vectors, a two-level decoding\nstrategy that includes multiple decoders attending to latent vectors with\ndifferent semantics is devised to better predict the elements. By visualizing\nlatent space, we conclude that MusER yields a disentangled and interpretable\nlatent space and gain insights into the contribution of distinct elements to\nthe emotional dimensions (i.e., arousal and valence). Experimental results\ndemonstrate that MusER outperforms the state-of-the-art models for generating\nemotional music in both objective and subjective evaluation. Besides, we\nrearrange music through element transfer and attempt to alter the emotion of\nmusic by transferring emotion-distinguishable elements.", "published": "2023-12-16 03:50:13", "link": "http://arxiv.org/abs/2312.10307v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Annotation-free Automatic Music Transcription with Scalable Synthetic\n  Data and Adversarial Domain Confusion", "abstract": "Automatic Music Transcription (AMT) is a vital technology in the field of\nmusic information processing. Despite recent enhancements in performance due to\nmachine learning techniques, current methods typically attain high accuracy in\ndomains where abundant annotated data is available. Addressing domains with low\nor no resources continues to be an unresolved challenge. To tackle this issue,\nwe propose a transcription model that does not require any MIDI-audio paired\ndata through the utilization of scalable synthetic audio for pre-training and\nadversarial domain confusion using unannotated real audio. In experiments, we\nevaluate methods under the real-world application scenario where training\ndatasets do not include the MIDI annotation of audio in the target data domain.\nOur proposed method achieved competitive performance relative to established\nbaseline methods, despite not utilizing any real datasets of paired MIDI-audio.\nAdditionally, ablation studies have provided insights into the scalability of\nthis approach and the forthcoming challenges in the field of AMT research.", "published": "2023-12-16 10:07:18", "link": "http://arxiv.org/abs/2312.10402v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Seq2seq for Automatic Paraphasia Detection in Aphasic Speech", "abstract": "Paraphasias are speech errors that are often characteristic of aphasia and\nthey represent an important signal in assessing disease severity and subtype.\nTraditionally, clinicians manually identify paraphasias by transcribing and\nanalyzing speech-language samples, which can be a time-consuming and burdensome\nprocess. Identifying paraphasias automatically can greatly help clinicians with\nthe transcription process and ultimately facilitate more efficient and\nconsistent aphasia assessment. Previous research has demonstrated the\nfeasibility of automatic paraphasia detection by training an automatic speech\nrecognition (ASR) model to extract transcripts and then training a separate\nparaphasia detection model on a set of hand-engineered features. In this paper,\nwe propose a novel, sequence-to-sequence (seq2seq) model that is trained\nend-to-end (E2E) to perform both ASR and paraphasia detection tasks. We show\nthat the proposed model outperforms the previous state-of-the-art approach for\nboth word-level and utterance-level paraphasia detection tasks and provide\nadditional follow-up evaluations to further understand the proposed model\nbehavior.", "published": "2023-12-16 18:22:37", "link": "http://arxiv.org/abs/2312.10518v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
