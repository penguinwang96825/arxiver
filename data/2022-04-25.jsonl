{"title": "Financial data analysis application via multi-strategy text processing", "abstract": "Maintaining financial system stability is critical to economic development,\nand early identification of risks and opportunities is essential. The financial\nindustry contains a wide variety of data, such as financial statements,\ncustomer information, stock trading data, news, etc. Massive heterogeneous data\ncalls for intelligent algorithms for machines to process and understand. This\npaper mainly focuses on the stock trading data and news about China A-share\ncompanies. We present a financial data analysis application, Financial Quotient\nPorter, designed to combine textual and numerical data by using a\nmulti-strategy data mining approach. Additionally, we present our efforts and\nplans in deep learning financial text processing application scenarios using\nnatural language processing (NLP) and knowledge graph (KG) technologies. Based\non KG technology, risks and opportunities can be identified from heterogeneous\ndata. NLP technology can be used to extract entities, relations, and events\nfrom unstructured text, and analyze market sentiment. Experimental results show\nmarket sentiments towards a company and an industry, as well as news-level\nassociations between companies.", "published": "2022-04-25 01:56:36", "link": "http://arxiv.org/abs/2204.11394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Headline Diagnosis: Manipulation of Content Farm Headlines", "abstract": "As technology grows faster, the news spreads through social media. In order\nto attract more readers and acquire additional profit, some news agencies\nreproduce massive news in a more appealing manner. Therefore, it is essential\nto accurately predict whether a news article is from official news agencies.\nThis work develops a headline classification based on Convoluted Neural Network\nto determine credibility of a news article. The model primarily focuses on\ninvestigating key factors from headlines. These factors include word\nsegmentation, part-of-speech tags, and sentiment features. With integrating\nthese features into the proposed classification model, the demonstrated\nevaluation achieves 93.99% for accuracy.", "published": "2022-04-25 02:55:33", "link": "http://arxiv.org/abs/2204.11408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It Takes Two Flints to Make a Fire: Multitask Learning of Neural\n  Relation and Explanation Classifiers", "abstract": "We propose an explainable approach for relation extraction that mitigates the\ntension between generalization and explainability by jointly training for the\ntwo goals. Our approach uses a multi-task learning architecture, which jointly\ntrains a classifier for relation extraction, and a sequence model that labels\nwords in the context of the relation that explain the decisions of the relation\nclassifier. We also convert the model outputs to rules to bring global\nexplanations to this approach. This sequence model is trained using a hybrid\nstrategy: supervised, when supervision from pre-existing patterns is available,\nand semi-supervised otherwise. In the latter situation, we treat the sequence\nmodel's labels as latent variables, and learn the best assignment that\nmaximizes the performance of the relation classifier. We evaluate the proposed\napproach on the two datasets and show that the sequence model provides labels\nthat serve as accurate explanations for the relation classifier's decisions,\nand, importantly, that the joint training generally improves the performance of\nthe relation classifier. We also evaluate the performance of the generated\nrules and show that the new rules are great add-on to the manual rules and\nbring the rule-based system much closer to the neural models.", "published": "2022-04-25 03:53:12", "link": "http://arxiv.org/abs/2204.11424v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-based Analysis of Advertising Appeals for Search Engine\n  Advertising", "abstract": "Writing an ad text that attracts people and persuades them to click or act is\nessential for the success of search engine advertising. Therefore, ad creators\nmust consider various aspects of advertising appeals (A$^3$) such as the price,\nproduct features, and quality. However, products and services exhibit unique\neffective A$^3$ for different industries. In this work, we focus on exploring\nthe effective A$^3$ for different industries with the aim of assisting the ad\ncreation process. To this end, we created a dataset of advertising appeals and\nused an existing model that detects various aspects for ad texts. Our\nexperiments demonstrated that different industries have their own effective\nA$^3$ and that the identification of the A$^3$ contributes to the estimation of\nadvertising performance.", "published": "2022-04-25 05:31:07", "link": "http://arxiv.org/abs/2204.11445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Islander: A Real-Time News Monitoring and Analysis System", "abstract": "With thousands of news articles from hundreds of sources distributed and\nshared every day, news consumption and information acquisition have been\nincreasingly difficult for readers. Additionally, the content of news articles\nis becoming catchy or even inciting to attract readership, harming the accuracy\nof news reporting. We present Islander, an online news analyzing system. The\nsystem allows users to browse trending topics with articles from multiple\nsources and perspectives. We define several metrics as proxies for news\nquality, and develop algorithms for automatic estimation. The quality\nestimation results are delivered through a web interface to newsreaders for\neasy access to news and information. The website is publicly available at\nhttps://islander.cc/", "published": "2022-04-25 06:20:49", "link": "http://arxiv.org/abs/2204.11457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local Hypergraph-based Nested Named Entity Recognition as Query-based\n  Sequence Labeling", "abstract": "There has been a growing academic interest in the recognition of nested named\nentities in many domains. We tackle the task with a novel local\nhypergraph-based method: We first propose start token candidates and generate\ncorresponding queries with their surrounding context, then use a query-based\nsequence labeling module to form a local hypergraph for each candidate. An end\ntoken estimator is used to correct the hypergraphs and get the final\npredictions. Compared to span-based approaches, our method is free of the high\ncomputation cost of span sampling and the risk of losing long entities.\nSequential prediction makes it easier to leverage information in word order\ninside nested structures, and richer representations are built with a local\nhypergraph. Experiments show that our proposed method outperforms all the\nprevious hypergraph-based and sequence labeling approaches with large margins\non all four nested datasets. It achieves a new state-of-the-art F1 score on the\nACE 2004 dataset and competitive F1 scores with previous state-of-the-art\nmethods on three other nested NER datasets: ACE 2005, GENIA, and KBP 2017.", "published": "2022-04-25 06:56:49", "link": "http://arxiv.org/abs/2204.11467v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Discriminator for Cooperative Text Generation?", "abstract": "Language models generate texts by successively predicting probability\ndistributions for next tokens given past ones. A growing field of interest\ntries to leverage external information in the decoding process so that the\ngenerated texts have desired properties, such as being more natural, non toxic,\nfaithful, or having a specific writing style. A solution is to use a classifier\nat each generation step, resulting in a cooperative environment where the\nclassifier guides the decoding of the language model distribution towards\nrelevant texts for the task at hand. In this paper, we examine three families\nof (transformer-based) discriminators for this specific task of cooperative\ndecoding: bidirectional, left-to-right and generative ones. We evaluate the\npros and cons of these different types of discriminators for cooperative\ngeneration, exploring respective accuracy on classification tasks along with\ntheir impact on the resulting sample quality and computational performances. We\nalso provide the code of a batched implementation of the powerful cooperative\ndecoding strategy used for our experiments, the Monte Carlo Tree Search,\nworking with each discriminator for Natural Language Generation.", "published": "2022-04-25 12:16:02", "link": "http://arxiv.org/abs/2204.11586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Causal News Corpus: Annotating Causal Relations in Event Sentences\n  from News", "abstract": "Despite the importance of understanding causality, corpora addressing causal\nrelations are limited. There is a discrepancy between existing annotation\nguidelines of event causality and conventional causality corpora that focus\nmore on linguistics. Many guidelines restrict themselves to include only\nexplicit relations or clause-based arguments. Therefore, we propose an\nannotation schema for event causality that addresses these concerns. We\nannotated 3,559 event sentences from protest event news with labels on whether\nit contains causal relations or not. Our corpus is known as the Causal News\nCorpus (CNC). A neural network built upon a state-of-the-art pre-trained\nlanguage model performed well with 81.20% F1 score on test set, and 83.46% in\n5-folds cross-validation. CNC is transferable across two external corpora:\nCausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of\nthese external datasets for training, we achieved up to approximately 64% F1 on\nthe CNC test set without additional fine-tuning. CNC also served as an\neffective training and pre-training dataset for the two external corpora.\nLastly, we demonstrate the difficulty of our task to the layman in a\ncrowd-sourced annotation exercise. Our annotated corpus is publicly available,\nproviding a valuable resource for causal text mining researchers.", "published": "2022-04-25 15:14:07", "link": "http://arxiv.org/abs/2204.11714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Self-Augmentation for Named Entity Recognition with Meta\n  Reweighting", "abstract": "Self-augmentation has received increasing research interest recently to\nimprove named entity recognition (NER) performance in low-resource scenarios.\nToken substitution and mixup are two feasible heterogeneous self-augmentation\ntechniques for NER that can achieve effective performance with certain\nspecialized efforts. Noticeably, self-augmentation may introduce potentially\nnoisy augmented data. Prior research has mainly resorted to heuristic\nrule-based constraints to reduce the noise for specific self-augmentation\nmethods individually. In this paper, we revisit these two typical\nself-augmentation methods for NER, and propose a unified meta-reweighting\nstrategy for them to achieve a natural integration. Our method is easily\nextensible, imposing little effort on a specific self-augmentation method.\nExperiments on different Chinese and English NER benchmarks show that our token\nsubstitution and mixup method, as well as their integration, can achieve\neffective performance improvement. Based on the meta-reweighting mechanism, we\ncan enhance the advantages of the self-augmentation techniques without much\nextra effort.", "published": "2022-04-25 02:51:55", "link": "http://arxiv.org/abs/2204.11406v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language to Code Translation with Execution", "abstract": "Generative models of code, pretrained on large corpora of programs, have\nshown great success in translating natural language to code (Chen et al., 2021;\nAustin et al., 2021; Li et al., 2022, inter alia). While these models do not\nexplicitly incorporate program semantics (i.e., execution results) during\ntraining, they are able to generate correct solutions for many problems.\nHowever, choosing a single correct program from a generated set for each\nproblem remains challenging. In this work, we introduce execution result--based\nminimum Bayes risk decoding (MBR-EXEC) for program selection and show that it\nimproves the few-shot performance of pretrained code models on\nnatural-language-to-code tasks. We select output programs from a generated\ncandidate set by marginalizing over program implementations that share the same\nsemantics. Because exact equivalence is intractable, we execute each program on\na small number of test inputs to approximate semantic equivalence. Across\ndatasets, execution or simulated execution significantly outperforms the\nmethods that do not involve program semantics. We find that MBR-EXEC\nconsistently improves over all execution-unaware selection methods, suggesting\nit as an effective approach for natural language to code translation. We\nopen-source our code at github.com/facebookresearch/mbr-exec and data at\ndl.fbaipublicfiles.com/mbr-exec/mbr-exec-release.zip", "published": "2022-04-25 06:06:08", "link": "http://arxiv.org/abs/2204.11454v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking\n  Inference", "abstract": "State-of-the-art neural models typically encode document-query pairs using\ncross-attention for re-ranking. To this end, models generally utilize an\nencoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach.\nThese paradigms, however, are not without flaws, i.e., running the model on all\nquery-document pairs at inference-time incurs a significant computational cost.\nThis paper proposes a new training and inference paradigm for re-ranking. We\npropose to finetune a pretrained encoder-decoder model using in the form of\ndocument to query generation. Subsequently, we show that this encoder-decoder\narchitecture can be decomposed into a decoder-only language model during\ninference. This results in significant inference time speedups since the\ndecoder-only architecture only needs to learn to interpret static encoder\nembeddings during inference. Our experiments show that this new paradigm\nachieves results that are comparable to the more expensive cross-attention\nranking approaches while being up to 6.8X faster. We believe this work paves\nthe way for more efficient neural rankers that leverage large pretrained\nmodels.", "published": "2022-04-25 06:26:29", "link": "http://arxiv.org/abs/2204.11458v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "\"Think Before You Speak\": Improving Multi-Action Dialog Policy by\n  Planning Single-Action Dialogs", "abstract": "Multi-action dialog policy (MADP), which generates multiple atomic dialog\nactions per turn, has been widely applied in task-oriented dialog systems to\nprovide expressive and efficient system responses. Existing MADP models usually\nimitate action combinations from the labeled multi-action dialog samples. Due\nto data limitations, they generalize poorly toward unseen dialog flows. While\ninteractive learning and reinforcement learning algorithms can be applied to\nincorporate external data sources of real users and user simulators, they take\nsignificant manual effort to build and suffer from instability. To address\nthese issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel\nmulti-task learning framework that learns single-action dialog dynamics to\nenhance multi-action prediction. Our PEDP method employs model-based planning\nfor conceiving what to express before deciding the current response through\nsimulating single-action dialogs. Experimental results on the MultiWOZ dataset\ndemonstrate that our fully supervised learning-based method achieves a solid\ntask success rate of 90.6%, improving 3% compared to the state-of-the-art\nmethods.", "published": "2022-04-25 07:55:53", "link": "http://arxiv.org/abs/2204.11481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A global analysis of metrics used for measuring performance in natural\n  language processing", "abstract": "Measuring the performance of natural language processing models is\nchallenging. Traditionally used metrics, such as BLEU and ROUGE, originally\ndevised for machine translation and summarization, have been shown to suffer\nfrom low correlation with human judgment and a lack of transferability to other\ntasks and languages. In the past 15 years, a wide range of alternative metrics\nhave been proposed. However, it is unclear to what extent this has had an\nimpact on NLP benchmarking efforts. Here we provide the first large-scale\ncross-sectional analysis of metrics used for measuring performance in natural\nlanguage processing. We curated, mapped and systematized more than 3500 machine\nlearning model performance results from the open repository 'Papers with Code'\nto enable a global and comprehensive analysis. Our results suggest that the\nlarge majority of natural language processing metrics currently used have\nproperties that may result in an inadequate reflection of a models'\nperformance. Furthermore, we found that ambiguities and inconsistencies in the\nreporting of metrics may lead to difficulties in interpreting and comparing\nmodel performances, impairing transparency and reproducibility in NLP research.", "published": "2022-04-25 11:41:50", "link": "http://arxiv.org/abs/2204.11574v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversational Question Answering on Heterogeneous Sources", "abstract": "Conversational question answering (ConvQA) tackles sequential information\nneeds where contexts in follow-up questions are left implicit. Current ConvQA\nsystems operate over homogeneous sources of information: either a knowledge\nbase (KB), or a text corpus, or a collection of tables. This paper addresses\nthe novel issue of jointly tapping into all of these together, this way\nboosting answer coverage and confidence. We present CONVINSE, an end-to-end\npipeline for ConvQA over heterogeneous sources, operating in three stages: i)\nlearning an explicit structured representation of an incoming question and its\nconversational context, ii) harnessing this frame-like representation to\nuniformly capture relevant evidences from KB, text, and tables, and iii)\nrunning a fusion-in-decoder model to generate the answer. We construct and\nrelease the first benchmark, ConvMix, for ConvQA over heterogeneous sources,\ncomprising 3000 real-user conversations with 16000 questions, along with entity\nannotations, completed question utterances, and question paraphrases.\nExperiments demonstrate the viability and advantages of our method, compared to\nstate-of-the-art baselines.", "published": "2022-04-25 14:13:44", "link": "http://arxiv.org/abs/2204.11677v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Discovering changes in birthing narratives during COVID-19", "abstract": "We investigate whether, and if so how, birthing narratives written by new\nparents on Reddit changed during COVID-19. Our results indicate that the\npresence of family members significantly decreased and themes related to\ninduced labor significantly increased in the narratives during COVID-19. Our\nwork builds upon recent research that analyze how new parents use Reddit to\ndescribe their birthing experiences.", "published": "2022-04-25 15:58:22", "link": "http://arxiv.org/abs/2204.11742v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Translation between Molecules and Natural Language", "abstract": "We present $\\textbf{MolT5}$ $-$ a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. $\\textbf{MolT5}$ allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Since\n$\\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the\nchemistry domain shortcoming of data scarcity. Furthermore, we consider several\nmetrics, including a new cross-modal embedding-based metric, to evaluate the\ntasks of molecule captioning and text-based molecule generation. Our results\nshow that $\\textbf{MolT5}$-based models are able to generate outputs, both\nmolecules and captions, which in many cases are high quality.", "published": "2022-04-25 17:48:09", "link": "http://arxiv.org/abs/2204.11817v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How can NLP Help Revitalize Endangered Languages? A Case Study and\n  Roadmap for the Cherokee Language", "abstract": "More than 43% of the languages spoken in the world are endangered, and\nlanguage loss currently occurs at an accelerated rate because of globalization\nand neocolonialism. Saving and revitalizing endangered languages has become\nvery important for maintaining the cultural diversity on our planet. In this\nwork, we focus on discussing how NLP can help revitalize endangered languages.\nWe first suggest three principles that may help NLP practitioners to foster\nmutual understanding and collaboration with language communities, and we\ndiscuss three ways in which NLP can potentially assist in language education.\nWe then take Cherokee, a severely-endangered Native American language, as a\ncase study. After reviewing the language's history, linguistic features, and\nexisting resources, we (in collaboration with Cherokee community members)\narrive at a few meaningful ways NLP practitioners can collaborate with\ncommunity partners. We suggest two approaches to enrich the Cherokee language's\nresources with machine-in-the-loop processing, and discuss several NLP tools\nthat people from the Cherokee community have shown interest in. We hope that\nour work serves not only to inform the NLP community about Cherokee, but also\nto provide inspiration for future work on endangered languages in general. Our\ncode and data will be open-sourced at\nhttps://github.com/ZhangShiyue/RevitalizeCherokee", "published": "2022-04-25 18:25:57", "link": "http://arxiv.org/abs/2204.11909v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce\n  Data Annotation Required in Visual Commonsense Tasks", "abstract": "Pre-trained language models have shown excellent results in few-shot learning\nscenarios using in-context learning. Although it is impressive, the size of\nlanguage models can be prohibitive to make them usable in on-device\napplications, such as sensors or smartphones. With smaller language models,\ntask-specific data annotation is needed to fine-tune the language model for a\nspecific purpose. However, data annotation can have a substantial financial and\ntime burden for small research groups, startups, and even companies. In this\npaper, we analyze different prompt-based fine-tuning techniques to improve\nresults on both language and multimodal causal transformer models. To evaluate\nour results, we use a dataset focusing on visual commonsense reasoning in time.\nOur results show that by simple model-agnostic prompt-based fine-tuning,\ncomparable results can be reached by only using 35%-40% of the fine-tuning\ntraining dataset. The proposed approaches result in significant time and\nfinancial savings. As the proposed methods make minimal architectural\nassumptions, other researchers can use the results in their transformer models\nwith minimal adaptations. We plan to release the source code freely to make it\neasier for the community to use and contribute to our work.", "published": "2022-04-25 18:56:55", "link": "http://arxiv.org/abs/2204.11922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "C3: Continued Pretraining with Contrastive Weak Supervision for Cross\n  Language Ad-Hoc Retrieval", "abstract": "Pretrained language models have improved effectiveness on numerous tasks,\nincluding ad-hoc retrieval. Recent work has shown that continuing to pretrain a\nlanguage model with auxiliary objectives before fine-tuning on the retrieval\ntask can further improve retrieval effectiveness. Unlike monolingual retrieval,\ndesigning an appropriate auxiliary task for cross-language mappings is\nchallenging. To address this challenge, we use comparable Wikipedia articles in\ndifferent languages to further pretrain off-the-shelf multilingual pretrained\nmodels before fine-tuning on the retrieval task. We show that our approach\nyields improvements in retrieval effectiveness.", "published": "2022-04-25 23:12:05", "link": "http://arxiv.org/abs/2204.11989v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Estimating the Personality of White-Box Language Models", "abstract": "Technology for open-ended language generation, a key application of\nartificial intelligence, has advanced to a great extent in recent years.\nLarge-scale language models, which are trained on large corpora of text, are\nbeing used in a wide range of applications everywhere, from virtual assistants\nto conversational bots. While these language models output fluent text,\nexisting research shows that these models can and do capture human biases. Many\nof these biases, especially those that could potentially cause harm, are being\nwell-investigated. On the other hand, studies that infer and change human\npersonality traits inherited by these models have been scarce or non-existent.\nOur work seeks to address this gap by exploring the personality traits of\nseveral large-scale language models designed for open-ended text generation and\nthe datasets used for training them. We build on the popular Big Five factors\nand develop robust methods that quantify the personality traits of these models\nand their underlying datasets. In particular, we trigger the models with a\nquestionnaire designed for personality assessment and subsequently classify the\ntext responses into quantifiable traits using a Zero-shot classifier. Our\nestimation scheme sheds light on an important anthropomorphic element found in\nsuch AI models and can help stakeholders decide how they should be applied as\nwell as how society could perceive them. Additionally, we examined approaches\nto alter these personalities, adding to our understanding of how AI models can\nbe adapted to specific contexts.", "published": "2022-04-25 23:53:53", "link": "http://arxiv.org/abs/2204.12000v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEMOS: Generating diverse human motions from textual descriptions", "abstract": "We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthe TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our webpage.", "published": "2022-04-25 14:53:06", "link": "http://arxiv.org/abs/2204.14109v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific\n  Human Gaze?", "abstract": "Learned self-attention functions in state-of-the-art NLP models often\ncorrelate with human attention. We investigate whether self-attention in\nlarge-scale pre-trained language models is as predictive of human eye fixation\npatterns during task-reading as classical cognitive models of human attention.\nWe compare attention functions across two task-specific reading datasets for\nsentiment analysis and relation extraction. We find the predictiveness of\nlarge-scale pre-trained self-attention for human attention depends on `what is\nin the tail', e.g., the syntactic nature of rare contexts. Further, we observe\nthat task-specific fine-tuning does not increase the correlation with human\ntask-specific reading. Through an input reduction experiment we give\ncomplementary insights on the sparsity and fidelity trade-off, showing that\nlower-entropy attention vectors are more faithful.", "published": "2022-04-25 08:23:13", "link": "http://arxiv.org/abs/2205.10226v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LoL: A Comparative Regularization Loss over Query Reformulation Losses\n  for Pseudo-Relevance Feedback", "abstract": "Pseudo-relevance feedback (PRF) has proven to be an effective query\nreformulation technique to improve retrieval accuracy. It aims to alleviate the\nmismatch of linguistic expressions between a query and its potential relevant\ndocuments. Existing PRF methods independently treat revised queries originating\nfrom the same query but using different numbers of feedback documents,\nresulting in severe query drift. Without comparing the effects of two different\nrevisions from the same query, a PRF model may incorrectly focus on the\nadditional irrelevant information increased in the more feedback, and thus\nreformulate a query that is less effective than the revision using the less\nfeedback. Ideally, if a PRF model can distinguish between irrelevant and\nrelevant information in the feedback, the more feedback documents there are,\nthe better the revised query will be. To bridge this gap, we propose the\nLoss-over-Loss (LoL) framework to compare the reformulation losses between\ndifferent revisions of the same query during training. Concretely, we revise an\noriginal query multiple times in parallel using different amounts of feedback\nand compute their reformulation losses. Then, we introduce an additional\nregularization loss on these reformulation losses to penalize revisions that\nuse more feedback but gain larger losses. With such comparative regularization,\nthe PRF model is expected to learn to suppress the extra increased irrelevant\ninformation by comparing the effects of different revised queries. Further, we\npresent a differentiable query reformulation method to implement this\nframework. This method revises queries in the vector space and directly\noptimizes the retrieval performance of query vectors, applicable for both\nsparse and dense retrieval models. Empirical evaluation demonstrates the\neffectiveness and robustness of our method for two typical sparse and dense\nretrieval models.", "published": "2022-04-25 10:42:50", "link": "http://arxiv.org/abs/2204.11545v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Speech Detection For Child-Clinician Conversations In Danish For\n  Low-Resource In-The-Wild Conditions: A Case Study", "abstract": "Use of speech models for automatic speech processing tasks can improve\nefficiency in the screening, analysis, diagnosis and treatment in medicine and\npsychiatry. However, the performance of pre-processing speech tasks like\nsegmentation and diarization can drop considerably on in-the-wild clinical\ndata, specifically when the target dataset comprises of atypical speech. In\nthis paper we study the performance of a pre-trained speech model on a dataset\ncomprising of child-clinician conversations in Danish with respect to the\nclassification threshold. Since we do not have access to sufficient labelled\ndata, we propose few-instance threshold adaptation, wherein we employ the first\nminutes of the speech conversation to obtain the optimum classification\nthreshold. Through our work in this paper, we learned that the model with\ndefault classification threshold performs worse on children from the patient\ngroup. Furthermore, the error rates of the model is directly correlated to the\nseverity of diagnosis in the patients. Lastly, our study on few-instance\nadaptation shows that three-minutes of clinician-child conversation is\nsufficient to obtain the optimum classification threshold.", "published": "2022-04-25 10:51:54", "link": "http://arxiv.org/abs/2204.11550v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Integrating Prior Knowledge in Post-hoc Explanations", "abstract": "In the field of eXplainable Artificial Intelligence (XAI), post-hoc\ninterpretability methods aim at explaining to a user the predictions of a\ntrained decision model. Integrating prior knowledge into such interpretability\nmethods aims at improving the explanation understandability and allowing for\npersonalised explanations adapted to each user. In this paper, we propose to\ndefine a cost function that explicitly integrates prior knowledge into the\ninterpretability objectives: we present a general framework for the\noptimization problem of post-hoc interpretability methods, and show that user\nknowledge can thus be integrated to any method by adding a compatibility term\nin the cost function. We instantiate the proposed formalization in the case of\ncounterfactual explanations and propose a new interpretability method called\nKnowledge Integration in Counterfactual Explanation (KICE) to optimize it. The\npaper performs an experimental study on several benchmark data sets to\ncharacterize the counterfactual instances generated by KICE, as compared to\nreference methods.", "published": "2022-04-25 13:09:53", "link": "http://arxiv.org/abs/2204.11634v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Survey on Word Meta-Embedding Learning", "abstract": "Meta-embedding (ME) learning is an emerging approach that attempts to learn\nmore accurate word embeddings given existing (source) word embeddings as the\nsole input.\n  Due to their ability to incorporate semantics from multiple source embeddings\nin a compact manner with superior performance, ME learning has gained\npopularity among practitioners in NLP.\n  To the best of our knowledge, there exist no prior systematic survey on ME\nlearning and this paper attempts to fill this need.\n  We classify ME learning methods according to multiple factors such as whether\nthey (a) operate on static or contextualised embeddings, (b) trained in an\nunsupervised manner or (c) fine-tuned for a particular task/domain.\n  Moreover, we discuss the limitations of existing ME learning methods and\nhighlight potential future research directions.", "published": "2022-04-25 13:51:48", "link": "http://arxiv.org/abs/2204.11660v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enable Deep Learning on Mobile Devices: Methods, Systems, and\n  Applications", "abstract": "Deep neural networks (DNNs) have achieved unprecedented success in the field\nof artificial intelligence (AI), including computer vision, natural language\nprocessing and speech recognition. However, their superior performance comes at\nthe considerable cost of computational complexity, which greatly hinders their\napplications in many resource-constrained devices, such as mobile phones and\nInternet of Things (IoT) devices. Therefore, methods and techniques that are\nable to lift the efficiency bottleneck while preserving the high accuracy of\nDNNs are in great demand in order to enable numerous edge AI applications. This\npaper provides an overview of efficient deep learning methods, systems and\napplications. We start from introducing popular model compression methods,\nincluding pruning, factorization, quantization as well as compact model design.\nTo reduce the large design cost of these manual solutions, we discuss the\nAutoML framework for each of them, such as neural architecture search (NAS) and\nautomated pruning and quantization. We then cover efficient on-device training\nto enable user customization based on the local data on mobile devices. Apart\nfrom general acceleration techniques, we also showcase several task-specific\naccelerations for point cloud, video and natural language processing by\nexploiting their spatial sparsity and temporal/token redundancy. Finally, to\nsupport all these algorithmic advancements, we introduce the efficient deep\nlearning system design from both software and hardware perspectives.", "published": "2022-04-25 16:52:48", "link": "http://arxiv.org/abs/2204.11786v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Can Rationalization Improve Robustness?", "abstract": "A growing line of work has investigated the development of neural NLP models\nthat can produce rationales--subsets of input that can explain their model\npredictions. In this paper, we ask whether such rationale models can also\nprovide robustness to adversarial attacks in addition to their interpretable\nnature. Since these models need to first generate rationales (\"rationalizer\")\nbefore making predictions (\"predictor\"), they have the potential to ignore\nnoise or adversarially added text by simply masking it out of the generated\nrationale. To this end, we systematically generate various types of 'AddText'\nattacks for both token and sentence-level rationalization tasks, and perform an\nextensive empirical evaluation of state-of-the-art rationale models across five\ndifferent tasks. Our experiments reveal that the rationale models show the\npromise to improve robustness, while they struggle in certain scenarios--when\nthe rationalizer is sensitive to positional bias or lexical choices of attack\ntext. Further, leveraging human rationale as supervision does not always\ntranslate to better performance. Our study is a first step towards exploring\nthe interplay between interpretability and robustness in the\nrationalize-then-predict framework.", "published": "2022-04-25 17:02:42", "link": "http://arxiv.org/abs/2204.11790v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-dimensional representation of infant and adult vocalization\n  acoustics", "abstract": "During the first years of life, infant vocalizations change considerably, as\ninfants develop the vocalization skills that enable them to produce speech\nsounds. Characterizations based on specific acoustic features, protophone\ncategories, or phonetic transcription are able to provide a representation of\nthe sounds infants make at different ages and in different contexts but do not\nfully describe how sounds are perceived by listeners, can be inefficient to\nobtain at large scales, and are difficult to visualize in two dimensions\nwithout additional statistical processing. Machine-learning-based approaches\nprovide the opportunity to complement these characterizations with purely\ndata-driven representations of infant sounds. Here, we use spectral features\nextraction and unsupervised machine learning, specifically Uniform Manifold\nApproximation (UMAP), to obtain a novel 2-dimensional spatial representation of\ninfant and caregiver vocalizations extracted from day-long home recordings.\nUMAP yields a continuous and well-distributed space conducive to certain\nanalyses of infant vocal development. For instance, we found that the\ndispersion of infant vocalization acoustics within the 2-D space over a day\nincreased from 3 to 9 months, and then decreased from 9 to 18 months. The\nmethod also permits analysis of similarity between infant and adult\nvocalizations, which also shows changes with infant age.", "published": "2022-04-25 17:58:13", "link": "http://arxiv.org/abs/2204.12279v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Supervised Attention in Sequence-to-Sequence Models for Speech\n  Recognition", "abstract": "Attention mechanism in sequence-to-sequence models is designed to model the\nalignments between acoustic features and output tokens in speech recognition.\nHowever, attention weights produced by models trained end to end do not always\ncorrespond well with actual alignments, and several studies have further argued\nthat attention weights might not even correspond well with the relevance\nattribution of frames. Regardless, visual similarity between attention weights\nand alignments is widely used during training as an indicator of the models\nquality. In this paper, we treat the correspondence between attention weights\nand alignments as a learning problem by imposing a supervised attention loss.\nExperiments have shown significant improved performance, suggesting that\nlearning the alignments well during training critically determines the\nperformance of sequence-to-sequence models.", "published": "2022-04-25 15:38:48", "link": "http://arxiv.org/abs/2204.12308v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Back-ends Selection for Deep Speaker Embeddings", "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) was the dominant and\nnecessary back-end for early speaker recognition approaches, like i-vector and\nx-vector. However, with the development of neural networks and margin-based\nloss functions, we can obtain deep speaker embeddings (DSEs), which have\nadvantages of increased inter-class separation and smaller intra-class\ndistances. In this case, PLDA seems unnecessary or even counterproductive for\nthe discriminative embeddings, and cosine similarity scoring (Cos) achieves\nbetter performance than PLDA in some situations. Motivated by this, in this\npaper, we systematically explore how to select back-ends (Cos or PLDA) for deep\nspeaker embeddings to achieve better performance in different situations. By\nanalyzing PLDA and the properties of DSEs extracted from models with different\nnumbers of segment-level layers, we make the conjecture that Cos is better in\nsame-domain situations and PLDA is better in cross-domain situations. We\nconduct experiments on VoxCeleb and NIST SRE datasets in four application\nsituations, single-/multi-domain training and same-/cross-domain test, to\nvalidate our conjecture and briefly explain why back-ends adaption algorithms\nwork.", "published": "2022-04-25 02:43:33", "link": "http://arxiv.org/abs/2204.11403v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph Convolutional Network Based Semi-Supervised Learning on\n  Multi-Speaker Meeting Data", "abstract": "Unsupervised clustering on speakers is becoming increasingly important for\nits potential uses in semi-supervised learning. In reality, we are often\npresented with enormous amounts of unlabeled data from multi-party meetings and\ndiscussions. An effective unsupervised clustering approach would allow us to\nsignificantly increase the amount of training data without additional costs for\nannotations. Recently, methods based on graph convolutional networks (GCN) have\nreceived growing attention for unsupervised clustering, as these methods\nexploit the connectivity patterns between nodes to improve learning\nperformance. In this work, we present a GCN-based approach for semi-supervised\nlearning. Given a pre-trained embedding extractor, a graph convolutional\nnetwork is trained on the labeled data and clusters unlabeled data with\n\"pseudo-labels\". We present a self-correcting training mechanism that\niteratively runs the cluster-train-correct process on pseudo-labels. We show\nthat this proposed approach effectively uses unlabeled data and improves\nspeaker recognition accuracy.", "published": "2022-04-25 08:30:26", "link": "http://arxiv.org/abs/2204.11501v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech", "abstract": "The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made\nfast and high-quality speech synthesis possible. However, current NAR-TTS\nmodels usually use phoneme sequence as input and thus cannot understand the\ntree-structured syntactic information of the input sequence, which hurts the\nprosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and\nlight-weight NAR-TTS model, which integrates tree-structured syntactic\ninformation into the prosody modeling modules in PortaSpeech\n\\cite{ren2021portaspeech}. Specifically, 1) We build a syntactic graph based on\nthe dependency tree of the input sentence, then process the text encoding with\na syntactic graph encoder to extract the syntactic information. 2) We\nincorporate the extracted syntactic encoding with PortaSpeech to improve the\nprosody prediction. 3) We introduce a multi-length discriminator to replace the\nflow-based post-net in PortaSpeech, which simplifies the training pipeline and\nimproves the inference speed, while keeping the naturalness of the generated\naudio. Experiments on three datasets not only show that the tree-structured\nsyntactic information grants SyntaSpeech the ability to synthesize better audio\nwith expressive prosody, but also demonstrate the generalization ability of\nSyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech.\nAblation studies demonstrate the necessity of each component in SyntaSpeech.\nSource code and audio samples are available at https://syntaspeech.github.io", "published": "2022-04-25 17:05:03", "link": "http://arxiv.org/abs/2204.11792v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parallel Synthesis for Autoregressive Speech Generation", "abstract": "Autoregressive neural vocoders have achieved outstanding performance in\nspeech synthesis tasks such as text-to-speech and voice conversion. An\nautoregressive vocoder predicts a sample at some time step conditioned on those\nat previous time steps. Though it synthesizes natural human speech, the\niterative generation inevitably makes the synthesis time proportional to the\nutterance length, leading to low efficiency. Many works were dedicated to\ngenerating the whole speech sequence in parallel and proposed GAN-based,\nflow-based, and score-based vocoders. This paper proposed a new thought for the\nautoregressive generation. Instead of iteratively predicting samples in a time\nsequence, the proposed model performs frequency-wise autoregressive generation\n(FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In\nFAR, a speech utterance is split into frequency subbands, and a subband is\ngenerated conditioned on the previously generated one. Similarly, in BAR, an\n8-bit quantized signal is generated iteratively from the first bit. By\nredesigning the autoregressive method to compute in domains other than the time\ndomain, the number of iterations in the proposed model is no longer\nproportional to the utterance length but to the number of subbands/bits,\nsignificantly increasing inference efficiency. Besides, a post-filter is\nemployed to sample signals from output posteriors; its training objective is\ndesigned based on the characteristics of the proposed methods. Experimental\nresults show that the proposed model can synthesize speech faster than\nreal-time without GPU acceleration. Compared with baseline vocoders, the\nproposed model achieves better MUSHRA results and shows good generalization\nability for unseen speakers and 44 kHz speech.", "published": "2022-04-25 17:33:22", "link": "http://arxiv.org/abs/2204.11806v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cleanformer: A multichannel array configuration-invariant neural\n  enhancement frontend for ASR in smart speakers", "abstract": "This work introduces the Cleanformer, a streaming multichannel neural based\nenhancement frontend for automatic speech recognition (ASR). This model has a\nconformer-based architecture which takes as inputs a single channel each of raw\nand enhanced signals, and uses self-attention to derive a time-frequency mask.\nThe enhanced input is generated by a multichannel adaptive noise cancellation\nalgorithm known as Speech Cleaner, which makes use of noise context to derive\nits filter taps. The time-frequency mask is applied to the noisy input to\nproduce enhanced output features for ASR. Detailed evaluations are presented\nwith simulated and re-recorded datasets in speech-based and non-speech-based\nnoise that show significant reduction in word error rate (WER) when using a\nlarge-scale state-of-the-art ASR model. It also will be shown to significantly\noutperform enhancement using a beamformer with ideal steering. The enhancement\nmodel is agnostic of the number of microphones and array configuration and,\ntherefore, can be used with different microphone arrays without the need for\nretraining. It is demonstrated that performance improves with more microphones,\nup to 4, with each additional microphone providing a smaller marginal benefit.\nSpecifically, for an SNR of -6dB, relative WER improvements of about 80\\% are\nshown in both noise conditions.", "published": "2022-04-25 19:25:01", "link": "http://arxiv.org/abs/2204.11933v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Measurement uncertainty and unicity of single number quantities\n  describing the spatial decay of speech level in open-plan offices", "abstract": "The ISO 3382-3 standard (2012) defines single number quantities (SNQs) which\nevaluate the acoustic quality of open-plan offices, but does not address the\nissue of measurement uncertainties. This study focusses on the SNQs present in\nthis standard related to spatial decay of speech, i.e. D 2S , L pAS4m and r c.\nThe aim is to provide additional information to the limited literature on the\nmeasurement uncertainties of these SNQs by use of both analytical developments\nand a stochastic approach based on simulations. The accuracy of the analytical\ndevelopments was studied thanks to simulations of the sound propagation within\na series of offices (1 layout, 16 acoustic configurations with different screen\nheights and different acoustic qualities of screens and ceiling). The SNQs\nobtained in the simulations cover a wide range: D 2S between 3.4 and 7.5 dB(A),\nL pAS4m between 40.6 and 51.9 dB(A) and r c between 2.5 and 14.7 m. Therefore,\nthe simulations are representative of a broad set of acoustic qualities.\nEstimated uncertainties have a magnitude of 0.4 dB(A) for D 2S and vary between\n0.4 and 0.7 dB(A) for L pAS4m and between 0.2 and 1.5 m for r c over a\nmeasurement path comprising 7 measurement positions. The simulations also raise\nthe question of describing the acoustic quality of an office using a single\nvalue for the indicators. The results of the simulations show that in some\ncases, D 2S values significantly depend on the measurement path, leading to a\nstrong increase of its measurement uncertainty if a unique value is to be\nconsidered.", "published": "2022-04-25 12:17:20", "link": "http://arxiv.org/abs/2204.12486v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time Speech Emotion Recognition Based on Syllable-Level Feature\n  Extraction", "abstract": "Speech emotion recognition systems have high prediction latency because of\nthe high computational requirements for deep learning models and low\ngeneralizability mainly because of the poor reliability of emotional\nmeasurements across multiple corpora. To solve these problems, we present a\nspeech emotion recognition system based on a reductionist approach of\ndecomposing and analyzing syllable-level features. Mel-spectrogram of an audio\nstream is decomposed into syllable-level components, which are then analyzed to\nextract statistical features. The proposed method uses formant attention,\nnoise-gate filtering, and rolling normalization contexts to increase feature\nprocessing speed and tolerance to adversity. A set of syllable-level formant\nfeatures is extracted and fed into a single hidden layer neural network that\nmakes predictions for each syllable as opposed to the conventional approach of\nusing a sophisticated deep learner to make sentence-wide predictions. The\nsyllable level predictions help to achieve the real-time latency and lower the\naggregated error in utterance level cross-corpus predictions. The experiments\non IEMOCAP (IE), MSP-Improv (MI), and RAVDESS (RA) databases show that the\nmethod archives real-time latency while predicting with state-of-the-art\ncross-corpus unweighted accuracy of 47.6% for IE to MI and 56.2% for MI to IE.", "published": "2022-04-25 00:20:28", "link": "http://arxiv.org/abs/2204.11382v3", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS", "I.5.2; I.5.5"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Scene Classification Using A Transfer Learning Based Joint\n  Optimization Strategy", "abstract": "Recently, audio-visual scene classification (AVSC) has attracted increasing\nattention from multidisciplinary communities. Previous studies tended to adopt\na pipeline training strategy, which uses well-trained visual and acoustic\nencoders to extract high-level representations (embeddings) first, then\nutilizes them to train the audio-visual classifier. In this way, the extracted\nembeddings are well suited for uni-modal classifiers, but not necessarily\nsuited for multi-modal ones. In this paper, we propose a joint training\nframework, using the acoustic features and raw images directly as inputs for\nthe AVSC task. Specifically, we retrieve the bottom layers of pre-trained image\nmodels as visual encoder, and jointly optimize the scene classifier and 1D-CNN\nbased acoustic encoder during training. We evaluate the approach on the\ndevelopment dataset of TAU Urban Audio-Visual Scenes 2021. The experimental\nresults show that our proposed approach achieves significant improvement over\nthe conventional pipeline training strategy. Moreover, our best single system\noutperforms previous state-of-the-art methods, yielding a log loss of 0.1517\nand accuracy of 94.59% on the official test fold.", "published": "2022-04-25 03:37:02", "link": "http://arxiv.org/abs/2204.11420v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Understanding Audio Features via Trainable Basis Functions", "abstract": "In this paper we explore the possibility of maximizing the information\nrepresented in spectrograms by making the spectrogram basis functions\ntrainable. We experiment with two different tasks, namely keyword spotting\n(KWS) and automatic speech recognition (ASR). For most neural network models,\nthe architecture and hyperparameters are typically fine-tuned and optimized in\nexperiments. Input features, however, are often treated as fixed. In the case\nof audio, signals can be mainly expressed in two main ways: raw waveforms\n(time-domain) or spectrograms (time-frequency-domain). In addition, different\nspectrogram types are often used and tailored to fit different applications. In\nour experiments, we allow for this tailoring directly as part of the network.\n  Our experimental results show that using trainable basis functions can boost\nthe accuracy of Keyword Spotting (KWS) by 14.2 percentage points, and lower the\nPhone Error Rate (PER) by 9.5 percentage points. Although models using\ntrainable basis functions become less effective as the model complexity\nincreases, the trained filter shapes could still provide us with insights on\nwhich frequency bins are important for that specific task. From our\nexperiments, we can conclude that trainable basis functions are a useful tool\nto boost the performance when the model complexity is limited.", "published": "2022-04-25 05:07:27", "link": "http://arxiv.org/abs/2204.11437v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "End-to-End Audio Strikes Back: Boosting Augmentations Towards An\n  Efficient Audio Classification Network", "abstract": "While efficient architectures and a plethora of augmentations for end-to-end\nimage classification tasks have been suggested and heavily investigated,\nstate-of-the-art techniques for audio classifications still rely on numerous\nrepresentations of the audio signal together with large architectures,\nfine-tuned from large datasets. By utilizing the inherited lightweight nature\nof audio and novel audio augmentations, we were able to present an efficient\nend-to-end network with strong generalization ability. Experiments on a variety\nof sound classification sets demonstrate the effectiveness and robustness of\nour approach, by achieving state-of-the-art results in various settings. Public\ncode is available at:\n\\href{https://github.com/Alibaba-MIIL/AudioClassfication}{this http url}", "published": "2022-04-25 07:50:45", "link": "http://arxiv.org/abs/2204.11479v5", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A quantum Fourier transform (QFT) based note detection algorithm", "abstract": "In quantum information processing (QIP), the quantum Fourier transform (QFT)\nhas a plethora of applications [1] [2] [3]: Shor's algorithm and phase\nestimation are just a few well-known examples. Shor's quantum factorization\nalgorithm, one of the most widely quoted quantum algorithms [4] [5] [6] relies\nheavily on the QFT and efficiently finds integer prime factors of large numbers\non quantum computers [4]. This seminal ground-breaking design for quantum\nalgorithms has triggered a cascade of viable alternatives to previously\nunsolvable problems on a classical computer that are potentially superior and\ncan run in polynomial time. In this work we examine the QFT's structure and\nimplementation for the creation of a quantum music note detection algorithm\nboth on a simulated and a real quantum computer. Though formal approaches [7]\n[1] [8] [9] exist for the verification of quantum algorithms, in this study we\nlimit ourselves to a simpler, symbolic representation which we validate using\nthe symbolic SymPy [10] [11] package which symbolically replicates quantum\ncomputing processes. The algorithm is then implemented as a quantum circuit,\nusing IBM's qiskit [12] library and finally period detection is exemplified on\nan actual single musical tone using a varying number of qubits.", "published": "2022-04-25 16:45:56", "link": "http://arxiv.org/abs/2204.11775v2", "categories": ["quant-ph", "cs.SD", "eess.AS"], "primary_category": "quant-ph"}
{"title": "On-demand compute reduction with stochastic wav2vec 2.0", "abstract": "Squeeze and Efficient Wav2vec (SEW) is a recently proposed architecture that\nsqueezes the input to the transformer encoder for compute efficient\npre-training and inference with wav2vec 2.0 (W2V2) models. In this work, we\npropose stochastic compression for on-demand compute reduction for W2V2 models.\nAs opposed to using a fixed squeeze factor, we sample it uniformly during\ntraining. We further introduce query and key-value pooling mechanisms that can\nbe applied to each transformer layer for further compression. Our results for\nmodels pre-trained on 960h Librispeech dataset and fine-tuned on 10h of\ntranscribed data show that using the same stochastic model, we get a smooth\ntrade-off between word error rate (WER) and inference time with only marginal\nWER degradation compared to the W2V2 and SEW models trained for a specific\nsetting. We further show that we can fine-tune the same stochastically\npre-trained model to a specific configuration to recover the WER difference\nresulting in significant computational savings on pre-training models from\nscratch.", "published": "2022-04-25 19:25:46", "link": "http://arxiv.org/abs/2204.11934v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Meta-AF: Meta-Learning for Adaptive Filters", "abstract": "Adaptive filtering algorithms are pervasive throughout signal processing and\nhave had a material impact on a wide variety of domains including audio\nprocessing, telecommunications, biomedical sensing, astrophysics and cosmology,\nseismology, and many more. Adaptive filters typically operate via specialized\nonline, iterative optimization methods such as least-mean squares or recursive\nleast squares and aim to process signals in unknown or nonstationary\nenvironments. Such algorithms, however, can be slow and laborious to develop,\nrequire domain expertise to create, and necessitate mathematical insight for\nimprovement. In this work, we seek to improve upon hand-derived adaptive filter\nalgorithms and present a comprehensive framework for learning online, adaptive\nsignal processing algorithms or update rules directly from data. To do so, we\nframe the development of adaptive filters as a meta-learning problem in the\ncontext of deep learning and use a form of self-supervision to learn online\niterative update rules for adaptive filters. To demonstrate our approach, we\nfocus on audio applications and systematically develop meta-learned adaptive\nfilters for five canonical audio problems including system identification,\nacoustic echo cancellation, blind equalization, multi-channel dereverberation,\nand beamforming. We compare our approach against common baselines and/or recent\nstate-of-the-art methods. We show we can learn high-performing adaptive filters\nthat operate in real-time and, in most cases, significantly outperform each\nmethod we compare against -- all using a single general-purpose configuration\nof our approach.", "published": "2022-04-25 19:44:24", "link": "http://arxiv.org/abs/2204.11942v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "On Machine Learning-Driven Surrogates for Sound Transmission Loss\n  Simulations", "abstract": "Surrogate models are data-based approximations of computationally expensive\nsimulations that enable efficient exploration of the model's design space and\ninformed decision-making in many physical domains. The usage of surrogate\nmodels in the vibroacoustic domain, however, is challenging due to the\nnon-smooth, complex behavior of wave phenomena. This paper investigates four\nMachine Learning (ML) approaches in the modelling of surrogates of Sound\nTransmission Loss (STL). Feature importance and feature engineering are used to\nimprove the models' accuracy while increasing their interpretability and\nphysical consistency. The transfer of the proposed techniques to other problems\nin the vibroacoustic domain and possible limitations of the models are\ndiscussed.", "published": "2022-04-25 08:04:25", "link": "http://arxiv.org/abs/2204.12290v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.med-ph"], "primary_category": "cs.SD"}
