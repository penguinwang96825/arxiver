{"title": "SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for\n  Abstractive Summarization", "abstract": "Sequence-to-sequence neural networks have recently achieved great success in\nabstractive summarization, especially through fine-tuning large pre-trained\nlanguage models on the downstream dataset. These models are typically decoded\nwith beam search to generate a unique summary. However, the search space is\nvery large, and with the exposure bias, such decoding is not optimal. In this\npaper, we show that it is possible to directly train a second-stage model\nperforming re-ranking on a set of summary candidates. Our mixture-of-experts\nSummaReranker learns to select a better candidate and consistently improves the\nperformance of the base model. With a base PEGASUS, we push ROUGE scores by\n5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%\non Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and\ncheckpoints will be available at https://github.com/ntunlp/SummaReranker.", "published": "2022-03-13 05:05:10", "link": "http://arxiv.org/abs/2203.06569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Prompt Tuning for Dialog State Tracking", "abstract": "A desirable dialog system should be able to continually learn new skills\nwithout forgetting old ones, and thereby adapt to new domains or tasks in its\nlife cycle. However, continually training a model often leads to a well-known\ncatastrophic forgetting issue. In this paper, we present Continual Prompt\nTuning, a parameter-efficient framework that not only avoids forgetting but\nalso enables knowledge transfer between tasks. To avoid forgetting, we only\nlearn and store a few prompt tokens' embeddings for each task while freezing\nthe backbone pre-trained model. To achieve bi-directional knowledge transfer\namong tasks, we propose several techniques (continual prompt initialization,\nquery fusion, and memory replay) to transfer knowledge from preceding tasks and\na memory-guided technique to transfer knowledge from subsequent tasks.\nExtensive experiments demonstrate the effectiveness and efficiency of our\nproposed method on continual learning for dialog state tracking, compared with\nstate-of-the-art baselines.", "published": "2022-03-13 13:22:41", "link": "http://arxiv.org/abs/2203.06654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciNLI: A Corpus for Natural Language Inference on Scientific Text", "abstract": "Existing Natural Language Inference (NLI) datasets, while being instrumental\nin the advancement of Natural Language Understanding (NLU) research, are not\nrelated to scientific text. In this paper, we introduce SciNLI, a large dataset\nfor NLI that captures the formality in scientific text and contains 107,412\nsentence pairs extracted from scholarly papers on NLP and computational\nlinguistics. Given that the text used in scientific literature differs vastly\nfrom the text used in everyday language both in terms of vocabulary and\nsentence structure, our dataset is well suited to serve as a benchmark for the\nevaluation of scientific NLU models. Our experiments show that SciNLI is harder\nto classify than the existing NLI datasets. Our best performing model with\nXLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%\nshowing that there is substantial room for improvement.", "published": "2022-03-13 18:23:37", "link": "http://arxiv.org/abs/2203.06728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProtagonistTagger -- a Tool for Entity Linkage of Persons in Texts from\n  Various Languages and Domains", "abstract": "Named entities recognition (NER) and disambiguation (NED) can add semantic\ncontext to the recognized named entities in texts. Named entity linkage in\ntexts, regardless of a domain, provides links between the entities mentioned in\nunstructured texts and individual instances of real-world objects. In this\nposter, we present a tool - protagonistTagger - for person NER and NED in\ntexts. The tool was tested on texts extracted from classic English novels and\nPolish Internet news. The tool's performance (both precision and recall)\nfluctuates between 78% and even 88%.", "published": "2022-03-13 19:50:23", "link": "http://arxiv.org/abs/2203.06746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pruned Graph Neural Network for Short Story Ordering", "abstract": "Text coherence is a fundamental problem in natural language generation and\nunderstanding. Organizing sentences into an order that maximizes coherence is\nknown as sentence ordering. This paper is proposing a new approach based on the\ngraph neural network approach to encode a set of sentences and learn orderings\nof short stories. We propose a new method for constructing sentence-entity\ngraphs of short stories to create the edges between sentences and reduce noise\nin our graph by replacing the pronouns with their referring entities. We\nimprove the sentence ordering by introducing an aggregation method based on\nmajority voting of state-of-the-art methods and our proposed one. Our approach\nemploys a BERT-based model to learn semantic representations of the sentences.\nThe results demonstrate that the proposed method significantly outperforms\nexisting baselines on a corpus of short stories with a new state-of-the-art\nperformance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau)\nmetrics. More precisely, our method increases PMR and Tau criteria by more than\n5% and 4.3%, respectively. These outcomes highlight the benefit of forming the\nedges between sentences based on their cosine similarity. We also observe that\nreplacing pronouns with their referring entities effectively encodes sentences\nin sentence-entity graphs.", "published": "2022-03-13 22:25:17", "link": "http://arxiv.org/abs/2203.06778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Impact of COVID-19 on Education by Social Network\n  Mining", "abstract": "The Covid-19 virus has been one of the most discussed topics on social\nnetworks in 2020 and 2021 and has affected the classic educational paradigm,\nworldwide. In this research, many tweets related to the Covid-19 virus and\neducation are considered and geo-tagged with the help of the GeoNames\ngeographic database, which contains a large number of place names. To detect\nthe feeling of users, sentiment analysis is performed using the RoBERTa\nlanguage-based model. Finally, we obtain the trends of frequency of total,\npositive, and negative tweets for countries with a high number of Covid-19\nconfirmed cases. Investigating the results reveals a correlation between the\ntrends of tweet frequency and the official statistic of confirmed cases for\nseveral countries.", "published": "2022-03-13 06:23:16", "link": "http://arxiv.org/abs/2203.06584v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Informative Causality Extraction from Medical Literature via\n  Dependency-tree based Patterns", "abstract": "Extracting cause-effect entities from medical literature is an important task\nin medical information retrieval. A solution for solving this task can be used\nfor compilation of various causality relations, such as, causality between\ndisease and symptoms, between medications and side effects, between genes and\ndiseases, etc. Existing solutions for extracting cause-effect entities work\nwell for sentences where the cause and the effect phrases are name entities,\nsingle-word nouns, or noun phrases consisting of two to three words.\nUnfortunately, in medical literature, cause and effect phrases in a sentence\nare not simply nouns or noun phrases, rather they are complex phrases\nconsisting of several words, and existing methods fail to correctly extract the\ncause and effect entities in such sentences. Partial extraction of cause and\neffect entities conveys poor quality, non informative, and often, contradictory\nfacts, comparing to the one intended in the given sentence. In this work, we\nsolve this problem by designing an unsupervised method for cause and effect\nphrase extraction, PatternCausality, which is specifically suitable for the\nmedical literature. Our proposed approach first uses a collection of\ncause-effect dependency patterns as template to extract head words of cause and\neffect phrases and then it uses a novel phrase extraction method to obtain\ncomplete and meaningful cause and effect phrases from a sentence. Experiments\non a cause-effect dataset built from sentences from PubMed articles show that\nfor extracting cause and effect entities, PatternCausality is substantially\nbetter than the existing methods with an order of magnitude improvement in the\nF-score metric over the best of the existing methods.", "published": "2022-03-13 07:26:04", "link": "http://arxiv.org/abs/2203.06592v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Study and Analysis of Bengali Folklore with Natural\n  Language Processing Systems", "abstract": "Folklore, a solid branch of folk literature, is the hallmark of any nation or\nany society. Such as oral tradition; as proverbs or jokes, it also includes\nmaterial culture as well as traditional folk beliefs, and various customs.\nBengali folklore is as rich in-depth as it is amazing. Nevertheless, in the\nwomb of time, it is determined to sustain its existence. Therefore, our aim in\nthis study is to make our rich folklore more comprehensible to everyone in a\nmore sophisticated computational way. Some studies concluded various aspects of\nthe Bengali language with NLP. Our proposed model is to be specific for Bengali\nfolklore. Technically, it will be the first step towards Bengali natural\nlanguage processing for studying and analyzing the folklore of Bengal.", "published": "2022-03-13 09:36:55", "link": "http://arxiv.org/abs/2203.06607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Personalized Intelligence at Scale", "abstract": "Personalized Intelligence (PI) is the problem of providing customized AI\nexperiences tailored to each individual user. In many applications, PI is\npreferred or even required. Existing personalization approaches involve\nfine-tuning pre-trained models to create new customized models. However, these\napproaches require a significant amount of computation to train, scaling with\nmodel size and the number of users, inhibiting PI to be realized widely. In\nthis work, we introduce a novel model architecture and training/inference\nframework to enable Personalized Intelligence at scale. We achieve this by\nattaching a Personalization Head (PH) to pre-trained language models (LM).\nDuring training, the base LMs are frozen and only the parameters in PH are\nupdated and are unique per user. This results in significantly smaller overall\nmodel sizes and training cost than traditional fine-tuning approaches when\nscaled across many users. We evaluate PHs on academia and industry-focused\ndatasets and show that the PHs outperform zeroshot baseline in F1 score and are\nsignificantly more scalable than traditional fine-tuning approaches. We\nidentify key factors required for effective PH design and training.", "published": "2022-03-13 14:49:17", "link": "http://arxiv.org/abs/2203.06668v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Summarizing a virtual robot's past actions in natural language", "abstract": "We propose and demonstrate the task of giving natural language summaries of\nthe actions of a robotic agent in a virtual environment. We explain why such a\ntask is important, what makes it difficult, and discuss how it might be\naddressed. To encourage others to work on this, we show how a popular existing\ndataset that matches robot actions with natural language descriptions designed\nfor an instruction following task can be repurposed to serve as a training\nground for robot action summarization work. We propose and test several methods\nof learning to generate such summaries, starting from either egocentric video\nframes of the robot taking actions or intermediate text representations of the\nactions used by an automatic planner. We provide quantitative and qualitative\nevaluations of our results, which can serve as a baseline for future work.", "published": "2022-03-13 15:00:46", "link": "http://arxiv.org/abs/2203.06671v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Towards Visual-Prompt Temporal Answering Grounding in Medical\n  Instructional Video", "abstract": "The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.", "published": "2022-03-13 14:42:53", "link": "http://arxiv.org/abs/2203.06667v6", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Spectral Modification Based Data Augmentation For Improving End-to-End\n  ASR For Children's Speech", "abstract": "Training a robust Automatic Speech Recognition (ASR) system for children's\nspeech recognition is a challenging task due to inherent differences in\nacoustic attributes of adult and child speech and scarcity of publicly\navailable children's speech dataset. In this paper, a novel segmental spectrum\nwarping and perturbations in formant energy are introduced, to generate a\nchildren-like speech spectrum from that of an adult's speech spectrum. Then,\nthis modified adult spectrum is used as augmented data to improve end-to-end\nASR systems for children's speech recognition. The proposed data augmentation\nmethods give 6.5% and 6.1% relative reduction in WER on children dev and test\nsets respectively, compared to the vocal tract length perturbation (VTLP)\nbaseline system trained on Librispeech 100 hours adult speech dataset. When\nchildren's speech data is added in training with Librispeech set, it gives a\n3.7 % and 5.1% relative reduction in WER, compared to the VTLP baseline system.", "published": "2022-03-13 08:46:31", "link": "http://arxiv.org/abs/2203.06600v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Bi-Sampling Approach to Classify Music Mood leveraging Raga-Rasa\n  Association in Indian Classical Music", "abstract": "The impact of Music on the mood or emotion of the listener is a\nwell-researched area in human psychology and behavioral science. In Indian\nclassical music, ragas are the melodic structure that defines the various\nstyles and forms of the music. Each raga has been found to evoke a specific\nemotion in the listener. With the advent of advanced capabilities of audio\nsignal processing and the application of machine learning, the demand for\nintelligent music classifiers and recommenders has received increased\nattention, especially in the 'Music as a service' cloud applications. This\npaper explores a novel framework to leverage the raga-rasa association in\nIndian classical Music to build an intelligent classifier and its application\nin music recommendation system based on user's current mood and the mood they\naspire to be in.", "published": "2022-03-13 06:12:27", "link": "http://arxiv.org/abs/2203.06583v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio\n  Classification", "abstract": "Audio classification is an active research area with a wide range of\napplications. Over the past decade, convolutional neural networks (CNNs) have\nbeen the de-facto standard building block for end-to-end audio classification\nmodels. Recently, neural networks based solely on self-attention mechanisms\nsuch as the Audio Spectrogram Transformer (AST) have been shown to outperform\nCNNs. In this paper, we find an intriguing interaction between the two very\ndifferent models - CNN and AST models are good teachers for each other. When we\nuse either of them as the teacher and train the other model as the student via\nknowledge distillation (KD), the performance of the student model noticeably\nimproves, and in many cases, is better than the teacher model. In our\nexperiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD)\nmethod we achieve new state-of-the-art performance on FSD50K, AudioSet, and\nESC-50.", "published": "2022-03-13 21:14:04", "link": "http://arxiv.org/abs/2203.06760v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
