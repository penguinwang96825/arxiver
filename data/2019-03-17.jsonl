{"title": "Audio De-identification: A New Entity Recognition Task", "abstract": "Named Entity Recognition (NER) has been mostly studied in the context of\nwritten text. Specifically, NER is an important step in de-identification\n(de-ID) of medical records, many of which are recorded conversations between a\npatient and a doctor. In such recordings, audio spans with personal information\nshould be redacted, similar to the redaction of sensitive character spans in\nde-ID for written text. The application of NER in the context of audio\nde-identification has yet to be fully investigated. To this end, we define the\ntask of audio de-ID, in which audio spans with entity mentions should be\ndetected. We then present our pipeline for this task, which involves Automatic\nSpeech Recognition (ASR), NER on the transcript text, and text-to-audio\nalignment. Finally, we introduce a novel metric for audio de-ID and a new\nevaluation benchmark consisting of a large labeled segment of the Switchboard\nand Fisher audio datasets and detail our pipeline's results on it.", "published": "2019-03-17 07:07:44", "link": "http://arxiv.org/abs/1903.07037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering via Web Extracted Tables and Pipelined Models", "abstract": "In this paper, we describe a dataset and baseline result for a question\nanswering that utilizes web tables. It contains commonly asked questions on the\nweb and their corresponding answers found in tables on websites. Our dataset is\nnovel in that every question is paired with a table of a different signature.\nIn particular, the dataset contains two classes of tables: entity-instance\ntables and the key-value tables. Each QA instance comprises a table of either\nkind, a natural language question, and a corresponding structured SQL query. We\nbuild our model by dividing question answering into several tasks, including\ntable retrieval and question element classification, and conduct experiments to\nmeasure the performance of each task. We extract various features specific to\neach task and compose a full pipeline which constructs the SQL query from its\nparts. Our work provides qualitative results and error analysis for each task,\nand identifies in detail the reasoning required to generate SQL expressions\nfrom natural language questions. This analysis of reasoning informs future\nmodels based on neural machine learning.", "published": "2019-03-17 15:46:05", "link": "http://arxiv.org/abs/1903.07113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Guided Variational Autoencoders for Text Generation", "abstract": "We propose a topic-guided variational autoencoder (TGVAE) model for text\ngeneration. Distinct from existing variational autoencoder (VAE) based\napproaches, which assume a simple Gaussian prior for the latent code, our model\nspecifies the prior as a Gaussian mixture model (GMM) parametrized by a neural\ntopic module. Each mixture component corresponds to a latent topic, which\nprovides guidance to generate sentences under the topic. The neural topic\nmodule and the VAE-based neural sequence module in our model are learned\njointly. In particular, a sequence of invertible Householder transformations is\napplied to endow the approximate posterior of the latent code with high\nflexibility during model inference. Experimental results show that our TGVAE\noutperforms alternative approaches on both unconditional and conditional text\ngeneration, which can generate semantically-meaningful sentences with various\ntopics.", "published": "2019-03-17 17:42:29", "link": "http://arxiv.org/abs/1903.07137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Technical notes: Syntax-aware Representation Learning With Pointer\n  Networks", "abstract": "This is a work-in-progress report, which aims to share preliminary results of\na novel sequence-to-sequence schema for dependency parsing that relies on a\ncombination of a BiLSTM and two Pointer Networks (Vinyals et al., 2015), in\nwhich the final softmax function has been replaced with the logistic\nregression. The two pointer networks co-operate to develop a latent syntactic\nknowledge, by learning the lexical properties of \"selection\" and the lexical\nproperties of \"selectability\", respectively. At the moment and without\nfine-tuning, the parser implementation gets a UAS of 93.14% on the English\nPenn-treebank (Marcus et al., 1993) annotated with Stanford Dependencies: 2-3%\nunder the SOTA but yet attractive as a baseline of the approach.", "published": "2019-03-17 20:26:56", "link": "http://arxiv.org/abs/1903.07161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Missing Ingredient in Zero-Shot Neural Machine Translation", "abstract": "Multilingual Neural Machine Translation (NMT) models are capable of\ntranslating between multiple source and target languages. Despite various\napproaches to train such models, they have difficulty with zero-shot\ntranslation: translating between language pairs that were not together seen\nduring training. In this paper we first diagnose why state-of-the-art\nmultilingual NMT models that rely purely on parameter sharing, fail to\ngeneralize to unseen language pairs. We then propose auxiliary losses on the\nNMT encoder that impose representational invariance across languages. Our\nsimple approach vastly improves zero-shot translation quality without\nregressing on supervised directions. For the first time, on WMT14\nEnglish-FrenchGerman, we achieve zero-shot performance that is on par with\npivoting. We also demonstrate the easy scalability of our approach to multiple\nlanguages on the IWSLT 2017 shared task.", "published": "2019-03-17 14:01:53", "link": "http://arxiv.org/abs/1903.07091v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
