{"title": "A Cascade Approach to Neural Abstractive Summarization with Content\n  Selection and Fusion", "abstract": "We present an empirical study in favor of a cascade architecture to neural\ntext summarization. Summarization practices vary widely but few other than news\nsummarization can provide a sufficient amount of training data enough to meet\nthe requirement of end-to-end neural abstractive systems which perform content\nselection and surface realization jointly to generate abstracts. Such systems\nalso pose a challenge to summarization evaluation, as they force content\nselection to be evaluated along with text generation, yet evaluation of the\nlatter remains an unsolved problem. In this paper, we present empirical results\nshowing that the performance of a cascaded pipeline that separately identifies\nimportant content pieces and stitches them together into a coherent text is\ncomparable to or outranks that of end-to-end systems, whereas a pipeline\narchitecture allows for flexible content selection. We finally discuss how we\ncan take advantage of a cascaded pipeline in neural text summarization and shed\nlight on important directions for future research.", "published": "2020-10-08 01:49:16", "link": "http://arxiv.org/abs/2010.03722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARADE: A New Dataset for Paraphrase Identification Requiring Computer\n  Science Domain Knowledge", "abstract": "We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.", "published": "2020-10-08 02:01:31", "link": "http://arxiv.org/abs/2010.03725v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Fuse Sentences with Transformers for Summarization", "abstract": "The ability to fuse sentences is highly attractive for summarization systems\nbecause it is an essential step to produce succinct abstracts. However, to\ndate, summarizers can fail on fusing sentences. They tend to produce few\nsummary sentences by fusion or generate incorrect fusions that lead the summary\nto fail to retain the original meaning. In this paper, we explore the ability\nof Transformers to fuse sentences and propose novel algorithms to enhance their\nability to perform sentence fusion by leveraging the knowledge of points of\ncorrespondence between sentences. Through extensive experiments, we investigate\nthe effects of different design choices on Transformer's performance. Our\nfindings highlight the importance of modeling points of correspondence between\nsentences for effective sentence fusion.", "published": "2020-10-08 02:01:35", "link": "http://arxiv.org/abs/2010.03726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Discourse Rewards for Document-Level Neural Machine\n  Translation", "abstract": "Document-level machine translation focuses on the translation of entire\ndocuments from a source to a target language. It is widely regarded as a\nchallenging task since the translation of the individual sentences in the\ndocument needs to retain aspects of the discourse at document level. However,\ndocument-level translation models are usually not trained to explicitly ensure\ndiscourse quality. Therefore, in this paper we propose a training approach that\nexplicitly optimizes two established discourse metrics, lexical cohesion (LC)\nand coherence (COH), by using a reinforcement learning objective. Experiments\nover four different language pairs and three translation domains have shown\nthat our training approach has been able to achieve more cohesive and coherent\ndocument translations than other competitive approaches, yet without\ncompromising the faithfulness to the reference translation. In the case of the\nZh-En language pair, our method has achieved an improvement of 2.46 percentage\npoints (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time\nimproving 0.63 pp in BLEU score and 0.47 pp in F_BERT.", "published": "2020-10-08 02:26:22", "link": "http://arxiv.org/abs/2010.03732v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow-to-Deep Training for Neural Machine Translation", "abstract": "Deep encoders have been proven to be effective in improving neural machine\ntranslation (NMT) systems, but training an extremely deep encoder is time\nconsuming. Moreover, why deep models help NMT is an open question. In this\npaper, we investigate the behavior of a well-tuned deep Transformer system. We\nfind that stacking layers is helpful in improving the representation ability of\nNMT models and adjacent layers perform similarly. This inspires us to develop a\nshallow-to-deep training method that learns deep models by stacking shallow\nmodels. In this way, we successfully train a Transformer system with a 54-layer\nencoder. Experimental results on WMT'16 English-German and WMT'14\nEnglish-French translation tasks show that it is $1.4$ $\\times$ faster than\ntraining from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two\ntasks. The code is publicly available at\nhttps://github.com/libeineu/SDT-Training/.", "published": "2020-10-08 02:36:07", "link": "http://arxiv.org/abs/2010.03737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-hop Inference for Question-driven Summarization", "abstract": "Question-driven summarization has been recently studied as an effective\napproach to summarizing the source document to produce concise but informative\nanswers for non-factoid questions. In this work, we propose a novel\nquestion-driven abstractive summarization method, Multi-hop Selective Generator\n(MSG), to incorporate multi-hop reasoning into question-driven summarization\nand, meanwhile, provide justifications for the generated summaries.\nSpecifically, we jointly model the relevance to the question and the\ninterrelation among different sentences via a human-like multi-hop inference\nmodule, which captures important sentences for justifying the summarized\nanswer. A gated selective pointer generator network with a multi-view coverage\nmechanism is designed to integrate diverse information from different\nperspectives. Experimental results show that the proposed method consistently\noutperforms state-of-the-art methods on two non-factoid QA datasets, namely\nWikiHow and PubMedQA.", "published": "2020-10-08 02:36:39", "link": "http://arxiv.org/abs/2010.03738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infusing Disease Knowledge into BERT for Health Question Answering,\n  Medical Inference and Disease Name Recognition", "abstract": "Knowledge of a disease includes information of various aspects of the\ndisease, such as signs and symptoms, diagnosis and treatment. This disease\nknowledge is critical for many health-related and biomedical tasks, including\nconsumer health question answering, medical language inference and disease name\nrecognition. While pre-trained language models like BERT have shown success in\ncapturing syntactic, semantic, and world knowledge from text, we find they can\nbe further complemented by specific information like knowledge of symptoms,\ndiagnoses, treatments, and other disease aspects. Hence, we integrate BERT with\ndisease knowledge for improving these important tasks. Specifically, we propose\na new disease knowledge infusion training procedure and evaluate it on a suite\nof BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and\nALBERT. Experiments over the three tasks show that these models can be enhanced\nin nearly all cases, demonstrating the viability of disease knowledge infusion.\nFor example, accuracy of BioBERT on consumer health question answering is\nimproved from 68.29% to 72.09%, while new SOTA results are observed in two\ndatasets. We make our data and code freely available.", "published": "2020-10-08 03:14:38", "link": "http://arxiv.org/abs/2010.03746v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Assessing Phrasal Representation and Composition in Transformers", "abstract": "Deep transformer models have pushed performance on NLP tasks to new limits,\nsuggesting sophisticated treatment of complex linguistic inputs, such as\nphrases. However, we have limited understanding of how these models handle\nrepresentation of phrases, and whether this reflects sophisticated composition\nof phrase meaning like that done by humans. In this paper, we present\nsystematic analysis of phrasal representations in state-of-the-art pre-trained\ntransformers. We use tests leveraging human judgments of phrase similarity and\nmeaning shift, and compare results before and after control of word overlap, to\ntease apart lexical effects versus composition effects. We find that phrase\nrepresentation in these models relies heavily on word content, with little\nevidence of nuanced composition. We also identify variations in phrase\nrepresentation quality across models, layers, and representation types, and\nmake corresponding recommendations for usage of representations from these\nmodels.", "published": "2020-10-08 04:59:39", "link": "http://arxiv.org/abs/2010.03763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Attention Mechanism with Query-Value Interaction", "abstract": "Attention mechanism has played critical roles in various state-of-the-art NLP\nmodels such as Transformer and BERT. It can be formulated as a ternary function\nthat maps the input queries, keys and values into an output by using a\nsummation of values weighted by the attention weights derived from the\ninteractions between queries and keys. Similar with query-key interactions,\nthere is also inherent relatedness between queries and values, and\nincorporating query-value interactions has the potential to enhance the output\nby learning customized values according to the characteristics of queries.\nHowever, the query-value interactions are ignored by existing attention\nmethods, which may be not optimal. In this paper, we propose to improve the\nexisting attention mechanism by incorporating query-value interactions. We\npropose a query-value interaction function which can learn query-aware\nattention values, and combine them with the original values and attention\nweights to form the final output. Extensive experiments on four datasets for\ndifferent tasks show that our approach can consistently improve the performance\nof many attention-based models by incorporating query-value interactions.", "published": "2020-10-08 05:12:52", "link": "http://arxiv.org/abs/2010.03766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Long-Tail Relation Extraction with Collaborating\n  Relation-Augmented Attention", "abstract": "Wrong labeling problem and long-tail relations are two main challenges caused\nby distant supervision in relation extraction. Recent works alleviate the wrong\nlabeling by selective attention via multi-instance learning, but cannot well\nhandle long-tail relations even if hierarchies of the relations are introduced\nto share knowledge. In this work, we propose a novel neural network,\nCollaborating Relation-augmented Attention (CoRA), to handle both the wrong\nlabeling and long-tail relations. Particularly, we first propose\nrelation-augmented attention network as base model. It operates on sentence bag\nwith a sentence-to-relation attention to minimize the effect of wrong labeling.\nThen, facilitated by the proposed base model, we introduce collaborating\nrelation features shared among relations in the hierarchies to promote the\nrelation-augmenting process and balance the training data for long-tail\nrelations. Besides the main training objective to predict the relation of a\nsentence bag, an auxiliary objective is utilized to guide the\nrelation-augmenting process for a more accurate bag-level representation. In\nthe experiments on the popular benchmark dataset NYT, the proposed CoRA\nimproves the prior state-of-the-art performance by a large margin in terms of\nPrecision@N, AUC and Hits@K. Further analyses verify its superior capability in\nhandling long-tail relations in contrast to the competitors.", "published": "2020-10-08 05:34:43", "link": "http://arxiv.org/abs/2010.03773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detect All Abuse! Toward Universal Abusive Language Detection Models", "abstract": "Online abusive language detection (ALD) has become a societal issue of\nincreasing importance in recent years. Several previous works in online ALD\nfocused on solving a single abusive language problem in a single domain, like\nTwitter, and have not been successfully transferable to the general ALD task or\ndomain. In this paper, we introduce a new generic ALD framework, MACAS, which\nis capable of addressing several types of ALD tasks across different domains.\nOur generic framework covers multi-aspect abusive language embeddings that\nrepresent the target and content aspects of abusive language and applies a\ntextual graph embedding that analyses the user's linguistic behaviour. Then, we\npropose and use the cross-attention gate flow mechanism to embrace multiple\naspects of abusive language. Quantitative and qualitative evaluation results\nshow that our ALD algorithm rivals or exceeds the six state-of-the-art ALD\nalgorithms across seven ALD datasets covering multiple aspects of abusive\nlanguage and different online community domains.", "published": "2020-10-08 05:39:00", "link": "http://arxiv.org/abs/2010.03776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two are Better than One: Joint Entity and Relation Extraction with\n  Table-Sequence Encoders", "abstract": "Named entity recognition and relation extraction are two important\nfundamental problems. Joint learning algorithms have been proposed to solve\nboth tasks simultaneously, and many of them cast the joint task as a\ntable-filling problem. However, they typically focused on learning a single\nencoder (usually learning representation in the form of a table) to capture\ninformation required for both tasks within the same space. We argue that it can\nbe beneficial to design two distinct encoders to capture such two different\ntypes of information in the learning process. In this work, we propose the\nnovel {\\em table-sequence encoders} where two different encoders -- a table\nencoder and a sequence encoder are designed to help each other in the\nrepresentation learning process. Our experiments confirm the advantages of\nhaving {\\em two} encoders over {\\em one} encoder. On several standard datasets,\nour model shows significant improvements over existing approaches.", "published": "2020-10-08 09:10:55", "link": "http://arxiv.org/abs/2010.03851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Co-Interactive Transformer for Joint Slot Filling and Intent Detection", "abstract": "Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding (SLU) system. The two tasks are closely related and the\ninformation of one task can be utilized in the other task. Previous studies\neither model the two tasks separately or only consider the single information\nflow from intent to slot. None of the prior approaches model the bidirectional\nconnection between the two tasks simultaneously. In this paper, we propose a\nCo-Interactive Transformer to consider the cross-impact between the two tasks.\nInstead of adopting the self-attention mechanism in vanilla Transformer, we\npropose a co-interactive module to consider the cross-impact by building a\nbidirectional connection between the two related tasks. In addition, the\nproposed co-interactive module can be stacked to incrementally enhance each\nother with mutual features. The experimental results on two public datasets\n(SNIPS and ATIS) show that our model achieves the state-of-the-art performance\nwith considerable improvements (+3.4% and +0.9% on overall acc). Extensive\nexperiments empirically verify that our model successfully captures the mutual\ninteraction knowledge.", "published": "2020-10-08 10:16:52", "link": "http://arxiv.org/abs/2010.03880v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Product Key Memory for Pretrained Language Models", "abstract": "Product key memory (PKM) proposed by Lample et al. (2019) enables to improve\nprediction accuracy by increasing model capacity efficiently with insignificant\ncomputational overhead. However, their empirical application is only limited to\ncausal language modeling. Motivated by the recent success of pretrained\nlanguage models (PLMs), we investigate how to incorporate large PKM into PLMs\nthat can be finetuned for a wide variety of downstream NLP tasks. We define a\nnew memory usage metric, and careful observation using this metric reveals that\nmost memory slots remain outdated during the training of PKM-augmented models.\nTo train better PLMs by tackling this issue, we propose simple but effective\nsolutions: (1) initialization from the model weights pretrained without memory\nand (2) augmenting PKM by addition rather than replacing a feed-forward\nnetwork. We verify that both of them are crucial for the pretraining of\nPKM-augmented PLMs, enhancing memory utilization and downstream performance.\nCode and pretrained weights are available at\nhttps://github.com/clovaai/pkm-transformers.", "published": "2020-10-08 10:19:50", "link": "http://arxiv.org/abs/2010.03881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Word Information with Multi-Level Word Adapter for Chinese\n  Spoken Language Understanding", "abstract": "In this paper, we improve Chinese spoken language understanding (SLU) by\ninjecting word information. Previous studies on Chinese SLU do not consider the\nword information, failing to detect word boundaries that are beneficial for\nintent detection and slot filling. To address this issue, we propose a\nmulti-level word adapter to inject word information for Chinese SLU, which\nconsists of (1) sentence-level word adapter, which directly fuses the sentence\nrepresentations of the word information and character information to perform\nintent detection and (2) character-level word adapter, which is applied at each\ncharacter for selectively controlling weights on word information as well as\ncharacter information. Experimental results on two Chinese SLU datasets show\nthat our model can capture useful word information and achieve state-of-the-art\nperformance.", "published": "2020-10-08 11:11:05", "link": "http://arxiv.org/abs/2010.03903v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Instructions at Different Levels of Abstraction", "abstract": "When generating technical instructions, it is often convenient to describe\ncomplex objects in the world at different levels of abstraction. A novice user\nmight need an object explained piece by piece, while for an expert, talking\nabout the complex object (e.g. a wall or railing) directly may be more succinct\nand efficient. We show how to generate building instructions at different\nlevels of abstraction in Minecraft. We introduce the use of hierarchical\nplanning to this end, a method from AI planning which can capture the structure\nof complex objects neatly. A crowdsourcing evaluation shows that the choice of\nabstraction level matters to users, and that an abstraction strategy which\nbalances low-level and high-level object descriptions compares favorably to\nones which don't.", "published": "2020-10-08 13:56:09", "link": "http://arxiv.org/abs/2010.03982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating\n  Open-Domain Dialogue Systems", "abstract": "Automatically evaluating dialogue coherence is a challenging but high-demand\nability for developing high-quality open-domain dialogue systems. However,\ncurrent evaluation metrics consider only surface features or utterance-level\nsemantics, without explicitly considering the fine-grained topic transition\ndynamics of dialogue flows. Here, we first consider that the graph structure\nconstituted with topics in a dialogue can accurately depict the underlying\ncommunication logic, which is a more natural way to produce persuasive metrics.\nCapitalized on the topic-level dialogue graph, we propose a new evaluation\nmetric GRADE, which stands for Graph-enhanced Representations for Automatic\nDialogue Evaluation. Specifically, GRADE incorporates both coarse-grained\nutterance-level contextualized representations and fine-grained topic-level\ngraph representations to evaluate dialogue coherence. The graph representations\nare obtained by reasoning over topic-level dialogue graphs enhanced with the\nevidence from a commonsense graph, including k-hop neighboring representations\nand hop-attention weights. Experimental results show that our GRADE\nsignificantly outperforms other state-of-the-art metrics on measuring diverse\ndialogue models in terms of the Pearson and Spearman correlations with human\njudgements. Besides, we release a new large-scale human evaluation benchmark to\nfacilitate future research on automatic metrics.", "published": "2020-10-08 14:07:32", "link": "http://arxiv.org/abs/2010.03994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precise Task Formalization Matters in Winograd Schema Evaluations", "abstract": "Performance on the Winograd Schema Challenge (WSC), a respected English\ncommonsense reasoning benchmark, recently rocketed from chance accuracy to 89%\non the SuperGLUE leaderboard, with relatively little corroborating evidence of\na correspondingly large improvement in reasoning ability. We hypothesize that\nmuch of this improvement comes from recent changes in task formalization---the\ncombination of input specification, loss function, and reuse of pretrained\nparameters---by users of the dataset, rather than improvements in the\npretrained model's reasoning ability. We perform an ablation on two Winograd\nSchema datasets that interpolates between the formalizations used before and\nafter this surge, and find (i) framing the task as multiple choice improves\nperformance by 2-6 points and (ii) several additional techniques, including the\nreuse of a pretrained language modeling head, can mitigate the model's extreme\nsensitivity to hyperparameters. We urge future benchmark creators to impose\nadditional structure to minimize the impact of formalization decisions on\nreported results.", "published": "2020-10-08 15:10:47", "link": "http://arxiv.org/abs/2010.04043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DART: A Lightweight Quality-Suggestive Data-to-Text Annotation Tool", "abstract": "We present a lightweight annotation tool, the Data AnnotatoR Tool (DART), for\nthe general task of labeling structured data with textual descriptions. The\ntool is implemented as an interactive application that reduces human efforts in\nannotating large quantities of structured data, e.g. in the format of a table\nor tree structure. By using a backend sequence-to-sequence model, our system\niteratively analyzes the annotated labels in order to better sample unlabeled\ndata. In a simulation experiment performed on annotating large quantities of\nstructured data, DART has been shown to reduce the total number of annotations\nneeded with active learning and automatically suggesting relevant labels.", "published": "2020-10-08 17:36:34", "link": "http://arxiv.org/abs/2010.04141v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PoinT-5: Pointer Network and T-5 based Financial NarrativeSummarisation", "abstract": "Companies provide annual reports to their shareholders at the end of the\nfinancial year that describes their operations and financial conditions. The\naverage length of these reports is 80, and it may extend up to 250 pages long.\nIn this paper, we propose our methodology PoinT-5 (the combination of Pointer\nNetwork and T-5 (Test-to-text transfer Transformer) algorithms) that we used in\nthe Financial Narrative Summarisation (FNS) 2020 task. The proposed method uses\npointer networks to extract important narrative sentences from the report, and\nthen T-5 is used to paraphrase extracted sentences into a concise yet\ninformative sentence. We evaluate our method using ROUGE-N (1,2), L, and SU4.\nThe proposed method achieves the highest precision scores in all the metrics\nand highest F1 scores in ROUGE1, and LCS and the only solution to cross the\nMUSE solution baseline in ROUGE-LCS metrics.", "published": "2020-10-08 18:09:45", "link": "http://arxiv.org/abs/2010.04191v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Inference for Improving Language Understanding and Generation", "abstract": "Natural language understanding (NLU) and Natural language generation (NLG)\ntasks hold a strong dual relationship, where NLU aims at predicting semantic\nlabels based on natural language utterances and NLG does the opposite. The\nprior work mainly focused on exploiting the duality in model training in order\nto obtain the models with better performance. However, regarding the\nfast-growing scale of models in the current NLP area, sometimes we may have\ndifficulty retraining whole NLU and NLG models. To better address the issue,\nthis paper proposes to leverage the duality in the inference stage without the\nneed of retraining. The experiments on three benchmark datasets demonstrate the\neffectiveness of the proposed method in both NLU and NLG, providing the great\npotential of practical usage.", "published": "2020-10-08 20:14:41", "link": "http://arxiv.org/abs/2010.04246v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effectiveness of Efficient Neural Architecture Search for\n  Sentence-Pair Tasks", "abstract": "Neural Architecture Search (NAS) methods, which automatically learn entire\nneural model or individual neural cell architectures, have recently achieved\ncompetitive or state-of-the-art (SOTA) performance on variety of natural\nlanguage processing and computer vision tasks, including language modeling,\nnatural language inference, and image classification. In this work, we explore\nthe applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search\n(ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and\nsemantic textual similarity. We use ENAS to perform a micro-level search and\nlearn a task-optimized RNN cell architecture as a drop-in replacement for an\nLSTM. We explore the effectiveness of ENAS through experiments on three\ndatasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and\ntwo sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS\nto NLP tasks, our results are mixed -- we find that ENAS architectures\nsometimes, but not always, outperform LSTMs and perform similarly to random\narchitecture search.", "published": "2020-10-08 20:26:34", "link": "http://arxiv.org/abs/2010.04249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Style in Parsing Speech with Neural Models", "abstract": "The differences in written text and conversational speech are substantial;\nprevious parsers trained on treebanked text have given very poor results on\nspontaneous speech. For spoken language, the mismatch in style also extends to\nprosodic cues, though it is less well understood. This paper re-examines the\nuse of written text in parsing speech in the context of recent advances in\nneural language processing. We show that neural approaches facilitate using\nwritten text to improve parsing of spontaneous speech, and that prosody further\nimproves over this state-of-the-art result. Further, we find an asymmetric\ndegradation from read vs. spontaneous mismatch, with spontaneous speech more\ngenerally useful for training parsers.", "published": "2020-10-08 22:44:19", "link": "http://arxiv.org/abs/2010.04288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Disfluency in Children's Speech", "abstract": "Disfluencies are prevalent in spontaneous speech, as shown in many studies of\nadult speech. Less is understood about children's speech, especially in\npre-school children who are still developing their language skills. We present\na novel dataset with annotated disfluencies of spontaneous explanations from 26\nchildren (ages 5--8), interviewed twice over a year-long period. Our\npreliminary analysis reveals significant differences between children's speech\nin our corpus and adult spontaneous speech from two corpora (Switchboard and\nCallHome). Children have higher disfluency and filler rates, tend to use nasal\nfilled pauses more frequently, and on average exhibit longer reparandums than\nrepairs, in contrast to adult speakers. Despite the differences, an automatic\ndisfluency detection system trained on adult (Switchboard) speech transcripts\nperforms reasonably well on children's speech, achieving an F1 score that is\n10\\% higher than the score on an adult out-of-domain dataset (CallHome).", "published": "2020-10-08 22:51:25", "link": "http://arxiv.org/abs/2010.04293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Evaluate Translation Beyond English: BLEURT Submissions to\n  the WMT Metrics 2020 Shared Task", "abstract": "The quality of machine translation systems has dramatically improved over the\nlast decade, and as a result, evaluation has become an increasingly challenging\nproblem. This paper describes our contribution to the WMT 2020 Metrics Shared\nTask, the main benchmark for automatic evaluation of translation. We make\nseveral submissions based on BLEURT, a previously published metric based on\ntransfer learning. We extend the metric beyond English and evaluate it on 14\nlanguage pairs for which fine-tuning data is available, as well as 4\n\"zero-shot\" language pairs, for which we have no labelled examples.\nAdditionally, we focus on English to German and demonstrate how to combine\nBLEURT's predictions with those of YiSi and use alternative reference\ntranslations to enhance the performance. Empirical results show that the models\nachieve competitive results on the WMT Metrics 2019 Shared Task, indicating\ntheir promise for the 2020 edition.", "published": "2020-10-08 23:16:26", "link": "http://arxiv.org/abs/2010.04297v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Recombine and Resample Data for Compositional Generalization", "abstract": "Flexible neural sequence models outperform grammar- and automaton-based\ncounterparts on a variety of tasks. However, neural models perform poorly in\nsettings requiring compositional generalization beyond the training data --\nparticularly to rare or unseen subsequences. Past work has found symbolic\nscaffolding (e.g. grammars or automata) essential in these settings. We\ndescribe R&R, a learned data augmentation scheme that enables a large category\nof compositional generalizations without appeal to latent symbolic structure.\nR&R has two components: recombination of original training examples via a\nprototype-based generative model and resampling of generated examples to\nencourage extrapolation. Training an ordinary neural sequence model on a\ndataset augmented with recombined and resampled examples significantly improves\ngeneralization in two language processing problems -- instruction following\n(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables\nlearning of new constructions and tenses from as few as eight initial examples.", "published": "2020-10-08 00:36:33", "link": "http://arxiv.org/abs/2010.03706v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizable and Explainable Dialogue Generation via Explicit Action\n  Learning", "abstract": "Response generation for task-oriented dialogues implicitly optimizes two\nobjectives at the same time: task completion and language quality. Conditioned\nresponse generation serves as an effective approach to separately and better\noptimize these two objectives. Such an approach relies on system action\nannotations which are expensive to obtain. To alleviate the need of action\nannotations, latent action learning is introduced to map each utterance to a\nlatent representation. However, this approach is prone to over-dependence on\nthe training data, and the generalization capability is thus restricted. To\naddress this issue, we propose to learn natural language actions that represent\nutterances as a span of words. This explicit action representation promotes\ngeneralization via the compositional structure of language. It also enables an\nexplainable generation process. Our proposed unsupervised approach learns a\nmemory component to summarize system utterances into a short span of words. To\nfurther promote a compact action representation, we propose an auxiliary task\nthat restores state annotations as the summarized dialogue context using the\nmemory component. Our proposed approach outperforms latent action baselines on\nMultiWOZ, a benchmark multi-domain dataset.", "published": "2020-10-08 04:37:22", "link": "http://arxiv.org/abs/2010.03755v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discriminatively-Tuned Generative Classifiers for Robust Natural\n  Language Inference", "abstract": "While discriminative neural network classifiers are generally preferred,\nrecent work has shown advantages of generative classifiers in term of data\nefficiency and robustness. In this paper, we focus on natural language\ninference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and\nempirically characterize its performance by comparing it to five baselines,\nincluding discriminative models and large-scale pretrained language\nrepresentation models like BERT. We explore training objectives for\ndiscriminative fine-tuning of our generative classifiers, showing improvements\nover log loss fine-tuning from prior work . In particular, we find strong\nresults with a simple unbounded modification to log loss, which we call the\n\"infinilog loss\". Our experiments show that GenNLI outperforms both\ndiscriminative and pretrained baselines across several challenging NLI\nexperimental settings, including small training sets, imbalanced label\ndistributions, and label noise.", "published": "2020-10-08 04:44:00", "link": "http://arxiv.org/abs/2010.03760v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Model-agnostic Debiasing Strategies for Robust\n  Natural Language Inference", "abstract": "The prior work on natural language inference (NLI) debiasing mainly targets\nat one or few known biases while not necessarily making the models more robust.\nIn this paper, we focus on the model-agnostic debiasing strategies and explore\nhow to (or is it possible to) make the NLI models robust to multiple distinct\nadversarial attacks while keeping or even strengthening the models'\ngeneralization power. We firstly benchmark prevailing neural NLI models\nincluding pretrained ones on various adversarial datasets. We then try to\ncombat distinct known biases by modifying a mixture of experts (MoE) ensemble\nmethod and show that it's nontrivial to mitigate multiple NLI biases at the\nsame time, and that model-level ensemble method outperforms MoE ensemble\nmethod. We also perform data augmentation including text swap, word\nsubstitution and paraphrase and prove its efficiency in combating various\n(though not all) adversarial attacks at the same time. Finally, we investigate\nseveral methods to merge heterogeneous training data (1.35M) and perform model\nensembling, which are straightforward but effective to strengthen NLI models.", "published": "2020-10-08 05:40:45", "link": "http://arxiv.org/abs/2010.03777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling", "abstract": "We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.", "published": "2020-10-08 07:06:38", "link": "http://arxiv.org/abs/2010.03802v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the importance of pre-training data volume for compact language\n  models", "abstract": "Recent advances in language modeling have led to computationally intensive\nand resource-demanding state-of-the-art models. In an effort towards\nsustainable practices, we study the impact of pre-training data volume on\ncompact language models. Multiple BERT-based models are trained on gradually\nincreasing amounts of French text. Through fine-tuning on the French Question\nAnswering Dataset (FQuAD), we observe that well-performing models are obtained\nwith as little as 100 MB of text. In addition, we show that past critically low\namounts of pre-training data, an intermediate pre-training step on the\ntask-specific corpus does not yield substantial improvements.", "published": "2020-10-08 07:40:21", "link": "http://arxiv.org/abs/2010.03813v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Can We Do to Improve Peer Review in NLP?", "abstract": "Peer review is our best tool for judging the quality of conference\nsubmissions, but it is becoming increasingly spurious. We argue that a part of\nthe problem is that the reviewers and area chairs face a poorly defined task\nforcing apples-to-oranges comparisons. There are several potential ways\nforward, but the key difficulty is creating the incentives and mechanisms for\ntheir consistent implementation in the NLP community.", "published": "2020-10-08 09:32:21", "link": "http://arxiv.org/abs/2010.03863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting Typological Features in WALS using Language Embeddings and\n  Conditional Probabilities: \u00daFAL Submission to the SIGTYP 2020 Shared Task", "abstract": "We present our submission to the SIGTYP 2020 Shared Task on the prediction of\ntypological features. We submit a constrained system, predicting typological\nfeatures only based on the WALS database. We investigate two approaches. The\nsimpler of the two is a system based on estimating correlation of feature\nvalues within languages by computing conditional probabilities and mutual\ninformation. The second approach is to train a neural predictor operating on\nprecomputed language embeddings based on WALS features. Our submitted system\ncombines the two approaches based on their self-estimated confidence scores. We\nreach the accuracy of 70.7% on the test data and rank first in the shared task.", "published": "2020-10-08 12:05:48", "link": "http://arxiv.org/abs/2010.03920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERTering RAMS: What and How Much does BERT Already Know About Event\n  Arguments? -- A Study on the RAMS Dataset", "abstract": "Using the attention map based probing frame-work from (Clark et al., 2019),\nwe observe that, on the RAMS dataset (Ebner et al., 2020), BERT's attention\nheads have modest but well above-chance ability to spot event arguments sans\nany training or domain finetuning, vary-ing from a low of 17.77% for Place to a\nhigh of 51.61% for Artifact. Next, we find that linear combinations of these\nheads, estimated with approx 11% of available total event argument detection\nsupervision, can push performance well-higher for some roles - highest two\nbeing Victim (68.29% Accuracy) and Artifact(58.82% Accuracy). Furthermore, we\ninvestigate how well our methods do for cross-sentence event arguments. We\npropose a procedure to isolate \"best heads\" for cross-sentence argument\ndetection separately of those for intra-sentence arguments. The heads thus\nestimated have superior cross-sentence performance compared to their jointly\nestimated equivalents, albeit only under the unrealistic assumption that we\nalready know the argument is present in an-other sentence. Lastly, we seek to\nisolate to what extent our numbers stem from lexical frequency based\nassociations between gold arguments and roles. We propose NONCE, a scheme to\ncreate adversarial test examples by replacing gold arguments with randomly\ngenerated \"nonce\" words. We find that learnt linear combinations are robust to\nNONCE, though individual best heads can be more sensitive.", "published": "2020-10-08 16:27:03", "link": "http://arxiv.org/abs/2010.04098v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake Reviews Detection through Analysis of Linguistic Features", "abstract": "Online reviews play an integral part for success or failure of businesses.\nPrior to purchasing services or goods, customers first review the online\ncomments submitted by previous customers. However, it is possible to\nsuperficially boost or hinder some businesses through posting counterfeit and\nfake reviews. This paper explores a natural language processing approach to\nidentify fake reviews. We present a detailed analysis of linguistic features\nfor distinguishing fake and trustworthy online reviews. We study 15 linguistic\nfeatures and measure their significance and importance towards the\nclassification schemes employed in this study. Our results indicate that fake\nreviews tend to include more redundant terms and pauses, and generally contain\nlonger sentences. The application of several machine learning classification\nalgorithms revealed that we were able to discriminate fake from real reviews\nwith high accuracy using these linguistic features.", "published": "2020-10-08 21:16:30", "link": "http://arxiv.org/abs/2010.04260v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis\n  Including Unsupervised Duration Modeling", "abstract": "This paper presents Non-Attentive Tacotron based on the Tacotron 2\ntext-to-speech model, replacing the attention mechanism with an explicit\nduration predictor. This improves robustness significantly as measured by\nunaligned duration ratio and word deletion rate, two metrics introduced in this\npaper for large-scale robustness evaluation using a pre-trained speech\nrecognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron\nachieves a 5-scale mean opinion score for naturalness of 4.41, slightly\noutperforming Tacotron 2. The duration predictor enables both utterance-wide\nand per-phoneme control of duration at inference time. When accurate target\ndurations are scarce or unavailable in the training data, we propose a method\nusing a fine-grained variational auto-encoder to train the duration predictor\nin a semi-supervised or unsupervised manner, with results almost as good as\nsupervised training.", "published": "2020-10-08 23:41:39", "link": "http://arxiv.org/abs/2010.04301v4", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Masked ELMo: An evolution of ELMo towards fully contextual RNN language\n  models", "abstract": "This paper presents Masked ELMo, a new RNN-based model for language model\npre-training, evolved from the ELMo language model. Contrary to ELMo which only\nuses independent left-to-right and right-to-left contexts, Masked ELMo learns\nfully bidirectional word representations. To achieve this, we use the same\nMasked language model objective as BERT. Additionally, thanks to optimizations\non the LSTM neuron, the integration of mask accumulation and bidirectional\ntruncated backpropagation through time, we have increased the training speed of\nthe model substantially. All these improvements make it possible to pre-train a\nbetter language model than ELMo while maintaining a low computational cost. We\nevaluate Masked ELMo by comparing it to ELMo within the same protocol on the\nGLUE benchmark, where our model outperforms significantly ELMo and is\ncompetitive with transformer approaches.", "published": "2020-10-08 23:58:57", "link": "http://arxiv.org/abs/2010.04302v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based\n  Decoding", "abstract": "Semantic parsing is one of the key components of natural language\nunderstanding systems. A successful parse transforms an input utterance to an\naction that is easily understood by the system. Many algorithms have been\nproposed to solve this problem, from conventional rulebased or statistical\nslot-filling systems to shiftreduce based neural parsers. For complex parsing\ntasks, the state-of-the-art method is based on autoregressive sequence to\nsequence models to generate the parse directly. This model is slow at inference\ntime, generating parses in O(n) decoding steps (n is the length of the target\nsequence). In addition, we demonstrate that this method performs poorly in\nzero-shot cross-lingual transfer learning settings. In this paper, we propose a\nnon-autoregressive parser which is based on the insertion transformer to\novercome these two issues. Our approach 1) speeds up decoding by 3x while\noutperforming the autoregressive model and 2) significantly improves\ncross-lingual transfer in the low-resource setting by 37% compared to\nautoregressive baseline. We test our approach on three well-known monolingual\ndatasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the\nMultiATIS++ and the multilingual TOP datasets.", "published": "2020-10-08 01:18:42", "link": "http://arxiv.org/abs/2010.03714v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent linguistic embedding for cross-lingual text-to-speech and voice\n  conversion", "abstract": "As the recently proposed voice cloning system, NAUTILUS, is capable of\ncloning unseen voices using untranscribed speech, we investigate the\nfeasibility of using it to develop a unified cross-lingual TTS/VC system.\nCross-lingual speech generation is the scenario in which speech utterances are\ngenerated with the voices of target speakers in a language not spoken by them\noriginally. This type of system is not simply cloning the voice of the target\nspeaker, but essentially creating a new voice that can be considered better\nthan the original under a specific framing. By using a well-trained English\nlatent linguistic embedding to create a cross-lingual TTS and VC system for\nseveral German, Finnish, and Mandarin speakers included in the Voice Conversion\nChallenge 2020, we show that our method not only creates cross-lingual VC with\nhigh speaker similarity but also can be seamlessly used for cross-lingual TTS\nwithout having to perform any extra steps. However, the subjective evaluations\nof perceived naturalness seemed to vary between target speakers, which is one\naspect for future improvement.", "published": "2020-10-08 01:25:07", "link": "http://arxiv.org/abs/2010.03717v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text-based RL Agents with Commonsense Knowledge: New Challenges,\n  Environments and Baselines", "abstract": "Text-based games have emerged as an important test-bed for Reinforcement\nLearning (RL) research, requiring RL agents to combine grounded language\nunderstanding with sequential decision making. In this paper, we examine the\nproblem of infusing RL agents with commonsense knowledge. Such knowledge would\nallow agents to efficiently act in the world by pruning out implausible\nactions, and to perform look-ahead planning to determine how current actions\nmight affect future world states. We design a new text-based gaming environment\ncalled TextWorld Commonsense (TWC) for training and evaluating RL agents with a\nspecific kind of commonsense knowledge about objects, their attributes, and\naffordances. We also introduce several baseline RL agents which track the\nsequential context and dynamically retrieve the relevant commonsense knowledge\nfrom ConceptNet. We show that agents which incorporate commonsense knowledge in\nTWC perform better, while acting more efficiently. We conduct user-studies to\nestimate human performance on TWC and show that there is ample room for future\nimprovement.", "published": "2020-10-08 06:20:00", "link": "http://arxiv.org/abs/2010.03790v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers", "abstract": "The COVID-19 pandemic has spawned a diverse body of scientific literature\nthat is challenging to navigate, stimulating interest in automated tools to\nhelp find useful knowledge. We pursue the construction of a knowledge base (KB)\nof mechanisms -- a fundamental concept across the sciences encompassing\nactivities, functions and causal relations, ranging from cellular processes to\neconomic impacts. We extract this information from the natural language of\nscientific papers by developing a broad, unified schema that strikes a balance\nbetween relevance and breadth. We annotate a dataset of mechanisms with our\nschema and train a model to extract mechanism relations from papers. Our\nexperiments demonstrate the utility of our KB in supporting interdisciplinary\nscientific search over COVID-19 literature, outperforming the prominent PubMed\nsearch in a study with clinical experts.", "published": "2020-10-08 07:54:14", "link": "http://arxiv.org/abs/2010.03824v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dense Relational Image Captioning via Multi-task Triple-Stream Networks", "abstract": "We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions for each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS; i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to learn not only to generate captions but also to\nunderstand the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. Then, we present applications of our framework to\nholistic image captioning, scene graph generation, and retrieval tasks.", "published": "2020-10-08 09:17:55", "link": "http://arxiv.org/abs/2010.03855v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Population Based Training for Data Augmentation and Regularization in\n  Speech Recognition", "abstract": "Varying data augmentation policies and regularization over the course of\noptimization has led to performance improvements over using fixed values. We\nshow that population based training is a useful tool to continuously search\nthose hyperparameters, within a fixed budget. This greatly simplifies the\nexperimental burden and computational cost of finding such optimal schedules.\nWe experiment in speech recognition by optimizing SpecAugment this way, as well\nas dropout. It compares favorably to a baseline that does not change those\nhyperparameters over the course of training, with an 8% relative WER\nimprovement. We obtain 5.18% word error rate on LibriSpeech's test-other.", "published": "2020-10-08 11:00:18", "link": "http://arxiv.org/abs/2010.03899v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10, 68T07", "I.5.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial\n  Explanations of Their Behavior in Natural Language?", "abstract": "Data collection for natural language (NL) understanding tasks has\nincreasingly included human explanations alongside data points, allowing past\nworks to introduce models that both perform a task and generate NL explanations\nfor their outputs. Yet to date, model-generated explanations have been\nevaluated on the basis of surface-level similarities to human explanations,\nboth through automatic metrics like BLEU and human evaluations. We argue that\nthese evaluations are insufficient, since they fail to indicate whether\nexplanations support actual model behavior (faithfulness), rather than simply\nmatch what a human would say (plausibility). In this work, we address the\nproblem of evaluating explanations from the model simulatability perspective.\nOur contributions are as follows: (1) We introduce a leakage-adjusted\nsimulatability (LAS) metric for evaluating NL explanations, which measures how\nwell explanations help an observer predict a model's output, while controlling\nfor how explanations can directly leak the output. We use a model as a proxy\nfor a human observer, and validate this choice with two human subject\nexperiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing\ngenerative graphical models and two new approaches; one rationalizing method we\nintroduce achieves roughly human-level LAS scores. (3) Lastly, we frame\nexplanation generation as a multi-agent game and optimize explanations for\nsimulatability while penalizing label leakage, which can improve LAS scores. We\nprovide code for the experiments in this paper at\nhttps://github.com/peterbhase/LAS-NL-Explanations", "published": "2020-10-08 16:59:07", "link": "http://arxiv.org/abs/2010.04119v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Topic-Guided Conversational Recommender System", "abstract": "Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. To develop an effective CRS, the\nsupport of high-quality datasets is essential. Existing CRS datasets mainly\nfocus on immediate requests from users, while lack proactive guidance to the\nrecommendation scenario. In this paper, we contribute a new CRS dataset named\n\\textbf{TG-ReDial} (\\textbf{Re}commendation through\n\\textbf{T}opic-\\textbf{G}uided \\textbf{Dial}og). Our dataset has two major\nfeatures. First, it incorporates topic threads to enforce natural semantic\ntransitions towards the recommendation scenario. Second, it is created in a\nsemi-automatic way, hence human annotation is more reasonable and controllable.\nBased on TG-ReDial, we present the task of topic-guided conversational\nrecommendation, and propose an effective approach to this task. Extensive\nexperiments have demonstrated the effectiveness of our approach on three\nsub-tasks, namely topic prediction, item recommendation and response\ngeneration. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.", "published": "2020-10-08 17:04:30", "link": "http://arxiv.org/abs/2010.04125v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Query-Key Normalization for Transformers", "abstract": "Low-resource language translation is a challenging but socially valuable NLP\ntask. Building on recent work adapting the Transformer's normalization to this\nsetting, we propose QKNorm, a normalization technique that modifies the\nattention mechanism to make the softmax function less prone to arbitrary\nsaturation without sacrificing expressivity. Specifically, we apply $\\ell_2$\nnormalization along the head dimension of each query and key matrix prior to\nmultiplying them and then scale up by a learnable parameter instead of dividing\nby the square root of the embedding dimension. We show improvements averaging\n0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource\ntranslation pairs from the TED Talks corpus and IWSLT'15.", "published": "2020-10-08 20:12:35", "link": "http://arxiv.org/abs/2010.04245v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Unpaired Text Data for Training End-to-End Speech-to-Intent\n  Systems", "abstract": "Training an end-to-end (E2E) neural network speech-to-intent (S2I) system\nthat directly extracts intents from speech requires large amounts of\nintent-labeled speech data, which is time consuming and expensive to collect.\nInitializing the S2I model with an ASR model trained on copious speech data can\nalleviate data sparsity. In this paper, we attempt to leverage NLU text\nresources. We implemented a CTC-based S2I system that matches the performance\nof a state-of-the-art, traditional cascaded SLU system. We performed controlled\nexperiments with varying amounts of speech and text training data. When only a\ntenth of the original data is available, intent classification accuracy\ndegrades by 7.6% absolute. Assuming we have additional text-to-intent data\n(without speech) available, we investigated two techniques to improve the S2I\nsystem: (1) transfer learning, in which acoustic embeddings for intent\nclassification are tied to fine-tuned BERT text embeddings; and (2) data\naugmentation, in which the text-to-intent data is converted into\nspeech-to-intent data using a multi-speaker text-to-speech system. The proposed\napproaches recover 80% of performance lost due to using limited intent-labeled\nspeech.", "published": "2020-10-08 22:16:26", "link": "http://arxiv.org/abs/2010.04284v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "comp-syn: Perceptually Grounded Word Embeddings with Color", "abstract": "Popular approaches to natural language processing create word embeddings\nbased on textual co-occurrence patterns, but often ignore embodied, sensory\naspects of language. Here, we introduce the Python package comp-syn, which\nprovides grounded word embeddings based on the perceptually uniform color\ndistributions of Google Image search results. We demonstrate that comp-syn\nsignificantly enriches models of distributional semantics. In particular, we\nshow that (1) comp-syn predicts human judgments of word concreteness with\ngreater accuracy and in a more interpretable fashion than word2vec using\nlow-dimensional word-color embeddings, and (2) comp-syn performs comparably to\nword2vec on a metaphorical vs. literal word-pair classification task. comp-syn\nis open-source on PyPi and is compatible with mainstream machine-learning\nPython packages. Our package release includes word-color embeddings for over\n40,000 English words, each associated with crowd-sourced word concreteness\njudgments.", "published": "2020-10-08 22:50:06", "link": "http://arxiv.org/abs/2010.04292v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Widget Captioning: Generating Natural Language Description for Mobile\n  User Interface Elements", "abstract": "Natural language descriptions of user interface (UI) elements such as\nalternative text are crucial for accessibility and language-based interaction\nin general. Yet, these descriptions are constantly missing in mobile UIs. We\npropose widget captioning, a novel task for automatically generating language\ndescriptions for UI elements from multimodal input including both the image and\nthe structural representations of user interfaces. We collected a large-scale\ndataset for widget captioning with crowdsourcing. Our dataset contains 162,859\nlanguage phrases created by human workers for annotating 61,285 UI elements\nacross 21,750 unique UI screens. We thoroughly analyze the dataset, and train\nand evaluate a set of deep model configurations to investigate how each feature\nmodality as well as the choice of learning strategies impact the quality of\npredicted captions. The task formulation and the dataset as well as our\nbenchmark models contribute a solid basis for this novel multimodal captioning\ntask that connects language and user interfaces.", "published": "2020-10-08 22:56:03", "link": "http://arxiv.org/abs/2010.04295v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Computational Skills by Stealth in Secondary School Data Science", "abstract": "The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.", "published": "2020-10-08 09:11:51", "link": "http://arxiv.org/abs/2010.07017v1", "categories": ["cs.CY", "cs.CL", "stat.OT"], "primary_category": "cs.CY"}
{"title": "Characterizing Datasets for Social Visual Question Answering, and the\n  New TinySocial Dataset", "abstract": "Modern social intelligence includes the ability to watch videos and answer\nquestions about social and theory-of-mind-related content, e.g., for a scene in\nHarry Potter, \"Is the father really upset about the boys flying the car?\"\nSocial visual question answering (social VQA) is emerging as a valuable\nmethodology for studying social reasoning in both humans (e.g., children with\nautism) and AI agents. However, this problem space spans enormous variations in\nboth videos and questions. We discuss methods for creating and characterizing\nsocial VQA datasets, including 1) crowdsourcing versus in-house authoring,\nincluding sample comparisons of two new datasets that we created\n(TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ\ndataset; 2) a new rubric for characterizing the difficulty and content of a\ngiven video; and 3) a new rubric for characterizing question types. We close by\ndescribing how having well-characterized social VQA datasets will enhance the\nexplainability of AI agents and can also inform assessments and educational\ninterventions for people.", "published": "2020-10-08 03:20:23", "link": "http://arxiv.org/abs/2010.11997v1", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.SI"], "primary_category": "cs.HC"}
{"title": "ALFWorld: Aligning Text and Embodied Environments for Interactive\n  Learning", "abstract": "Given a simple request like Put a washed apple in the kitchen fridge, humans\ncan reason in purely abstract terms by imagining action sequences and scoring\ntheir likelihood of success, prototypicality, and efficiency, all without\nmoving a muscle. Once we see the kitchen in question, we can update our\nabstract plans to fit the scene. Embodied agents require the same abilities,\nbut existing work does not yet provide the infrastructure necessary for both\nreasoning abstractly and executing concretely. We address this limitation by\nintroducing ALFWorld, a simulator that enables agents to learn abstract, text\nbased policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from\nthe ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.\nALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,\nlearned in TextWorld, corresponds directly to concrete, visually grounded\nactions. In turn, as we demonstrate empirically, this fosters better agent\ngeneralization than training only in the visually grounded environment.\nBUTLER's simple, modular design factors the problem to allow researchers to\nfocus on models for improving every piece of the pipeline (language\nunderstanding, planning, navigation, and visual scene understanding).", "published": "2020-10-08 05:13:36", "link": "http://arxiv.org/abs/2010.03768v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "HLT-NUS Submission for NIST 2019 Multimedia Speaker Recognition\n  Evaluation", "abstract": "This work describes the speaker verification system developed by Human\nLanguage Technology Laboratory, National University of Singapore (HLT-NUS) for\n2019 NIST Multimedia Speaker Recognition Evaluation (SRE). The multimedia\nresearch has gained attention to a wide range of applications and speaker\nrecognition is no exception to it. In contrast to the previous NIST SREs, the\nlatest edition focuses on a multimedia track to recognize speakers with both\naudio and visual information. We developed separate systems for audio and\nvisual inputs followed by a score level fusion of the systems from the two\nmodalities to collectively use their information. The audio systems are based\non x-vector based speaker embedding, whereas the face recognition systems are\nbased on ResNet and InsightFace based face embeddings. With post evaluation\nstudies and refinements, we obtain an equal error rate (EER) of 0.88% and an\nactual detection cost function (actDCF) of 0.026 on the evaluation set of 2019\nNIST multimedia SRE corpus.", "published": "2020-10-08 11:21:41", "link": "http://arxiv.org/abs/2010.03905v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Classification of Speech with and without Face Mask using Acoustic\n  Features", "abstract": "The understanding and interpretation of speech can be affected by various\nexternal factors. The use of face masks is one such factors that can create\nobstruction to speech while communicating. This may lead to degradation of\nspeech processing and affect humans perceptually. Knowing whether a speaker\nwears a mask may be useful for modeling speech for different applications. With\nthis motivation, finding whether a speaker wears face mask from a given speech\nis included as a task in Computational Paralinguistics Evaluation (ComParE)\n2020. We study novel acoustic features based on linear filterbanks,\ninstantaneous phase and long-term information that can capture the artifacts\nfor classification of speech with and without face mask. These acoustic\nfeatures are used along with the state-of-the-art baselines of ComParE\nfunctionals, bag-of-audio-words, DeepSpectrum and auDeep features for ComParE\n2020. The studies reveal the effectiveness of acoustic features, and their\nscore level fusion with the ComParE 2020 baselines leads to an unweighted\naverage recall of 73.50% on the test set.", "published": "2020-10-08 11:36:32", "link": "http://arxiv.org/abs/2010.03907v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion Invariant Speaker Embeddings for Speaker Identification with\n  Emotional Speech", "abstract": "Emotional state of a speaker is found to have significant effect in speech\nproduction, which can deviate speech from that arising from neutral state. This\nmakes identifying speakers with different emotions a challenging task as\ngenerally the speaker models are trained using neutral speech. In this work, we\npropose to overcome this problem by creation of emotion invariant speaker\nembedding. We learn an extractor network that maps the test embeddings with\ndifferent emotions obtained using i-vector based system to an emotion invariant\nspace. The resultant test embeddings thus become emotion invariant and thereby\ncompensate the mismatch between various emotional states. The studies are\nconducted using four different emotion classes from IEMOCAP database. We obtain\nan absolute improvement of 2.6% in accuracy for speaker identification studies\nusing emotion invariant speaker embedding against average speaker model based\nframework with different emotions.", "published": "2020-10-08 11:43:16", "link": "http://arxiv.org/abs/2010.03909v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gender domain adaptation for automatic speech recognition task", "abstract": "This paper is focused on the finetuning of acoustic models for speaker\nadaptation goals on a given gender. We pretrained the Transformer baseline\nmodel on Librispeech-960 and conduct experiments with finetuning on the\ngender-specific test subsets and. In general, we do not obtain essential WER\nreduction by finetuning techniques by this approach. We achieved up to ~5%\nlower word error rate on the male subset and 3% on the female subset if the\nlayers in the encoder and decoder are not frozen, but the tuning is started\nfrom the last checkpoints. Moreover, we adapted our base model on the full L2\nArctic dataset of accented speech and fine-tuned it for particular speakers and\nmale and female genders separately. The models trained on the gender subsets\nobtained 1-2% higher accuracy when compared to the model tuned on the whole L2\nArctic dataset. Finally, we tested the concatenation of the pretrained x-vector\nvoice embeddings and embeddings from a conventional encoder, but its gain in\naccuracy is not significant.", "published": "2020-10-08 19:07:48", "link": "http://arxiv.org/abs/2010.04224v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "All for One and One for All: Improving Music Separation by Bridging\n  Networks", "abstract": "This paper proposes several improvements for music separation with deep\nneural networks (DNNs), namely a multi-domain loss (MDL) and two combination\nschemes. First, by using MDL we take advantage of the frequency and time domain\nrepresentation of audio signals. Next, we utilize the relationship among\ninstruments by jointly considering them. We do this on the one hand by\nmodifying the network architecture and introducing a CrossNet structure. On the\nother hand, we consider combinations of instrument estimates by using a new\ncombination loss (CL). MDL and CL can easily be applied to many existing\nDNN-based separation methods as they are merely loss functions which are only\nused during training and which do not affect the inference step. Experimental\nresults show that the performance of Open-Unmix (UMX), a well-known and\nstate-of-the-art open source library for music separation, can be improved by\nutilizing our above schemes. Our modifications of UMX are open-sourced together\nwith this paper.", "published": "2020-10-08 19:14:04", "link": "http://arxiv.org/abs/2010.04228v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Randomized Overdrive Neural Networks", "abstract": "By processing audio signals in the time-domain with randomly weighted\ntemporal convolutional networks (TCNs), we uncover a wide range of novel, yet\ncontrollable overdrive effects. We discover that architectural aspects, such as\nthe depth of the network, the kernel size, the number of channels, the\nactivation function, as well as the weight initialization, all have a clear\nimpact on the sonic character of the resultant effect, without the need for\ntraining. In practice, these effects range from conventional overdrive and\ndistortion, to more extreme effects, as the receptive field grows, similar to a\nfusion of distortion, equalization, delay, and reverb. To enable use by\nmusicians and producers, we provide a real-time plugin implementation. This\nallows users to dynamically design networks, listening to the results in\nreal-time. We provide a demonstration and code at\nhttps://csteinmetz1.github.io/ronn.", "published": "2020-10-08 19:42:03", "link": "http://arxiv.org/abs/2010.04237v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dataset Augmentation and Dimensionality Reduction of Pinna-Related\n  Transfer Functions", "abstract": "Efficient modeling of the inter-individual variations of head-related\ntransfer functions (HRTFs) is a key matterto the individualization of binaural\nsynthesis. In previous work, we augmented a dataset of 119 pairs of earshapes\nand pinna-related transfer functions (PRTFs), thus creating a wide dataset of\n1005 ear shapes and PRTFsgenerated by random ear drawings (WiDESPREaD) and\nacoustical simulations. In this article, we investigate thedimensionality\nreduction capacity of two principal component analysis (PCA) models of\nmagnitude PRTFs, trainedon WiDESPREaD and on the original dataset,\nrespectively. We find that the model trained on the WiDESPREaDdataset performs\nbest, regardless of the number of retained principal components.", "published": "2020-10-08 07:50:15", "link": "http://arxiv.org/abs/2010.04546v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tatum-Level Drum Transcription Based on a Convolutional Recurrent Neural\n  Network with Language Model-Based Regularized Training", "abstract": "This paper describes a neural drum transcription method that detects from\nmusic signals the onset times of drums at the $\\textit{tatum}$ level, where\ntatum times are assumed to be estimated in advance. In conventional studies on\ndrum transcription, deep neural networks (DNNs) have often been used to take a\nmusic spectrogram as input and estimate the onset times of drums at the\n$\\textit{frame}$ level. The major problem with such frame-to-frame DNNs,\nhowever, is that the estimated onset times do not often conform with the\ntypical tatum-level patterns appearing in symbolic drum scores because the\nlong-term musically meaningful structures of those patterns are difficult to\nlearn at the frame level. To solve this problem, we propose a regularized\ntraining method for a frame-to-tatum DNN. In the proposed method, a tatum-level\nprobabilistic language model (gated recurrent unit (GRU) network or\nrepetition-aware bi-gram model) is trained from an extensive collection of drum\nscores. Given that the musical naturalness of tatum-level onset times can be\nevaluated by the language model, the frame-to-tatum DNN is trained with a\nregularizer based on the pretrained language model. The experimental results\ndemonstrate the effectiveness of the proposed regularized training method.", "published": "2020-10-08 03:47:25", "link": "http://arxiv.org/abs/2010.03749v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance", "abstract": "This paper describes the process of developing a shared instrument for\nmusic-dance performance, with a particular focus on exploring the boundaries\nbetween standstill vs motion, and silence vs sound. The piece Vrengt grew from\nthe idea of enabling a true partnership between a musician and a dancer,\ndeveloping an instrument that would allow for active co-performance. Using a\nparticipatory design approach, we worked with sonification as a tool for\nsystematically exploring the dancer's bodily expressions. The exploration used\na \"spatiotemporal matrix\", with a particular focus on sonic microinteraction.\nIn the final performance, two Myo armbands were used for capturing muscle\nactivity of the arm and leg of the dancer, together with a wireless headset\nmicrophone capturing the sound of breathing. In the paper we reflect on\nmulti-user instrument paradigms, discuss our approach to creating a shared\ninstrument using sonification as a tool for the sound design, and reflect on\nthe performers' subjective evaluation of the instrument.", "published": "2020-10-08 05:50:44", "link": "http://arxiv.org/abs/2010.03779v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Texture-based Presentation Attack Detection for Automatic Speaker\n  Verification", "abstract": "Biometric systems are nowadays employed across a broad range of applications.\nThey provide high security and efficiency and, in many cases, are user\nfriendly. Despite these and other advantages, biometric systems in general and\nAutomatic speaker verification (ASV) systems in particular can be vulnerable to\nattack presentations. The most recent ASVSpoof 2019 competition showed that\nmost forms of attacks can be detected reliably with ensemble classifier-based\npresentation attack detection (PAD) approaches. These, though, depend\nfundamentally upon the complementarity of systems in the ensemble. With the\nmotivation to increase the generalisability of PAD solutions, this paper\nreports our exploration of texture descriptors applied to the analysis of\nspeech spectrogram images. In particular, we propose a common fisher vector\nfeature space based on a generative model. Experimental results show the\nsoundness of our approach: at most, 16 in 100 bona fide presentations are\nrejected whereas only one in 100 attack presentations are accepted.", "published": "2020-10-08 15:03:29", "link": "http://arxiv.org/abs/2010.04038v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastVC: Fast Voice Conversion with non-parallel data", "abstract": "This paper introduces FastVC, an end-to-end model for fast Voice Conversion\n(VC). The proposed model can convert speech of arbitrary length from multiple\nsource speakers to multiple target speakers. FastVC is based on a conditional\nAutoEncoder (AE) trained on non-parallel data and requires no annotations at\nall. This model's latent representation is shown to be speaker-independent and\nsimilar to phonemes, which is a desirable feature for VC systems. While the\ncurrent VC systems primarily focus on achieving the highest overall speech\nquality, this paper tries to balance the development concerning resources\nneeded to run the systems. Despite the simple structure of the proposed model,\nit outperforms the VC Challenge 2020 baselines on the cross-lingual task in\nterms of naturalness.", "published": "2020-10-08 18:05:30", "link": "http://arxiv.org/abs/2010.04185v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "interface : Electronic Chamber Ensemble", "abstract": "This paper presents the interface developments and music of the duo\n\"interface,\" formed by Curtis Bahn and Dan Trueman. We describe gestural\ninstrument design, interactive performance interfaces for improvisational\nmusic, spherical speakers (multi-channel, outward-radiating geodesic speaker\narrays) and Sensor-Speaker-Arrays (SenSAs: combinations of various sensor\ndevices with spherical speaker arrays). We discuss the concept, design and\nconstruction of these systems, and, give examples from several newly published\nCDs of work by Bahn and Trueman.", "published": "2020-10-08 21:57:52", "link": "http://arxiv.org/abs/2010.04276v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Emergent Jaw Predominance in Vocal Development through Stochastic\n  Optimization", "abstract": "Infant vocal babbling strongly relies on jaw oscillations, especially at the\nstage of canonical babbling, which underlies the syllabic structure of world\nlanguages. In this paper, we propose, model and analyze an hypothesis to\nexplain this predominance of the jaw in early babbling. This hypothesis states\nthat general stochastic optimization principles, when applied to learning\nsensorimotor control, automatically generate ordered babbling stages with a\npredominant exploration of jaw movements in early stages. The reason is that\nthose movements impact the auditory effects more than other articulators. In\nprevious computational models, such general principles were shown to\nselectively freeze and free degrees of freedom in a model reproducing the\nproximo-distal development observed in infant arm reaching. The contribution of\nthis paper is to show how, using the same methods, we are able to explain such\npatterns in vocal development. We present three experiments. The two first ones\nshow that the recruitment order of articulators emerging from stochastic\noptimization depends on the target sound to be achieved but that on average the\njaw is largely chosen as the first recruited articulator. The third experiment\nanalyses in more detail how the emerging recruitment order is shaped by the\ndynamics of the optimization process.", "published": "2020-10-08 15:25:06", "link": "http://arxiv.org/abs/2010.07208v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
