{"title": "Evaluating Neural Word Representations in Tensor-Based Compositional\n  Settings", "abstract": "We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks.", "published": "2014-08-26 16:28:21", "link": "http://arxiv.org/abs/1408.6179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "abstract": "This paper provides a method for improving tensor-based compositional\ndistributional models of meaning by the addition of an explicit disambiguation\nstep prior to composition. In contrast with previous research where this\nhypothesis has been successfully tested against relatively simple compositional\nmodels, in our work we use a robust model trained with linear regression. The\nresults we get in two experiments show the superiority of the prior\ndisambiguation method and suggest that the effectiveness of this approach is\nmodel-independent.", "published": "2014-08-26 16:43:30", "link": "http://arxiv.org/abs/1408.6181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
