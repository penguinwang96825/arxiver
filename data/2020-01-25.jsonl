{"title": "Reducing Noise from Competing Neighbours: Word Retrieval with Lateral\n  Inhibition in Multilink", "abstract": "Multilink is a computational model for word retrieval in monolingual and\nmultilingual individuals under different task circumstances (Dijkstra et al.,\n2018). In the present study, we added lateral inhibition to Multilink's lexical\nnetwork. Parameters were fit on the basis of reaction times from the English,\nBritish, and Dutch Lexicon Projects. We found a maximum correlation of 0.643\n(N=1,205) on these data sets as a whole. Furthermore, the simulations\nthemselves became faster as a result of adding lateral inhibition. We tested\nthe fitted model to stimuli from a neighbourhood study (Mulder et al., 2018).\nLateral inhibition was found to improve Multilink's correlations for this\nstudy, yielding an overall correlation of 0.67. Next, we explored the role of\nlateral inhibition as part of the model's task/decision system by running\nsimulations on data from two studies concerning interlingual homographs\n(Vanlangendonck et al., in press; Goertz, 2018). We found that, while lateral\ninhibition plays a substantial part in the word selection process, this alone\nis not enough to result in a correct response selection. To solve this problem,\nwe added a new task component to Multilink, especially designed to account for\nthe translation process of interlingual homographs, cognates, and\nlanguage-specific control words. The subsequent simulation results showed\npatterns remarkably similar to those in the Goertz study. The isomorphicity of\nthe simulated data to the empirical data was further attested by an overall\ncorrelation of 0.538 (N=254) between reaction times and simulated model cycle\ntimes, as well as a condition pattern correlation of 0.853 (N=8). We conclude\nthat Multilink yields an excellent fit to empirical data, particularly when a\ntask-specific setting of the inhibition parameters is allowed.", "published": "2020-01-25 16:40:43", "link": "http://arxiv.org/abs/2002.00730v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT's output layer recognizes all hidden layers? Some Intriguing\n  Phenomena and a simple way to boost BERT", "abstract": "Although Bidirectional Encoder Representations from Transformers (BERT) have\nachieved tremendous success in many natural language processing (NLP) tasks, it\nremains a black box. A variety of previous works have tried to lift the veil of\nBERT and understand each layer's functionality. In this paper, we found that\nsurprisingly the output layer of BERT can reconstruct the input sentence by\ndirectly taking each layer of BERT as input, even though the output layer has\nnever seen the input other than the final hidden layer. This fact remains true\nacross a wide variety of BERT-based models, even when some layers are\nduplicated. Based on this observation, we propose a quite simple method to\nboost the performance of BERT. By duplicating some layers in the BERT-based\nmodels to make it deeper (no extra training required in this step), they obtain\nbetter performance in the downstream tasks after fine-tuning.", "published": "2020-01-25 13:35:34", "link": "http://arxiv.org/abs/2001.09309v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-task self-supervised learning for Robust Speech Recognition", "abstract": "Despite the growing interest in unsupervised learning, extracting meaningful\nknowledge from unlabelled audio remains an open challenge. To take a step in\nthis direction, we recently proposed a problem-agnostic speech encoder (PASE),\nthat combines a convolutional encoder followed by multiple neural networks,\ncalled workers, tasked to solve self-supervised problems (i.e., ones that do\nnot require manual annotations as ground truth). PASE was shown to capture\nrelevant speech information, including speaker voice-print and phonemes. This\npaper proposes PASE+, an improved version of PASE for robust speech recognition\nin noisy and reverberant environments. To this end, we employ an online speech\ndistortion module, that contaminates the input signals with a variety of random\ndisturbances. We then propose a revised encoder that better learns short- and\nlong-term speech dynamics with an efficient combination of recurrent and\nconvolutional networks. Finally, we refine the set of workers used in\nself-supervision to encourage better cooperation. Results on TIMIT, DIRHA and\nCHiME-5 show that PASE+ significantly outperforms both the previous version of\nPASE as well as common acoustic features. Interestingly, PASE+ learns\ntransferable representations suitable for highly mismatched acoustic\nconditions.", "published": "2020-01-25 00:24:45", "link": "http://arxiv.org/abs/2001.09239v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning To Detect Keyword Parts And Whole By Smoothed Max Pooling", "abstract": "We propose smoothed max pooling loss and its application to keyword spotting\nsystems. The proposed approach jointly trains an encoder (to detect keyword\nparts) and a decoder (to detect whole keyword) in a semi-supervised manner. The\nproposed new loss function allows training a model to detect parts and whole of\na keyword, without strictly depending on frame-level labeling from LVCSR (Large\nvocabulary continuous speech recognition), making further optimization\npossible. The proposed system outperforms the baseline keyword spotting model\nin [1] due to increased optimizability. Further, it can be more easily adapted\nfor on-device learning applications due to reduced dependency on LVCSR.", "published": "2020-01-25 01:19:19", "link": "http://arxiv.org/abs/2001.09246v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Intent Classification in Question-Answering Using LSTM Architectures", "abstract": "Question-answering (QA) is certainly the best known and probably also one of\nthe most complex problem within Natural Language Processing (NLP) and\nartificial intelligence (AI). Since the complete solution to the problem of\nfinding a generic answer still seems far away, the wisest thing to do is to\nbreak down the problem by solving single simpler parts. Assuming a modular\napproach to the problem, we confine our research to intent classification for\nan answer, given a question. Through the use of an LSTM network, we show how\nthis type of classification can be approached effectively and efficiently, and\nhow it can be properly used within a basic prototype responder.", "published": "2020-01-25 15:07:07", "link": "http://arxiv.org/abs/2001.09330v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An Analysis of Word2Vec for the Italian Language", "abstract": "Word representation is fundamental in NLP tasks, because it is precisely from\nthe coding of semantic closeness between words that it is possible to think of\nteaching a machine to understand text. Despite the spread of word embedding\nconcepts, still few are the achievements in linguistic contexts other than\nEnglish. In this work, analysing the semantic capacity of the Word2Vec\nalgorithm, an embedding for the Italian language is produced. Parameter setting\nsuch as the number of epochs, the size of the context window and the number of\nnegatively backpropagated samples is explored.", "published": "2020-01-25 15:12:01", "link": "http://arxiv.org/abs/2001.09332v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Introduction of Quantification in Frame Semantics", "abstract": "Feature Structures (FSs) are a widespread tool used for decompositional\nframeworks of Attribute-Value associations. Even though they thrive in simple\nsystems, they lack a way of representing higher-order entities and relations.\nThis is however needed in Frame Semantics, where semantic dependencies should\nbe able to connect groups of individuals and their properties, especially to\nmodel quantification. To answer this issue, this master report introduces\nwrappings as a way to envelop a sub-FS and treat it as a node. Following the\nwork of [Kallmeyer, Osswald 2013], we extend its syntax, semantics and some\nproperties (translation to FOL, subsumption, unification). We can then expand\nthe proposed pipeline. Lexical minimal model sets are generated from formulas.\nThey unify by FS value equations obtained by LTAG parsing to an underspecified\nsentence representation. The syntactic approach of quantifiers allows us to use\nexisting methods to produce any possible reading. Finally, we give a\ntranscription to type-logical formulas to interact with the context in the view\nof dynamic semantics. Supported by ideas of Frame Types, this system provides a\nworkable and tractable tool for higher-order relations with FS.", "published": "2020-01-25 15:52:29", "link": "http://arxiv.org/abs/2002.00720v1", "categories": ["cs.LO", "cs.AI", "cs.CL", "03B65 (Primary) 03C30 (Secondary)", "J.5; F.4.2"], "primary_category": "cs.LO"}
{"title": "Generation-Distillation for Efficient Natural Language Understanding in\n  Low-Data Settings", "abstract": "Over the past year, the emergence of transfer learning with large-scale\nlanguage models (LM) has led to dramatic performance improvements across a\nbroad range of natural language understanding tasks. However, the size and\nmemory footprint of these large LMs makes them difficult to deploy in many\nscenarios (e.g. on mobile phones). Recent research points to knowledge\ndistillation as a potential solution, showing that when training data for a\ngiven task is abundant, it is possible to distill a large (teacher) LM into a\nsmall task-specific (student) network with minimal loss of performance.\nHowever, when such data is scarce, there remains a significant performance gap\nbetween large pretrained LMs and smaller task-specific models, even when\ntraining via distillation. In this paper, we bridge this gap with a novel\ntraining approach, called generation-distillation, that leverages large\nfinetuned LMs in two ways: (1) to generate new (unlabeled) training examples,\nand (2) to distill their knowledge into a small network using these examples.\nAcross three low-resource text classification datsets, we achieve comparable\nperformance to BERT while using 300x fewer parameters, and we outperform prior\napproaches to distillation for text classification while using 3x fewer\nparameters.", "published": "2020-01-25 08:20:46", "link": "http://arxiv.org/abs/2002.00733v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Lattice-based Improvements for Voice Triggering Using Graph Neural\n  Networks", "abstract": "Voice-triggered smart assistants often rely on detection of a trigger-phrase\nbefore they start listening for the user request. Mitigation of false triggers\nis an important aspect of building a privacy-centric non-intrusive smart\nassistant. In this paper, we address the task of false trigger mitigation (FTM)\nusing a novel approach based on analyzing automatic speech recognition (ASR)\nlattices using graph neural networks (GNN). The proposed approach uses the fact\nthat decoding lattice of a falsely triggered audio exhibits uncertainties in\nterms of many alternative paths and unexpected words on the lattice arcs as\ncompared to the lattice of a correctly triggered audio. A pure trigger-phrase\ndetector model doesn't fully utilize the intent of the user speech whereas by\nusing the complete decoding lattice of user audio, we can effectively mitigate\nspeech not intended for the smart assistant. We deploy two variants of GNNs in\nthis paper based on 1) graph convolution layers and 2) self-attention mechanism\nrespectively. Our experiments demonstrate that GNNs are highly accurate in FTM\ntask by mitigating ~87% of false triggers at 99% true positive rate (TPR).\nFurthermore, the proposed models are fast to train and efficient in parameter\nrequirements.", "published": "2020-01-25 01:34:15", "link": "http://arxiv.org/abs/2001.10822v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Regression-based music emotion prediction using triplet neural networks", "abstract": "In this paper, we adapt triplet neural networks (TNNs) to a regression task,\nmusic emotion prediction. Since TNNs were initially introduced for\nclassification, and not for regression, we propose a mechanism that allows them\nto provide meaningful low dimensional representations for regression tasks. We\nthen use these new representations as the input for regression algorithms such\nas support vector machines and gradient boosting machines. To demonstrate the\nTNNs' effectiveness at creating meaningful representations, we compare them to\ndifferent dimensionality reduction methods on music emotion prediction, i.e.,\npredicting valence and arousal values from musical audio signals. Our results\non the DEAM dataset show that by using TNNs we achieve 90% feature\ndimensionality reduction with a 9% improvement in valence prediction and 4%\nimprovement in arousal prediction with respect to our baseline models (without\nTNN). Our TNN method outperforms other dimensionality reduction methods such as\nprincipal component analysis (PCA) and autoencoders (AE). This shows that, in\naddition to providing a compact latent space representation of audio features,\nthe proposed approach has a higher performance than the baseline models.", "published": "2020-01-25 03:34:59", "link": "http://arxiv.org/abs/2001.09988v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The impact of Audio input representations on neural network based music\n  transcription", "abstract": "This paper thoroughly analyses the effect of different input representations\non polyphonic multi-instrument music transcription. We use our own GPU based\nspectrogram extraction tool, nnAudio, to investigate the influence of using a\nlinear-frequency spectrogram, log-frequency spectrogram, Mel spectrogram, and\nconstant-Q transform (CQT). Our results show that a $8.33$% increase in\ntranscription accuracy and a $9.39$% reduction in error can be obtained by\nchoosing the appropriate input representation (log-frequency spectrogram with\nSTFT window length 4,096 and 2,048 frequency bins in the spectrogram) without\nchanging the neural network design (single layer fully connected). Our\nexperiments also show that Mel spectrogram is a compact representation for\nwhich we can reduce the number of frequency bins to only 512 while still\nkeeping a relatively high music transcription accuracy.", "published": "2020-01-25 03:47:21", "link": "http://arxiv.org/abs/2001.09989v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gesticulator: A framework for semantically-aware speech-driven gesture\n  generation", "abstract": "During speech, people spontaneously gesticulate, which plays a key role in\nconveying information. Similarly, realistic co-speech gestures are crucial to\nenable natural and smooth interactions with social agents. Current end-to-end\nco-speech gesture generation systems use a single modality for representing\nspeech: either audio or text. These systems are therefore confined to producing\neither acoustically-linked beat gestures or semantically-linked gesticulation\n(e.g., raising a hand when saying \"high\"): they cannot appropriately learn to\ngenerate both gesture types. We present a model designed to produce arbitrary\nbeat and semantic gestures together. Our deep-learning based model takes both\nacoustic and semantic representations of speech as input, and generates\ngestures as a sequence of joint angle rotations as output. The resulting\ngestures can be applied to both virtual agents and humanoid robots. Subjective\nand objective evaluations confirm the success of our approach. The code and\nvideo are available at the project page\nhttps://svito-zar.github.io/gesticulator .", "published": "2020-01-25 14:42:23", "link": "http://arxiv.org/abs/2001.09326v5", "categories": ["cs.HC", "cs.LG", "eess.AS", "I.2.7; I.2.6; I.3.7"], "primary_category": "cs.HC"}
