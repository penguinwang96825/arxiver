{"title": "Hierarchical Transformer Encoders for Vietnamese Spelling Correction", "abstract": "In this paper, we propose a Hierarchical Transformer model for Vietnamese\nspelling correction problem. The model consists of multiple Transformer\nencoders and utilizes both character-level and word-level to detect errors and\nmake corrections. In addition, to facilitate future work in Vietnamese spelling\ncorrection tasks, we propose a realistic dataset collected from real-life texts\nfor the problem. We compare our method with other methods and publicly\navailable systems. The proposed method outperforms all of the contemporary\nmethods in terms of recall, precision, and f1-score. A demo version is publicly\navailable.", "published": "2021-05-28 04:09:15", "link": "http://arxiv.org/abs/2105.13578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alleviating the Knowledge-Language Inconsistency: A Study for Deep\n  Commonsense Knowledge", "abstract": "Knowledge facts are typically represented by relational triples, while we\nobserve that some commonsense facts are represented by the triples whose forms\nare inconsistent with the expression of language. This inconsistency puts\nforward a challenge for pre-trained language models to deal with these\ncommonsense knowledge facts. In this paper, we term such knowledge as deep\ncommonsense knowledge and conduct extensive exploratory experiments on it. We\nshow that deep commonsense knowledge occupies a significant part of commonsense\nknowledge while conventional methods fail to capture it effectively. We further\npropose a novel method to mine the deep commonsense knowledge distributed in\nsentences, alleviating the reliance of conventional methods on the triple\nrepresentation form of knowledge. Experiments demonstrate that the proposal\nsignificantly improves the performance in mining deep commonsense knowledge.", "published": "2021-05-28 06:26:19", "link": "http://arxiv.org/abs/2105.13607v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "abstract": "Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. By comparison, token-free models that\noperate directly on raw text (bytes or characters) have many benefits: they can\nprocess text in any language out of the box, they are more robust to noise, and\nthey minimize technical debt by removing complex and error-prone text\npreprocessing pipelines. Since byte or character sequences are longer than\ntoken sequences, past work on token-free models has often introduced new model\narchitectures designed to amortize the cost of operating directly on raw text.\nIn this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte sequences. We characterize the\ntrade-offs in terms of parameter count, training FLOPs, and inference speed,\nand show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more\nrobust to noise and perform better on tasks that are sensitive to spelling and\npronunciation. As part of our contribution, we release a new set of pre-trained\nbyte-level Transformer models based on the T5 architecture, as well as all code\nand data used in our experiments.", "published": "2021-05-28 07:03:22", "link": "http://arxiv.org/abs/2105.13626v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noised Consistency Training for Text Summarization", "abstract": "Neural abstractive summarization methods often require large quantities of\nlabeled training data. However, labeling large amounts of summarization data is\noften prohibitive due to time, financial, and expertise constraints, which has\nlimited the usefulness of summarization systems to practical applications. In\nthis paper, we argue that this limitation can be overcome by a semi-supervised\napproach: consistency training which is to leverage large amounts of unlabeled\ndata to improve the performance of supervised learning over a small corpus. The\nconsistency regularization semi-supervised learning can regularize model\npredictions to be invariant to small noise applied to input articles. By adding\nnoised unlabeled corpus to help regularize consistency training, this framework\nobtains comparative performance without using the full dataset. In particular,\nwe have verified that leveraging large amounts of unlabeled data decently\nimproves the performance of supervised learning over an insufficient labeled\ndataset.", "published": "2021-05-28 07:21:39", "link": "http://arxiv.org/abs/2105.13635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Abstractive Summarization with Limited Parallel Resources", "abstract": "Parallel cross-lingual summarization data is scarce, requiring models to\nbetter use the limited available cross-lingual resources. Existing methods to\ndo so often adopt sequence-to-sequence networks with multi-task frameworks.\nSuch approaches apply multiple decoders, each of which is utilized for a\nspecific task. However, these independent decoders share no parameters, hence\nfail to capture the relationships between the discrete phrases of summaries in\ndifferent languages, breaking the connections in order to transfer the\nknowledge of the high-resource languages to low-resource languages. To bridge\nthese connections, we propose a novel Multi-Task framework for Cross-Lingual\nAbstractive Summarization (MCLAS) in a low-resource setting. Employing one\nunified decoder to generate the sequential concatenation of monolingual and\ncross-lingual summaries, MCLAS makes the monolingual summarization task a\nprerequisite of the cross-lingual summarization (CLS) task. In this way, the\nshared decoder learns interactions involving alignments and summary patterns\nacross languages, which encourages attaining knowledge transfer. Experiments on\ntwo CLS datasets demonstrate that our model significantly outperforms three\nbaseline models in both low-resource and full-dataset scenarios. Moreover,\nin-depth analysis on the generated summaries and attention heads verifies that\ninteractions are learned well using MCLAS, which benefits the CLS task under\nlimited parallel resources.", "published": "2021-05-28 07:51:42", "link": "http://arxiv.org/abs/2105.13648v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing 4 All (NLP4All): A New Online Platform for\n  Teaching and Learning NLP Concepts", "abstract": "Natural Language Processing offers new insights into language data across\nalmost all disciplines and domains, and allows us to corroborate and/or\nchallenge existing knowledge. The primary hurdles to widening participation in\nand use of these new research tools are, first, a lack of coding skills in\nstudents across K-16, and in the population at large, and second, a lack of\nknowledge of how NLP-methods can be used to answer questions of disciplinary\ninterest outside of linguistics and/or computer science. To broaden\nparticipation in NLP and improve NLP-literacy, we introduced a new tool\nweb-based tool called Natural Language Processing 4 All (NLP4All). The intended\npurpose of NLP4All is to help teachers facilitate learning with and about NLP,\nby providing easy-to-use interfaces to NLP-methods, data, and analyses, making\nit possible for non- and novice-programmers to learn NLP concepts\ninteractively.", "published": "2021-05-28 09:57:22", "link": "http://arxiv.org/abs/2105.13704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OTTers: One-turn Topic Transitions for Open-Domain Dialogue", "abstract": "Mixed initiative in open-domain dialogue requires a system to pro-actively\nintroduce new topics. The one-turn topic transition task explores how a system\nconnects two topics in a cooperative and coherent manner. The goal of the task\nis to generate a \"bridging\" utterance connecting the new topic to the topic of\nthe previous conversation turn. We are especially interested in commonsense\nexplanations of how a new topic relates to what has been mentioned before. We\nfirst collect a new dataset of human one-turn topic transitions, which we call\nOTTers. We then explore different strategies used by humans when asked to\ncomplete such a task, and notice that the use of a bridging utterance to\nconnect the two topics is the approach used the most. We finally show how\nexisting state-of-the-art text generation models can be adapted to this task\nand examine the performance of these baselines on different splits of the\nOTTers data.", "published": "2021-05-28 10:16:59", "link": "http://arxiv.org/abs/2105.13710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Explanatory Query-Based Framework for Exploring Academic Expertise", "abstract": "The success of research institutions heavily relies upon identifying the\nright researchers \"for the job\": researchers may need to identify appropriate\ncollaborators, often from across disciplines; students may need to identify\nsuitable supervisors for projects of their interest; administrators may need to\nmatch funding opportunities with relevant researchers, and so on. Usually,\nfinding potential collaborators in institutions is a time-consuming manual\nsearch task prone to bias. In this paper, we propose a novel query-based\nframework for searching, scoring, and exploring research expertise\nautomatically, based upon processing abstracts of academic publications. Given\nuser queries in natural language, our framework finds researchers with relevant\nexpertise, making use of domain-specific knowledge bases and word embeddings.\nIt also generates explanations for its recommendations. We evaluate our\nframework with an institutional repository of papers from a leading university,\nusing, as baselines, artificial neural networks and transformer-based models\nfor a multilabel classification task to identify authors of publication\nabstracts. We also assess the cross-domain effectiveness of our framework with\na (separate) research funding repository for the same institution. We show that\nour simple method is effective in identifying matches, while satisfying\ndesirable properties and being efficient.", "published": "2021-05-28 10:48:08", "link": "http://arxiv.org/abs/2105.13728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech\n  Translation", "abstract": "Having recognized gender bias as a major issue affecting current translation\ntechnologies, researchers have primarily attempted to mitigate it by working on\nthe data front. However, whether algorithmic aspects concur to exacerbate\nunwanted outputs remains so far under-investigated. In this work, we bring the\nanalysis on gender bias in automatic translation onto a seemingly neutral yet\ncritical component: word segmentation. Can segmenting methods influence the\nability to translate gender? Do certain segmentation approaches penalize the\nrepresentation of feminine linguistic markings? We address these questions by\ncomparing 5 existing segmentation strategies on the target side of speech\ntranslation systems. Our results on two language pairs (English-Italian/French)\nshow that state-of-the-art sub-word splitting (BPE) comes at the cost of higher\ngender bias. In light of this finding, we propose a combined approach that\npreserves BPE overall translation quality, while leveraging the higher ability\nof character-based segmentation to properly translate gender.", "published": "2021-05-28 12:38:21", "link": "http://arxiv.org/abs/2105.13782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Use Monotonicity to Assess NPI Licensing", "abstract": "We investigate the semantic knowledge of language models (LMs), focusing on\n(1) whether these LMs create categories of linguistic environments based on\ntheir semantic monotonicity properties, and (2) whether these categories play a\nsimilar role in LMs as in human language understanding, using negative polarity\nitem licensing as a case study. We introduce a series of experiments consisting\nof probing with diagnostic classifiers (DCs), linguistic acceptability tasks,\nas well as a novel DC ranking method that tightly connects the probing results\nto the inner workings of the LM. By applying our experimental pipeline to LMs\ntrained on various filtered corpora, we are able to gain stronger insights into\nthe semantic generalizations that are acquired by these models.", "published": "2021-05-28 13:32:00", "link": "http://arxiv.org/abs/2105.13818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular\n  Data in Scientific Documents (SEM-TAB-FACTS)", "abstract": "Understanding tables is an important and relevant task that involves\nunderstanding table structure as well as being able to compare and contrast\ninformation within cells. In this paper, we address this challenge by\npresenting a new dataset and tasks that addresses this goal in a shared task in\nSemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in\nScientific Documents (SEM-TAB-FACTS). Our dataset contains 981\nmanually-generated tables and an auto-generated dataset of 1980 tables\nproviding over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS\nfeatured two sub-tasks. In sub-task A, the goal was to determine if a statement\nis supported, refuted or unknown in relation to a table. In sub-task B, the\nfocus was on identifying the specific cells of a table that provide evidence\nfor the statement. 69 teams signed up to participate in the task with 19\nsuccessful submissions to subtask A and 12 successful submissions to subtask B.\nWe present our results and main findings from the competition.", "published": "2021-05-28 17:21:11", "link": "http://arxiv.org/abs/2105.13995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What if This Modified That? Syntactic Interventions via Counterfactual\n  Embeddings", "abstract": "Neural language models exhibit impressive performance on a variety of tasks,\nbut their internal reasoning may be difficult to understand. Prior art aims to\nuncover meaningful properties within model representations via probes, but it\nis unclear how faithfully such probes portray information that the models\nactually use. To overcome such limitations, we propose a technique, inspired by\ncausal analysis, for generating counterfactual embeddings within models. In\nexperiments testing our technique, we produce evidence that suggests some\nBERT-based models use a tree-distance-like representation of syntax in\ndownstream prediction tasks.", "published": "2021-05-28 17:27:04", "link": "http://arxiv.org/abs/2105.14002v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature extraction and evaluation for BioMedical Question Answering", "abstract": "In this paper, we present our work on the BioASQ pipeline. The goal is to\nanswer four types of questions: summary, yes/no, factoids, and list. Our goal\nis to empirically evaluate different modules involved: the feature extractor\nand the sentence selection block. We used our pipeline to test the\neffectiveness of each module for all kinds of question types and perform error\nanalysis. We defined metrics that are useful for future research related to the\nBioASQ pipeline critical to improve the performance of the training pipeline.", "published": "2021-05-28 17:41:56", "link": "http://arxiv.org/abs/2105.14013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCPhrase: Unsupervised Context-aware Quality Phrase Tagging", "abstract": "Identifying and understanding quality phrases from context is a fundamental\ntask in text mining. The most challenging part of this task arguably lies in\nuncommon, emerging, and domain-specific phrases. The infrequent nature of these\nphrases significantly hurts the performance of phrase mining methods that rely\non sufficient phrase occurrences in the input corpus. Context-aware tagging\nmodels, though not restricted by frequency, heavily rely on domain experts for\neither massive sentence-level gold labels or handcrafted gazetteers. In this\nwork, we propose UCPhrase, a novel unsupervised context-aware quality phrase\ntagger. Specifically, we induce high-quality phrase spans as silver labels from\nconsistently co-occurring word sequences within each document. Compared with\ntypical context-agnostic distant supervision based on existing knowledge bases\n(KBs), our silver labels root deeply in the input domain and context, thus\nhaving unique advantages in preserving contextual completeness and capturing\nemerging, out-of-KB phrases. Training a conventional neural tagger based on\nsilver labels usually faces the risk of overfitting phrase surface names.\nAlternatively, we observe that the contextualized attention maps generated from\na transformer-based neural language model effectively reveal the connections\nbetween words in a surface-agnostic way. Therefore, we pair such attention maps\nwith the silver labels to train a lightweight span prediction model, which can\nbe applied to new input to recognize (unseen) quality phrases regardless of\ntheir surface names or frequency. Thorough experiments on various tasks and\ndatasets, including corpus-level phrase ranking, document-level keyphrase\nextraction, and sentence-level phrase tagging, demonstrate the superiority of\nour design over state-of-the-art pre-trained, unsupervised, and distantly\nsupervised methods.", "published": "2021-05-28 19:44:24", "link": "http://arxiv.org/abs/2105.14078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bhasacitra: Visualising the dialect geography of South Asia", "abstract": "We present Bhasacitra, a dialect mapping system for South Asia built on a\ndatabase of linguistic studies of languages of the region annotated for topic\nand location data. We analyse language coverage and look towards applications\nto typology by visualising example datasets. The application is not only meant\nto be useful for feature mapping, but also serves as a new kind of interactive\nbibliography for linguists of South Asian languages.", "published": "2021-05-28 19:52:42", "link": "http://arxiv.org/abs/2105.14082v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards More Equitable Question Answering Systems: How Much More Data Do\n  You Need?", "abstract": "Question answering (QA) in English has been widely explored, but multilingual\ndatasets are relatively new, with several methods attempting to bridge the gap\nbetween high- and low-resourced languages using data augmentation through\ntranslation and cross-lingual transfer. In this project, we take a step back\nand study which approaches allow us to take the most advantage of existing\nresources in order to produce QA systems in many languages. Specifically, we\nperform extensive analysis to measure the efficacy of few-shot approaches\naugmented with automatic translations and permutations of\ncontext-question-answer pairs. In addition, we make suggestions for future\ndataset development efforts that make better use of a fixed annotation budget,\nwith a goal of increasing the language coverage of QA datasets and systems.\nCode and data for reproducing our experiments are available here:\nhttps://github.com/NavidRajabi/EMQA.", "published": "2021-05-28 21:32:04", "link": "http://arxiv.org/abs/2105.14115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment\n  Prediction and Explanation", "abstract": "An automated system that could assist a judge in predicting the outcome of a\ncase would help expedite the judicial process. For such a system to be\npractically useful, predictions by the system should be explainable. To promote\nresearch in developing such a system, we introduce ILDC (Indian Legal Documents\nCorpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated\nwith original court decisions. A portion of the corpus (a separate test set) is\nannotated with gold standard explanations by legal experts. Based on ILDC, we\npropose the task of Court Judgment Prediction and Explanation (CJPE). The task\nrequires an automated system to predict an explainable outcome of a case. We\nexperiment with a battery of baseline models for case predictions and propose a\nhierarchical occlusion based model for explainability. Our best prediction\nmodel has an accuracy of 78% versus 94% for human legal experts, pointing\ntowards the complexity of the prediction task. The analysis of explanations by\nthe proposed algorithm reveals a significant difference in the point of view of\nthe algorithm and legal experts for explaining the judgments, pointing towards\nscope for future research.", "published": "2021-05-28 03:07:32", "link": "http://arxiv.org/abs/2105.13562v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Code-Mixed Modern Standard Arabic-Egyptian to English\n  Machine Translation", "abstract": "Recent progress in neural machine translation (NMT) has made it possible to\ntranslate successfully between monolingual language pairs where large parallel\ndata exist, with pre-trained models improving performance even further.\nAlthough there exists work on translating in code-mixed settings (where one of\nthe pairs includes text from two or more languages), it is still unclear what\nrecent success in NMT and language modeling exactly means for translating\ncode-mixed text. We investigate one such context, namely MT from code-mixed\nModern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop\nmodels under different conditions, employing both (i) standard end-to-end\nsequence-to-sequence (S2S) Transformers trained from scratch and (ii)\npre-trained S2S language models (LMs). We are able to acquire reasonable\nperformance using only MSA-EN parallel data with S2S models trained from\nscratch. We also find LMs fine-tuned on data from various Arabic dialects to\nhelp the MSAEA-EN task. Our work is in the context of the Shared Task on\nMachine Translation in Code-Switching. Our best model achieves $\\bf25.72$ BLEU,\nplacing us first on the official shared task evaluation for MSAEA-EN.", "published": "2021-05-28 03:38:35", "link": "http://arxiv.org/abs/2105.13573v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data\n  Augmentation via MiniMax", "abstract": "In Natural Language Processing (NLP), finding data augmentation techniques\nthat can produce high-quality human-interpretable examples has always been\nchallenging. Recently, leveraging kNN such that augmented examples are\nretrieved from large repositories of unlabelled sentences has made a step\ntoward interpretable augmentation. Inspired by this paradigm, we introduce\nMinimax-kNN, a sample efficient data augmentation strategy tailored for\nKnowledge Distillation (KD). We exploit a semi-supervised approach based on KD\nto train a model on augmented data. In contrast to existing kNN augmentation\ntechniques that blindly incorporate all samples, our method dynamically selects\na subset of augmented samples that maximizes KL-divergence between the teacher\nand student models. This step aims to extract the most efficient samples to\nensure our augmented data covers regions in the input space with maximum loss\nvalue. We evaluated our technique on several text classification tasks and\ndemonstrated that Minimax-kNN consistently outperforms strong baselines. Our\nresults show that Minimax-kNN requires fewer augmented examples and less\ncomputation to achieve superior performance over the state-of-the-art kNN-based\naugmentation techniques.", "published": "2021-05-28 06:32:32", "link": "http://arxiv.org/abs/2105.13608v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "THINK: A Novel Conversation Model for Generating Grammatically Correct\n  and Coherent Responses", "abstract": "Many existing conversation models that are based on the encoder-decoder\nframework have focused on ways to make the encoder more complicated to enrich\nthe context vectors so as to increase the diversity and informativeness of\ngenerated responses. However, these approaches face two problems. First, the\ndecoder is too simple to effectively utilize the previously generated\ninformation and tends to generate duplicated and self-contradicting responses.\nSecond, the complex encoder tends to generate diverse but incoherent responses\nbecause the complex context vectors may deviate from the original semantics of\ncontext. In this work, we proposed a conversation model named \"THINK\" (Teamwork\ngeneration Hover around Impressive Noticeable Keywords) to make the decoder\nmore complicated and avoid generating duplicated and self-contradicting\nresponses. The model simplifies the context vectors and increases the coherence\nof generated responses in a reasonable way. For this model, we propose Teamwork\ngeneration framework and Semantics Extractor. Compared with other baselines,\nboth automatic and human evaluation showed the advantages of our model.", "published": "2021-05-28 07:11:32", "link": "http://arxiv.org/abs/2105.13630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Text Generation Without Any Augmented Data", "abstract": "Data augmentation is an effective way to improve the performance of many\nneural text generation models. However, current data augmentation methods need\nto define or choose proper data mapping functions that map the original samples\ninto the augmented samples. In this work, we derive an objective to formulate\nthe problem of data augmentation on text generation tasks without any use of\naugmented data constructed by specific mapping functions. Our proposed\nobjective can be efficiently optimized and applied to popular loss functions on\ntext generation tasks with a convergence rate guarantee. Experiments on five\ndatasets of two text generation tasks show that our approach can approximate or\neven surpass popular data augmentation methods.", "published": "2021-05-28 07:56:51", "link": "http://arxiv.org/abs/2105.13650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage\n  in Question Answering", "abstract": "ASCENT is a fully automated methodology for extracting and consolidating\ncommonsense assertions from web contents (Nguyen et al., WWW 2021). It advances\ntraditional triple-based commonsense knowledge representation by capturing\nsemantic facets like locations and purposes, and composite concepts, i.e.,\nsubgroups and related aspects of subjects. In this demo, we present a web\nportal that allows users to understand its construction process, explore its\ncontent, and observe its impact in the use case of question answering. The demo\nwebsite and an introductory video are both available online.", "published": "2021-05-28 08:17:33", "link": "http://arxiv.org/abs/2105.13662v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Domain-Adaptive Pretraining Methods for Dialogue Understanding", "abstract": "Language models like BERT and SpanBERT pretrained on open-domain data have\nobtained impressive gains on various NLP tasks. In this paper, we probe the\neffectiveness of domain-adaptive pretraining objectives on downstream tasks. In\nparticular, three objectives, including a novel objective focusing on modeling\npredicate-argument relations, are evaluated on two challenging dialogue\nunderstanding tasks. Experimental results demonstrate that domain-adaptive\npretraining with proper objectives can significantly improve the performance of\na strong baseline on these tasks, achieving the new state-of-the-art\nperformances.", "published": "2021-05-28 08:25:27", "link": "http://arxiv.org/abs/2105.13665v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Early Exiting with Ensemble Internal Classifiers", "abstract": "As a simple technique to accelerate inference of large-scale pre-trained\nmodels, early exiting has gained much attention in the NLP community. It allows\nsamples to exit early at internal classifiers without passing through the\nentire model. Most existing work usually trains the internal classifiers\nindependently and employs an exiting strategy to decide whether or not to exit\nbased on the confidence of the current internal classifier. However, none of\nthese works takes full advantage of the fact that the internal classifiers are\ntrained to solve the same task therefore can be used to construct an ensemble.\nIn this paper, we show that a novel objective function for the training of the\nensemble internal classifiers can be naturally induced from the perspective of\nensemble learning and information theory. The proposed training objective\nconsists of two terms: one for accuracy and the other for the diversity of the\ninternal classifiers. In contrast, the objective used in prior work is exactly\nthe accuracy term of our training objective therefore only optimizes the\naccuracy but not diversity. Further, we propose a simple voting-based strategy\nthat considers predictions of all the past internal classifiers to infer the\ncorrect label and decide whether to exit. Experimental results on various NLP\ntasks show that our proposed objective function and voting-based strategy can\nachieve better accuracy-speed trade-offs.", "published": "2021-05-28 12:54:11", "link": "http://arxiv.org/abs/2105.13792v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lightweight Cross-Lingual Sentence Representation Learning", "abstract": "Large-scale models for learning fixed-dimensional cross-lingual sentence\nrepresentations like LASER (Artetxe and Schwenk, 2019b) lead to significant\nimprovement in performance on downstream tasks. However, further increases and\nmodifications based on such large-scale models are usually impractical due to\nmemory limitations. In this work, we introduce a lightweight dual-transformer\narchitecture with just 2 layers for generating memory-efficient cross-lingual\nsentence representations. We explore different training tasks and observe that\ncurrent cross-lingual training tasks leave a lot to be desired for this shallow\narchitecture. To ameliorate this, we propose a novel cross-lingual language\nmodel, which combines the existing single-word masked language model with the\nnewly proposed cross-lingual token-level reconstruction task. We further\naugment the training task by the introduction of two computationally-lite\nsentence-level contrastive learning tasks to enhance the alignment of\ncross-lingual sentence representation space, which compensates for the learning\nbottleneck of the lightweight transformer for generative tasks. Our comparisons\nwith competing models on cross-lingual sentence retrieval and multilingual\ndocument classification confirm the effectiveness of the newly proposed\ntraining tasks for a shallow model.", "published": "2021-05-28 14:10:48", "link": "http://arxiv.org/abs/2105.13856v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Approximate and Exact Numeral Systems via Reinforcement\n  Learning", "abstract": "Recent work (Xu et al., 2020) has suggested that numeral systems in different\nlanguages are shaped by a functional need for efficient communication in an\ninformation-theoretic sense. Here we take a learning-theoretic approach and\nshow how efficient communication emerges via reinforcement learning. In our\nframework, two artificial agents play a Lewis signaling game where the goal is\nto convey a numeral concept. The agents gradually learn to communicate using\nreinforcement learning and the resulting numeral systems are shown to be\nefficient in the information-theoretic framework of Regier et al. (2015);\nGibson et al. (2017). They are also shown to be similar to human numeral\nsystems of same type. Our results thus provide a mechanistic explanation via\nreinforcement learning of the recent results in Xu et al. (2020) and can\npotentially be generalized to other semantic domains.", "published": "2021-05-28 14:12:10", "link": "http://arxiv.org/abs/2105.13857v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit", "abstract": "Both performance and efficiency are crucial factors for sequence labeling\ntasks in many real-world scenarios. Although the pre-trained models (PTMs) have\nsignificantly improved the performance of various sequence labeling tasks,\ntheir computational cost is expensive. To alleviate this problem, we extend the\nrecent successful early-exit mechanism to accelerate the inference of PTMs for\nsequence labeling tasks. However, existing early-exit mechanisms are\nspecifically designed for sequence-level tasks, rather than sequence labeling.\nIn this paper, we first propose a simple extension of sentence-level early-exit\nfor sequence labeling tasks. To further reduce the computational cost, we also\npropose a token-level early-exit mechanism that allows partial tokens to exit\nearly at different layers. Considering the local dependency inherent in\nsequence labeling, we employed a window-based criterion to decide for a token\nwhether or not to exit. The token-level early-exit brings the gap between\ntraining and inference, so we introduce an extra self-sampling fine-tuning\nstage to alleviate it. The extensive experiments on three popular sequence\nlabeling tasks show that our approach can save up to 66%-75% inference cost\nwith minimal performance degradation. Compared with competitive compressed\nmodels such as DistilBERT, our approach can achieve better performance under\nthe same speed-up ratios of 2X, 3X, and 4X.", "published": "2021-05-28 14:39:26", "link": "http://arxiv.org/abs/2105.13878v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Changing the World by Changing the Data", "abstract": "NLP community is currently investing a lot more research and resources into\ndevelopment of deep learning models than training data. While we have made a\nlot of progress, it is now clear that our models learn all kinds of spurious\npatterns, social biases, and annotation artifacts. Algorithmic solutions have\nso far had limited success. An alternative that is being actively discussed is\nmore careful design of datasets so as to deliver specific signals. This\nposition paper maps out the arguments for and against data curation, and argues\nthat fundamentally the point is moot: curation already is and will be\nhappening, and it is changing the world. The question is only how much thought\nwe want to invest into that process.", "published": "2021-05-28 16:17:22", "link": "http://arxiv.org/abs/2105.13947v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cisco at SemEval-2021 Task 5: What's Toxic?: Leveraging Transformers for\n  Multiple Toxic Span Extraction from Online Comments", "abstract": "Social network platforms are generally used to share positive, constructive,\nand insightful content. However, in recent times, people often get exposed to\nobjectionable content like threat, identity attacks, hate speech, insults,\nobscene texts, offensive remarks or bullying. Existing work on toxic speech\ndetection focuses on binary classification or on differentiating toxic speech\namong a small set of categories. This paper describes the system proposed by\nteam Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared\ntask focusing on detecting the spans in the text that attribute to its\ntoxicity, in English language. We approach this problem primarily in two ways:\na sequence tagging approach and a dependency parsing approach. In our sequence\ntagging approach we tag each token in a sentence under a particular tagging\nscheme. Our best performing architecture in this approach also proved to be our\nbest performing architecture overall with an F1 score of 0.6922, thereby\nplacing us 7th on the final evaluation phase leaderboard. We also explore a\ndependency parsing approach where we extract spans from the input sentence\nunder the supervision of target span boundaries and rank our spans using a\nbiaffine model. Finally, we also provide a detailed analysis of our results and\nmodel performance in our paper.", "published": "2021-05-28 16:27:49", "link": "http://arxiv.org/abs/2105.13959v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controllable Abstractive Dialogue Summarization with Sketch Supervision", "abstract": "In this paper, we aim to improve abstractive dialogue summarization quality\nand, at the same time, enable granularity control. Our model has two primary\ncomponents and stages: 1) a two-stage generation strategy that generates a\npreliminary summary sketch serving as the basis for the final summary. This\nsummary sketch provides a weakly supervised signal in the form of\npseudo-labeled interrogative pronoun categories and key phrases extracted using\na constituency parser. 2) A simple strategy to control the granularity of the\nfinal summary, in that our model can automatically determine or control the\nnumber of generated summary sentences for a given dialogue by predicting and\nhighlighting different text spans from the source text. Our model achieves\nstate-of-the-art performance on the largest dialogue summarization corpus\nSAMSum, with as high as 50.79 in ROUGE-L score. In addition, we conduct a case\nstudy and show competitive human evaluation results and controllability to\nhuman-annotated summaries.", "published": "2021-05-28 19:05:36", "link": "http://arxiv.org/abs/2105.14064v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for on-line Sequence Transformation", "abstract": "A number of problems in the processing of sound and natural language, as well\nas in other areas, can be reduced to simultaneously reading an input sequence\nand writing an output sequence of generally different length. There are well\ndeveloped methods that produce the output sequence based on the entirely known\ninput. However, efficient methods that enable such transformations on-line do\nnot exist. In this paper we introduce an architecture that learns with\nreinforcement to make decisions about whether to read a token or write another\ntoken. This architecture is able to transform potentially infinite sequences\non-line. In an experimental study we compare it with state-of-the-art methods\nfor neural machine translation. While it produces slightly worse translations\nthan Transformer, it outperforms the autoencoder with attention, even though\nour architecture translates texts on-line thereby solving a more difficult\nproblem than both reference methods.", "published": "2021-05-28 20:31:25", "link": "http://arxiv.org/abs/2105.14097v2", "categories": ["cs.LG", "cs.CL", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Highlight Timestamp Detection Model for Comedy Videos via Multimodal\n  Sentiment Analysis", "abstract": "Nowadays, the videos on the Internet are prevailing. The precise and in-depth\nunderstanding of the videos is a difficult but valuable problem for both\nplatforms and researchers. The existing video understand models do well in\nobject recognition tasks but currently still cannot understand the abstract and\ncontextual features like highlight humor frames in comedy videos. The current\nindustrial works are also mainly focused on the basic category classification\ntask based on the appearances of objects. The feature detection methods for the\nabstract category remains blank. A data structure that includes the information\nof video frames, audio spectrum and texts provide a new direction to explore.\nThe multimodal models are proposed to make this in-depth video understanding\nmission possible. In this paper, we analyze the difficulties in abstract\nunderstanding of videos and propose a multimodal structure to obtain\nstate-of-the-art performance in this field. Then we select several benchmarks\nfor multimodal video understanding and apply the most suitable model to find\nthe best performance. At last, we evaluate the overall spotlights and drawbacks\nof the models and methods in this paper and point out the possible directions\nfor further improvements.", "published": "2021-05-28 08:39:19", "link": "http://arxiv.org/abs/2106.00451v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SciFive: a text-to-text transformer model for biomedical literature", "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has\nbeen pre-trained on large biomedical corpora. Our model outperforms the current\nSOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation,\nrelation extraction, natural language inference, and question-answering. We\nshow that text-generation methods have significant potential in a broad array\nof biomedical NLP tasks, particularly those requiring longer, more complex\noutputs. Our results support the exploration of more difficult text generation\ntasks and the development of new methods in this area", "published": "2021-05-28 06:09:23", "link": "http://arxiv.org/abs/2106.03598v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Relation Alignment for Calibrated Cross-modal Retrieval", "abstract": "Despite the achievements of large-scale multimodal pre-training approaches,\ncross-modal retrieval, e.g., image-text retrieval, remains a challenging task.\nTo bridge the semantic gap between the two modalities, previous studies mainly\nfocus on word-region alignment at the object level, lacking the matching\nbetween the linguistic relation among the words and the visual relation among\nthe regions. The neglect of such relation consistency impairs the\ncontextualized representation of image-text pairs and hinders the model\nperformance and the interpretability. In this paper, we first propose a novel\nmetric, Intra-modal Self-attention Distance (ISD), to quantify the relation\nconsistency by measuring the semantic distance between linguistic and visual\nrelations. In response, we present Inter-modal Alignment on Intra-modal\nSelf-attentions (IAIS), a regularized training method to optimize the ISD and\ncalibrate intra-modal self-attentions from the two modalities mutually via\ninter-modal alignment. The IAIS regularizer boosts the performance of\nprevailing models on Flickr30k and MS COCO datasets by a considerable margin,\nwhich demonstrates the superiority of our approach.", "published": "2021-05-28 14:25:49", "link": "http://arxiv.org/abs/2105.13868v2", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion", "abstract": "Singing voice conversion (SVC) is one promising technique which can enrich\nthe way of human-computer interaction by endowing a computer the ability to\nproduce high-fidelity and expressive singing voice. In this paper, we propose\nDiffSVC, an SVC system based on denoising diffusion probabilistic model.\nDiffSVC uses phonetic posteriorgrams (PPGs) as content features. A denoising\nmodule is trained in DiffSVC, which takes destroyed mel spectrogram produced by\nthe diffusion/forward process and its corresponding step information as input\nto predict the added Gaussian noise. We use PPGs, fundamental frequency\nfeatures and loudness features as auxiliary input to assist the denoising\nprocess. Experiments show that DiffSVC can achieve superior conversion\nperformance in terms of naturalness and voice similarity to current\nstate-of-the-art SVC approaches.", "published": "2021-05-28 14:26:40", "link": "http://arxiv.org/abs/2105.13871v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Inheritance for Pre-trained Language Models", "abstract": "Recent explorations of large-scale pre-trained language models (PLMs) have\nrevealed the power of PLMs with huge amounts of parameters, setting off a wave\nof training ever-larger PLMs. However, it requires tremendous computational\nresources to train a large-scale PLM, which may be practically unaffordable. In\naddition, existing large-scale PLMs are mainly trained from scratch\nindividually, ignoring that many well-trained PLMs are available. To this end,\nwe explore the question how could existing PLMs benefit training large-scale\nPLMs in future. Specifically, we introduce a pre-training framework named\n\"knowledge inheritance\" (KI) and explore how could knowledge distillation serve\nas auxiliary supervision during pre-training to efficiently learn larger PLMs.\nExperimental results demonstrate the superiority of KI in training efficiency.\nWe also conduct empirical analyses to explore the effects of teacher PLMs'\npre-training settings, including model architecture, pre-training data, etc.\nFinally, we show that KI could be applied to domain adaptation and knowledge\ntransfer.", "published": "2021-05-28 14:43:26", "link": "http://arxiv.org/abs/2105.13880v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weighted Training for Cross-Task Learning", "abstract": "In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted\ntraining algorithm for cross-task learning based on minimizing a\nrepresentation-based task distance between the source and target tasks. We show\nthat TAWT is easy to implement, is computationally efficient, requires little\nhyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.\nThe effectiveness of TAWT is corroborated through extensive experiments with\nBERT on four sequence tagging tasks in natural language processing (NLP),\nincluding part-of-speech (PoS) tagging, chunking, predicate detection, and\nnamed entity recognition (NER). As a byproduct, the proposed\nrepresentation-based task distance allows one to reason in a theoretically\nprincipled way about several critical aspects of cross-task learning, such as\nthe choice of the source data and the impact of fine-tuning.", "published": "2021-05-28 20:27:02", "link": "http://arxiv.org/abs/2105.14095v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An Attention Free Transformer", "abstract": "We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.", "published": "2021-05-28 20:45:30", "link": "http://arxiv.org/abs/2105.14103v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Low-complexity acoustic scene classification for multi-device audio:\n  analysis of DCASE 2021 Challenge systems", "abstract": "This paper presents the details of Task 1A Acoustic Scene Classification in\nthe DCASE 2021 Challenge. The task targeted development of low-complexity\nsolutions with good generalization properties. The provided baseline system is\nbased on a CNN architecture and post-training quantization of parameters. The\nsystem is trained using all the available training data, without any specific\ntechnique for handling device mismatch, and obtains an overall accuracy of\n47.7%, with a log loss of 1.473. The task received 99 submissions from 30\nteams, and most of the submitted systems outperformed the baseline. The most\nused techniques among the submissions were residual networks and weight\nquantization, with the top systems reaching over 70% accuracy, and log loss\nunder 0.8. The acoustic scene classification task remained a popular task in\nthe challenge, despite the increasing difficulty of the setup.", "published": "2021-05-28 10:58:11", "link": "http://arxiv.org/abs/2105.13734v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio-visual scene classification: analysis of DCASE 2021 Challenge\n  submissions", "abstract": "This paper presents the details of the Audio-Visual Scene Classification task\nin the DCASE 2021 Challenge (Task 1 Subtask B). The task is concerned with\nclassification using audio and video modalities, using a dataset of\nsynchronized recordings. This task has attracted 43 submissions from 13\ndifferent teams around the world. Among all submissions, more than half of the\nsubmitted systems have better performance than the baseline. The common\ntechniques among the top systems are the usage of large pretrained models such\nas ResNet or EfficientNet which are trained for the task-specific problem.\nFine-tuning, transfer learning, and data augmentation techniques are also\nemployed to boost the performance. More importantly, multi-modal methods using\nboth audio and video are employed by all the top 5 teams. The best system among\nall achieved a logloss of 0.195 and accuracy of 93.8%, compared to the baseline\nsystem with logloss of 0.662 and accuracy of 77.1%.", "published": "2021-05-28 08:50:31", "link": "http://arxiv.org/abs/2105.13675v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Activity Detection for Ultrasound-based Silent Speech Interfaces\n  using Convolutional Neural Networks", "abstract": "Voice Activity Detection (VAD) is not easy task when the input audio signal\nis noisy, and it is even more complicated when the input is not even an audio\nrecording. This is the case with Silent Speech Interfaces (SSI) where we record\nthe movement of the articulatory organs during speech, and we aim to\nreconstruct the speech signal from this recording. Our SSI system synthesizes\nspeech from ultrasonic videos of the tongue movement, and the quality of the\nresulting speech signals are evaluated by metrics such as the mean squared\nerror loss function of the underlying neural network and the Mel-Cepstral\nDistortion (MCD) of the reconstructed speech compared to the original. Here, we\nfirst demonstrate that the amount of silence in the training data can have an\ninfluence both on the MCD evaluation metric and on the performance of the\nneural network model. Then, we train a convolutional neural network classifier\nto separate silent and speech-containing ultrasound tongue images, using a\nconventional VAD algorithm to create the training labels from the corresponding\nspeech signal. In the experiments our ultrasound-based speech/silence separator\nachieved a classification accuracy of about 85\\% and an AUC score around 86\\%.", "published": "2021-05-28 10:33:22", "link": "http://arxiv.org/abs/2105.13718v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Artificial Reverberation", "abstract": "Artificial reverberation (AR) models play a central role in various audio\napplications. Therefore, estimating the AR model parameters (ARPs) of a\nreference reverberation is a crucial task. Although a few recent\ndeep-learning-based approaches have shown promising performance, their\nnon-end-to-end training scheme prevents them from fully exploiting the\npotential of deep neural networks. This motivates the introduction of\ndifferentiable artificial reverberation (DAR) models, allowing loss gradients\nto be back-propagated end-to-end. However, implementing the AR models with\ntheir difference equations \"as is\" in the deep learning framework severely\nbottlenecks the training speed when executed with a parallel processor like GPU\ndue to their infinite impulse response (IIR) components. We tackle this problem\nby replacing the IIR filters with finite impulse response (FIR) approximations\nwith the frequency-sampling method. Using this technique, we implement three\nDAR models -- differentiable Filtered Velvet Noise (FVN), Advanced Filtered\nVelvet Noise (AFVN), and Delay Network (DN). For each AR model, we train its\nARP estimation networks for analysis-synthesis (RIR-to-ARP) and blind\nestimation (reverberant-speech-to-ARP) task in an end-to-end manner with its\nDAR model counterpart. Experiment results show that the proposed method\nachieves consistent performance improvement over the non-end-to-end approaches\nin both objective metrics and subjective listening test results.", "published": "2021-05-28 16:02:14", "link": "http://arxiv.org/abs/2105.13940v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessing the intelligibility of vocoded speech using a remote testing\n  framework", "abstract": "Over the past year, remote speech intelligibility testing has become a\npopular and necessary alternative to traditional in-person experiments due to\nthe need for physical distancing during the COVID-19 pandemic. A remote\nframework was developed for conducting speech intelligibility tests with normal\nhearing listeners. In this study, subjects used their personal computers to\ncomplete sentence recognition tasks in anechoic and reverberant listening\nenvironments. The results obtained using this remote framework were compared\nwith previously collected in-lab results, and showed higher levels of speech\nintelligibility among remote study participants than subjects who completed the\ntest in the laboratory.", "published": "2021-05-28 21:53:52", "link": "http://arxiv.org/abs/2105.14120v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phoneme-Based Ratio Mask Estimation for Reverberant Speech Enhancement\n  in Cochlear Implant Processors", "abstract": "Cochlear implant (CI) users have considerable difficulty in understanding\nspeech in reverberant listening environments. Time-frequency (T-F) masking is a\ncommon technique that aims to improve speech intelligibility by multiplying\nreverberant speech by a matrix of gain values to suppress T-F bins dominated by\nreverberation. Recently proposed mask estimation algorithms leverage machine\nlearning approaches to distinguish between target speech and reverberant\nreflections. However, the spectro-temporal structure of speech is highly\nvariable and dependent on the underlying phoneme. One way to potentially\novercome this variability is to leverage explicit knowledge of phonemic\ninformation during mask estimation. This study proposes a phoneme-based mask\nestimation algorithm, where separate mask estimation models are trained for\neach phoneme. Sentence recognition tests were conducted in normal hearing\nlisteners to determine whether a phoneme-based mask estimation algorithm is\nbeneficial in the ideal scenario where perfect knowledge of the phoneme is\navailable. The results showed that the phoneme-based masks improved the\nintelligibility of vocoded speech when compared to conventional\nphoneme-independent masks. The results suggest that a phoneme-based speech\nenhancement strategy may potentially benefit CI users in reverberant listening\nenvironments.", "published": "2021-05-28 23:02:14", "link": "http://arxiv.org/abs/2105.14135v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Control Architecture of the Double-Cross-Correlation Processor for\n  Sampling-Rate-Offset Estimation in Acoustic Sensor Networks", "abstract": "Distributed hardware of acoustic sensor networks bears inconsistency of local\nsampling frequencies, which is detrimental to signal processing. Fundamentally,\nsampling rate offset (SRO) nonlinearly relates the discrete-time signals\nacquired by different sensor nodes. As such, retrieval of SRO from the\navailable signals requires nonlinear estimation, like double-cross-correlation\nprocessing (DXCP), and frequently results in biased estimation. SRO\ncompensation by asynchronous sampling rate conversion (ASRC) on the signals\nthen leaves an unacceptable residual. As a remedy to this problem, multi-stage\nprocedures have been devised to diminish the SRO residual with multiple\niterations of SRO estimation and ASRC over the entire signal. This paper\nconverts the mechanism of offline multi-stage processing into a continuous\nfeedback-control loop comprising a controlled ASRC unit followed by an online\nimplementation of DXCP-based SRO estimation. To support the design of an\noptimum internal model control unit for this closed-loop system, the paper\ndeploys an analytical dynamical model of the proposed online DXCP. The\nresulting control architecture then merely applies a single treatment of each\nsignal frame, while efficiently diminishing SRO bias with time. Evaluations\nwith both speech and Gaussian input demonstrate that the high accuracy of\nmulti-stage processing is maintained at the low complexity of single-stage\n(open-loop) processing.", "published": "2021-05-28 11:13:35", "link": "http://arxiv.org/abs/2105.13743v1", "categories": ["eess.AS", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
{"title": "DIVE: End-to-end Speech Diarization via Iterative Speaker Embedding", "abstract": "We introduce DIVE, an end-to-end speaker diarization algorithm. Our neural\nalgorithm presents the diarization task as an iterative process: it repeatedly\nbuilds a representation for each speaker before predicting the voice activity\nof each speaker conditioned on the extracted representations. This strategy\nintrinsically resolves the speaker ordering ambiguity without requiring the\nclassical permutation invariant training loss. In contrast with prior work, our\nmodel does not rely on pretrained speaker representations and optimizes all\nparameters of the system with a multi-speaker voice activity loss. Importantly,\nour loss explicitly excludes unreliable speaker turn boundaries from training,\nwhich is adapted to the standard collar-based Diarization Error Rate (DER)\nevaluation. Overall, these contributions yield a system redefining the\nstate-of-the-art on the standard CALLHOME benchmark, with 6.7% DER compared to\n7.8% for the best alternative.", "published": "2021-05-28 13:15:52", "link": "http://arxiv.org/abs/2105.13802v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
