{"title": "STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension", "abstract": "Abstractive dialogue summarization has long been viewed as an important\nstandalone task in natural language processing, but no previous work has\nexplored the possibility of whether abstractive dialogue summarization can also\nbe used as a means to boost an NLP system's performance on other important\ndialogue comprehension tasks. In this paper, we propose a novel type of\ndialogue summarization task - STRUctured DiaLoguE Summarization - that can help\npre-trained language models to better understand dialogues and improve their\nperformance on important dialogue comprehension tasks. We further collect human\nannotations of STRUDEL summaries over 400 dialogues and introduce a new STRUDEL\ndialogue comprehension modeling framework that integrates STRUDEL into a\ngraph-neural-network-based dialogue reasoning module over transformer encoder\nlanguage models to improve their dialogue comprehension abilities. In our\nempirical experiments on two important downstream dialogue comprehension tasks\n- dialogue question answering and dialogue response prediction - we show that\nour STRUDEL dialogue comprehension model can significantly improve the dialogue\ncomprehension performance of transformer encoder language models.", "published": "2022-12-24 04:39:54", "link": "http://arxiv.org/abs/2212.12652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Deep Transformers for Chinese-Thai Low-Resource Translation", "abstract": "In this paper, we study the use of deep Transformer translation model for the\nCCMT 2022 Chinese-Thai low-resource machine translation task. We first explore\nthe experiment settings (including the number of BPE merge operations, dropout\nprobability, embedding size, etc.) for the low-resource scenario with the\n6-layer Transformer. Considering that increasing the number of layers also\nincreases the regularization on new model parameters (dropout modules are also\nintroduced when using more layers), we adopt the highest performance setting\nbut increase the depth of the Transformer to 24 layers to obtain improved\ntranslation quality. Our work obtains the SOTA performance in the\nChinese-to-Thai translation in the constrained evaluation.", "published": "2022-12-24 05:35:04", "link": "http://arxiv.org/abs/2212.12662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Study of Gender Bias in Chemical Named Entity\n  Recognition Models", "abstract": "Chemical named entity recognition (NER) models are used in many downstream\ntasks, from adverse drug reaction identification to pharmacoepidemiology.\nHowever, it is unknown whether these models work the same for everyone.\nPerformance disparities can potentially cause harm rather than the intended\ngood. This paper assesses gender-related performance disparities in chemical\nNER systems. We develop a framework for measuring gender bias in chemical NER\nmodels using synthetic data and a newly annotated corpus of over 92,405 words\nwith self-identified gender information from Reddit. Our evaluation of multiple\nbiomedical NER models reveals evident biases. For instance, synthetic data\nsuggests female-related names are frequently misclassified as chemicals,\nespecially for brand name mentions. Additionally, we observe performance\ndisparities between female- and male-associated data in both datasets. Many\nsystems fail to detect contraceptives such as birth control. Our findings\nemphasize the biases in chemical NER models, urging practitioners to account\nfor these biases in downstream applications.", "published": "2022-12-24 18:35:35", "link": "http://arxiv.org/abs/2212.12799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Marker-based Neural Network System for Extracting Social Determinants\n  of Health", "abstract": "Objective. The impact of social determinants of health (SDoH) on patients'\nhealthcare quality and the disparity is well-known. Many SDoH items are not\ncoded in structured forms in electronic health records. These items are often\ncaptured in free-text clinical notes, but there are limited methods for\nautomatically extracting them. We explore a multi-stage pipeline involving\nnamed entity recognition (NER), relation classification (RC), and text\nclassification methods to extract SDoH information from clinical notes\nautomatically.\n  Materials and Methods. The study uses the N2C2 Shared Task data, which was\ncollected from two sources of clinical notes: MIMIC-III and University of\nWashington Harborview Medical Centers. It contains 4480 social history sections\nwith full annotation for twelve SDoHs. In order to handle the issue of\noverlapping entities, we developed a novel marker-based NER model. We used it\nin a multi-stage pipeline to extract SDoH information from clinical notes.\n  Results. Our marker-based system outperformed the state-of-the-art span-based\nmodels at handling overlapping entities based on the overall Micro-F1 score\nperformance. It also achieved state-of-the-art performance compared to the\nshared task methods.\n  Conclusion. The major finding of this study is that the multi-stage pipeline\neffectively extracts SDoH information from clinical notes. This approach can\npotentially improve the understanding and tracking of SDoHs in clinical\nsettings. However, error propagation may be an issue, and further research is\nneeded to improve the extraction of entities with complex semantic meanings and\nlow-resource entities using external knowledge.", "published": "2022-12-24 18:40:23", "link": "http://arxiv.org/abs/2212.12800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Elements of Engaging Customer Service Discourse on Social\n  Media", "abstract": "Customers are rapidly turning to social media for customer support. While\nbrand agents on these platforms are motivated and well-intentioned to help and\nengage with customers, their efforts are often ignored if their initial\nresponse to the customer does not match a specific tone, style, or topic the\ncustomer is aiming to receive. The length of a conversation can reflect the\neffort and quality of the initial response made by a brand toward collaborating\nand helping consumers, even when the overall sentiment of the conversation\nmight not be very positive. Thus, through this study, we aim to bridge this\ncritical gap in the existing literature by analyzing language's content and\nstylistic aspects such as expressed empathy, psycho-linguistic features,\ndialogue tags, and metrics for quantifying personalization of the utterances\nthat can influence the engagement of an interaction. This paper demonstrates\nthat we can predict engagement using initial customer and brand posts.", "published": "2022-12-24 18:49:03", "link": "http://arxiv.org/abs/2212.12801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kannudi -- A Reference Editor for Kannada", "abstract": "Kannudi is a reference editor for Kannada based on OPOK! and OHOK!\nprinciples, and domain knowledge. It introduces a method of input for Kannada,\ncalled OHOK!, that is, Ottu Haku Ottu Kodu! (apply pressure and give ottu).\nThis is especially suited for pressure sensitive input devices, though the\ncurrent online implementation uses the regular mechanical keyboard. OHOK! has\nthree possible modes, namely, sva-ottu (self-conjunct), kandante (as you see),\nand andante (as you say). It may be noted that kandante mode does not follow\nthe phonetic order. However, this mode may work well for those who are inclined\nto visualize as they type rather than vocalizing the sounds.\n  Kannudi also demonstrates how domain knowledge can be effectively used to\npotentially increase speed, accuracy, and user friendliness. For example,\nselection of a default vowel, automatic shunyification, and arkification. Also\nimplemented are four types Deletes that are necessary for phono-syllabic\nlanguages like Kannada.", "published": "2022-12-24 01:40:56", "link": "http://arxiv.org/abs/2301.00836v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Utilizing Priming to Identify Optimal Class Ordering to Alleviate\n  Catastrophic Forgetting", "abstract": "In order for artificial neural networks to begin accurately mimicking\nbiological ones, they must be able to adapt to new exigencies without\nforgetting what they have learned from previous training. Lifelong learning\napproaches to artificial neural networks attempt to strive towards this goal,\nyet have not progressed far enough to be realistically deployed for natural\nlanguage processing tasks. The proverbial roadblock of catastrophic forgetting\nstill gate-keeps researchers from an adequate lifelong learning model. While\nefforts are being made to quell catastrophic forgetting, there is a lack of\nresearch that looks into the importance of class ordering when training on new\nclasses for incremental learning. This is surprising as the ordering of\n\"classes\" that humans learn is heavily monitored and incredibly important.\nWhile heuristics to develop an ideal class order have been researched, this\npaper examines class ordering as it relates to priming as a scheme for\nincremental class learning. By examining the connections between various\nmethods of priming found in humans and how those are mimicked yet remain\nunexplained in life-long machine learning, this paper provides a better\nunderstanding of the similarities between our biological systems and the\nsynthetic systems while simultaneously improving current practices to combat\ncatastrophic forgetting. Through the merging of psychological priming practices\nwith class ordering, this paper is able to identify a generalizable method for\nclass ordering in NLP incremental learning tasks that consistently outperforms\nrandom class ordering.", "published": "2022-12-24 03:22:26", "link": "http://arxiv.org/abs/2212.12643v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Real or Fake Text?: Investigating Human Ability to Detect Boundaries\n  Between Human-Written and Machine-Generated Text", "abstract": "As text generated by large language models proliferates, it becomes vital to\nunderstand how humans engage with such text, and whether or not they are able\nto detect when the text they are reading did not originate with a human writer.\nPrior work on human detection of generated text focuses on the case where an\nentire passage is either human-written or machine-generated. In this paper, we\nstudy a more realistic setting where text begins as human-written and\ntransitions to being generated by state-of-the-art neural language models. We\nshow that, while annotators often struggle at this task, there is substantial\nvariance in annotator skill and that given proper incentives, annotators can\nimprove at this task over time. Furthermore, we conduct a detailed comparison\nstudy and analyze how a variety of variables (model size, decoding strategy,\nfine-tuning, prompt genre, etc.) affect human detection performance. Finally,\nwe collect error annotations from our participants and use them to show that\ncertain textual genres influence models to make different types of errors and\nthat certain sentence-level features correlate highly with annotator selection.\nWe release the RoFT dataset: a collection of over 21,000 human annotations\npaired with error classifications to encourage future work in human detection\nand evaluation of generated text.", "published": "2022-12-24 06:40:25", "link": "http://arxiv.org/abs/2212.12672v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Development of a Thermodynamics of Human Cognition and Human Culture", "abstract": "Inspired by foundational studies in classical and quantum physics, and by\ninformation retrieval studies in quantum information theory, we prove that the\nnotions of 'energy' and 'entropy' can be consistently introduced in human\nlanguage and, more generally, in human culture. More explicitly, if energy is\nattributed to words according to their frequency of appearance in a text, then\nthe ensuing energy levels are distributed non-classically, namely, they obey\nBose-Einstein, rather than Maxwell-Boltzmann, statistics, as a consequence of\nthe genuinely 'quantum indistinguishability' of the words that appear in the\ntext. Secondly, the 'quantum entanglement' due to the way meaning is carried by\na text reduces the (von Neumann) entropy of the words that appear in the text,\na behaviour which cannot be explained within classical (thermodynamic or\ninformation) entropy. We claim here that this 'quantum-type behaviour is valid\nin general in human language', namely, any text is conceptually more concrete\nthan the words composing it, which entails that the entropy of the overall text\ndecreases. In addition, we provide examples taken from cognition, where\nquantization of energy appears in categorical perception, and from culture,\nwhere entities collaborate, thus 'entangle', to decrease overall entropy. We\nuse these findings to propose the development of a new 'non-classical\nthermodynamic theory' for human cognition, which also covers broad parts of\nhuman culture and its artefacts and bridges concepts with quantum physics\nentities.", "published": "2022-12-24 18:19:05", "link": "http://arxiv.org/abs/2212.12795v2", "categories": ["q-bio.NC", "cs.CL", "quant-ph"], "primary_category": "q-bio.NC"}
