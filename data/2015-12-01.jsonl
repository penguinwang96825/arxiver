{"title": "Multilingual Language Processing From Bytes", "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads\ntext as bytes and outputs span annotations of the form [start, length, label]\nwhere start positions, lengths, and labels are separate entries in our\nvocabulary. Because we operate directly on unicode bytes rather than\nlanguage-specific words or characters, we can analyze text in many languages\nwith a single model. Due to the small vocabulary size, these multilingual\nmodels are very compact, but produce results similar to or better than the\nstate-of- the-art in Part-of-Speech tagging and Named Entity Recognition that\nuse only the provided training datasets (no external data sources). Our models\nare learning \"from scratch\" in that they do not rely on any elements of the\nstandard pipeline in Natural Language Processing (including tokenization), and\nthus can run in standalone fashion on raw text.", "published": "2015-12-01 00:23:44", "link": "http://arxiv.org/abs/1512.00103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT", "abstract": "Pivot language is employed as a way to solve the data sparseness problem in\nmachine translation, especially when the data for a particular language pair\ndoes not exist. The combination of source-to-pivot and pivot-to-target\ntranslation models can induce a new translation model through the pivot\nlanguage. However, the errors in two models may compound as noise, and still,\nthe combined model may suffer from a serious phrase sparsity problem. In this\npaper, we directly employ the word lexical model in IBM models as an additional\nresource to augment pivot phrase table. In addition, we also propose a phrase\ntable pruning method which takes into account both of the source and target\nphrasal coverage. Experimental result shows that our pruning method\nsignificantly outperforms the conventional one, which only considers source\nside phrasal coverage. Furthermore, by including the entries in the lexicon\nmodel, the phrase coverage increased, and we achieved improved results in\nChinese-to-Japanese translation using English as pivot language.", "published": "2015-12-01 08:10:49", "link": "http://arxiv.org/abs/1512.00170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inferring Interpersonal Relations in Narrative Summaries", "abstract": "Characterizing relationships between people is fundamental for the\nunderstanding of narratives. In this work, we address the problem of inferring\nthe polarity of relationships between people in narrative summaries. We\nformulate the problem as a joint structured prediction for each narrative, and\npresent a model that combines evidence from linguistic and semantic features,\nas well as features based on the structure of the social community in the text.\nWe also provide a clustering-based approach that can exploit regularities in\nnarrative types. e.g., learn an affinity for love-triangles in romantic\nstories. On a dataset of movie summaries from Wikipedia, our structured models\nprovide more than a 30% error-reduction over a competitive baseline that\nconsiders pairs of characters in isolation.", "published": "2015-12-01 01:11:46", "link": "http://arxiv.org/abs/1512.00112v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "abstract": "Artificial neural networks are powerful models, which have been widely\napplied into many aspects of machine translation, such as language modeling and\ntranslation modeling. Though notable improvements have been made in these\nareas, the reordering problem still remains a challenge in statistical machine\ntranslations. In this paper, we present a novel neural reordering model that\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\nnetworks, much longer context could be learned for reordering prediction.\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\n1000-best rescoring task show that our LSTM neural reordering feature is robust\nand achieves significant improvements over various baseline systems.", "published": "2015-12-01 08:43:19", "link": "http://arxiv.org/abs/1512.00177v3", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
