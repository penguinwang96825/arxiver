{"title": "Iterative Tree Analysis for Medical Critics", "abstract": "Large Language Models (LLMs) have been widely adopted across various domains,\nyet their application in the medical field poses unique challenges,\nparticularly concerning the generation of hallucinations. Hallucinations in\nopen-ended long medical text manifest as misleading critical claims, which are\ndifficult to verify due to two reasons. First, critical claims are often deeply\nentangled within the text and cannot be extracted based solely on surface-level\npresentation. Second, verifying these claims is challenging because\nsurface-level token-based retrieval often lacks precise or specific evidence,\nleaving the claims unverifiable without deeper mechanism-based analysis. In\nthis paper, we introduce a novel method termed Iterative Tree Analysis (ITA)\nfor medical critics. ITA is designed to extract implicit claims from long\nmedical texts and verify each claim through an iterative and adaptive tree-like\nreasoning process. This process involves a combination of top-down task\ndecomposition and bottom-up evidence consolidation, enabling precise\nverification of complex medical claims through detailed mechanism-level\nreasoning. Our extensive experiments demonstrate that ITA significantly\noutperforms previous methods in detecting factual inaccuracies in complex\nmedical text verification tasks by 10%. Additionally, we will release a\ncomprehensive test set to the public, aiming to foster further advancements in\nresearch within this domain.", "published": "2025-01-18 03:13:26", "link": "http://arxiv.org/abs/2501.10642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DNA 1.0 Technical Report", "abstract": "In this report, we present DNA 1.0 8B Instruct, a state-of-the-art bilingual\nlanguage model optimized for Korean and English language tasks. By applying\ncontinual pre-training (CPT) with high-quality Korean datasets to Llama 3.1 8B\nand subsequent supervised fine-tuning (SFT), we create an instruction-following\nmodel with enhanced Korean language capabilities. This model is then merged\nwith Llama 3.1 8B Instruct via spherical linear interpolation (SLERP) and\nundergoes further optimization through direct preference optimization (DPO) and\nknowledge distillation (KD). DNA 1.0 8B Instruct achieves state-of-the-art\nresults on Korean-specific tasks, including KMMLU (53.26%), KoBEST (83.40%),\nand BELEBELE (57.99%), while maintaining strong English capabilities on MMLU\n(66.64%), MMLU-Pro (43.05%) and GSM8K (80.52%). As an open model, DNA 1.0 8B\nInstruct represents a significant advancement in bilingual language modeling.\n  As an open model, DNA 1.0 8B Instruct is freely available through\nhttps://huggingface.co/dnotitia/Llama-DNA-1.0-8B-Instruct . For commercial\nlicensing inquiries or feedback, please contact us at\nhttps://www.dnotitia.com/contact/post-form", "published": "2025-01-18 03:48:56", "link": "http://arxiv.org/abs/2501.10648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations", "abstract": "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.", "published": "2025-01-18 07:47:25", "link": "http://arxiv.org/abs/2501.10685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing the Effects of Translation on Intertextuality using\n  Multilingual Embedding Spaces", "abstract": "Rhetorical devices are difficult to translate, but they are crucial to the\ntranslation of literary documents. We investigate the use of multilingual\nembedding spaces to characterize the preservation of intertextuality, one\ncommon rhetorical device, across human and machine translation. To do so, we\nuse Biblical texts, which are both full of intertextual references and are\nhighly translated works. We provide a metric to characterize intertextuality at\nthe corpus level and provide a quantitative analysis of the preservation of\nthis rhetorical device across extant human translations and machine-generated\ncounterparts. We go on to provide qualitative analysis of cases wherein human\ntranslations over- or underemphasize the intertextuality present in the text,\nwhereas machine translations provide a neutral baseline. This provides support\nfor established scholarship proposing that human translators have a propensity\nto amplify certain literary characteristics of the original manuscripts.", "published": "2025-01-18 11:36:17", "link": "http://arxiv.org/abs/2501.10731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Discovery of Chiasmus in Ancient Religious Text", "abstract": "Chiasmus, a debated literary device in Biblical texts, has captivated mystics\nwhile sparking ongoing scholarly discussion. In this paper, we introduce the\nfirst computational approach to systematically detect chiasmus within Biblical\npassages. Our method leverages neural embeddings to capture lexical and\nsemantic patterns associated with chiasmus, applied at multiple levels of\ntextual granularity (half-verses, verses). We also involve expert annotators to\nreview a subset of the detected patterns. Despite its computational efficiency,\nour method achieves robust results, with high inter-annotator agreement and\nsystem precision@k of 0.80 at the verse level and 0.60 at the half-verse level.\nWe further provide a qualitative analysis of the distribution of detected\nchiasmi, along with selected examples that highlight the effectiveness of our\napproach.", "published": "2025-01-18 12:02:30", "link": "http://arxiv.org/abs/2501.10739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark of French ASR Systems Based on Error Severity", "abstract": "Automatic Speech Recognition (ASR) transcription errors are commonly assessed\nusing metrics that compare them with a reference transcription, such as Word\nError Rate (WER), which measures spelling deviations from the reference, or\nsemantic score-based metrics. However, these approaches often overlook what is\nunderstandable to humans when interpreting transcription errors. To address\nthis limitation, a new evaluation is proposed that categorizes errors into four\nlevels of severity, further divided into subtypes, based on objective\nlinguistic criteria, contextual patterns, and the use of content words as the\nunit of analysis. This metric is applied to a benchmark of 10 state-of-the-art\nASR systems on French language, encompassing both HMM-based and end-to-end\nmodels. Our findings reveal the strengths and weaknesses of each system,\nidentifying those that provide the most comfortable reading experience for\nusers.", "published": "2025-01-18 21:07:18", "link": "http://arxiv.org/abs/2501.10879v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Latent-space adversarial training with post-aware calibration for\n  defending large language models against jailbreak attacks", "abstract": "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks.", "published": "2025-01-18 02:57:12", "link": "http://arxiv.org/abs/2501.10639v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MappedTrace: Tracing Pointer Remotely with Compiler-generated Maps", "abstract": "Existing precise pointer tracing methods introduce substantial runtime\noverhead to the program being traced and are applicable only at specific\nprogram execution points. We propose MappedTrace that leverages\ncompiler-generated read-only maps to accurately identify all pointers in any\ngiven snapshot of a program's execution state. The maps record the locations\nand types of pointers, allowing the tracer to precisely identify pointers\nwithout requiring the traced program to maintain bookkeeping data structures or\npoll at safe points, thereby reducing runtime overhead. By running the tracer\nfrom a different address space or machine, MappedTrace presents new\nopportunities to improve memory management techniques like memory leak\ndetection and enables novel use cases such as infinite memory abstraction for\nresource-constrained environments.", "published": "2025-01-18 06:22:28", "link": "http://arxiv.org/abs/2501.10668v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The\n  answer is No!", "abstract": "Multimodal Large Language Models (MLLMs) have achieved significant\nadvancements in tasks like Visual Question Answering (VQA) by leveraging\nfoundational Large Language Models (LLMs). However, their abilities in specific\nareas such as visual temporal understanding, which is crucial for comprehending\nreal-world dynamics, remain underexplored. To address this, we propose a\nchallenging evaluation benchmark named TemporalVQA, consisting of two parts: 1)\nTemporal Order Understanding and 2) Time-lapse Estimation. The first part\nrequires MLLMs to determine the sequence of events by analyzing temporally\nconsecutive video frames. The second part presents image pairs with varying\ntime differences, framed as multiple-choice questions, asking MLLMs to estimate\nthe time-lapse between images with options ranging from seconds to years. Our\nevaluations of advanced MLLMs, including models like GPT-4o and Gemini-1.5-Pro,\nreveal significant challenges: GPT-4o achieved only 49.1% average consistent\naccuracy in temporal order task and 70% in time-lapse estimation, with\nopen-source models performing even poorly. These findings underscore the\nlimitations of current MLLMs in visual temporal understanding and reasoning,\nhighlighting the need for further improvements for their temporal capability.\nOur dataset can be found at\nhttps://huggingface.co/datasets/fazliimam/temporal-vqa.", "published": "2025-01-18 06:41:48", "link": "http://arxiv.org/abs/2501.10674v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review", "abstract": "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.", "published": "2025-01-18 12:05:05", "link": "http://arxiv.org/abs/2501.10741v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "BAP v2: An Enhanced Task Framework for Instruction Following in\n  Minecraft Dialogues", "abstract": "Interactive agents capable of understanding and executing instructions in the\nphysical world have long been a central goal in AI research. The Minecraft\nCollaborative Building Task (MCBT) provides one such setting to work towards\nthis goal (Narayan-Chen, Jayannavar, and Hockenmaier 2019). It is a two-player\ngame in which an Architect (A) instructs a Builder (B) to construct a target\nstructure in a simulated Blocks World Environment. We focus on the challenging\nBuilder Action Prediction (BAP) subtask of predicting correct action sequences\nin a given multimodal game context with limited training data (Jayannavar,\nNarayan-Chen, and Hockenmaier 2020). We take a closer look at evaluation and\ndata for the BAP task, discovering key challenges and making significant\nimprovements on both fronts to propose BAP v2, an upgraded version of the task.\nThis will allow future work to make more efficient and meaningful progress on\nit. It comprises of: (1) an enhanced evaluation benchmark that includes a\ncleaner test set and fairer, more insightful metrics, and (2) additional\nsynthetic training data generated from novel Minecraft dialogue and target\nstructure simulators emulating the MCBT. We show that the synthetic data can be\nused to train more performant and robust neural models even with relatively\nsimple training methods. Looking ahead, such data could also be crucial for\ntraining more sophisticated, data-hungry deep transformer models and\ntraining/fine-tuning increasingly large LLMs. Although modeling is not the\nprimary focus of this work, we also illustrate the impact of our data and\ntraining methodologies on a simple LLM- and transformer-based model, thus\nvalidating the robustness of our approach, and setting the stage for more\nadvanced architectures and LLMs going forward.", "published": "2025-01-18 18:06:03", "link": "http://arxiv.org/abs/2501.10836v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot and Few-shot Learning with Instruction-following LLMs for\n  Claim Matching in Automated Fact-checking", "abstract": "The claim matching (CM) task can benefit an automated fact-checking pipeline\nby putting together claims that can be resolved with the same fact-check. In\nthis work, we are the first to explore zero-shot and few-shot learning\napproaches to the task. We consider CM as a binary classification task and\nexperiment with a set of instruction-following large language models\n(GPT-3.5-turbo, Gemini-1.5-flash, Mistral-7B-Instruct, and\nLlama-3-8B-Instruct), investigating prompt templates. We introduce a new CM\ndataset, ClaimMatch, which will be released upon acceptance. We put LLMs to the\ntest in the CM task and find that it can be tackled by leveraging more mature\nyet similar tasks such as natural language inference or paraphrase detection.\nWe also propose a pipeline for CM, which we evaluate on texts of different\nlengths.", "published": "2025-01-18 19:57:54", "link": "http://arxiv.org/abs/2501.10860v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language\n  Models", "abstract": "Reliably generating structured outputs has become a critical capability for\nmodern language model (LM) applications. Constrained decoding has emerged as\nthe dominant technology across sectors for enforcing structured outputs during\ngeneration. Despite its growing adoption, little has been done with the\nsystematic evaluation of the behaviors and performance of constrained decoding.\nConstrained decoding frameworks have standardized around JSON Schema as a\nstructured data format, with most uses guaranteeing constraint compliance given\na schema. However, there is poor understanding of the effectiveness of the\nmethods in practice. We present an evaluation framework to assess constrained\ndecoding approaches across three critical dimensions: efficiency in generating\nconstraint-compliant outputs, coverage of diverse constraint types, and quality\nof the generated outputs. To facilitate this evaluation, we introduce\nJSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world\nJSON schemas that encompass a wide range of constraints with varying\ncomplexity. We pair the benchmark with the existing official JSON Schema Test\nSuite and evaluate six state-of-the-art constrained decoding frameworks,\nincluding Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through\nextensive experiments, we gain insights into the capabilities and limitations\nof constrained decoding on structured generation with real-world JSON schemas.\nOur work provides actionable insights for improving constrained decoding\nframeworks and structured generation tasks, setting a new standard for\nevaluating constrained decoding and structured generation. We release\nJSONSchemaBench at https://github.com/guidance-ai/jsonschemabench", "published": "2025-01-18 20:26:00", "link": "http://arxiv.org/abs/2501.10868v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fanar: An Arabic-Centric Multimodal Generative AI Platform", "abstract": "We present Fanar, a platform for Arabic-centric multimodal generative AI\nsystems, that supports language, speech and image generation tasks. At the\nheart of Fanar are Fanar Star and Fanar Prime, two highly capable Arabic Large\nLanguage Models (LLMs) that are best in the class on well established\nbenchmarks for similar sized models. Fanar Star is a 7B (billion) parameter\nmodel that was trained from scratch on nearly 1 trillion clean and deduplicated\nArabic, English and Code tokens. Fanar Prime is a 9B parameter model\ncontinually trained on the Gemma-2 9B base model on the same 1 trillion token\nset. Both models are concurrently deployed and designed to address different\ntypes of prompts transparently routed through a custom-built orchestrator. The\nFanar platform provides many other capabilities including a customized Islamic\nRetrieval Augmented Generation (RAG) system for handling religious prompts, a\nRecency RAG for summarizing information about current or recent events that\nhave occurred after the pre-training data cut-off date. The platform provides\nadditional cognitive capabilities including in-house bilingual speech\nrecognition that supports multiple Arabic dialects, voice and image generation\nthat is fine-tuned to better reflect regional characteristics. Finally, Fanar\nprovides an attribution service that can be used to verify the authenticity of\nfact based generated content.\n  The design, development, and implementation of Fanar was entirely undertaken\nat Hamad Bin Khalifa University's Qatar Computing Research Institute (QCRI) and\nwas sponsored by Qatar's Ministry of Communications and Information Technology\nto enable sovereign AI technology development.", "published": "2025-01-18 05:35:32", "link": "http://arxiv.org/abs/2501.13944v1", "categories": ["cs.CL", "cs.AI", "I.2.0; D.2.0"], "primary_category": "cs.CL"}
{"title": "Unveiling the Mystery of Weight in Large Foundation Models: Gaussian\n  Distribution Never Fades", "abstract": "This paper presents a pioneering exploration of the mechanisms underlying\nlarge foundation models' (LFMs) weights, aiming to simplify AI research.\nThrough extensive observation and analysis on prevailing LFMs, we find that\nregardless of initialization strategies, their weights predominantly follow a\nGaussian distribution, with occasional sharp, inverted T-shaped, or linear\npatterns. We further discover that the weights share the i.i.d. properties of\nGaussian noise, and explore their direct relationship. We find that\ntransformation weights can be derived from Gaussian noise, and they primarily\nserve to increase the standard deviation of pre-trained weights, with their\nstandard deviation growing with layer depth. In other words, transformation\nweights broaden the acceptable deviation from the optimal weights, facilitating\nadaptation to downstream tasks. Building upon the above conclusions, we\nthoroughly discussed the nature of optimal weights, ultimately concluding that\nthey should exhibit zero-mean, symmetry, and sparsity, with the sparse values\nbeing a truncated Gaussian distribution and a few outliers. Our experiments in\nLFM adaptation and editing demonstrate the effectiveness of these insights. We\nhope these findings can provide a foundational understanding to pave the way\nfor future advancements in the LFM community.", "published": "2025-01-18 05:43:17", "link": "http://arxiv.org/abs/2501.10661v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Neural Algorithmic Reasoning for Hypergraphs with Looped Transformers", "abstract": "Looped Transformers have shown exceptional neural algorithmic reasoning\ncapability in simulating traditional graph algorithms, but their application to\nmore complex structures like hypergraphs remains underexplored. Hypergraphs\ngeneralize graphs by modeling higher-order relationships among multiple\nentities, enabling richer representations but introducing significant\ncomputational challenges. In this work, we extend the Loop Transformer\narchitecture's neural algorithmic reasoning capability to simulate hypergraph\nalgorithms, addressing the gap between neural networks and combinatorial\noptimization over hypergraphs. Specifically, we propose a novel degradation\nmechanism for reducing hypergraphs to graph representations, enabling the\nsimulation of graph-based algorithms, such as Dijkstra's shortest path.\nFurthermore, we introduce a hyperedge-aware encoding scheme to simulate\nhypergraph-specific algorithms, exemplified by Helly's algorithm. We establish\ntheoretical guarantees for these simulations, demonstrating the feasibility of\nprocessing high-dimensional and combinatorial data using Loop Transformers.\nThis work highlights the potential of Transformers as general-purpose\nalgorithmic solvers for structured data.", "published": "2025-01-18 07:58:45", "link": "http://arxiv.org/abs/2501.10688v2", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks\n  For LLMs", "abstract": "Various benchmarks have been proposed to assess the performance of large\nlanguage models (LLMs) in different coding scenarios. We refer to them as\ncode-related benchmarks. However, there are no systematic guidelines by which\nsuch a benchmark should be developed to ensure its quality, reliability, and\nreproducibility. We propose How2Bench, which is comprised of a 55-criteria\nchecklist as a set of guidelines to govern the development of code-related\nbenchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks\nreleased within the past decade and found concerning issues. Nearly 70% of the\nbenchmarks did not take measures for data quality assurance; over 10% did not\neven open source or only partially open source. Many highly cited benchmarks\nhave loopholes, including duplicated samples, incorrect reference\ncodes/tests/prompts, and unremoved sensitive/confidential information. Finally,\nwe conducted a human study involving 49 participants, which revealed\nsignificant gaps in awareness of the importance of data quality,\nreproducibility, and transparency.", "published": "2025-01-18 09:51:57", "link": "http://arxiv.org/abs/2501.10711v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Language Representation Favored Zero-Shot Cross-Domain Cognitive\n  Diagnosis", "abstract": "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).", "published": "2025-01-18 03:35:44", "link": "http://arxiv.org/abs/2501.13943v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Method for Multi-Hop Question Answering on Persian Knowledge Graph", "abstract": "Question answering systems are the latest evolution in information retrieval\ntechnology, designed to accept complex queries in natural language and provide\naccurate answers using both unstructured and structured knowledge sources.\nKnowledge Graph Question Answering (KGQA) systems fulfill users' information\nneeds by utilizing structured data, representing a vast number of facts as a\ngraph. However, despite significant advancements, major challenges persist in\nanswering multi-hop complex questions, particularly in Persian. One of the main\nchallenges is the accurate understanding and transformation of these multi-hop\ncomplex questions into semantically equivalent SPARQL queries, which allows for\nprecise answer retrieval from knowledge graphs. In this study, to address this\nissue, a dataset of 5,600 Persian multi-hop complex questions was developed,\nalong with their decomposed forms based on the semantic representation of the\nquestions. Following this, Persian language models were trained using this\ndataset, and an architecture was proposed for answering complex questions using\na Persian knowledge graph. Finally, the proposed method was evaluated against\nsimilar systems on the PeCoQ dataset. The results demonstrated the superiority\nof our approach, with an improvement of 12.57% in F1-score and 12.06% in\naccuracy compared to the best comparable method.", "published": "2025-01-18 18:11:29", "link": "http://arxiv.org/abs/2501.16350v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "FlashSR: One-step Versatile Audio Super-resolution via Diffusion\n  Distillation", "abstract": "Versatile audio super-resolution (SR) is the challenging task of restoring\nhigh-frequency components from low-resolution audio with sampling rates between\n4kHz and 32kHz in various domains such as music, speech, and sound effects.\nPrevious diffusion-based SR methods suffer from slow inference due to the need\nfor a large number of sampling steps. In this paper, we introduce FlashSR, a\nsingle-step diffusion model for versatile audio super-resolution aimed at\nproducing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion\ndistillation with three objectives: distillation loss, adversarial loss, and\ndistribution-matching distillation loss. We further enhance performance by\nproposing the SR Vocoder, which is specifically designed for SR models\noperating on mel-spectrograms. FlashSR demonstrates competitive performance\nwith the current state-of-the-art model in both objective and subjective\nevaluations while being approximately 22 times faster.", "published": "2025-01-18 16:01:33", "link": "http://arxiv.org/abs/2501.10807v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusicEval: A Generative Music Dataset with Expert Ratings for Automatic\n  Text-to-Music Evaluation", "abstract": "The technology for generating music from textual descriptions has seen rapid\nadvancements. However, evaluating text-to-music (TTM) systems remains a\nsignificant challenge, primarily due to the difficulty of balancing performance\nand cost with existing objective and subjective evaluation methods. In this\npaper, we propose an automatic assessment task for TTM models to align with\nhuman perception. To address the TTM evaluation challenges posed by the\nprofessional requirements of music evaluation and the complexity of the\nrelationship between text and music, we collect MusicEval, the first generative\nmusic assessment dataset. This dataset contains 2,748 music clips generated by\n31 advanced and widely used models in response to 384 text prompts, along with\n13,740 ratings from 14 music experts. Furthermore, we design a CLAP-based\nassessment model built on this dataset, and our experimental results validate\nthe feasibility of the proposed task, providing a valuable reference for future\ndevelopment in TTM evaluation. The dataset is available at\nhttps://www.aishelltech.com/AISHELL_7A.", "published": "2025-01-18 16:21:03", "link": "http://arxiv.org/abs/2501.10811v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Detection Based on MFCC and CNN-LSTM Architecture", "abstract": "Emotion detection techniques have been applied to multiple cases mainly from\nfacial image features and vocal audio features, of which the latter aspect is\ndisputed yet not only due to the complexity of speech audio processing but also\nthe difficulties of extracting appropriate features. Part of the SAVEE and\nRAVDESS datasets are selected and combined as the dataset, containing seven\nsorts of common emotions (i.e. happy, neutral, sad, anger, disgust, fear, and\nsurprise) and thousands of samples. Based on the Librosa package, this paper\nprocesses the initial audio input into waveplot and spectrum for analysis and\nconcentrates on multiple features including MFCC as targets for feature\nextraction. The hybrid CNN-LSTM architecture is adopted by virtue of its strong\ncapability to deal with sequential data and time series, which mainly consists\nof four convolutional layers and three long short-term memory layers. As a\nresult, the architecture achieved an accuracy of 61.07% comprehensively for the\ntest set, among which the detection of anger and neutral reaches a performance\nof 75.31% and 71.70% respectively. It can also be concluded that the\nclassification accuracy is dependent on the properties of emotion to some\nextent, with frequently-used and distinct-featured emotions having less\nprobability to be misclassified into other categories. Emotions like surprise\nwhose meaning depends on the specific context are more likely to confuse with\npositive or negative emotions, and negative emotions also have a possibility to\nget mixed with each other.", "published": "2025-01-18 06:15:54", "link": "http://arxiv.org/abs/2501.10666v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented\n  Generation for Automatic Speech Recognition Systems", "abstract": "Automatic Speech Recognition (ASR) systems have demonstrated remarkable\nperformance across various applications. However, limited data and the unique\nlanguage features of specific domains, such as low-resource languages,\nsignificantly degrade their performance and lead to higher Word Error Rates\n(WER). In this study, we propose Generative Error Correction via\nRetrieval-Augmented Generation (GEC-RAG), a novel approach designed to improve\nASR accuracy for low-resource domains, like Persian. Our approach treats the\nASR system as a black-box, a common practice in cloud-based services, and\nproposes a Retrieval-Augmented Generation (RAG) approach within the In-Context\nLearning (ICL) scheme to enhance the quality of ASR predictions. By\nconstructing a knowledge base that pairs ASR predictions (1-best and 5-best\nhypotheses) with their corresponding ground truths, GEC-RAG retrieves lexically\nsimilar examples to the ASR transcription using the Term Frequency-Inverse\nDocument Frequency (TF-IDF) measure. This process provides relevant error\npatterns of the system alongside the ASR transcription to the Generative Large\nLanguage Model (LLM), enabling targeted corrections. Our results demonstrate\nthat this strategy significantly reduces WER in Persian and highlights a\npotential for domain adaptation and low-resource scenarios. This research\nunderscores the effectiveness of using RAG in enhancing ASR systems without\nrequiring direct model modification or fine-tuning, making it adaptable to any\ndomain by simply updating the transcription knowledge base with domain-specific\ndata.", "published": "2025-01-18 11:53:22", "link": "http://arxiv.org/abs/2501.10734v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Experimental Study on Joint Modeling for Sound Event Localization and\n  Detection with Source Distance Estimation", "abstract": "In traditional sound event localization and detection (SELD) tasks, the focus\nis typically on sound event detection (SED) and direction-of-arrival (DOA)\nestimation, but they fall short of providing full spatial information about the\nsound source. The 3D SELD task addresses this limitation by integrating source\ndistance estimation (SDE), allowing for complete spatial localization. We\npropose three approaches to tackle this challenge: a novel method with\nindependent training and joint prediction, which firstly treats DOA and\ndistance estimation as separate tasks and then combines them to solve 3D SELD;\na dual-branch representation with source Cartesian coordinate used for\nsimultaneous DOA and distance estimation; and a three-branch structure that\njointly models SED, DOA, and SDE within a unified framework. Our proposed\nmethod ranked first in the DCASE 2024 Challenge Task 3, demonstrating the\neffectiveness of joint modeling for addressing the 3D SELD task. The relevant\ncode for this paper will be open-sourced in the future.", "published": "2025-01-18 12:57:21", "link": "http://arxiv.org/abs/2501.10755v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
