{"title": "TradingAgents: Multi-Agents LLM Financial Trading Framework", "abstract": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/PioneerFintech.", "published": "2024-12-28 12:54:06", "link": "http://arxiv.org/abs/2412.20138v5", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "Optimal Execution Strategies Incorporating Internal Liquidity Through Market Making", "abstract": "This paper introduces a new algorithmic execution model that integrates\ninterbank limit and market orders with internal liquidity generated through\nmarket making. Based on the Cartea et al.\\cite{cartea2015algorithmic}\nframework, we incorporate market impact in interbank orders while excluding it\nfor internal market-making transactions. Our model aims to optimize the balance\nbetween interbank and internal liquidity, reducing market impact and improving\nexecution efficiency.", "published": "2024-12-28 03:07:57", "link": "http://arxiv.org/abs/2501.07581v1", "categories": ["q-fin.TR", "math.PR"], "primary_category": "q-fin.TR"}
<<<<<<< HEAD
{"title": "STAYKATE: Hybrid In-Context Example Selection Combining\n  Representativeness Sampling and Retrieval-based Approach -- A Case Study on\n  Science Domains", "abstract": "Large language models (LLMs) demonstrate the ability to learn in-context,\noffering a potential solution for scientific information extraction, which\noften contends with challenges such as insufficient training data and the high\ncost of annotation processes. Given that the selection of in-context examples\ncan significantly impact performance, it is crucial to design a proper method\nto sample the efficient ones. In this paper, we propose STAYKATE, a\nstatic-dynamic hybrid selection method that combines the principles of\nrepresentativeness sampling from active learning with the prevalent\nretrieval-based approach. The results across three domain-specific datasets\nindicate that STAYKATE outperforms both the traditional supervised methods and\nexisting selection methods. The enhancement in performance is particularly\npronounced for entity types that other methods pose challenges.", "published": "2024-12-28 06:13:50", "link": "http://arxiv.org/abs/2412.20043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"My life is miserable, have to sign 500 autographs everyday\": Exposing\n  Humblebragging, the Brags in Disguise", "abstract": "Humblebragging is a phenomenon where individuals present self-promotional\nstatements under the guise of modesty or complaints. For example, a statement\nlike, \"Ugh, I can't believe I got promoted to lead the entire team. So\nstressful!\", subtly highlights an achievement while pretending to be\ncomplaining. Detecting humblebragging is important for machines to better\nunderstand the nuances of human language, especially in tasks like sentiment\nanalysis and intent recognition. However, this topic has not yet been studied\nin computational linguistics. For the first time, we introduce the task of\nautomatically detecting humblebragging in text. We formalize the task by\nproposing a 4-tuple definition of humblebragging and evaluate machine learning,\ndeep learning, and large language models (LLMs) on this task, comparing their\nperformance with humans. We also create and release a dataset called HB24,\ncontaining 3,340 humblebrags generated using GPT-4o. Our experiments show that\ndetecting humblebragging is non-trivial, even for humans. Our best model\nachieves an F1-score of 0.88. This work lays the foundation for further\nexploration of this nuanced linguistic phenomenon and its integration into\nbroader natural language understanding systems.", "published": "2024-12-28 07:14:55", "link": "http://arxiv.org/abs/2412.20057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts", "abstract": "Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions.", "published": "2024-12-28 07:30:05", "link": "http://arxiv.org/abs/2412.20061v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Multi-Agent Collaboration with Tool Use for Online Planning in\n  Complex Table Question Answering", "abstract": "Complex table question answering (TQA) aims to answer questions that require\ncomplex reasoning, such as multi-step or multi-category reasoning, over data\nrepresented in tabular form. Previous approaches demonstrated notable\nperformance by leveraging either closed-source large language models (LLMs) or\nfine-tuned open-weight LLMs. However, fine-tuning LLMs requires high-quality\ntraining data, which is costly to obtain, and utilizing closed-source LLMs\nposes accessibility challenges and leads to reproducibility issues. In this\npaper, we propose Multi-Agent Collaboration with Tool use (MACT), a framework\nthat requires neither closed-source models nor fine-tuning. In MACT, a planning\nagent and a coding agent that also make use of tools collaborate to answer\nquestions. Our experiments on four TQA benchmarks show that MACT outperforms\nprevious SoTA systems on three out of four benchmarks and that it performs\ncomparably to the larger and more expensive closed-source model GPT-4 on two\nbenchmarks, even when using only open-weight models without any fine-tuning. We\nconduct extensive analyses to prove the effectiveness of MACT's multi-agent\ncollaboration in TQA.", "published": "2024-12-28 13:13:33", "link": "http://arxiv.org/abs/2412.20145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YAD: Leveraging T5 for Improved Automatic Diacritization of Yor\u00f9b\u00e1\n  Text", "abstract": "In this work, we present Yor\\`ub\\'a automatic diacritization (YAD) benchmark\ndataset for evaluating Yor\\`ub\\'a diacritization systems. In addition, we\npre-train text-to-text transformer, T5 model for Yor\\`ub\\'a and showed that\nthis model outperform several multilingually trained T5 models. Lastly, we\nshowed that more data and larger models are better at diacritization for\nYor\\`ub\\'a", "published": "2024-12-28 17:03:30", "link": "http://arxiv.org/abs/2412.20218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriHG: News headline generation for African Languages", "abstract": "This paper introduces AfriHG -- a news headline generation dataset created by\ncombining from XLSum and MasakhaNEWS datasets focusing on 16 languages widely\nspoken by Africa. We experimented with two seq2eq models (mT5-base and AfriTeVa\nV2), and Aya-101 LLM. Our results show that Africa-centric seq2seq models such\nas AfriTeVa V2 outperform the massively multilingual mT5-base model. Finally,\nwe show that the performance of fine-tuning AfriTeVa V2 with 313M parameters is\ncompetitive to prompting Aya-101 LLM with more than 13B parameters.", "published": "2024-12-28 17:34:17", "link": "http://arxiv.org/abs/2412.20223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning", "abstract": "Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.", "published": "2024-12-28 17:48:33", "link": "http://arxiv.org/abs/2412.20227v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge\n  Frequency Control and Uncertainty", "abstract": "The rapid development of LLMs has sparked extensive research into their\nfactual knowledge. Current works claim that LLMs fall short on questions\nrequiring less frequent knowledge. However, their proof is incomplete since\nthey only study the influence of entity frequency, which can not fully\nrepresent knowledge frequency. So we introduce ComparisonQA benchmark,\ncontaining 283K abstract questions, each instantiated by a pair of\nhigh-frequency and low-frequency entities. It ensures a controllable comparison\nbecause the difference of knowledge frequency between such a pair is only\nrelated to entity frequency. In addition, to avoid possible semantic shortcuts,\nwhich is a severe problem of current LLMs study, we design a two-round method\nfor knowledge robustness measurement utilizing both correctness and\nuncertainty. Experiments reveal that LLMs exhibit particularly low robustness\nregarding low-frequency knowledge, and GPT-4o is even the worst under this\nmeasurement. Besides, we introduce an automatic method to filter out questions\nwith low-quality and shortcuts to form ComparisonQA-Hard. We find that\nuncertainty effectively identifies such questions while maintaining the data\nsize.", "published": "2024-12-28 19:51:08", "link": "http://arxiv.org/abs/2412.20251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scoring with Large Language Models: A Study on Measuring Empathy of\n  Responses in Dialogues", "abstract": "In recent years, Large Language Models (LLMs) have become increasingly more\npowerful in their ability to complete complex tasks. One such task in which\nLLMs are often employed is scoring, i.e., assigning a numerical value from a\ncertain scale to a subject. In this paper, we strive to understand how LLMs\nscore, specifically in the context of empathy scoring. We develop a novel and\ncomprehensive framework for investigating how effective LLMs are at measuring\nand scoring empathy of responses in dialogues, and what methods can be employed\nto deepen our understanding of LLM scoring. Our strategy is to approximate the\nperformance of state-of-the-art and fine-tuned LLMs with explicit and\nexplainable features. We train classifiers using various features of dialogues\nincluding embeddings, the Motivational Interviewing Treatment Integrity (MITI)\nCode, a set of explicit subfactors of empathy as proposed by LLMs, and a\ncombination of the MITI Code and the explicit subfactors. Our results show that\nwhen only using embeddings, it is possible to achieve performance close to that\nof generic LLMs, and when utilizing the MITI Code and explicit subfactors\nscored by an LLM, the trained classifiers can closely match the performance of\nfine-tuned LLMs. We employ feature selection methods to derive the most crucial\nfeatures in the process of empathy scoring. Our work provides a new perspective\ntoward understanding LLM empathy scoring and helps the LLM community explore\nthe potential of LLM scoring in social science studies.", "published": "2024-12-28 20:37:57", "link": "http://arxiv.org/abs/2412.20264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Preference Left Behind: Group Distributional Preference Optimization", "abstract": "Preferences within a group of people are not uniform but follow a\ndistribution. While existing alignment methods like Direct Preference\nOptimization (DPO) attempt to steer models to reflect human preferences, they\nstruggle to capture the distributional pluralistic preferences within a group.\nThese methods often skew toward dominant preferences, overlooking the diversity\nof opinions, especially when conflicting preferences arise. To address this\nissue, we propose Group Distribution Preference Optimization (GDPO), a novel\nframework that aligns language models with the distribution of preferences\nwithin a group by incorporating the concept of beliefs that shape individual\npreferences. GDPO calibrates a language model using statistical estimation of\nthe group's belief distribution and aligns the model with belief-conditioned\npreferences, offering a more inclusive alignment framework than traditional\nmethods. In experiments using both synthetic controllable opinion generation\nand real-world movie review datasets, we show that DPO fails to align with the\ntargeted belief distributions, while GDPO consistently reduces this alignment\ngap during training. Moreover, our evaluation metrics demonstrate that GDPO\noutperforms existing approaches in aligning with group distributional\npreferences, marking a significant advance in pluralistic alignment.", "published": "2024-12-28 23:30:47", "link": "http://arxiv.org/abs/2412.20299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Context Gaps: Enhancing Comprehension in Long-Form Social\n  Conversations Through Contextualized Excerpts", "abstract": "We focus on enhancing comprehension in small-group recorded conversations,\nwhich serve as a medium to bring people together and provide a space for\nsharing personal stories and experiences on crucial social matters. One way to\nparse and convey information from these conversations is by sharing highlighted\nexcerpts in subsequent conversations. This can help promote a collective\nunderstanding of relevant issues, by highlighting perspectives and experiences\nto other groups of people who might otherwise be unfamiliar with and thus\nunable to relate to these experiences. The primary challenge that arises then\nis that excerpts taken from one conversation and shared in another setting\nmight be missing crucial context or key elements that were previously\nintroduced in the original conversation. This problem is exacerbated when\nconversations become lengthier and richer in themes and shared experiences. To\naddress this, we explore how Large Language Models (LLMs) can enrich these\nexcerpts by providing socially relevant context. We present approaches for\neffective contextualization to improve comprehension, readability, and empathy.\nWe show significant improvements in understanding, as assessed through\nsubjective and objective evaluations. While LLMs can offer valuable context,\nthey struggle with capturing key social aspects. We release the Human-annotated\nSalient Excerpts (HSE) dataset to support future work. Additionally, we show\nhow context-enriched excerpts can provide more focused and comprehensive\nconversation summaries.", "published": "2024-12-28 01:29:53", "link": "http://arxiv.org/abs/2412.19966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical\n  Characters", "abstract": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online.", "published": "2024-12-28 05:01:26", "link": "http://arxiv.org/abs/2412.20024v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Children's Acquisition of Tail-recursion Sequences: A Review of Locative\n  Recursion and Possessive Recursion as Examples", "abstract": "Recursion is the nature of human natural language. Since Chomsky proposed\ngenerative grammar, many scholars have studied recursion either theoretically\nor empirically. However, by observing children's acquisition of tail recursion\nsequences, we can verify the nativism of language supported by universal\ngrammar and reveal the cognitive mechanism of human brain. To date, our\nunderstanding of children's acquisition path of recursion and influencing\nfactors still remain controversial. This systematic review summarizes the\nresearch of tail recursive sequence by taking possessive recursion and locative\nrecursion as examples, focusing on the experimental methods, acquisition paths,\nand influencing factors of tail recursive sequence. The current behavioural\nexperiments reveal that, the debate about children's performance revolves\naround: 1) Gradual acquisition or synchronous acquisition. 2) symmetry or\nasymmetry between the acquisition of locative recursion sequences and\npossessive recursion sequences. We presume that children can acquire recursion\nquickly in a short period of time thanks to the language acquisition device,\nthough there are also scholars who believe that a third factor also plays a\nrole.", "published": "2024-12-28 05:47:38", "link": "http://arxiv.org/abs/2412.20033v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Extract Information from Hybrid Long Documents Leveraging LLMs: A\n  Framework and Dataset", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments.", "published": "2024-12-28 07:54:14", "link": "http://arxiv.org/abs/2412.20072v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation", "abstract": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.", "published": "2024-12-28 12:11:28", "link": "http://arxiv.org/abs/2412.20127v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building a Rich Dataset to Empower the Persian Question Answering\n  Systems", "abstract": "Question answering systems provide short, precise, and specific answers to\nquestions. So far, many robust question answering systems have been developed\nfor English, while some languages with fewer resources, like Persian, have few\nnumbers of standard dataset. In this study, a comprehensive open-domain dataset\nis presented for Persian. This dataset is called NextQuAD and has 7,515\ncontexts, including 23,918 questions and answers. Then, a BERT-based question\nanswering model has been applied to this dataset using two pre-trained language\nmodels, including ParsBERT and XLM-RoBERTa. The results of these two models\nhave been ensembled using mean logits. Evaluation on the development set shows\n0.95 Exact Match (EM) and 0.97 Fl_score. Also, to compare the NextQuAD with\nother Persian datasets, our trained model on the NextQuAD, is evaluated on two\nother datasets named PersianQA and ParSQuAD. Comparisons show that the proposed\nmodel increased EM by 0.39 and 0.14 respectively in PersianQA and\nParSQuAD-manual, while a slight EM decline of 0.007 happened in\nParSQuAD-automatic.", "published": "2024-12-28 16:53:25", "link": "http://arxiv.org/abs/2412.20212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Emotion: Speech Perception Patterns in Individuals with\n  Self-reported Depression", "abstract": "The current study examines the relationship between self-reported depression\nand the perception of affective speech within the Indian population. PANAS and\nPHQ-9 were used to assess current mood and depression, respectively.\nParticipants' emotional reactivity was recorded on a valence and arousal scale\nagainst the affective speech audio presented in a sequence. No significant\ndifferences between the depression and no-depression groups were observed for\nany of the emotional stimuli, except the audio file depicting neutral emotion.\nSignificantly higher PANAS scores by the depression than the no-depression\ngroup indicate the impact of pre-disposed mood on the current mood status.\nContrary to previous findings, this study did not observe reduced positive\nemotional reactivity by the depression group. However, the results demonstrated\nconsistency in emotional reactivity for speech stimuli depicting sadness and\nanger across all measures of emotion perception.", "published": "2024-12-28 16:54:25", "link": "http://arxiv.org/abs/2412.20213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Mathematical Analysis", "abstract": "Mathematical problem-solving is a key field in artificial intelligence (AI)\nand a critical benchmark for evaluating the capabilities of large language\nmodels (LLMs). While extensive research has focused on mathematical\nproblem-solving, most existing work and datasets concentrate on computational\ntasks, leaving gaps in areas like mathematical analysis, which demands rigorous\nproofs and formal reasoning. We developed the DEMI-MathAnalysis dataset,\ncomprising proof-based problems from mathematical analysis topics such as\nSequences and Limits, Infinite Series, and Convex Functions. We also designed a\nguiding framework to rigorously enhance LLMs' ability to solve these problems.\nThrough fine-tuning LLMs on this dataset and employing our framework, we\nobserved significant improvements in their capability to generate logical,\ncomplete, and elegant proofs. This work addresses critical gaps in mathematical\nreasoning and contributes to advancing trustworthy AI capable of handling\nformalized mathematical language. The code is publicly accessible at LLMs for\nMathematical Analysis.", "published": "2024-12-28 20:37:55", "link": "http://arxiv.org/abs/2501.00059v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models For Optimized Item Categorization using\n  UNSPSC Taxonomy", "abstract": "Effective item categorization is vital for businesses, enabling the\ntransformation of unstructured datasets into organized categories that\nstreamline inventory management. Despite its importance, item categorization\nremains highly subjective and lacks a uniform standard across industries and\nbusinesses. The United Nations Standard Products and Services Code (UNSPSC)\nprovides a standardized system for cataloguing inventory, yet employing UNSPSC\ncategorizations often demands significant manual effort. This paper\ninvestigates the deployment of Large Language Models (LLMs) to automate the\nclassification of inventory data into UNSPSC codes based on Item Descriptions.\nWe evaluate the accuracy and efficiency of LLMs in categorizing diverse\ndatasets, exploring their language processing capabilities and their potential\nas a tool for standardizing inventory classification. Our findings reveal that\nLLMs can substantially diminish the manual labor involved in item\ncategorization while maintaining high accuracy, offering a scalable solution\nfor businesses striving to enhance their inventory management practices.", "published": "2024-12-28 00:12:13", "link": "http://arxiv.org/abs/2503.04728v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Generalist to Specialist: A Survey of Large Language Models for\n  Chemistry", "abstract": "Large Language Models (LLMs) have significantly transformed our daily life\nand established a new paradigm in natural language processing (NLP). However,\nthe predominant pretraining of LLMs on extensive web-based texts remains\ninsufficient for advanced scientific discovery, particularly in chemistry. The\nscarcity of specialized chemistry data, coupled with the complexity of\nmulti-modal data such as 2D graph, 3D structure and spectrum, present distinct\nchallenges. Although several studies have reviewed Pretrained Language Models\n(PLMs) in chemistry, there is a conspicuous absence of a systematic survey\nspecifically focused on chemistry-oriented LLMs. In this paper, we outline\nmethodologies for incorporating domain-specific chemistry knowledge and\nmulti-modal information into LLMs, we also conceptualize chemistry LLMs as\nagents using chemistry tools and investigate their potential to accelerate\nscientific research. Additionally, we conclude the existing benchmarks to\nevaluate chemistry ability of LLMs. Finally, we critically examine the current\nchallenges and identify promising directions for future research. Through this\ncomprehensive survey, we aim to assist researchers in staying at the forefront\nof developments in chemistry LLMs and to inspire innovative applications in the\nfield.", "published": "2024-12-28 03:40:25", "link": "http://arxiv.org/abs/2412.19994v1", "categories": ["physics.chem-ph", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "physics.chem-ph"}
{"title": "The Emotional Spectrum of LLMs: Leveraging Empathy and Emotion-Based\n  Markers for Mental Health Support", "abstract": "The increasing demand for mental health services has highlighted the need for\ninnovative solutions, particularly in the realm of psychological conversational\nAI, where the availability of sensitive data is scarce. In this work, we\nexplored the development of a system tailored for mental health support with a\nnovel approach to psychological assessment based on explainable emotional\nprofiles in combination with empathetic conversational models, offering a\npromising tool for augmenting traditional care, particularly where immediate\nexpertise is unavailable. Our work can be divided into two main parts,\nintrinsecaly connected to each other. First, we present RACLETTE, a\nconversational system that demonstrates superior emotional accuracy compared to\nstate-of-the-art benchmarks in both understanding users' emotional states and\ngenerating empathetic responses during conversations, while progressively\nbuilding an emotional profile of the user through their interactions. Second,\nwe show how the emotional profiles of a user can be used as interpretable\nmarkers for mental health assessment. These profiles can be compared with\ncharacteristic emotional patterns associated with different mental disorders,\nproviding a novel approach to preliminary screening and support.", "published": "2024-12-28 07:42:29", "link": "http://arxiv.org/abs/2412.20068v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "On the Compositional Generalization of Multimodal LLMs for Medical\n  Imaging", "abstract": "Multimodal large language models (MLLMs) hold significant potential in the\nmedical field, but their capabilities are often limited by insufficient data in\ncertain medical domains, highlighting the need for understanding what kinds of\nimages can be used by MLLMs for generalization. Current research suggests that\nmulti-task training outperforms single-task as different tasks can benefit each\nother, but they often overlook the internal relationships within these tasks,\nproviding limited guidance on selecting datasets to enhance specific tasks. To\nanalyze this phenomenon, we attempted to employ compositional generalization\n(CG)-the ability of models to understand novel combinations by recombining\nlearned elements-as a guiding framework. Since medical images can be precisely\ndefined by Modality, Anatomical area, and Task, naturally providing an\nenvironment for exploring CG. Therefore, we assembled 106 medical datasets to\ncreate Med-MAT for comprehensive experiments. The experiments confirmed that\nMLLMs can use CG to understand unseen medical images and identified CG as one\nof the main drivers of the generalization observed in multi-task training.\nAdditionally, further studies demonstrated that CG effectively supports\ndatasets with limited data and delivers consistent performance across different\nbackbones, highlighting its versatility and broad applicability. Med-MAT is\npublicly available at https://github.com/FreedomIntelligence/Med-MAT.", "published": "2024-12-28 07:50:00", "link": "http://arxiv.org/abs/2412.20070v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors", "abstract": "Security concerns surrounding text-to-image diffusion models have driven\nresearchers to unlearn inappropriate concepts through fine-tuning. Recent\nfine-tuning methods typically align the prediction distributions of unsafe\nprompts with those of predefined text anchors. However, these techniques\nexhibit a considerable performance trade-off between eliminating undesirable\nconcepts and preserving other concepts. In this paper, we systematically\nanalyze the impact of diverse text anchors on unlearning performance. Guided by\nthis analysis, we propose AdvAnchor, a novel approach that generates\nadversarial anchors to alleviate the trade-off issue. These adversarial anchors\nare crafted to closely resemble the embeddings of undesirable concepts to\nmaintain overall model performance, while selectively excluding defining\nattributes of these concepts for effective erasure. Extensive experiments\ndemonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is\npublicly available at https://anonymous.4open.science/r/AdvAnchor.", "published": "2024-12-28 04:44:07", "link": "http://arxiv.org/abs/2501.00054v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "abstract": "While safety-aligned large language models (LLMs) are increasingly used as\nthe cornerstone for powerful systems such as multi-agent frameworks to solve\ncomplex real-world problems, they still suffer from potential adversarial\nqueries, such as jailbreak attacks, which attempt to induce harmful content.\nResearching attack methods allows us to better understand the limitations of\nLLM and make trade-offs between helpfulness and safety. However, existing\njailbreak attacks are primarily based on opaque optimization techniques (e.g.\ntoken-level gradient descent) and heuristic search methods like LLM refinement,\nwhich fall short in terms of transparency, transferability, and computational\ncost. In light of these limitations, we draw inspiration from the evolution and\ninfection processes of biological viruses and propose LLM-Virus, a jailbreak\nattack method based on evolutionary algorithm, termed evolutionary jailbreak.\nLLM-Virus treats jailbreak attacks as both an evolutionary and transfer\nlearning problem, utilizing LLMs as heuristic evolutionary operators to ensure\nhigh attack efficiency, transferability, and low time cost. Our experimental\nresults on multiple safety benchmarks show that LLM-Virus achieves competitive\nor even superior performance compared to existing attack methods.", "published": "2024-12-28 07:48:57", "link": "http://arxiv.org/abs/2501.00055v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of\n  Automated Defense Vehicles", "abstract": "The evolution of Artificial Intelligence (AI) and its subset Deep Learning\n(DL), has profoundly impacted numerous domains, including autonomous driving.\nThe integration of autonomous driving in military settings reduces human\ncasualties and enables precise and safe execution of missions in hazardous\nenvironments while allowing for reliable logistics support without the risks\nassociated with fatigue-related errors. However, relying on autonomous driving\nsolely requires an advanced decision-making model that is adaptable and optimum\nin any situation. Considering the presence of numerous interconnected\nautonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency\nCommunication (URLLC) is vital for ensuring seamless coordination, real-time\ndata exchange, and instantaneous response to dynamic driving environments. The\nadvent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV)\nconcept within the realm of Internet of Military Defense Things (IoMDT) by\nenabling robust connectivity, crucial for real-time data exchange, advanced\nnavigation, and enhanced safety features through IoADV interactions. On the\nother hand, a critical advancement in this space is using pre-trained\nGenerative Large Language Models (LLMs) for decision-making and communication\noptimization for autonomous driving. Hence, this work presents opportunities\nand challenges with a vision of realizing the full potential of these\ntechnologies in critical defense applications, especially through the\nadvancement of IoADV and its role in enhancing autonomous military operations.", "published": "2024-12-28 23:07:25", "link": "http://arxiv.org/abs/2501.06205v2", "categories": ["cs.NI", "cs.AI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System", "abstract": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.", "published": "2024-12-28 04:01:30", "link": "http://arxiv.org/abs/2412.20005v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Social Media Data and Artificial Intelligence for Improving\n  Earthquake Response Efforts", "abstract": "The integration of social media and artificial intelligence (AI) into\ndisaster management, particularly for earthquake response, represents a\nprofound evolution in emergency management practices. In the digital age,\nreal-time information sharing has reached unprecedented levels, with social\nmedia platforms emerging as crucial communication channels during crises. This\nshift has transformed traditional, centralized emergency services into more\ndecentralized, participatory models of disaster situational awareness. Our\nstudy includes an experimental analysis of 8,900 social media interactions,\nincluding 2,920 posts and 5,980 replies on X (formerly Twitter), following a\nmagnitude 5.1 earthquake in Oklahoma on February 2, 2024. The analysis covers\ndata from the immediate aftermath and extends over the following seven days,\nillustrating the critical role of digital platforms in modern disaster\nresponse. The results demonstrate that social media platforms can be\neffectively used as real-time situational awareness tools, delivering critical\ninformation to society and authorities during emergencies.", "published": "2024-12-28 11:08:06", "link": "http://arxiv.org/abs/2501.14767v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.SI", "I.2.7"], "primary_category": "cs.CY"}
{"title": "Distance Based Single-Channel Target Speech Extraction", "abstract": "This paper aims to achieve single-channel target speech extraction (TSE) in\nenclosures by solely utilizing distance information. This is the first work\nthat utilizes only distance cues without using speaker physiological\ninformation for single-channel TSE. Inspired by recent single-channel\nDistance-based separation and extraction methods, we introduce a novel model\nthat efficiently fuses distance information with time-frequency (TF) bins for\nTSE. Experimental results in both single-room and multi-room scenarios\ndemonstrate the feasibility and effectiveness of our approach. This method can\nalso be employed to estimate the distances of different speakers in mixed\nspeech. Online demos are available at\nhttps://runwushi.github.io/distance-demo-page.", "published": "2024-12-28 13:13:15", "link": "http://arxiv.org/abs/2412.20144v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language\n  and Speaker Generation", "abstract": "The goal of this work is to generate natural speech in multiple languages\nwhile maintaining the same speaker identity, a task known as cross-lingual\nspeech synthesis. A key challenge of cross-lingual speech synthesis is the\nlanguage-speaker entanglement problem, which causes the quality of\ncross-lingual systems to lag behind that of intra-lingual systems. In this\npaper, we propose CrossSpeech++, which effectively disentangles language and\nspeaker information and significantly improves the quality of cross-lingual\nspeech synthesis. To this end, we break the complex speech generation pipeline\ninto two simple components: language-dependent and speaker-dependent\ngenerators. The language-dependent generator produces linguistic variations\nthat are not biased by specific speaker attributes. The speaker-dependent\ngenerator models acoustic variations that characterize speaker identity. By\nhandling each type of information in separate modules, our method can\neffectively disentangle language and speaker representation. We conduct\nextensive experiments using various metrics, and demonstrate that CrossSpeech++\nachieves significant improvements in cross-lingual speech synthesis,\noutperforming existing methods by a large margin.", "published": "2024-12-28 06:32:49", "link": "http://arxiv.org/abs/2412.20048v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Bird Vocalization Embedding Extraction Using Self-Supervised\n  Disentangled Representation Learning", "abstract": "This paper addresses the extraction of the bird vocalization embedding from\nthe whole song level using disentangled representation learning (DRL). Bird\nvocalization embeddings are necessary for large-scale bioacoustic tasks, and\nself-supervised methods such as Variational Autoencoder (VAE) have shown their\nperformance in extracting such low-dimensional embeddings from vocalization\nsegments on the note or syllable level. To extend the processing level to the\nentire song instead of cutting into segments, this paper regards each\nvocalization as the generalized and discriminative part and uses two encoders\nto learn these two parts. The proposed method is evaluated on the Great Tits\ndataset according to the clustering performance, and the results outperform the\ncompared pre-trained models and vanilla VAE. Finally, this paper analyzes the\ninformative part of the embedding, further compresses its dimension, and\nexplains the disentangled performance of bird vocalizations.", "published": "2024-12-28 13:14:30", "link": "http://arxiv.org/abs/2412.20146v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody\n  Prompting", "abstract": "Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable\nattention due to its broad range of applications, such as personalized voice\nassistant services. While several approaches have been proposed, they often\nexhibit high sensitivity to either the quantity or the quality of target speech\nsamples. To address these limitations, we introduce Stable-TTS, a novel\nspeaker-adaptive TTS framework that leverages a small subset of a high-quality\npre-training dataset, referred to as prior samples. Specifically, Stable-TTS\nachieves prosody consistency by leveraging the high-quality prosody of prior\nsamples, while effectively capturing the timbre of the target speaker.\nAdditionally, it employs a prior-preservation loss during fine-tuning to\nmaintain the synthesis ability for prior samples to prevent overfitting on\ntarget samples. Extensive experiments demonstrate the effectiveness of\nStable-TTS even under limited amounts of and noisy target speech samples.", "published": "2024-12-28 13:54:30", "link": "http://arxiv.org/abs/2412.20155v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound\n  Diffusion Field", "abstract": "Passive human speed estimation plays a critical role in acoustic sensing.\nDespite extensive study, existing systems, however, suffer from various\nlimitations: First, previous acoustic speed estimation exploits Doppler\nFrequency Shifts (DFS) created by moving targets and relies on microphone\narrays, making them only capable of sensing the radial speed within a\nconstrained distance. Second, the channel measurement rate proves inadequate to\nestimate high moving speeds. To overcome these issues, we present ASE, an\naccurate and robust Acoustic Speed Estimation system on a single commodity\nmicrophone. We model the sound propagation from a unique perspective of the\nacoustic diffusion field, and infer the speed from the acoustic spatial\ndistribution, a completely different way of thinking about speed estimation\nbeyond prior DFS-based approaches. We then propose a novel Orthogonal\nTime-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a\nhigh rate that was previously infeasible, making it possible to estimate high\nspeeds. We further develop novel techniques for motion detection and signal\nenhancement to deliver a robust and practical system. We implement and evaluate\nASE through extensive real-world experiments. Our results show that ASE\nreliably tracks walking speed, independently of target location and direction,\nwith a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection\nrate of 97.4% for large coverage, e.g., free walking in a 4m $\\times$ 4m room.\nWe believe ASE pushes acoustic speed estimation beyond the conventional\nDFS-based paradigm and will inspire exciting research in acoustic sensing.", "published": "2024-12-28 13:10:50", "link": "http://arxiv.org/abs/2412.20142v1", "categories": ["cs.HC", "cs.NI", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.HC"}
=======
{"title": "TradingAgents: Multi-Agents LLM Financial Trading Framework", "abstract": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/TauricResearch.", "published": "2024-12-28 12:54:06", "link": "http://arxiv.org/abs/2412.20138v6", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.TR"}
>>>>>>> 33f049834fead0e168638dee581a3663e744788f
