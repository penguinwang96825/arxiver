{"title": "Task-Oriented Dialogue as Dataflow Synthesis", "abstract": "We describe an approach to task-oriented dialogue in which dialogue state is\nrepresented as a dataflow graph. A dialogue agent maps each user utterance to a\nprogram that extends this graph. Programs include metacomputation operators for\nreference and revision that reuse dataflow fragments from previous turns. Our\ngraph-based state enables the expression and manipulation of complex user\nintents, and explicit metacomputation makes these intents easier for learned\nmodels to predict. We introduce a new dataset, SMCalFlow, featuring complex\ndialogues about events, weather, places, and people. Experiments show that\ndataflow graphs and metacomputation substantially improve representability and\npredictability in these natural dialogues. Additional experiments on the\nMultiWOZ dataset show that our dataflow representation enables an otherwise\noff-the-shelf sequence-to-sequence model to match the best existing\ntask-specific state tracking model. The SMCalFlow dataset and code for\nreplicating experiments are available at\nhttps://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.", "published": "2020-09-24 00:35:26", "link": "http://arxiv.org/abs/2009.11423v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\n  Models", "abstract": "Pretrained neural language models (LMs) are prone to generating racist,\nsexist, or otherwise toxic language which hinders their safe deployment. We\ninvestigate the extent to which pretrained LMs can be prompted to generate\ntoxic language, and the effectiveness of controllable text generation\nalgorithms at preventing such toxic degeneration. We create and release\nRealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level\nprompts derived from a large corpus of English web text, paired with toxicity\nscores from a widely-used toxicity classifier. Using RealToxicityPrompts, we\nfind that pretrained LMs can degenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation\nmethods, and find that while data- or compute-intensive methods (e.g., adaptive\npretraining on non-toxic data) are more effective at steering away from\ntoxicity than simpler solutions (e.g., banning \"bad\" words), no current method\nis failsafe against neural toxic degeneration. To pinpoint the potential cause\nof such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et. al, 2019), and find a\nsignificant amount of offensive, factually unreliable, and otherwise toxic\ncontent. Our work provides a test bed for evaluating toxic generations by LMs\nand stresses the need for better data selection processes for pretraining.", "published": "2020-09-24 03:17:19", "link": "http://arxiv.org/abs/2009.11462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding\n  and Generation", "abstract": "Ancient Chinese is the essence of Chinese culture. There are several natural\nlanguage processing tasks of ancient Chinese domain, such as ancient-modern\nChinese translation, poem generation, and couplet generation. Previous studies\nusually use the supervised models which deeply rely on parallel data. However,\nit is difficult to obtain large-scale parallel data of ancient Chinese. In\norder to make full use of the more easily available monolingual ancient Chinese\ncorpora, we release AnchiBERT, a pre-trained language model based on the\narchitecture of BERT, which is trained on large-scale ancient Chinese corpora.\nWe evaluate AnchiBERT on both language understanding and generation tasks,\nincluding poem classification, ancient-modern Chinese translation, poem\ngeneration, and couplet generation. The experimental results show that\nAnchiBERT outperforms BERT as well as the non-pretrained models and achieves\nstate-of-the-art results in all cases.", "published": "2020-09-24 03:41:13", "link": "http://arxiv.org/abs/2009.11473v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems", "abstract": "Automatic math word problem solving has attracted growing attention in recent\nyears. The evaluation datasets used by previous works have serious limitations\nin terms of scale and diversity. In this paper, we release a new large-scale\nand template-rich math word problem dataset named Ape210K. It consists of 210K\nChinese elementary school-level math problems, which is 9 times the size of the\nlargest public dataset Math23K. Each problem contains both the gold answer and\nthe equations needed to derive the answer. Ape210K is also of greater diversity\nwith 56K templates, which is 25 times more than Math23K. Our analysis shows\nthat solving Ape210K requires not only natural language understanding but also\ncommonsense knowledge. We expect Ape210K to be a benchmark for math word\nproblem solving systems. Experiments indicate that state-of-the-art models on\nthe Math23K dataset perform poorly on Ape210K. We propose a copy-augmented and\nfeature-enriched sequence to sequence (seq2seq) model, which outperforms\nexisting models by 3.2% on the Math23K dataset and serves as a strong baseline\nof the Ape210K dataset. The gap is still significant between human and our\nbaseline model, calling for further research efforts. We make Ape210K dataset\npublicly available at https://github.com/yuantiku/ape210k", "published": "2020-09-24 06:17:10", "link": "http://arxiv.org/abs/2009.11506v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounded Compositional Outputs for Adaptive Language Modeling", "abstract": "Language models have emerged as a central component across NLP, and a great\ndeal of progress depends on the ability to cheaply adapt them (e.g., through\nfinetuning) to new domains and tasks. A language model's vocabulary$-$typically\nselected before training and permanently fixed later$-$affects its size and is\npart of what makes it resistant to such adaptation. Prior work has used\ncompositional input embeddings based on surface forms to ameliorate this issue.\nIn this work, we go one step beyond and propose a fully compositional output\nembedding layer for language models, which is further grounded in information\nfrom a structured lexicon (WordNet), namely semantically related words and\nfree-text definitions. To our knowledge, the result is the first word-level\nlanguage model with a size that does not depend on the training vocabulary. We\nevaluate the model on conventional language modeling as well as challenging\ncross-domain settings with an open vocabulary, finding that it matches or\noutperforms previous state-of-the-art output embedding methods and adaptation\napproaches. Our analysis attributes the improvements to sample efficiency: our\nmodel is more accurate for low-frequency words.", "published": "2020-09-24 07:21:14", "link": "http://arxiv.org/abs/2009.11523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Adaptation of Pre-Trained Language Models across Languages and\n  Domains with Robust Self-Training", "abstract": "Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has\ngained much attention recently. Instead of fine-tuning PrLMs as done in most\nprevious work, we investigate how to adapt the features of PrLMs to new domains\nwithout fine-tuning. We explore unsupervised domain adaptation (UDA) in this\npaper. With the features from PrLMs, we adapt the models trained with labeled\ndata from the source domain to the unlabeled target domain. Self-training is\nwidely used for UDA which predicts pseudo labels on the target domain data for\ntraining. However, the predicted pseudo labels inevitably include noise, which\nwill negatively affect training a robust model. To improve the robustness of\nself-training, in this paper we present class-aware feature self-distillation\n(CFd) to learn discriminative features from PrLMs, in which PrLM features are\nself-distilled into a feature adaptation module and the features from the same\nclass are more tightly clustered. We further extend CFd to a cross-language\nsetting, in which language discrepancy is studied. Experiments on two\nmonolingual and multilingual Amazon review datasets show that CFd can\nconsistently improve the performance of self-training in cross-domain and\ncross-language settings.", "published": "2020-09-24 08:04:37", "link": "http://arxiv.org/abs/2009.11538v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N-LTP: An Open-source Neural Language Technology Platform for Chinese", "abstract": "We introduce \\texttt{N-LTP}, an open-source neural language technology\nplatform supporting six fundamental Chinese NLP tasks: {lexical analysis}\n(Chinese word segmentation, part-of-speech tagging, and named entity\nrecognition), {syntactic parsing} (dependency parsing), and {semantic parsing}\n(semantic dependency parsing and semantic role labeling). Unlike the existing\nstate-of-the-art toolkits, such as \\texttt{Stanza}, that adopt an independent\nmodel for each task, \\texttt{N-LTP} adopts the multi-task framework by using a\nshared pre-trained model, which has the advantage of capturing the shared\nknowledge across relevant Chinese tasks. In addition, a knowledge distillation\nmethod \\cite{DBLP:journals/corr/abs-1907-04829} where the single-task model\nteaches the multi-task model is further introduced to encourage the multi-task\nmodel to surpass its single-task teacher. Finally, we provide a collection of\neasy-to-use APIs and a visualization tool to make users to use and view the\nprocessing results more easily and directly. To the best of our knowledge, this\nis the first toolkit to support six Chinese NLP fundamental tasks. Source code,\ndocumentation, and pre-trained models are available at\n\\url{https://github.com/HIT-SCIR/ltp}.", "published": "2020-09-24 11:45:39", "link": "http://arxiv.org/abs/2009.11616v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Generation with Multi-Hop Reasoning on Commonsense Knowledge\n  Graph", "abstract": "Despite the success of generative pre-trained language models on a series of\ntext generation tasks, they still suffer in cases where reasoning over\nunderlying commonsense knowledge is required during generation. Existing\napproaches that integrate commonsense knowledge into generative pre-trained\nlanguage models simply transfer relational knowledge by post-training on\nindividual knowledge triples while ignoring rich connections within the\nknowledge graph. We argue that exploiting both the structural and semantic\ninformation of the knowledge graph facilitates commonsense-aware text\ngeneration. In this paper, we propose Generation with Multi-Hop Reasoning Flow\n(GRF) that enables pre-trained models with dynamic multi-hop reasoning on\nmulti-relational paths extracted from the external commonsense knowledge graph.\nWe empirically show that our model outperforms existing baselines on three text\ngeneration tasks that require reasoning over commonsense knowledge. We also\ndemonstrate the effectiveness of the dynamic multi-hop reasoning module with\nreasoning paths inferred by the model that provide rationale to the generation.", "published": "2020-09-24 13:55:32", "link": "http://arxiv.org/abs/2009.11692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Commonsense Explanation by Extracting Bridge Concepts from\n  Reasoning Paths", "abstract": "Commonsense explanation generation aims to empower the machine's sense-making\ncapability by generating plausible explanations to statements against\ncommonsense. While this task is easy to human, the machine still struggles to\ngenerate reasonable and informative explanations. In this work, we propose a\nmethod that first extracts the underlying concepts which are served as\n\\textit{bridges} in the reasoning chain and then integrates these concepts to\ngenerate the final explanation. To facilitate the reasoning process, we utilize\nexternal commonsense knowledge to build the connection between a statement and\nthe bridge concepts by extracting and pruning multi-hop paths to build a\nsubgraph. We design a bridge concept extraction model that first scores the\ntriples, routes the paths in the subgraph, and further selects bridge concepts\nwith weak supervision at both the triple level and the concept level. We\nconduct experiments on the commonsense explanation generation task and our\nmodel outperforms the state-of-the-art baselines in both automatic and human\nevaluation.", "published": "2020-09-24 15:27:20", "link": "http://arxiv.org/abs/2009.11753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novel Keyword Extraction and Language Detection Approaches", "abstract": "Fuzzy string matching and language classification are important tools in\nNatural Language Processing pipelines, this paper provides advances in both\nareas. We propose a fast novel approach to string tokenisation for fuzzy\nlanguage matching and experimentally demonstrate an 83.6% decrease in\nprocessing time with an estimated improvement in recall of 3.1% at the cost of\na 2.6% decrease in precision. This approach is able to work even where keywords\nare subdivided into multiple words, without needing to scan\ncharacter-to-character. So far there has been little work considering using\nmetadata to enhance language classification algorithms. We provide\nobservational data and find the Accept-Language header is 14% more likely to\nmatch the classification than the IP Address.", "published": "2020-09-24 17:28:59", "link": "http://arxiv.org/abs/2009.11832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding\n  Evaluation", "abstract": "Word embeddings can reflect the semantic representations, and the embedding\nqualities can be comprehensively evaluated with human natural reading-related\ncognitive data sources. In this paper, we proposed the CogniFNN framework,\nwhich is the first attempt at using fuzzy neural networks to extract non-linear\nand non-stationary characteristics for evaluations of English word embeddings\nagainst the corresponding cognitive datasets. In our experiment, we used 15\nhuman cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,\nand selected the mean square error and multiple hypotheses testing as metrics\nto evaluate our proposed CogniFNN framework. Compared to the recent pioneer\nframework, our proposed CogniFNN showed smaller prediction errors of both\ncontext-independent (GloVe) and context-sensitive (BERT) word embeddings, and\nachieved higher significant ratios with randomly generated word embeddings. Our\nfindings suggested that the CogniFNN framework could provide a more accurate\nand comprehensive evaluation of cognitive word embeddings. It will potentially\nbe beneficial to the further word embeddings evaluation on extrinsic natural\nlanguage processing tasks.", "published": "2020-09-24 04:39:38", "link": "http://arxiv.org/abs/2009.11485v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AliMe KG: Domain Knowledge Graph Construction and Application in\n  E-commerce", "abstract": "Pre-sales customer service is of importance to E-commerce platforms as it\ncontributes to optimizing customers' buying process. To better serve users, we\npropose AliMe KG, a domain knowledge graph in E-commerce that captures user\nproblems, points of interests (POI), item information and relations thereof. It\nhelps to understand user needs, answer pre-sales questions and generate\nexplanation texts. We applied AliMe KG to several online business scenarios\nsuch as shopping guide, question answering over properties and recommendation\nreason generation, and gained positive results. In the paper, we systematically\nintroduce how we construct domain knowledge graph from free text, and\ndemonstrate its business value with several applications. Our experience shows\nthat mining structured knowledge from free text in vertical domain is\npracticable, and can be of substantial value in industrial settings.", "published": "2020-09-24 13:40:18", "link": "http://arxiv.org/abs/2009.11684v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Adapting BERT for Word Sense Disambiguation with Gloss Selection\n  Objective and Example Sentences", "abstract": "Domain adaptation or transfer learning using pre-trained language models such\nas BERT has proven to be an effective approach for many natural language\nprocessing tasks. In this work, we propose to formulate word sense\ndisambiguation as a relevance ranking task, and fine-tune BERT on sequence-pair\nranking task to select the most probable sense definition given a context\nsentence and a list of candidate sense definitions. We also introduce a data\naugmentation technique for WSD using existing example sentences from WordNet.\nUsing the proposed training objective and data augmentation technique, our\nmodels are able to achieve state-of-the-art results on the English all-words\nbenchmark datasets.", "published": "2020-09-24 16:37:04", "link": "http://arxiv.org/abs/2009.11795v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward a Thermodynamics of Meaning", "abstract": "As language models such as GPT-3 become increasingly successful at generating\nrealistic text, questions about what purely text-based modeling can learn about\nthe world have become more urgent. Is text purely syntactic, as skeptics argue?\nOr does it in fact contain some semantic information that a sufficiently\nsophisticated language model could use to learn about the world without any\nadditional inputs? This paper describes a new model that suggests some\nqualified answers to those questions. By theorizing the relationship between\ntext and the world it describes as an equilibrium relationship between a\nthermodynamic system and a much larger reservoir, this paper argues that even\nvery simple language models do learn structural facts about the world, while\nalso proposing relatively precise limits on the nature and extent of those\nfacts. This perspective promises not only to answer questions about what\nlanguage models actually learn, but also to explain the consistent and\nsurprising success of cooccurrence prediction as a meaning-making strategy in\nAI.", "published": "2020-09-24 21:56:02", "link": "http://arxiv.org/abs/2009.11963v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Type B Reflexivization as an Unambiguous Testbed for Multilingual\n  Multi-Task Gender Bias", "abstract": "The one-sided focus on English in previous studies of gender bias in NLP\nmisses out on opportunities in other languages: English challenge datasets such\nas GAP and WinoGender highlight model preferences that are \"hallucinatory\",\ne.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors.\nWe show that for languages with type B reflexivization, e.g., Swedish and\nRussian, we can construct multi-task challenge datasets for detecting gender\nbias that lead to unambiguously wrong model predictions: In these languages,\nthe direct translation of 'the doctor removed his mask' is not ambiguous\nbetween a coreferential reading and a disjoint reading. Instead, the\ncoreferential reading requires a non-gendered pronoun, and the gendered,\npossessive pronouns are anti-reflexive. We present a multilingual, multi-task\nchallenge dataset, which spans four languages and four NLP tasks and focuses\nonly on this phenomenon. We find evidence for gender bias across all\ntask-language combinations and correlate model bias with national labor market\nstatistics.", "published": "2020-09-24 23:47:18", "link": "http://arxiv.org/abs/2009.11982v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effects of Word-frequency based Pre- and Post- Processings for Audio\n  Captioning", "abstract": "The system we used for Task 6 (Automated Audio Captioning)of the Detection\nand Classification of Acoustic Scenes and Events(DCASE) 2020 Challenge combines\nthree elements, namely, dataaugmentation, multi-task learning, and\npost-processing, for audiocaptioning. The system received the highest\nevaluation scores, butwhich of the individual elements most fully contributed\nto its perfor-mance has not yet been clarified. Here, to asses their\ncontributions,we first conducted an element-wise ablation study on our systemto\nestimate to what extent each element is effective. We then con-ducted a\ndetailed module-wise ablation study to further clarify thekey processing\nmodules for improving accuracy. The results showthat data augmentation and\npost-processing significantly improvethe score in our system. In particular,\nmix-up data augmentationand beam search in post-processing improve SPIDEr by\n0.8 and 1.6points, respectively.", "published": "2020-09-24 01:07:33", "link": "http://arxiv.org/abs/2009.11436v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bootstrapped Q-learning with Context Relevant Observation Pruning to\n  Generalize in Text-based Games", "abstract": "We show that Reinforcement Learning (RL) methods for solving Text-Based Games\n(TBGs) often fail to generalize on unseen games, especially in small data\nregimes. To address this issue, we propose Context Relevant Episodic State\nTruncation (CREST) for irrelevant token removal in observation text for\nimproved generalization. Our method first trains a base model using Q-learning,\nwhich typically overfits the training games. The base model's action token\ndistribution is used to perform observation pruning that removes irrelevant\ntokens. A second bootstrapped model is then retrained on the pruned observation\ntext. Our bootstrapped agent shows improved generalization in solving unseen\nTextWorld games, using 10x-20x fewer training games compared to previous\nstate-of-the-art methods despite requiring less number of training episodes.", "published": "2020-09-24 18:38:30", "link": "http://arxiv.org/abs/2009.11896v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Comparative Study of Feature Types for Age-Based Text Classification", "abstract": "The ability to automatically determine the age audience of a novel provides\nmany opportunities for the development of information retrieval tools. Firstly,\ndevelopers of book recommendation systems and electronic libraries may be\ninterested in filtering texts by the age of the most likely readers. Further,\nparents may want to select literature for children. Finally, it will be useful\nfor writers and publishers to determine which features influence whether the\ntexts are suitable for children. In this article, we compare the empirical\neffectiveness of various types of linguistic features for the task of age-based\nclassification of fiction texts. For this purpose, we collected a text corpus\nof book previews labeled with one of two categories -- children's or adult. We\nevaluated the following types of features: readability indices, sentiment,\nlexical, grammatical and general features, and publishing attributes. The\nresults obtained show that the features describing the text at the document\nlevel can significantly increase the quality of machine learning models.", "published": "2020-09-24 18:41:10", "link": "http://arxiv.org/abs/2009.11898v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "Tuning Word2vec for Large Scale Recommendation Systems", "abstract": "Word2vec is a powerful machine learning tool that emerged from Natural\nLan-guage Processing (NLP) and is now applied in multiple domains, including\nrecom-mender systems, forecasting, and network analysis. As Word2vec is often\nused offthe shelf, we address the question of whether the default\nhyperparameters are suit-able for recommender systems. The answer is\nemphatically no. In this paper, wefirst elucidate the importance of\nhyperparameter optimization and show that un-constrained optimization yields an\naverage 221% improvement in hit rate over thedefault parameters. However,\nunconstrained optimization leads to hyperparametersettings that are very\nexpensive and not feasible for large scale recommendationtasks. To this end, we\ndemonstrate 138% average improvement in hit rate with aruntime\nbudget-constrained hyperparameter optimization. Furthermore, to\nmakehyperparameter optimization applicable for large scale recommendation\nproblemswhere the target dataset is too large to search over, we investigate\ngeneralizinghyperparameters settings from samples. We show that applying\nconstrained hy-perparameter optimization using only a 10% sample of the data\nstill yields a 91%average improvement in hit rate over the default parameters\nwhen applied to thefull datasets. Finally, we apply hyperparameters learned\nusing our method of con-strained optimization on a sample to the Who To Follow\nrecommendation serviceat Twitter and are able to increase follow rates by 15%.", "published": "2020-09-24 10:50:19", "link": "http://arxiv.org/abs/2009.12192v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "BiteNet: Bidirectional Temporal Encoder Network to Predict Medical\n  Outcomes", "abstract": "Electronic health records (EHRs) are longitudinal records of a patient's\ninteractions with healthcare systems. A patient's EHR data is organized as a\nthree-level hierarchy from top to bottom: patient journey - all the experiences\nof diagnoses and treatments over a period of time; individual visit - a set of\nmedical codes in a particular visit; and medical code - a specific record in\nthe form of medical codes. As EHRs begin to amass in millions, the potential\nbenefits, which these data might hold for medical research and medical outcome\nprediction, are staggering - including, for example, predicting future\nadmissions to hospitals, diagnosing illnesses or determining the efficacy of\nmedical treatments. Each of these analytics tasks requires a domain knowledge\nextraction method to transform the hierarchical patient journey into a vector\nrepresentation for further prediction procedure. The representations should\nembed a sequence of visits and a set of medical codes with a specific\ntimestamp, which are crucial to any downstream prediction tasks. Hence,\nexpressively powerful representations are appealing to boost learning\nperformance. To this end, we propose a novel self-attention mechanism that\ncaptures the contextual dependency and temporal relationships within a\npatient's healthcare journey. An end-to-end bidirectional temporal encoder\nnetwork (BiteNet) then learns representations of the patient's journeys, based\nsolely on the proposed attention mechanism. We have evaluated the effectiveness\nof our methods on two supervised prediction and two unsupervised clustering\ntasks with a real-world EHR dataset. The empirical results demonstrate the\nproposed BiteNet model produces higher-quality representations than\nstate-of-the-art baseline methods.", "published": "2020-09-24 00:42:36", "link": "http://arxiv.org/abs/2009.13252v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A New Dataset for Amateur Vocal Percussion Analysis", "abstract": "The imitation of percussive instruments via the human voice is a natural way\nfor us to communicate rhythmic ideas and, for this reason, it attracts the\ninterest of music makers. Specifically, the automatic mapping of these vocal\nimitations to their emulated instruments would allow creators to realistically\nprototype rhythms in a faster way. The contribution of this study is two-fold.\nFirstly, a new Amateur Vocal Percussion (AVP) dataset is introduced to\ninvestigate how people with little or no experience in beatboxing approach the\ntask of vocal percussion. The end-goal of this analysis is that of helping\nmapping algorithms to better generalise between subjects and achieve higher\nperformances. The dataset comprises a total of 9780 utterances recorded by 28\nparticipants with fully annotated onsets and labels (kick drum, snare drum,\nclosed hi-hat and opened hi-hat). Lastly, we conducted baseline experiments on\naudio onset detection with the recorded dataset, comparing the performance of\nfour state-of-the-art algorithms in a vocal percussion context.", "published": "2020-09-24 14:57:10", "link": "http://arxiv.org/abs/2009.11737v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The COUGHVID crowdsourcing dataset: A corpus for the study of\n  large-scale cough analysis algorithms", "abstract": "Cough audio signal classification has been successfully used to diagnose a\nvariety of respiratory conditions, and there has been significant interest in\nleveraging Machine Learning (ML) to provide widespread COVID-19 screening.\nHowever, there is currently no validated database of cough sounds with which to\ntrain such ML models. The COUGHVID dataset provides over 20,000 crowdsourced\ncough recordings representing a wide range of subject ages, genders, geographic\nlocations, and COVID-19 statuses. First, we filtered the dataset using our\nopen-sourced cough detection algorithm. Second, experienced pulmonologists\nlabeled more than 2,000 recordings to diagnose medical abnormalities present in\nthe coughs, thereby contributing one of the largest expert-labeled cough\ndatasets in existence that can be used for a plethora of cough audio\nclassification tasks. Finally, we ensured that coughs labeled as symptomatic\nand COVID-19 originate from countries with high infection rates, and that their\nexpert labels are consistent. As a result, the COUGHVID dataset contributes a\nwealth of cough recordings for training ML models to address the world's most\nurgent health crises.", "published": "2020-09-24 12:58:41", "link": "http://arxiv.org/abs/2009.11644v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Timbre Space Representation of a Subtractive Synthesizer", "abstract": "In this study, we produce a geometrically scaled perceptual timbre space from\ndissimilarity ratings of subtractive synthesized sounds and correlate the\nresulting dimensions with a set of acoustic descriptors. We curate a set of 15\nsounds, produced by a synthesis model that uses varying source waveforms,\nfrequency modulation (FM) and a lowpass filter with an enveloped cutoff\nfrequency. Pairwise dissimilarity ratings were collected within an online\nbrowser-based experiment. We hypothesized that a varied waveform input source\nand enveloped filter would act as the main vehicles for timbral variation,\nproviding novel acoustic correlates for the perception of synthesized timbres.", "published": "2020-09-24 14:04:51", "link": "http://arxiv.org/abs/2009.11706v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
