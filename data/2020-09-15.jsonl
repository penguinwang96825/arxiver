{"title": "MatScIE: An automated tool for the generation of databases of methods\n  and parameters used in the computational materials science literature", "abstract": "The number of published articles in the field of materials science is growing\nrapidly every year. This comparatively unstructured data source, which contains\na large amount of information, has a restriction on its re-usability, as the\ninformation needed to carry out further calculations using the data in it must\nbe extracted manually. It is very important to obtain valid and contextually\ncorrect information from the online (offline) data, as it can be useful not\nonly to generate inputs for further calculations, but also to incorporate them\ninto a querying framework. Retaining this context as a priority, we have\ndeveloped an automated tool, MatScIE (Material Scince Information Extractor)\nthat can extract relevant information from material science literature and make\na structured database that is much easier to use for material simulations.\nSpecifically, we extract the material details, methods, code, parameters, and\nstructure from the various research articles. Finally, we created a web\napplication where users can upload published articles and view/download the\ninformation obtained from this tool and can create their own databases for\ntheir personal uses.", "published": "2020-09-15 01:52:47", "link": "http://arxiv.org/abs/2009.06819v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global-aware Beam Search for Neural Abstractive Summarization", "abstract": "This study develops a calibrated beam-based algorithm with awareness of the\nglobal attention distribution for neural abstractive summarization, aiming to\nimprove the local optimality problem of the original beam search in a rigorous\nway. Specifically, a novel global protocol is proposed based on the attention\ndistribution to stipulate how a global optimal hypothesis should attend to the\nsource. A global scoring mechanism is then developed to regulate beam search to\ngenerate summaries in a near-global optimal fashion. This novel design enjoys a\ndistinctive property, i.e., the global attention distribution could be\npredicted before inference, enabling step-wise improvements on the beam search\nthrough the global scoring mechanism. Extensive experiments on nine datasets\nshow that the global (attention)-aware inference significantly improves\nstate-of-the-art summarization models even using empirical hyper-parameters.\nThe algorithm is also proven robust as it remains to generate meaningful texts\nwith corrupted attention distributions. The codes and a comprehensive set of\nexamples are available.", "published": "2020-09-15 07:09:26", "link": "http://arxiv.org/abs/2009.06891v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-order Refining for End-to-end Chinese Semantic Role Labeling", "abstract": "Current end-to-end semantic role labeling is mostly accomplished via\ngraph-based neural models. However, these all are first-order models, where\neach decision for detecting any predicate-argument pair is made in isolation\nwith local features. In this paper, we present a high-order refining mechanism\nto perform interaction between all predicate-argument pairs. Based on the\nbaseline graph model, our high-order refining module learns higher-order\nfeatures between all candidate pairs via attention calculation, which are later\nused to update the original token representations. After several iterations of\nrefinement, the underlying token representations can be enriched with globally\ninteracted features. Our high-order model achieves state-of-the-art results on\nChinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile\nrelieving the long-range dependency issues.", "published": "2020-09-15 10:01:27", "link": "http://arxiv.org/abs/2009.06957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Response Ranking Training with Large-Scale Human Feedback Data", "abstract": "Existing open-domain dialog models are generally trained to minimize the\nperplexity of target human responses. However, some human replies are more\nengaging than others, spawning more followup interactions. Current\nconversational models are increasingly capable of producing turns that are\ncontext-relevant, but in order to produce compelling agents, these models need\nto be able to predict and optimize for turns that are genuinely engaging. We\nleverage social media feedback data (number of replies and upvotes) to build a\nlarge-scale training dataset for feedback prediction. To alleviate possible\ndistortion between the feedback and engagingness, we convert the ranking\nproblem to a comparison of response pairs which involve few confounding\nfactors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of\nhuman feedback data and the resulting ranker outperformed several baselines.\nParticularly, our ranker outperforms the conventional dialog perplexity\nbaseline with a large margin on predicting Reddit feedback. We finally combine\nthe feedback prediction models and a human-like scoring model to rank the\nmachine-generated dialog responses. Crowd-sourced human evaluation shows that\nour ranking method correlates better with real human preferences than baseline\nmodels.", "published": "2020-09-15 10:50:05", "link": "http://arxiv.org/abs/2009.06978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noisy Self-Knowledge Distillation for Text Summarization", "abstract": "In this paper we apply self-knowledge distillation to text summarization\nwhich we argue can alleviate problems with maximum-likelihood training on\nsingle reference and noisy datasets. Instead of relying on one-hot annotation\nlabels, our student summarization model is trained with guidance from a teacher\nwhich generates smoothed labels to help regularize training. Furthermore, to\nbetter model uncertainty during training, we introduce multiple noise signals\nfor both teacher and student models. We demonstrate experimentally on three\nbenchmarks that our framework boosts the performance of both pretrained and\nnon-pretrained summarizers achieving state-of-the-art results.", "published": "2020-09-15 12:53:09", "link": "http://arxiv.org/abs/2009.07032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Referenced Training for Dialogue Response Generation", "abstract": "In open-domain dialogue response generation, a dialogue context can be\ncontinued with diverse responses, and the dialogue models should capture such\none-to-many relations. In this work, we first analyze the training objective of\ndialogue models from the view of Kullback-Leibler divergence (KLD) and show\nthat the gap between the real world probability distribution and the\nsingle-referenced data's probability distribution prevents the model from\nlearning the one-to-many relations efficiently. Then we explore approaches to\nmulti-referenced training in two aspects. Data-wise, we generate diverse pseudo\nreferences from a powerful pretrained model to build multi-referenced data that\nprovides a better approximation of the real-world distribution. Model-wise, we\npropose to equip variational models with an expressive prior, named linear\nGaussian model (LGM). Experimental results of automated evaluation and human\nevaluation show that the methods yield significant improvements over baselines.\nWe will release our code and data in\nhttps://github.com/ZHAOTING/dialog-processing.", "published": "2020-09-15 14:17:53", "link": "http://arxiv.org/abs/2009.07117v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Joint Layer RNN based Keyphrase Extraction by Using\n  Syntactical Features", "abstract": "Keyphrase extraction as a task to identify important words or phrases from a\ntext, is a crucial process to identify main topics when analyzing texts from a\nsocial media platform. In our study, we focus on text written in Indonesia\nlanguage taken from Twitter. Different from the original joint layer recurrent\nneural network (JRNN) with output of one sequence of keywords and using only\nword embedding, here we propose to modify the input layer of JRNN to extract\nmore than one sequence of keywords by additional information of syntactical\nfeatures, namely part of speech, named entity types, and dependency structures.\nSince JRNN in general requires a large amount of data as the training examples\nand creating those examples is expensive, we used a data augmentation method to\nincrease the number of training examples. Our experiment had shown that our\nmethod outperformed the baseline methods. Our method achieved .9597 in accuracy\nand .7691 in F1.", "published": "2020-09-15 14:20:04", "link": "http://arxiv.org/abs/2009.07119v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cascaded Semantic and Positional Self-Attention Network for Document\n  Classification", "abstract": "Transformers have shown great success in learning representations for\nlanguage modelling. However, an open challenge still remains on how to\nsystematically aggregate semantic information (word embedding) with positional\n(or temporal) information (word orders). In this work, we propose a new\narchitecture to aggregate the two sources of information using cascaded\nsemantic and positional self-attention network (CSPAN) in the context of\ndocument classification. The CSPAN uses a semantic self-attention layer\ncascaded with Bi-LSTM to process the semantic and positional information in a\nsequential manner, and then adaptively combine them together through a residue\nconnection. Compared with commonly used positional encoding schemes, CSPAN can\nexploit the interaction between semantics and word positions in a more\ninterpretable and adaptive manner, and the classification performance can be\nnotably improved while simultaneously preserving a compact model size and high\nconvergence rate. We evaluate the CSPAN model on several benchmark data sets\nfor document classification with careful ablation studies, and demonstrate the\nencouraging results compared with state of the art.", "published": "2020-09-15 15:02:28", "link": "http://arxiv.org/abs/2009.07148v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Joint Attribute Prediction and Value Extraction for\n  E-commerce Product", "abstract": "Product attribute values are essential in many e-commerce scenarios, such as\ncustomer service robots, product recommendations, and product retrieval. While\nin the real world, the attribute values of a product are usually incomplete and\nvary over time, which greatly hinders the practical applications. In this\npaper, we propose a multimodal method to jointly predict product attributes and\nextract values from textual product descriptions with the help of the product\nimages. We argue that product attributes and values are highly correlated,\ne.g., it will be easier to extract the values on condition that the product\nattributes are given. Thus, we jointly model the attribute prediction and value\nextraction tasks from multiple aspects towards the interactions between\nattributes and values. Moreover, product images have distinct effects on our\ntasks for different product attributes and values. Thus, we selectively draw\nuseful visual information from product images to enhance our model. We annotate\na multimodal product attribute value dataset that contains 87,194 instances,\nand the experimental results on this dataset demonstrate that explicitly\nmodeling the relationship between attributes and values facilitates our method\nto establish the correspondence between them, and selectively utilizing visual\nproduct information is necessary for the task. Our code and dataset will be\nreleased to the public.", "published": "2020-09-15 15:10:51", "link": "http://arxiv.org/abs/2009.07162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Refinement in the Continuous Space for Non-Autoregressive\n  Neural Machine Translation", "abstract": "We propose an efficient inference procedure for non-autoregressive machine\ntranslation that iteratively refines translation purely in the continuous\nspace. Given a continuous latent variable model for machine translation (Shu et\nal., 2020), we train an inference network to approximate the gradient of the\nmarginal log probability of the target sentence, using only the latent variable\nas input. This allows us to use gradient-based optimization to find the target\nsentence at inference time that approximately maximizes its marginal\nprobability. As each refinement step only involves computation in the latent\nspace of low dimensionality (we use 8 in our experiments), we avoid\ncomputational overhead incurred by existing non-autoregressive inference\nprocedures that often refine in token space. We compare our approach to a\nrecently proposed EM-like inference procedure (Shu et al., 2020) that optimizes\nin a hybrid space, consisting of both discrete and continuous variables. We\nevaluate our approach on WMT'14 En-De, WMT'16 Ro-En and IWSLT'16 De-En, and\nobserve two advantages over the EM-like inference: (1) it is computationally\nefficient, i.e. each refinement step is twice as fast, and (2) it is more\neffective, resulting in higher marginal probabilities and BLEU scores with the\nsame number of refinement steps. On WMT'14 En-De, for instance, our approach is\nable to decode 6.2 times faster than the autoregressive model with minimal\ndegradation to translation quality (0.9 BLEU).", "published": "2020-09-15 15:30:14", "link": "http://arxiv.org/abs/2009.07177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Presence Prediction Helps Trigger Detection Across Languages", "abstract": "The task of event detection and classification is central to most information\nretrieval applications. We show that a Transformer based architecture can\neffectively model event extraction as a sequence labeling task. We propose a\ncombination of sentence level and token level training objectives that\nsignificantly boosts the performance of a BERT based event extraction model.\nOur approach achieves a new state-of-the-art performance on ACE 2005 data for\nEnglish and Chinese. We also test our model on ERE Spanish, achieving an\naverage gain of 2 absolute F1 points over prior best performing model.", "published": "2020-09-15 15:52:21", "link": "http://arxiv.org/abs/2009.07188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Machine Translation with Visual Context", "abstract": "Simultaneous machine translation (SiMT) aims to translate a continuous input\ntext stream into another language with the lowest latency and highest quality\npossible. The translation thus has to start with an incomplete source text,\nwhich is read progressively, creating the need for anticipation. In this paper,\nwe seek to understand whether the addition of visual information can compensate\nfor the missing source context. To this end, we analyse the impact of different\nmultimodal approaches and visual features on state-of-the-art SiMT frameworks.\nOur results show that visual context is helpful and that visually-grounded\nmodels based on explicit object region information are much better than\ncommonly used global features, reaching up to 3 BLEU points improvement under\nlow latency scenarios. Our qualitative analysis illustrates cases where only\nthe multimodal systems are able to translate correctly from English into\ngender-marked languages, as well as deal with differences in word order, such\nas adjective-noun placement between English and French.", "published": "2020-09-15 18:19:11", "link": "http://arxiv.org/abs/2009.07310v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cascaded Models for Better Fine-Grained Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is an essential precursor task for many\nnatural language applications, such as relation extraction or event extraction.\nMuch of the NER research has been done on datasets with few classes of entity\ntypes (e.g. PER, LOC, ORG, MISC), but many real world applications (disaster\nrelief, complex event extraction, law enforcement) can benefit from a larger\nNER typeset. More recently, datasets were created that have hundreds to\nthousands of types of entities, sparking new lines of research (Sekine,\n2008;Ling and Weld, 2012; Gillick et al., 2014; Choiet al., 2018). In this\npaper we present a cascaded approach to labeling fine-grained NER, applying to\na newly released fine-grained NER dataset that was used in the TAC KBP 2019\nevaluation (Ji et al., 2019), inspired by the fact that training data is\navailable for some of the coarse labels. Using a combination of transformer\nnetworks, we show that performance can be improved by about 20 F1 absolute, as\ncompared with the straightforward model built on the full fine-grained types,\nand show that, surprisingly, using course-labeled data in three languages leads\nto an improvement in the English data.", "published": "2020-09-15 18:41:29", "link": "http://arxiv.org/abs/2009.07317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An information theoretic view on selecting linguistic probes", "abstract": "There is increasing interest in assessing the linguistic knowledge encoded in\nneural representations. A popular approach is to attach a diagnostic classifier\n-- or \"probe\" -- to perform supervised classification from internal\nrepresentations. However, how to select a good probe is in debate. Hewitt and\nLiang (2019) showed that a high performance on diagnostic classification itself\nis insufficient, because it can be attributed to either \"the representation\nbeing rich in knowledge\", or \"the probe learning the task\", which Pimentel et\nal. (2020) challenged. We show this dichotomy is valid\ninformation-theoretically. In addition, we find that the methods to construct\nand select good probes proposed by the two papers, *control task* (Hewitt and\nLiang, 2019) and *control function* (Pimentel et al., 2020), are equivalent --\nthe errors of their approaches are identical (modulo irrelevant terms).\nEmpirically, these two selection criteria lead to results that highly agree\nwith each other.", "published": "2020-09-15 21:52:16", "link": "http://arxiv.org/abs/2009.07364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast semantic parsing with well-typedness guarantees", "abstract": "AM dependency parsing is a linguistically principled method for neural\nsemantic parsing with high accuracy across multiple graphbanks. It relies on a\ntype system that models semantic valency but makes existing parsers slow. We\ndescribe an A* parser and a transition-based parser for AM dependency parsing\nwhich guarantee well-typedness and improve parsing speed by up to 3 orders of\nmagnitude, while maintaining or improving accuracy.", "published": "2020-09-15 21:54:01", "link": "http://arxiv.org/abs/2009.07365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-span Style Extraction for Generative Reading Comprehension", "abstract": "Generative machine reading comprehension (MRC) requires a model to generate\nwell-formed answers. For this type of MRC, answer generation method is crucial\nto the model performance. However, generative models, which are supposed to be\nthe right model for the task, in generally perform poorly. At the same time,\nsingle-span extraction models have been proven effective for extractive MRC,\nwhere the answer is constrained to a single span in the passage. Nevertheless,\nthey generally suffer from generating incomplete answers or introducing\nredundant words when applied to the generative MRC. Thus, we extend the\nsingle-span extraction method to multi-span, proposing a new framework which\nenables generative MRC to be smoothly solved as multi-span extraction. Thorough\nexperiments demonstrate that this novel approach can alleviate the dilemma\nbetween generative models and single-span models and produce answers with\nbetter-formed syntax and semantics.", "published": "2020-09-15 23:06:48", "link": "http://arxiv.org/abs/2009.07382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Known Words to Learn More Words: A Distributional Analysis of\n  Child Vocabulary Development", "abstract": "Why do children learn some words before others? Understanding individual\nvariability across children and also variability across words, may be\ninformative of the learning processes that underlie language learning. We\ninvestigated item-based variability in vocabulary development using lexical\nproperties of distributional statistics derived from a large corpus of\nchild-directed speech. Unlike previous analyses, we predicted word trajectories\ncross-sectionally, shedding light on trends in vocabulary development that may\nnot have been evident at a single time point. We also show that whether one\nlooks at a single age group or across ages as a whole, the best distributional\npredictor of whether a child knows a word is the number of other known words\nwith which that word tends to co-occur. Keywords: age of acquisition;\nvocabulary development; lexical diversity; child-directed speech;", "published": "2020-09-15 01:18:21", "link": "http://arxiv.org/abs/2009.06810v2", "categories": ["cs.CL", "stat.OT"], "primary_category": "cs.CL"}
{"title": "Real-Time Execution of Large-scale Language Models on Mobile", "abstract": "Pre-trained large-scale language models have increasingly demonstrated high\naccuracy on many natural language processing (NLP) tasks. However, the limited\nweight storage and computational speed on hardware platforms have impeded the\npopularity of pre-trained models, especially in the era of edge computing. In\nthis paper, we seek to find the best model structure of BERT for a given\ncomputation size to match specific devices. We propose the first compiler-aware\nneural architecture optimization framework. Our framework can guarantee the\nidentified model to meet both resource and real-time specifications of mobile\ndevices, thus achieving real-time execution of large transformer-based models\nlike BERT variants. We evaluate our model on several NLP tasks, achieving\ncompetitive results on well-known benchmarks with lower latency on mobile\ndevices. Specifically, our model is 5.2x faster on CPU and 4.1x faster on GPU\nwith 0.5-2% accuracy loss compared with BERT-base. Our overall framework\nachieves up to 7.8x speedup compared with TensorFlow-Lite with only minor\naccuracy loss.", "published": "2020-09-15 01:59:17", "link": "http://arxiv.org/abs/2009.06823v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Current Limitations of Language Models: What You Need is Retrieval", "abstract": "We classify and re-examine some of the current approaches to improve the\nperformance-computes trade-off of language models, including (1) non-causal\nmodels (such as masked language models), (2) extension of batch length with\nefficient attention, (3) recurrence, (4) conditional computation and (5)\nretrieval. We identify some limitations (1) - (4) suffer from. For example, (1)\ncurrently struggles with open-ended text generation with the output loosely\nconstrained by the input as well as performing general textual tasks like\nGPT-2/3 due to its need for a specific fine-tuning dataset. (2) and (3) do not\nimprove the prediction of the first $\\sim 10^3$ tokens. Scaling up a model size\n(e.g. efficiently with (4)) still results in poor performance scaling for some\ntasks. We argue (5) would resolve many of these limitations, and it can (a)\nreduce the amount of supervision and (b) efficiently extend the context over\nthe entire training dataset and the entire past of the current sample. We\nspeculate how to modify MARGE to perform unsupervised causal modeling that\nachieves (b) with the retriever jointly trained.", "published": "2020-09-15 04:04:20", "link": "http://arxiv.org/abs/2009.06857v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model", "abstract": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They\nhowever scale with man-hours and high-quality data. Masked Language Models\n(MLMs), such as BERT, scale with computing power as well as unstructured raw\ntext data. The knowledge contained within those models is however not directly\ninterpretable. We propose to perform link prediction with MLMs to address both\nthe KBs scalability issues and the MLMs interpretability issues. To do that we\nintroduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing\nthe mean likelihood of generating the different entities to perform link\nprediction in a tractable manner. We obtain State of the Art (SotA) results on\nthe WN18RR dataset and the best non-entity-embedding based results on the\nFB15k-237 dataset. We also obtain convincing results on link prediction on\npreviously unseen entities, making MLMLM a suitable approach to introducing new\nentities to a KB.", "published": "2020-09-15 13:11:13", "link": "http://arxiv.org/abs/2009.07058v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Critical Thinking for Language Models", "abstract": "This paper takes a first step towards a critical thinking curriculum for\nneural auto-regressive language models. We introduce a synthetic corpus of\ndeductively valid arguments, and generate artificial argumentative texts to\ntrain and evaluate GPT-2. Significant transfer learning effects can be\nobserved: Training a model on three simple core schemes allows it to accurately\ncomplete conclusions of different, and more complex types of arguments, too.\nThe language models generalize the core argument schemes in a correct way.\nMoreover, we obtain consistent and promising results for NLU benchmarks. In\nparticular, pre-training on the argument schemes raises zero-shot accuracy on\nthe GLUE diagnostics by up to 15 percentage points. The findings suggest that\nintermediary pre-training on texts that exemplify basic reasoning abilities\n(such as typically covered in critical thinking textbooks) might help language\nmodels to acquire a broad range of reasoning skills. The synthetic\nargumentative texts presented in this paper are a promising starting point for\nbuilding such a \"critical thinking curriculum for language models.\"", "published": "2020-09-15 15:49:19", "link": "http://arxiv.org/abs/2009.07185v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autoregressive Knowledge Distillation through Imitation Learning", "abstract": "The performance of autoregressive models on natural language generation tasks\nhas dramatically improved due to the adoption of deep, self-attentive\narchitectures. However, these gains have come at the cost of hindering\ninference speed, making state-of-the-art models cumbersome to deploy in\nreal-world, time-sensitive settings. We develop a compression technique for\nautoregressive models that is driven by an imitation learning perspective on\nknowledge distillation. The algorithm is designed to address the exposure bias\nproblem. On prototypical language generation tasks such as translation and\nsummarization, our method consistently outperforms other distillation\nalgorithms, such as sequence-level knowledge distillation. Student models\ntrained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those\ntrained from scratch, while increasing inference speed by up to 14 times in\ncomparison to the teacher model.", "published": "2020-09-15 17:43:02", "link": "http://arxiv.org/abs/2009.07253v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking", "abstract": "Query expansion aims to mitigate the mismatch between the language used in a\nquery and in a document. However, query expansion methods can suffer from\nintroducing non-relevant information when expanding the query. To bridge this\ngap, inspired by recent advances in applying contextualized models like BERT to\nthe document retrieval task, this paper proposes a novel query expansion model\nthat leverages the strength of the BERT model to select relevant document\nchunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test\ncollections, the proposed BERT-QE model significantly outperforms BERT-Large\nmodels.", "published": "2020-09-15 17:50:09", "link": "http://arxiv.org/abs/2009.07258v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Domain Knowledge Empowered Structured Neural Net for End-to-End Event\n  Temporal Relation Extraction", "abstract": "Extracting event temporal relations is a critical task for information\nextraction and plays an important role in natural language understanding. Prior\nsystems leverage deep learning and pre-trained language models to improve the\nperformance of the task. However, these systems often suffer from two\nshort-comings: 1) when performing maximum a posteriori (MAP) inference based on\nneural models, previous systems only used structured knowledge that are assumed\nto be absolutely correct, i.e., hard constraints; 2) biased predictions on\ndominant temporal relations when training with a limited amount of data. To\naddress these issues, we propose a framework that enhances deep neural network\nwith distributional constraints constructed by probabilistic domain knowledge.\nWe solve the constrained inference problem via Lagrangian Relaxation and apply\nit on end-to-end event temporal relation extraction tasks. Experimental results\nshow our framework is able to improve the baseline neural network models with\nstrong statistical significance on two widely used datasets in news and\nclinical domains.", "published": "2020-09-15 22:20:27", "link": "http://arxiv.org/abs/2009.07373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study of Genetic Algorithms for Hyperparameter Optimization of Neural\n  Networks in Machine Translation", "abstract": "With neural networks having demonstrated their versatility and benefits, the\nneed for their optimal performance is as prevalent as ever. A defining\ncharacteristic, hyperparameters, can greatly affect its performance. Thus\nengineers go through a process, tuning, to identify and implement optimal\nhyperparameters. That being said, excess amounts of manual effort are required\nfor tuning network architectures, training configurations, and preprocessing\nsettings such as Byte Pair Encoding (BPE). In this study, we propose an\nautomatic tuning method modeled after Darwin's Survival of the Fittest Theory\nvia a Genetic Algorithm (GA). Research results show that the proposed method, a\nGA, outperforms a random selection of hyperparameters.", "published": "2020-09-15 02:24:16", "link": "http://arxiv.org/abs/2009.08928v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes", "abstract": "High-quality dialogue-summary paired data is expensive to produce and\ndomain-sensitive, making abstractive dialogue summarization a challenging task.\nIn this work, we propose the first unsupervised abstractive dialogue\nsummarization model for tete-a-tetes (SuTaT). Unlike standard text\nsummarization, a dialogue summarization method should consider the\nmulti-speaker scenario where the speakers have different roles, goals, and\nlanguage styles. In a tete-a-tete, such as a customer-agent conversation, SuTaT\naims to summarize for each speaker by modeling the customer utterances and the\nagent utterances separately while retaining their correlations. SuTaT consists\nof a conditional generative module and two unsupervised summarization modules.\nThe conditional generative module contains two encoders and two decoders in a\nvariational autoencoder framework where the dependencies between two latent\nspaces are captured. With the same encoders and decoders, two unsupervised\nsummarization modules equipped with sentence-level self-attention mechanisms\ngenerate summaries without using any annotations. Experimental results show\nthat SuTaT is superior on unsupervised dialogue summarization for both\nautomatic and human evaluations, and is capable of dialogue classification and\nsingle-turn conversation generation.", "published": "2020-09-15 03:27:52", "link": "http://arxiv.org/abs/2009.06851v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "abstract": "When scaled to hundreds of billions of parameters, pretrained language models\nsuch as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.\nHowever, enormous amounts of compute are required for training and applying\nsuch big models, resulting in a large carbon footprint and making it difficult\nfor researchers and practitioners to use them. We show that performance similar\nto GPT-3 can be obtained with language models that are much \"greener\" in that\ntheir parameter count is several orders of magnitude smaller. This is achieved\nby converting textual inputs into cloze questions that contain a task\ndescription, combined with gradient-based optimization; exploiting unlabeled\ndata gives further improvements. We identify key factors required for\nsuccessful natural language understanding with small language models.", "published": "2020-09-15 14:18:53", "link": "http://arxiv.org/abs/2009.07118v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lessons Learned from Applying off-the-shelf BERT: There is no Silver\n  Bullet", "abstract": "One of the challenges in the NLP field is training large classification\nmodels, a task that is both difficult and tedious. It is even harder when GPU\nhardware is unavailable. The increased availability of pre-trained and\noff-the-shelf word embeddings, models, and modules aim at easing the process of\ntraining large models and achieving a competitive performance. We explore the\nuse of off-the-shelf BERT models and share the results of our experiments and\ncompare their results to those of LSTM networks and more simple baselines. We\nshow that the complexity and computational cost of BERT is not a guarantee for\nenhanced predictive performance in the classification tasks at hand.", "published": "2020-09-15 17:24:52", "link": "http://arxiv.org/abs/2009.07238v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Characterization of Sampling Algorithms for Open-ended\n  Language Generation", "abstract": "This work studies the widely adopted ancestral sampling algorithms for\nauto-regressive language models, which is not widely studied in the literature.\nWe use the quality-diversity (Q-D) trade-off to investigate three popular\nsampling algorithms (top-k, nucleus and tempered sampling). We focus on the\ntask of open-ended language generation. We first show that the existing\nsampling algorithms have similar performance. After carefully inspecting the\ntransformations defined by different sampling algorithms, we identify three key\nproperties that are shared among them: entropy reduction, order preservation,\nand slope preservation. To validate the importance of the identified\nproperties, we design two sets of new sampling algorithms: one set in which\neach algorithm satisfies all three properties, and one set in which each\nalgorithm violates at least one of the properties. We compare their performance\nwith existing sampling algorithms, and find that violating the identified\nproperties could lead to drastic performance degradation, as measured by the\nQ-D trade-off. On the other hand, we find that the set of sampling algorithms\nthat satisfies these properties performs on par with the existing sampling\nalgorithms. Our data and code are available at\nhttps://github.com/moinnadeem/characterizing-sampling-algorithms", "published": "2020-09-15 17:28:42", "link": "http://arxiv.org/abs/2009.07243v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pardon the Interruption: An Analysis of Gender and Turn-Taking in U.S.\n  Supreme Court Oral Arguments", "abstract": "This study presents a corpus of turn changes between speakers in U.S. Supreme\nCourt oral arguments. Each turn change is labeled on a spectrum of\n\"cooperative\" to \"competitive\" by a human annotator with legal experience in\nthe United States. We analyze the relationship between speech features, the\nnature of exchanges, and the gender and legal role of the speakers. Finally, we\ndemonstrate that the models can be used to predict the label of an exchange\nwith moderate success. The automatic classification of the nature of exchanges\nindicates that future studies of turn-taking in oral arguments can rely on\nlarger, unlabeled corpora.", "published": "2020-09-15 23:37:48", "link": "http://arxiv.org/abs/2009.07391v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Augmented Natural Language for Generative Sequence Labeling", "abstract": "We propose a generative framework for joint sequence labeling and\nsentence-level classification. Our model performs multiple sequence labeling\ntasks at once using a single, shared natural language output space. Unlike\nprior discriminative methods, our model naturally incorporates label semantics\nand shares knowledge across tasks. Our framework is general purpose, performing\nwell on few-shot, low-resource, and high-resource tasks. We demonstrate these\nadvantages on popular named entity recognition, slot labeling, and intent\nclassification benchmarks. We set a new state-of-the-art for few-shot slot\nlabeling, improving substantially upon the previous 5-shot ($75.0\\% \\rightarrow\n90.9\\%$) and 1-shot ($70.4\\% \\rightarrow 81.0\\%$) state-of-the-art results.\nFurthermore, our model generates large improvements ($46.27\\% \\rightarrow\n63.83\\%$) in low-resource slot labeling over a BERT baseline by incorporating\nlabel semantics. We also maintain competitive results on high-resource tasks,\nperforming within two points of the state-of-the-art on all tasks and setting a\nnew state-of-the-art on the SNIPS dataset.", "published": "2020-09-15 19:23:53", "link": "http://arxiv.org/abs/2009.13272v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The Devil is the Classifier: Investigating Long Tail Relation\n  Classification with Decoupling Analysis", "abstract": "Long-tailed relation classification is a challenging problem as the head\nclasses may dominate the training phase, thereby leading to the deterioration\nof the tail performance. Existing solutions usually address this issue via\nclass-balancing strategies, e.g., data re-sampling and loss re-weighting, but\nall these methods adhere to the schema of entangling learning of the\nrepresentation and classifier. In this study, we conduct an in-depth empirical\ninvestigation into the long-tailed problem and found that pre-trained models\nwith instance-balanced sampling already capture the well-learned\nrepresentations for all classes; moreover, it is possible to achieve better\nlong-tailed classification ability at low cost by only adjusting the\nclassifier. Inspired by this observation, we propose a robust classifier with\nattentive relation routing, which assigns soft weights by automatically\naggregating the relations. Extensive experiments on two datasets demonstrate\nthe effectiveness of our proposed approach. Code and datasets are available in\nhttps://github.com/zjunlp/deepke.", "published": "2020-09-15 12:47:00", "link": "http://arxiv.org/abs/2009.07022v1", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When Automatic Voice Disguise Meets Automatic Speaker Verification", "abstract": "The technique of transforming voices in order to hide the real identity of a\nspeaker is called voice disguise, among which automatic voice disguise (AVD) by\nmodifying the spectral and temporal characteristics of voices with\nmiscellaneous algorithms are easily conducted with softwares accessible to the\npublic. AVD has posed great threat to both human listening and automatic\nspeaker verification (ASV). In this paper, we have found that ASV is not only a\nvictim of AVD but could be a tool to beat some simple types of AVD. Firstly,\nthree types of AVD, pitch scaling, vocal tract length normalization (VTLN) and\nvoice conversion (VC), are introduced as representative methods.\nState-of-the-art ASV methods are subsequently utilized to objectively evaluate\nthe impact of AVD on ASV by equal error rates (EER). Moreover, an approach to\nrestore disguised voice to its original version is proposed by minimizing a\nfunction of ASV scores w.r.t. restoration parameters. Experiments are then\nconducted on disguised voices from Voxceleb, a dataset recorded in real-world\nnoisy scenario. The results have shown that, for the voice disguise by pitch\nscaling, the proposed approach obtains an EER around 7% comparing to the 30%\nEER of a recently proposed baseline using the ratio of fundamental frequencies.\nThe proposed approach generalizes well to restore the disguise with nonlinear\nfrequency warping in VTLN by reducing its EER from 34.3% to 18.5%. However, it\nis difficult to restore the source speakers in VC by our approach, where more\ncomplex forms of restoration functions or other paralinguistic cues might be\nnecessary to restore the nonlinear transform in VC. Finally, contrastive\nvisualization on ASV features with and without restoration illustrate the role\nof the proposed approach in an intuitive way.", "published": "2020-09-15 04:41:52", "link": "http://arxiv.org/abs/2009.06863v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
