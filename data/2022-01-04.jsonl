{"title": "Semantics-Preserved Distortion for Personal Privacy Protection in\n  Information Management", "abstract": "In recent years, machine learning - particularly deep learning - has\nsignificantly impacted the field of information management. While several\nstrategies have been proposed to restrict models from learning and memorizing\nsensitive information from raw texts, this paper suggests a more\nlinguistically-grounded approach to distort texts while maintaining semantic\nintegrity. To this end, we leverage Neighboring Distribution Divergence, a\nnovel metric to assess the preservation of semantic meaning during distortion.\nBuilding on this metric, we present two distinct frameworks for\nsemantic-preserving distortion: a generative approach and a substitutive\napproach. Our evaluations across various tasks, including named entity\nrecognition, constituency parsing, and machine reading comprehension, affirm\nthe plausibility and efficacy of our distortion technique in personal privacy\nprotection. We also test our method against attribute attacks in three\nprivacy-focused assignments within the NLP domain, and the findings underscore\nthe simplicity and efficacy of our data-based improvement approach over\nstructural improvement approaches. Moreover, we explore privacy protection in a\nspecific medical information management scenario, showing our method\neffectively limits sensitive data memorization, underscoring its practicality.", "published": "2022-01-04 04:01:05", "link": "http://arxiv.org/abs/2201.00965v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DigNet: Digging Clues from Local-Global Interactive Graph for\n  Aspect-level Sentiment Classification", "abstract": "In aspect-level sentiment classification (ASC), state-of-the-art models\nencode either syntax graph or relation graph to capture the local syntactic\ninformation or global relational information. Despite the advantages of syntax\nand relation graphs, they have respective shortages which are neglected,\nlimiting the representation power in the graph modeling process. To resolve\ntheir limitations, we design a novel local-global interactive graph, which\nmarries their advantages by stitching the two graphs via interactive edges. To\nmodel this local-global interactive graph, we propose a novel neural network\ntermed DigNet, whose core module is the stacked local-global interactive (LGI)\nlayers performing two processes: intra-graph message passing and cross-graph\nmessage passing. In this way, the local syntactic and global relational\ninformation can be reconciled as a whole in understanding the aspect-level\nsentiment. Concretely, we design two variants of local-global interactive\ngraphs with different kinds of interactive edges and three variants of LGI\nlayers. We conduct experiments on several public benchmark datasets and the\nresults show that we outperform previous best scores by 3\\%, 2.32\\%, and 6.33\\%\nin terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,\nconfirming the effectiveness and superiority of the proposed local-global\ninteractive graph and DigNet.", "published": "2022-01-04 05:34:02", "link": "http://arxiv.org/abs/2201.00989v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games", "abstract": "Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.", "published": "2022-01-04 17:19:52", "link": "http://arxiv.org/abs/2201.01251v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Stacked Local Attention Networks for Diverse Video\n  Captioning", "abstract": "While describing Spatio-temporal events in natural language, video captioning\nmodels mostly rely on the encoder's latent visual representation. Recent\nprogress on the encoder-decoder model attends encoder features mainly in linear\ninteraction with the decoder. However, growing model complexity for visual data\nencourages more explicit feature interaction for fine-grained information,\nwhich is currently absent in the video captioning domain. Moreover, feature\naggregations methods have been used to unveil richer visual representation,\neither by the concatenation or using a linear layer. Though feature sets for a\nvideo semantically overlap to some extent, these approaches result in objective\nmismatch and feature redundancy. In addition, diversity in captions is a\nfundamental component of expressing one event from several meaningful\nperspectives, currently missing in the temporal, i.e., video captioning domain.\nTo this end, we propose Variational Stacked Local Attention Network (VSLAN),\nwhich exploits low-rank bilinear pooling for self-attentive feature interaction\nand stacking multiple video feature streams in a discount fashion. Each feature\nstack's learned attributes contribute to our proposed diversity encoding\nmodule, followed by the decoding query stage to facilitate end-to-end diverse\nand natural captions without any explicit supervision on attributes. We\nevaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity.\nThe CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\\%$\non MSVD and $4.5\\%$ on MSR-VTT, respectively. On the same datasets, VSLAN\nachieves competitive results in caption diversity metrics.", "published": "2022-01-04 05:14:34", "link": "http://arxiv.org/abs/2201.00985v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Predicting Influenza A Viral Host Using PSSM and Word Embeddings", "abstract": "The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.", "published": "2022-01-04 14:05:49", "link": "http://arxiv.org/abs/2201.01140v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Attention AI to translate low light photos to captions for\n  night scene understanding in women safety", "abstract": "There is amazing progress in Deep Learning based models for Image captioning\nand Low Light image enhancement. For the first time in literature, this paper\ndevelops a Deep Learning model that translates night scenes to sentences,\nopening new possibilities for AI applications in the safety of visually\nimpaired women. Inspired by Image Captioning and Visual Question Answering, a\nnovel Interactive Image Captioning is developed. A user can make the AI focus\non any chosen person of interest by influencing the attention scoring.\nAttention context vectors are computed from CNN feature vectors and\nuser-provided start word. The Encoder-Attention-Decoder neural network learns\nto produce captions from low brightness images. This paper demonstrates how\nwomen safety can be enabled by researching a novel AI capability in the\nInteractive Vision-Language model for perception of the environment in the\nnight.", "published": "2022-01-04 04:21:07", "link": "http://arxiv.org/abs/2201.00969v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.0"], "primary_category": "cs.CV"}
{"title": "Submix: Practical Private Prediction for Large-Scale Language Models", "abstract": "Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.", "published": "2022-01-04 04:23:38", "link": "http://arxiv.org/abs/2201.00971v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StyleM: Stylized Metrics for Image Captioning Built with Contrastive\n  N-grams", "abstract": "In this paper, we build two automatic evaluation metrics for evaluating the\nassociation between a machine-generated caption and a ground truth stylized\ncaption: OnlyStyle and StyleCIDEr.", "published": "2022-01-04 04:44:05", "link": "http://arxiv.org/abs/2201.00975v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MDFEND: Multi-domain Fake News Detection", "abstract": "Fake news spread widely on social media in various domains, which lead to\nreal-world threats in many aspects like politics, disasters, and finance. Most\nexisting approaches focus on single-domain fake news detection (SFND), which\nleads to unsatisfying performance when these methods are applied to\nmulti-domain fake news detection. As an emerging field, multi-domain fake news\ndetection (MFND) is increasingly attracting attention. However, data\ndistributions, such as word frequency and propagation patterns, vary from\ndomain to domain, namely domain shift. Facing the challenge of serious domain\nshift, existing fake news detection techniques perform poorly for multi-domain\nscenarios. Therefore, it is demanding to design a specialized model for MFND.\nIn this paper, we first design a benchmark of fake news dataset for MFND with\ndomain label annotated, namely Weibo21, which consists of 4,488 fake news and\n4,640 real news from 9 different domains. We further propose an effective\nMulti-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to\naggregate multiple representations extracted by a mixture of experts. The\nexperiments show that MDFEND can significantly improve the performance of\nmulti-domain fake news detection. Our dataset and code are available at\nhttps://github.com/kennqiang/MDFEND-Weibo21.", "published": "2022-01-04 05:28:25", "link": "http://arxiv.org/abs/2201.00987v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Speech-to-SQL: Towards Speech-driven SQL Query Generation From Natural\n  Language Question", "abstract": "Speech-based inputs have been gaining significant momentum with the\npopularity of smartphones and tablets in our daily lives, since voice is the\nmost easiest and efficient way for human-computer interaction. This paper works\ntowards designing more effective speech-based interfaces to query the\nstructured data in relational databases. We first identify a new task named\nSpeech-to-SQL, which aims to understand the information conveyed by human\nspeech and directly translate it into structured query language (SQL)\nstatements. A naive solution to this problem can work in a cascaded manner,\nthat is, an automatic speech recognition (ASR) component followed by a\ntext-to-SQL component. However, it requires a high-quality ASR system and also\nsuffers from the error compounding problem between the two components,\nresulting in limited performance. To handle these challenges, we further\npropose a novel end-to-end neural architecture named SpeechSQLNet to directly\ntranslate human speech into SQL queries without an external ASR step.\nSpeechSQLNet has the advantage of making full use of the rich linguistic\ninformation presented in speech. To the best of our knowledge, this is the\nfirst attempt to directly synthesize SQL based on arbitrary natural language\nquestions, rather than a natural language-based version of SQL or its variants\nwith a limited SQL grammar. To validate the effectiveness of the proposed\nproblem and model, we further construct a dataset named SpeechQL, by\npiggybacking the widely-used text-to-SQL datasets. Extensive experimental\nevaluations on this dataset show that SpeechSQLNet can directly synthesize\nhigh-quality SQL queries from human speech, outperforming various competitive\ncounterparts as well as the cascaded methods in terms of exact match\naccuracies.", "published": "2022-01-04 15:38:36", "link": "http://arxiv.org/abs/2201.01209v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling", "abstract": "Traditional text classification approaches often require a good amount of\nlabeled data, which is difficult to obtain, especially in restricted domains or\nless widespread languages. This lack of labeled data has led to the rise of\nlow-resource methods, that assume low data availability in natural language\nprocessing. Among them, zero-shot learning stands out, which consists of\nlearning a classifier without any previously labeled data. The best results\nreported with this approach use language models such as Transformers, but fall\ninto two problems: high execution time and inability to handle long texts as\ninput. This paper proposes a new model, ZeroBERTo, which leverages an\nunsupervised clustering step to obtain a compressed data representation before\nthe classification task. We show that ZeroBERTo has better performance for long\ninputs and shorter execution time, outperforming XLM-R by about 12% in the F1\nscore in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,\nZero-Shot Learning, Topic Modeling, Transformers.", "published": "2022-01-04 20:08:17", "link": "http://arxiv.org/abs/2201.01337v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Discriminative Hierarchical PLDA-based Model for Spoken Language\n  Recognition", "abstract": "Spoken language recognition (SLR) refers to the automatic process used to\ndetermine the language present in a speech sample. SLR is an important task in\nits own right, for example, as a tool to analyze or categorize large amounts of\nmulti-lingual data. Further, it is also an essential tool for selecting\ndownstream applications in a work flow, for example, to chose appropriate\nspeech recognition or machine translation models. SLR systems are usually\ncomposed of two stages, one where an embedding representing the audio sample is\nextracted and a second one which computes the final scores for each language.\nIn this work, we approach the SLR task as a detection problem and implement the\nsecond stage as a probabilistic linear discriminant analysis (PLDA) model. We\nshow that discriminative training of the PLDA parameters gives large gains with\nrespect to the usual generative training. Further, we propose a novel\nhierarchical approach where two PLDA models are trained, one to generate scores\nfor clusters of highly-related languages and a second one to generate scores\nconditional to each cluster. The final language detection scores are computed\nas a combination of these two sets of scores. The complete model is trained\ndiscriminatively to optimize a cross-entropy objective. We show that this\nhierarchical approach consistently outperforms the non-hierarchical one for\ndetection of highly related languages, in many cases by large margins. We train\nour systems on a collection of datasets including over 100 languages, and test\nthem both on matched and mismatched conditions, showing that the gains are\nrobust to condition mismatch.", "published": "2022-01-04 22:10:36", "link": "http://arxiv.org/abs/2201.01364v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational\n  Recommender System", "abstract": "Conversational recommender systems (CRS) aim to recommend suitable items to\nusers through natural language conversations. For developing effective CRSs, a\nmajor technical issue is how to accurately infer user preference from very\nlimited conversation context. To address issue, a promising solution is to\nincorporate external data for enriching the context information. However, prior\nstudies mainly focus on designing fusion models tailored for some specific type\nof external data, which is not general to model and utilize multi-type external\ndata.\n  To effectively leverage multi-type external data, we propose a novel\ncoarse-to-fine contrastive learning framework to improve data semantic fusion\nfor CRS. In our approach, we first extract and represent multi-grained semantic\nunits from different data signals, and then align the associated multi-type\nsemantic units in a coarse-to-fine way. To implement this framework, we design\nboth coarse-grained and fine-grained procedures for modeling user preference,\nwhere the former focuses on more general, coarse-grained semantic fusion and\nthe latter focuses on more specific, fine-grained semantic fusion. Such an\napproach can be extended to incorporate more kinds of external data. Extensive\nexperiments on two public CRS datasets have demonstrated the effectiveness of\nour approach in both recommendation and conversation tasks.", "published": "2022-01-04 11:39:41", "link": "http://arxiv.org/abs/2201.02732v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adversarial Transformation of Spoofing Attacks for Voice Biometrics", "abstract": "Voice biometric systems based on automatic speaker verification (ASV) are\nexposed to \\textit{spoofing} attacks which may compromise their security. To\nincrease the robustness against such attacks, anti-spoofing or presentation\nattack detection (PAD) systems have been proposed for the detection of replay,\nsynthesis and voice conversion based attacks. Recently, the scientific\ncommunity has shown that PAD systems are also vulnerable to adversarial\nattacks. However, to the best of our knowledge, no previous work have studied\nthe robustness of full voice biometrics systems (ASV + PAD) to these new types\nof adversarial \\textit{spoofing} attacks. In this work, we develop a new\nadversarial biometrics transformation network (ABTN) which jointly processes\nthe loss of the PAD and ASV systems in order to generate white-box and\nblack-box adversarial \\textit{spoofing} attacks. The core idea of this system\nis to generate adversarial \\textit{spoofing} attacks which are able to fool the\nPAD system without being detected by the ASV system. The experiments were\ncarried out on the ASVspoof 2019 corpus, including both logical access (LA) and\nphysical access (PA) scenarios. The experimental results show that the proposed\nABTN clearly outperforms some well-known adversarial techniques in both\nwhite-box and black-box attack scenarios.", "published": "2022-01-04 16:14:03", "link": "http://arxiv.org/abs/2201.01226v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Classifying Autism from Crowdsourced Semi-Structured Speech Recordings:\n  A Machine Learning Approach", "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder which results\nin altered behavior, social development, and communication patterns. In past\nyears, autism prevalence has tripled, with 1 in 54 children now affected. Given\nthat traditional diagnosis is a lengthy, labor-intensive process, significant\nattention has been given to developing systems that automatically screen for\nautism. Prosody abnormalities are among the clearest signs of autism, with\naffected children displaying speech idiosyncrasies including echolalia,\nmonotonous intonation, atypical pitch, and irregular linguistic stress\npatterns. In this work, we present a suite of machine learning approaches to\ndetect autism in self-recorded speech audio captured from autistic and\nneurotypical (NT) children in home environments. We consider three methods to\ndetect autism in child speech: first, Random Forests trained on extracted audio\nfeatures (including Mel-frequency cepstral coefficients); second, convolutional\nneural networks (CNNs) trained on spectrograms; and third, fine-tuned wav2vec\n2.0--a state-of-the-art Transformer-based ASR model. We train our classifiers\non our novel dataset of cellphone-recorded child speech audio curated from\nStanford's Guess What? mobile game, an app designed to crowdsource videos of\nautistic and neurotypical children in a natural home environment. The Random\nForest classifier achieves 70% accuracy, the fine-tuned wav2vec 2.0 model\nachieves 77% accuracy, and the CNN achieves 79% accuracy when classifying\nchildren's audio as either ASD or NT. Our models were able to predict autism\nstatus when training on a varied selection of home audio clips with\ninconsistent recording quality, which may be more generalizable to real world\nconditions. These results demonstrate that machine learning methods offer\npromise in detecting autism automatically from speech without specialized\nequipment.", "published": "2022-01-04 01:31:02", "link": "http://arxiv.org/abs/2201.00927v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Longitudinal Cough, Breath, and Voice Data for COVID-19\n  Progression Prediction via Sequential Deep Learning: Model Development and\n  Validation", "abstract": "Recent work has shown the potential of using audio data (eg, cough,\nbreathing, and voice) in the screening for COVID-19. However, these approaches\nonly focus on one-off detection and detect the infection given the current\naudio sample, but do not monitor disease progression in COVID-19. Limited\nexploration has been put forward to continuously monitor COVID-19 progression,\nespecially recovery, through longitudinal audio data. Tracking disease\nprogression characteristics could lead to more timely treatment.\n  The primary objective of this study is to explore the potential of\nlongitudinal audio samples over time for COVID-19 progression prediction and,\nespecially, recovery trend prediction using sequential deep learning\ntechniques.\n  Crowdsourced respiratory audio data, including breathing, cough, and voice\nsamples, from 212 individuals over 5-385 days were analyzed. We developed a\ndeep learning-enabled tracking tool using gated recurrent units (GRUs) to\ndetect COVID-19 progression by exploring the audio dynamics of the individuals'\nhistorical audio biomarkers. The investigation comprised 2 parts: (1) COVID-19\ndetection in terms of positive and negative (healthy) tests, and (2)\nlongitudinal disease progression prediction over time in terms of probability\nof positive tests.\n  The strong performance for COVID-19 detection, yielding an AUROC of 0.79, a\nsensitivity of 0.75, and a specificity of 0.71 supported the effectiveness of\nthe approach compared to methods that do not leverage longitudinal dynamics. We\nfurther examined the predicted disease progression trajectory, displaying high\nconsistency with test results with a correlation of 0.75 in the test cohort and\n0.86 in a subset of the test cohort who reported recovery. Our findings suggest\nthat monitoring COVID-19 evolution via longitudinal audio data has potential in\nthe tracking of individuals' disease progression and recovery.", "published": "2022-01-04 16:30:15", "link": "http://arxiv.org/abs/2201.01232v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
