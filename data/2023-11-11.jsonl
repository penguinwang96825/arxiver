{"title": "L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational\n  Language Models", "abstract": "Fine-tuning pre-trained foundational language models (FLM) for specific tasks\nis often impractical, especially for resource-constrained devices. This\nnecessitates the development of a Lifelong Learning (L3) framework that\ncontinuously adapts to a stream of Natural Language Processing (NLP) tasks\nefficiently. We propose an approach that focuses on extracting meaningful\nrepresentations from unseen data, constructing a structured knowledge base, and\nimproving task performance incrementally. We conducted experiments on various\nNLP tasks to validate its effectiveness, including benchmarks like GLUE and\nSuperGLUE. We measured good performance across the accuracy, training\nefficiency, and knowledge transfer metrics. Initial experimental results show\nthat the proposed L3 ensemble method increases the model accuracy by 4% ~ 36%\ncompared to the fine-tuned FLM. Furthermore, L3 model outperforms naive\nfine-tuning approaches while maintaining competitive or superior performance\n(up to 15.4% increase in accuracy) compared to the state-of-the-art language\nmodel (T5) for the given task, STS benchmark.", "published": "2023-11-11 06:59:50", "link": "http://arxiv.org/abs/2311.06493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Added Toxicity Mitigation at Inference Time for Multimodal and Massively\n  Multilingual Translation", "abstract": "Added toxicity in the context of translation refers to the fact of producing\na translation output with more toxicity than there exists in the input. In this\npaper, we present MinTox which is a novel pipeline to identify added toxicity\nand mitigate this issue which works at inference time. MinTox uses a toxicity\ndetection classifier which is multimodal (speech and text) and works in\nlanguages at scale. The mitigation method is applied to languages at scale and\ndirectly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest\nmultimodal and massively multilingual machine translation system. For this\nsystem, MinTox achieves significant added toxicity mitigation across domains,\nmodalities and language directions. MinTox manages to approximately filter out\nfrom 25% to 95% of added toxicity (depending on the modality and domain) while\nkeeping translation quality.", "published": "2023-11-11 11:02:02", "link": "http://arxiv.org/abs/2311.06532v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Translating Legalese: Enhancing Public Understanding of Court Opinions\n  with Legal Summarizers", "abstract": "Judicial opinions are written to be persuasive and could build public trust\nin court decisions, yet they can be difficult for non-experts to understand. We\npresent a pipeline for using an AI assistant to generate simplified summaries\nof judicial opinions. Compared to existing expert-written summaries, these\nAI-generated simple summaries are more accessible to the public and more easily\nunderstood by non-experts. We show in a survey experiment that the AI summaries\nhelp respondents understand the key features of a ruling, and have higher\nperceived quality, especially for respondents with less formal education.", "published": "2023-11-11 11:05:27", "link": "http://arxiv.org/abs/2311.06534v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Lingual Sentiment Classification under Distribution\n  Shift: an Exploratory Study", "abstract": "The brittleness of finetuned language model performance on\nout-of-distribution (OOD) test samples in unseen domains has been well-studied\nfor English, yet is unexplored for multi-lingual models. Therefore, we study\ngeneralization to OOD test data specifically in zero-shot cross-lingual\ntransfer settings, analyzing performance impacts of both language and domain\nshifts between train and test data. We further assess the effectiveness of\ncounterfactually augmented data (CAD) in improving OOD generalization for the\ncross-lingual setting, since CAD has been shown to benefit in a monolingual\nEnglish setting. Finally, we propose two new approaches for OOD generalization\nthat avoid the costly annotation process associated with CAD, by exploiting the\npower of recent large language models (LLMs). We experiment with 3 multilingual\nmodels, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and\nevaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and\nRestaurant reviews. Results echo the OOD performance decline observed in the\nmonolingual English setting. Further, (i) counterfactuals from the original\nhigh-resource language do improve OOD generalization in the low-resource\nlanguage, and (ii) our newly proposed cost-effective approaches reach similar\nor up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.", "published": "2023-11-11 11:56:56", "link": "http://arxiv.org/abs/2311.06549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Classification to Generation: Insights into Crosslingual Retrieval\n  Augmented ICL", "abstract": "The remarkable ability of Large Language Models (LLMs) to understand and\nfollow instructions has sometimes been limited by their in-context learning\n(ICL) performance in low-resource languages. To address this, we introduce a\nnovel approach that leverages cross-lingual retrieval-augmented in-context\nlearning (CREA-ICL). By extracting semantically similar prompts from\nhigh-resource languages, we aim to improve the zero-shot performance of\nmultilingual pre-trained language models (MPLMs) across diverse tasks. Though\nour approach yields steady improvements in classification tasks, it faces\nchallenges in generation tasks. Our evaluation offers insights into the\nperformance dynamics of retrieval-augmented in-context learning across both\nclassification and generation domains.", "published": "2023-11-11 15:40:21", "link": "http://arxiv.org/abs/2311.06595v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance", "abstract": "Answering questions within business and finance requires reasoning,\nprecision, and a wide-breadth of technical knowledge. Together, these\nrequirements make this domain difficult for large language models (LLMs). We\nintroduce BizBench, a benchmark for evaluating models' ability to reason about\nrealistic financial problems. BizBench comprises eight quantitative reasoning\ntasks, focusing on question-answering (QA) over financial data via program\nsynthesis. We include three financially-themed code-generation tasks from newly\ncollected and augmented QA data. Additionally, we isolate the reasoning\ncapabilities required for financial QA: reading comprehension of financial text\nand tables for extracting intermediate values, and understanding financial\nconcepts and formulas needed to calculate complex solutions. Collectively,\nthese tasks evaluate a model's financial background knowledge, ability to parse\nfinancial documents, and capacity to solve problems with code. We conduct an\nin-depth evaluation of open-source and commercial LLMs, comparing and\ncontrasting the behavior of code-focused and language-focused models. We\ndemonstrate that the current bottleneck in performance is due to LLMs' limited\nbusiness and financial understanding, highlighting the value of a challenging\nbenchmark for quantitative reasoning within this domain.", "published": "2023-11-11 16:16:11", "link": "http://arxiv.org/abs/2311.06602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Text Classification: Analyzing Prototype-Based Networks", "abstract": "Downstream applications often require text classification models to be\naccurate and robust. While the accuracy of the state-of-the-art Language Models\n(LMs) approximates human performance, they often exhibit a drop in performance\non noisy data found in the real world. This lack of robustness can be\nconcerning, as even small perturbations in the text, irrelevant to the target\ntask, can cause classifiers to incorrectly change their predictions. A\npotential solution can be the family of Prototype-Based Networks (PBNs) that\nclassifies examples based on their similarity to prototypical examples of a\nclass (prototypes) and has been shown to be robust to noise for computer vision\ntasks. In this paper, we study whether the robustness properties of PBNs\ntransfer to text classification tasks under both targeted and static\nadversarial attack settings. Our results show that PBNs, as a mere\narchitectural variation of vanilla LMs, offer more robustness compared to\nvanilla LMs under both targeted and static settings. We showcase how PBNs'\ninterpretability can help us to understand PBNs' robustness properties.\nFinally, our ablation studies reveal the sensitivity of PBNs' robustness to how\nstrictly clustering is done in the training phase, as tighter clustering\nresults in less robust PBNs.", "published": "2023-11-11 19:34:06", "link": "http://arxiv.org/abs/2311.06647v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Template Is All You Meme", "abstract": "Templatic memes, characterized by a semantic structure adaptable to the\ncreator's intent, represent a significant yet underexplored area within meme\nprocessing literature. With the goal of establishing a new direction for\ncomputational meme analysis, here we create a knowledge base composed of more\nthan 5,200 meme templates, information about them, and 54,000 examples of\ntemplate instances (templatic memes). To investigate the semantic signal of\nmeme templates, we show that we can match memes in datasets to base templates\ncontained in our knowledge base with a distance-based lookup. To demonstrate\nthe power of meme templates, we create TSplit, a method to reorganize datasets,\nwhere a template or templatic instance can only appear in either the training\nor test split. Our re-split datasets enhance general meme knowledge and improve\nsample efficiency, leading to more robust models. Our examination of meme\ntemplates results in state-of-the-art performance for every dataset we\nconsider, paving the way for analysis grounded in templateness.", "published": "2023-11-11 19:38:14", "link": "http://arxiv.org/abs/2311.06649v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intentional Biases in LLM Responses", "abstract": "In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.", "published": "2023-11-11 19:59:24", "link": "http://arxiv.org/abs/2311.07611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Overlook the Grammatical Gender: Bias Evaluation for Hindi-English\n  Machine Translation", "abstract": "Neural Machine Translation (NMT) models, though state-of-the-art for\ntranslation, often reflect social biases, particularly gender bias. Existing\nevaluation benchmarks primarily focus on English as the source language of\ntranslation. For source languages other than English, studies often employ\ngender-neutral sentences for bias evaluation, whereas real-world sentences\nfrequently contain gender information in different forms. Therefore, it makes\nmore sense to evaluate for bias using such source sentences to determine if NMT\nmodels can discern gender from the grammatical gender cues rather than relying\non biased associations. To illustrate this, we create two gender-specific\nsentence sets in Hindi to automatically evaluate gender bias in various\nHindi-English (HI-EN) NMT systems. We emphasise the significance of tailoring\nbias evaluation test sets to account for grammatical gender markers in the\nsource language.", "published": "2023-11-11 09:28:43", "link": "http://arxiv.org/abs/2312.03710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Separating the Wheat from the Chaff with BREAD: An open-source benchmark\n  and metrics to detect redundancy in text", "abstract": "Data quality is a problem that perpetually resurfaces throughout the field of\nNLP, regardless of task, domain, or architecture, and remains especially severe\nfor lower-resource languages. A typical and insidious issue, affecting both\ntraining data and model output, is data that is repetitive and dominated by\nlinguistically uninteresting boilerplate, such as price catalogs or\ncomputer-generated log files. Though this problem permeates many web-scraped\ncorpora, there has yet to be a benchmark to test against, or a systematic study\nto find simple metrics that generalize across languages and agree with human\njudgements of data quality. In the present work, we create and release BREAD, a\nhuman-labeled benchmark on repetitive boilerplate vs. plausible linguistic\ncontent, spanning 360 languages. We release several baseline CRED (Character\nREDundancy) scores along with it, and evaluate their effectiveness on BREAD. We\nhope that the community will use this resource to develop better filtering\nmethods, and that our reference implementations of CRED scores can become\nstandard corpus evaluation tools, driving the development of cleaner language\nmodeling corpora, especially in low-resource languages.", "published": "2023-11-11 00:11:50", "link": "http://arxiv.org/abs/2311.06440v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech", "abstract": "Detecting harmful content on social media, such as Twitter, is made difficult\nby the fact that the seemingly simple yes/no classification conceals a\nsignificant amount of complexity. Unfortunately, while several datasets have\nbeen collected for training classifiers in hate and offensive speech, there is\na scarcity of datasets labeled with a finer granularity of target classes and\nspecific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets\nmanually labeled with fine-grained annotations about the target of the message.\nWe demonstrate that this dataset makes it feasible to train classifiers, based\non Large Language Models, to perform classification at this level of\ngranularity.", "published": "2023-11-11 00:30:31", "link": "http://arxiv.org/abs/2311.06446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocGen: Generating Detailed Parameter Docstrings in Python", "abstract": "Documentation debt hinders the effective utilization of open-source software.\nAlthough code summarization tools have been helpful for developers, most would\nprefer a detailed account of each parameter in a function rather than a\nhigh-level summary. However, generating such a summary is too intricate for a\nsingle generative model to produce reliably due to the lack of high-quality\ntraining data. Thus, we propose a multi-step approach that combines multiple\ntask-specific models, each adept at producing a specific section of a\ndocstring. The combination of these models ensures the inclusion of each\nsection in the final docstring. We compared the results from our approach with\nexisting generative models using both automatic metrics and a human-centred\nevaluation with 17 participating developers, which proves the superiority of\nour approach over existing methods.", "published": "2023-11-11 01:14:37", "link": "http://arxiv.org/abs/2311.06453v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "ALBA: Adaptive Language-based Assessments for Mental Health", "abstract": "Mental health issues differ widely among individuals, with varied signs and\nsymptoms. Recently, language-based assessments have shown promise in capturing\nthis diversity, but they require a substantial sample of words per person for\naccuracy. This work introduces the task of Adaptive Language-Based Assessment\nALBA, which involves adaptively ordering questions while also scoring an\nindividual's latent psychological trait using limited language responses to\nprevious questions. To this end, we develop adaptive testing methods under two\npsychometric measurement theories: Classical Test Theory and Item Response\nTheory. We empirically evaluate ordering and scoring strategies, organizing\ninto two new methods: a semi-supervised item response theory-based method ALIRT\nand a supervised Actor-Critic model. While we found both methods to improve\nover non-adaptive baselines, We found ALIRT to be the most accurate and\nscalable, achieving the highest accuracy with fewer questions (e.g., Pearson r\n~ 0.93 after only 3 questions as compared to typically needing at least 7\nquestions). In general, adaptive language-based assessments of depression and\nanxiety were able to utilize a smaller sample of language without compromising\nvalidity or large computational costs.", "published": "2023-11-11 03:37:17", "link": "http://arxiv.org/abs/2311.06467v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question\n  Answering", "abstract": "Deploying large language models (LLMs) to real scenarios for domain-specific\nquestion answering (QA) is a key thrust for LLM applications, which poses\nnumerous challenges, especially in ensuring that responses are both\naccommodating to user requirements and appropriately leveraging domain-specific\nknowledge bases. They are the two major difficulties for LLM application as\nvanilla fine-tuning falls short of addressing. Combining these requirements, we\nconceive of them as the requirement for the model's preference to be\nharmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle\nthe two issues. Besides, we design a new alignment objective to align the LLM\npreference with different human preferences uniformly, aiming to optimize LLM\nperformance in real-world, domain-specific QA settings. Adequate experiments\nand comprehensive comparisons with 15 baseline methods illustrate that our\nKnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.", "published": "2023-11-11 07:56:40", "link": "http://arxiv.org/abs/2311.06503v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step by Step to Fairness: Attributing Societal Bias in Task-oriented\n  Dialogue Systems", "abstract": "Recent works have shown considerable improvements in task-oriented dialogue\n(TOD) systems by utilizing pretrained large language models (LLMs) in an\nend-to-end manner. However, the biased behavior of each component in a TOD\nsystem and the error propagation issue in the end-to-end framework can lead to\nseriously biased TOD responses. Existing works of fairness only focus on the\ntotal bias of a system. In this paper, we propose a diagnosis method to\nattribute bias to each component of a TOD system. With the proposed attribution\nmethod, we can gain a deeper understanding of the sources of bias.\nAdditionally, researchers can mitigate biased model behavior at a more granular\nlevel. We conduct experiments to attribute the TOD system's bias toward three\ndemographic axes: gender, age, and race. Experimental results show that the\nbias of a TOD system usually comes from the response generation model.", "published": "2023-11-11 09:06:15", "link": "http://arxiv.org/abs/2311.06513v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Minimum Description Length Hopfield Networks", "abstract": "Associative memory architectures are designed for memorization but also\noffer, through their retrieval method, a form of generalization to unseen\ninputs: stored memories can be seen as prototypes from this point of view.\nFocusing on Modern Hopfield Networks (MHN), we show that a large memorization\ncapacity undermines the generalization opportunity. We offer a solution to\nbetter optimize this tradeoff. It relies on Minimum Description Length (MDL) to\ndetermine during training which memories to store, as well as how many of them.", "published": "2023-11-11 09:23:54", "link": "http://arxiv.org/abs/2311.06518v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven\n  Prompting Strategy for Document-Level Event Argument Extraction", "abstract": "In this study, we investigate in-context learning (ICL) in document-level\nevent argument extraction (EAE) to alleviate the dependency on large-scale\nlabeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy\n(HD-LoA) prompting to address the challenge of example selection and to develop\na prompting strategy tailored for EAE. Specifically, we hypothesize and\nvalidate that LLMs learn task-specific heuristics from demonstrations via ICL.\nBuilding upon this hypothesis, we introduce an explicit heuristic-driven\ndemonstration construction approach, which transforms the haphazard example\nselection process into a methodical method that emphasizes task heuristics.\nAdditionally, inspired by the analogical reasoning of human, we propose the\nlink-of-analogy prompting, which enables LLMs to process new situations by\ndrawing analogies to known situations, enhancing their performance on unseen\nclasses beyond limited ICL examples. Experiments show that our method\noutperforms existing prompting methods and few-shot supervised learning methods\non document-level EAE datasets. Additionally, the HD-LoA prompting shows\neffectiveness in diverse tasks like sentiment analysis and natural language\ninference, demonstrating its broad adaptability.", "published": "2023-11-11 12:05:01", "link": "http://arxiv.org/abs/2311.06555v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PerceptionGPT: Effectively Fusing Visual Perception into LLM", "abstract": "The integration of visual inputs with large language models (LLMs) has led to\nremarkable advancements in multi-modal capabilities, giving rise to visual\nlarge language models (VLLMs). However, effectively harnessing VLLMs for\nintricate visual perception tasks remains a challenge. In this paper, we\npresent a novel end-to-end framework named PerceptionGPT, which efficiently and\neffectively equips the VLLMs with visual perception abilities by leveraging the\nrepresentation power of LLMs' token embedding. Our proposed method treats the\ntoken embedding of the LLM as the carrier of spatial information, then leverage\nlightweight visual task encoders and decoders to perform visual perception\ntasks (e.g., detection, segmentation). Our approach significantly alleviates\nthe training difficulty suffered by previous approaches that formulate the\nvisual outputs as discrete tokens, and enables achieving superior performance\nwith fewer trainable parameters, less training data and shorted training time.\nMoreover, as only one token embedding is required to decode the visual outputs,\nthe resulting sequence length during inference is significantly reduced.\nConsequently, our approach enables accurate and flexible representations,\nseamless integration of visual perception tasks, and efficient handling of a\nmultiple of visual outputs. We validate the effectiveness and efficiency of our\napproach through extensive experiments. The results demonstrate significant\nimprovements over previous methods with much fewer trainable parameters and GPU\nhours, which facilitates future research in enabling LLMs with visual\nperception abilities.", "published": "2023-11-11 16:59:20", "link": "http://arxiv.org/abs/2311.06612v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TrainerAgent: Customizable and Efficient Model Training through\n  LLM-Powered Multi-Agent System", "abstract": "Training AI models has always been challenging, especially when there is a\nneed for custom models to provide personalized services. Algorithm engineers\noften face a lengthy process to iteratively develop models tailored to specific\nbusiness requirements, making it even more difficult for non-experts. The quest\nfor high-quality and efficient model development, along with the emergence of\nLarge Language Model (LLM) Agents, has become a key focus in the industry.\nLeveraging the powerful analytical, planning, and decision-making capabilities\nof LLM, we propose a TrainerAgent system comprising a multi-agent framework\nincluding Task, Data, Model and Server agents. These agents analyze\nuser-defined tasks, input data, and requirements (e.g., accuracy, speed),\noptimizing them comprehensively from both data and model perspectives to obtain\nsatisfactory models, and finally deploy these models as online service.\nExperimental evaluations on classical discriminative and generative tasks in\ncomputer vision and natural language processing domains demonstrate that our\nsystem consistently produces models that meet the desired criteria.\nFurthermore, the system exhibits the ability to critically identify and reject\nunattainable tasks, such as fantastical scenarios or unethical requests,\nensuring robustness and safety. This research presents a significant\nadvancement in achieving desired models with increased efficiency and quality\nas compared to traditional model development, facilitated by the integration of\nLLM-powered analysis, decision-making, and execution capabilities, as well as\nthe collaboration among four agents. We anticipate that our work will\ncontribute to the advancement of research on TrainerAgent in both academic and\nindustry communities, potentially establishing it as a new paradigm for model\ndevelopment in the field of AI.", "published": "2023-11-11 17:39:24", "link": "http://arxiv.org/abs/2311.06622v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring ChatGPT's Capabilities on Vulnerability Management", "abstract": "Recently, ChatGPT has attracted great attention from the code analysis\ndomain. Prior works show that ChatGPT has the capabilities of processing\nfoundational code analysis tasks, such as abstract syntax tree generation,\nwhich indicates the potential of using ChatGPT to comprehend code syntax and\nstatic behaviors. However, it is unclear whether ChatGPT can complete more\ncomplicated real-world vulnerability management tasks, such as the prediction\nof security relevance and patch correctness, which require an all-encompassing\nunderstanding of various aspects, including code syntax, program semantics, and\nrelated manual comments.\n  In this paper, we explore ChatGPT's capabilities on 6 tasks involving the\ncomplete vulnerability management process with a large-scale dataset containing\n70,346 samples. For each task, we compare ChatGPT against SOTA approaches,\ninvestigate the impact of different prompts, and explore the difficulties. The\nresults suggest promising potential in leveraging ChatGPT to assist\nvulnerability management. One notable example is ChatGPT's proficiency in tasks\nlike generating titles for software bug reports. Furthermore, our findings\nreveal the difficulties encountered by ChatGPT and shed light on promising\nfuture directions. For instance, directly providing random demonstration\nexamples in the prompt cannot consistently guarantee good performance in\nvulnerability management. By contrast, leveraging ChatGPT in a self-heuristic\nway -- extracting expertise from demonstration examples itself and integrating\nthe extracted expertise in the prompt is a promising research direction.\nBesides, ChatGPT may misunderstand and misuse the information in the prompt.\nConsequently, effectively guiding ChatGPT to focus on helpful information\nrather than the irrelevant content is still an open problem.", "published": "2023-11-11 11:01:13", "link": "http://arxiv.org/abs/2311.06530v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.SE"}
{"title": "Monkey: Image Resolution and Text Label Are Important Things for Large\n  Multi-modal Models", "abstract": "Large Multimodal Models (LMMs) have shown promise in vision-language tasks\nbut struggle with high-resolution input and detailed scene understanding.\nAddressing these challenges, we introduce Monkey to enhance LMM capabilities.\nFirstly, Monkey processes input images by dividing them into uniform patches,\neach matching the size (e.g., 448x448) used in the original training of the\nwell-trained vision encoder. Equipped with individual adapter for each patch,\nMonkey can handle higher resolutions up to 1344x896 pixels, enabling the\ndetailed capture of complex visual information. Secondly, it employs a\nmulti-level description generation method, enriching the context for\nscene-object associations. This two-part strategy ensures more effective\nlearning from generated data: the higher resolution allows for a more detailed\ncapture of visuals, which in turn enhances the effectiveness of comprehensive\ndescriptions. Extensive ablative results validate the effectiveness of our\ndesigns. Additionally, experiments on 18 datasets further demonstrate that\nMonkey surpasses existing LMMs in many tasks like Image Captioning and various\nVisual Question Answering formats. Specially, in qualitative tests focused on\ndense text question answering, Monkey has exhibited encouraging results\ncompared with GPT4V. Code is available at\nhttps://github.com/Yuliang-Liu/Monkey.", "published": "2023-11-11 16:37:41", "link": "http://arxiv.org/abs/2311.06607v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In-context Vectors: Making In Context Learning More Effective and\n  Controllable Through Latent Space Steering", "abstract": "Large language models (LLMs) demonstrate emergent in-context learning\ncapabilities, where they adapt to new tasks based on example demonstrations.\nHowever, in-context learning has seen limited effectiveness in many settings,\nis difficult to quantitatively control and takes up context window space. To\novercome these limitations, we propose an alternative approach that recasts\nin-context learning as in-context vectors (ICV). Using ICV has two steps. We\nfirst use a forward pass on demonstration examples to create the in-context\nvector from the latent embedding of the LLM. This vector captures essential\ninformation about the intended task. On a new query, instead of adding\ndemonstrations to the prompt, we shift the latent states of the LLM using the\nICV. The ICV approach has several benefits: 1) it enables the LLM to more\neffectively follow the demonstration examples; 2) it's easy to control by\nadjusting the magnitude of the ICV; 3) it reduces the length of the prompt by\nremoving the in-context demonstrations; 4) ICV is computationally much more\nefficient than fine-tuning. We demonstrate that ICV achieves better performance\ncompared to standard in-context learning and fine-tuning on diverse tasks\nincluding safety, style transfer, role-playing and formatting. Moreover, we\nshow that we can flexibly teach LLM to simultaneously follow different types of\ninstructions by simple vector arithmetics on the corresponding ICVs.", "published": "2023-11-11 21:19:44", "link": "http://arxiv.org/abs/2311.06668v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Fine-tuning using Generated Respiratory Sound to Address\n  Class Imbalance", "abstract": "Deep generative models have emerged as a promising approach in the medical\nimage domain to address data scarcity. However, their use for sequential data\nlike respiratory sounds is less explored. In this work, we propose a\nstraightforward approach to augment imbalanced respiratory sound data using an\naudio diffusion model as a conditional neural vocoder. We also demonstrate a\nsimple yet effective adversarial fine-tuning method to align features between\nthe synthetic and real respiratory sound samples to improve respiratory sound\nclassification performance. Our experimental results on the ICBHI dataset\ndemonstrate that the proposed adversarial fine-tuning is effective, while only\nusing the conventional augmentation method shows performance degradation.\nMoreover, our method outperforms the baseline by 2.24% on the ICBHI Score and\nimproves the accuracy of the minority classes up to 26.58%. For the\nsupplementary material, we provide the code at\nhttps://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound.", "published": "2023-11-11 05:02:54", "link": "http://arxiv.org/abs/2311.06480v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
