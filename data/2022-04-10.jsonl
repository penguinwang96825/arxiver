{"title": "Re-Examining Human Annotations for Interpretable NLP", "abstract": "Explanation methods in Interpretable NLP often explain the model's decision\nby extracting evidence (rationale) from the input texts supporting the\ndecision. Benchmark datasets for rationales have been released to evaluate how\ngood the rationale is. The ground truth rationales in these datasets are often\nhuman annotations obtained via crowd-sourced websites. Valuable as these\ndatasets are, the details on how those human annotations are obtained are often\nnot clearly specified. We conduct comprehensive controlled experiments using\ncrowd-sourced websites on two widely used datasets in Interpretable NLP to\nunderstand how those unsaid details can affect the annotation results.\nSpecifically, we compare the annotation results obtained from recruiting\nworkers satisfying different levels of qualification. We also provide\nhigh-quality workers with different instructions for completing the same\nunderlying tasks. Our results reveal that the annotation quality is highly\nsubject to the workers' qualification, and workers can be guided to provide\ncertain annotations by the instructions. We further show that specific\nexplanation methods perform better when evaluated using the ground truth\nrationales obtained by particular instructions. Based on these observations, we\nhighlight the importance of providing complete details of the annotation\nprocess and call for careful interpretation of any experiment results obtained\nusing those annotations.", "published": "2022-04-10 02:27:30", "link": "http://arxiv.org/abs/2204.04580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained\n  Language Models For Classification Tasks", "abstract": "Parameter-efficient tuning aims to distill knowledge for downstream tasks by\noptimizing a few introduced parameters while freezing the pretrained language\nmodels (PLMs). Continuous prompt tuning which prepends a few trainable vectors\nto the embeddings of input is one of these methods and has drawn much attention\ndue to its effectiveness and efficiency. This family of methods can be\nillustrated as exerting nonlinear transformations of hidden states inside PLMs.\nHowever, a natural question is ignored: can the hidden states be directly used\nfor classification without changing them? In this paper, we aim to answer this\nquestion by proposing a simple tuning method which only introduces three\ntrainable vectors. Firstly, we integrate all layers hidden states using the\nintroduced vectors. And then, we input the integrated hidden state(s) to a\ntask-specific linear classifier to predict categories. This scheme is similar\nto the way ELMo utilises hidden states except that they feed the hidden states\nto LSTM-based models. Although our proposed tuning scheme is simple, it\nachieves comparable performance with prompt tuning methods like P-tuning and\nP-tuning v2, verifying that original hidden states do contain useful\ninformation for classification tasks. Moreover, our method has an advantage\nover prompt tuning in terms of time and the number of parameters.", "published": "2022-04-10 04:14:02", "link": "http://arxiv.org/abs/2204.04596v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ME-GCN: Multi-dimensional Edge-Embedded Graph Convolutional Networks for\n  Semi-supervised Text Classification", "abstract": "Compared to sequential learning models, graph-based neural networks exhibit\nexcellent ability in capturing global information and have been used for\nsemi-supervised learning tasks. Most Graph Convolutional Networks are designed\nwith the single-dimensional edge feature and failed to utilise the rich edge\ninformation about graphs. This paper introduces the ME-GCN (Multi-dimensional\nEdge-enhanced Graph Convolutional Networks) for semi-supervised text\nclassification. A text graph for an entire corpus is firstly constructed to\ndescribe the undirected and multi-dimensional relationship of word-to-word,\ndocument-document, and word-to-document. The graph is initialised with\ncorpus-trained multi-dimensional word and document node representation, and the\nrelations are represented according to the distance of those words/documents\nnodes. Then, the generated graph is trained with ME-GCN, which considers the\nedge features as multi-stream signals, and each stream performs a separate\ngraph convolutional operation. Our ME-GCN can integrate a rich source of graph\nedge information of the entire text corpus. The results have demonstrated that\nour proposed model has significantly outperformed the state-of-the-art methods\nacross eight benchmark datasets.", "published": "2022-04-10 07:05:12", "link": "http://arxiv.org/abs/2204.04618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Framework for Fast Automated Phonological Reconstruction Using\n  Trimmed Alignments and Sound Correspondence Patterns", "abstract": "Computational approaches in historical linguistics have been increasingly\napplied during the past decade and many new methods that implement parts of the\ntraditional comparative method have been proposed. Despite these increased\nefforts, there are not many easy-to-use and fast approaches for the task of\nphonological reconstruction. Here we present a new framework that combines\nstate-of-the-art techniques for automated sequence comparison with novel\ntechniques for phonetic alignment analysis and sound correspondence pattern\ndetection to allow for the supervised reconstruction of word forms in ancestral\nlanguages. We test the method on a new dataset covering six groups from three\ndifferent language families. The results show that our method yields promising\nresults while at the same time being not only fast but also easy to apply and\nexpand.", "published": "2022-04-10 07:11:19", "link": "http://arxiv.org/abs/2204.04619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing on Personality Detection from Verbal Behavior: A Transformer\n  Meets Text Contours of Psycholinguistic Features", "abstract": "Research at the intersection of personality psychology, computer science, and\nlinguistics has recently focused increasingly on modeling and predicting\npersonality from language use. We report two major improvements in predicting\npersonality traits from text data: (1) to our knowledge, the most comprehensive\nset of theory-based psycholinguistic features and (2) hybrid models that\nintegrate a pre-trained Transformer Language Model BERT and Bidirectional Long\nShort-Term Memory (BLSTM) networks trained on within-text distributions ('text\ncontours') of psycholinguistic features. We experiment with BLSTM models (with\nand without Attention) and with two techniques for applying pre-trained\nlanguage representations from the transformer model - 'feature-based' and\n'fine-tuning'. We evaluate the performance of the models we built on two\nbenchmark datasets that target the two dominant theoretical models of\npersonality: the Big Five Essay dataset and the MBTI Kaggle dataset. Our\nresults are encouraging as our models outperform existing work on the same\ndatasets. More specifically, our models achieve improvement in classification\naccuracy by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. In\naddition, we perform ablation experiments to quantify the impact of different\ncategories of psycholinguistic features in the respective personality\nprediction models.", "published": "2022-04-10 08:08:46", "link": "http://arxiv.org/abs/2204.04629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniDU: Towards A Unified Generative Dialogue Understanding Framework", "abstract": "With the development of pre-trained language models, remarkable success has\nbeen witnessed in dialogue understanding (DU). However, current DU approaches\nusually employ independent models for each distinct DU task without considering\nshared knowledge across different DU tasks. In this paper, we propose a unified\ngenerative dialogue understanding framework, named {\\em UniDU}, to achieve\neffective information exchange across diverse DU tasks. Here, we reformulate\nall DU tasks into a unified prompt-based generative model paradigm. More\nimportantly, a novel model-agnostic multi-task training strategy (MATS) is\nintroduced to dynamically adapt the weights of diverse tasks for best knowledge\nsharing during training, based on the nature and available data of each task.\nExperiments on ten DU datasets covering five fundamental DU tasks show that the\nproposed UniDU framework largely outperforms task-specific well-designed\nmethods on all tasks. MATS also reveals the knowledge-sharing structure of\nthese tasks. Finally, UniDU obtains promising performance in the unseen\ndialogue domain, showing the great potential for generalization.", "published": "2022-04-10 09:32:34", "link": "http://arxiv.org/abs/2204.04637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Model Jitter: Stable Re-training of Semantic Parsers in\n  Production Environments", "abstract": "Retraining modern deep learning systems can lead to variations in model\nperformance even when trained using the same data and hyper-parameters by\nsimply using different random seeds. We call this phenomenon model jitter. This\nissue is often exacerbated in production settings, where models are retrained\non noisy data. In this work we tackle the problem of stable retraining with a\nfocus on conversational semantic parsers. We first quantify the model jitter\nproblem by introducing the model agreement metric and showing the variation\nwith dataset noise and model sizes. We then demonstrate the effectiveness of\nvarious jitter reduction techniques such as ensembling and distillation.\nLastly, we discuss practical trade-offs between such techniques and show that\nco-distillation provides a sweet spot in terms of jitter reduction for semantic\nparsing systems with only a modest increase in resource usage.", "published": "2022-04-10 17:57:55", "link": "http://arxiv.org/abs/2204.04735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Character: Are Subwords Good Enough for MRLs After All?", "abstract": "Large pretrained language models (PLMs) typically tokenize the input string\ninto contiguous subwords before any pretraining or inference. However, previous\nstudies have claimed that this form of subword tokenization is inadequate for\nprocessing morphologically-rich languages (MRLs). We revisit this hypothesis by\npretraining a BERT-style masked language model over character sequences instead\nof word-pieces. We compare the resulting model, dubbed TavBERT, against\ncontemporary PLMs based on subwords for three highly complex and ambiguous MRLs\n(Hebrew, Turkish, and Arabic), testing them on both morphological and semantic\ntasks. Our results show, for all tested languages, that while TavBERT obtains\nmild improvements on surface-level tasks \\`a la POS tagging and full\nmorphological disambiguation, subword-based PLMs achieve significantly higher\nperformance on semantic tasks, such as named entity recognition and extractive\nquestion answering. These results showcase and (re)confirm the potential of\nsubword tokenization as a reasonable modeling assumption for many languages,\nincluding MRLs.", "published": "2022-04-10 18:54:43", "link": "http://arxiv.org/abs/2204.04748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake news detection using parallel BERT deep neural networks", "abstract": "Fake news is a growing challenge for social networks and media. Detection of\nfake news always has been a problem for many years, but after the evolution of\nsocial networks and increasing speed of news dissemination in recent years has\nbeen considered again. There are several approaches to solving this problem,\none of which is to detect fake news based on its text style using deep neural\nnetworks. In recent years, one of the most used forms of deep neural networks\nfor natural language processing is transfer learning with transformers. BERT is\none of the most promising transformers who outperforms other models in many NLP\nbenchmarks. This article, we introduce MWPBert, which uses two parallel BERT\nnetworks to perform veracity detection on full-text news articles. One of the\nBERT networks encodes news headline, and another encodes news body. Since the\ninput length of the BERT network is limited and constant and the news body is\nusually a long text, we cannot fed the whole news text into the BERT.\nTherefore, using the MaxWorth algorithm, we selected the part of the news text\nthat is more valuable for fact-checking, and fed it into the BERT network.\nFinally, we encode the output of the two BERT networks to an output network to\nclassify the news. The experiment results showed that the proposed model\noutperformed previous models in terms of accuracy and other performance\nmeasures.", "published": "2022-04-10 23:16:00", "link": "http://arxiv.org/abs/2204.04793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decay No More: A Persistent Twitter Dataset for Learning Social Meaning", "abstract": "With the proliferation of social media, many studies resort to social media\nto construct datasets for developing social meaning understanding systems. For\nthe popular case of Twitter, most researchers distribute tweet IDs without the\nactual text contents due to the data distribution policy of the platform. One\nissue is that the posts become increasingly inaccessible over time, which leads\nto unfair comparisons and a temporal bias in social media research. To\nalleviate this challenge of data decay, we leverage a paraphrase model to\npropose a new persistent English Twitter dataset for social meaning (PTSM).\nPTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We\nexperiment with two SOTA pre-trained language models and show that our PTSM can\nsubstitute the actual tweets with paraphrases with marginal performance loss.", "published": "2022-04-10 06:07:54", "link": "http://arxiv.org/abs/2204.04611v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Biomedical Factoid Question Answering", "abstract": "We study the effect of seven data augmentation (da) methods in factoid\nquestion answering, focusing on the biomedical domain, where obtaining training\ninstances is particularly difficult. We experiment with data from the BioASQ\nchallenge, which we augment with training instances obtained from an artificial\nbiomedical machine reading comprehension dataset, or via back-translation,\ninformation retrieval, word substitution based on word2vec embeddings, or\nmasked language modeling, question generation, or extending the given passage\nwith additional context. We show that da can lead to very significant\nperformance gains, even when using large pre-trained Transformers, contributing\nto a broader discussion of if/when da benefits large pre-trained models. One of\nthe simplest da methods, word2vec-based word substitution, performed best and\nis recommended. We release our artificial training instances and code.", "published": "2022-04-10 15:57:53", "link": "http://arxiv.org/abs/2204.04711v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedDistant19: Towards an Accurate Benchmark for Broad-Coverage\n  Biomedical Relation Extraction", "abstract": "Relation extraction in the biomedical domain is challenging due to the lack\nof labeled data and high annotation costs, needing domain experts. Distant\nsupervision is commonly used to tackle the scarcity of annotated data by\nautomatically pairing knowledge graph relationships with raw texts. Such a\npipeline is prone to noise and has added challenges to scale for covering a\nlarge number of biomedical concepts. We investigated existing broad-coverage\ndistantly supervised biomedical relation extraction benchmarks and found a\nsignificant overlap between training and test relationships ranging from 26% to\n86%. Furthermore, we noticed several inconsistencies in the data construction\nprocess of these benchmarks, and where there is no train-test leakage, the\nfocus is on interactions between narrower entity types. This work presents a\nmore accurate benchmark MedDistant19 for broad-coverage distantly supervised\nbiomedical relation extraction that addresses these shortcomings and is\nobtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical\nTerms knowledge base. Lacking thorough evaluation with domain-specific language\nmodels, we also conduct experiments validating general domain relation\nextraction findings to biomedical relation extraction.", "published": "2022-04-10 22:07:25", "link": "http://arxiv.org/abs/2204.04779v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Pre-trained Language Models with QA-Memory for Open-Domain\n  Question Answering", "abstract": "Retrieval augmented language models have recently become the standard for\nknowledge intensive tasks. Rather than relying purely on latent semantics\nwithin the parameters of large neural models, these methods enlist a\nsemi-parametric memory to encode an index of knowledge for the model to\nretrieve over. Most prior work has employed text passages as the unit of\nknowledge, which has high coverage at the cost of interpretability,\ncontrollability, and efficiency. The opposite properties arise in other methods\nwhich have instead relied on knowledge base (KB) facts. At the same time, more\nrecent work has demonstrated the effectiveness of storing and retrieving from\nan index of Q-A pairs derived from text \\citep{lewis2021paq}. This approach\nyields a high coverage knowledge representation that maintains KB-like\nproperties due to its representations being more atomic units of information.\nIn this work we push this line of research further by proposing a\nquestion-answer augmented encoder-decoder model and accompanying pretraining\nstrategy. This yields an end-to-end system that not only outperforms prior QA\nretrieval methods on single-hop QA tasks but also enables compositional\nreasoning, as demonstrated by strong performance on two multi-hop QA datasets.\nTogether, these methods improve the ability to interpret and control the model\nwhile narrowing the performance gap with passage retrieval systems.", "published": "2022-04-10 02:33:00", "link": "http://arxiv.org/abs/2204.04581v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"That Is a Suspicious Reaction!\": Interpreting Logits Variation to\n  Detect NLP Adversarial Attacks", "abstract": "Adversarial attacks are a major challenge faced by current machine learning\nresearch. These purposely crafted inputs fool even the most advanced models,\nprecluding their deployment in safety-critical applications. Extensive research\nin computer vision has been carried to develop reliable defense strategies.\nHowever, the same issue remains less explored in natural language processing.\nOur work presents a model-agnostic detector of adversarial text examples. The\napproach identifies patterns in the logits of the target classifier when\nperturbing the input text. The proposed detector improves the current\nstate-of-the-art performance in recognizing adversarial inputs and exhibits\nstrong generalization capabilities across different NLP models, datasets, and\nword-level attacks.", "published": "2022-04-10 09:24:41", "link": "http://arxiv.org/abs/2204.04636v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Linear Complexity Randomized Self-attention Mechanism", "abstract": "Recently, random feature attentions (RFAs) are proposed to approximate the\nsoftmax attention in linear time and space complexity by linearizing the\nexponential kernel. In this paper, we first propose a novel perspective to\nunderstand the bias in such approximation by recasting RFAs as self-normalized\nimportance samplers. This perspective further sheds light on an \\emph{unbiased}\nestimator for the whole softmax attention, called randomized attention (RA). RA\nconstructs positive random features via query-specific distributions and enjoys\ngreatly improved approximation fidelity, albeit exhibiting quadratic\ncomplexity. By combining the expressiveness in RA and the efficiency in RFA, we\ndevelop a novel linear complexity self-attention mechanism called linear\nrandomized attention (LARA). Extensive experiments across various domains\ndemonstrate that RA and LARA significantly improve the performance of RFAs by a\nsubstantial margin.", "published": "2022-04-10 12:10:28", "link": "http://arxiv.org/abs/2204.04667v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of\n  Code-Mixed Clinical Texts", "abstract": "Despite the advances in digital healthcare systems offering curated\nstructured knowledge, much of the critical information still lies in large\nvolumes of unlabeled and unstructured clinical texts. These texts, which often\ncontain protected health information (PHI), are exposed to information\nextraction tools for downstream applications, risking patient identification.\nExisting works in de-identification rely on using large-scale annotated corpora\nin English, which often are not suitable in real-world multilingual settings.\nPre-trained language models (LM) have shown great potential for cross-lingual\ntransfer in low-resource settings. In this work, we empirically show the\nfew-shot cross-lingual transfer property of LMs for named entity recognition\n(NER) and apply it to solve a low-resource and real-world challenge of\ncode-mixed (Spanish-Catalan) clinical notes de-identification in the stroke\ndomain. We annotate a gold evaluation dataset to assess few-shot setting\nperformance where we only use a few hundred labeled examples for training. Our\nmodel improves the zero-shot F1-score from 73.7% to 91.2% on the gold\nevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)\nfrom the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual\ntarget corpus. When generalized to an out-of-sample test set, the best model\nachieves a human-evaluation F1-score of 97.2%.", "published": "2022-04-10 21:46:52", "link": "http://arxiv.org/abs/2204.04775v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inferring Pitch from Coarse Spectral Features", "abstract": "Fundamental frequency (F0) has long been treated as the physical definition\nof \"pitch\" in phonetic analysis. But there have been many demonstrations that\nF0 is at best an approximation to pitch, both in production and in perception:\npitch is not F0, and F0 is not pitch. Changes in the pitch involve many\narticulatory and acoustic covariates; pitch perception often deviates from what\nF0 analysis predicts; and in fact, quasi-periodic signals from a single voice\nsource are often incompletely characterized by an attempt to define a single\ntime-varying F0. In this paper, we find strong support for the existence of\ncovariates for pitch in aspects of relatively coarse spectra, in which an\novertone series is not available. Thus linear regression can predict the pitch\nof simple vocalizations, produced by an articulatory synthesizer or by human,\nfrom single frames of such coarse spectra. Across speakers, and in more complex\nvocalizations, our experiments indicate that the covariates are not quite so\nsimple, though apparently still available for more sophisticated modeling. On\nthis basis, we propose that the field needs a better way of thinking about\nspeech pitch, just as celestial mechanics requires us to go beyond Newton's\npoint mass approximations to heavenly bodies.", "published": "2022-04-10 02:13:03", "link": "http://arxiv.org/abs/2204.04579v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource\n  Parallel Data", "abstract": "Multimodal pre-training for audio-and-text has recently been proved to be\neffective and has significantly improved the performance of many downstream\nspeech understanding tasks. However, these state-of-the-art pre-training\naudio-text models work well only when provided with large amount of parallel\naudio-and-text data, which brings challenges on many languages that are rich in\nunimodal corpora but scarce of parallel cross-modal corpus. In this paper, we\ninvestigate whether it is possible to pre-train an audio-text multimodal model\nwith extremely low-resource parallel data and extra non-parallel unimodal data.\nOur pre-training framework consists of the following components: (1)\nIntra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input\ntext (audio) representations from a noisy version of itself. (2) Cross-modal\nDenoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input\ntext (audio), given both a noisy version of the input text (audio) and the\ncorresponding translated noisy audio features (text embeddings). (3) Iterative\nDenoising Process (IDP), which iteratively translates raw audio (text) and the\ncorresponding text embeddings (audio features) translated from previous\niteration into the new less-noisy text embeddings (audio features). We adapt a\ndual cross-modal Transformer as our backbone model which consists of two\nunimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our\nmethod achieves comparable performance on multiple downstream speech\nunderstanding tasks compared with the model pre-trained on fully parallel data,\ndemonstrating the great potential of the proposed method. Our code is available\nat: \\url{https://github.com/KarlYuKang/Low-Resource-Multimodal-Pre-training}.", "published": "2022-04-10 10:25:37", "link": "http://arxiv.org/abs/2204.04645v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Embeddings for Robust User-Based Amateur Vocal Percussion\n  Classification", "abstract": "Vocal Percussion Transcription (VPT) is concerned with the automatic\ndetection and classification of vocal percussion sound events, allowing music\ncreators and producers to sketch drum lines on the fly. Classifier algorithms\nin VPT systems learn best from small user-specific datasets, which usually\nrestrict modelling to small input feature sets to avoid data overfitting. This\nstudy explores several deep supervised learning strategies to obtain\ninformative feature sets for amateur vocal percussion classification. We\nevaluated the performance of these sets on regular vocal percussion\nclassification tasks and compared them with several baseline approaches\nincluding feature selection methods and a speech recognition engine. These\nproposed learning models were supervised with several label sets containing\ninformation from four different levels of abstraction: instrument-level,\nsyllable-level, phoneme-level, and boxeme-level. Results suggest that\nconvolutional neural networks supervised with syllable-level annotations\nproduced the most informative embeddings for classification, which can be used\nas input representations to fit classifiers with. Finally, we used\nback-propagation-based saliency maps to investigate the importance of different\nspectrogram regions for feature learning.", "published": "2022-04-10 10:26:11", "link": "http://arxiv.org/abs/2204.04646v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Conditional Representation Learning for Drum Sample Retrieval by\n  Vocalisation", "abstract": "Imitating musical instruments with the human voice is an efficient way of\ncommunicating ideas between music producers, from sketching melody lines to\nclarifying desired sonorities. For this reason, there is an increasing interest\nin building applications that allow artists to efficiently pick target samples\nfrom big sound libraries just by imitating them vocally. In this study, we\ninvestigated the potential of conditional autoencoder models to learn\ninformative features for Drum Sample Retrieval by Vocalisation (DSRV). We\nassessed the usefulness of their embeddings using four evaluation metrics, two\nof them relative to their acoustic properties and two of them relative to their\nperceptual properties via human listeners' similarity ratings. Results suggest\nthat models conditioned on both sound-type labels (drum vs imitation) and\ndrum-type labels (kick vs snare vs closed hi-hat vs opened hi-hat) learn the\nmost informative embeddings for DSRV. We finally looked into individual\ndifferences in vocal imitation style via the Mantel test and found salient\ndifferences among participants, highlighting the importance of user information\nwhen designing DSRV systems.", "published": "2022-04-10 10:58:01", "link": "http://arxiv.org/abs/2204.04651v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Evaluation of Autonomously Generated Musical Compositions: A\n  Comprehensive Survey", "abstract": "There are many applications that aim to create a complete model for an\nautonomously generated composition; systems are able to generate muzak songs,\nassist singers in transcribing songs or can imitate long-dead authors.\nSubjective understanding of creativity or aesthetics differs not only within\npreferences (popular authors or genres), but also differs on the basis of\nexperienced experience or socio-cultural environment. So, what do we want to\nachieve with such an adaptation? What is the benefit of the resulting work for\nthe author, who can no longer evaluate this composition? And in what ways\nshould we evaluate such a composition at all?", "published": "2022-04-10 19:42:52", "link": "http://arxiv.org/abs/2204.04756v1", "categories": ["cs.SD", "cs.CY", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
