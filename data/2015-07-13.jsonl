{"title": "Supervised Hierarchical Classification for Student Answer Scoring", "abstract": "This paper describes a hierarchical system that predicts one label at a time\nfor automated student response analysis. For the task, we build a\nclassification binary tree that delays more easily confused labels to later\nstages using hierarchical processes. In particular, the paper describes how the\nhierarchical classifier has been built and how the classification task has been\nbroken down into binary subtasks. It finally discusses the motivations and\nfundamentals of such an approach.", "published": "2015-07-13 14:00:22", "link": "http://arxiv.org/abs/1507.03462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental LSTM-based Dialog State Tracker", "abstract": "A dialog state tracker is an important component in modern spoken dialog\nsystems. We present an incremental dialog state tracker, based on LSTM\nnetworks. It directly uses automatic speech recognition hypotheses to track the\nstate. We also present the key non-standard aspects of the model that bring its\nperformance close to the state-of-the-art and experimentally analyze their\ncontribution: including the ASR confidence scores, abstracting scarcely\nrepresented values, including transcriptions in the training data, and model\naveraging.", "published": "2015-07-13 14:27:16", "link": "http://arxiv.org/abs/1507.03471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural CRF Parsing", "abstract": "This paper describes a parsing model that combines the exact dynamic\nprogramming of CRF parsing with the rich nonlinear featurization of neural net\napproaches. Our model is structurally a CRF that factors over anchored rule\nproductions, but instead of linear potential functions based on sparse\nfeatures, we use nonlinear potentials computed via a feedforward neural\nnetwork. Because potentials are still local to anchored rules, structured\ninference (CKY) is unchanged from the sparse case. Computing gradients during\nlearning involves backpropagating an error signal formed from standard CRF\nsufficient statistics (expected rule counts). Using only dense features, our\nneural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In\ncombination with sparse features, our system achieves 91.1 F1 on section 23 of\nthe Penn Treebank, and more generally outperforms the best prior single parser\nresults on a range of languages.", "published": "2015-07-13 22:23:51", "link": "http://arxiv.org/abs/1507.03641v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
