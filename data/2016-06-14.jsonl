{"title": "Active Discriminative Text Representation Learning", "abstract": "We propose a new active learning (AL) method for text classification with\nconvolutional neural networks (CNNs). In AL, one selects the instances to be\nmanually labeled with the aim of maximizing model performance with minimal\neffort. Neural models capitalize on word embeddings as representations\n(features), tuning these to the task at hand. We argue that AL strategies for\nmulti-layered neural models should focus on selecting instances that most\naffect the embedding space (i.e., induce discriminative word representations).\nThis is in contrast to traditional AL approaches (e.g., entropy-based\nuncertainty sampling), which specify higher level objectives. We propose a\nsimple approach for sentence classification that selects instances containing\nwords whose embeddings are likely to be updated with the greatest magnitude,\nthereby rapidly learning discriminative, task-specific embeddings. We extend\nthis approach to document classification by jointly considering: (1) the\nexpected changes to the constituent word representations; and (2) the model's\ncurrent overall uncertainty regarding the instance. The relative emphasis\nplaced on these criteria is governed by a stochastic process that favors\nselecting instances likely to improve representations at the outset of\nlearning, and then shifts toward general uncertainty sampling as AL progresses.\nEmpirical results show that our method outperforms baseline AL approaches on\nboth sentence and document classification tasks. We also show that, as\nexpected, the method quickly learns discriminative word embeddings. To the best\nof our knowledge, this is the first work on AL addressing neural models for\ntext classification.", "published": "2016-06-14 06:49:40", "link": "http://arxiv.org/abs/1606.04212v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Morphological Tagging for Low-Resource Languages", "abstract": "Morphologically rich languages often lack the annotated linguistic resources\nrequired to develop accurate natural language processing tools. We propose\nmodels suitable for training morphological taggers with rich tagsets for\nlow-resource languages without using direct supervision. Our approach extends\nexisting approaches of projecting part-of-speech tags across languages, using\nbitext to infer constraints on the possible tags for a given word type or\ntoken. We propose a tagging model using Wsabie, a discriminative\nembedding-based model with rank-based learning. In our evaluation on 11\nlanguages, on average this model performs on par with a baseline\nweakly-supervised HMM, while being more scalable. Multilingual experiments show\nthat the method performs best when projecting between related language pairs.\nDespite the inherently lossy projection, we show that the morphological tags\npredicted by our models improve the downstream performance of a parser by +0.6\nLAS on average.", "published": "2016-06-14 09:43:36", "link": "http://arxiv.org/abs/1606.04279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Word Segmentation Learning for Chinese", "abstract": "Most previous approaches to Chinese word segmentation formalize this problem\nas a character-based sequence labeling task where only contextual information\nwithin fixed sized local windows and simple interactions between adjacent tags\ncan be captured. In this paper, we propose a novel neural framework which\nthoroughly eliminates context windows and can utilize complete segmentation\nhistory. Our model employs a gated combination neural network over characters\nto produce distributed representations of word candidates, which are then given\nto a long short-term memory (LSTM) language scoring model. Experiments on the\nbenchmark datasets show that without the help of feature engineering as most\nexisting approaches, our models achieve competitive or better performances with\nprevious state-of-the-art methods.", "published": "2016-06-14 10:52:21", "link": "http://arxiv.org/abs/1606.04300v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow Discourse Parsing Using Distributed Argument Representations and\n  Bayesian Optimization", "abstract": "This paper describes the Georgia Tech team's approach to the CoNLL-2016\nsupplementary evaluation on discourse relation sense classification. We use\nlong short-term memories (LSTM) to induce distributed representations of each\nargument, and then combine these representations with surface features in a\nneural network. The architecture of the neural network is determined by\nBayesian hyperparameter search.", "published": "2016-06-14 19:00:59", "link": "http://arxiv.org/abs/1606.04503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT)\nproblems using neural networks and has exhibited promising results in recent\nyears. However, most of the existing NMT models are shallow and there is still\na performance gap between a single NMT model and the best conventional MT\nsystem. In this work, we introduce a new type of linear connections, named\nfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,\nand an interleaved bi-directional architecture for stacking the LSTM layers.\nFast-forward connections play an essential role in propagating the gradients\nand building a deep topology of depth 16. On the WMT'14 English-to-French task,\nwe achieve BLEU=37.7 with a single attention model, which outperforms the\ncorresponding single shallow model by 6.2 BLEU points. This is the first time\nthat a single NMT model achieves state-of-the-art performance and outperforms\nthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3\neven without using an attention mechanism. After special handling of unknown\nwords and model ensembling, we obtain the best score reported to date on this\ntask with BLEU=40.4. Our models are also validated on the more difficult WMT'14\nEnglish-to-German task.", "published": "2016-06-14 03:53:00", "link": "http://arxiv.org/abs/1606.04199v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Representation Models for Morphologically Rich Languages in Neural\n  Machine Translation", "abstract": "Dealing with the complex word forms in morphologically rich languages is an\nopen problem in language processing, and is particularly important in\ntranslation. In contrast to most modern neural systems of translation, which\ndiscard the identity for rare words, in this paper we propose several\narchitectures for learning word representations from character and morpheme\nlevel word decompositions. We incorporate these representations in a novel\nmachine translation model which jointly learns word alignments and translations\nvia a hard attention mechanism. Evaluating on translating from several\nmorphologically rich languages into English, we show consistent improvements\nover strong baseline methods, of between 1 and 1.5 BLEU points.", "published": "2016-06-14 07:04:37", "link": "http://arxiv.org/abs/1606.04217v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation", "abstract": "The selection of a suitable document representation approach plays a crucial\nrole in the performance of a document clustering task. Being able to pick out\nrepresentative words within a document can lead to substantial improvements in\ndocument clustering. In the case of web documents, the HTML markup that defines\nthe layout of the content provides additional structural information that can\nbe further exploited to identify representative words. In this paper we\nintroduce a fuzzy term weighing approach that makes the most of the HTML\nstructure for document clustering. We set forth and build on the hypothesis\nthat a good representation can take advantage of how humans skim through\ndocuments to extract the most representative words. The authors of web pages\nmake use of HTML tags to convey the most important message of a web page\nthrough page elements that attract the readers' attention, such as page titles\nor emphasized elements. We define a set of criteria to exploit the information\nprovided by these page elements, and introduce a fuzzy combination of these\ncriteria that we evaluate within the context of a web page clustering task. Our\nproposed approach, called Abstract Fuzzy Combination of Criteria (AFCC), can\nadapt to datasets whose features are distributed differently, achieving good\nresults compared to other similar fuzzy logic based approaches and TF-IDF\nacross different datasets.", "published": "2016-06-14 15:44:52", "link": "http://arxiv.org/abs/1606.04429v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Query-Reduction Networks for Question Answering", "abstract": "In this paper, we study the problem of question answering when reasoning over\nmultiple facts is required. We propose Query-Reduction Network (QRN), a variant\nof Recurrent Neural Network (RNN) that effectively handles both short-term\n(local) and long-term (global) sequential dependencies to reason over multiple\nfacts. QRN considers the context sentences as a sequence of state-changing\ntriggers, and reduces the original query to a more informed query as it\nobserves each trigger (context sentence) through time. Our experiments show\nthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and\nin a real goal-oriented dialog dataset. In addition, QRN formulation allows\nparallelization on RNN's time axis, saving an order of magnitude in time\ncomplexity for training and inference.", "published": "2016-06-14 21:54:46", "link": "http://arxiv.org/abs/1606.04582v6", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Automatic Text Scoring Using Neural Networks", "abstract": "Automated Text Scoring (ATS) provides a cost-effective and consistent\nalternative to human marking. However, in order to achieve good performance,\nthe predictive features of the system need to be manually engineered by human\nexperts. We introduce a model that forms word representations by learning the\nextent to which specific words contribute to the text's score. Using Long-Short\nTerm Memory networks to represent the meaning of texts, we demonstrate that a\nfully automated framework is able to achieve excellent results over similar\napproaches. In an attempt to make our results more interpretable, and inspired\nby recent advances in visualizing neural networks, we introduce a novel method\nfor identifying the regions of the text that the model has found more\ndiscriminative.", "published": "2016-06-14 10:17:27", "link": "http://arxiv.org/abs/1606.04289v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "abstract": "This paper describes the participation of the team \"TwiSE\" in the SemEval\n2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment\nAnalysis in Twitter\" for which we implemented sentiment classification systems\nfor subtasks A, B, C and D. Our approach consists of two steps. In the first\nstep, we generate and validate diverse feature sets for twitter sentiment\nevaluation, inspired by the work of participants of previous editions of such\nchallenges. In the second step, we focus on the optimization of the evaluation\nmeasures of the different subtasks. To this end, we examine different learning\nstrategies by validating them on the data provided by the task organisers. For\nour final submissions we used an ensemble learning approach (stacked\ngeneralization) for Subtask A and single linear models for the rest of the\nsubtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14\nfor subtasks A, B, C and D respectively.\\footnote{We make the code available\nfor research purposes at\n\\url{https://github.com/balikasg/SemEval2016-Twitter\\_Sentiment\\_Evaluation}.}", "published": "2016-06-14 13:36:00", "link": "http://arxiv.org/abs/1606.04351v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
