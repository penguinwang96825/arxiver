{"title": "ExcavatorCovid: Extracting Events and Relations from Text Corpora for\n  Temporal and Causal Analysis for COVID-19", "abstract": "Timely responses from policy makers to mitigate the impact of the COVID-19\npandemic rely on a comprehensive grasp of events, their causes, and their\nimpacts. These events are reported at such a speed and scale as to be\noverwhelming. In this paper, we present ExcavatorCovid, a machine reading\nsystem that ingests open-source text documents (e.g., news and scientific\npublications), extracts COVID19 related events and relations between them, and\nbuilds a Temporal and Causal Analysis Graph (TCAG). Excavator will help\ngovernment agencies alleviate the information overload, understand likely\ndownstream effects of political and economic decisions and events related to\nthe pandemic, and respond in a timely manner to mitigate the impact of\nCOVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic:\nanalysts and decision makers will be empowered by Excavator to better\nunderstand and solve complex problems in the future. An interactive TCAG\nvisualization is available at http://afrl402.bbn.com:5050/index.html. We also\nreleased a demonstration video at https://vimeo.com/528619007.", "published": "2021-05-05 01:18:46", "link": "http://arxiv.org/abs/2105.01819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind Reading at Work: Cooperation without common ground", "abstract": "As Stefan Kopp and Nicole Kramer say in their recent paper[Frontiers in\nPsychology 12 (2021) 597], despite some very impressive demonstrations over the\nlast decade or so, we still don't know how how to make a computer have a half\ndecent conversation with a human. They argue that the capabilities required to\ndo this include incremental joint co-construction and mentalizing. Although\nagreeing whole heartedly with their statement of the problem, this paper argues\nfor a different approach to the solution based on the \"new\" AI of situated\naction.", "published": "2021-05-05 09:37:21", "link": "http://arxiv.org/abs/2105.01949v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation Of Word Embeddings From Large-Scale French Web Content", "abstract": "Distributed word representations are popularly used in many tasks in natural\nlanguage processing. Adding that pretrained word vectors on huge text corpus\nachieved high performance in many different NLP tasks. This paper introduces\nmultiple high-quality word vectors for the French language where two of them\nare trained on massive crawled French data during this study and the others are\ntrained on an already existing French corpus. We also evaluate the quality of\nour proposed word vectors and the existing French word vectors on the French\nword analogy task. In addition, we do the evaluation on multiple real NLP tasks\nthat shows the important performance enhancement of the pre-trained word\nvectors compared to the existing and random ones. Finally, we created a demo\nweb application to test and visualize the obtained word embeddings. The\nproduced French word embeddings are available to the public, along with the\nfinetuning code on the NLU tasks and the demo code.", "published": "2021-05-05 11:34:22", "link": "http://arxiv.org/abs/2105.01990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak\n  Supervision", "abstract": "The identification of rare diseases from clinical notes with Natural Language\nProcessing (NLP) is challenging due to the few cases available for machine\nlearning and the need of data annotation from clinical experts. We propose a\nmethod using ontologies and weak supervision. The approach includes two steps:\n(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language\nSystem (UMLS), with a named entity linking tool (e.g. SemEHR) and weak\nsupervision based on customised rules and Bidirectional Encoder Representations\nfrom Transformers (BERT) based contextual representations, and (ii)\nUMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease\nOntology (ORDO). Using MIMIC-III US intensive care discharge summaries as a\ncase study, we show that the Text-to-UMLS process can be greatly improved with\nweak supervision, without any annotated data from domain experts. Our analysis\nshows that the overall pipeline processing discharge summaries can surface rare\ndisease cases, which are mostly uncaptured in manual ICD codes of the hospital\nadmissions.", "published": "2021-05-05 11:49:09", "link": "http://arxiv.org/abs/2105.01995v3", "categories": ["cs.CL", "68T50 (Primary), 68T30 (Secondary)", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "ADAM: A Sandbox for Implementing Language Learning", "abstract": "We present ADAM, a software system for designing and running child language\nlearning experiments in Python. The system uses a virtual world to simulate a\ngrounded language acquisition process in which the language learner utilizes\ncognitively plausible learning algorithms to form perceptual and linguistic\nrepresentations of the observed world. The modular nature of ADAM makes it easy\nto design and test different language learning curricula as well as learning\nalgorithms. In this report, we describe the architecture of the ADAM system in\ndetail, and illustrate its components with examples. We provide our code.", "published": "2021-05-05 18:19:29", "link": "http://arxiv.org/abs/2105.02263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Quality Assessment: A Brief Survey on Manual and Automatic\n  Methods", "abstract": "To facilitate effective translation modeling and translation studies, one of\nthe crucial questions to address is how to assess translation quality. From the\nperspectives of accuracy, reliability, repeatability and cost, translation\nquality assessment (TQA) itself is a rich and challenging task. In this work,\nwe present a high-level and concise survey of TQA methods, including both\nmanual judgement criteria and automated evaluation metrics, which we classify\ninto further detailed sub-categories. We hope that this work will be an asset\nfor both translation model researchers and quality assessment researchers. In\naddition, we hope that it will enable practitioners to quickly develop a better\nunderstanding of the conventional TQA field, and to find corresponding closely\nrelevant evaluation solutions for their own needs. This work may also serve\ninspire further development of quality assessment and evaluation methodologies\nfor other natural language processing (NLP) tasks in addition to machine\ntranslation (MT), such as automatic text summarization (ATS), natural language\nunderstanding (NLU) and natural language generation (NLG).", "published": "2021-05-05 18:28:10", "link": "http://arxiv.org/abs/2105.03311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Full-Sentence Models Perform Better in Simultaneous Translation Using\n  the Information Enhanced Decoding Strategy", "abstract": "Simultaneous translation, which starts translating each sentence after\nreceiving only a few words in source sentence, has a vital role in many\nscenarios. Although the previous prefix-to-prefix framework is considered\nsuitable for simultaneous translation and achieves good performance, it still\nhas two inevitable drawbacks: the high computational resource costs caused by\nthe need to train a separate model for each latency $k$ and the insufficient\nability to encode information because each target token can only attend to a\nspecific source prefix. We propose a novel framework that adopts a simple but\neffective decoding strategy which is designed for full-sentence models. Within\nthis framework, training a single full-sentence model can achieve arbitrary\ngiven latency and save computational resources. Besides, with the competence of\nthe full-sentence model to encode the whole sentence, our decoding strategy can\nenhance the information maintained in the decoded states in real time.\nExperimental results show that our method achieves better translation quality\nthan baselines on 4 directions: Zh$\\rightarrow$En, En$\\rightarrow$Ro and\nEn$\\leftrightarrow$De.", "published": "2021-05-05 07:03:41", "link": "http://arxiv.org/abs/2105.01893v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Natural Language Explanations Represent Valid Logical Arguments?\n  Verifying Entailment in Explainable NLI Gold Standards", "abstract": "An emerging line of research in Explainable NLP is the creation of datasets\nenriched with human-annotated explanations and rationales, used to build and\nevaluate models with step-wise inference and explanation generation\ncapabilities. While human-annotated explanations are used as ground-truth for\nthe inference, there is a lack of systematic assessment of their consistency\nand rigour. In an attempt to provide a critical quality assessment of\nExplanation Gold Standards (XGSs) for NLI, we propose a systematic annotation\nmethodology, named Explanation Entailment Verification (EEV), to quantify the\nlogical validity of human-annotated explanations. The application of EEV on\nthree mainstream datasets reveals the surprising conclusion that a majority of\nthe explanations, while appearing coherent on the surface, represent logically\ninvalid arguments, ranging from being incomplete to containing clearly\nidentifiable logical errors. This conclusion confirms that the inferential\nproperties of explanations are still poorly formalised and understood, and that\nadditional work on this line of research is necessary to improve the way\nExplanation Gold Standards are constructed.", "published": "2021-05-05 10:59:26", "link": "http://arxiv.org/abs/2105.01974v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Search: Making Domain Experts out of Dilettantes", "abstract": "When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.", "published": "2021-05-05 18:40:00", "link": "http://arxiv.org/abs/2105.02274v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Content4All Open Research Sign Language Translation Datasets", "abstract": "Computational sign language research lacks the large-scale datasets that\nenables the creation of useful reallife applications. To date, most research\nhas been limited to prototype systems on small domains of discourse, e.g.\nweather forecasts. To address this issue and to push the field forward, we\nrelease six datasets comprised of 190 hours of footage on the larger domain of\nnews. From this, 20 hours of footage have been annotated by Deaf experts and\ninterpreters and is made publicly available for research purposes. In this\npaper, we share the dataset collection process and tools developed to enable\nthe alignment of sign language video and subtitles, as well as baseline\ntranslation results to underpin future research.", "published": "2021-05-05 22:14:53", "link": "http://arxiv.org/abs/2105.02351v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Genetic Algorithms For Extractive Summarization", "abstract": "Most current work in NLP utilizes deep learning, which requires a lot of\ntraining data and computational power. This paper investigates the strengths of\nGenetic Algorithms (GAs) for extractive summarization, as we hypothesized that\nGAs could construct more efficient solutions for the summarization task due to\ntheir relative customizability relative to deep learning models. This is done\nby building a vocabulary set, the words of which are represented as an array of\nweights, and optimizing those set of weights with the GA. These weights can be\nused to build an overall weighting of a sentence, which can then be passed to\nsome threshold for extraction. Our results showed that the GA was able to learn\na weight representation that could filter out excessive vocabulary and thus\ndictate sentence importance based on common English words.", "published": "2021-05-05 23:14:41", "link": "http://arxiv.org/abs/2105.02365v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Base Construction in the Age of Big Data", "abstract": "Compiling commonsense knowledge is traditionally an AI topic approached by\nmanual labor. Recent advances in web data processing have enabled automated\napproaches. In this demonstration we will showcase three systems for automated\ncommonsense knowledge base construction, highlighting each time one aspect of\nspecific interest to the data management community. (i) We use Quasimodo to\nillustrate knowledge extraction systems engineering, (ii) Dice to illustrate\nthe role that schema constraints play in cleaning fuzzy commonsense knowledge,\nand (iii) Ascent to illustrate the relevance of conceptual modelling. The demos\nare available online at https://quasimodo.r2.enst.fr,\nhttps://dice.mpi-inf.mpg.de and ascent.mpi-inf.mpg.de.", "published": "2021-05-05 08:27:36", "link": "http://arxiv.org/abs/2105.01925v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Polynomial Graph Parsing with Non-Structural Reentrancies", "abstract": "Graph-based semantic representations are valuable in natural language\nprocessing, where it is often simple and effective to represent linguistic\nconcepts as nodes, and relations as edges between them. Several attempts has\nbeen made to find a generative device that is sufficiently powerful to\nrepresent languages of semantic graphs, while at the same allowing efficient\nparsing. We add to this line of work by introducing graph extension grammar,\nwhich consists of an algebra over graphs together with a regular tree grammar\nthat generates expressions over the operations of the algebra. Due to the\ndesign of the operations, these grammars can generate graphs with\nnon-structural reentrancies; a type of node-sharing that is excessively common\nin formalisms such as abstract meaning representation, but for which existing\ndevices offer little support. We provide a parsing algorithm for graph\nextension grammars, which is proved to be correct and run in polynomial time.", "published": "2021-05-05 13:05:01", "link": "http://arxiv.org/abs/2105.02033v3", "categories": ["cs.FL", "cs.CL", "cs.DM", "68R10 (Primary) 05C85 (Secondary)", "F.4.3; I.2.7; I.2.4"], "primary_category": "cs.FL"}
{"title": "Acoustic Scene Classification Using Multichannel Observation with\n  Partially Missing Channels", "abstract": "Sounds recorded with smartphones or IoT devices often have partially\nunreliable observations caused by clipping, wind noise, and completely missing\nparts due to microphone failure and packet loss in data transmission over the\nnetwork. In this paper, we investigate the impact of the partially missing\nchannels on the performance of acoustic scene classification using multichannel\naudio recordings, especially for a distributed microphone array. Missing\nobservations cause not only losses of time-frequency and spatial information on\nsound sources but also a mismatch between a trained model and evaluation data.\nWe thus investigate how a missing channel affects the performance of acoustic\nscene classification in detail. We also propose simple data augmentation\nmethods for scene classification using multichannel observations with partially\nmissing channels and evaluate the scene classification performance using the\ndata augmentation methods.", "published": "2021-05-05 02:21:58", "link": "http://arxiv.org/abs/2105.01836v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved feature extraction for CRNN-based multiple sound source\n  localization", "abstract": "In this work, we propose to extend a state-of-the-art multi-source\nlocalization system based on a convolutional recurrent neural network and\nAmbisonics signals. We significantly improve the performance of the baseline\nnetwork by changing the layout between convolutional and pooling layers. We\npropose several configurations with more convolutional layers and smaller\npooling sizes in-between, so that less information is lost across the layers,\nleading to a better feature extraction. In parallel, we test the system's\nability to localize up to 3 sources, in which case the improved feature\nextraction provides the most significant boost in accuracy. We evaluate and\ncompare these improved configurations on synthetic and real-world data. The\nobtained results show a quite substantial improvement of the multiple sound\nsource localization performance over the baseline network.", "published": "2021-05-05 07:12:54", "link": "http://arxiv.org/abs/2105.01897v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accent Recognition with Hybrid Phonetic Features", "abstract": "The performance of voice-controlled systems is usually influenced by accented\nspeech. To make these systems more robust, the frontend accent recognition (AR)\ntechnologies have received increased attention in recent years. As accent is a\nhigh-level abstract feature that has a profound relationship with the language\nknowledge, AR is more challenging than other language-agnostic audio\nclassification tasks. In this paper, we use an auxiliary automatic speech\nrecognition (ASR) task to extract language-related phonetic features.\nFurthermore, we propose a hybrid structure that incorporates the embeddings of\nboth a fixed acoustic model and a trainable acoustic model, making the\nlanguage-related acoustic feature more robust. We conduct several experiments\non the Accented English Speech Recognition Challenge (AESRC) 2020 dataset. The\nresults demonstrate that our approach can obtain a 6.57% relative improvement\non the validation set. We also get a 7.28% relative improvement on the final\ntest set for this competition, showing the merits of the proposed method.", "published": "2021-05-05 08:12:15", "link": "http://arxiv.org/abs/2105.01920v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "How do Voices from Past Speech Synthesis Challenges Compare Today?", "abstract": "Shared challenges provide a venue for comparing systems trained on common\ndata using a standardized evaluation, and they also provide an invaluable\nresource for researchers when the data and evaluation results are publicly\nreleased. The Blizzard Challenge and Voice Conversion Challenge are two such\nchallenges for text-to-speech synthesis and for speaker conversion,\nrespectively, and their publicly-available system samples and listening test\nresults comprise a historical record of state-of-the-art synthesis methods over\nthe years. In this paper, we revisit these past challenges and conduct a\nlarge-scale listening test with samples from many challenges combined. Our aims\nare to analyze and compare opinions of a large number of systems together, to\ndetermine whether and how opinions change over time, and to collect a\nlarge-scale dataset of a diverse variety of synthetic samples and their ratings\nfor further research. We found strong correlations challenge by challenge at\nthe system level between the original results and our new listening test. We\nalso observed the importance of the choice of speaker on synthesis quality.", "published": "2021-05-05 23:53:27", "link": "http://arxiv.org/abs/2105.02373v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Interpretable and Transferable Speech Emotion Recognition:\n  Latent Representation Based Analysis of Features, Methods and Corpora", "abstract": "In recent years, speech emotion recognition (SER) has been used in wide\nranging applications, from healthcare to the commercial sector. In addition to\nsignal processing approaches, methods for SER now also use deep learning\ntechniques. However, generalizing over languages, corpora and recording\nconditions is still an open challenge in the field. Furthermore, due to the\nblack-box nature of deep learning algorithms, a newer challenge is the lack of\ninterpretation and transparency in the models and the decision making process.\nThis is critical when the SER systems are deployed in applications that\ninfluence human lives. In this work we address this gap by providing an\nin-depth analysis of the decision making process of the proposed SER system.\nTowards that end, we present low-complexity SER based on undercomplete- and\ndenoising- autoencoders that achieve an average classification accuracy of over\n55\\% for four-class emotion classification. Following this, we investigate the\nclustering of emotions in the latent space to understand the influence of the\ncorpora on the model behavior and to obtain a physical interpretation of the\nlatent embedding. Lastly, we explore the role of each input feature towards the\nperformance of the SER.", "published": "2021-05-05 13:47:39", "link": "http://arxiv.org/abs/2105.02055v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Diarization for Variable Number of Speakers with Local-Global\n  Networks and Discriminative Speaker Embeddings", "abstract": "We present an end-to-end deep network model that performs meeting diarization\nfrom single-channel audio recordings. End-to-end diarization models have the\nadvantage of handling speaker overlap and enabling straightforward handling of\ndiscriminative training, unlike traditional clustering-based diarization\nmethods. The proposed system is designed to handle meetings with unknown\nnumbers of speakers, using variable-number permutation-invariant cross-entropy\nbased loss functions. We introduce several components that appear to help with\ndiarization performance, including a local convolutional network followed by a\nglobal self-attention module, multi-task transfer learning using a speaker\nidentification component, and a sequential approach where the model is refined\nwith a second stage. These are trained and validated on simulated meeting data\nbased on LibriSpeech and LibriTTS datasets; final evaluations are done using\nLibriCSS, which consists of simulated meetings recorded using real acoustics\nvia loudspeaker playback. The proposed model performs better than previously\nproposed end-to-end diarization models on these data.", "published": "2021-05-05 14:55:29", "link": "http://arxiv.org/abs/2105.02096v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Learning from Automatically Separated Sound Scenes", "abstract": "Real-world sound scenes consist of time-varying collections of sound sources,\neach generating characteristic sound events that are mixed together in audio\nrecordings. The association of these constituent sound events with their\nmixture and each other is semantically constrained: the sound scene contains\nthe union of source classes and not all classes naturally co-occur. With this\nmotivation, this paper explores the use of unsupervised automatic sound\nseparation to decompose unlabeled sound scenes into multiple\nsemantically-linked views for use in self-supervised contrastive learning. We\nfind that learning to associate input mixtures with their automatically\nseparated outputs yields stronger representations than past approaches that use\nthe mixtures alone. Further, we discover that optimal source separation is not\nrequired for successful contrastive learning by demonstrating that a range of\nseparation system convergence states all lead to useful and often complementary\nexample transformations. Our best system incorporates these unsupervised\nseparation models into a single augmentation front-end and jointly optimizes\nsimilarity maximization and coincidence prediction objectives across the views.\nThe result is an unsupervised audio representation that rivals state-of-the-art\nalternatives on the established shallow AudioSet classification benchmark.", "published": "2021-05-05 15:37:17", "link": "http://arxiv.org/abs/2105.02132v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Retrieval with Natural Language Queries", "abstract": "We consider the task of retrieving audio using free-form natural language\nqueries. To study this problem, which has received limited attention in the\nexisting literature, we introduce challenging new benchmarks for text-based\naudio retrieval using text annotations sourced from the Audiocaps and Clotho\ndatasets. We then employ these benchmarks to establish baselines for\ncross-modal audio retrieval, where we demonstrate the benefits of pre-training\non diverse audio tasks. We hope that our benchmarks will inspire further\nresearch into cross-modal text-based audio retrieval with free-form text\nqueries.", "published": "2021-05-05 17:04:56", "link": "http://arxiv.org/abs/2105.02192v2", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Model reduction in acoustic inversion by artificial neural network", "abstract": "In ultrasound tomography, the speed of sound inside an object is estimated\nbased on acoustic measurements carried out by sensors surrounding the object.\nAn accurate forward model is a prominent factor for high-quality image\nreconstruction, but it can make computations far too time-consuming in many\napplications. Using approximate forward models, it is possible to speed up the\ncomputations, but the quality of the reconstruction may have to be compromised.\nIn this paper, a neural network -based approach is proposed, that can\ncompensate for modeling errors caused by the approximate forward models. The\napproach is tested with various different imaging scenarios in a simulated\ntwo-dimensional domain. The results show that with fairly small training\ndatasets, the proposed approach can be utilized to approximate the modelling\nerrors, and to significantly improve the image reconstruction quality in\nultrasound tomography, compared to commonly used inversion algorithms.", "published": "2021-05-05 19:14:02", "link": "http://arxiv.org/abs/2105.02225v2", "categories": ["eess.IV", "cs.SD", "eess.AS", "physics.comp-ph", "q-bio.QM"], "primary_category": "eess.IV"}
