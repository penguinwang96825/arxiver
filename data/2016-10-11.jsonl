{"title": "An Empirical Exploration of Skip Connections for Sequential Tagging", "abstract": "In this paper, we empirically explore the effects of various kinds of skip\nconnections in stacked bidirectional LSTMs for sequential tagging. We\ninvestigate three kinds of skip connections connecting to LSTM cells: (a) skip\nconnections to the gates, (b) skip connections to the internal states and (c)\nskip connections to the cell outputs. We present comprehensive experiments\nshowing that skip connections to cell outputs outperform the remaining two.\nFurthermore, we observe that using gated identity functions as skip mappings\nworks pretty well. Based on this novel skip connections, we successfully train\ndeep stacked bidirectional LSTM models and obtain state-of-the-art results on\nCCG supertagging and comparable results on POS tagging.", "published": "2016-10-11 03:02:38", "link": "http://arxiv.org/abs/1610.03167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward a new instances of NELL", "abstract": "We are developing the method to start new instances of NELL in various\nlanguages and develop then NELL multilingualism. We base our method on our\nexperience on NELL Portuguese and NELL French. This reports explain our method\nand develops some research perspectives.", "published": "2016-10-11 09:19:06", "link": "http://arxiv.org/abs/1610.03246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "abstract": "Recently, attempts have been made to remove Gaussian mixture models (GMM)\nfrom the training process of deep neural network-based hidden Markov models\n(HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two\nproblems, namely the initial alignment of the frame-level state labels and the\ncreation of context-dependent states. Although flat-start training via\niteratively realigning and retraining the DNN using a frame-level error\nfunction is viable, it is quite cumbersome. Here, we propose to use a\nsequence-discriminative training criterion for flat start. While\nsequence-discriminative training is routinely applied only in the final phase\nof model training, we show that with proper caution it is also suitable for\ngetting an alignment of context-independent DNN models. For the construction of\ntied states we apply a recently proposed KL-divergence-based state clustering\nmethod, hence our whole training process is GMM-free. In the experimental\nevaluation we found that the sequence-discriminative flat start training method\nis not only significantly faster than the straightforward approach of iterative\nretraining and realignment, but the word error rates attained are slightly\nbetter as well.", "published": "2016-10-11 09:52:57", "link": "http://arxiv.org/abs/1610.03256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keystroke dynamics as signal for shallow syntactic parsing", "abstract": "Keystroke dynamics have been extensively used in psycholinguistic and writing\nresearch to gain insights into cognitive processing. But do keystroke logs\ncontain actual signal that can be used to learn better natural language\nprocessing models?\n  We postulate that keystroke dynamics contain information about syntactic\nstructure that can inform shallow syntactic parsing. To test this hypothesis,\nwe explore labels derived from keystroke logs as auxiliary task in a multi-task\nbidirectional Long Short-Term Memory (bi-LSTM). Our results show promising\nresults on two shallow syntactic parsing tasks, chunking and CCG supertagging.\nOur model is simple, has the advantage that data can come from distinct\nsources, and produces models that are significantly better than models trained\non the text annotations alone.", "published": "2016-10-11 13:20:52", "link": "http://arxiv.org/abs/1610.03321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey on the Use of Typological Information in Natural Language\n  Processing", "abstract": "In recent years linguistic typology, which classifies the world's languages\naccording to their functional and structural properties, has been widely used\nto support multilingual NLP. While the growing importance of typological\ninformation in supporting multilingual tasks has been recognised, no systematic\nsurvey of existing typological resources and their use in NLP has been\npublished. This paper provides such a survey as well as discussion which we\nhope will both inform and inspire future work in the area.", "published": "2016-10-11 14:11:55", "link": "http://arxiv.org/abs/1610.03349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for\n  Large Vocabulary Speech Recognition", "abstract": "Long short-term memory (LSTM) recurrent neural networks (RNNs) have been\nshown to give state-of-the-art performance on many speech recognition tasks, as\nthey are able to provide the learned dynamically changing contextual window of\nall sequence history. On the other hand, the convolutional neural networks\n(CNNs) have brought significant improvements to deep feed-forward neural\nnetworks (FFNNs), as they are able to better reduce spectral variation in the\ninput signal. In this paper, a network architecture called as convolutional\nrecurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN.\nIn the proposed CRNNs, each speech frame, without adjacent context frames, is\norganized as a number of local feature patches along the frequency axis, and\nthen a LSTM network is performed on each feature patch along the time axis. We\ntrain and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various\nnumber of configurations. Experimental results show that the LSTM CRNNs can\nexceed state-of-the-art speech recognition performance.", "published": "2016-10-11 02:48:13", "link": "http://arxiv.org/abs/1610.03165v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "From phonemes to images: levels of representation in a recurrent neural\n  model of visually-grounded language learning", "abstract": "We present a model of visually-grounded language learning based on stacked\ngated recurrent neural networks which learns to predict visual features given\nan image description in the form of a sequence of phonemes. The learning task\nresembles that faced by human language learners who need to discover both\nstructure and meaning from noisy and ambiguous data across modalities. We show\nthat our model indeed learns to predict features of the visual context given\nphonetically transcribed image descriptions, and show that it represents\nlinguistic information in a hierarchy of levels: lower layers in the stack are\ncomparatively more sensitive to form, whereas higher layers are more sensitive\nto meaning.", "published": "2016-10-11 14:00:28", "link": "http://arxiv.org/abs/1610.03342v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigational Instruction Generation as Inverse Reinforcement Learning\n  with Neural Machine Translation", "abstract": "Modern robotics applications that involve human-robot interaction require\nrobots to be able to communicate with humans seamlessly and effectively.\nNatural language provides a flexible and efficient medium through which robots\ncan exchange information with their human partners. Significant advancements\nhave been made in developing robots capable of interpreting free-form\ninstructions, but less attention has been devoted to endowing robots with the\nability to generate natural language. We propose a navigational guide model\nthat enables robots to generate natural language instructions that allow humans\nto navigate a priori unknown environments. We first decide which information to\nshare with the user according to their preferences, using a policy trained from\nhuman demonstrations via inverse reinforcement learning. We then \"translate\"\nthis information into a natural language instruction using a neural\nsequence-to-sequence model that learns to generate free-form instructions from\nnatural language corpora. We evaluate our method on a benchmark route\ninstruction dataset and achieve a BLEU score of 72.18% when compared to\nhuman-generated reference instructions. We additionally conduct navigation\nexperiments with human participants that demonstrate that our method generates\ninstructions that people follow as accurately and easily as those produced by\nhumans.", "published": "2016-10-11 02:47:09", "link": "http://arxiv.org/abs/1610.03164v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
