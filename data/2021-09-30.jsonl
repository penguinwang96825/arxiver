{"title": "Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in\n  the McGill Pain Questionnaire", "abstract": "Modern medical diagnosis relies on precise pain assessment tools in\ntranslating clinical information from patient to physician. The McGill Pain\nQuestionnaire (MPQ) is a clinical pain assessment technique that utilizes 78\nadjectives of different intensities in 20 different categories to quantity a\npatient's pain. The questionnaire's efficacy depends on a predictable pattern\nof adjective use by patients experiencing pain. In this study, I recreate the\nMPQ's adjective intensity orderings using data gathered from patient forums and\nmodern NLP techniques. I extract adjective intensity relationships by searching\nfor key linguistic contexts, and then combine the relationship information to\nform robust adjective scales. Of 17 adjective relationships predicted by this\nresearch, only 4 diverge from the MPQ's orderings, which is statistically\nsignificant at the 0.1 alpha level. The results suggest predictable patterns of\nadjective use by people experiencing pain, but call into question the MPQ's\ncategories for grouping adjectives.", "published": "2021-09-30 01:20:55", "link": "http://arxiv.org/abs/2109.14788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phonetic Word Embeddings", "abstract": "This work presents a novel methodology for calculating the phonetic\nsimilarity between words taking motivation from the human perception of sounds.\nThis metric is employed to learn a continuous vector embedding space that\ngroups similar sounding words together and can be used for various downstream\ncomputational phonology tasks. The efficacy of the method is presented for two\ndifferent languages (English, Hindi) and performance gains over previous\nreported works are discussed on established tests for predicting phonetic\nsimilarity. To address limited benchmarking mechanisms in this field, we also\nintroduce a heterographic pun dataset based evaluation methodology to compare\nthe effectiveness of acoustic similarity algorithms. Further, a visualization\nof the embedding space is presented with a discussion on the various possible\nuse-cases of this novel algorithm. An open-source implementation is also shared\nto aid reproducibility and enable adoption in related tasks.", "published": "2021-09-30 01:46:01", "link": "http://arxiv.org/abs/2109.14796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 Fake News Detection Using Bidirectional Encoder Representations\n  from Transformers Based Models", "abstract": "Nowadays, the development of social media allows people to access the latest\nnews easily. During the COVID-19 pandemic, it is important for people to access\nthe news so that they can take corresponding protective measures. However, the\nfake news is flooding and is a serious issue especially under the global\npandemic. The misleading fake news can cause significant loss in terms of the\nindividuals and the society. COVID-19 fake news detection has become a novel\nand important task in the NLP field. However, fake news always contain the\ncorrect portion and the incorrect portion. This fact increases the difficulty\nof the classification task. In this paper, we fine tune the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model as our\nbase model. We add BiLSTM layers and CNN layers on the top of the finetuned\nBERT model with frozen parameters or not frozen parameters methods\nrespectively. The model performance evaluation results showcase that our best\nmodel (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves\nstate-of-the-art results towards COVID-19 fake news detection task. We also\nexplore keywords evaluation methods using our best model and evaluate the model\nperformance after removing keywords.", "published": "2021-09-30 02:50:05", "link": "http://arxiv.org/abs/2109.14816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by\n  Machine Translation Systems", "abstract": "In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.", "published": "2021-09-30 07:35:56", "link": "http://arxiv.org/abs/2109.14895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DICoE@FinSim-3: Financial Hypernym Detection using Augmented Terms and\n  Distance-based Features", "abstract": "We present the submission of team DICoE for FinSim-3, the 3rd Shared Task on\nLearning Semantic Similarities for the Financial Domain. The task provides a\nset of terms in the financial domain and requires to classify them into the\nmost relevant hypernym from a financial ontology. After augmenting the terms\nwith their Investopedia definitions, our system employs a Logistic Regression\nclassifier over financial word embeddings and a mix of hand-crafted and\ndistance-based features. Also, for the first time in this task, we employ\ndifferent replacement methods for out-of-vocabulary terms, leading to improved\nperformance. Finally, we have also experimented with word representations\ngenerated from various financial corpora. Our best-performing submission ranked\n4th on the task's leaderboard.", "published": "2021-09-30 08:01:48", "link": "http://arxiv.org/abs/2109.14906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT got a Date: Introducing Transformers to Temporal Tagging", "abstract": "Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nIn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. Based\non our study of token classification variants and encoder-decoder\narchitectures, we present a transformer encoder-decoder model using the RoBERTa\nlanguage model as our best performing system. By supplementing training\nresources with weakly labeled data from rule-based systems, our model surpasses\nprevious works in temporal tagging and type classification, especially on rare\nclasses. Our code and pre-trained experiments are available at:\nhttps://github.com/satya77/Transformer_Temporal_Tagger", "published": "2021-09-30 08:54:21", "link": "http://arxiv.org/abs/2109.14927v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prose2Poem: The Blessing of Transformers in Translating Prose to Persian\n  Poetry", "abstract": "Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.", "published": "2021-09-30 09:04:11", "link": "http://arxiv.org/abs/2109.14934v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Persistence in Language Models: Priming as a Window into\n  Abstract Language Representations", "abstract": "We investigate the extent to which modern, neural language models are\nsusceptible to structural priming, the phenomenon whereby the structure of a\nsentence makes the same structure more probable in a follow-up sentence. We\nexplore how priming can be used to study the potential of these models to learn\nabstract structural information, which is a prerequisite for good performance\non tasks that require natural language understanding skills. We introduce a\nnovel metric and release Prime-LM, a large corpus where we control for various\nlinguistic factors which interact with priming strength. We find that\nTransformer models indeed show evidence of structural priming, but also that\nthe generalisations they learned are to some extent modulated by semantic\ninformation. Our experiments also show that the representations acquired by the\nmodels may not only encode abstract sequential structure but involve certain\nlevel of hierarchical syntactic information. More generally, our study shows\nthat the priming paradigm is a useful, additional tool for gaining insights\ninto the capacities of language models and opens the door to future\npriming-based investigations that probe the model's internal states.", "published": "2021-09-30 10:38:38", "link": "http://arxiv.org/abs/2109.14989v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A surprisal--duration trade-off across and within the world's languages", "abstract": "While there exist scores of natural languages, each with its unique features\nand idiosyncrasies, they all share a unifying theme: enabling human\ncommunication. We may thus reasonably predict that human cognition shapes how\nthese languages evolve and are used. Assuming that the capacity to process\ninformation is roughly constant across human populations, we expect a\nsurprisal--duration trade-off to arise both across and within languages. We\nanalyse this trade-off using a corpus of 600 languages and, after controlling\nfor several potential confounds, we find strong supporting evidence in both\nsettings. Specifically, we find that, on average, phones are produced faster in\nlanguages where they are less surprising, and vice versa. Further, we confirm\nthat more surprising phones are longer, on average, in 319 languages out of the\n600. We thus conclude that there is strong evidence of a surprisal--duration\ntrade-off in operation, both across and within the world's languages.", "published": "2021-09-30 10:56:30", "link": "http://arxiv.org/abs/2109.15000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Post-training Quantization of Pre-trained Language\n  Models", "abstract": "Network quantization has gained increasing attention with the rapid growth of\nlarge pre-trained language models~(PLMs). However, most existing quantization\nmethods for PLMs follow quantization-aware training~(QAT) that requires\nend-to-end training with full access to the entire dataset. Therefore, they\nsuffer from slow training, large memory overhead, and data security issues. In\nthis paper, we study post-training quantization~(PTQ) of PLMs, and propose\nmodule-wise quantization error minimization~(MREM), an efficient solution to\nmitigate these issues. By partitioning the PLM into multiple modules, we\nminimize the reconstruction error incurred by quantization for each module. In\naddition, we design a new model parallel training strategy such that each\nmodule can be trained locally on separate computing devices without waiting for\npreceding modules, which brings nearly the theoretical training speed-up (e.g.,\n$4\\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our\nproposed PTQ solution not only performs close to QAT, but also enjoys\nsignificant reductions in training time, memory overhead, and data consumption.", "published": "2021-09-30 12:50:06", "link": "http://arxiv.org/abs/2109.15082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Key Point Analysis via Contrastive Learning and Extractive Argument\n  Summarization", "abstract": "Key point analysis is the task of extracting a set of concise and high-level\nstatements from a given collection of arguments, representing the gist of these\narguments. This paper presents our proposed approach to the Key Point Analysis\nshared task, collocated with the 8th Workshop on Argument Mining. The approach\nintegrates two complementary components. One component employs contrastive\nlearning via a siamese neural network for matching arguments to key points; the\nother is a graph-based extractive summarization model for generating key\npoints. In both automatic and manual evaluation, our approach was ranked best\namong all submissions to the shared task.", "published": "2021-09-30 12:54:26", "link": "http://arxiv.org/abs/2109.15086v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Sarcasm Detection Based on Contrastive Attention Mechanism", "abstract": "In the past decade, sarcasm detection has been intensively conducted in a\ntextual scenario. With the popularization of video communication, the analysis\nin multi-modal scenarios has received much attention in recent years.\nTherefore, multi-modal sarcasm detection, which aims at detecting sarcasm in\nvideo conversations, becomes increasingly hot in both the natural language\nprocessing community and the multi-modal analysis community. In this paper,\nconsidering that sarcasm is often conveyed through incongruity between\nmodalities (e.g., text expressing a compliment while acoustic tone indicating a\ngrumble), we construct a Contras-tive-Attention-based Sarcasm Detection\n(ConAttSD) model, which uses an inter-modality contrastive attention mechanism\nto extract several contrastive features for an utterance. A contrastive feature\nrepresents the incongruity of information between two modalities. Our\nexperiments on MUStARD, a benchmark multi-modal sarcasm dataset, demonstrate\nthe effectiveness of the proposed ConAttSD model.", "published": "2021-09-30 14:17:51", "link": "http://arxiv.org/abs/2109.15153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SlovakBERT: Slovak Masked Language Model", "abstract": "We introduce a new Slovak masked language model called SlovakBERT. This is to\nour best knowledge the first paper discussing Slovak transformers-based\nlanguage models. We evaluate our model on several NLP tasks and achieve\nstate-of-the-art results. This evaluation is likewise the first attempt to\nestablish a benchmark for Slovak language models. We publish the masked\nlanguage model, as well as the fine-tuned models for part-of-speech tagging,\nsentiment analysis and semantic textual similarity.", "published": "2021-09-30 16:36:49", "link": "http://arxiv.org/abs/2109.15254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-granular Legal Topic Classification on Greek Legislation", "abstract": "In this work, we study the task of classifying legal texts written in the\nGreek language. We introduce and make publicly available a novel dataset based\non Greek legislation, consisting of more than 47 thousand official, categorized\nGreek legislation resources. We experiment with this dataset and evaluate a\nbattery of advanced methods and classifiers, ranging from traditional machine\nlearning and RNN-based methods to state-of-the-art Transformer-based methods.\nWe show that recurrent architectures with domain-specific word embeddings offer\nimproved overall performance while being competitive even to transformer-based\nmodels. Finally, we show that cutting-edge multilingual and monolingual\ntransformer-based models brawl on the top of the classifiers' ranking, making\nus question the necessity of training monolingual transfer learning models as a\nrule of thumb. To the best of our knowledge, this is the first time the task of\nGreek legal text classification is considered in an open research project,\nwhile also Greek is a language with very limited NLP resources in general.", "published": "2021-09-30 17:43:00", "link": "http://arxiv.org/abs/2109.15298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Text Classification via Self-Pretraining", "abstract": "We present a neural semi-supervised learning model termed Self-Pretraining.\nOur model is inspired by the classic self-training algorithm. However, as\nopposed to self-training, Self-Pretraining is threshold-free, it can\npotentially update its belief about previously labeled documents, and can cope\nwith the semantic drift problem. Self-Pretraining is iterative and consists of\ntwo classifiers. In each iteration, one classifier draws a random set of\nunlabeled documents and labels them. This set is used to initialize the second\nclassifier, to be further trained by the set of labeled documents. The\nalgorithm proceeds to the next iteration and the classifiers' roles are\nreversed. To improve the flow of information across the iterations and also to\ncope with the semantic drift problem, Self-Pretraining employs an iterative\ndistillation process, transfers hypotheses across the iterations, utilizes a\ntwo-stage training model, uses an efficient learning rate schedule, and employs\na pseudo-label transformation heuristic. We have evaluated our model in three\npublicly available social media datasets. Our experiments show that\nSelf-Pretraining outperforms the existing state-of-the-art semi-supervised\nclassifiers across multiple settings. Our code is available at\nhttps://github.com/p-karisani/self_pretraining.", "published": "2021-09-30 17:45:16", "link": "http://arxiv.org/abs/2109.15300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variance of Twitter Embeddings and Temporal Trends of COVID-19 cases", "abstract": "The severity of the coronavirus pandemic necessitates the need of effective\nadministrative decisions. Over 4 lakh people in India succumbed to COVID-19,\nwith over 3 crore confirmed cases, and still counting. The threat of a\nplausible third wave continues to haunt millions. In this ever changing dynamic\nof the virus, predictive modeling methods can serve as an integral tool. The\npandemic has further triggered an unprecedented usage of social media. This\npaper aims to propose a method for harnessing social media, specifically\nTwitter, to predict the upcoming scenarios related to COVID-19 cases. In this\nstudy, we seek to understand how the surges in COVID-19 related tweets can\nindicate rise in the cases. This prospective analysis can be utilised to aid\nadministrators about timely resource allocation to lessen the severity of the\ndamage. Using word embeddings to capture the semantic meaning of tweets, we\nidentify Significant Dimensions (SDs).Our methodology predicts the rise in\ncases with a lead time of 15 days and 30 days with R2 scores of 0.80 and 0.62\nrespectively. Finally, we explain the thematic utility of the SDs.", "published": "2021-09-30 18:03:10", "link": "http://arxiv.org/abs/2110.00031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-conditioning pre-trained language models", "abstract": "In this paper we aim to investigate the mechanisms that guide text generation\nwith pre-trained Transformer-based Language Models (TLMs). Grounded on the\nProduct of Experts formulation by Hinton (1999), we describe a generative\nmechanism that exploits expert units which naturally exist in TLMs. Such units\nare responsible for detecting concepts in the input and conditioning text\ngeneration on such concepts. We describe how to identify expert units and how\nto activate them during inference in order to induce any desired concept in the\ngenerated output. We find that the activation of a surprisingly small amount of\nunits is sufficient to steer text generation (as little as 3 units in a model\nwith 345M parameters). While the objective of this work is to learn more about\nhow TLMs work, we show that our method is effective for conditioning without\nfine-tuning or using extra parameters, even on fine-grained homograph concepts.\nAdditionally, we show that our method can be used to correct gender bias\npresent in the output of TLMs and achieves gender parity for all evaluated\ncontexts. We compare our method with FUDGE and PPLM-BoW, and show that our\napproach is able to achieve gender parity at a lower perplexity. The proposed\nmethod is accessible to a wide audience thanks to its simplicity and minimal\ncompute needs. The findings in this paper are a step forward in understanding\nthe generative mechanisms of TLMs.", "published": "2021-09-30 11:18:19", "link": "http://arxiv.org/abs/2110.02802v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Neural Compression Via Concurrent Pruning and Self-Distillation", "abstract": "Pruning aims to reduce the number of parameters while maintaining performance\nclose to the original network. This work proposes a novel\n\\emph{self-distillation} based pruning strategy, whereby the representational\nsimilarity between the pruned and unpruned versions of the same network is\nmaximized. Unlike previous approaches that treat distillation and pruning\nseparately, we use distillation to inform the pruning criteria, without\nrequiring a separate student network as in knowledge distillation. We show that\nthe proposed {\\em cross-correlation objective for self-distilled pruning}\nimplicitly encourages sparse solutions, naturally complementing magnitude-based\npruning criteria. Experiments on the GLUE and XGLUE benchmarks show that\nself-distilled pruning increases mono- and cross-lingual language model\nperformance. Self-distilled pruned models also outperform smaller Transformers\nwith an equal number of parameters and are competitive against (6 times) larger\ndistilled networks. We also observe that self-distillation (1) maximizes class\nseparability, (2) increases the signal-to-noise ratio, and (3) converges faster\nafter pruning steps, providing further insights into why self-distilled pruning\nimproves generalization.", "published": "2021-09-30 11:08:30", "link": "http://arxiv.org/abs/2109.15014v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact\n  Verification Models", "abstract": "Fact verification datasets are typically constructed using crowdsourcing\ntechniques due to the lack of text sources with veracity labels. However, the\ncrowdsourcing process often produces undesired biases in data that cause models\nto learn spurious patterns. In this paper, we propose CrossAug, a contrastive\ndata augmentation method for debiasing fact verification models. Specifically,\nwe employ a two-stage augmentation pipeline to generate new claims and\nevidences from existing samples. The generated samples are then paired\ncross-wise with the original pair, forming contrastive samples that facilitate\nthe model to rely less on spurious patterns and learn more robust\nrepresentations. Experimental results show that our method outperforms the\nprevious state-of-the-art debiasing technique by 3.6% on the debiased extension\nof the FEVER dataset, with a total performance boost of 10.13% from the\nbaseline. Furthermore, we evaluate our approach in data-scarce settings, where\nmodels can be more susceptible to biases due to the lack of training data.\nExperimental results demonstrate that our approach is also effective at\ndebiasing in these low-resource conditions, exceeding the baseline performance\non the Symmetric dataset with just 1% of the original data.", "published": "2021-09-30 13:19:19", "link": "http://arxiv.org/abs/2109.15107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Review of Text Style Transfer using Deep Learning", "abstract": "Style is an integral component of a sentence indicated by the choice of words\na person makes. Different people have different ways of expressing themselves,\nhowever, they adjust their speaking and writing style to a social context, an\naudience, an interlocutor or the formality of an occasion. Text style transfer\nis defined as a task of adapting and/or changing the stylistic manner in which\na sentence is written, while preserving the meaning of the original sentence.\n  A systematic review of text style transfer methodologies using deep learning\nis presented in this paper. We point out the technological advances in deep\nneural networks that have been the driving force behind current successes in\nthe fields of natural language understanding and generation. The review is\nstructured around two key stages in the text style transfer process, namely,\nrepresentation learning and sentence generation in a new style. The discussion\nhighlights the commonalities and differences between proposed solutions as well\nas challenges and opportunities that are expected to direct and foster further\nresearch in the field.", "published": "2021-09-30 14:06:36", "link": "http://arxiv.org/abs/2109.15144v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual AMR Parsing with Noisy Knowledge Distillation", "abstract": "We study multilingual AMR parsing from the perspective of knowledge\ndistillation, where the aim is to learn and improve a multilingual AMR parser\nby using an existing English parser as its teacher. We constrain our\nexploration in a strict multilingual setting: there is but one model to parse\nall different languages including English. We identify that noisy input and\nprecise output are the key to successful distillation. Together with extensive\npre-training, we obtain an AMR parser whose performances surpass all previously\npublished results on four different foreign languages, including German,\nSpanish, Italian, and Chinese, by large margins (up to 18.8 \\textsc{Smatch}\npoints on Chinese and on average 11.3 \\textsc{Smatch} points). Our parser also\nachieves comparable performance on English to the latest state-of-the-art\nEnglish-only parser.", "published": "2021-09-30 15:13:48", "link": "http://arxiv.org/abs/2109.15196v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MatSciBERT: A Materials Domain Language Model for Text Mining and\n  Information Extraction", "abstract": "An overwhelmingly large amount of knowledge in the materials domain is\ngenerated and stored as text published in peer-reviewed scientific literature.\nRecent developments in natural language processing, such as bidirectional\nencoder representations from transformers (BERT) models, provide promising\ntools to extract information from these texts. However, direct application of\nthese models in the materials domain may yield suboptimal results as the models\nthemselves may not be trained on notations and jargon that are specific to the\ndomain. Here, we present a materials-aware language model, namely, MatSciBERT,\nwhich is trained on a large corpus of scientific literature published in the\nmaterials domain. We further evaluate the performance of MatSciBERT on three\ndownstream tasks, namely, abstract classification, named entity recognition,\nand relation extraction, on different materials datasets. We show that\nMatSciBERT outperforms SciBERT, a language model trained on science corpus, on\nall the tasks. Further, we discuss some of the applications of MatSciBERT in\nthe materials domain for extracting information, which can, in turn, contribute\nto materials discovery or optimization. Finally, to make the work accessible to\nthe larger materials community, we make the pretrained and finetuned weights\nand the models of MatSciBERT freely accessible.", "published": "2021-09-30 17:35:02", "link": "http://arxiv.org/abs/2109.15290v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Measuring Sentence-Level and Aspect-Level (Un)certainty in Science\n  Communications", "abstract": "Certainty and uncertainty are fundamental to science communication. Hedges\nhave widely been used as proxies for uncertainty. However, certainty is a\ncomplex construct, with authors expressing not only the degree but the type and\naspects of uncertainty in order to give the reader a certain impression of what\nis known. Here, we introduce a new study of certainty that models both the\nlevel and the aspects of certainty in scientific findings. Using a new dataset\nof 2167 annotated scientific findings, we demonstrate that hedges alone account\nfor only a partial explanation of certainty. We show that both the overall\ncertainty and individual aspects can be predicted with pre-trained language\nmodels, providing a more complete picture of the author's intended\ncommunication. Downstream analyses on 431K scientific findings from news and\nscientific abstracts demonstrate that modeling sentence-level and aspect-level\ncertainty is meaningful for areas like science communication. Both the model\nand datasets used in this paper are released at\nhttps://blablablab.si.umich.edu/projects/certainty/.", "published": "2021-09-30 00:50:51", "link": "http://arxiv.org/abs/2109.14776v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Process discovery on deviant traces and other stranger things", "abstract": "As the need to understand and formalise business processes into a model has\ngrown over the last years, the process discovery research field has gained more\nand more importance, developing two different classes of approaches to model\nrepresentation: procedural and declarative. Orthogonally to this\nclassification, the vast majority of works envisage the discovery task as a\none-class supervised learning process guided by the traces that are recorded\ninto an input log. In this work instead, we focus on declarative processes and\nembrace the less-popular view of process discovery as a binary supervised\nlearning task, where the input log reports both examples of the normal system\nexecution, and traces representing \"stranger\" behaviours according to the\ndomain semantics. We therefore deepen how the valuable information brought by\nboth these two sets can be extracted and formalised into a model that is\n\"optimal\" according to user-defined goals. Our approach, namely NegDis, is\nevaluated w.r.t. other relevant works in this field, and shows promising\nresults as regards both the performance and the quality of the obtained\nsolution.", "published": "2021-09-30 06:58:34", "link": "http://arxiv.org/abs/2109.14883v1", "categories": ["cs.LG", "cs.CL", "cs.LO"], "primary_category": "cs.LG"}
{"title": "Compositional generalization in semantic parsing with pretrained\n  transformers", "abstract": "Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.", "published": "2021-09-30 13:06:29", "link": "http://arxiv.org/abs/2109.15101v3", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Focused Contrastive Training for Test-based Constituency Analysis", "abstract": "We propose a scheme for self-training of grammaticality models for\nconstituency analysis based on linguistic tests. A pre-trained language model\nis fine-tuned by contrastive estimation of grammatical sentences from a corpus,\nand ungrammatical sentences that were perturbed by a syntactic test, a\ntransformation that is motivated by constituency theory. We show that\nconsistent gains can be achieved if only certain positive instances are chosen\nfor training, depending on whether they could be the result of a test\ntransformation. This way, the positives, and negatives exhibit similar\ncharacteristics, which makes the objective more challenging for the language\nmodel, and also allows for additional markup that indicates the position of the\ntest application within the sentence.", "published": "2021-09-30 14:22:15", "link": "http://arxiv.org/abs/2109.15159v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language\n  Navigation in Continuous Environments", "abstract": "In the Vision-and-Language Navigation (VLN) task an embodied agent navigates\na 3D environment, following natural language instructions. A challenge in this\ntask is how to handle 'off the path' scenarios where an agent veers from a\nreference path. Prior work supervises the agent with actions based on the\nshortest path from the agent's location to the goal, but such goal-oriented\nsupervision is often not in alignment with the instruction. Furthermore, the\nevaluation metrics employed by prior work do not measure how much of a language\ninstruction the agent is able to follow. In this work, we propose a simple and\neffective language-aligned supervision scheme, and a new metric that measures\nthe number of sub-instructions the agent has completed during navigation.", "published": "2021-09-30 15:28:24", "link": "http://arxiv.org/abs/2109.15207v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "A formal model for ledger management systems based on contracts and\n  temporal logic", "abstract": "A key component of blockchain technology is the ledger, viz., a database\nthat, unlike standard databases, keeps in memory the complete history of past\ntransactions as in a notarial archive for the benefit of any future test. In\nsecond-generation blockchains such as Ethereum the ledger is coupled with smart\ncontracts, which enable the automation of transactions associated with\nagreements between the parties of a financial or commercial nature. The\ncoupling of smart contracts and ledgers provides the technological background\nfor very innovative application areas, such as Decentralized Autonomous\nOrganizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance\n(DeFi), which propelled blockchains beyond cryptocurrencies that were the only\nfocus of first generation blockchains such as the Bitcoin. However, the\ncurrently used implementation of smart contracts as arbitrary programming\nconstructs has made them susceptible to dangerous bugs that can be exploited\nmaliciously and has moved their semantics away from that of legal contracts. We\npropose here to recompose the split and recover the reliability of databases by\nformalizing a notion of contract modelled as a finite-state automaton with\nwell-defined computational characteristics derived from an encoding in terms of\nallocations of resources to actors, as an alternative to the approach based on\nprogramming. To complete the work, we use temporal logic as the basis for an\nabstract query language that is effectively suited to the historical nature of\nthe information kept in the ledger.", "published": "2021-09-30 15:34:28", "link": "http://arxiv.org/abs/2109.15212v1", "categories": ["cs.CR", "cs.CL", "cs.LO", "F.1.1; F.4.1; H.2.1"], "primary_category": "cs.CR"}
{"title": "Inducing Transformer's Compositional Generalization Ability via\n  Auxiliary Sequence Prediction Tasks", "abstract": "Systematic compositionality is an essential mechanism in human language,\nallowing the recombination of known parts to create novel expressions. However,\nexisting neural models have been shown to lack this basic ability in learning\nsymbolic structures. Motivated by the failure of a Transformer model on the\nSCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing\na command into actions, we propose two auxiliary sequence prediction tasks that\ntrack the progress of function and argument semantics, as additional training\nsupervision. These automatically-generated sequences are more representative of\nthe underlying compositional symbolic structures of the input data. During\ninference, the model jointly predicts the next action and the next tokens in\nthe auxiliary sequences at each step. Experiments on the SCAN dataset show that\nour method encourages the Transformer to understand compositional structures of\nthe command, improving its accuracy on multiple challenging splits from <= 10%\nto 100%. With only 418 (5%) training instances, our approach still achieves\n97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can\nbe induced in Transformers given minimal but proper guidance. We also show that\na better result is achieved using less contextualized vectors as the\nattention's query, providing insights into architecture choices in achieving\nsystematic compositionality. Finally, we show positive generalization results\non the groundedSCAN task (Ruis et al., 2020). Our code is publicly available\nat: https://github.com/jiangycTarheel/compositional-auxseq", "published": "2021-09-30 16:41:19", "link": "http://arxiv.org/abs/2109.15256v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "#ContextMatters: Advantages and Limitations of Using Machine Learning to\n  Support Women in Politics", "abstract": "The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.", "published": "2021-09-30 22:55:49", "link": "http://arxiv.org/abs/2110.00116v2", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "USEV: Universal Speaker Extraction with Visual Cue", "abstract": "A speaker extraction algorithm seeks to extract the target speaker's speech\nfrom a multi-talker speech mixture. The prior studies focus mostly on speaker\nextraction from a highly overlapped multi-talker speech mixture. However, the\ntarget-interference speaker overlapping ratios could vary over a wide range\nfrom 0% to 100% in natural speech communication, furthermore, the target\nspeaker could be absent in the speech mixture, the speech mixtures in such\nuniversal multi-talker scenarios are described as general speech mixtures. The\nspeaker extraction algorithm requires an auxiliary reference, such as a video\nrecording or a pre-recorded speech, to form top-down auditory attention on the\ntarget speaker. We advocate that a visual cue, i.e., lip movement, is more\ninformative than an audio cue, i.e., pre-recorded speech, to serve as the\nauxiliary reference for speaker extraction in disentangling the target speaker\nfrom a general speech mixture. In this paper, we propose a universal speaker\nextraction network with a visual cue, that works for all multi-talker\nscenarios. In addition, we propose a scenario-aware differentiated loss\nfunction for network training, to balance the network performance over\ndifferent target-interference speaker pairing scenarios. The experimental\nresults show that our proposed method outperforms various competitive baselines\nfor general speech mixtures in terms of signal fidelity.", "published": "2021-09-30 03:37:10", "link": "http://arxiv.org/abs/2109.14831v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Impact of Channel Variation on One-Class Learning for Spoof Detection", "abstract": "Margin-based losses, especially one-class classification loss, have improved\nthe generalization capabilities of countermeasure systems (CMs), but their\nreliability is not tested with spoofing attacks degraded with channel\nvariation. Our experiments aim to tackle this in two ways: first, by\ninvestigating the impact of various codec simulations and their corresponding\nparameters, namely bit-rate, discontinuous transmission (DTX), and loss, on the\nperformance of the one-class classification-based CM system; second, by testing\nthe efficacy of the various settings of margin-based losses for training and\nevaluating our CM system on codec simulated data. Multi-conditional training\n(MCT) along with various data-feeding and custom mini-batching strategies were\nalso explored to handle the added variability in the new data setting and to\nfind an optimal setting to carry out the above experiments. Our experimental\nresults reveal that a strict restrain over the embedding space degrades the\nperformance of the one-class classification model. MCT relatively improves\nperformance by 35.55\\%, and custom mini-batching captures more generalized\nfeatures for the new data setting. Whereas varying the codec parameters made a\nsignificant impact on the performance of the countermeasure system.", "published": "2021-09-30 07:56:16", "link": "http://arxiv.org/abs/2109.14900v3", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "An investigation of pre-upsampling generative modelling and Generative\n  Adversarial Networks in audio super resolution", "abstract": "There have been several successful deep learning models that perform audio\nsuper-resolution. Many of these approaches involve using preprocessed feature\nextraction which requires a lot of domain-specific signal processing knowledge\nto implement. Convolutional Neural Networks (CNNs) improved upon this framework\nby automatically learning filters. An example of a convolutional approach is\nAudioUNet, which takes inspiration from novel methods of upsampling images. Our\npaper compares the pre-upsampling AudioUNet to a new generative model that\nupsamples the signal before using deep learning to transform it into a more\nbelievable signal. Based on the EDSR network for image super-resolution, the\nnewly proposed model outperforms UNet with a 20% increase in log spectral\ndistance and a mean opinion score of 4.06 compared to 3.82 for the two times\nupsampling case. AudioEDSR also has 87% fewer parameters than AudioUNet. How\nincorporating AudioUNet into a Wasserstein GAN (with gradient penalty)\n(WGAN-GP) structure can affect training is also explored. Finally the effects\nartifacting has on the current state of the art is analysed and solutions to\nthis problem are proposed. The methods used in this paper have broad\napplications to telephony, audio recognition and audio generation tasks.", "published": "2021-09-30 10:46:23", "link": "http://arxiv.org/abs/2109.14994v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Federated Learning in ASR: Not as Easy as You Think", "abstract": "With the growing availability of smart devices and cloud services, personal\nspeech assistance systems are increasingly used on a daily basis. Most devices\nredirect the voice recordings to a central server, which uses them for\nupgrading the recognizer model. This leads to major privacy concerns, since\nprivate data could be misused by the server or third parties. Federated\nlearning is a decentralized optimization strategy that has been proposed to\naddress such concerns. Utilizing this approach, private data is used for\non-device training. Afterwards, updated model parameters are sent to the server\nto improve the global model, which is redistributed to the clients. In this\nwork, we implement federated learning for speech recognition in a hybrid and an\nend-to-end model. We discuss the outcomes of these systems, which both show\ngreat similarities and only small improvements, pointing to a need for a deeper\nunderstanding of federated learning for speech recognition.", "published": "2021-09-30 13:21:10", "link": "http://arxiv.org/abs/2109.15108v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech", "abstract": "Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and\nGlow-TTS can synthesize high-quality speech from the given text in parallel.\nAfter analyzing two kinds of generative NAR-TTS models (VAE and normalizing\nflow), we find that: VAE is good at capturing the long-range semantics features\n(e.g., prosody) even with small model size but suffers from blurry and\nunnatural results; and normalizing flow is good at reconstructing the frequency\nbin-wise details but performs poorly when the number of model parameters is\nlimited. Inspired by these observations, to generate diverse speech with\nnatural details and rich prosody using a lightweight architecture, we propose\nPortaSpeech, a portable and high-quality generative text-to-speech model.\nSpecifically, 1) to model both the prosody and mel-spectrogram details\naccurately, we adopt a lightweight VAE with an enhanced prior followed by a\nflow-based post-net with strong conditional inputs as the main architecture. 2)\nTo further compress the model size and memory footprint, we introduce the\ngrouped parameter sharing mechanism to the affine coupling layers in the\npost-net. 3) To improve the expressiveness of synthesized speech and reduce the\ndependency on accurate fine-grained alignment between text and speech, we\npropose a linguistic encoder with mixture alignment combining hard inter-word\nalignment and soft intra-word alignment, which explicitly extracts word-level\nsemantic information. Experimental results show that PortaSpeech outperforms\nother TTS models in both voice quality and prosody modeling in terms of\nsubjective and objective evaluation metrics, and shows only a slight\nperformance degradation when reducing the model parameters to 6.7M (about 4x\nmodel size and 3x runtime memory compression ratio compared with FastSpeech 2).\nOur extensive ablation studies demonstrate that each design in PortaSpeech is\neffective.", "published": "2021-09-30 14:35:47", "link": "http://arxiv.org/abs/2109.15166v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emergency Vehicles Audio Detection and Localization in Autonomous\n  Driving", "abstract": "Emergency vehicles in service have right-of-way over all other vehicles.\nHence, all other vehicles are supposed to take proper actions to yield\nemergency vehicles with active sirens. As this task requires the cooperation\nbetween ears and eyes for human drivers, it also needs audio detection as a\nsupplement to vision-based algorithms for fully autonomous driving vehicles. In\nurban driving scenarios, we need to know both the existence of emergency\nvehicles and their relative positions to us to decide the proper actions. We\npresent a novel system from collecting the real-world siren data to the\ndeployment of models using only two cost-efficient microphones. We are able to\nachieve promising performance for each task separately, especially within the\ncrucial 10m to 50m distance range to react (the size of our ego vehicle is\naround 5m in length and 2m in width). The recall rate to determine the\nexistence of sirens is 99.16% , the median and mean angle absolute error is\n9.64{\\deg} and 19.18{\\deg} respectively, and the median and mean distance\nabsolute error of 9.30m and 10.58m respectively within that range. We also\nbenchmark various machine learning approaches that can determine the siren\nexistence and sound source localization which includes direction and distance\nsimultaneously within 50ms of latency.", "published": "2021-09-30 01:54:44", "link": "http://arxiv.org/abs/2109.14797v2", "categories": ["cs.SD", "cs.AI", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Xenakis: Experimenting with Data, Cities, and Sounds", "abstract": "In this work, we report on the results and lessons learned from different\ndisciplines while researching the loosely-defined problem of hearing a city. We\npresent Xenakis, a tool for the musification of urban data, which is able to\ncapture some features of a city's topology through the distribution of street\norientations, and turn it into a (very) small piece of music, a loop, which can\nbe used as building block for compositions. Besides providing complementary\nvisual and auditory channels to interface with this data, we also allow the\npiping of \\textit{midi} signals to other applications. This concept was\ndeveloped by visualization researchers collaborating with musicians using\ndesign study methodologies in an open-ended way. Our results include musical\ntracks, and we take advantage of the scope of alt.VIS to communicate our\nresearch in a sincere, humorous, and engaging format.", "published": "2021-09-30 10:41:51", "link": "http://arxiv.org/abs/2109.14992v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Fine-tuning wav2vec2 for speaker recognition", "abstract": "This paper explores applying the wav2vec2 framework to speaker recognition\ninstead of speech recognition. We study the effectiveness of the pre-trained\nweights on the speaker recognition task, and how to pool the wav2vec2 output\nsequence into a fixed-length speaker embedding. To adapt the framework to\nspeaker recognition, we propose a single-utterance classification variant with\nCE or AAM softmax loss, and an utterance-pair classification variant with BCE\nloss. Our best performing variant, w2v2-aam, achieves a 1.88% EER on the\nextended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline.\nCode is available at https://github.com/nikvaessen/w2v2-speaker.", "published": "2021-09-30 12:16:47", "link": "http://arxiv.org/abs/2109.15053v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessing Algorithmic Biases for Musical Version Identification", "abstract": "Version identification (VI) systems now offer accurate and scalable solutions\nfor detecting different renditions of a musical composition, allowing the use\nof these systems in industrial applications and throughout the wider music\necosystem. Such use can have an important impact on various stakeholders\nregarding recognition and financial benefits, including how royalties are\ncirculated for digital rights management. In this work, we take a step toward\nacknowledging this impact and consider VI systems as socio-technical systems\nrather than isolated technologies. We propose a framework for quantifying\nperformance disparities across 5 systems and 6 relevant side attributes:\ngender, popularity, country, language, year, and prevalence. We also consider 3\nmain stakeholders for this particular information retrieval use case: the\nperforming artists of query tracks, those of reference (original) tracks, and\nthe composers. By categorizing the recordings in our dataset using such\nattributes and stakeholders, we analyze whether the considered VI systems show\nany implicit biases. We find signs of disparities in identification performance\nfor most of the groups we include in our analyses. Moreover, we also find that\nlearning- and rule-based systems behave differently for some attributes, which\nsuggests an additional dimension to consider along with accuracy and\nscalability when evaluating VI systems. Lastly, we share our dataset with\nattribute annotations to encourage VI researchers to take these aspects into\naccount while building new systems.", "published": "2021-09-30 15:04:02", "link": "http://arxiv.org/abs/2109.15188v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpliceOut: A Simple and Efficient Audio Augmentation Method", "abstract": "Time masking has become a de facto augmentation technique for speech and\naudio tasks, including automatic speech recognition (ASR) and audio\nclassification, most notably as a part of SpecAugment. In this work, we propose\nSpliceOut, a simple modification to time masking which makes it computationally\nmore efficient. SpliceOut performs comparably to (and sometimes outperforms)\nSpecAugment on a wide variety of speech and audio tasks, including ASR for\nseven different languages using varying amounts of training data, as well as on\nspeech translation, sound and music classification, thus establishing itself as\na broadly applicable audio augmentation method. SpliceOut also provides\nadditional gains when used in conjunction with other augmentation techniques.\nApart from the fully-supervised setting, we also demonstrate that SpliceOut can\ncomplement unsupervised representation learning with performance gains in the\nsemi-supervised and self-supervised settings.", "published": "2021-09-30 18:55:04", "link": "http://arxiv.org/abs/2110.00046v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Evaluation of Oratory Skills", "abstract": "What makes a talk successful? Is it the content or the presentation? We try\nto estimate the contribution of the speaker's oratory skills to the talk's\nsuccess, while ignoring the content of the talk. By oratory skills we refer to\nfacial expressions, motions and gestures, as well as the vocal features. We use\nTED Talks as our dataset, and measure the success of each talk by its view\ncount. Using this dataset we train a neural network to assess the oratory\nskills in a talk through three factors: body pose, facial expressions, and\nacoustic features.\n  Most previous work on automatic evaluation of oratory skills uses\nhand-crafted expert annotations for both the quality of the talk and for the\nidentification of predefined actions. Unlike prior art, we measure the quality\nto be equivalent to the view count of the talk as counted by TED, and allow the\nnetwork to automatically learn the actions, expressions, and sounds that are\nrelevant to the success of a talk. We find that oratory skills alone contribute\nsubstantially to the chances of a talk being successful.", "published": "2021-09-30 11:38:19", "link": "http://arxiv.org/abs/2110.01367v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
