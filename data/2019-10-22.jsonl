{"title": "Fine-Tuned Neural Models for Propaganda Detection at the Sentence and\n  Fragment levels", "abstract": "This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on\nFineGrained Propaganda Detection. Our system finished 5th out of 26 teams on\nthe sentence-level classification task and 5th out of 11 teams on the\nfragment-level classification task based on our scores on the blind test set.\nWe present our models, a discussion of our ablation studies and experiments,\nand an analysis of our performance on all eighteen propaganda techniques\npresent in the corpus of the shared task.", "published": "2019-10-22 00:06:52", "link": "http://arxiv.org/abs/1910.09702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Gender, Neo-Whorfianism, and Word Embeddings: A Data-Driven\n  Approach to Linguistic Relativity", "abstract": "The relation between language and thought has occupied linguists for at least\na century. Neo-Whorfianism, a weak version of the controversial Sapir-Whorf\nhypothesis, holds that our thoughts are subtly influenced by the grammatical\nstructures of our native language. One area of investigation in this vein\nfocuses on how the grammatical gender of nouns affects the way we perceive the\ncorresponding objects. For instance, does the fact that key is masculine in\nGerman (der Schl\\\"ussel), but feminine in Spanish (la llave) change the\nspeakers' views of those objects? Psycholinguistic evidence presented by\nBoroditsky et al. (2003, {\\S}4) suggested the answer might be yes: When asked\nto produce adjectives that best described a key, German and Spanish speakers\nnamed more stereotypically masculine and feminine ones, respectively. However,\nrecent attempts to replicate those experiments have failed (Mickan et al.,\n2014). In this work, we offer a computational analogue of Boroditsky et al.\n(2003, {\\S}4)'s experimental design on 9 languages, finding evidence against\nneo-Whorfianism.", "published": "2019-10-22 02:16:47", "link": "http://arxiv.org/abs/1910.09729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MRQA 2019 Shared Task: Evaluating Generalization in Reading\n  Comprehension", "abstract": "We present the results of the Machine Reading for Question Answering (MRQA)\n2019 shared task on evaluating the generalization capabilities of reading\ncomprehension systems. In this task, we adapted and unified 18 distinct\nquestion answering datasets into the same format. Among them, six datasets were\nmade available for training, six datasets were made available for development,\nand the final six were hidden for final evaluation. Ten teams submitted\nsystems, which explored various ideas including data sampling, multi-task\nlearning, adversarial training and ensembling. The best system achieved an\naverage F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points\nhigher than our initial baseline based on BERT.", "published": "2019-10-22 03:41:33", "link": "http://arxiv.org/abs/1910.09753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Fact Verification with Kernel Graph Attention Network", "abstract": "Fact Verification requires fine-grained natural language inference capability\nthat finds subtle clues to identify the syntactical and semantically correct\nbut not well-supported claims. This paper presents Kernel Graph Attention\nNetwork (KGAT), which conducts more fine-grained fact verification with\nkernel-based attentions. Given a claim and a set of potential evidence\nsentences that form an evidence graph, KGAT introduces node kernels, which\nbetter measure the importance of the evidence node, and edge kernels, which\nconduct fine-grained evidence propagation in the graph, into Graph Attention\nNetworks for more accurate fact verification. KGAT achieves a 70.38% FEVER\nscore and significantly outperforms existing fact verification models on FEVER,\na large-scale benchmark for fact verification. Our analyses illustrate that,\ncompared to dot-product attentions, the kernel-based attention concentrates\nmore on relevant evidence sentences and meaningful clues in the evidence graph,\nwhich is the main source of KGAT's effectiveness.", "published": "2019-10-22 07:06:45", "link": "http://arxiv.org/abs/1910.09796v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Personality from Text: Challenges and\n  Opportunities", "abstract": "In this study, we examined the possibility to extract personality traits from\na text. We created an extensive dataset by having experts annotate personality\ntraits in a large number of texts from multiple online sources. From these\nannotated texts, we selected a sample and made further annotations ending up in\na large low-reliability dataset and a small high-reliability dataset. We then\nused the two datasets to train and test several machine learning models to\nextract personality from text, including a language model. Finally, we\nevaluated our best models in the wild, on datasets from different domains. Our\nresults show that the models based on the small high-reliability dataset\nperformed better (in terms of $\\textrm{R}^2$) than models based on large\nlow-reliability dataset. Also, language model based on small high-reliability\ndataset performed better than the random baseline. Finally, and more\nimportantly, the results showed our best model did not perform better than the\nrandom baseline when tested in the wild. Taken together, our results show that\ndetermining personality traits from a text remains a challenge and that no firm\nconclusions can be made on model performance before testing in the wild.", "published": "2019-10-22 12:16:00", "link": "http://arxiv.org/abs/1910.09916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Neural Dialogue State Tracking", "abstract": "A Dialogue State Tracker (DST) is a key component in a dialogue system aiming\nat estimating the beliefs of possible user goals at each dialogue turn. Most of\nthe current DST trackers make use of recurrent neural networks and are based on\ncomplex architectures that manage several aspects of a dialogue, including the\nuser utterance, the system actions, and the slot-value pairs defined in a\ndomain ontology. However, the complexity of such neural architectures incurs\ninto a considerable latency in the dialogue state prediction, which limits the\ndeployments of the models in real-world applications, particularly when task\nscalability (i.e. amount of slots) is a crucial factor. In this paper, we\npropose an innovative neural model for dialogue state tracking, named Global\nencoder and Slot-Attentive decoders (G-SAT), which can predict the dialogue\nstate with a very low latency time, while maintaining high-level performance.\nWe report experiments on three different languages (English, Italian, and\nGerman) of the WoZ2.0 dataset, and show that the proposed approach provides\ncompetitive advantages over state-of-art DST systems, both in terms of accuracy\nand in terms of time complexity for predictions, being over 15 times faster\nthan the other systems.", "published": "2019-10-22 13:01:00", "link": "http://arxiv.org/abs/1910.09942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Decompositional Semantic Parsing", "abstract": "We introduce a transductive model for parsing into Universal Decompositional\nSemantics (UDS) representations, which jointly learns to map natural language\nutterances into UDS graph structures and annotate the graph with\ndecompositional semantic attribute scores. We also introduce a strong pipeline\nmodel for parsing into the UDS graph structure, and show that our transductive\nparser performs comparably while additionally performing attribute prediction.\nBy analyzing the attribute prediction errors, we find the model captures\nnatural relationships between attribute groups.", "published": "2019-10-22 17:41:48", "link": "http://arxiv.org/abs/1910.10138v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Greater Context for Question Generation", "abstract": "Automatic question generation can benefit many applications ranging from\ndialogue systems to reading comprehension. While questions are often asked with\nrespect to long documents, there are many challenges with modeling such long\ndocuments. Many existing techniques generate questions by effectively looking\nat one sentence at a time, leading to questions that are easy and not\nreflective of the human process of question generation. Our goal is to\nincorporate interactions across multiple sentences to generate realistic\nquestions for long documents. In order to link a broad document context to the\ntarget answer, we represent the relevant context via a multi-stage attention\nmechanism, which forms the foundation of a sequence to sequence model. We\noutperform state-of-the-art methods on question generation on three\nquestion-answering datasets -- SQuAD, MS MARCO and NewsQA.", "published": "2019-10-22 23:19:04", "link": "http://arxiv.org/abs/1910.10274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Search-based Neural Model for Biomedical Nested and Overlapping Event\n  Detection", "abstract": "We tackle the nested and overlapping event detection task and propose a novel\nsearch-based neural network (SBNN) structured prediction model that treats the\ntask as a search problem on a relation graph of trigger-argument structures.\nUnlike existing structured prediction tasks such as dependency parsing, the\ntask targets to detect DAG structures, which constitute events, from the\nrelation graph. We define actions to construct events and use all the beams in\na beam search to detect all event structures that may be overlapping and\nnested. The search process constructs events in a bottom-up manner while\nmodelling the global properties for nested and overlapping structures\nsimultaneously using neural networks. We show that the model achieves\nperformance comparable to the state-of-the-art model Turku Event Extraction\nSystem (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the\nuse of any syntactic and hand-engineered features. Further analyses on the\ndevelopment set show that our model is more computationally efficient while\nyielding higher F1-score performance.", "published": "2019-10-22 23:41:53", "link": "http://arxiv.org/abs/1910.10281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering over Knowledge Graphs via Structural Query Patterns", "abstract": "Natural language question answering over knowledge graphs is an important and\ninteresting task as it enables common users to gain accurate answers in an easy\nand intuitive manner. However, it remains a challenge to bridge the gap between\nunstructured questions and structured knowledge graphs. To address the problem,\na natural discipline is building a structured query to represent the input\nquestion. Searching the structured query over the knowledge graph can produce\nanswers to the question. Distinct from the existing methods that are based on\nsemantic parsing or templates, we propose an effective approach powered by a\nnovel notion, structural query pattern, in this paper. Given an input question,\nwe first generate its query sketch that is compatible with the underlying\nstructure of the knowledge graph. Then, we complete the query graph by labeling\nthe nodes and edges under the guidance of the structural query pattern.\nFinally, answers can be retrieved by executing the constructed query graph over\nthe knowledge graph. Evaluations on three question answering benchmarks show\nthat our proposed approach outperforms state-of-the-art methods significantly.", "published": "2019-10-22 04:21:06", "link": "http://arxiv.org/abs/1910.09760v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Transformer-based Acoustic Modeling for Hybrid Speech Recognition", "abstract": "We propose and evaluate transformer-based acoustic models (AMs) for hybrid\nspeech recognition. Several modeling choices are discussed in this work,\nincluding various positional embedding methods and an iterated loss to enable\ntraining deep transformers. We also present a preliminary study of using\nlimited right context in transformer models, which makes it possible for\nstreaming applications. We demonstrate that on the widely used Librispeech\nbenchmark, our transformer-based AM outperforms the best published hybrid\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\nused. Combined with neural network LM for rescoring, our proposed approach\nachieves state-of-the-art results on Librispeech. Our findings are also\nconfirmed on a much larger internal dataset.", "published": "2019-10-22 07:20:02", "link": "http://arxiv.org/abs/1910.09799v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Combinational Relation Linking over Knowledge Graphs", "abstract": "Given a natural language phrase, relation linking aims to find a relation\n(predicate or property) from the underlying knowledge graph to match the\nphrase. It is very useful in many applications, such as natural language\nquestion answering, personalized recommendation and text summarization.\nHowever, the previous relation linking algorithms usually produce a single\nrelation for the input phrase and pay little attention to a more general and\nchallenging problem, i.e., combinational relation linking that extracts a\nsubgraph pattern to match the compound phrase (e.g. mother-in-law). In this\npaper, we focus on the task of combinational relation linking over knowledge\ngraphs. To resolve the problem, we design a systematic method based on the\ndata-driven relation assembly technique, which is performed under the guidance\nof meta patterns. We also introduce external knowledge to enhance the system\nunderstanding ability. Finally, we conduct extensive experiments over the real\nknowledge graph to study the performance of the proposed method.", "published": "2019-10-22 10:35:41", "link": "http://arxiv.org/abs/1910.09879v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "GPU-Accelerated Viterbi Exact Lattice Decoder for Batched Online and\n  Offline Speech Recognition", "abstract": "We present an optimized weighted finite-state transducer (WFST) decoder\ncapable of online streaming and offline batch processing of audio using\nGraphics Processing Units (GPUs). The decoder is efficient in memory\nutilization, input/output (I/O) bandwidth, and uses a novel Viterbi\nimplementation designed to maximize parallelism. The reduced memory footprint\nallows the decoder to process significantly larger graphs than previously\npossible, while optimizing I/O increases the number of simultaneous streams\nsupported. GPU preprocessing of lattice segments enables intermediate lattice\nresults to be returned to the requestor during streaming inference.\nCollectively, the proposed algorithm yields up to a 240x speedup over single\ncore CPU decoding, and up to 40x faster decoding than the current\nstate-of-the-art GPU decoder, while returning equivalent results. This decoder\ndesign enables deployment of production-grade ASR models on a large spectrum of\nsystems, ranging from large data center servers to low-power edge devices.", "published": "2019-10-22 15:08:49", "link": "http://arxiv.org/abs/1910.10032v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One-Shot Template Matching for Automatic Document Data Capture", "abstract": "In this paper, we propose a novel one-shot template-matching algorithm to\nautomatically capture data from business documents with an aim to minimize\nmanual data entry. Given one annotated document, our algorithm can\nautomatically extract similar data from other documents having the same format.\nBased on a set of engineered visual and textual features, our method is\ninvariant to changes in position and value. Experiments on a dataset of 595\nreal invoices demonstrate 86.4% accuracy.", "published": "2019-10-22 15:15:27", "link": "http://arxiv.org/abs/1910.10037v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Depth-Adaptive Transformer", "abstract": "State of the art sequence-to-sequence models for large scale tasks perform a\nfixed number of computations for each input sequence regardless of whether it\nis easy or hard to process. In this paper, we train Transformer models which\ncan make output predictions at different stages of the network and we\ninvestigate different ways to predict how much computation is required for a\nparticular sequence. Unlike dynamic computation in Universal Transformers,\nwhich applies the same set of layers iteratively, we apply different layers at\nevery step to adjust both the amount of computation as well as the model\ncapacity. On IWSLT German-English translation our approach matches the accuracy\nof a well tuned baseline Transformer while using less than a quarter of the\ndecoder layers.", "published": "2019-10-22 16:15:58", "link": "http://arxiv.org/abs/1910.10073v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward estimating personal well-being using voice", "abstract": "Estimating personal well-being draws increasing attention particularly from\nhealthcare and pharmaceutical industries. We propose an approach to estimate\npersonal well-being in terms of various measurements such as anxiety, sleep\nquality and mood using voice. With clinically validated questionnaires to score\nthose measurements in a self-assessed way, we extract salient features from\nvoice and train regression models with deep neural networks. Experiments with\nthe collected database of 219 subjects show promising results in predicting the\nwell-being related measurements; concordance correlation coefficients (CCC)\nbetween self-assessed scores and predicted scores are 0.41 for anxiety, 0.44\nfor sleep quality and 0.38 for mood.", "published": "2019-10-22 16:20:35", "link": "http://arxiv.org/abs/1910.10082v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Robust Neural Machine Translation for Clean and Noisy Speech Transcripts", "abstract": "Neural machine translation models have shown to achieve high quality when\ntrained and fed with well structured and punctuated input texts. Unfortunately,\nthe latter condition is not met in spoken language translation, where the input\nis generated by an automatic speech recognition (ASR) system. In this paper, we\nstudy how to adapt a strong NMT system to make it robust to typical ASR errors.\nAs in our application scenarios transcripts might be post-edited by human\nexperts, we propose adaptation strategies to train a single system that can\ntranslate either clean or noisy input with no supervision on the input type.\nOur experimental results on a public speech translation data set show that\nadapting a model on a significant amount of parallel data including ASR\ntranscripts is beneficial with test data of the same type, but produces a small\ndegradation when translating clean text. Adapting on both clean and noisy\nvariants of the same data leads to the best results on both input types.", "published": "2019-10-22 21:24:24", "link": "http://arxiv.org/abs/1910.10238v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word-level Embeddings for Cross-Task Transfer Learning in Speech\n  Processing", "abstract": "Recent breakthroughs in deep learning often rely on representation learning\nand knowledge transfer. In recent years, unsupervised and self-supervised\ntechniques for learning speech representation were developed to foster\nautomatic speech recognition. Up to date, most of these approaches are\ntask-specific and designed for within-task transfer learning between different\ndatasets or setups of a particular task. In turn, learning task-independent\nrepresentation of speech and cross-task applications of transfer learning\nremain less common. Here, we introduce an encoder capturing word-level\nrepresentations of speech for cross-task transfer learning. We demonstrate the\napplication of the pre-trained encoder in four distinct speech and audio\nprocessing tasks: (i) speech enhancement, (ii) language identification, (iii)\nspeech, noise, and music classification, and (iv) speaker identification. In\neach task, we compare the performance of our cross-task transfer learning\napproach to task-specific baselines. Our results show that the speech\nrepresentation captured by the encoder through the pre-training is transferable\nacross distinct speech processing tasks and datasets. Notably, even simple\napplications of our pre-trained encoder outperformed task-specific methods, or\nwere comparable, depending on the task.", "published": "2019-10-22 11:58:59", "link": "http://arxiv.org/abs/1910.09909v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Transformer-based Speech Recognition Using Unsupervised\n  Pre-training", "abstract": "Speech recognition technologies are gaining enormous popularity in various\nindustrial applications. However, building a good speech recognition system\nusually requires large amounts of transcribed data, which is expensive to\ncollect. To tackle this problem, an unsupervised pre-training method called\nMasked Predictive Coding is proposed, which can be applied for unsupervised\npre-training with Transformer based model. Experiments on HKUST show that using\nthe same training data, we can achieve CER 23.3%, exceeding the best end-to-end\nmodel by over 0.2% absolute CER. With more pre-training data, we can further\nreduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.", "published": "2019-10-22 12:47:29", "link": "http://arxiv.org/abs/1910.09932v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Automated Website Classification by Deep Learning", "abstract": "In recent years, the interest in Big Data sources has been steadily growing\nwithin the Official Statistic community. The Italian National Institute of\nStatistics (Istat) is currently carrying out several Big Data pilot studies.\nOne of these studies, the ICT Big Data pilot, aims at exploiting massive\namounts of textual data automatically scraped from the websites of Italian\nenterprises in order to predict a set of target variables (e.g. e-commerce)\nthat are routinely observed by the traditional ICT Survey. In this paper, we\nshow that Deep Learning techniques can successfully address this problem.\nEssentially, we tackle a text classification task: an algorithm must learn to\ninfer whether an Italian enterprise performs e-commerce from the textual\ncontent of its website. To reach this goal, we developed a sophisticated\nprocessing pipeline and evaluated its performance through extensive\nexperiments. Our pipeline uses Convolutional Neural Networks and relies on Word\nEmbeddings to encode raw texts into grayscale images (i.e. normalized numeric\nmatrices). Web-scraped texts are huge and have very low signal to noise ratio:\nto overcome these issues, we adopted a framework known as False Positive\nReduction, which has seldom (if ever) been applied before to text\nclassification tasks. Several original contributions enable our processing\npipeline to reach good classification results. Empirical evidence shows that\nour proposal outperforms all the alternative Machine Learning solutions already\ntested in Istat for the same task.", "published": "2019-10-22 14:07:17", "link": "http://arxiv.org/abs/1910.09991v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Language-guided Semantic Mapping and Mobile Manipulation in Partially\n  Observable Environments", "abstract": "Recent advances in data-driven models for grounded language understanding\nhave enabled robots to interpret increasingly complex instructions. Two\nfundamental limitations of these methods are that most require a full model of\nthe environment to be known a priori, and they attempt to reason over a world\nrepresentation that is flat and unnecessarily detailed, which limits\nscalability. Recent semantic mapping methods address partial observability by\nexploiting language as a sensor to infer a distribution over topological,\nmetric and semantic properties of the environment. However, maintaining a\ndistribution over highly detailed maps that can support grounding of diverse\ninstructions is computationally expensive and hinders real-time human-robot\ncollaboration. We propose a novel framework that learns to adapt perception\naccording to the task in order to maintain compact distributions over semantic\nmaps. Experiments with a mobile manipulator demonstrate more efficient\ninstruction following in a priori unknown environments.", "published": "2019-10-22 15:09:02", "link": "http://arxiv.org/abs/1910.10034v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "IPOD: An Industrial and Professional Occupations Dataset and its\n  Applications to Occupational Data Mining and Analysis", "abstract": "Occupational data mining and analysis is an important task in understanding\ntoday's industry and job market. Various machine learning techniques are\nproposed and gradually deployed to improve companies' operations for upstream\ntasks, such as employee churn prediction, career trajectory modelling and\nautomated interview. Job titles analysis and embedding, as the fundamental\nbuilding blocks, are crucial upstream tasks to address these occupational data\nmining and analysis problems. In this work, we present the Industrial and\nProfessional Occupations Dataset (IPOD), which consists of over 190,000 job\ntitles crawled from over 56,000 profiles from Linkedin. We also illustrate the\nusefulness of IPOD by addressing two challenging upstream tasks, including: (i)\nproposing Title2vec, a contextual job title vector representation using a\nbidirectional Language Model (biLM) approach; and (ii) addressing the important\noccupational Named Entity Recognition problem using Conditional Random Fields\n(CRF) and bidirectional Long Short-Term Memory with CRF (LSTM-CRF). Both CRF\nand LSTM-CRF outperform human and baselines in both exact-match accuracy and F1\nscores. The dataset and pre-trained embeddings are available at\nhttps://www.github.com/junhua/ipod.", "published": "2019-10-22 08:30:26", "link": "http://arxiv.org/abs/1910.10495v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative Neural Clustering for Speaker Diarisation", "abstract": "In this paper, we propose Discriminative Neural Clustering (DNC) that\nformulates data clustering with a maximum number of clusters as a supervised\nsequence-to-sequence learning problem. Compared to traditional unsupervised\nclustering algorithms, DNC learns clustering patterns from training data\nwithout requiring an explicit definition of a similarity measure. An\nimplementation of DNC based on the Transformer architecture is shown to be\neffective on a speaker diarisation task using the challenging AMI dataset.\nSince AMI contains only 147 complete meetings as individual input sequences,\ndata scarcity is a significant issue for training a Transformer model for DNC.\nAccordingly, this paper proposes three data augmentation schemes: sub-sequence\nrandomisation, input vector randomisation, and Diaconis augmentation, which\ngenerates new data samples by rotating the entire input sequence of\nL2-normalised speaker embeddings. Experimental results on AMI show that DNC\nachieves a reduction in speaker error rate (SER) of 29.4% relative to spectral\nclustering.", "published": "2019-10-22 00:09:22", "link": "http://arxiv.org/abs/1910.09703v2", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-Step Sound Source Separation: Training on Learned Latent Targets", "abstract": "In this paper, we propose a two-step training procedure for source separation\nvia a deep neural network. In the first step we learn a transform (and it's\ninverse) to a latent space where masking-based separation performance using\noracles is optimal. For the second step, we train a separation module that\noperates on the previously learned space. In order to do so, we also make use\nof a scale-invariant signal to distortion ratio (SI-SDR) loss function that\nworks in the latent space, and we prove that it lower-bounds the SI-SDR in the\ntime domain. We run various sound separation experiments that show how this\napproach can obtain better performance as compared to systems that learn the\ntransform and the separation module jointly. The proposed methodology is\ngeneral enough to be applicable to a large class of neural network end-to-end\nseparation systems.", "published": "2019-10-22 07:49:21", "link": "http://arxiv.org/abs/1910.09804v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GCI detection from raw speech using a fully-convolutional network", "abstract": "Glottal Closure Instants (GCI) detection consists in automatically detecting\ntemporal locations of most significant excitation of the vocal tract from the\nspeech signal. It is used in many speech analysis and processing applications,\nand various algorithms have been proposed for this purpose. Recently, new\napproaches using convolutional neural networks have emerged, with encouraging\nresults. Following this trend, we propose a simple approach that performs a\nmapping from the speech waveform to a target signal from which the GCIs are\nobtained by peak-picking. However, the ground truth GCIs used for training and\nevaluation are usually extracted from EGG signals, which are not perfectly\nreliable and often not available. To overcome this problem, we propose to train\nour network on high-quality synthetic speech with perfect ground truth. The\nperformances of the proposed algorithm are compared with three other\nstate-of-the-art approaches using publicly available datasets, and the impact\nof using controlled synthetic or real speech signals in the training stage is\ninvestigated. The experimental results demonstrate that the proposed method\nobtains similar or better results than other state-of-the-art algorithms and\nthat using large synthetic datasets with many speakers offers a better\ngeneralization ability than using a smaller database of real speech and EGG\nsignals.", "published": "2019-10-22 21:15:52", "link": "http://arxiv.org/abs/1910.10235v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel\n  Separable Convolutions", "abstract": "We propose a new end-to-end neural acoustic model for automatic speech\nrecognition. The model is composed of multiple blocks with residual connections\nbetween them. Each block consists of one or more modules with 1D time-channel\nseparable convolutional layers, batch normalization, and ReLU layers. It is\ntrained with CTC loss. The proposed network achieves near state-of-the-art\naccuracy on LibriSpeech and Wall Street Journal, while having fewer parameters\nthan all competing models. We also demonstrate that this model can be\neffectively fine-tuned on new datasets.", "published": "2019-10-22 22:34:04", "link": "http://arxiv.org/abs/1910.10261v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Joint spatial filter and time-varying MCLP for dereverberation and\n  interference suppression of a dynamic/static speech source", "abstract": "Dereverberation of a moving speech source in the presence of other\ndirectional interferers, is a harder problem than that of stationary source and\ninterference cancellation. We explore joint multi channel linear prediction\n(MCLP) and relative transfer function (RTF) formulation in a stochastic\nframework and maximum likelihood estimation. We found that the combination of\nspatial filtering with distortion-less response constraint, and time-varying\ncomplex Gaussian model for the desired source signal at a reference microphone\ndoes provide better signal estimation. For a stationary source, we consider\nbatch estimation, and obtain an iterative solution. Extending to a moving\nsource, we formulate a linear time-varying dynamic system model for the MCLP\ncoefficients and RTF based online adaptive spatial filter. For the case of\ntracking a desired source in the presence of interfering sources, the same\nformulation is used by specifying the RTF. Simulated experimental results show\nthat the proposed scheme provides better spatial selectivity and\ndereverberation than the traditional methods, for both stationary and dynamic\nsources even in the presence of interfering sources.", "published": "2019-10-22 06:16:20", "link": "http://arxiv.org/abs/1910.09782v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Spiking neural networks trained with backpropagation for low power\n  neuromorphic implementation of voice activity detection", "abstract": "Recent advances in Voice Activity Detection (VAD) are driven by artificial\nand Recurrent Neural Networks (RNNs), however, using a VAD system in\nbattery-operated devices requires further power efficiency. This can be\nachieved by neuromorphic hardware, which enables Spiking Neural Networks (SNNs)\nto perform inference at very low energy consumption. Spiking networks are\ncharacterized by their ability to process information efficiently, in a sparse\ncascade of binary events in time called spikes. However, a big performance gap\nseparates artificial from spiking networks, mostly due to a lack of powerful\nSNN training algorithms. To overcome this problem we exploit an SNN model that\ncan be recast into an RNN-like model and trained with known deep learning\ntechniques. We describe an SNN training procedure that achieves low spiking\nactivity and pruning algorithms to remove 85% of the network connections with\nno performance loss. The model achieves state-of-the-art performance with a\nfraction of power consumption comparing to other methods.", "published": "2019-10-22 14:10:56", "link": "http://arxiv.org/abs/1910.09993v2", "categories": ["eess.AS", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Sound Event Localization and Detection Using CRNN on Pairs of\n  Microphones", "abstract": "This paper proposes sound event localization and detection methods from\nmultichannel recording. The proposed system is based on two Convolutional\nRecurrent Neural Networks (CRNNs) to perform sound event detection (SED) and\ntime difference of arrival (TDOA) estimation on each pair of microphones in a\nmicrophone array. In this paper, the system is evaluated with a four-microphone\narray, and thus combines the results from six pairs of microphones to provide a\nfinal classification and a 3-D direction of arrival (DOA) estimate. Results\ndemonstrate that the proposed approach outperforms the DCASE 2019 baseline\nsystem.", "published": "2019-10-22 15:41:52", "link": "http://arxiv.org/abs/1910.10049v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WHAMR!: Noisy and Reverberant Single-Channel Speech Separation", "abstract": "While significant advances have been made with respect to the separation of\noverlapping speech signals, studies have been largely constrained to mixtures\nof clean, near anechoic speech, not representative of many real-world\nscenarios. Although the WHAM! dataset introduced noise to the ubiquitous\nwsj0-2mix dataset, it did not include reverberation, which is generally present\nin indoor recordings outside of recording studios. The spectral smearing caused\nby reverberation can result in significant performance degradation for standard\ndeep learning-based speech separation systems, which rely on spectral structure\nand the sparsity of speech signals to tease apart sources. To address this, we\nintroduce WHAMR!, an augmented version of WHAM! with synthetic reverberated\nsources, and provide a thorough baseline analysis of current techniques as well\nas novel cascaded architectures on the newly introduced conditions.", "published": "2019-10-22 23:34:59", "link": "http://arxiv.org/abs/1910.10279v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sparse Array Design for Maximizing the\n  Signal-to-Interference-plus-Noise-Ratio by Matrix Completion", "abstract": "We consider sparse array beamfomer design achieving maximum signal-to\ninterference plus noise ratio (MaxSINR). Both array configuration and weights\nare attuned to the changing sensing environment. This is accomplished by\nsimultaneously switching among antenna positions and adjusting the\ncorresponding weights. The sparse array optimization design requires estimating\nthe data autocorrelations at all spatial lags across the array aperture.\nTowards this end, we adopt low rank matrix completion under the semidefinite\nToeplitz constraint for interpolating those autocorrelation values\ncorresponding to the missing lags. We compare the performance of matrix\ncompletion approach with that of the fully augmentable sparse array design\nacting on the same objective function. The optimization tool employed is the\nregularized $l_1$-norm successive convex approximation (SCA). Design examples\nwith simulated data are presented using different operating scenarios, along\nwith performance comparisons among various configurations.", "published": "2019-10-22 23:36:35", "link": "http://arxiv.org/abs/1910.10280v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Self-supervised pre-training with acoustic configurations for replay\n  spoofing detection", "abstract": "Constructing a dataset for replay spoofing detection requires a physical\nprocess of playing an utterance and re-recording it, presenting a challenge to\nthe collection of large-scale datasets. In this study, we propose a\nself-supervised framework for pretraining acoustic configurations using\ndatasets published for other tasks, such as speaker verification. Here,\nacoustic configurations refer to the environmental factors generated during the\nprocess of voice recording but not the voice itself, including microphone\ntypes, place and ambient noise levels. Specifically, we select pairs of\nsegments from utterances and train deep neural networks to determine whether\nthe acoustic configurations of the two segments are identical. We validate the\neffectiveness of the proposed method based on the ASVspoof 2019 physical access\ndataset utilizing two well-performing systems. The experimental results\ndemonstrate that the proposed method outperforms the baseline approach by 30%.", "published": "2019-10-22 05:54:41", "link": "http://arxiv.org/abs/1910.09778v2", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cross-task pre-training for on-device acoustic scene classification", "abstract": "Acoustic scene classification (ASC) and acoustic event detection (AED) are\ndifferent but related tasks. Acoustic events can provide useful information for\nrecognizing acoustic scenes. However, most of the datasets are provided without\neither the acoustic event or scene labels. To utilize the acoustic event\ninformation to improve the performance of ASC tasks, we present the cross-task\npre-training mechanism which utilizes acoustic event information from the\npre-trained AED model for ASC tasks. On the other hand, most of the models were\ndesigned and implemented on platforms with rich computing resources, and the\non-device applications were limited. To solve this problem, we use model\ndistillation method to compress our cross-task model to enable on-device\nacoustic scene classification. In this paper, the cross-task models and their\nstudent model were trained and evaluated on two datasets: TAU Urban Acoustic\nScenes 2019 dataset and TUT Acoustic Scenes 2017 dataset. Results have shown\nthat cross-task pre-training mechanism can significantly improve the\nperformance of ASC tasks. The performance of our best model improved relatively\n9.5% in the TAU Urban Acoustic Scenes 2019 dataset, and also improved 10% in\nthe TUT Acoustic Scenes 2017 dataset compared with the official baseline. At\nthe same time, the performance of the student model is much better than that of\nthe model without teachers.", "published": "2019-10-22 12:53:48", "link": "http://arxiv.org/abs/1910.09935v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sequence-to-sequence Singing Synthesis Using the Feed-forward\n  Transformer", "abstract": "We propose a sequence-to-sequence singing synthesizer, which avoids the need\nfor training data with pre-aligned phonetic and acoustic features. Rather than\nthe more common approach of a content-based attention mechanism combined with\nan autoregressive decoder, we use a different mechanism suitable for\nfeed-forward synthesis. Given that phonetic timings in singing are highly\nconstrained by the musical score, we derive an approximate initial alignment\nwith the help of a simple duration model. Then, using a decoder based on a\nfeed-forward variant of the Transformer model, a series of self-attention and\nconvolutional layers refines the result of the initial alignment to reach the\ntarget acoustic features. Advantages of this approach include faster inference\nand avoiding the exposure bias issues that affect autoregressive models trained\nby teacher forcing. We evaluate the effectiveness of this model compared to an\nautoregressive baseline, the importance of self-attention, and the importance\nof the accuracy of the duration model.", "published": "2019-10-22 14:04:50", "link": "http://arxiv.org/abs/1910.09989v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Example Detection by Classification for Deep Speech\n  Recognition", "abstract": "Machine Learning systems are vulnerable to adversarial attacks and will\nhighly likely produce incorrect outputs under these attacks. There are\nwhite-box and black-box attacks regarding to adversary's access level to the\nvictim learning algorithm. To defend the learning systems from these attacks,\nexisting methods in the speech domain focus on modifying input signals and\ntesting the behaviours of speech recognizers. We, however, formulate the\ndefense as a classification problem and present a strategy for systematically\ngenerating adversarial example datasets: one for white-box attacks and one for\nblack-box attacks, containing both adversarial and normal examples. The\nwhite-box attack is a gradient-based method on Baidu DeepSpeech with the\nMozilla Common Voice database while the black-box attack is a gradient-free\nmethod on a deep model-based keyword spotting system with the Google Speech\nCommand dataset. The generated datasets are used to train a proposed\nConvolutional Neural Network (CNN), together with cepstral features, to detect\nadversarial examples. Experimental results show that, it is possible to\naccurately distinct between adversarial and normal examples for known attacks,\nin both single-condition and multi-condition training settings, while the\nperformance degrades dramatically for unknown attacks. The adversarial datasets\nand the source code are made publicly available.", "published": "2019-10-22 14:46:00", "link": "http://arxiv.org/abs/1910.10013v1", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving singing voice separation with the Wave-U-Net using Minimum\n  Hyperspherical Energy", "abstract": "In recent years, deep learning has surpassed traditional approaches to the\nproblem of singing voice separation. The Wave-U-Net is a recent deep network\narchitecture that operates directly on the time domain. The standard Wave-U-Net\nis trained with data augmentation and early stopping to prevent overfitting.\nMinimum hyperspherical energy (MHE) regularization has recently proven to\nincrease generalization in image classification problems by encouraging a\ndiversified filter configuration. In this work, we apply MHE regularization to\nthe 1D filters of the Wave-U-Net. We evaluated this approach for separating the\nvocal part from mixed music audio recordings on the MUSDB18 dataset. We found\nthat adding MHE regularization to the loss function consistently improves\nsinging voice separation, as measured in the Signal to Distortion Ratio on test\nrecordings, leading to the current best time-domain system for singing voice\nextraction.", "published": "2019-10-22 16:14:25", "link": "http://arxiv.org/abs/1910.10071v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Modeling plate and spring reverberation using a DSP-informed deep neural\n  network", "abstract": "Plate and spring reverberators are electromechanical systems first used and\nresearched as means to substitute real room reverberation. Nowadays they are\noften used in music production for aesthetic reasons due to their particular\nsonic characteristics. The modeling of these audio processors and their\nperceptual qualities is difficult since they use mechanical elements together\nwith analog electronics resulting in an extremely complex response. Based on\ndigital reverberators that use sparse FIR filters, we propose a signal\nprocessing-informed deep learning architecture for the modeling of artificial\nreverberators. We explore the capabilities of deep neural networks to learn\nsuch highly nonlinear electromechanical responses and we perform modeling of\nplate and spring reverberators. In order to measure the performance of the\nmodel, we conduct a perceptual evaluation experiment and we also analyze how\nthe given task is accomplished and what the model is actually learning.", "published": "2019-10-22 16:45:38", "link": "http://arxiv.org/abs/1910.10105v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Complex Transformer: A Framework for Modeling Complex-Valued Sequence", "abstract": "While deep learning has received a surge of interest in a variety of fields\nin recent years, major deep learning models barely use complex numbers.\nHowever, speech, signal and audio data are naturally complex-valued after\nFourier Transform, and studies have shown a potentially richer representation\nof complex nets. In this paper, we propose a Complex Transformer, which\nincorporates the transformer model as a backbone for sequence modeling; we also\ndevelop attention and encoder-decoder network operating for complex input. The\nmodel achieves state-of-the-art performance on the MusicNet dataset and an\nIn-phase Quadrature (IQ) signal dataset.", "published": "2019-10-22 19:21:12", "link": "http://arxiv.org/abs/1910.10202v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning the helix topology of musical pitch", "abstract": "To explain the consonance of octaves, music psychologists represent pitch as\na helix where azimuth and axial coordinate correspond to pitch class and pitch\nheight respectively. This article addresses the problem of discovering this\nhelical structure from unlabeled audio data. We measure Pearson correlations in\nthe constant-Q transform (CQT) domain to build a K-nearest neighbor graph\nbetween frequency subbands. Then, we run the Isomap manifold learning algorithm\nto represent this graph in a three-dimensional space in which straight lines\napproximate graph geodesics. Experiments on isolated musical notes demonstrate\nthat the resulting manifold resembles a helix which makes a full turn at every\noctave. A circular shape is also found in English speech, but not in urban\nnoise. We discuss the impact of various design choices on the visualization:\ninstrumentarium, loudness mapping function, and number of neighbors K.", "published": "2019-10-22 21:56:16", "link": "http://arxiv.org/abs/1910.10246v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "G2G: TTS-Driven Pronunciation Learning for Graphemic Hybrid ASR", "abstract": "Grapheme-based acoustic modeling has recently been shown to outperform\nphoneme-based approaches in both hybrid and end-to-end automatic speech\nrecognition (ASR), even on non-phonemic languages like English. However,\ngraphemic ASR still has problems with rare long-tail words that do not follow\nthe standard spelling conventions seen in training, such as entity names. In\nthis work, we present a novel method to train a statistical\ngrapheme-to-grapheme (G2G) model on text-to-speech data that can rewrite an\narbitrary character sequence into more phonetically consistent forms. We show\nthat using G2G to provide alternative pronunciations during decoding reduces\nWord Error Rate by 3% to 11% relative over a strong graphemic baseline and\nbridges the gap on rare name recognition with an equivalent phonetic setup.\nUnlike many previously proposed methods, our method does not require any change\nto the acoustic model training procedure. This work reaffirms the efficacy of\ngrapheme-based modeling and shows that specialized linguistic knowledge, when\navailable, can be leveraged to improve graphemic ASR.", "published": "2019-10-22 21:49:50", "link": "http://arxiv.org/abs/1910.12612v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "CycleGAN Voice Conversion of Spectral Envelopes using Adversarial\n  Weights", "abstract": "This paper tackles GAN optimization and stability issues in the context of\nvoice conversion. First, to simplify the conversion task, we propose to use\nspectral envelopes as inputs. Second we propose two adversarial weight training\nparadigms, the generalized weighted GAN and the generator impact GAN, both aim\nat reducing the impact of the generator on the discriminator, so both can learn\nmore gradually and efficiently during training. Applying an energy constraint\nto the cycleGAN paradigm considerably improved conversion quality. A subjective\nexperiment conducted on a voice conversion task on the voice conversion\nchallenge 2018 dataset shows first that despite a significantly reduced network\ncomplexity, the proposed method achieves state-of-the-art results, and second\nthat the proposed weighted GAN methods outperform a previously proposed one.", "published": "2019-10-22 12:18:18", "link": "http://arxiv.org/abs/1910.12614v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simultaneous Separation and Transcription of Mixtures with Multiple\n  Polyphonic and Percussive Instruments", "abstract": "We present a single deep learning architecture that can both separate an\naudio recording of a musical mixture into constituent single-instrument\nrecordings and transcribe these instruments into a human-readable format at the\nsame time, learning a shared musical representation for both tasks. This novel\narchitecture, which we call Cerberus, builds on the Chimera network for source\nseparation by adding a third \"head\" for transcription. By training each head\nwith different losses, we are able to jointly learn how to separate and\ntranscribe up to 5 instruments in our experiments with a single network. We\nshow that the two tasks are highly complementary with one another and when\nlearned jointly, lead to Cerberus networks that are better at both separation\nand transcription and generalize better to unseen mixtures.", "published": "2019-10-22 04:46:34", "link": "http://arxiv.org/abs/1910.12621v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Learning For Classroom Activity Detection", "abstract": "Classroom activity detection (CAD) focuses on accurately classifying whether\nthe teacher or student is speaking and recording both the length of individual\nutterances during a class. A CAD solution helps teachers get instant feedback\non their pedagogical instructions. This greatly improves educators' teaching\nskills and hence leads to students' achievement. However, CAD is very\nchallenging because (1) the CAD model needs to be generalized well enough for\ndifferent teachers and students; (2) data from both vocal and language\nmodalities has to be wisely fused so that they can be complementary; and (3)\nthe solution shouldn't heavily rely on additional recording device. In this\npaper, we address the above challenges by using a novel attention based neural\nframework. Our framework not only extracts both speech and language\ninformation, but utilizes attention mechanism to capture long-term semantic\ndependence. Our framework is device-free and is able to take any classroom\nrecording as input. The proposed CAD learning framework is evaluated in two\nreal-world education applications. The experimental results demonstrate the\nbenefits of our approach on learning attention based neural network from\nclassroom data with different modalities, and show our approach is able to\noutperform state-of-the-art baselines in terms of various evaluation metrics.", "published": "2019-10-22 12:14:07", "link": "http://arxiv.org/abs/1910.13799v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms", "abstract": "This paper shows the susceptibility of spectrogram-based audio classifiers to\nadversarial attacks and the transferability of such attacks to audio waveforms.\nSome commonly used adversarial attacks to images have been applied to\nMel-frequency and short-time Fourier transform spectrograms, and such perturbed\nspectrograms are able to fool a 2D convolutional neural network (CNN). Such\nattacks produce perturbed spectrograms that are visually imperceptible by\nhumans. Furthermore, the audio waveforms reconstructed from the perturbed\nspectrograms are also able to fool a 1D CNN trained on the original audio.\nExperimental results on a dataset of western music have shown that the 2D CNN\nachieves up to 81.87% of mean accuracy on legitimate examples and such\nperformance drops to 12.09% on adversarial examples. Likewise, the 1D CNN\nachieves up to 78.29% of mean accuracy on original audio samples and such\nperformance drops to 27.91% on adversarial audio waveforms reconstructed from\nthe perturbed spectrograms.", "published": "2019-10-22 16:46:37", "link": "http://arxiv.org/abs/1910.10106v4", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
