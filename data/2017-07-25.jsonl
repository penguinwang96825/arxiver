{"title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing", "abstract": "To learn a semantic parser from denotations, a learning algorithm must search\nover a combinatorially large space of logical forms for ones consistent with\nthe annotated denotations. We propose a new online learning algorithm that\nsearches faster as training progresses. The two key ideas are using macro\ngrammars to cache the abstract patterns of useful logical forms found thus far,\nand holistic triggering to efficiently retrieve the most relevant patterns\nbased on sentence similarity. On the WikiTableQuestions dataset, we first\nexpand the search space of an existing model to improve the state-of-the-art\naccuracy from 38.7% to 42.7%, and then use macro grammars and holistic\ntriggering to achieve an 11x speedup and an accuracy of 43.7%.", "published": "2017-07-25 03:42:40", "link": "http://arxiv.org/abs/1707.07806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation at Booking.com: Journey and Lessons Learned", "abstract": "We describe our recently developed neural machine translation (NMT) system\nand benchmark it against our own statistical machine translation (SMT) system\nas well as two other general purpose online engines (statistical and neural).\nWe present automatic and human evaluation results of the translation output\nprovided by each system. We also analyze the effect of sentence length on the\nquality of output for SMT and NMT systems.", "published": "2017-07-25 10:51:19", "link": "http://arxiv.org/abs/1707.07911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Dependent Recurrent Entity Network for Question Answering", "abstract": "Question Answering is a task which requires building models capable of\nproviding answers to questions expressed in human language. Full question\nanswering involves some form of reasoning ability. We introduce a neural\nnetwork architecture for this task, which is a form of $Memory\\ Network$, that\nrecognizes entities and their relations to answers through a focus attention\nmechanism. Our model is named $Question\\ Dependent\\ Recurrent\\ Entity\\ Network$\nand extends $Recurrent\\ Entity\\ Network$ by exploiting aspects of the question\nduring the memorization process. We validate the model on both synthetic and\nreal datasets: the $bAbI$ question answering dataset and the $CNN\\ \\&\\ Daily\\\nNews$ $reading\\ comprehension$ dataset. In our experiments, the models achieved\na State-of-The-Art in the former and competitive results in the latter.", "published": "2017-07-25 11:34:14", "link": "http://arxiv.org/abs/1707.07922v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthesising Sign Language from semantics, approaching \"from the target\n  and back\"", "abstract": "We present a Sign Language modelling approach allowing to build grammars and\ncreate linguistic input for Sign synthesis through avatars. We comment on the\ntype of grammar it allows to build, and observe a resemblance between the\nresulting expressions and traditional semantic representations. Comparing the\nways in which the paradigms are designed, we name and contrast two essentially\ndifferent strategies for building higher-level linguistic input:\n\"source-and-forward\" vs. \"target-and-back\". We conclude by favouring the\nlatter, acknowledging the power of being able to automatically generate output\nfrom semantically relevant input straight into articulations of the target\nlanguage.", "published": "2017-07-25 15:28:59", "link": "http://arxiv.org/abs/1707.08041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Data-to-Document Generation", "abstract": "Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements.", "published": "2017-07-25 15:42:25", "link": "http://arxiv.org/abs/1707.08052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Relatedness over Time", "abstract": "Search systems are often focused on providing relevant results for the \"now\",\nassuming both corpora and user needs that focus on the present. However, many\ncorpora today reflect significant longitudinal collections ranging from 20\nyears of the Web to hundreds of years of digitized newspapers and books.\nUnderstanding the temporal intent of the user and retrieving the most relevant\nhistorical content has become a significant challenge. Common search features,\nsuch as query expansion, leverage the relationship between terms but cannot\nfunction well across all times when relationships vary temporally. In this\nwork, we introduce a temporal relationship model that is extracted from\nlongitudinal data collections. The model supports the task of identifying,\ngiven two words, when they relate to each other. We present an algorithmic\nframework for this task and show its application for the task of query\nexpansion, achieving high gain.", "published": "2017-07-25 16:41:49", "link": "http://arxiv.org/abs/1707.08081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShotgunWSD: An unsupervised algorithm for global word sense\n  disambiguation inspired by DNA sequencing", "abstract": "In this paper, we present a novel unsupervised algorithm for word sense\ndisambiguation (WSD) at the document level. Our algorithm is inspired by a\nwidely-used approach in the field of genetics for whole genome sequencing,\nknown as the Shotgun sequencing technique. The proposed WSD algorithm is based\non three main steps. First, a brute-force WSD algorithm is applied to short\ncontext windows (up to 10 words) selected from the document in order to\ngenerate a short list of likely sense configurations for each window. In the\nsecond step, these local sense configurations are assembled into longer\ncomposite configurations based on suffix and prefix matching. The resulted\nconfigurations are ranked by their length, and the sense of each word is chosen\nbased on a voting scheme that considers only the top k configurations in which\nthe word appears. We compare our algorithm with other state-of-the-art\nunsupervised WSD algorithms and demonstrate better performance, sometimes by a\nvery large margin. We also show that our algorithm can yield better performance\nthan the Most Common Sense (MCS) baseline on one data set. Moreover, our\nalgorithm has a very small number of parameters, is robust to parameter tuning,\nand, unlike other bio-inspired methods, it gives a deterministic solution (it\ndoes not involve random choices).", "published": "2017-07-25 16:56:53", "link": "http://arxiv.org/abs/1707.08084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Image to Text Classification: A Novel Approach based on Clustering\n  Word Embeddings", "abstract": "In this paper, we propose a novel approach for text classification based on\nclustering word embeddings, inspired by the bag of visual words model, which is\nwidely used in computer vision. After each word in a collection of documents is\nrepresented as word vector using a pre-trained word embeddings model, a k-means\nalgorithm is applied on the word vectors in order to obtain a fixed-size set of\nclusters. The centroid of each cluster is interpreted as a super word embedding\nthat embodies all the semantically related word vectors in a certain region of\nthe embedding space. Every embedded word in the collection of documents is then\nassigned to the nearest cluster centroid. In the end, each document is\nrepresented as a bag of super word embeddings by computing the frequency of\neach super word embedding in the respective document. We also diverge from the\nidea of building a single vocabulary for the entire collection of documents,\nand propose to build class-specific vocabularies for better performance. Using\nthis kind of representation, we report results on two text mining tasks, namely\ntext categorization by topic and polarity classification. On both tasks, our\nmodel yields better performance than the standard bag of words.", "published": "2017-07-25 17:29:18", "link": "http://arxiv.org/abs/1707.08098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference\n  with Sentence Representations", "abstract": "This paper presents the results of the RepEval 2017 Shared Task, which\nevaluated neural network sentence representation learning models on the\nMulti-Genre Natural Language Inference corpus (MultiNLI) recently introduced by\nWilliams et al. (2017). All of the five participating teams beat the\nbidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in\nWilliams et al.. The best single model used stacked BiLSTMs with residual\nconnections to extract sentence features and reached 74.5% accuracy on the\ngenre-matched test set. Surprisingly, the results of the competition were\nfairly consistent across the genre-matched and genre-mismatched test sets, and\nacross subsets of the test data representing a variety of linguistic phenomena,\nsuggesting that all of the submitted systems learned reasonably\ndomain-independent representations for sentence meaning.", "published": "2017-07-25 19:27:05", "link": "http://arxiv.org/abs/1707.08172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All that is English may be Hindi: Enhancing language identification\n  through automatic ranking of likeliness of word borrowing in social media", "abstract": "In this paper, we present a set of computational methods to identify the\nlikeliness of a word being borrowed, based on the signals from social media. In\nterms of Spearman correlation coefficient values, our methods perform more than\ntwo times better (nearly 0.62) in predicting the borrowing likeliness compared\nto the best performing baseline (nearly 0.26) reported in literature. Based on\nthis likeliness estimate we asked annotators to re-annotate the language tags\nof foreign words in predominantly native contexts. In 88 percent of cases the\nannotators felt that the foreign language tag should be replaced by native\nlanguage tag, thus indicating a huge scope for improvement of automatic\nlanguage identification systems.", "published": "2017-07-25 04:17:42", "link": "http://arxiv.org/abs/1707.08446v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Lexical and Temporal Signals in Neural Ranking Models for\n  Searching Social Media Streams", "abstract": "Time is an important relevance signal when searching streams of social media\nposts. The distribution of document timestamps from the results of an initial\nquery can be leveraged to infer the distribution of relevant documents, which\ncan then be used to rerank the initial results. Previous experiments have shown\nthat kernel density estimation is a simple yet effective implementation of this\nidea. This paper explores an alternative approach to mining temporal signals\nwith recurrent neural networks. Our intuition is that neural networks provide a\nmore expressive framework to capture the temporal coherence of neighboring\ndocuments in time. To our knowledge, we are the first to integrate lexical and\ntemporal signals in an end-to-end neural network architecture, in which\nexisting neural ranking models are used to generate query-document similarity\nvectors that feed into a bidirectional LSTM layer for temporal modeling. Our\nresults are mixed: existing neural models for document ranking alone yield\nlimited improvements over simple baselines, but the integration of lexical and\ntemporal signals yield significant improvements over competitive temporal\nbaselines.", "published": "2017-07-25 02:29:31", "link": "http://arxiv.org/abs/1707.07792v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring the Effectiveness of Convolutional Neural Networks for Answer\n  Selection in End-to-End Question Answering", "abstract": "Most work on natural language question answering today focuses on answer\nselection: given a candidate list of sentences, determine which contains the\nanswer. Although important, answer selection is only one stage in a standard\nend-to-end question answering pipeline. This paper explores the effectiveness\nof convolutional neural networks (CNNs) for answer selection in an end-to-end\ncontext using the standard TrecQA dataset. We observe that a simple\nidf-weighted word overlap algorithm forms a very strong baseline, and that\ndespite substantial efforts by the community in applying deep learning to\ntackle answer selection, the gains are modest at best on this dataset.\nFurthermore, it is unclear if a CNN is more effective than the baseline in an\nend-to-end context based on standard retrieval metrics. To further explore this\nfinding, we conducted a manual user evaluation, which confirms that answers\nfrom the CNN are detectably better than those from idf-weighted word overlap.\nThis result suggests that users are sensitive to relatively small differences\nin answer selection quality.", "published": "2017-07-25 03:38:57", "link": "http://arxiv.org/abs/1707.07804v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Semantic Query Segmentation", "abstract": "Query Segmentation is one of the critical components for understanding users'\nsearch intent in Information Retrieval tasks. It involves grouping tokens in\nthe search query into meaningful phrases which help downstream tasks like\nsearch relevance and query understanding. In this paper, we propose a novel\napproach to segment user queries using distributed query embeddings. Our key\ncontribution is a supervised approach to the segmentation task using\nlow-dimensional feature vectors for queries, getting rid of traditional hand\ntuned and heuristic NLP features which are quite expensive.\n  We benchmark on a 50,000 human-annotated web search engine query corpus\nachieving comparable accuracy to state-of-the-art techniques. The advantage of\nour technique is its fast and does not use external knowledge-base like\nWikipedia for score boosting. This helps us generalize our approach to other\ndomains like eCommerce without any fine-tuning. We demonstrate the\neffectiveness of this method on another 50,000 human-annotated eCommerce query\ncorpus from eBay search logs. Our approach is easy to implement and generalizes\nwell across different search domains proving the power of low-dimensional\nembeddings in query segmentation task, opening up a new direction of research\nfor this problem.", "published": "2017-07-25 06:57:39", "link": "http://arxiv.org/abs/1707.07835v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hyperbolic Representation Learning for Fast and Efficient Neural\n  Question Answering", "abstract": "The dominant neural architectures in question answer retrieval are based on\nrecurrent or convolutional encoders configured with complex word matching\nlayers. Given that recent architectural innovations are mostly new word\ninteraction layers or attention-based matching mechanisms, it seems to be a\nwell-established fact that these components are mandatory for good performance.\nUnfortunately, the memory and computation cost incurred by these complex\nmechanisms are undesirable for practical applications. As such, this paper\ntackles the question of whether it is possible to achieve competitive\nperformance with simple neural architectures. We propose a simple but novel\ndeep learning architecture for fast and efficient question-answer ranking and\nretrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a\nparameter efficient neural network that outperforms other parameter intensive\nmodels such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple\nQA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking\nobjective that models the relationship between question and answer embeddings\nin Hyperbolic space instead of Euclidean space. This empowers our model with a\nself-organizing ability and enables automatic discovery of latent hierarchies\nwhile learning embeddings of questions and answers. Our model requires no\nfeature engineering, no similarity matrix matching, no complicated attention\nmechanisms nor over-parameterized layers and yet outperforms and remains\ncompetitive to many models that have these functionalities on multiple\nbenchmarks.", "published": "2017-07-25 08:21:30", "link": "http://arxiv.org/abs/1707.07847v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Analogs of Linguistic Structure in Deep Representations", "abstract": "We investigate the compositional structure of message vectors computed by a\ndeep network trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation, conjunction, and disjunction. Our results suggest that neural\nrepresentations are capable of spontaneously developing a \"syntax\" with\nfunctional analogues to qualitative properties of natural language.", "published": "2017-07-25 18:10:48", "link": "http://arxiv.org/abs/1707.08139v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Structural Regularities in Text-based Entity Vector Spaces", "abstract": "Entity retrieval is the task of finding entities such as people or products\nin response to a query, based solely on the textual documents they are\nassociated with. Recent semantic entity retrieval algorithms represent queries\nand experts in finite-dimensional vector spaces, where both are constructed\nfrom text sequences.\n  We investigate entity vector spaces and the degree to which they capture\nstructural regularities. Such vector spaces are constructed in an unsupervised\nmanner without explicit information about structural aspects. For concreteness,\nwe address these questions for a specific type of entity: experts in the\ncontext of expert finding. We discover how clusterings of experts correspond to\ncommittees in organizations, the ability of expert representations to encode\nthe co-author graph, and the degree to which they encode academic rank. We\ncompare latent, continuous representations created using methods based on\ndistributional semantics (LSI), topic models (LDA) and neural networks\n(word2vec, doc2vec, SERT). Vector spaces created using neural methods, such as\ndoc2vec and SERT, systematically perform better at clustering than LSI, LDA and\nword2vec. When it comes to encoding entity relations, SERT performs best.", "published": "2017-07-25 11:54:19", "link": "http://arxiv.org/abs/1707.07930v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\n  Functions in Quasi-Recurrent Neural Networks", "abstract": "In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\nunbounded positive and negative image, can be used as a drop-in replacement for\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\nprone to the vanishing gradient problem, they are noise robust, and they induce\nsparse activations.\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\nlanguage modeling. Additionally, we evaluate on character-level language\nmodeling, showing that we are able to stack up to eight QRNN layers with\nDReLUs, thus making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over shallow architectures based on LSTMs.", "published": "2017-07-25 20:52:32", "link": "http://arxiv.org/abs/1707.08214v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
