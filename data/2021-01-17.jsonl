{"title": "Few Shot Dialogue State Tracking using Meta-learning", "abstract": "Dialogue State Tracking (DST) forms a core component of automated chatbot\nbased systems designed for specific goals like hotel, taxi reservation, tourist\ninformation, etc. With the increasing need to deploy such systems in new\ndomains, solving the problem of zero/few-shot DST has become necessary. There\nhas been a rising trend for learning to transfer knowledge from resource-rich\ndomains to unknown domains with minimal need for additional data. In this work,\nwe explore the merits of meta-learning algorithms for this transfer and hence,\npropose a meta-learner D-REPTILE specific to the DST problem. With extensive\nexperimentation, we provide clear evidence of benefits over conventional\napproaches across different domains, methods, base models, and datasets with\nsignificant (5-25%) improvement over the baseline in a low-data setting. Our\nproposed meta-learner is agnostic of the underlying model and hence any\nexisting state-of-the-art DST system can improve its performance on unknown\ndomains using our training strategy.", "published": "2021-01-17 20:47:06", "link": "http://arxiv.org/abs/2101.06779v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narration Generation for Cartoon Videos", "abstract": "Research on text generation from multimodal inputs has largely focused on\nstatic images, and less on video data. In this paper, we propose a new task,\nnarration generation, that is complementing videos with narration texts that\nare to be interjected in several places. The narrations are part of the video\nand contribute to the storyline unfolding in it. Moreover, they are\ncontext-informed, since they include information appropriate for the timeframe\nof video they cover, and also, do not need to include every detail shown in\ninput scenes, as a caption would. We collect a new dataset from the animated\ntelevision series Peppa Pig. Furthermore, we formalize the task of narration\ngeneration as including two separate tasks, timing and content generation, and\npresent a set of models on the new task.", "published": "2021-01-17 23:23:09", "link": "http://arxiv.org/abs/2101.06803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes Good In-Context Examples for GPT-$3$?", "abstract": "GPT-$3$ has attracted lots of attention due to its superior performance\nacross a wide range of NLP tasks, especially with its powerful and versatile\nin-context few-shot learning ability. Despite its success, we found that the\nempirical results of GPT-$3$ depend heavily on the choice of in-context\nexamples. In this work, we investigate whether there are more effective\nstrategies for judiciously selecting in-context examples (relative to random\nsampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by the\nrecent success of leveraging a retrieval module to augment large-scale neural\nnetwork models, we propose to retrieve examples that are semantically-similar\nto a test sample to formulate its corresponding prompt. Intuitively, the\nin-context examples selected with such a strategy may serve as more informative\ninputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposed\napproach on several natural language understanding and generation benchmarks,\nwhere the retrieval-based prompt selection approach consistently outperforms\nthe random baseline. Moreover, it is observed that the sentence encoders\nfine-tuned on task-related datasets yield even more helpful retrieval results.\nNotably, significant gains are observed on tasks such as table-to-text\ngeneration (41.9% on the ToTTo dataset) and open-domain question answering\n(45.5% on the NQ dataset). We hope our investigation could help understand the\nbehaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhance\ntheir few-shot capabilities.", "published": "2021-01-17 23:38:40", "link": "http://arxiv.org/abs/2101.06804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GENIE: Toward Reproducible and Standardized Human Evaluation for Text\n  Generation", "abstract": "While often assumed a gold standard, effective human evaluation of text\ngeneration remains an important, open area for research. We revisit this\nproblem with a focus on producing consistent evaluations that are reproducible\n-- over time and across different populations. We study this goal in different\nstages of the human evaluation pipeline. In particular, we consider design\nchoices for the annotation interface used to elicit human judgments and their\nimpact on reproducibility. Furthermore, we develop an automated mechanism for\nmaintaining annotator quality via a probabilistic model that detects and\nexcludes noisy annotators. Putting these lessons together, we introduce GENIE:\na system for running standardized human evaluations across different generation\ntasks. We instantiate GENIE with datasets representing four core challenges in\ntext generation: machine translation, summarization, commonsense reasoning, and\nmachine comprehension. For each task, GENIE offers a leaderboard that\nautomatically crowdsources annotations for submissions, evaluating them along\naxes such as correctness, conciseness, and fluency. We have made the GENIE\nleaderboards publicly available, and have already ranked 50 submissions from 10\ndifferent research groups. We hope GENIE encourages further progress toward\neffective, standardized evaluations for text generation.", "published": "2021-01-17 00:40:47", "link": "http://arxiv.org/abs/2101.06561v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HySTER: A Hybrid Spatio-Temporal Event Reasoner", "abstract": "The task of Video Question Answering (VideoQA) consists in answering natural\nlanguage questions about a video and serves as a proxy to evaluate the\nperformance of a model in scene sequence understanding. Most methods designed\nfor VideoQA up-to-date are end-to-end deep learning architectures which\nstruggle at complex temporal and causal reasoning and provide limited\ntransparency in reasoning steps. We present the HySTER: a Hybrid\nSpatio-Temporal Event Reasoner to reason over physical events in videos. Our\nmodel leverages the strength of deep learning methods to extract information\nfrom video frames with the reasoning capabilities and explainability of\nsymbolic artificial intelligence in an answer set programming framework. We\ndefine a method based on general temporal, causal and physics rules which can\nbe transferred across tasks. We apply our model to the CLEVRER dataset and\ndemonstrate state-of-the-art results in question answering accuracy. This work\nsets the foundations for the incorporation of inductive logic programming in\nthe field of VideoQA.", "published": "2021-01-17 11:07:17", "link": "http://arxiv.org/abs/2101.06644v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.CV"}
{"title": "Efficiently Fusing Pretrained Acoustic and Linguistic Encoders for\n  Low-resource Speech Recognition", "abstract": "End-to-end models have achieved impressive results on the task of automatic\nspeech recognition (ASR). For low-resource ASR tasks, however, labeled data can\nhardly satisfy the demand of end-to-end models. Self-supervised acoustic\npre-training has already shown its amazing ASR performance, while the\ntranscription is still inadequate for language modeling in end-to-end models.\nIn this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a\npre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused\nmodel only needs to learn the transfer from speech to language during\nfine-tuning on limited labeled data. The length of the two modalities is\nmatched by a monotonic attention mechanism without additional parameters.\nBesides, a fully connected layer is introduced for the hidden mapping between\nmodalities. We further propose a scheduled fine-tuning strategy to preserve and\nutilize the text context modeling ability of the pre-trained linguistic\nencoder. Experiments show our effective utilizing of pre-trained modules. Our\nmodel achieves better recognition performance on CALLHOME corpus (15 hours)\nthan other end-to-end models.", "published": "2021-01-17 16:12:44", "link": "http://arxiv.org/abs/2101.06699v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A System for Efficiently Hunting for Cyber Threats in Computer Systems\n  Using Threat Intelligence", "abstract": "Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated cyber attacks. However, existing approaches require\nnon-trivial efforts of manual query construction and have overlooked the rich\nexternal knowledge about threat behaviors provided by open-source Cyber Threat\nIntelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that\nfacilitates cyber threat hunting in computer systems using OSCTI. Built upon\nmature system auditing frameworks, ThreatRaptor provides (1) an unsupervised,\nlight-weight, and accurate NLP pipeline that extracts structured threat\nbehaviors from unstructured OSCTI text, (2) a concise and expressive\ndomain-specific query language, TBQL, to hunt for malicious system activities,\n(3) a query synthesis mechanism that automatically synthesizes a TBQL query\nfrom the extracted threat behaviors, and (4) an efficient query execution\nengine to search the big system audit logging data.", "published": "2021-01-17 19:44:09", "link": "http://arxiv.org/abs/2101.06761v2", "categories": ["cs.CR", "cs.CL", "cs.DB"], "primary_category": "cs.CR"}
{"title": "An embedded multichannel sound acquisition system for drone audition", "abstract": "Microphone array techniques can improve the acoustic sensing performance on\ndrones, compared to the use of a single microphone. However, multichannel sound\nacquisition systems are not available in current commercial drone platforms. To\nencourage the research in drone audition, we present an embedded sound\nacquisition and recording system with eight microphones and a multichannel\nsound recorder mounted on a quadcopter. In addition to recording and storing\nlocally the sound from multiple microphones simultaneously, the embedded system\ncan connect wirelessly to a remote terminal to transfer audio files for further\nprocessing. This will be the first stage towards creating a fully embedded\nsolution for drone audition. We present experimental results obtained by\nstate-of-the-art drone audition algorithms applied to the sound recorded by the\nembedded system.", "published": "2021-01-17 22:42:36", "link": "http://arxiv.org/abs/2101.06795v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
