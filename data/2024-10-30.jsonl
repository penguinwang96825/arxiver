{"title": "Continuous Risk Factor Models: Analyzing Asset Correlations through Energy Distance", "abstract": "This paper introduces a novel approach to financial risk analysis that does\nnot rely on traditional price and market data, instead using market news to\nmodel assets as distributions over a metric space of risk factors. By\nrepresenting asset returns as integrals over the scalar field of these risk\nfactors, we derive the covariance structure between asset returns. Utilizing\nencoder-only language models to embed this news data, we explore the\nrelationships between asset return distributions through the concept of Energy\nDistance, establishing connections between distributional differences and\nexcess returns co-movements. This data-agnostic approach provides new insights\ninto portfolio diversification, risk management, and the construction of\nhedging strategies. Our findings have significant implications for both\ntheoretical finance and practical risk management, offering a more robust\nframework for modelling complex financial systems without depending on\nconventional market data.", "published": "2024-10-30 20:39:32", "link": "http://arxiv.org/abs/2410.23447v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "AI in Investment Analysis: LLMs for Equity Stock Ratings", "abstract": "Investment Analysis is a cornerstone of the Financial Services industry. The\nrapid integration of advanced machine learning techniques, particularly Large\nLanguage Models (LLMs), offers opportunities to enhance the equity rating\nprocess. This paper explores the application of LLMs to generate multi-horizon\nstock ratings by ingesting diverse datasets. Traditional stock rating methods\nrely heavily on the expertise of financial analysts, and face several\nchallenges such as data overload, inconsistencies in filings, and delayed\nreactions to market events. Our study addresses these issues by leveraging LLMs\nto improve the accuracy and consistency of stock ratings. Additionally, we\nassess the efficacy of using different data modalities with LLMs for the\nfinancial domain.\n  We utilize varied datasets comprising fundamental financial, market, and news\ndata from January 2022 to June 2024, along with GPT-4-32k (v0613) (with a\ntraining cutoff in Sep. 2021 to prevent information leakage). Our results show\nthat our benchmark method outperforms traditional stock rating methods when\nassessed by forward returns, specially when incorporating financial\nfundamentals. While integrating news data improves short-term performance,\nsubstituting detailed news summaries with sentiment scores reduces token use\nwithout loss of performance. In many cases, omitting news data entirely\nenhances performance by reducing bias.\n  Our research shows that LLMs can be leveraged to effectively utilize large\namounts of multimodal financial data, as showcased by their effectiveness at\nthe stock rating prediction task. Our work provides a reproducible and\nefficient framework for generating accurate stock ratings, serving as a\ncost-effective alternative to traditional methods. Future work will extend to\nlonger timeframes, incorporate diverse data, and utilize newer models for\nenhanced insights.", "published": "2024-10-30 15:06:57", "link": "http://arxiv.org/abs/2411.00856v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "68T50, 91G60 (Primary) 68T07 (Secondary)", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Emerging countries' counter-currency cycles in the face of crises and dominant currencies", "abstract": "This article examines how emerging economies use countercyclical monetary\npolicies to manage economic crises and fluctuations in dominant currencies,\nsuch as the US dollar and the euro. Global economic cycles are marked by phases\nof expansion and recession, often exacerbated by major financial crises. These\ncrises, such as those of 1997, 2008 and the disruption caused by the COVID-19\npandemic, have a particular impact on emerging economies due to their\nheightened vulnerability to foreign capital flows and exports.Counter-cyclical\nmonetary policies, including interest rate adjustments, foreign exchange\ninterventions and capital controls, are essential to stabilize these economies.\nThese measures aim to mitigate the effects of economic shocks, maintain price\nstability and promote sustainable growth. This article presents a theoretical\nanalysis of economic cycles and financial crises, highlighting the role of\ndominant currencies in global economic stability. Currencies such as the dollar\nand the euro strongly influence emerging economies, notably through exchange\nrate variations and international capital movements. Analysis of the monetary\nstrategies of emerging economies, through case studies of Brazil, India and\nNigeria, reveals how these countries use tools such as interest rates, foreign\nexchange interventions and capital controls to manage the impacts of crises and\nfluctuations in dominant currencies. The article also highlights the challenges\nand limitations faced by these countries, including structural and\ninstitutional constraints and the reactions of international financial\nmarkets.Finally, an econometric analysis using a Vector AutoRegression (VAR)\nmodel illustrates the impact of monetary policies on key economic variables,\nsuch as GDP, interest rates, inflation and exchange rates. The results show\nthat emerging economies, although sensitive to external shocks, can adjust\ntheir policies to stabilize economic growth in the medium and long term.", "published": "2024-10-30 13:29:48", "link": "http://arxiv.org/abs/2410.23002v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Rebalancing-versus-Rebalancing: Improving the fidelity of Loss-versus-Rebalancing", "abstract": "Automated Market Makers (AMMs) hold assets and are constantly being\nrebalanced by external arbitrageurs to match external market prices.\nLoss-versus-rebalancing (LVR) is a pivotal metric for measuring how an AMM pool\nperforms for its liquidity providers (LPs) relative to an idealised benchmark\nwhere rebalancing is done not via the action of arbitrageurs but instead by\ntrading with a perfect centralised exchange with no fees, spread or slippage.\nThis renders it an imperfect tool for judging rebalancing efficiency between\nexecution platforms.\n  We introduce Rebalancing-versus-rebalancing (RVR), a higher-fidelity model\nthat better captures the frictions present in centralised rebalancing.\n  We perform a battery of experiments comparing managing a portfolio on AMMs vs\nthis new and more realistic centralised exchange benchmark-RVR. We are also\nparticularly interested in dynamic AMMs that run strategies beyond fixed weight\nallocations-Temporal Function Market Makers. This is particularly important for\nasset managers evaluating execution management systems. In this paper we\nsimulate more than 1000 different strategies settings as well as testing\nhundreds of different variations in centralised exchange (CEX) fees, AMM fees &\ngas costs.\n  We find that, under this modeling approach, AMM pools (even with no\nretail/noise traders) often offer superior execution and rebalancing efficiency\ncompared to centralised rebalancing, for all but the lowest CEX fee levels. We\nalso take a simple approach to model noise traders & find that even a small\namount of noise volume increases modeled AMM performance such that CEX\nrebalancing finds it hard to compete. This indicates that decentralised\nAMM-based asset management can offer superior performance and execution\nmanagement for asset managers looking to rebalance portfolios, offering an\nalternative use case for dynamic AMMs beyond core liquidity providing.", "published": "2024-10-30 19:14:59", "link": "http://arxiv.org/abs/2410.23404v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Linguistics Theory Meets LLM: Code-Switched Text Generation via\n  Equivalence Constrained Large Language Models", "abstract": "Code-switching, the phenomenon of alternating between two or more languages\nin a single conversation, presents unique challenges for Natural Language\nProcessing (NLP). Most existing research focuses on either syntactic\nconstraints or neural generation, with few efforts to integrate linguistic\ntheory with large language models (LLMs) for generating natural code-switched\ntext. In this paper, we introduce EZSwitch, a novel framework that combines\nEquivalence Constraint Theory (ECT) with LLMs to produce linguistically valid\nand fluent code-switched text. We evaluate our method using both human\njudgments and automatic metrics, demonstrating a significant improvement in the\nquality of generated code-switching sentences compared to baseline LLMs. To\naddress the lack of suitable evaluation metrics, we conduct a comprehensive\ncorrelation study of various automatic metrics against human scores, revealing\nthat current metrics often fail to capture the nuanced fluency of code-switched\ntext. Additionally, we create CSPref, a human preference dataset based on human\nratings and analyze model performance across ``hard`` and ``easy`` examples.\nOur findings indicate that incorporating linguistic constraints into LLMs leads\nto more robust and human-aligned generation, paving the way for scalable\ncode-switching text generation across diverse language pairs.", "published": "2024-10-30 03:03:32", "link": "http://arxiv.org/abs/2410.22660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Multimodal Datasets from Scratch for Rapid Development of a\n  Japanese Visual Language Model", "abstract": "To develop high-performing Visual Language Models (VLMs), it is essential to\nprepare multimodal resources, such as image-text pairs, interleaved data, and\ninstruction data. While multimodal resources for English are abundant, there is\na significant lack of corresponding resources for non-English languages, such\nas Japanese. To address this problem, we take Japanese as a non-English\nlanguage and propose a method for rapidly creating Japanese multimodal datasets\nfrom scratch. We collect Japanese image-text pairs and interleaved data from\nweb archives and generate Japanese instruction data directly from images using\nan existing VLM. Our experimental results show that a VLM trained on these\nnative datasets outperforms those relying on machine-translated content.", "published": "2024-10-30 06:46:33", "link": "http://arxiv.org/abs/2410.22736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do Large Language Models Disambiguate Swedish Words?", "abstract": "We evaluate a battery of recent large language models on two benchmarks for\nword sense disambiguation in Swedish. At present, all current models are less\naccurate than the best supervised disambiguators in cases where a training set\nis available, but most models outperform graph-based unsupervised systems.\nDifferent prompting approaches are compared, with a focus on how to express the\nset of possible senses in a given context. The best accuracies are achieved\nwhen human-written definitions of the senses are included in the prompts.", "published": "2024-10-30 09:10:41", "link": "http://arxiv.org/abs/2410.22827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining psychoanalysis and computer science: an empirical study of the\n  relationship between emotions and the Lacanian discourses", "abstract": "This research explores the interdisciplinary interaction between\npsychoanalysis and computer science, suggesting a mutually beneficial exchange.\nIndeed, psychoanalytic concepts can enrich technological applications involving\nunconscious, elusive aspects of the human factor, such as social media and\nother interactive digital platforms. Conversely, computer science, especially\nArtificial Intelligence (AI), can contribute quantitative concepts and methods\nto psychoanalysis, identifying patterns and emotional cues in human expression.\nIn particular, this research aims to apply computer science methods to\nestablish fundamental relationships between emotions and Lacanian discourses.\nSuch relations are discovered in our approach via empirical investigation and\nstatistical analysis, and are eventually validated in a theoretical\n(psychoanalytic) way. It is worth noting that, although emotions have been\nsporadically studied in Lacanian theory, to the best of our knowledge a\nsystematic, detailed investigation of their role is missing. Such fine-grained\nunderstanding of the role of emotions can also make the identification of\nLacanian discourses more effective and easy in practise. In particular, our\nmethods indicate the emotions with highest differentiation power in terms of\ncorresponding discourses; conversely, we identify for each discourse the most\ncharacteristic emotions it admits. As a matter of fact, we develop a method\nwhich we call Lacanian Discourse Discovery (LDD), that simplifies (via\nsystematizing) the identification of Lacanian discourses in texts. Although the\nmain contribution of this paper is inherently theoretical (psychoanalytic), it\ncan also facilitate major practical applications in the realm of interactive\ndigital systems. Indeed, our approach can be automated through Artificial\nIntelligence methods that effectively identify emotions (and corresponding\ndiscourses) in texts.", "published": "2024-10-30 10:49:33", "link": "http://arxiv.org/abs/2410.22895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Babble to Words: Pre-Training Language Models on Continuous Streams\n  of Phonemes", "abstract": "Language models are typically trained on large corpora of text in their\ndefault orthographic form. However, this is not the only option; representing\ndata as streams of phonemes can offer unique advantages, from deeper insights\ninto phonological language acquisition to improved performance on sound-based\ntasks. The challenge lies in evaluating the impact of phoneme-based training,\nas most benchmarks are also orthographic. To address this, we develop a\npipeline to convert text datasets into a continuous stream of phonemes. We\napply this pipeline to the 100-million-word pre-training dataset from the\nBabyLM challenge, as well as to standard language and grammatical benchmarks,\nenabling us to pre-train and evaluate a model using phonemic input\nrepresentations. Our results show that while phoneme-based training slightly\nreduces performance on traditional language understanding tasks, it offers\nvaluable analytical and practical benefits.", "published": "2024-10-30 11:05:01", "link": "http://arxiv.org/abs/2410.22906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Behavior Cloning: Teaching Large Language Model Agents\n  through Learning by Demonstration", "abstract": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations.", "published": "2024-10-30 11:14:33", "link": "http://arxiv.org/abs/2410.22916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Large Language Models for Conversational Task-Solving", "abstract": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.", "published": "2024-10-30 11:38:13", "link": "http://arxiv.org/abs/2410.22932v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Private Synthetic Text Generation with Diffusion Models", "abstract": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research.", "published": "2024-10-30 12:38:49", "link": "http://arxiv.org/abs/2410.22971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based\n  Encoder For Legal Violation Detection and Resolution", "abstract": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts.", "published": "2024-10-30 12:42:38", "link": "http://arxiv.org/abs/2410.22977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall", "abstract": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.", "published": "2024-10-30 13:29:36", "link": "http://arxiv.org/abs/2410.23000v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Memorization of Large Language Models in Logical Reasoning", "abstract": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolate\nand memorize the training puzzles (achieving near-perfect accuracy) after\nfine-tuning, yet they struggle with slight variations of these puzzles. On the\nother hand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. Through in-depth analyses\nwith perturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers, we establish that LLMs develop\nreasoning skills on K&K puzzles alongside memorization. Finally, our analysis\nbased on a per-sample memorization score sheds light on how LLMs switch between\nreasoning and memorization when solving logical puzzles. Our code and data are\navailable at https://memkklogic.github.io.", "published": "2024-10-30 15:31:54", "link": "http://arxiv.org/abs/2410.23123v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Lexical Diversity", "abstract": "Lexical-semantic resources (LSRs), such as online lexicons or wordnets, are\nfundamental for natural language processing applications. In many languages,\nhowever, such resources suffer from quality issues: incorrect entries,\nincompleteness, but also, the rarely addressed issue of bias towards the\nEnglish language and Anglo-Saxon culture. Such bias manifests itself in the\nabsence of concepts specific to the language or culture at hand, the presence\nof foreign (Anglo-Saxon) concepts, as well as in the lack of an explicit\nindication of untranslatability, also known as cross-lingual \\emph{lexical\ngaps}, when a term has no equivalent in another language. This paper proposes a\nnovel crowdsourcing methodology for reducing bias in LSRs. Crowd workers\ncompare lexemes from two languages, focusing on domains rich in lexical\ndiversity, such as kinship or food. Our LingoGap crowdsourcing tool facilitates\ncomparisons through microtasks identifying equivalent terms, language-specific\nterms, and lexical gaps across languages. We validated our method by applying\nit to two case studies focused on food-related terminology: (1) English and\nArabic, and (2) Standard Indonesian and Banjarese. These experiments identified\n2,140 lexical gaps in the first case study and 951 in the second. The success\nof these experiments confirmed the usability of our method and tool for future\nlarge-scale lexicon enrichment tasks.", "published": "2024-10-30 15:45:09", "link": "http://arxiv.org/abs/2410.23133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliability of Topic Modeling", "abstract": "Topic models allow researchers to extract latent factors from text data and\nuse those variables in downstream statistical analyses. However, these\nmethodologies can vary significantly due to initialization differences,\nrandomness in sampling procedures, or noisy data. Reliability of these methods\nis of particular concern as many researchers treat learned topic models as\nground truth for subsequent analyses. In this work, we show that the standard\npractice for quantifying topic model reliability fails to capture essential\naspects of the variation in two widely-used topic models. Drawing from a\nextensive literature on measurement theory, we provide empirical and\ntheoretical analyses of three other metrics for evaluating the reliability of\ntopic models. On synthetic and real-world data, we show that McDonald's\n$\\omega$ provides the best encapsulation of reliability. This metric provides\nan essential tool for validation of topic model methodologies that should be a\nstandard component of any topic model-based research.", "published": "2024-10-30 16:42:04", "link": "http://arxiv.org/abs/2410.23186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Cultural and Social Awareness of LLM Web Agents", "abstract": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle.", "published": "2024-10-30 17:35:44", "link": "http://arxiv.org/abs/2410.23252v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Models Help Us Create Better Models? Evaluating LLMs as Data\n  Scientists", "abstract": "We present a benchmark for large language models designed to tackle one of\nthe most knowledge-intensive tasks in data science: writing feature engineering\ncode, which requires domain knowledge in addition to a deep understanding of\nthe underlying problem and data structure. The model is provided with a dataset\ndescription in a prompt and asked to generate code transforming it. The\nevaluation score is derived from the improvement achieved by an XGBoost model\nfit on the modified dataset compared to the original data. By an extensive\nevaluation of state-of-the-art models and comparison to well-established\nbenchmarks, we demonstrate that the FeatEng of our proposal can cheaply and\nefficiently assess the broad capabilities of LLMs, in contrast to the existing\nmethods.", "published": "2024-10-30 17:59:01", "link": "http://arxiv.org/abs/2410.23331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Language Models and Bandit Algorithms to Drive Adoption of\n  Battery-Electric Vehicles", "abstract": "Behavior change interventions are important to coordinate societal action\nacross a wide array of important applications, including the adoption of\nelectrified vehicles to reduce emissions. Prior work has demonstrated that\ninterventions for behavior must be personalized, and that the intervention that\nis most effective on average across a large group can result in a backlash\neffect that strengthens opposition among some subgroups. Thus, it is important\nto target interventions to different audiences, and to present them in a\nnatural, conversational style. In this context, an important emerging\napplication domain for large language models (LLMs) is conversational\ninterventions for behavior change. In this work, we leverage prior work on\nunderstanding values motivating the adoption of battery electric vehicles. We\nleverage new advances in LLMs, combined with a contextual bandit, to develop\nconversational interventions that are personalized to the values of each study\nparticipant. We use a contextual bandit algorithm to learn to target values\nbased on the demographics of each participant. To train our bandit algorithm in\nan offline manner, we leverage LLMs to play the role of study participants. We\nbenchmark the persuasive effectiveness of our bandit-enhanced LLM against an\nunaided LLM generating conversational interventions without\ndemographic-targeted values.", "published": "2024-10-30 18:15:43", "link": "http://arxiv.org/abs/2410.23371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Science Meets LLMs: How Reliable Are Large Language Models in\n  Social Simulations?", "abstract": "Large Language Models (LLMs) are increasingly employed for simulations,\nenabling applications in role-playing agents and Computational Social Science\n(CSS). However, the reliability of these simulations is under-explored, which\nraises concerns about the trustworthiness of LLMs in these applications. In\nthis paper, we aim to answer ``How reliable is LLM-based simulation?'' To\naddress this, we introduce TrustSim, an evaluation dataset covering 10\nCSS-related topics, to systematically investigate the reliability of the LLM\nsimulation. We conducted experiments on 14 LLMs and found that inconsistencies\npersist in the LLM-based simulated roles. In addition, the consistency level of\nLLMs does not strongly correlate with their general performance. To enhance the\nreliability of LLMs in simulation, we proposed Adaptive Learning Rate Based\nORPO (AdaORPO), a reinforcement learning-based algorithm to improve the\nreliability in simulation across 7 LLMs. Our research provides a foundation for\nfuture studies to explore more robust and trustworthy LLM-based simulations.", "published": "2024-10-30 20:09:37", "link": "http://arxiv.org/abs/2410.23426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smaller Large Language Models Can Do Moral Self-Correction", "abstract": "Self-correction is one of the most amazing emerging capabilities of Large\nLanguage Models (LLMs), enabling LLMs to self-modify an inappropriate output\ngiven a natural language feedback which describes the problems of that output.\nMoral self-correction is a post-hoc approach correcting unethical generations\nwithout requiring a gradient update, making it both computationally lightweight\nand capable of preserving the language modeling ability. Previous works have\nshown that LLMs can self-debias, and it has been reported that small models,\ni.e., those with less than 22B parameters, are not capable of moral\nself-correction. However, there is no direct proof as to why such smaller\nmodels fall short of moral self-correction, though previous research\nhypothesizes that larger models are skilled in following instructions and\nunderstanding abstract social norms. In this paper, we empirically validate\nthis hypothesis in the context of social stereotyping, through meticulous\nprompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs\nwith proper safety alignment fine-tuning can achieve very good moral\nself-correction performance, highlighting the significant effects of safety\nalignment; and (ii) small LLMs are indeed weaker than larger-scale models in\nterms of comprehending social norms and self-explanation through CoT, but all\nscales of LLMs show bad self-correction performance given unethical\ninstructions.", "published": "2024-10-30 22:58:57", "link": "http://arxiv.org/abs/2410.23496v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient and Interpretable Grammatical Error Correction with Mixture of\n  Experts", "abstract": "Error type information has been widely used to improve the performance of\ngrammatical error correction (GEC) models, whether for generating corrections,\nre-ranking them, or combining GEC models. Combining GEC models that have\ncomplementary strengths in correcting different error types is very effective\nin producing better corrections. However, system combination incurs a high\ncomputational cost due to the need to run inference on the base systems before\nrunning the combination method itself. Therefore, it would be more efficient to\nhave a single model with multiple sub-networks that specialize in correcting\ndifferent error types. In this paper, we propose a mixture-of-experts model,\nMoECE, for grammatical error correction. Our model successfully achieves the\nperformance of T5-XL with three times fewer effective parameters. Additionally,\nour model produces interpretable corrections by also identifying the error type\nduring inference.", "published": "2024-10-30 23:27:54", "link": "http://arxiv.org/abs/2410.23507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural spell-checker: Beyond words with synthetic data generation", "abstract": "Spell-checkers are valuable tools that enhance communication by identifying\nmisspelled words in written texts. Recent improvements in deep learning, and in\nparticular in large language models, have opened new opportunities to improve\ntraditional spell-checkers with new functionalities that not only assess\nspelling correctness but also the suitability of a word for a given context. In\nour work, we present and compare two new spell-checkers and evaluate them on\nsynthetic, learner, and more general-domain Slovene datasets. The first\nspell-checker is a traditional, fast, word-based approach, based on a\nmorphological lexicon with a significantly larger word list compared to\nexisting spell-checkers. The second approach uses a language model trained on a\nlarge corpus with synthetically inserted errors. We present the training data\nconstruction strategies, which turn out to be a crucial component of neural\nspell-checkers. Further, the proposed neural model significantly outperforms\nall existing spell-checkers for Slovene in both precision and recall.", "published": "2024-10-30 23:51:01", "link": "http://arxiv.org/abs/2410.23514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prove Your Point!: Bringing Proof-Enhancement Principles to\n  Argumentative Essay Generation", "abstract": "Argumentative essay generation (AEG) aims to generate complete texts on\nspecific controversial topics or debates. Although current AEG methods can\ngenerate individual opinions, they often overlook the high-level connections\nbetween these opinions. This often leads to the generated results being mired\nin logical confusion, unable to proof their own arguments effectively. The\ngenerated essay may present evidence that contradicts the claims or they may\nfail to assemble the claims into logical flow. In this paper, we present a\nunified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for\nAEG with a focus on logical enhancement. Specifically, we first construct\npseudo-labels for logical information,claims and grounds, using a large\nlanguage model. We then propose a tree planning approach that introduces proof\nprinciples and ensures logical consistency. Extensive experimental results show\nthat, benefiting from proof principle guidance, PESA generates argumentative\nessays with better logical validity and persuasiveness than strong baseline\nmodels.", "published": "2024-10-30 02:13:39", "link": "http://arxiv.org/abs/2410.22642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot", "abstract": "Goal-oriented chatbots are essential for automating user tasks, such as\nbooking flights or making restaurant reservations. A key component of these\nsystems is Dialogue State Tracking (DST), which interprets user intent and\nmaintains the dialogue state. However, existing DST methods often rely on fixed\nontologies and manually compiled slot values, limiting their adaptability to\nopen-domain dialogues. We propose a novel approach that leverages instruction\ntuning and advanced prompt strategies to enhance DST performance, without\nrelying on any predefined ontologies. Our method enables Large Language Model\n(LLM) to infer dialogue states through carefully designed prompts and includes\nan anti-hallucination mechanism to ensure accurate tracking in diverse\nconversation contexts. Additionally, we employ a Variational Graph Auto-Encoder\n(VGAE) to model and predict subsequent user intent. Our approach achieved\nstate-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST\nmodels, and performed well in open-domain real-world conversations. This work\npresents a significant advancement in creating more adaptive and accurate\ngoal-oriented chatbots.", "published": "2024-10-30 07:36:23", "link": "http://arxiv.org/abs/2410.22767v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced\n  Multi-Task Learning", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly\nimproved the adaptation of LLMs to downstream tasks in a resource-efficient\nmanner. However, in multi-task scenarios, challenges such as training imbalance\nand the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which\ncombines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by\npromoting task-specific learning across experts. Despite this, MoLoRA remains\ninefficient in terms of training speed, parameter utilization, and overall\nmulti-task performance. In this paper, we propose Mixture of Asymmetric\nLow-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages\nasymmetric optimization across LoRA experts. MALoRA reduces the number of\ntrainable parameters by 30% to 48%, increases training speed by 1.2x, and\nmatches the computational efficiency of single-task LoRA models. Additionally,\nMALoRA addresses overfitting issues commonly seen in high-rank configurations,\nenhancing performance stability. Extensive experiments across diverse\nmulti-task learning scenarios demonstrate that MALoRA consistently outperforms\nall baseline methods in both inter-domain and intra-domain tasks.", "published": "2024-10-30 07:53:52", "link": "http://arxiv.org/abs/2410.22782v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific\n  Evaluations", "abstract": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released.", "published": "2024-10-30 08:57:59", "link": "http://arxiv.org/abs/2410.22821v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via\n  Contrastive Explanations", "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.", "published": "2024-10-30 10:11:53", "link": "http://arxiv.org/abs/2410.22874v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models\n  with Cognitively-Plausible Curriculum Learning Strategies", "abstract": "Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories.", "published": "2024-10-30 10:31:54", "link": "http://arxiv.org/abs/2410.22886v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune\n  Attention in Extreme Multi-Label Text Classification", "abstract": "State-of-the-art Extreme Multi-Label Text Classification (XMTC) models rely\nheavily on multi-label attention layers to focus on key tokens in input text,\nbut obtaining optimal attention weights is challenging and resource-intensive.\nTo address this, we introduce PLANT -- Pretrained and Leveraged AtteNTion -- a\nnovel transfer learning strategy for fine-tuning XMTC decoders. PLANT surpasses\nexisting state-of-the-art methods across all metrics on mimicfull, mimicfifty,\nmimicfour, eurlex, and wikiten datasets. It particularly excels in few-shot\nscenarios, outperforming previous models specifically designed for few-shot\nscenarios by over 50 percentage points in F1 scores on mimicrare and by over 36\npercentage points on mimicfew, demonstrating its superior capability in\nhandling rare codes. PLANT also shows remarkable data efficiency in few-shot\nscenarios, achieving precision comparable to traditional models with\nsignificantly less data. These results are achieved through key technical\ninnovations: leveraging a pretrained Learning-to-Rank model as the planted\nattention layer, integrating mutual-information gain to enhance attention,\nintroducing an inattention mechanism, and implementing a stateful-decoder to\nmaintain context. Comprehensive ablation studies validate the importance of\nthese contributions in realizing the performance gains.", "published": "2024-10-30 14:41:23", "link": "http://arxiv.org/abs/2410.23066v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Programming Language Sandbox for LLMs", "abstract": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.", "published": "2024-10-30 14:46:43", "link": "http://arxiv.org/abs/2410.23074v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference", "abstract": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.", "published": "2024-10-30 14:53:37", "link": "http://arxiv.org/abs/2410.23079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation", "abstract": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.", "published": "2024-10-30 15:06:32", "link": "http://arxiv.org/abs/2410.23090v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Teaching a Language Model to Distinguish Between Similar Details using a\n  Small Adversarial Training Set", "abstract": "Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity).", "published": "2024-10-30 15:27:55", "link": "http://arxiv.org/abs/2410.23118v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources", "abstract": "Pre-training is notoriously compute-intensive and academic researchers are\nnotoriously under-resourced. It is, therefore, commonly assumed that academics\ncan't pre-train models. In this paper, we seek to clarify this assumption. We\nfirst survey academic researchers to learn about their available compute and\nthen empirically measure the time to replicate models on such resources. We\nintroduce a benchmark to measure the time to pre-train models on given GPUs and\nalso identify ideal settings for maximizing training speed. We run our\nbenchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on\nour experiments. Our results reveal a brighter picture for academic\npre-training: for example, although Pythia-1B was originally trained on 64 GPUs\nfor 3 days, we find it is also possible to replicate this model (with the same\nhyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude\nwith a cost-benefit analysis to help clarify the trade-offs between price and\npre-training time. We believe our benchmark will help academic researchers\nconduct experiments that require training larger models on more data. We fully\nrelease our codebase at: https://github.com/apoorvkh/academic-pretraining.", "published": "2024-10-30 17:46:20", "link": "http://arxiv.org/abs/2410.23261v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning and Transferring Sparse Contextual Bigrams with Linear\n  Transformers", "abstract": "Transformers have excelled in natural language modeling and one reason behind\nthis success is their exceptional ability to combine contextual informal and\nglobal knowledge. However, the theoretical basis remains unclear. In this\npaper, first we introduce the Sparse Contextual Bigram (SCB), a natural\nextension of the classical bigram model, where the next token's generation\ndepends on a sparse set of earlier positions determined by the last token. We\nthen analyze the training dynamics and sample complexity of learning SCB using\na one-layer linear transformer with a gradient-based algorithm. We show that\nwhen trained from scratch, the training process can be split into an initial\nsample-intensive stage where the correlation is boosted from zero to a\nnontrivial value, followed by a more sample-efficient stage of further\nimprovement. Additionally, we prove that, provided a nontrivial correlation\nbetween the downstream and pretraining tasks, finetuning from a pretrained\nmodel allows us to bypass the initial sample-intensive stage. We also\nempirically demonstrate that our algorithm can outperform SGD in this setting\nand discuss its relationship with the usual softmax-based transformers.", "published": "2024-10-30 20:29:10", "link": "http://arxiv.org/abs/2410.23438v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Graph-Augmented Relation Extraction Model with LLMs-Generated Support\n  Document", "abstract": "This study introduces a novel approach to sentence-level relation extraction\n(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models\n(LLMs) to generate contextually enriched support documents. By harnessing the\npower of LLMs to generate auxiliary information, our approach crafts an\nintricate graph representation of textual data. This graph is subsequently\nprocessed through a Graph Neural Network (GNN) to refine and enrich the\nembeddings associated with each entity ensuring a more nuanced and\ninterconnected understanding of the data. This methodology addresses the\nlimitations of traditional sentence-level RE models by incorporating broader\ncontexts and leveraging inter-entity interactions, thereby improving the\nmodel's ability to capture complex relationships across sentences. Our\nexperiments, conducted on the CrossRE dataset, demonstrate the effectiveness of\nour approach, with notable improvements in performance across various domains.\nThe results underscore the potential of combining GNNs with LLM-generated\ncontext to advance the field of relation extraction.", "published": "2024-10-30 20:48:34", "link": "http://arxiv.org/abs/2410.23452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following", "abstract": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nchallenges, such as managing inter-document dependencies, redundancy, and\nincoherent structures. We introduce MDCure, a scalable and effective\nfine-tuning pipeline to enhance the MD capabilities of LLMs without the\ncomputational cost of pre-training or reliance on human annotated data. MDCure\nis based on generation of high-quality synthetic MD instruction data from sets\nof related articles via targeted prompts. We further introduce MDCureRM, a\nmulti-objective reward model which filters generated data based on their\ntraining utility for MD settings. With MDCure, we fine-tune a variety of LLMs,\nfrom the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in\nsize. Extensive evaluations on a wide range of MD and long-context benchmarks\nspanning various tasks show MDCure consistently improves performance over\npre-trained baselines and over corresponding base models by up to 75.5%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure.", "published": "2024-10-30 21:08:07", "link": "http://arxiv.org/abs/2410.23463v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Collage: Decomposable Rapid Prototyping for Information Extraction on\n  Scientific PDFs", "abstract": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.", "published": "2024-10-30 22:00:34", "link": "http://arxiv.org/abs/2410.23478v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Tiny Transformers Excel at Sentence Compression", "abstract": "It is staggering that words of the English language, which are on average\nrepresented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served\nto large language models. We show that there is room for more information in\nevery token embedding. We demonstrate that 1--3-layer transformers are capable\nof encoding and subsequently decoding standard English sentences into as little\nas a single 3-kilobyte token. Our work implies that even small networks can\nlearn to construct valid English sentences and suggests the possibility of\noptimising large language models by moving from sub-word token embeddings\ntowards larger fragments of text.", "published": "2024-10-30 23:34:45", "link": "http://arxiv.org/abs/2410.23510v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Larger models yield better results? Streamlined severity classification\n  of ADHD-related concerns using BERT-based knowledge distillation", "abstract": "This work focuses on the efficiency of the knowledge distillation approach in\ngenerating a lightweight yet powerful BERT based model for natural language\nprocessing applications. After the model creation, we applied the resulting\nmodel, LastBERT, to a real-world task classifying severity levels of Attention\nDeficit Hyperactivity Disorder (ADHD)-related concerns from social media text\ndata. Referring to LastBERT, a customized student BERT model, we significantly\nlowered model parameters from 110 million BERT base to 29 million, resulting in\na model approximately 73.64% smaller. On the GLUE benchmark, comprising\nparaphrase identification, sentiment analysis, and text classification, the\nstudent model maintained strong performance across many tasks despite this\nreduction. The model was also used on a real-world ADHD dataset with an\naccuracy and F1 score of 85%. When compared to DistilBERT (66M) and\nClinicalBERT (110M), LastBERT demonstrated comparable performance, with\nDistilBERT slightly outperforming it at 87%, and ClinicalBERT achieving 86%\nacross the same metrics. These findings highlight the LastBERT model's capacity\nto classify degrees of ADHD severity properly, so it offers a useful tool for\nmental health professionals to assess and comprehend material produced by users\non social networking platforms. The study emphasizes the possibilities of\nknowledge distillation to produce effective models fit for use in\nresource-limited conditions, hence advancing NLP and mental health diagnosis.\nFurthermore underlined by the considerable decrease in model size without\nappreciable performance loss is the lower computational resources needed for\ntraining and deployment, hence facilitating greater applicability. Especially\nusing readily available computational tools like Google Colab. This study shows\nthe accessibility and usefulness of advanced NLP methods in pragmatic world\napplications.", "published": "2024-10-30 17:57:44", "link": "http://arxiv.org/abs/2411.00052v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration", "abstract": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.", "published": "2024-10-30 19:09:02", "link": "http://arxiv.org/abs/2411.00053v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Diverse Negations from Affirmative Sentences", "abstract": "Despite the impressive performance of large language models across various\ntasks, they often struggle with reasoning under negated statements. Negations\nare important in real-world applications as they encode negative polarity in\nverb phrases, clauses, or other expressions. Nevertheless, they are\nunderrepresented in current benchmarks, which mainly include basic negation\nforms and overlook more complex ones, resulting in insufficient data for\ntraining a language model. In this work, we propose NegVerse, a method that\ntackles the lack of negation datasets by producing a diverse range of negation\ntypes from affirmative sentences, including verbal, non-verbal, and affixal\nforms commonly found in English text. We provide new rules for masking parts of\nsentences where negations are most likely to occur, based on syntactic\nstructure and use a frozen baseline LLM and prompt tuning to generate negated\nsentences. We also propose a filtering mechanism to identify negation cues and\nremove degenerate examples, producing a diverse range of meaningful\nperturbations. Our results show that NegVerse outperforms existing methods and\ngenerates negations with higher lexical similarity to the original sentences,\nbetter syntactic preservation and negation diversity. The code is available in\nhttps://github.com/DarianRodriguez/NegVerse", "published": "2024-10-30 21:25:02", "link": "http://arxiv.org/abs/2411.00056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Survey of Cultural Awareness in Language Models: Text and Beyond", "abstract": "Large-scale deployment of large language models (LLMs) in various\napplications, such as chatbots and virtual assistants, requires LLMs to be\nculturally sensitive to the user to ensure inclusivity. Culture has been widely\nstudied in psychology and anthropology, and there has been a recent surge in\nresearch on making LLMs more culturally inclusive in LLMs that goes beyond\nmultilinguality and builds on findings from psychology and anthropology. In\nthis paper, we survey efforts towards incorporating cultural awareness into\ntext-based and multimodal LLMs. We start by defining cultural awareness in\nLLMs, taking the definitions of culture from anthropology and psychology as a\npoint of departure. We then examine methodologies adopted for creating\ncross-cultural datasets, strategies for cultural inclusion in downstream tasks,\nand methodologies that have been used for benchmarking cultural awareness in\nLLMs. Further, we discuss the ethical implications of cultural alignment, the\nrole of Human-Computer Interaction in driving cultural inclusion in LLMs, and\nthe role of cultural alignment in driving social science research. We finally\nprovide pointers to future research based on our findings about gaps in the\nliterature.", "published": "2024-10-30 16:37:50", "link": "http://arxiv.org/abs/2411.00860v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Next-Token Prediction Task Assumes Optimal Data Ordering for LLM\n  Training in Proof Generation", "abstract": "In the field of large language model (LLM)-based proof generation, despite\nbeing trained on extensive corpora such as OpenWebMath and Arxiv, these models\nstill exhibit only modest performance on proving tasks of moderate difficulty.\nWe believe that this is partly due to the suboptimal order of each proof data\nused in training. Published proofs often follow a purely logical order, where\neach step logically proceeds from the previous steps based on the deductive\nrules. However, this order aims to facilitate the verification of the proof's\nsoundness, rather than to help people and models learn the discovery process of\nthe proof. In proof generation, we argue that the optimal order for one\ntraining data sample occurs when the relevant intermediate supervision for a\nparticular proof step in the proof is always positioned to the left of that\nproof step. We call such order the intuitively sequential order. We validate\nour claims using two tasks: intuitionistic propositional logic theorem-proving\nand digit multiplication. Our experiments verify the order effect and provide\nsupport for our explanations. We demonstrate that training is most effective\nwhen the proof is in the intuitively sequential order. Moreover, the order\neffect and the performance gap between models trained on different data orders\nare substantial -- with an 11 percent improvement in proof success rate\nobserved in the propositional logic theorem-proving task, between models\ntrained on the optimal order compared to the worst order.", "published": "2024-10-30 18:00:04", "link": "http://arxiv.org/abs/2411.00863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Learning and Machine Learning -- Natural Language Processing: From\n  Theory to Application", "abstract": "With a focus on natural language processing (NLP) and the role of large\nlanguage models (LLMs), we explore the intersection of machine learning, deep\nlearning, and artificial intelligence. As artificial intelligence continues to\nrevolutionize fields from healthcare to finance, NLP techniques such as\ntokenization, text classification, and entity recognition are essential for\nprocessing and understanding human language. This paper discusses advanced data\npreprocessing techniques and the use of frameworks like Hugging Face for\nimplementing transformer-based models. Additionally, it highlights challenges\nsuch as handling multilingual data, reducing bias, and ensuring model\nrobustness. By addressing key aspects of data processing and model fine-tuning,\nthis work aims to provide insights into deploying effective and ethically sound\nAI solutions.", "published": "2024-10-30 09:35:35", "link": "http://arxiv.org/abs/2411.05026v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automated Trustworthiness Oracle Generation for Machine Learning Text\n  Classifiers", "abstract": "Machine learning (ML) for text classification has been widely used in various\ndomains. These applications can significantly impact ethics, economics, and\nhuman behavior, raising serious concerns about trusting ML decisions. Studies\nindicate that conventional metrics are insufficient to build human trust in ML\nmodels. These models often learn spurious correlations and predict based on\nthem. In the real world, their performance can deteriorate significantly. To\navoid this, a common practice is to test whether predictions are reasonable\nbased on valid patterns in the data. Along with this, a challenge known as the\ntrustworthiness oracle problem has been introduced. Due to the lack of\nautomated trustworthiness oracles, the assessment requires manual validation of\nthe decision process disclosed by explanation methods. However, this is\ntime-consuming, error-prone, and unscalable.\n  We propose TOKI, the first automated trustworthiness oracle generation method\nfor text classifiers. TOKI automatically checks whether the words contributing\nthe most to a prediction are semantically related to the predicted class.\nSpecifically, we leverage ML explanations to extract the decision-contributing\nwords and measure their semantic relatedness with the class based on word\nembeddings. We also introduce a novel adversarial attack method that targets\ntrustworthiness vulnerabilities identified by TOKI. To evaluate their alignment\nwith human judgement, experiments are conducted. We compare TOKI with a naive\nbaseline based solely on model confidence and TOKI-guided adversarial attack\nmethod with A2T, a SOTA adversarial attack method. Results show that relying on\nprediction uncertainty cannot effectively distinguish between trustworthy and\nuntrustworthy predictions, TOKI achieves 142% higher accuracy than the naive\nbaseline, and TOKI-guided attack method is more effective with fewer\nperturbations than A2T.", "published": "2024-10-30 03:26:37", "link": "http://arxiv.org/abs/2410.22663v2", "categories": ["cs.SE", "cs.CL", "cs.CR"], "primary_category": "cs.SE"}
{"title": "Improving Uncertainty Quantification in Large Language Models via\n  Semantic Embeddings", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial\nfor their reliable deployment, especially in high-stakes applications. Current\nstate-of-the-art methods for measuring semantic uncertainty in LLMs rely on\nstrict bidirectional entailment criteria between multiple generated responses\nand also depend on sequence likelihoods. While effective, these approaches\noften overestimate uncertainty due to their sensitivity to minor wording\ndifferences, additional correct information, and non-important words in the\nsequence. We propose a novel approach that leverages semantic embeddings to\nachieve smoother and more robust estimation of semantic uncertainty in LLMs. By\ncapturing semantic similarities without depending on sequence likelihoods, our\nmethod inherently reduces any biases introduced by irrelevant words in the\nanswers. Furthermore, we introduce an amortised version of our approach by\nexplicitly modelling semantics as latent variables in a joint probabilistic\nmodel. This allows for uncertainty estimation in the embedding space with a\nsingle forward pass, significantly reducing computational overhead compared to\nexisting multi-pass methods. Experiments across multiple question-answering\ndatasets and frontier LLMs demonstrate that our embedding-based methods provide\nmore accurate and nuanced uncertainty quantification than traditional\napproaches.", "published": "2024-10-30 04:41:46", "link": "http://arxiv.org/abs/2410.22685v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection\n  Guardrail Models", "abstract": "Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/leolee99/InjecGuard.", "published": "2024-10-30 07:39:42", "link": "http://arxiv.org/abs/2410.22770v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Danoliteracy of Generative Large Language Models", "abstract": "The language technology moonshot moment of Generative Large Language Models\n(GLLMs) was not limited to English: These models brought a surge of\ntechnological applications, investments, and hype to low-resource languages as\nwell. However, the capabilities of these models in languages such as Danish\nwere, until recently, difficult to verify beyond qualitative demonstrations due\nto a lack of applicable evaluation corpora. We present a GLLM benchmark to\nevaluate \\emph{Danoliteracy}, a measure of Danish language and cultural\ncompetency across eight diverse scenarios such as Danish citizenship tests and\nabstractive social media question answering. This limited-size benchmark was\nfound to produce a robust ranking that correlates to human feedback at $\\rho\n\\sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.\nAnalyzing these model results across scenarios, we find one strong underlying\nfactor explaining $95\\%$ of scenario performance variance for GLLMs in Danish,\nsuggesting a $g$ factor of model consistency in language adaptation.", "published": "2024-10-30 09:18:31", "link": "http://arxiv.org/abs/2410.22839v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Stealing User Prompts from Mixture of Experts", "abstract": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.", "published": "2024-10-30 10:25:35", "link": "http://arxiv.org/abs/2410.22884v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Effective and Efficient Adversarial Detection for Vision-Language Models\n  via A Single Vector", "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks,\nespecially those from adversarial images, which is however under-explored in\nliterature. To facilitate research on this critical safety problem, we first\nconstruct a new laRge-scale Adervsarial images dataset with Diverse hArmful\nResponses (RADAR), given that existing datasets are either small-scale or only\ncontain limited types of harmful responses. With the new RADAR dataset, we\nfurther develop a novel and effective iN-time Embedding-based AdveRSarial Image\nDEtection (NEARSIDE) method, which exploits a single vector that distilled from\nthe hidden states of VLMs, which we call the attacking direction, to achieve\nthe detection of adversarial images against benign ones in the input. Extensive\nexperiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the\neffectiveness, efficiency, and cross-model transferrability of our proposed\nmethod. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE", "published": "2024-10-30 10:33:10", "link": "http://arxiv.org/abs/2410.22888v1", "categories": ["cs.CV", "cs.CL", "cs.CR"], "primary_category": "cs.CV"}
{"title": "VPO: Leveraging the Number of Votes in Preference Optimization", "abstract": "Direct Preference Optimization (DPO) trains a language model using human\npreference data, bypassing the explicit reward modeling phase of Reinforcement\nLearning from Human Feedback (RLHF). By iterating over sentence pairs in a\npreference dataset, DPO enhances generation quality by increasing the\nlikelihood of producing preferred sentences over less favored ones. Preference\ndatasets are typically created by selecting preferred sentences through a\nvoting process involving multiple individuals, as opinions can vary due to the\nsubjective nature of human preferences. While the number of votes offers\ninsight into whether a sentence pair is clearly preferable or controversial,\ncurrent methods do not fully leverage this information. In this paper, we\nintroduce a technique that leverages user voting data to better align with\ndiverse subjective preferences. We employ the Bayesian Minimum Mean Square\nError (Bayesian MMSE) estimator to model the probability that one generation is\npreferable to another. Using this estimated probability as a target, we develop\nthe Vote-based Preference Optimization (VPO) framework, which incorporates the\nnumber of votes on both sides to distinguish between controversial and obvious\ngeneration pairs. We show that previous algorithms, such as DPO and Identity\nPreference Optimization (IPO), can be extended using the proposed framework,\ntermed VDPO and VIPO. Our experiments demonstrate that these proposed\nalgorithms outperform various existing methods, including their base\nalgorithms.", "published": "2024-10-30 10:39:34", "link": "http://arxiv.org/abs/2410.22891v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification", "abstract": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.", "published": "2024-10-30 12:01:48", "link": "http://arxiv.org/abs/2410.22944v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning", "abstract": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.", "published": "2024-10-30 13:19:44", "link": "http://arxiv.org/abs/2410.22995v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback", "abstract": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose \\oni, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. By studying their relative tradeoffs, we shed light on\nquestions regarding intrinsic reward design for sparse reward problems. Our\napproach achieves state-of-the-art performance across a range of challenging,\nsparse reward tasks from the NetHack Learning Environment in a simple unified\nprocess, solely using the agent's gathered experience, without requiring\nexternal datasets. We make our code available at\n\\url{https://github.com/facebookresearch/oni}.", "published": "2024-10-30 13:52:43", "link": "http://arxiv.org/abs/2410.23022v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Controlling Language and Diffusion Models by Transporting Activations", "abstract": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.", "published": "2024-10-30 14:21:33", "link": "http://arxiv.org/abs/2410.23054v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T07, 49Q22", "I.2.6; I.2.7; I.4.8"], "primary_category": "cs.LG"}
{"title": "Comparative Analysis of Demonstration Selection Algorithms for LLM\n  In-Context Learning", "abstract": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview.", "published": "2024-10-30 15:11:58", "link": "http://arxiv.org/abs/2410.23099v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models", "abstract": "Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.", "published": "2024-10-30 15:25:06", "link": "http://arxiv.org/abs/2410.23114v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SciPIP: An LLM-based Scientific Paper Idea Proposer", "abstract": "The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.", "published": "2024-10-30 16:18:22", "link": "http://arxiv.org/abs/2410.23166v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm", "abstract": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.", "published": "2024-10-30 16:38:09", "link": "http://arxiv.org/abs/2410.23182v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents", "abstract": "Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs.", "published": "2024-10-30 17:10:19", "link": "http://arxiv.org/abs/2410.23218v1", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences", "abstract": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.", "published": "2024-10-30 17:13:02", "link": "http://arxiv.org/abs/2410.23223v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.LG"}
{"title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal\n  Foundation Models", "abstract": "Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.", "published": "2024-10-30 17:50:23", "link": "http://arxiv.org/abs/2410.23266v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploiting Phonological Similarities between African Languages to\n  achieve Speech to Speech Translation", "abstract": "This paper presents a pilot study on direct speech-to-speech translation\n(S2ST) by leveraging linguistic similarities among selected African languages\nwithin the same phylum, particularly in cases where traditional data annotation\nis expensive or impractical. We propose a segment-based model that maps speech\nsegments both within and across language phyla, effectively eliminating the\nneed for large paired datasets. By utilizing paired segments and guided\ndiffusion, our model enables translation between any two languages in the\ndataset. We evaluate the model on a proprietary dataset from the Kenya\nBroadcasting Corporation (KBC), which includes five languages: Swahili, Luo,\nKikuyu, Nandi, and English. The model demonstrates competitive performance in\nsegment pairing and translation quality, particularly for languages within the\nsame phylum. Our experiments reveal that segment length significantly\ninfluences translation accuracy, with average-length segments yielding the\nhighest pairing quality. Comparative analyses with traditional cascaded ASR-MT\ntechniques show that the proposed model delivers nearly comparable translation\nperformance. This study underscores the potential of exploiting linguistic\nsimilarities within language groups to perform efficient S2ST, especially in\nlow-resource language contexts.", "published": "2024-10-30 09:44:52", "link": "http://arxiv.org/abs/2410.23323v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment", "abstract": "Retrieval-Augmented Generation (RAG) systems enhance text generation by\nincorporating external knowledge but often struggle when retrieving context\nacross different text modalities due to semantic gaps. We introduce a\ngeneralized projection-based method, inspired by adapter modules in transfer\nlearning, that efficiently bridges these gaps between various text types, such\nas programming code and pseudocode, or English and French sentences. Our\napproach emphasizes speed, accuracy, and data efficiency, requiring minimal\nresources for training and inference. By aligning embeddings from heterogeneous\ntext modalities into a unified space through a lightweight projection network,\nour model significantly outperforms traditional retrieval methods like the\nOkapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while\napproaching the accuracy of Sentence Transformers. Extensive evaluations\ndemonstrate the effectiveness and generalizability of our method across\ndifferent tasks, highlighting its potential for real-time, resource-constrained\napplications.", "published": "2024-10-30 20:28:10", "link": "http://arxiv.org/abs/2410.23437v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "H.3.3; I.2.7; I.2.6"], "primary_category": "cs.LG"}
{"title": "All or None: Identifiable Linear Properties of Next-token Predictors in\n  Language Modeling", "abstract": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.", "published": "2024-10-30 23:19:29", "link": "http://arxiv.org/abs/2410.23501v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "The Belief State Transformer", "abstract": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\nWebsite: https://sites.google.com/view/belief-state-transformer", "published": "2024-10-30 23:26:06", "link": "http://arxiv.org/abs/2410.23506v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dynamic Strategy Planning for Efficient Question Answering with Large\n  Language Models", "abstract": "Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),\nplanning (e.g., SelfAsk), and retrieval augmented generation strategies to\nimprove the performance of Large Language Models (LLMs) on various tasks, such\nas question answering. However, using a single fixed strategy to answer\ndifferent kinds of questions is suboptimal in performance and inefficient in\nterms of generated output tokens and performed retrievals. In our work, we\npropose a novel technique DyPlan, to induce a dynamic strategy selection\nprocess in LLMs, to improve performance and reduce costs in question-answering.\nDyPlan incorporates an initial decision step to select the most suitable\nstrategy conditioned on the input question and guides the LLM's response\ngeneration accordingly. We extend DyPlan to DyPlan-verify, adding an internal\nverification and correction process to further enrich the generated answer.\nExperiments on three prominent multi-hop question answering (MHQA) datasets\nreveal how DyPlan can improve model performance by 7-13% while reducing the\ncost by 11-32% relative to the best baseline model.", "published": "2024-10-30 23:35:21", "link": "http://arxiv.org/abs/2410.23511v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rule by Rule: Learning with Confidence through Vocabulary Expansion", "abstract": "In this paper, we present an innovative iterative approach to rule learning\nspecifically designed for (but not limited to) text-based data. Our method\nfocuses on progressively expanding the vocabulary utilized in each iteration\nresulting in a significant reduction of memory consumption. Moreover, we\nintroduce a Value of Confidence as an indicator of the reliability of the\ngenerated rules. By leveraging the Value of Confidence, our approach ensures\nthat only the most robust and trustworthy rules are retained, thereby improving\nthe overall quality of the rule learning process. We demonstrate the\neffectiveness of our method through extensive experiments on various textual as\nwell as non-textual datasets including a use case of significant interest to\ninsurance industries, showcasing its potential for real-world applications.", "published": "2024-10-30 07:54:01", "link": "http://arxiv.org/abs/2411.00049v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Theoretical Perspective for Speculative Decoding Algorithm", "abstract": "Transformer-based autoregressive sampling has been the major bottleneck for\nslowing down large language model inferences. One effective way to accelerate\ninference is \\emph{Speculative Decoding}, which employs a small model to sample\na sequence of draft tokens and a large model to validate. Given its empirical\neffectiveness, the theoretical understanding of Speculative Decoding is falling\nbehind. This paper tackles this gap by conceptualizing the decoding problem via\nmarkov chain abstraction and studying the key properties, \\emph{output quality\nand inference acceleration}, from a theoretical perspective. Our analysis\ncovers the theoretical limits of speculative decoding, batch algorithms, and\noutput quality-inference acceleration tradeoffs. Our results reveal the\nfundamental connections between different components of LLMs via total\nvariation distances and show how they jointly affect the efficiency of decoding\nalgorithms.", "published": "2024-10-30 01:53:04", "link": "http://arxiv.org/abs/2411.00841v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation", "abstract": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.", "published": "2024-10-30 04:20:10", "link": "http://arxiv.org/abs/2411.00843v2", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models", "abstract": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory.", "published": "2024-10-30 11:16:04", "link": "http://arxiv.org/abs/2411.00850v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Accelerated AI Inference via Dynamic Execution Methods", "abstract": "In this paper, we focus on Dynamic Execution techniques that optimize the\ncomputation flow based on input. This aims to identify simpler problems that\ncan be solved using fewer resources, similar to human cognition. The techniques\ndiscussed include early exit from deep networks, speculative sampling for\nlanguage models, and adaptive steps for diffusion models. Experimental results\ndemonstrate that these dynamic approaches can significantly improve latency and\nthroughput without compromising quality. When combined with model-based\noptimizations, such as quantization, dynamic execution provides a powerful\nmulti-pronged strategy to optimize AI inference.\n  Generative AI requires a large amount of compute resources. This is expected\nto grow, and demand for resources in data centers through to the edge is\nexpected to continue to increase at high rates. We take advantage of existing\nresearch and provide additional innovations for some generative optimizations.\nIn the case of LLMs, we provide more efficient sampling methods that depend on\nthe complexity of the data. In the case of diffusion model generation, we\nprovide a new method that also leverages the difficulty of the input prompt to\npredict an optimal early stopping point.\n  Therefore, dynamic execution methods are relevant because they add another\ndimension of performance optimizations. Performance is critical from a\ncompetitive point of view, but increasing capacity can result in significant\npower savings and cost savings. We have provided several integrations of these\ntechniques into several Intel performance libraries and Huggingface Optimum.\nThese integrations will make them easier to use and increase the adoption of\nthese techniques.", "published": "2024-10-30 12:49:23", "link": "http://arxiv.org/abs/2411.00853v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Vision-Language Models Can Self-Improve Reasoning via Reflection", "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of\nlarge language models (LLMs). However, due to the complexity of multimodal\nscenarios and the difficulty in collecting high-quality CoT data, CoT reasoning\nin multimodal LLMs has been largely overlooked. To this end, we propose a\nsimple yet effective self-training framework, R3V, which iteratively enhances\nthe model's Vision-language Reasoning by Reflecting on CoT Rationales. Our\nframework consists of two interleaved parts: (1) iteratively bootstrapping\npositive and negative solutions for reasoning datasets, and (2) reflection on\nrationale for learning from mistakes. Specifically, we introduce the\nself-refine and self-select losses, enabling the model to refine flawed\nrationale and derive the correct answer by comparing rationale candidates.\nExperiments on a wide range of vision-language tasks show that R3V consistently\nimproves multimodal LLM reasoning, achieving a relative improvement of 23 to 60\npercent over GPT-distilled baselines. Additionally, our approach supports\nself-reflection on generated solutions, further boosting performance through\ntest-time computation.", "published": "2024-10-30 14:45:00", "link": "http://arxiv.org/abs/2411.00855v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Demo-Craft: Using In-Context Learning to Improve Code Generation in\n  Large Language Models", "abstract": "Generating executable code from natural language instructions using Large\nLanguage Models (LLMs) poses challenges such as semantic ambiguity and\nunderstanding taskspecific contexts. To address these issues, we propose a\nsystem called DemoCraft, which enhances code generation by leveraging\nin-context learning and demonstration selection, combined with latent concept\nlearning. Latent concept learning introduces additional concept tokens, which\nare trainable embeddings that capture task-specific knowledge. We then test our\nsystem on two major datasets: MBPP and Humaneval. Our experimental results\ndemonstrate that the proposed system achieves an approximate 2x increase in the\npass@k metric compared to baseline models. Furthermore, we introduce two novel\nevaluation metrics: correctness@k and similarity@k. Our empirical studies\nindicate that our system attains nearly a 3x improvement in these metrics as\nwell.", "published": "2024-10-30 19:45:50", "link": "http://arxiv.org/abs/2411.00865v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Comprehensive Study on Quantization Techniques for Large Language\n  Models", "abstract": "Large Language Models (LLMs) have been extensively researched and used in\nboth academia and industry since the rise in popularity of the Transformer\nmodel, which demonstrates excellent performance in AI. However, the\ncomputational demands of LLMs are immense, and the energy resources required to\nrun them are often limited. For instance, popular models like GPT-3, with 175\nbillion parameters and a storage requirement of 350 GB, present significant\nchallenges for deployment on resource-constrained IoT devices and embedded\nsystems. These systems often lack the computational capacity to handle such\nlarge models. Quantization, a technique that reduces the precision of model\nvalues to a smaller set of discrete values, offers a promising solution by\nreducing the size of LLMs and accelerating inference. In this research, we\nprovide a comprehensive analysis of quantization techniques within the machine\nlearning field, with a particular focus on their application to LLMs. We begin\nby exploring the mathematical theory of quantization, followed by a review of\ncommon quantization methods and how they are implemented. Furthermore, we\nexamine several prominent quantization methods applied to LLMs, detailing their\nalgorithms and performance outcomes.", "published": "2024-10-30 04:55:26", "link": "http://arxiv.org/abs/2411.02530v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in\n  Lie Detection", "abstract": "We investigate how low-quality AI advisors, lacking quality disclosures, can\nhelp spread text-based lies while seeming to help people detect lies.\nParticipants in our experiment discern truth from lies by evaluating\ntranscripts from a game show that mimicked deceptive social media exchanges on\ntopics with objective truths. We find that when relying on low-quality advisors\nwithout disclosures, participants' truth-detection rates fall below their own\nabilities, which recovered once the AI's true effectiveness was revealed.\nConversely, high-quality advisor enhances truth detection, regardless of\ndisclosure. We discover that participants' expectations about AI capabilities\ncontribute to their undue reliance on opaque, low-quality advisors.", "published": "2024-10-30 15:58:05", "link": "http://arxiv.org/abs/2410.23143v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "abstract": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.", "published": "2024-10-30 17:46:31", "link": "http://arxiv.org/abs/2410.23262v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation", "abstract": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io", "published": "2024-10-30 17:55:52", "link": "http://arxiv.org/abs/2410.23277v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and\n  Perceptions", "abstract": "The rise of large language models (LLMs) has led many researchers to consider\ntheir usage for scientific work. Some have found benefits using LLMs to augment\nor automate aspects of their research pipeline, while others have urged caution\ndue to risks and ethical concerns. Yet little work has sought to quantify and\ncharacterize how researchers use LLMs and why. We present the first large-scale\nsurvey of 816 verified research article authors to understand how the research\ncommunity leverages and perceives LLMs as research tools. We examine\nparticipants' self-reported LLM usage, finding that 81% of researchers have\nalready incorporated LLMs into different aspects of their research workflow. We\nalso find that traditionally disadvantaged groups in academia (non-White,\njunior, and non-native English speaking researchers) report higher LLM usage\nand perceived benefits, suggesting potential for improved research equity.\nHowever, women, non-binary, and senior researchers have greater ethical\nconcerns, potentially hindering adoption.", "published": "2024-10-30 04:25:23", "link": "http://arxiv.org/abs/2411.05025v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and\n  High-Compression-Rate Neural Audio Codec with Staged Training Paradigm", "abstract": "This paper proposes a novel neural audio codec, named APCodec+, which is an\nimproved version of APCodec. The APCodec+ takes the audio amplitude and phase\nspectra as the coding object, and employs an adversarial training strategy.\nInnovatively, we propose a two-stage joint-individual training paradigm for\nAPCodec+. In the joint training stage, the encoder, quantizer, decoder and\ndiscriminator are jointly trained with complete spectral loss, quantization\nloss, and adversarial loss. In the individual training stage, the encoder and\nquantizer fix their parameters and provide high-quality training data for the\ndecoder and discriminator. The decoder and discriminator are individually\ntrained from scratch without the quantization loss. The purpose of introducing\nindividual training is to reduce the learning difficulty of the decoder,\nthereby further improving the fidelity of the decoded audio. Experimental\nresults confirm that our proposed APCodec+ at low bitrates achieves comparable\nperformance with baseline codecs at higher bitrates, thanks to the proposed\nstaged training paradigm.", "published": "2024-10-30 08:36:17", "link": "http://arxiv.org/abs/2410.22807v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Augmenting Polish Automatic Speech Recognition System With Synthetic\n  Data", "abstract": "This paper presents a system developed for submission to Poleval 2024, Task\n3: Polish Automatic Speech Recognition Challenge. We describe Voicebox-based\nspeech synthesis pipeline and utilize it to augment Conformer and Whisper\nspeech recognition models with synthetic data. We show that addition of\nsynthetic speech to training improves achieved results significantly. We also\npresent final results achieved by our models in the competition.", "published": "2024-10-30 11:02:57", "link": "http://arxiv.org/abs/2410.22903v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Musical Accompaniment Co-creation via Diffusion Transformers", "abstract": "Building upon Diff-A-Riff, a latent diffusion model for musical instrument\naccompaniment generation, we present a series of improvements targeting\nquality, diversity, inference speed, and text-driven control. First, we upgrade\nthe underlying autoencoder to a stereo-capable model with superior fidelity and\nreplace the latent U-Net with a Diffusion Transformer. Additionally, we refine\ntext prompting by training a cross-modality predictive network to translate\ntext-derived CLAP embeddings to audio-derived CLAP embeddings. Finally, we\nimprove inference speed by training the latent model using a consistency\nframework, achieving competitive quality with fewer denoising steps. Our model\nis evaluated against the original Diff-A-Riff variant using objective metrics\nin ablation experiments, demonstrating promising advancements in all targeted\nareas. Sound examples are available at:\nhttps://sonycslparis.github.io/improved_dar/.", "published": "2024-10-30 13:31:54", "link": "http://arxiv.org/abs/2410.23005v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundCollage: Automated Discovery of New Classes in Audio Datasets", "abstract": "Developing new machine learning applications often requires the collection of\nnew datasets. However, existing datasets may already contain relevant\ninformation to train models for new purposes. We propose SoundCollage: a\nframework to discover new classes within audio datasets by incorporating (1) an\naudio pre-processing pipeline to decompose different sounds in audio samples,\nand (2) an automated model-based annotation mechanism to identify the\ndiscovered classes. Furthermore, we introduce the clarity measure to assess the\ncoherence of the discovered classes for better training new downstream\napplications. Our evaluations show that the accuracy of downstream audio\nclassifiers within discovered class samples and a held-out dataset improves\nover the baseline by up to 34.7% and 4.5%, respectively. These results\nhighlight the potential of SoundCollage in making datasets reusable by labeling\nwith newly discovered classes. To encourage further research in this area, we\nopen-source our code at\nhttps://github.com/nokia-bell-labs/audio-class-discovery.", "published": "2024-10-30 13:34:19", "link": "http://arxiv.org/abs/2410.23008v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audiovisual angle and voice incongruence do not affect audiovisual\n  verbal short-term memory in virtual reality", "abstract": "Virtual reality (VR) environments are frequently used in auditory and\ncognitive research to imitate real-life scenarios, presumably enhancing\nstate-of-the-art approaches with traditional computer screens. However, the\neffects of different display technologies on audiovisual processing remain\nunderexplored. This study investigated how VR displayed with an head-mounted\ndisplay (HMD) affects serial recall performance compared to traditional\ncomputer monitors, focusing on their effects on audiovisual processing in\ncognitive tasks. For that matter, we conducted two experiments with both an HMD\nand a computer monitor as display devices and two types of audiovisual\nincongruences: angle (Exp. 1) and voice (Exp. 2) incongruence. To quantify\ncognitive performance an audiovisual verbal serial recall (avVSR) task was\ndeveloped where an embodied conversational agent (ECA) was animated to speak\nthe target digit sequence. Even though subjective evaluations showed a higher\nsense of presence in the HMD condition, we found no effect of the display\ndevice on the proportion of correctly recalled digits. For the extreme\nconditions of angle incongruence in the computer monitor presentation the\nproportion of correctly recalled digits increased marginally, presumably due to\nraised attention, but the effect is likely too small to be meaningful. Response\ntimes were not affected by incongruences in either display device across both\nexperiments. These findings suggest that the avVSR task is robust against\nangular and voice audiovisual incongruences, irrespective of the display\ndevice, at least for the conditions studied here. Hence, the study introduces\nthe avVSR task in VR and contributes to the understanding of audiovisual\nintegration.", "published": "2024-10-30 13:41:30", "link": "http://arxiv.org/abs/2410.23015v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Run-Time Adaptation of Neural Beamforming for Robust Speech\n  Dereverberation and Denoising", "abstract": "This paper describes speech enhancement for realtime automatic speech\nrecognition (ASR) in real environments. A standard approach to this task is to\nuse neural beamforming that can work efficiently in an online manner. It\nestimates the masks of clean dry speech from a noisy echoic mixture spectrogram\nwith a deep neural network (DNN) and then computes a enhancement filter used\nfor beamforming. The performance of such a supervised approach, however, is\ndrastically degraded under mismatched conditions. This calls for run-time\nadaptation of the DNN. Although the ground-truth speech spectrogram required\nfor adaptation is not available at run time, blind dereverberation and\nseparation methods such as weighted prediction error (WPE) and fast\nmultichannel nonnegative matrix factorization (FastMNMF) can be used for\ngenerating pseudo groundtruth data from a mixture. Based on this idea, a prior\nwork proposed a dual-process system based on a cascade of WPE and minimum\nvariance distortionless response (MVDR) beamforming asynchronously fine-tuned\nby block-online FastMNMF. To integrate the dereverberation capability into\nneural beamforming and make it fine-tunable at run time, we propose to use\nweighted power minimization distortionless response (WPD) beamforming, a\nunified version of WPE and minimum power distortionless response (MPDR), whose\njoint dereverberation and denoising filter is estimated using a DNN. We\nevaluated the impact of run-time adaptation under various conditions with\ndifferent numbers of speakers, reverberation times, and signal-to-noise ratios\n(SNRs).", "published": "2024-10-30 08:32:47", "link": "http://arxiv.org/abs/2410.22805v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Transformer Model for Segmentation, Classification, and Caller\n  Identification of Marmoset Vocalization", "abstract": "Marmoset, a highly vocalized primate, has become a popular animal model for\nstudying social-communicative behavior and its underlying mechanism comparing\nwith human infant linguistic developments. In the study of vocal communication,\nit is vital to know the caller identities, call contents, and vocal exchanges.\nPrevious work of a CNN has achieved a joint model for call segmentation,\nclassification, and caller identification for marmoset vocalizations. However,\nthe CNN has limitations in modeling long-range acoustic patterns; the\nTransformer architecture that has been shown to outperform CNNs, utilizes the\nself-attention mechanism that efficiently segregates information parallelly\nover long distances and captures the global structure of marmoset vocalization.\nWe propose using the Transformer to jointly segment and classify the marmoset\ncalls and identify the callers for each vocalization.", "published": "2024-10-30 17:57:13", "link": "http://arxiv.org/abs/2410.23279v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lina-Speech: Gated Linear Attention is a Fast and Parameter-Efficient\n  Learner for text-to-speech synthesis", "abstract": "Neural codec language models have achieved state-of-the-art performance in\ntext-to-speech (TTS) synthesis, leveraging scalable architectures like\nautoregressive transformers and large-scale speech datasets. By framing voice\ncloning as a prompt continuation task, these models excel at cloning voices\nfrom short audio samples. However, this approach is limited in its ability to\nhandle numerous or lengthy speech excerpts, since the concatenation of source\nand target speech must fall within the maximum context length which is\ndetermined during training. In this work, we introduce Lina-Speech, a model\nthat replaces traditional self-attention mechanisms with emerging recurrent\narchitectures like Gated Linear Attention (GLA). Building on the success of\ninitial-state tuning on RWKV, we extend this technique to voice cloning,\nenabling the use of multiple speech samples and full utilization of the context\nwindow in synthesis. This approach is fast, easy to deploy, and achieves\nperformance comparable to fine-tuned baselines when the dataset size ranges\nfrom 3 to 15 minutes. Notably, Lina-Speech matches or outperforms\nstate-of-the-art baseline models, including some with a parameter count up to\nfour times higher or trained in an end-to-end style. We release our code and\ncheckpoints. Audio samples are available at\nhttps://theodorblackbird.github.io/blog/demo_lina/.", "published": "2024-10-30 04:50:40", "link": "http://arxiv.org/abs/2410.23320v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning in Vocal Education: Technical Evaluation of Limited\n  Samples Describing Mezzo-soprano", "abstract": "Vocal education in the music field is difficult to quantify due to the\nindividual differences in singers' voices and the different quantitative\ncriteria of singing techniques. Deep learning has great potential to be applied\nin music education due to its efficiency to handle complex data and perform\nquantitative analysis. However, accurate evaluations with limited samples over\nrare vocal types, such as Mezzo-soprano, requires extensive well-annotated data\nsupport using deep learning models. In order to attain the objective, we\nperform transfer learning by employing deep learning models pre-trained on the\nImageNet and Urbansound8k datasets for the improvement on the precision of\nvocal technique evaluation. Furthermore, we tackle the problem of the lack of\nsamples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS),\nfor vocal technique assessment. Our experimental results indicate that transfer\nlearning increases the overall accuracy (OAcc) of all models by an average of\n8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach\nto evaluating Mezzo-soprano vocal techniques but also introduce a new\nquantitative assessment method for music education.", "published": "2024-10-30 13:17:13", "link": "http://arxiv.org/abs/2410.23325v1", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event\n  Localization and Detection", "abstract": "This paper describes sound event localization and detection (SELD) for\nspatial audio recordings captured by firstorder ambisonics (FOA) microphones.\nIn this task, one may train a deep neural network (DNN) using FOA data\nannotated with the classes and directions of arrival (DOAs) of sound events.\nHowever, the performance of this approach is severely bounded by the amount of\nannotated data. To overcome this limitation, we propose a novel method of\npretraining the feature extraction part of the DNN in a self-supervised manner.\nWe use spatial audio-visual recordings abundantly available as virtual reality\ncontents. Assuming that sound objects are concurrently observed by the FOA\nmicrophones and the omni-directional camera, we jointly train audio and visual\nencoders with contrastive learning such that the audio and visual embeddings of\nthe same recording and DOA are made close. A key feature of our method is that\nthe DOA-wise audio embeddings are jointly extracted from the raw audio data,\nwhile the DOA-wise visual embeddings are separately extracted from the local\nvisual crops centered on the corresponding DOA. This encourages the latent\nfeatures of the audio encoder to represent both the classes and DOAs of sound\nevents. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows\nnon-annotated audio-visual recordings of 100 hours reduced the error score of\nSELD from 36.4 pts to 34.9 pts.", "published": "2024-10-30 08:31:58", "link": "http://arxiv.org/abs/2410.22803v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow", "abstract": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.", "published": "2024-10-30 17:18:53", "link": "http://arxiv.org/abs/2410.23230v2", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
