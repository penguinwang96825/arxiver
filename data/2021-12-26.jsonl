{"title": "An Interdisciplinary Approach for the Automated Detection and\n  Visualization of Media Bias in News Articles", "abstract": "Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of gold-standard data sets and\nhigh context dependencies. In this research project, I aim to devise data sets\nand methods to identify media bias. To achieve this, I plan to research methods\nusing natural language processing and deep learning while employing models and\nusing analysis concepts from psychology and linguistics. The first results\nindicate the effectiveness of an interdisciplinary research approach. My vision\nis to devise a system that helps news readers become aware of media coverage\ndifferences caused by bias. So far, my best performing BERT-based model is\npre-trained on a larger corpus consisting of distant labels, indicating that\ndistant supervision has the potential to become a solution for the difficult\ntask of bias detection.", "published": "2021-12-26 10:46:32", "link": "http://arxiv.org/abs/2112.13352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Budget Sensitive Reannotation of Noisy Relation Classification Data\n  Using Label Hierarchy", "abstract": "Large crowd-sourced datasets are often noisy and relation classification (RC)\ndatasets are no exception. Reannotating the entire dataset is one probable\nsolution however it is not always viable due to time and budget constraints.\nThis paper addresses the problem of efficient reannotation of a large noisy\ndataset for the RC. Our goal is to catch more annotation errors in the dataset\nwhile reannotating fewer instances. Existing work on RC dataset reannotation\nlacks the flexibility about how much data to reannotate. We introduce the\nconcept of a reannotation budget to overcome this limitation. The immediate\nfollow-up problem is: Given a specific reannotation budget, which subset of the\ndata should we reannotate? To address this problem, we present two strategies\nto selectively reannotate RC datasets. Our strategies utilize the taxonomic\nhierarchy of relation labels. The intuition of our work is to rely on the graph\ndistance between actual and predicted relation labels in the label hierarchy\ngraph. We evaluate our reannotation strategies on the well-known TACRED\ndataset. We design our experiments to answer three specific research questions.\nFirst, does our strategy select novel candidates for reannotation? Second, for\na given reannotation budget is our reannotation strategy more efficient at\ncatching annotation errors? Third, what is the impact of data reannotation on\nRC model performance measurement? Experimental results show that our both\nreannotation strategies are novel and efficient. Our analysis indicates that\nthe current reported performance of RC models on noisy TACRED data is inflated.", "published": "2021-12-26 05:50:51", "link": "http://arxiv.org/abs/2112.13320v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArT: All-round Thinker for Unsupervised Commonsense Question-Answering", "abstract": "Without labeled question-answer pairs for necessary training, unsupervised\ncommonsense question-answering (QA) appears to be extremely challenging due to\nits indispensable unique prerequisite on commonsense source like knowledge\nbases (KBs), which are usually highly resource consuming in construction.\nRecently pre-trained language models (PLMs) show effectiveness as an\nalternative for commonsense clues when they play a role of knowledge generator.\nHowever, existing work either relies on large-scale in-domain or out-of-domain\nlabeled data, or fails to generate knowledge of high quality in a general way.\nMotivated by human thinking experience, we propose an approach of All-round\nThinker (ArT) by fully taking association during knowledge generating. In\ndetail, our model first focuses on key parts in the given context, and then\ngenerates highly related knowledge on such a basis in an association way like\nhuman thinking. Besides, for causal reasoning, a reverse thinking mechanism is\nespecially added to further enhance bidirectional inferring between cause and\neffect. ArT is totally unsupervised and KBs-free. We evaluate it on three\ncommonsense QA benchmarks: COPA, SocialIQA and SCT. On all scales of PLM\nbackbones, ArT shows its brilliant performance and outperforms previous\nadvanced unsupervised models. Our code is available at\nhttps://github.com/WangJW424/commonsenseQA-ArT.", "published": "2021-12-26 18:06:44", "link": "http://arxiv.org/abs/2112.13428v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "New Methods & Metrics for LFQA tasks", "abstract": "Long-form question answering (LFQA) tasks require retrieving the documents\npertinent to a query, using them to form a paragraph-length answer. Despite\nconsiderable progress in LFQA modeling, fundamental issues impede its progress:\ni) train/validation/test dataset overlap, ii) absence of automatic metrics and\niii) generated answers not being \"grounded\" in retrieved documents. This work\naddresses every one these critical bottlenecks, contributing natural language\ninference/generation (NLI/NLG) methods and metrics that make significant\nstrides to their alleviation.", "published": "2021-12-26 18:38:05", "link": "http://arxiv.org/abs/2112.13432v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantum Algorithm for the Shortest Superstring Problem", "abstract": "In this paper, we consider the ``Shortest Superstring Problem''(SSP) or the\n``Shortest Common Superstring Problem''(SCS). The problem is as follows. For a\npositive integer $n$, a sequence of n strings $S=(s^1,\\dots,s^n)$ is given. We\nshould construct the shortest string $t$ (we call it superstring) that contains\neach string from the given sequence as a substring. The problem is connected\nwith the sequence assembly method for reconstructing a long DNA sequence from\nsmall fragments. We present a quantum algorithm with running time\n$O^*(1.728^n)$. Here $O^*$ notation does not consider polynomials of $n$ and\nthe length of $t$.", "published": "2021-12-26 05:37:56", "link": "http://arxiv.org/abs/2112.13319v1", "categories": ["quant-ph", "cs.CL", "cs.DS"], "primary_category": "quant-ph"}
{"title": "Delivery Issues Identification from Customer Feedback Data", "abstract": "Millions of packages are delivered successfully by online and local retail\nstores across the world every day. The proper delivery of packages is needed to\nensure high customer satisfaction and repeat purchases. These deliveries suffer\nvarious problems despite the best efforts from the stores. These issues happen\nnot only due to the large volume and high demand for low turnaround time but\nalso due to mechanical operations and natural factors. These issues range from\nreceiving wrong items in the package to delayed shipment to damaged packages\nbecause of mishandling during transportation. Finding solutions to various\ndelivery issues faced by both sending and receiving parties plays a vital role\nin increasing the efficiency of the entire process. This paper shows how to\nfind these issues using customer feedback from the text comments and uploaded\nimages. We used transfer learning for both Text and Image models to minimize\nthe demand for thousands of labeled examples. The results show that the model\ncan find different issues. Furthermore, it can also be used for tasks like\nbottleneck identification, process improvement, automating refunds, etc.\nCompared with the existing process, the ensemble of text and image models\nproposed in this paper ensures the identification of several types of delivery\nissues, which is more suitable for the real-life scenarios of delivery of items\nin retail businesses. This method can supply a new idea of issue detection for\nthe delivery of packages in similar industries.", "published": "2021-12-26 12:41:10", "link": "http://arxiv.org/abs/2112.13372v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.1; I.2.7; I.4.9"], "primary_category": "cs.CL"}
{"title": "Novel Dual-Channel Long Short-Term Memory Compressed Capsule Networks\n  for Emotion Recognition", "abstract": "Recent analysis on speech emotion recognition has made considerable advances\nwith the use of MFCCs spectrogram features and the implementation of neural\nnetwork approaches such as convolutional neural networks (CNNs). Capsule\nnetworks (CapsNet) have gained gratitude as alternatives to CNNs with their\nlarger capacities for hierarchical representation. To address these issues,\nthis research introduces a text-independent and speaker-independent SER novel\narchitecture, where a dual-channel long short-term memory compressed-CapsNet\n(DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features\nof CapsNet. Our proposed novel classifier can ensure the energy efficiency of\nthe model and adequate compression method in speech emotion recognition, which\nis not delivered through the original structure of a CapsNet. Moreover, the\ngrid search approach is used to attain optimal solutions. Results witnessed an\nimproved performance and reduction in the training and testing running time.\nThe speech datasets used to evaluate our algorithm are: Arabic Emirati-accented\ncorpus, English speech under simulated and actual stress corpus, English\nRyerson audio-visual database of emotional speech and song corpus, and\ncrowd-sourced emotional multimodal actors dataset. This work reveals that the\noptimum feature extraction method compared to other known methods is MFCCs\ndelta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM\nCOMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers,\nCNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our\nresults demonstrate that the proposed work yields average emotion recognition\naccuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and\n31.9% based on CapsNet, CNN, support vector machine, multi-layer perceptron,\nk-nearest neighbor, radial basis function, and naive Bayes, respectively.", "published": "2021-12-26 10:37:35", "link": "http://arxiv.org/abs/2112.13350v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Novel Hybrid DNN Approaches for Speaker Verification in Emotional and\n  Stressful Talking Environments", "abstract": "In this work, we conducted an empirical comparative study of the performance\nof text-independent speaker verification in emotional and stressful\nenvironments. This work combined deep models with shallow architecture, which\nresulted in novel hybrid classifiers. Four distinct hybrid models were\nutilized: deep neural network-hidden Markov model (DNN-HMM), deep neural\nnetwork-Gaussian mixture model (DNN-GMM), Gaussian mixture model-deep neural\nnetwork (GMM-DNN), and hidden Markov model-deep neural network (HMM-DNN). All\nmodels were based on novel implemented architecture. The comparative study used\nthree distinct speech datasets: a private Arabic dataset and two public English\ndatabases, namely, Speech Under Simulated and Actual Stress (SUSAS) and Ryerson\nAudio-Visual Database of Emotional Speech and Song (RAVDESS). The test results\nof the aforementioned hybrid models demonstrated that the proposed HMM-DNN\nleveraged the verification performance in emotional and stressful environments.\nResults also showed that HMM-DNN outperformed all other hybrid models in terms\nof equal error rate (EER) and area under the curve (AUC) evaluation metrics.\nThe average resulting verification system based on the three datasets yielded\nEERs of 7.19%, 16.85%, 11.51%, and 11.90% based on HMM-DNN, DNN-HMM, DNN-GMM,\nand GMM-DNN, respectively. Furthermore, we found that the DNN-GMM model\ndemonstrated the least computational complexity compared to all other hybrid\nmodels in both talking environments. Conversely, the HMM-DNN model required the\ngreatest amount of training time. Findings also demonstrated that EER and AUC\nvalues depended on the database when comparing average emotional and stressful\nperformances.", "published": "2021-12-26 10:47:14", "link": "http://arxiv.org/abs/2112.13353v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AIDA: An Active Inference-based Design Agent for Audio Processing\n  Algorithms", "abstract": "In this paper we present AIDA, which is an active inference-based agent that\niteratively designs a personalized audio processing algorithm through situated\ninteractions with a human client. The target application of AIDA is to propose\non-the-spot the most interesting alternative values for the tuning parameters\nof a hearing aid (HA) algorithm, whenever a HA client is not satisfied with\ntheir HA performance. AIDA interprets searching for the \"most interesting\nalternative\" as an issue of optimal (acoustic) context-aware Bayesian trial\ndesign. In computational terms, AIDA is realized as an active inference-based\nagent with an Expected Free Energy criterion for trial design. This type of\narchitecture is inspired by neuro-economic models on efficient (Bayesian) trial\ndesign in brains and implies that AIDA comprises generative probabilistic\nmodels for acoustic signals and user responses. We propose a novel generative\nmodel for acoustic signals as a sum of time-varying auto-regressive filters and\na user response model based on a Gaussian Process Classifier. The full AIDA\nagent has been implemented in a factor graph for the generative model and all\ntasks (parameter learning, acoustic context classification, trial design, etc.)\nare realized by variational message passing on the factor graph. All\nverification and validation experiments and demonstrations are freely\naccessible at our GitHub repository.", "published": "2021-12-26 11:56:16", "link": "http://arxiv.org/abs/2112.13366v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Acoustic scene classification using auditory datasets", "abstract": "The approach used not only challenges some of the fundamental mathematical\ntechniques used so far in early experiments of the same trend but also\nintroduces new scopes and new horizons for interesting results. The physics\ngoverning spectrograms have been optimized in the project along with exploring\nhow it handles the intense requirements of the problem at hand. Major\ncontributions and developments brought under the light, through this project\ninvolve using better mathematical techniques and problem-specific machine\nlearning methods. Improvised data analysis and data augmentation for audio\ndatasets like frequency masking and random frequency-time stretching are used\nin the project and hence are explained in this paper. In the used methodology,\nthe audio transforms principle were also tried and explored, and indeed the\ninsights gained were used constructively in the later stages of the project.\nUsing a deep learning principle is surely one of them. Also, in this paper, the\npotential scopes and upcoming research openings in both short and long term\ntunnel of time has been presented. Although much of the results gained are\ndomain-specific as of now, they are surely potent enough to produce novel\nsolutions in various different domains of diverse backgrounds.", "published": "2021-12-26 21:06:28", "link": "http://arxiv.org/abs/2112.13450v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Retrieving Effective Acoustic Impedance and Refractive Index for Size\n  Mismatch Samples", "abstract": "In this paper, we have presented an analytical solution to extract the\neffective properties of acoustic metamaterials from the measured complex\ntransmission and reflection coefficients when the metamaterial and impedance\ntube have different sizes. We have first modeled this problem as a bilayer\nmetamaterial located inside a duct and treated the air gap as a separate\ndomain. Then we have mathematically proved that the effective properties of\nacoustic metamaterial can be obtained by solving a set of eight linear\nequations when the dimensions are known. Finally, we have evaluated the\nproposed method with results from numerical simulations. It is shown that the\nproposed method can calculate the effective refractive index and impedance with\nan error of below 1\\%. This method provides an efficient approach to analyzing\nthe effective properties of acoustic metamaterials of various sizes.", "published": "2021-12-26 21:50:42", "link": "http://arxiv.org/abs/2112.13453v1", "categories": ["cs.SD", "eess.AS", "physics.class-ph"], "primary_category": "cs.SD"}
{"title": "Bilingual Speech Recognition by Estimating Speaker Geometry from Video\n  Data", "abstract": "Speech recognition is very challenging in student learning environments that\nare characterized by significant cross-talk and background noise. To address\nthis problem, we present a bilingual speech recognition system that uses an\ninteractive video analysis system to estimate the 3D speaker geometry for\nrealistic audio simulations. We demonstrate the use of our system in generating\na complex audio dataset that contains significant cross-talk and background\nnoise that approximate real-life classroom recordings. We then test our\nproposed system with real-life recordings.\n  In terms of the distance of the speakers from the microphone, our interactive\nvideo analysis system obtained a better average error rate of 10.83% compared\nto 33.12% for a baseline approach. Our proposed system gave an accuracy of\n27.92% that is 1.5% better than Google Speech-to-text on the same dataset. In\nterms of 9 important keywords, our approach gave an average sensitivity of 38%\ncompared to 24% for Google Speech-to-text, while both methods maintained high\naverage specificity of 90% and 92%.\n  On average, sensitivity improved from 24% to 38% for our proposed approach.\nOn the other hand, specificity remained high for both methods (90% to 92%).", "published": "2021-12-26 23:29:56", "link": "http://arxiv.org/abs/2112.13463v1", "categories": ["cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal\n  Derivatives", "abstract": "Diffusion generative models have emerged as a new challenger to popular deep\nneural generative models such as GANs, but have the drawback that they often\nrequire a huge number of neural function evaluations (NFEs) during synthesis\nunless some sophisticated sampling strategies are employed. This paper proposes\nnew efficient samplers based on the numerical schemes derived by the familiar\nTaylor expansion, which directly solves the ODE/SDE of interest. In general, it\nis not easy to compute the derivatives that are required in higher-order Taylor\nschemes, but in the case of diffusion models, this difficulty is alleviated by\nthe trick that the authors call ``ideal derivative substitution,'' in which the\nhigher-order derivatives are replaced by tractable ones. To derive ideal\nderivatives, the authors argue the ``single point approximation,'' in which the\ntrue score function is approximated by a conditional one, holds in many cases,\nand considered the derivatives of this approximation. Applying thus obtained\nnew quasi-Taylor samplers to image generation tasks, the authors experimentally\nconfirmed that the proposed samplers could synthesize plausible images in small\nnumber of NFEs, and that the performance was better or at the same level as\nDDIM and Runge-Kutta methods. The paper also argues the relevance of the\nproposed samplers to the existing ones mentioned above.", "published": "2021-12-26 09:38:11", "link": "http://arxiv.org/abs/2112.13339v2", "categories": ["stat.ML", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
