{"title": "Action based Network for Conversation Question Reformulation", "abstract": "Conversation question answering requires the ability to interpret a question\ncorrectly. Current models, however, are still unsatisfactory due to the\ndifficulty of understanding the co-references and ellipsis in daily\nconversation. Even though generative approaches achieved remarkable progress,\nthey are still trapped by semantic incompleteness. This paper presents an\naction-based approach to recover the complete expression of the question.\nSpecifically, we first locate the positions of co-reference or ellipsis in the\nquestion while assigning the corresponding action to each candidate span. We\nthen look for matching phrases related to the candidate clues in the\nconversation context. Finally, according to the predicted action, we decide\nwhether to replace the co-reference or supplement the ellipsis with the matched\ninformation. We demonstrate the effectiveness of our method on both English and\nChinese utterance rewrite tasks, improving the state-of-the-art EM (exact\nmatch) by 3.9\\% and ROUGE-L by 1.0\\% respectively on the Restoration-200K\ndataset.", "published": "2021-11-29 10:58:15", "link": "http://arxiv.org/abs/2111.14445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with\n  Semi-Supervised Learning and Explicit Policy Injection", "abstract": "Pre-trained models have proved to be powerful in enhancing task-oriented\ndialog systems. However, current pre-training methods mainly focus on enhancing\ndialog understanding and generation tasks while neglecting the exploitation of\ndialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog\nmodel that explicitly learns dialog policy from limited labeled dialogs and\nlarge-scale unlabeled dialog corpora via semi-supervised learning.\nSpecifically, we introduce a dialog act prediction task for policy optimization\nduring pre-training and employ a consistency regularization term to refine the\nlearned representation with the help of unlabeled dialogs. We also implement a\ngating mechanism to weigh suitable unlabeled dialog samples. Empirical results\nshow that GALAXY substantially improves the performance of task-oriented dialog\nsystems, and achieves new state-of-the-art results on benchmark datasets:\nIn-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores\nby 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a\nstronger few-shot ability than existing models under various low-resource\nsettings.", "published": "2021-11-29 15:24:36", "link": "http://arxiv.org/abs/2111.14592v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Tasks Relevant to Sleepiness Determined with Deep Transfer\n  Learning", "abstract": "Excessive sleepiness in attention-critical contexts can lead to adverse\nevents, such as car crashes. Detecting and monitoring sleepiness can help\nprevent these adverse events from happening. In this paper, we use the Voiceome\ndataset to extract speech from 1,828 participants to develop a deep transfer\nlearning model using Hidden-Unit BERT (HuBERT) speech representations to detect\nsleepiness from individuals. Speech is an under-utilized source of data in\nsleep detection, but as speech collection is easy, cost-effective, and\nnon-invasive, it provides a promising resource for sleepiness detection. Two\ncomplementary techniques were conducted in order to seek converging evidence\nregarding the importance of individual speech tasks. Our first technique,\nmasking, evaluated task importance by combining all speech tasks, masking\nselected responses in the speech, and observing systematic changes in model\naccuracy. Our second technique, separate training, compared the accuracy of\nmultiple models, each of which used the same architecture, but was trained on a\ndifferent subset of speech tasks. Our evaluation shows that the best-performing\nmodel utilizes the memory recall task and categorical naming task from the\nBoston Naming Test, which achieved an accuracy of 80.07% (F1-score of 0.85) and\n81.13% (F1-score of 0.89), respectively.", "published": "2021-11-29 16:46:29", "link": "http://arxiv.org/abs/2111.14684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Knowledge in Data Augmentation for Natural Language\n  Processing: An Example on Chinese Question Matching", "abstract": "To investigate the role of linguistic knowledge in data augmentation (DA) for\nNatural Language Processing (NLP), we designed two adapted DA programs and\napplied them to LCQMC (a Large-scale Chinese Question Matching Corpus) for a\nbinary Chinese question matching classification task. The two DA programs\nproduce augmented texts by five simple text editing operations (or DA\ntechniques), largely irrespective of language generation rules, but one is\nenhanced with a pre-trained n-gram language model to fuse it with prior\nlinguistic knowledge. We then trained four neural network models (BOW, CNN,\nLSTM, and GRU) and a pre-trained model (ERNIE-Gram) on the LCQMCs train sets of\nvarying size as well as the related augmented train sets produced by the two DA\nprograms. The results show that there are no significant performance\ndifferences between the models trained on the two types of augmented train\nsets, both when the five DA techniques are applied together or separately.\nMoreover, due to the inability of the five DA techniques to make strictly\nparaphrastic augmented texts, the results indicate the need of sufficient\namounts of training examples for the classification models trained on them to\nmediate the negative impact of false matching augmented text pairs and improve\nperformance, a limitation of random text editing perturbations used as a DA\napproach. Similar results were also obtained for English.", "published": "2021-11-29 17:07:49", "link": "http://arxiv.org/abs/2111.14709v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for a Hybrid Approach to Aspect-Based Sentiment\n  Analysis", "abstract": "The increasing popularity of the Web has subsequently increased the abundance\nof reviews on products and services. Mining these reviews for expressed\nsentiment is beneficial for both companies and consumers, as quality can be\nimproved based on this information. In this paper, we consider the\nstate-of-the-art HAABSA++ algorithm for aspect-based sentiment analysis tasked\nwith identifying the sentiment expressed towards a given aspect in review\nsentences. Specifically, we train the neural network part of this algorithm\nusing an adversarial network, a novel machine learning training method where a\ngenerator network tries to fool the classifier network by generating highly\nrealistic new samples, as such increasing robustness. This method, as of yet\nnever in its classical form applied to aspect-based sentiment analysis, is\nfound to be able to considerably improve the out-of-sample accuracy of\nHAABSA++: for the SemEval 2015 dataset, accuracy was increased from 81.7% to\n82.5%, and for the SemEval 2016 task, accuracy increased from 84.4% to 87.3%.", "published": "2021-11-29 22:02:06", "link": "http://arxiv.org/abs/2111.14988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lyric document embeddings for music tagging", "abstract": "We present an empirical study on embedding the lyrics of a song into a\nfixed-dimensional feature for the purpose of music tagging. Five methods of\ncomputing token-level and four methods of computing document-level\nrepresentations are trained on an industrial-scale dataset of tens of millions\nof songs. We compare simple averaging of pretrained embeddings to modern\nrecurrent and attention-based neural architectures. Evaluating on a wide range\nof tagging tasks such as genre classification, explicit content identification\nand era detection, we find that averaging word embeddings outperform more\ncomplex architectures in many downstream metrics.", "published": "2021-11-29 11:02:24", "link": "http://arxiv.org/abs/2112.11436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PSG: Prompt-based Sequence Generation for Acronym Extraction", "abstract": "Acronym extraction aims to find acronyms (i.e., short-forms) and their\nmeanings (i.e., long-forms) from the documents, which is important for\nscientific document understanding (SDU@AAAI-22) tasks. Previous works are\ndevoted to modeling this task as a paragraph-level sequence labeling problem.\nHowever, it lacks the effective use of the external knowledge, especially when\nthe datasets are in a low-resource setting. Recently, the prompt-based method\nwith the vast pre-trained language model can significantly enhance the\nperformance of the low-resourced downstream tasks. In this paper, we propose a\nPrompt-based Sequence Generation (PSG) method for the acronym extraction task.\nSpecifically, we design a template for prompting the extracted acronym texts\nwith auto-regression. A position extraction algorithm is designed for\nextracting the position of the generated answers. The results on the acronym\nextraction of Vietnamese and Persian in a low-resource setting show that the\nproposed method outperforms all other competitive state-of-the-art (SOTA)\nmethods.", "published": "2021-11-29 02:14:38", "link": "http://arxiv.org/abs/2111.14301v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimCLAD: A Simple Framework for Contrastive Learning of Acronym\n  Disambiguation", "abstract": "Acronym disambiguation means finding the correct meaning of an ambiguous\nacronym from the dictionary in a given sentence, which is one of the key points\nfor scientific document understanding (SDU@AAAI-22). Recently, many attempts\nhave tried to solve this problem via fine-tuning the pre-trained masked\nlanguage models (MLMs) in order to obtain a better acronym representation.\nHowever, the acronym meaning is varied under different contexts, whose\ncorresponding phrase representation mapped in different directions lacks\ndiscrimination in the entire vector space. Thus, the original representations\nof the pre-trained MLMs are not ideal for the acronym disambiguation task. In\nthis paper, we propose a Simple framework for Contrastive Learning of Acronym\nDisambiguation (SimCLAD) method to better understand the acronym meanings.\nSpecifically, we design a continual contrastive pre-training method that\nenhances the pre-trained model's generalization ability by learning the\nphrase-level contrastive distributions between true meaning and ambiguous\nphrases. The results on the acronym disambiguation of the scientific domain in\nEnglish show that the proposed method outperforms all other competitive\nstate-of-the-art (SOTA) methods.", "published": "2021-11-29 02:39:59", "link": "http://arxiv.org/abs/2111.14306v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Customer Sentiment Analysis using Weak Supervision for Customer-Agent\n  Chat", "abstract": "Prior work on sentiment analysis using weak supervision primarily focuses on\ndifferent reviews such as movies (IMDB), restaurants (Yelp), products\n(Amazon).~One under-explored field in this regard is customer chat data for a\ncustomer-agent chat in customer support due to the lack of availability of free\npublic data. Here, we perform sentiment analysis on customer chat using weak\nsupervision on our in-house dataset. We fine-tune the pre-trained language\nmodel (LM) RoBERTa as a sentiment classifier using weak supervision. Our\ncontribution is as follows:1) We show that by using weak sentiment classifiers\nalong with domain-specific lexicon-based rules as Labeling Functions (LF), we\ncan train a fairly accurate customer chat sentiment classifier using weak\nsupervision. 2) We compare the performance of our custom-trained model with\noff-the-shelf google cloud NLP API for sentiment analysis. We show that by\ninjecting domain-specific knowledge using LFs, even with weak supervision, we\ncan train a model to handle some domain-specific use cases better than\noff-the-shelf google cloud NLP API. 3) We also present an analysis of how\ncustomer sentiment in a chat relates to problem resolution.", "published": "2021-11-29 00:58:22", "link": "http://arxiv.org/abs/2111.14282v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A General Framework for Defending Against Backdoor Attacks via Influence\n  Graph", "abstract": "In this work, we propose a new and general framework to defend against\nbackdoor attacks, inspired by the fact that attack triggers usually follow a\n\\textsc{specific} type of attacking pattern, and therefore, poisoned training\nexamples have greater impacts on each other during training. We introduce the\nnotion of the {\\it influence graph}, which consists of nodes and edges\nrespectively representative of individual training points and associated\npair-wise influences. The influence between a pair of training points\nrepresents the impact of removing one training point on the prediction of\nanother, approximated by the influence function \\citep{koh2017understanding}.\nMalicious training points are extracted by finding the maximum average\nsub-graph subject to a particular size. Extensive experiments on computer\nvision and natural language processing tasks demonstrate the effectiveness and\ngenerality of the proposed framework.", "published": "2021-11-29 02:55:42", "link": "http://arxiv.org/abs/2111.14309v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic\n  Arithmetic", "abstract": "Recent text-to-image matching models apply contrastive learning to large\ncorpora of uncurated pairs of images and sentences. While such models can\nprovide a powerful score for matching and subsequent zero-shot tasks, they are\nnot capable of generating caption given an image. In this work, we repurpose\nsuch models to generate a descriptive text given an image at inference time,\nwithout any further training or tuning steps. This is done by combining the\nvisual-semantic model with a large language model, benefiting from the\nknowledge in both web-scale models. The resulting captions are much less\nrestrictive than those obtained by supervised captioning methods. Moreover, as\na zero-shot learning method, it is extremely flexible and we demonstrate its\nability to perform image arithmetic in which the inputs can be either images or\ntext, and the output is a sentence. This enables novel high-level vision\ncapabilities such as comparing two images or solving visual analogy tests. Our\ncode is available at: https://github.com/YoadTew/zero-shot-image-to-text.", "published": "2021-11-29 11:01:49", "link": "http://arxiv.org/abs/2111.14447v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet", "abstract": "As Automatic Speech Processing (ASR) systems are getting better, there is an\nincreasing interest of using the ASR output to do downstream Natural Language\nProcessing (NLP) tasks. However, there are few open source toolkits that can be\nused to generate reproducible results on different Spoken Language\nUnderstanding (SLU) benchmarks. Hence, there is a need to build an open source\nstandard that can be used to have a faster start into SLU research. We present\nESPnet-SLU, which is designed for quick development of spoken language\nunderstanding in a single framework. ESPnet-SLU is a project inside end-to-end\nspeech processing toolkit, ESPnet, which is a widely used open-source standard\nfor various speech processing tasks like ASR, Text to Speech (TTS) and Speech\nTranslation (ST). We enhance the toolkit to provide implementations for various\nSLU benchmarks that enable researchers to seamlessly mix-and-match different\nASR and NLU models. We also provide pretrained models with intensively tuned\nhyper-parameters that can match or even outperform the current state-of-the-art\nperformances. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.", "published": "2021-11-29 17:05:49", "link": "http://arxiv.org/abs/2111.14706v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding Out-of-distribution: A Perspective of Data Dynamics", "abstract": "Despite machine learning models' success in Natural Language Processing (NLP)\ntasks, predictions from these models frequently fail on out-of-distribution\n(OOD) samples. Prior works have focused on developing state-of-the-art methods\nfor detecting OOD. The fundamental question of how OOD samples differ from\nin-distribution samples remains unanswered. This paper explores how data\ndynamics in training models can be used to understand the fundamental\ndifferences between OOD and in-distribution samples in extensive detail. We\nfound that syntactic characteristics of the data samples that the model\nconsistently predicts incorrectly in both OOD and in-distribution cases\ndirectly contradict each other. In addition, we observed preliminary evidence\nsupporting the hypothesis that models are more likely to latch on trivial\nsyntactic heuristics (e.g., overlap of words between two sentences) when making\npredictions on OOD samples. We hope our preliminary study accelerates the\ndata-centric analysis on various machine learning phenomena.", "published": "2021-11-29 17:34:38", "link": "http://arxiv.org/abs/2111.14730v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Referring Video Object Segmentation with Multimodal\n  Transformers", "abstract": "The referring video object segmentation task (RVOS) involves segmentation of\na text-referred object instance in the frames of a given video. Due to the\ncomplex nature of this multimodal task, which combines text reasoning, video\nunderstanding, instance segmentation and tracking, existing approaches\ntypically rely on sophisticated pipelines in order to tackle it. In this paper,\nwe propose a simple Transformer-based approach to RVOS. Our framework, termed\nMultimodal Tracking Transformer (MTTR), models the RVOS task as a sequence\nprediction problem. Following recent advancements in computer vision and\nnatural language processing, MTTR is based on the realization that video and\ntext can be processed together effectively and elegantly by a single multimodal\nTransformer model. MTTR is end-to-end trainable, free of text-related inductive\nbias components and requires no additional mask-refinement post-processing\nsteps. As such, it simplifies the RVOS pipeline considerably compared to\nexisting methods. Evaluation on standard benchmarks reveals that MTTR\nsignificantly outperforms previous art across multiple metrics. In particular,\nMTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and\nJHMDB-Sentences datasets respectively, while processing 76 frames per second.\nIn addition, we report strong results on the public validation set of\nRefer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the\nattention of researchers. The code to reproduce our experiments is available at\nhttps://github.com/mttr2021/MTTR", "published": "2021-11-29 18:59:32", "link": "http://arxiv.org/abs/2111.14821v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Do We Still Need Automatic Speech Recognition for Spoken Language\n  Understanding?", "abstract": "Spoken language understanding (SLU) tasks are usually solved by first\ntranscribing an utterance with automatic speech recognition (ASR) and then\nfeeding the output to a text-based model. Recent advances in self-supervised\nrepresentation learning for speech data have focused on improving the ASR\ncomponent. We investigate whether representation learning for speech has\nmatured enough to replace ASR in SLU. We compare learned speech features from\nwav2vec 2.0, state-of-the-art ASR transcripts, and the ground truth text as\ninput for a novel speech-based named entity recognition task, a cardiac arrest\ndetection task on real-world emergency calls and two existing SLU benchmarks.\nWe show that learned speech features are superior to ASR transcripts on three\nclassification tasks. For machine translation, ASR transcripts are still the\nbetter choice. We highlight the intrinsic robustness of wav2vec 2.0\nrepresentations to out-of-vocabulary words as key to better performance.", "published": "2021-11-29 15:13:36", "link": "http://arxiv.org/abs/2111.14842v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Natural Language Processing and Deep Learning based Model for\n  Automated Vehicle Diagnostics using Free-Text Customer Service Reports", "abstract": "Initial fault detection and diagnostics are imperative measures to improve\nthe efficiency, safety, and stability of vehicle operation. In recent years,\nnumerous studies have investigated data-driven approaches to improve the\nvehicle diagnostics process using available vehicle data. Moreover, data-driven\nmethods are employed to enhance customer-service agent interactions. In this\nstudy, we demonstrate a machine learning pipeline to improve automated vehicle\ndiagnostics. First, Natural Language Processing (NLP) is used to automate the\nextraction of crucial information from free-text failure reports (generated\nduring customers' calls to the service department). Then, deep learning\nalgorithms are employed to validate service requests and filter vague or\nmisleading claims. Ultimately, different classification algorithms are\nimplemented to classify service requests so that valid service requests can be\ndirected to the relevant service department. The proposed model- Bidirectional\nLong Short Term Memory (BiLSTM) along with Convolution Neural Network (CNN)-\nshows more than 18\\% accuracy improvement in validating service requests\ncompared to technicians' capabilities. In addition, using domain-based NLP\ntechniques at preprocessing and feature extraction stages along with CNN-BiLSTM\nbased request validation enhanced the accuracy ($>25\\%$), sensitivity\n($>39\\%$), specificity ($>11\\%$), and precision ($>11\\%$) of Gradient Tree\nBoosting (GTB) service classification model. The Receiver Operating\nCharacteristic Area Under the Curve (ROC-AUC) reached 0.82.", "published": "2021-11-29 21:41:34", "link": "http://arxiv.org/abs/2111.14977v1", "categories": ["eess.SY", "cs.CL", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Joint Modeling of Code-Switched and Monolingual ASR via Conditional\n  Factorization", "abstract": "Conversational bilingual speech encompasses three types of utterances: two\npurely monolingual types and one intra-sententially code-switched type. In this\nwork, we propose a general framework to jointly model the likelihoods of the\nmonolingual and code-switch sub-tasks that comprise bilingual speech\nrecognition. By defining the monolingual sub-tasks with label-to-frame\nsynchronization, our joint modeling framework can be conditionally factorized\nsuch that the final bilingual output, which may or may not be code-switched, is\nobtained given only monolingual information. We show that this conditionally\nfactorized joint framework can be modeled by an end-to-end differentiable\nneural network. We demonstrate the efficacy of our proposed model on bilingual\nMandarin-English speech recognition across both monolingual and code-switched\ncorpora.", "published": "2021-11-29 23:14:54", "link": "http://arxiv.org/abs/2111.15016v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mixed Precision Low-bit Quantization of Neural Network Language Models\n  for Speech Recognition", "abstract": "State-of-the-art language models (LMs) represented by long-short term memory\nrecurrent neural networks (LSTM-RNNs) and Transformers are becoming\nincreasingly complex and expensive for practical applications. Low-bit neural\nnetwork quantization provides a powerful solution to dramatically reduce their\nmodel size. Current quantization methods are based on uniform precision and\nfail to account for the varying performance sensitivity at different parts of\nLMs to quantization errors. To this end, novel mixed precision neural network\nLM quantization methods are proposed in this paper. The optimal local precision\nchoices for LSTM-RNN and Transformer based neural LMs are automatically learned\nusing three techniques. The first two approaches are based on quantization\nsensitivity metrics in the form of either the KL-divergence measured between\nfull precision and quantized LMs, or Hessian trace weighted quantization\nperturbation that can be approximated efficiently using matrix free techniques.\nThe third approach is based on mixed precision neural architecture search. In\norder to overcome the difficulty in using gradient descent methods to directly\nestimate discrete quantized weights, alternating direction methods of\nmultipliers (ADMM) are used to efficiently train quantized LMs. Experiments\nwere conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed\nperturbation, i-Vector and learning hidden unit contribution (LHUC) based\nspeaker adaptation on two tasks: Switchboard telephone speech and AMI meeting\ntranscription. The proposed mixed precision quantization techniques achieved\n\"lossless\" quantization on both tasks, by producing model size compression\nratios of up to approximately 16 times over the full precision LSTM and\nTransformer baseline LMs, while incurring no statistically significant word\nerror rate increase.", "published": "2021-11-29 12:24:02", "link": "http://arxiv.org/abs/2112.11438v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automated Drug-Related Information Extraction from French Clinical\n  Documents: ReLyfe Approach", "abstract": "Structuring medical data in France remains a challenge mainly because of the\nlack of medical data due to privacy concerns and the lack of methods and\napproaches on processing the French language. One of these challenges is\nstructuring drug-related information in French clinical documents. To our\nknowledge, over the last decade, there are less than five relevant papers that\nstudy French prescriptions. This paper proposes a new approach for extracting\ndrug-related information from French clinical scanned documents while\npreserving patients' privacy. In addition, we deployed our method in a health\ndata management platform where it is used to structure drug medical data and\nhelp patients organize their drug schedules. It can be implemented on any web\nor mobile platform. This work closes the gap between theoretical and practical\nwork by creating an application adapted to real production problems. It is a\ncombination of a rule-based phase and a Deep Learning approach. Finally,\nnumerical results show the outperformance and relevance of the proposed\nmethodology.", "published": "2021-11-29 22:11:23", "link": "http://arxiv.org/abs/2112.11439v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixed Precision of Quantization of Transformer Language Models for\n  Speech Recognition", "abstract": "State-of-the-art neural language models represented by Transformers are\nbecoming increasingly complex and expensive for practical applications. Low-bit\ndeep neural network quantization techniques provides a powerful solution to\ndramatically reduce their model size. Current low-bit quantization methods are\nbased on uniform precision and fail to account for the varying performance\nsensitivity at different parts of the system to quantization errors. To this\nend, novel mixed precision DNN quantization methods are proposed in this paper.\nThe optimal local precision settings are automatically learned using two\ntechniques. The first is based on a quantization sensitivity metric in the form\nof Hessian trace weighted quantization perturbation. The second is based on\nmixed precision Transformer architecture search. Alternating direction methods\nof multipliers (ADMM) are used to efficiently train mixed precision quantized\nDNN systems. Experiments conducted on Penn Treebank (PTB) and a Switchboard\ncorpus trained LF-MMI TDNN system suggest the proposed mixed precision\nTransformer quantization techniques achieved model size compression ratios of\nup to 16 times over the full precision baseline with no recognition performance\ndegradation. When being used to compress a larger full precision Transformer LM\nwith more layers, overall word error rate (WER) reductions up to 1.7% absolute\n(18% relative) were obtained.", "published": "2021-11-29 09:57:00", "link": "http://arxiv.org/abs/2112.11540v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Changepoint Analysis of Topic Proportions in Temporal Text Data", "abstract": "Changepoint analysis deals with unsupervised detection and/or estimation of\ntime-points in time-series data, when the distribution generating the data\nchanges. In this article, we consider \\emph{offline} changepoint detection in\nthe context of large scale textual data. We build a specialised temporal topic\nmodel with provisions for changepoints in the distribution of topic\nproportions. As full likelihood based inference in this model is\ncomputationally intractable, we develop a computationally tractable approximate\ninference procedure. More specifically, we use sample splitting to estimate\ntopic polytopes first and then apply a likelihood ratio statistic together with\na modified version of the wild binary segmentation algorithm of Fryzlewicz et\nal. (2014). Our methodology facilitates automated detection of structural\nchanges in large corpora without the need of manual processing by domain\nexperts. As changepoints under our model correspond to changes in topic\nstructure, the estimated changepoints are often highly interpretable as marking\nthe surge or decline in popularity of a fashionable topic. We apply our\nprocedure on two large datasets: (i) a corpus of English literature from the\nperiod 1800-1922 (Underwoodet al., 2015); (ii) abstracts from the High Energy\nPhysics arXiv repository (Clementet al., 2019). We obtain some historically\nwell-known changepoints and discover some new ones.", "published": "2021-11-29 17:20:51", "link": "http://arxiv.org/abs/2112.00827v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Mixed Precision DNN Qunatization for Overlapped Speech Separation and\n  Recognition", "abstract": "Recognition of overlapped speech has been a highly challenging task to date.\nState-of-the-art multi-channel speech separation system are becoming\nincreasingly complex and expensive for practical applications. To this end,\nlow-bit neural network quantization provides a powerful solution to\ndramatically reduce their model size. However, current quantization methods are\nbased on uniform precision and fail to account for the varying performance\nsensitivity at different model components to quantization errors. In this\npaper, novel mixed precision DNN quantization methods are proposed by applying\nlocally variable bit-widths to individual TCN components of a TF masking based\nmulti-channel speech separation system. The optimal local precision settings\nare automatically learned using three techniques. The first two approaches\nutilize quantization sensitivity metrics based on either the mean square error\n(MSE) loss function curvature, or the KL-divergence measured between full\nprecision and quantized separation models. The third approach is based on mixed\nprecision neural architecture search. Experiments conducted on the LRS3-TED\ncorpus simulated overlapped speech data suggest that the proposed mixed\nprecision quantization techniques consistently outperform the uniform precision\nbaseline speech separation systems of comparable bit-widths in terms of SI-SNR\nand PESQ scores as well as word error rate (WER) reductions up to 2.88%\nabsolute (8% relative).", "published": "2021-11-29 11:50:42", "link": "http://arxiv.org/abs/2111.14479v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Responding to Challenge Call of Machine Learning Model Development in\n  Diagnosing Respiratory Disease Sounds", "abstract": "In this study, a machine learning model was developed for automatically\ndetecting respiratory system sounds such as sneezing and coughing in disease\ndiagnosis. The automatic model and approach development of breath sounds, which\ncarry valuable information, results in early diagnosis and treatment. A\nsuccessful machine learning model was developed in this study, which was a\nstrong response to the challenge called the \"Pfizer digital medicine challenge\"\non the \"OSFHOME\" open access platform. \"Environmental sound classification\"\ncalled ESC-50 and AudioSet sound files were used to prepare the dataset. In\nthis dataset, which consisted of three parts, features that effectively showed\ncoughing and sneezing sound analysis were extracted from training, testing and\nvalidating samples. Based on the Mel frequency cepstral coefficients (MFCC)\nfeature extraction method, mathematical and statistical features were prepared.\nThree different classification techniques were considered to perform successful\nrespiratory sound classification in the dataset containing more than 3800\ndifferent sounds. Support vector machine (SVM) with radial basis function (RBF)\nkernels, ensemble aggregation and decision tree classification methods were\nused as classification techniques. In an attempt to classify coughing and\nsneezing sounds from other sounds, SVM with RBF kernels was achieved with 83%\nsuccess.", "published": "2021-11-29 07:18:36", "link": "http://arxiv.org/abs/2111.14354v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "AVA-AVD: Audio-Visual Speaker Diarization in the Wild", "abstract": "Audio-visual speaker diarization aims at detecting \"who spoke when\" using\nboth auditory and visual signals. Existing audio-visual diarization datasets\nare mainly focused on indoor environments like meeting rooms or news studios,\nwhich are quite different from in-the-wild videos in many scenarios such as\nmovies, documentaries, and audience sitcoms. To develop diarization methods for\nthese challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)\ndataset. Our experiments demonstrate that adding AVA-AVD into training set can\nproduce significantly better diarization models for in-the-wild videos despite\nthat the data is relatively small. Moreover, this benchmark is challenging due\nto the diverse scenes, complicated acoustic conditions, and completely\noff-screen speakers. As a first step towards addressing the challenges, we\ndesign the Audio-Visual Relation Network (AVR-Net) which introduces a simple\nyet effective modality mask to capture discriminative information based on face\nvisibility. Experiments show that our method not only can outperform\nstate-of-the-art methods but is more robust as varying the ratio of off-screen\nspeakers. Our data and code has been made publicly available at\nhttps://github.com/showlab/AVA-AVD.", "published": "2021-11-29 11:02:41", "link": "http://arxiv.org/abs/2111.14448v5", "categories": ["cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Expressive Communication: A Common Framework for Evaluating Developments\n  in Generative Models and Steering Interfaces", "abstract": "There is an increasing interest from ML and HCI communities in empowering\ncreators with better generative models and more intuitive interfaces with which\nto control them. In music, ML researchers have focused on training models\ncapable of generating pieces with increasing long-range structure and musical\ncoherence, while HCI researchers have separately focused on designing steering\ninterfaces that support user control and ownership. In this study, we\ninvestigate through a common framework how developments in both models and user\ninterfaces are important for empowering co-creation where the goal is to create\nmusic that communicates particular imagery or ideas (e.g., as is common for\nother purposeful tasks in music creation like establishing mood or creating\naccompanying music for another media). Our study is distinguished in that it\nmeasures communication through both composer's self-reported experiences, and\nhow listeners evaluate this communication through the music. In an evaluation\nstudy with 26 composers creating 100+ pieces of music and listeners providing\n1000+ head-to-head comparisons, we find that more expressive models and more\nsteerable interfaces are important and complementary ways to make a difference\nin composers communicating through music and supporting their creative\nempowerment.", "published": "2021-11-29 20:57:55", "link": "http://arxiv.org/abs/2111.14951v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped\n  Environments with Moving Sounds", "abstract": "Audio-visual navigation combines sight and hearing to navigate to a\nsound-emitting source in an unmapped environment. While recent approaches have\ndemonstrated the benefits of audio input to detect and find the goal, they\nfocus on clean and static sound sources and struggle to generalize to unheard\nsounds. In this work, we propose the novel dynamic audio-visual navigation\nbenchmark which requires catching a moving sound source in an environment with\nnoisy and distracting sounds, posing a range of new challenges. We introduce a\nreinforcement learning approach that learns a robust navigation policy for\nthese complex settings. To achieve this, we propose an architecture that fuses\naudio-visual information in the spatial feature space to learn correlations of\ngeometric information inherent in both local maps and audio signals. We\ndemonstrate that our approach consistently outperforms the current\nstate-of-the-art by a large margin across all tasks of moving sounds, unheard\nsounds, and noisy environments, on two challenging 3D scanned real-world\nenvironments, namely Matterport3D and Replica. The benchmark is available at\nhttp://dav-nav.cs.uni-freiburg.de.", "published": "2021-11-29 15:17:46", "link": "http://arxiv.org/abs/2111.14843v4", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
