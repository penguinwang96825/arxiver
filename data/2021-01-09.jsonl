{"title": "A Gamification of Japanese Dependency Parsing", "abstract": "Gamification approaches have been used as a way for creating language\nresources for NLP. It is also used for presenting and teaching the algorithms\nin NLP and linguistic phenomena. This paper argues about a design of\ngamification for Japanese syntactic dependendency parsing for the latter\nobjective. The user interface design is based on a transition-based shift\nreduce dependency parsing which needs only two actions of SHIFT (not attach)\nand REDUCE (attach) in Japanese dependency structure. We assign the two actions\nfor two-way directional control on a gamepad or other devices. We also design\nthe target sentences from psycholinguistics researches.", "published": "2021-01-09 01:42:07", "link": "http://arxiv.org/abs/2101.03269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual\n  Natural Language Processing", "abstract": "We introduce Trankit, a light-weight Transformer-based Toolkit for\nmultilingual Natural Language Processing (NLP). It provides a trainable\npipeline for fundamental NLP tasks over 100 languages, and 90 pretrained\npipelines for 56 languages. Built on a state-of-the-art pretrained language\nmodel, Trankit significantly outperforms prior multilingual NLP pipelines over\nsentence segmentation, part-of-speech tagging, morphological feature tagging,\nand dependency parsing while maintaining competitive performance for\ntokenization, multi-word token expansion, and lemmatization over 90 Universal\nDependencies treebanks. Despite the use of a large pretrained transformer, our\ntoolkit is still efficient in memory usage and speed. This is achieved by our\nnovel plug-and-play mechanism with Adapters where a multilingual pretrained\ntransformer is shared across pipelines for different languages. Our toolkit\nalong with pretrained models and code are publicly available at:\nhttps://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also\navailable at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video\nfor Trankit at: https://youtu.be/q0KGP3zGjGc.", "published": "2021-01-09 04:55:52", "link": "http://arxiv.org/abs/2101.03289v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combating Hostility: Covid-19 Fake News and Hostile Post Detection in\n  Social Media", "abstract": "This paper illustrates a detail description of the system and its results\nthat developed as a part of the participation at CONSTRAINT shared task in\nAAAI-2021. The shared task comprises two tasks: a) COVID19 fake news detection\nin English b) Hostile post detection in Hindi. Task-A is a binary\nclassification problem with fake and real class, while task-B is a multi-label\nmulti-class classification task with five hostile classes (i.e. defame, fake,\nhate, offense, non-hostile). Various techniques are used to perform the\nclassification task, including SVM, CNN, BiLSTM, and CNN+BiLSTM with tf-idf and\nWord2Vec embedding techniques. Results indicate that SVM with tf-idf features\nachieved the highest 94.39% weighted $f_1$ score on the test set in task-A.\nLabel powerset SVM with n-gram features obtained the maximum coarse-grained and\nfine-grained $f_1$ score of 86.03% and 50.98% on the task-B test set\nrespectively.", "published": "2021-01-09 05:15:41", "link": "http://arxiv.org/abs/2101.03291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Better Sentence Representation with Syntax Information", "abstract": "Sentence semantic understanding is a key topic in the field of natural\nlanguage processing. Recently, contextualized word representations derived from\npre-trained language models such as ELMO and BERT have shown significant\nimprovements for a wide range of semantic tasks, e.g. question answering, text\nclassification and sentiment analysis. However, how to add external knowledge\nto further improve the semantic modeling capability of model is worth probing.\nIn this paper, we propose a novel approach to combining syntax information with\na pre-trained language model. In order to evaluate the effect of the\npre-training model, first, we introduce RNN-based and Transformer-based\npre-trained language models; secondly, to better integrate external knowledge,\nsuch as syntactic information integrate with the pre-training model, we propose\na dependency syntax expansion (DSE) model. For evaluation, we have selected two\nsubtasks: sentence completion task and biological relation extraction task. The\nexperimental results show that our model achieves 91.2\\% accuracy,\noutperforming the baseline model by 37.8\\% on sentence completion task. And it\nalso gets competitive performance by 75.1\\% $F_{1}$ score on relation\nextraction task.", "published": "2021-01-09 12:15:08", "link": "http://arxiv.org/abs/2101.03343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Relational Sentence Generation and Retrieval for Medical Image\n  Report Composition", "abstract": "Beyond generating long and topic-coherent paragraphs in traditional\ncaptioning tasks, the medical image report composition task poses more\ntask-oriented challenges by requiring both the highly-accurate medical term\ndiagnosis and multiple heterogeneous forms of information including impression\nand findings. Current methods often generate the most common sentences due to\ndataset bias for individual case, regardless of whether the sentences properly\ncapture key entities and relationships. Such limitations severely hinder their\napplicability and generalization capability in medical report composition where\nthe most critical sentences lie in the descriptions of abnormal diseases that\nare relatively rare. Moreover, some medical terms appearing in one report are\noften entangled with each other and co-occurred, e.g. symptoms associated with\na specific disease. To enforce the semantic consistency of medical terms to be\nincorporated into the final reports and encourage the sentence generation for\nrare abnormal descriptions, we propose a novel framework that unifies template\nretrieval and sentence generation to handle both common and rare abnormality\nwhile ensuring the semantic-coherency among the detected medical terms.\nSpecifically, our approach exploits hybrid-knowledge co-reasoning: i) explicit\nrelationships among all abnormal medical terms to induce the visual attention\nlearning and topic representation encoding for better topic-oriented symptoms\ndescriptions; ii) adaptive generation mode that changes between the template\nretrieval and sentence generation according to a contextual topic encoder.\nExperimental results on two medical report benchmarks demonstrate the\nsuperiority of the proposed framework in terms of both human and metrics\nevaluation.", "published": "2021-01-09 04:33:27", "link": "http://arxiv.org/abs/2101.03287v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LightXML: Transformer with Dynamic Negative Sampling for\n  High-Performance Extreme Multi-label Text Classification", "abstract": "Extreme Multi-label text Classification (XMC) is a task of finding the most\nrelevant labels from a large label set. Nowadays deep learning-based methods\nhave shown significant success in XMC. However, the existing methods (e.g.,\nAttentionXML and X-Transformer etc) still suffer from 1) combining several\nmodels to train and predict for one dataset, and 2) sampling negative labels\nstatically during the process of training label ranking model, which reduces\nboth the efficiency and accuracy of the model. To address the above problems,\nwe proposed LightXML, which adopts end-to-end training and dynamic negative\nlabels sampling. In LightXML, we use generative cooperative networks to recall\nand rank labels, in which label recalling part generates negative and positive\nlabels, and label ranking part distinguishes positive labels from these labels.\nThrough these networks, negative labels are sampled dynamically during label\nranking part training by feeding with the same text representation. Extensive\nexperiments show that LightXML outperforms state-of-the-art methods in five\nextreme multi-label datasets with much smaller model size and lower\ncomputational complexity. In particular, on the Amazon dataset with 670K\nlabels, LightXML can reduce the model size up to 72% compared to AttentionXML.", "published": "2021-01-09 07:04:18", "link": "http://arxiv.org/abs/2101.03305v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Normalization Algorithm for Noisy Text: A Case Study for\n  Information Retrieval and Stance Detection", "abstract": "A large fraction of textual data available today contains various types of\n'noise', such as OCR noise in digitized documents, noise due to informal\nwriting style of users on microblogging sites, and so on. To enable tasks such\nas search/retrieval and classification over all the available data, we need\nrobust algorithms for text normalization, i.e., for cleaning different kinds of\nnoise in the text. There have been several efforts towards cleaning or\nnormalizing noisy text; however, many of the existing text normalization\nmethods are supervised and require language-dependent resources or large\namounts of training data that is difficult to obtain. We propose an\nunsupervised algorithm for text normalization that does not need any training\ndata / human intervention. The proposed algorithm is applicable to text over\ndifferent languages, and can handle both machine-generated and human-generated\nnoise. Experiments over several standard datasets show that text normalization\nthrough the proposed algorithm enables better retrieval and stance detection,\nas compared to that using several baseline text normalization methods.\nImplementation of our algorithm can be found at\nhttps://github.com/ranarag/UnsupClean.", "published": "2021-01-09 06:57:09", "link": "http://arxiv.org/abs/2101.03303v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Task Adaptive Pretraining of Transformers for Hostility Detection", "abstract": "Identifying adverse and hostile content on the web and more particularly, on\nsocial media, has become a problem of paramount interest in recent years. With\ntheir ever increasing popularity, fine-tuning of pretrained Transformer-based\nencoder models with a classifier head are gradually becoming the new baseline\nfor natural language classification tasks. In our work, we explore the gains\nattributed to Task Adaptive Pretraining (TAPT) prior to fine-tuning of\nTransformer-based architectures. We specifically study two problems, namely,\n(a) Coarse binary classification of Hindi Tweets into Hostile or Not, and (b)\nFine-grained multi-label classification of Tweets into four categories: hate,\nfake, offensive, and defamation. Building up on an architecture which takes\nemojis and segmented hashtags into consideration for classification, we are\nable to experimentally showcase the performance upgrades due to TAPT. Our\nsystem (with team name 'iREL IIIT') ranked first in the 'Hostile Post Detection\nin Hindi' shared task with an F1 score of 97.16% for coarse-grained detection\nand a weighted F1 score of 62.96% for fine-grained multi-label classification\non the provided blind test corpora.", "published": "2021-01-09 15:45:26", "link": "http://arxiv.org/abs/2101.03382v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generate Natural Language Explanations for Recommendation", "abstract": "Providing personalized explanations for recommendations can help users to\nunderstand the underlying insight of the recommendation results, which is\nhelpful to the effectiveness, transparency, persuasiveness and trustworthiness\nof recommender systems. Current explainable recommendation models mostly\ngenerate textual explanations based on pre-defined sentence templates. However,\nthe expressiveness power of template-based explanation sentences are limited to\nthe pre-defined expressions, and manually defining the expressions require\nsignificant human efforts. Motivated by this problem, we propose to generate\nfree-text natural language explanations for personalized recommendation. In\nparticular, we propose a hierarchical sequence-to-sequence model (HSS) for\npersonalized explanation generation. Different from conventional sentence\ngeneration in NLP research, a great challenge of explanation generation in\ne-commerce recommendation is that not all sentences in user reviews are of\nexplanation purpose. To solve the problem, we further propose an auto-denoising\nmechanism based on topical item feature words for sentence generation.\nExperiments on various e-commerce product domains show that our approach can\nnot only improve the recommendation accuracy, but also the explanation quality\nin terms of the offline measures and feature words coverage. This research is\none of the initial steps to grant intelligent agents with the ability to\nexplain itself based on natural language sentences.", "published": "2021-01-09 17:00:41", "link": "http://arxiv.org/abs/2101.03392v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Are We There Yet? Learning to Localize in Embodied Instruction Following", "abstract": "Embodied instruction following is a challenging problem requiring an agent to\ninfer a sequence of primitive actions to achieve a goal environment state from\ncomplex language and visual inputs. Action Learning From Realistic Environments\nand Directives (ALFRED) is a recently proposed benchmark for this problem\nconsisting of step-by-step natural language instructions to achieve subgoals\nwhich compose to an ultimate high-level goal. Key challenges for this task\ninclude localizing target locations and navigating to them through visual\ninputs, and grounding language instructions to visual appearance of objects. To\naddress these challenges, in this study, we augment the agent's field of view\nduring navigation subgoals with multiple viewing angles, and train the agent to\npredict its relative spatial relation to the target location at each timestep.\nWe also improve language grounding by introducing a pre-trained object\ndetection module to the model pipeline. Empirical studies show that our\napproach exceeds the baseline model performance.", "published": "2021-01-09 21:49:41", "link": "http://arxiv.org/abs/2101.03431v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Tracking Short-Term Temporal Linguistic Dynamics to Characterize\n  Candidate Therapeutics for COVID-19 in the CORD-19 Corpus", "abstract": "Scientific literature tends to grow as a function of funding and interest in\na given field. Mining such literature can reveal trends that may not be\nimmediately apparent. The CORD-19 corpus represents a growing corpus of\nscientific literature associated with COVID-19. We examined the intersection of\na set of candidate therapeutics identified in a drug-repurposing study with\ntemporal instances of the CORD-19 corpus to determine if it was possible to\nfind and measure changes associated with them over time. We propose that the\ntechniques we used could form the basis of a tool to pre-screen new candidate\ntherapeutics early in the research process.", "published": "2021-01-09 23:24:05", "link": "http://arxiv.org/abs/2101.11710v1", "categories": ["q-bio.OT", "cs.CL", "cs.LG"], "primary_category": "q-bio.OT"}
{"title": "Coupling a generative model with a discriminative learning framework for\n  speaker verification", "abstract": "The speaker verification (SV) task is to decide whether an utterance is\nspoken by a target or an imposter speaker. For most studies, a log-likelihood\nratio (LLR) score is estimated based on a generative probability model on\nspeaker features and compared with a threshold for making a decision. However,\nthe generative model usually focuses on individual feature distributions, does\nnot have the discriminative feature selection ability, and is easy to be\ndistracted by nuisance features. The SV could be formulated as a binary\ndiscrimination task where neural network-based discriminative learning could be\napplied. In discriminative learning, the nuisance features could be removed\nwith the help of label supervision. However, discriminative learning pays more\nattention to classification boundaries and is prone to overfitting to a\ntraining set which may result in bad generalization on a test set. Thus, we\npropose a hybrid learning framework, i.e., coupling a joint Bayesian (JB)\ngenerative model structure and parameters with a neural discriminative learning\nframework for SV. A two-branch Siamese neural network is built with dense\nlayers that are coupled with factorized affine transforms as used in the JB\nmodel. The LLR score estimation in the JB model is formulated according to the\ndistance metric in the discriminative learning framework. By initializing the\ntwo-branch neural network with the generatively learned model parameters of the\nJB model, we train the model parameters with the pairwise samples as a binary\ndiscrimination task. Moreover, a direct evaluation metric in SV based on\nminimum empirical Bayes risk is designed and integrated as an objective\nfunction in discriminative learning. We carried out SV experiments on Speakers\nin the wild and Voxceleb. Experimental results showed that our proposed model\nimproved the performance with a large margin compared with state-of-art models\nfor SV.", "published": "2021-01-09 10:10:04", "link": "http://arxiv.org/abs/2101.03329v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
