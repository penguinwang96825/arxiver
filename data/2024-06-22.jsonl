{"title": "SS-GEN: A Social Story Generation Framework with Large Language Models", "abstract": "Children with Autism Spectrum Disorder (ASD) often misunderstand social\nsituations and struggle to participate in daily routines. Social Stories are\ntraditionally crafted by psychology experts under strict constraints to address\nthese challenges but are costly and limited in diversity. As Large Language\nModels (LLMs) advance, there's an opportunity to develop more automated,\naffordable, and accessible methods to generate Social Stories in real-time with\nbroad coverage. However, adapting LLMs to meet the unique and strict\nconstraints of Social Stories is a challenging issue. To this end, we propose\n\\textbf{SS-GEN}, a \\textbf{S}ocial \\textbf{S}tory \\textbf{GEN}eration framework\nwith LLMs. Firstly, we develop a constraint-driven sophisticated strategy named\n\\textbf{\\textsc{StarSow}} to hierarchically prompt LLMs to generate Social\nStories at scale, followed by rigorous human filtering to build a high-quality\ndataset. Additionally, we introduce \\textbf{quality assessment criteria} to\nevaluate the effectiveness of these generated stories. Considering that\npowerful closed-source large models require very complex instructions and\nexpensive API fees, we finally fine-tune smaller language models with our\ncurated high-quality dataset, achieving comparable results at lower costs and\nwith simpler instruction and deployment. This work marks a significant step in\nleveraging AI to personalize Social Stories cost-effectively for autistic\nchildren at scale, which we hope can encourage future research. The prompt,\ncode and data will release in the \\texttt{Technical Appendix} and \\texttt{Code\n\\& Data Appendix} at \\url{https://github.com/MIMIFY/SS-GEN}.", "published": "2024-06-22 00:14:48", "link": "http://arxiv.org/abs/2406.15695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex\n  Models", "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.", "published": "2024-06-22 03:20:10", "link": "http://arxiv.org/abs/2406.15718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Fact Memorization of Large Language Models", "abstract": "Fact knowledge memorization is crucial for Large Language Models (LLM) to\ngenerate factual and reliable responses. However, the behaviors of LLM fact\nmemorization remain under-explored. In this paper, we analyze the scaling laws\nfor LLM's fact knowledge and LLMs' behaviors of memorizing different types of\nfacts. We find that LLMs' fact knowledge capacity has a linear and negative\nexponential law relationship with model size and training epochs, respectively.\nEstimated by the built scaling law, memorizing the whole Wikidata's facts\nrequires training an LLM with 1000B non-embed parameters for 100 epochs,\nsuggesting that using LLMs to memorize all public facts is almost implausible\nfor a general pre-training setting. Meanwhile, we find that LLMs can generalize\non unseen fact knowledge and its scaling law is similar to general\npre-training. Additionally, we analyze the compatibility and preference of\nLLMs' fact memorization. For compatibility, we find LLMs struggle with\nmemorizing redundant facts in a unified way. Only when correlated facts have\nthe same direction and structure, the LLM can compatibly memorize them. This\nshows the inefficiency of LLM memorization for redundant facts. For preference,\nthe LLM pays more attention to memorizing more frequent and difficult facts,\nand the subsequent facts can overwrite prior facts' memorization, which\nsignificantly hinders low-frequency facts memorization. Our findings reveal the\ncapacity and characteristics of LLMs' fact knowledge learning, which provide\ndirections for LLMs' fact knowledge augmentation.", "published": "2024-06-22 03:32:09", "link": "http://arxiv.org/abs/2406.15720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DABL: Detecting Semantic Anomalies in Business Processes Using Large\n  Language Models", "abstract": "Detecting anomalies in business processes is crucial for ensuring operational\nsuccess. While many existing methods rely on statistical frequency to detect\nanomalies, it's important to note that infrequent behavior doesn't necessarily\nimply undesirability. To address this challenge, detecting anomalies from a\nsemantic viewpoint proves to be a more effective approach. However, current\nsemantic anomaly detection methods treat a trace (i.e., process instance) as\nmultiple event pairs, disrupting long-distance dependencies. In this paper, we\nintroduce DABL, a novel approach for detecting semantic anomalies in business\nprocesses using large language models (LLMs). We collect 143,137 real-world\nprocess models from various domains. By generating normal traces through the\nplayout of these process models and simulating both ordering and exclusion\nanomalies, we fine-tune Llama 2 using the resulting log. Through extensive\nexperiments, we demonstrate that DABL surpasses existing state-of-the-art\nsemantic anomaly detection methods in terms of both generalization ability and\nlearning of given processes. Users can directly apply DABL to detect semantic\nanomalies in their own datasets without the need for additional training.\nFurthermore, DABL offers the capability to interpret the causes of anomalies in\nnatural language, providing valuable insights into the detected anomalies.", "published": "2024-06-22 08:20:19", "link": "http://arxiv.org/abs/2406.15781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis", "abstract": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.", "published": "2024-06-22 09:40:07", "link": "http://arxiv.org/abs/2406.15796v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CaT-BENCH: Benchmarking Language Model Understanding of Causal and\n  Temporal Dependencies in Plans", "abstract": "Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.", "published": "2024-06-22 11:46:04", "link": "http://arxiv.org/abs/2406.15823v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Analysis of Language Varieties in Italy", "abstract": "Italy exhibits rich linguistic diversity across its territory due to the\ndistinct regional languages spoken in different areas. Recent advances in\nself-supervised learning provide new opportunities to analyze Italy's\nlinguistic varieties using speech data alone. This includes the potential to\nleverage representations learned from large amounts of data to better examine\nnuances between closely related linguistic varieties. In this study, we focus\non automatically identifying the geographic region of origin of speech samples\ndrawn from Italy's diverse language varieties. We leverage self-supervised\nlearning models to tackle this task and analyze differences and similarities\nbetween Italy's regional languages. In doing so, we also seek to uncover new\ninsights into the relationships among these diverse yet closely related\nvarieties, which may help linguists understand their interconnected evolution\nand regional development over time and space. To improve the discriminative\nability of learned representations, we evaluate several supervised contrastive\nlearning objectives, both as pre-training steps and additional fine-tuning\nobjectives. Experimental evidence shows that pre-trained self-supervised models\ncan effectively identify regions from speech recording. Additionally,\nincorporating contrastive objectives during fine-tuning improves classification\naccuracy and yields embeddings that distinctly separate regional varieties,\ndemonstrating the value of combining self-supervised pre-training and\ncontrastive learning for this task.", "published": "2024-06-22 14:19:51", "link": "http://arxiv.org/abs/2406.15862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multitask learning framework for leveraging subjectivity of annotators\n  to identify misogyny", "abstract": "Identifying misogyny using artificial intelligence is a form of combating\nonline toxicity against women. However, the subjective nature of interpreting\nmisogyny poses a significant challenge to model the phenomenon. In this paper,\nwe propose a multitask learning approach that leverages the subjectivity of\nthis task to enhance the performance of the misogyny identification systems. We\nincorporated diverse perspectives from annotators in our model design,\nconsidering gender and age across six profile groups, and conducted extensive\nexperiments and error analysis using two language models to validate our four\nalternative designs of the multitask learning technique to identify\nmisogynistic content in English tweets. The results demonstrate that\nincorporating various viewpoints enhances the language models' ability to\ninterpret different forms of misogyny. This research advances content\nmoderation and highlights the importance of embracing diverse perspectives to\nbuild effective online moderation systems.", "published": "2024-06-22 15:06:08", "link": "http://arxiv.org/abs/2406.15869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unlikely Duel: Evaluating Creative Writing in LLMs through a Unique\n  Scenario", "abstract": "This is a summary of the paper \"A Confederacy of Models: a Comprehensive\nEvaluation of LLMs on Creative Writing\", which was published in Findings of\nEMNLP 2023. We evaluate a range of recent state-of-the-art, instruction-tuned\nlarge language models (LLMs) on an English creative writing task, and compare\nthem to human writers. For this purpose, we use a specifically-tailored prompt\n(based on an epic combat between Ignatius J. Reilly, main character of John\nKennedy Toole's \"A Confederacy of Dunces\", and a pterodactyl) to minimize the\nrisk of training data leakage and force the models to be creative rather than\nreusing existing stories. The same prompt is presented to LLMs and human\nwriters, and evaluation is performed by humans using a detailed rubric\nincluding various aspects like fluency, style, originality or humor. Results\nshow that some state-of-the-art commercial LLMs match or slightly outperform\nour human writers in most of the evaluated dimensions. Open-source LLMs lag\nbehind. Humans keep a close lead in originality, and only the top three LLMs\ncan handle humor at human-like levels.", "published": "2024-06-22 17:01:59", "link": "http://arxiv.org/abs/2406.15891v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "abstract": "Multilingual LLMs often have knowledge disparities across languages, with\nlarger gaps in under-resourced languages. Teaching LLMs to abstain in the face\nof knowledge gaps is thus a promising strategy to mitigate hallucinations in\nmultilingual settings. However, previous studies on LLM abstention primarily\nfocus on English; we find that directly applying existing solutions beyond\nEnglish results in up to 20.5% performance gaps between high and low-resource\nlanguages, potentially due to LLMs' drop in calibration and reasoning beyond a\nfew resource-rich languages. To this end, we propose strategies to enhance LLM\nabstention by learning from multilingual feedback, where LLMs self-reflect on\nproposed answers in one language by generating multiple feedback items in\nrelated languages: we show that this helps identifying the knowledge gaps\nacross diverse languages, cultures, and communities. Extensive experiments\ndemonstrate that our multilingual feedback approach outperforms various strong\nbaselines, achieving up to 9.2% improvement for low-resource languages across\nthree black-box and open models on three datasets, featuring open-book,\nclosed-book, and commonsense QA. Further analysis reveals that multilingual\nfeedback is both an effective and a more equitable abstain strategy to serve\ndiverse language speakers, and cultural factors have great impact on language\nselection and LLM abstention behavior, highlighting future directions for\nmultilingual and multi-cultural reliable language modeling.", "published": "2024-06-22 21:59:12", "link": "http://arxiv.org/abs/2406.15948v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration", "abstract": "While existing alignment paradigms have been integral in developing large\nlanguage models (LLMs), LLMs often learn an averaged human preference and\nstruggle to model diverse preferences across cultures, demographics, and\ncommunities. We propose Modular Pluralism, a modular framework based on\nmulti-LLM collaboration for pluralistic alignment: it \"plugs into\" a base LLM a\npool of smaller but specialized community LMs, where models collaborate in\ndistinct modes to flexibility support three modes of pluralism: Overton,\nsteerable, and distributional. Modular Pluralism is uniquely compatible with\nblack-box LLMs and offers the modular control of adding new community LMs for\npreviously underrepresented communities. We evaluate Modular Pluralism with six\ntasks and four datasets featuring questions/instructions with value-laden and\nperspective-informed responses. Extensive experiments demonstrate that Modular\nPluralism advances the three pluralism objectives across six black-box and\nopen-source LLMs. Further analysis reveals that LLMs are generally faithful to\nthe inputs from smaller community LLMs, allowing seamless patching by adding a\nnew community LM to better cover previously underrepresented communities.", "published": "2024-06-22 22:07:40", "link": "http://arxiv.org/abs/2406.15951v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning\n  Pruned LLMs via Performance Model", "abstract": "The efficient compression of large language models (LLMs) has become\nincreasingly popular. However, recovering the performance of compressed LLMs\nremains a major challenge. The current practice in LLM compression entails the\nimplementation of structural pruning, complemented by a recovery phase that\nleverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning's uneven\nmodification of model architecture, coupled with standard LoRA's fixed\nconfiguration allocation across layers in an online pipeline, leads to\nsuboptimal performance in various downstream tasks for pruned models. To\naddress this challenge, we introduce RankAdaptor, a hierarchical rank\nallocation method that enables efficient fine-tuning of pruned LLMs according\nto layerwise specific recovery requirements. We employ a performance model that\nconducts offline meta-learning and online incremental learning to explore\noptimal rank values for each layer. Comprehensive experiments on popular\nbenchmarks show that RankAdaptor consistently outperforms state-of-the-art\nmethods across a variety of pruning settings and LLM architectures, with\nimprovements ranging from 0.7\\% to 5.5\\%.", "published": "2024-06-22 04:52:58", "link": "http://arxiv.org/abs/2406.15734v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large\n  Language Models without Training through Attention Calibration", "abstract": "Attention is a fundamental component behind the remarkable achievements of\nlarge language models (LLMs). However, our current understanding of the\nattention mechanism, especially regarding how attention distributions are\nestablished, remains limited. Inspired by recent studies that explore the\npresence of attention sink in the initial token, which receives\ndisproportionately large attention scores despite their lack of semantic\nimportance, this work delves deeper into this phenomenon. We aim to provide a\nmore profound understanding of the existence of attention sinks within LLMs and\nto uncover ways to enhance the achievable accuracy of LLMs by directly\noptimizing the attention distributions, without the need for weight finetuning.\nSpecifically, this work begins with comprehensive visualizations of the\nattention distributions in LLMs during inference across various inputs and\ntasks. Based on these visualizations, to the best of our knowledge, we are the\nfirst to discover that (1) attention sinks occur not only at the start of\nsequences but also within later tokens of the input, and (2) not all attention\nsinks have a positive impact on the achievable accuracy of LLMs. Building upon\nour findings, we propose a training-free Attention Calibration Technique (ACT)\nthat automatically optimizes the attention distributions on the fly during\ninference in an input-adaptive manner. Extensive experiments validate that ACT\nconsistently enhances the accuracy of various LLMs across different\napplications. Specifically, ACT achieves an average improvement of up to 7.30%\nin accuracy across different datasets when applied to Llama-30B. Our code is\navailable at https://github.com/GATECH-EIC/ACT.", "published": "2024-06-22 07:00:43", "link": "http://arxiv.org/abs/2406.15765v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided\n  Extractive Summarization of User Incident Reports", "abstract": "Citizen reporting platforms like Safe City in India help the public and\nauthorities stay informed about sexual harassment incidents. However, the high\nvolume of data shared on these platforms makes reviewing each individual case\nchallenging. Therefore, a summarization algorithm capable of processing and\nunderstanding various Indian code-mixed languages is essential. In recent\nyears, Large Language Models (LLMs) have shown exceptional performance in NLP\ntasks, including summarization. LLMs inherently produce abstractive summaries\nby paraphrasing the original text, while the generation of extractive summaries\n- selecting specific subsets from the original text - through LLMs remains\nlargely unexplored. Moreover, LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle these\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries for large collections of Safe City posts using\nLLMs. LaMSUM integrates summarization with different voting methods to achieve\nrobust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral\nand GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive\nsummarization methods for Safe City posts. Overall, this work represents one of\nthe first attempts to achieve extractive summarization through LLMs, and is\nlikely to support stakeholders by offering a comprehensive overview and\nenabling them to develop effective policies to minimize incidents of\nunwarranted harassment.", "published": "2024-06-22 10:25:55", "link": "http://arxiv.org/abs/2406.15809v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Interpolation Augmentation for Speech-to-Text Generation", "abstract": "Speech-to-text (S2T) generation systems frequently face challenges in\nlow-resource scenarios, primarily due to the lack of extensive labeled\ndatasets. One emerging solution is constructing virtual training samples by\ninterpolating inputs and labels, which has notably enhanced system\ngeneralization in other domains. Despite its potential, this technique's\napplication in S2T tasks has remained under-explored. In this paper, we delve\ninto the utility of interpolation augmentation, guided by several pivotal\nquestions. Our findings reveal that employing an appropriate strategy in\ninterpolation augmentation significantly enhances performance across diverse\ntasks, architectures, and data scales, offering a promising avenue for more\nrobust S2T systems in resource-constrained settings.", "published": "2024-06-22 13:24:58", "link": "http://arxiv.org/abs/2406.15846v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Uncovering Hidden Intentions: Exploring Prompt Recovery for Deeper\n  Insights into Generated Texts", "abstract": "Today, the detection of AI-generated content is receiving more and more\nattention. Our idea is to go beyond detection and try to recover the prompt\nused to generate a text. This paper, to the best of our knowledge, introduces\nthe first investigation in this particular domain without a closed set of\ntasks. Our goal is to study if this approach is promising. We experiment with\nzero-shot and few-shot in-context learning but also with LoRA fine-tuning.\nAfter that, we evaluate the benefits of using a semi-synthetic dataset. For\nthis first study, we limit ourselves to text generated by a single model. The\nresults show that it is possible to recover the original prompt with a\nreasonable degree of accuracy.", "published": "2024-06-22 15:16:11", "link": "http://arxiv.org/abs/2406.15871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimSMoE: Solving Representational Collapse via Similarity Measure", "abstract": "Sparse mixture of experts (SMoE) have emerged as an effective approach for\nscaling large language models while keeping a constant computational cost.\nRegardless of several notable successes of SMoE, effective training such\narchitecture remains elusive due to the representation collapse problem, which\nin turn harms model performance and causes parameter redundancy. In this work,\nwe present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel\nsimilarity of neural network algorithm, that guarantees a solution to address\nthe representation collapse issue between experts given a fixed FLOPs budget.\nWe conduct extensive empirical evaluations on three large language models for\nboth Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness,\nand scalability of our method. The results demonstrate that SimSMoE\nsignificantly enhances existing routing policy and outperforms other SMoE\ntraining methods in performance for the tasks.", "published": "2024-06-22 16:10:45", "link": "http://arxiv.org/abs/2406.15883v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Teach Better or Show Smarter? On Instructions and Exemplars in Automatic\n  Prompt Optimization", "abstract": "Large language models have demonstrated remarkable capabilities, but their\nperformance is heavily reliant on effective prompt engineering. Automatic\nprompt optimization (APO) methods are designed to automate this and can be\nbroadly categorized into those targeting instructions (instruction\noptimization, IO) vs. those targeting exemplars (exemplar optimization, EO).\nDespite their shared objective, these have evolved rather independently, with\nIO receiving more research attention recently. This paper seeks to bridge this\ngap by comprehensively comparing the performance of representative IO and EO\ntechniques both isolation and combination on a diverse set of challenging\ntasks. Our findings reveal that intelligently reusing model-generated\ninput-output pairs obtained from evaluating prompts on the validation set as\nexemplars, consistently improves performance on top of IO methods but is\ncurrently under-investigated. We also find that despite the recent focus on IO,\nhow we select exemplars can outweigh how we optimize instructions, with EO\nstrategies as simple as random search outperforming state-of-the-art IO methods\nwith seed instructions without any optimization. Moreover, we observe a synergy\nbetween EO and IO, with optimal combinations surpassing the individual\ncontributions. We conclude that studying exemplar optimization both as a\nstandalone method and its optimal combination with instruction optimization\nremain a crucial aspect of APO and deserve greater consideration in future\nresearch, even in the era of highly capable instruction-following models.", "published": "2024-06-22 02:07:10", "link": "http://arxiv.org/abs/2406.15708v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic Feature Mixup for Balanced Multi-aspect Pronunciation\n  Assessment", "abstract": "In automated pronunciation assessment, recent emphasis progressively lies on\nevaluating multiple aspects to provide enriched feedback. However, acquiring\nmulti-aspect-score labeled data for non-native language learners' speech poses\nchallenges; moreover, it often leads to score-imbalanced distributions. In this\npaper, we propose two Acoustic Feature Mixup strategies, linearly and\nnon-linearly interpolating with the in-batch averaged feature, to address data\nscarcity and score-label imbalances. Primarily using goodness-of-pronunciation\nas an acoustic feature, we tailor mixup designs to suit pronunciation\nassessment. Further, we integrate fine-grained error-rate features by comparing\nspeech recognition results with the original answer phonemes, giving direct\nhints for mispronunciation. Effective mixing of the acoustic features notably\nenhances overall scoring performances on the speechocean762 dataset, and\ndetailed analysis highlights our potential to predict unseen distortions.", "published": "2024-06-22 03:56:29", "link": "http://arxiv.org/abs/2406.15723v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Vision-and-Language Models on Children's Mathematical\n  Olympiads", "abstract": "Recent years have seen a significant progress in the general-purpose problem\nsolving abilities of large vision and language models (LVLMs), such as ChatGPT,\nGemini, etc.; some of these breakthroughs even seem to enable AI models to\noutperform human abilities in varied tasks that demand higher-order cognitive\nskills. Are the current large AI models indeed capable of generalized problem\nsolving as humans do? A systematic analysis of AI capabilities for joint vision\nand text reasoning, however, is missing in the current scientific literature.\nIn this paper, we make an effort towards filling this gap, by evaluating\nstate-of-the-art LVLMs on their mathematical and algorithmic reasoning\nabilities using visuo-linguistic problems from children's Olympiads.\nSpecifically, we consider problems from the Mathematical Kangaroo (MK)\nOlympiad, which is a popular international competition targeted at children\nfrom grades 1-12, that tests children's deeper mathematical abilities using\npuzzles that are appropriately gauged to their age and skills. Using the\npuzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840\nproblems from years 2020-2024. With our dataset, we analyze LVLMs power on\nmathematical reasoning; their responses on our puzzles offer a direct way to\ncompare against that of children. Our results show that modern LVLMs do\ndemonstrate increasingly powerful reasoning skills in solving problems for\nhigher grades, but lack the foundations to correctly answer problems designed\nfor younger children. Further analysis shows that there is no significant\ncorrelation between the reasoning capabilities of AI models and that of young\nchildren, and their capabilities appear to be based on a different type of\nreasoning than the cumulative knowledge that underlies children's mathematics\nand logic skills.", "published": "2024-06-22 05:04:39", "link": "http://arxiv.org/abs/2406.15736v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level", "abstract": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/MT-Ladder", "published": "2024-06-22 05:33:35", "link": "http://arxiv.org/abs/2406.15741v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient\n  Zero-Shot Text to Speech Synthesizers", "abstract": "Neural codec language model (LM) has demonstrated strong capability in\nzero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers\nfrom limitations in inference speed and stability, due to its auto-regressive\nnature and implicit alignment between text and audio. In this work, to handle\nthese challenges, we introduce a new variant of neural codec LM, namely TacoLM.\nSpecifically, TacoLM introduces a gated attention mechanism to improve the\ntraining and inference efficiency and reduce the model size. Meanwhile, an\nadditional gated cross-attention layer is included for each decoder layer,\nwhich improves the efficiency and content accuracy of the synthesized speech.\nIn the evaluation of the Librispeech corpus, the proposed TacoLM achieves a\nbetter word error rate, speaker similarity, and mean opinion score, with 90%\nfewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is\navailable at https://ereboas.github.io/TacoLM/.", "published": "2024-06-22 06:39:52", "link": "http://arxiv.org/abs/2406.15752v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "What Matters in Transformers? Not All Attention is Needed", "abstract": "While scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks, it also introduces redundant\narchitectures, posing efficiency challenges for real-world deployment. Despite\nsome recognition of redundancy in LLMs, the variability of redundancy across\ndifferent architectures in transformers, such as MLP and Attention layers, is\nunder-explored. In this work, we investigate redundancy across different\nmodules within Transformers, including Blocks, MLP, and Attention layers, using\na similarity-based metric. Surprisingly, despite the critical role of attention\nlayers in distinguishing transformers from other architectures, we found that a\nlarge portion of these layers exhibit excessively high similarity and can be\npruned without degrading performance. For instance, Llama-2-70B achieved a\n48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the\nattention layers. Furthermore, by tracing model checkpoints throughout the\ntraining process, we observed that attention layer redundancy is inherent and\nconsistent across training stages. Additionally, we further propose a method\nthat jointly drops Attention and MLP layers, allowing us to more aggressively\ndrop additional layers. For instance, when dropping 31 layers (Attention +\nMLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our\nwork provides valuable insights for future network architecture design. The\ncode is released at: \\url{https://github.com/Shwai-He/LLM-Drop}.", "published": "2024-06-22 08:41:48", "link": "http://arxiv.org/abs/2406.15786v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Intrinsic Dimension Correlation: uncovering nonlinear connections in\n  multimodal representations", "abstract": "To gain insight into the mechanisms behind machine learning methods, it is\ncrucial to establish connections among the features describing data points.\nHowever, these correlations often exhibit a high-dimensional and strongly\nnonlinear nature, which makes them challenging to detect using standard\nmethods. This paper exploits the entanglement between intrinsic dimensionality\nand correlation to propose a metric that quantifies the (potentially nonlinear)\ncorrelation between high-dimensional manifolds. We first validate our method on\nsynthetic data in controlled environments, showcasing its advantages and\ndrawbacks compared to existing techniques. Subsequently, we extend our analysis\nto large-scale applications in neural network representations. Specifically, we\nfocus on latent representations of multimodal data, uncovering clear\ncorrelations between paired visual and textual embeddings, whereas existing\nmethods struggle significantly in detecting similarity. Our results indicate\nthe presence of highly nonlinear correlation patterns between latent manifolds.", "published": "2024-06-22 10:36:04", "link": "http://arxiv.org/abs/2406.15812v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions", "abstract": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.", "published": "2024-06-22 15:52:04", "link": "http://arxiv.org/abs/2406.15877v4", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Language Alignment via Nash-learning and Adaptive feedback", "abstract": "Recent research has shown the potential of Nash Learning via Human Feedback\nfor large language model alignment by incorporating the notion of a preference\nmodel in a minimax game setup. We take this idea further by casting the\nalignment as a mirror descent algorithm against the adaptive feedback of an\nimproved opponent, thereby removing the need for learning a preference model or\nthe existence of an annotated dataset altogether. The resulting algorithm,\nwhich we refer to as Language Alignment via Nash-learning and Adaptive feedback\n(LANA), is capable of self-alignment without the need for a human-annotated\npreference dataset. We support this statement with various experiments and\nmathematical discussion.", "published": "2024-06-22 16:55:21", "link": "http://arxiv.org/abs/2406.15890v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.LG"}
{"title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in\n  LLMs", "abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for\nuncertainty quantification in Large Language Models (LLMs). Hallucinations,\nwhich are plausible-sounding but factually incorrect and arbitrary model\ngenerations, present a major challenge to the practical adoption of LLMs.\nRecent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can\ndetect hallucinations by estimating uncertainty in the space semantic meaning\nfor a set of model generations. However, the 5-to-10-fold increase in\ncomputation cost associated with SE computation hinders practical adoption. To\naddress this, we propose SEPs, which directly approximate SE from the hidden\nstates of a single generation. SEPs are simple to train and do not require\nsampling multiple model generations at test time, reducing the overhead of\nsemantic uncertainty quantification to almost zero. We show that SEPs retain\nhigh performance for hallucination detection and generalize better to\nout-of-distribution data than previous probing methods that directly predict\nmodel accuracy. Our results across models and tasks suggest that model hidden\nstates capture SE, and our ablation studies give further insights into the\ntoken positions and model layers for which this is the case.", "published": "2024-06-22 19:46:06", "link": "http://arxiv.org/abs/2406.15927v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RuleR: Improving LLM Controllability by Rule-based Data Recycling", "abstract": "Large language models (LLMs) still lack delicate controllability over their\nresponses, which is critical to enhancing their performance and the user\nexperience. However, curating supervised fine-tuning (SFT) datasets to improve\nLLM controllability usually relies on human experts or proprietary LLMs, which\nrequires additional costs. To bridge this gap, we propose Rule-based Data\nRecycling (RuleR), a data augmentation method incorporating multiple\nconstraints into the original data samples according to predefined rules, which\ncreates new training tasks to consolidate the controllability of LLMs. Instead\nof creating new data from scratch, RuleR \"recycles\" existing data by simply\napplying rule-based edits to their responses and appending the\nrule-instructions in their original instructions. Experimental results\ndemonstrate RuleR's effectiveness in improving LLM controllability while\nmaintaining general instruction-following capabilities.", "published": "2024-06-22 20:57:12", "link": "http://arxiv.org/abs/2406.15938v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Individual Facts: Investigating Categorical Knowledge Locality of\n  Taxonomy and Meronomy Concepts in GPT Models", "abstract": "The location of knowledge within Generative Pre-trained Transformer\n(GPT)-like models has seen extensive recent investigation. However, much of the\nwork is focused towards determining locations of individual facts, with the end\ngoal being the editing of facts that are outdated, erroneous, or otherwise\nharmful, without the time and expense of retraining the entire model. In this\nwork, we investigate a broader view of knowledge location, that of concepts or\nclusters of related information, instead of disparate individual facts. To do\nthis, we first curate a novel dataset, called DARC, that includes a total of 34\nconcepts of ~120K factual statements divided into two types of hierarchical\ncategories, namely taxonomy and meronomy. Next, we utilize existing causal\nmediation analysis methods developed for determining regions of importance for\nindividual facts and apply them to a series of related categories to provide\ndetailed investigation into whether concepts are associated with distinct\nregions within these models. We find that related categories exhibit similar\nareas of importance in contrast to less similar categories. However,\nfine-grained localization of individual category subsets to specific regions is\nnot apparent.", "published": "2024-06-22 21:12:57", "link": "http://arxiv.org/abs/2406.15940v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MetaGreen: Meta-Learning Inspired Transformer Selection for Green\n  Semantic Communication", "abstract": "Semantic Communication can transform the way we transmit information,\nprioritizing meaningful and effective content over individual symbols or bits.\nThis evolution promises significant benefits, including reduced latency, lower\nbandwidth usage, and higher throughput compared to traditional communication.\nHowever, the development of Semantic Communication faces a crucial challenge:\nthe need for universal metrics to benchmark the joint effects of semantic\ninformation loss and energy consumption. This research introduces an innovative\nsolution: the ``Energy-Optimized Semantic Loss'' (EOSL) function, a novel\nmulti-objective loss function that effectively balances semantic information\nloss and energy consumption. Through comprehensive experiments on transformer\nmodels, including energy benchmarking, we demonstrate the remarkable\neffectiveness of EOSL-based model selection. We have established that\nEOSL-based transformer model selection achieves up to 83\\% better\nsimilarity-to-power ratio (SPR) compared to BLEU score-based selection and 67\\%\nbetter SPR compared to solely lowest power usage-based selection. Furthermore,\nwe extend the applicability of EOSL to diverse and varying contexts, inspired\nby the principles of Meta-Learning. By cumulatively applying EOSL, we enable\nthe model selection system to adapt to this change, leveraging historical EOSL\nvalues to guide the learning process. This work lays the foundation for\nenergy-efficient model selection and the development of green semantic\ncommunication.", "published": "2024-06-22 00:49:40", "link": "http://arxiv.org/abs/2406.16962v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A multi-speaker multi-lingual voice cloning system based on vits2 for\n  limmits 2024 challenge", "abstract": "This paper presents the development of a speech synthesis system for the\nLIMMITS'24 Challenge, focusing primarily on Track 2. The objective of the\nchallenge is to establish a multi-speaker, multi-lingual Indic Text-to-Speech\nsystem with voice cloning capabilities, covering seven Indian languages with\nboth male and female speakers. The system was trained using challenge data and\nfine-tuned for few-shot voice cloning on target speakers. Evaluation included\nboth mono-lingual and cross-lingual synthesis across all seven languages, with\nsubjective tests assessing naturalness and speaker similarity. Our system uses\nthe VITS2 architecture, augmented with a multi-lingual ID and a BERT model to\nenhance contextual language comprehension. In Track 1, where no additional data\nusage was permitted, our model achieved a Speaker Similarity score of 4.02. In\nTrack 2, which allowed the use of extra data, it attained a Speaker Similarity\nscore of 4.17.", "published": "2024-06-22 10:49:36", "link": "http://arxiv.org/abs/2406.17801v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Understanding the Role of User Profile in the Personalization of Large\n  Language Models", "abstract": "Utilizing user profiles to personalize Large Language Models (LLMs) has been\nshown to enhance the performance on a wide range of tasks. However, the precise\nrole of user profiles and their effect mechanism on LLMs remains unclear. This\nstudy first confirms that the effectiveness of user profiles is primarily due\nto personalization information rather than semantic information. Furthermore,\nwe investigate how user profiles affect the personalization of LLMs. Within the\nuser profile, we reveal that it is the historical personalized response\nproduced or approved by users that plays a pivotal role in personalizing LLMs.\nThis discovery unlocks the potential of LLMs to incorporate a greater number of\nuser profiles within the constraints of limited input length. As for the\nposition of user profiles, we observe that user profiles integrated into\ndifferent positions of the input context do not contribute equally to\npersonalization. Instead, where the user profile that is closer to the\nbeginning affects more on the personalization of LLMs. Our findings reveal the\nrole of user profiles for the personalization of LLMs, and showcase how\nincorporating user profiles impacts performance providing insight to leverage\nuser profiles effectively.", "published": "2024-06-22 14:32:35", "link": "http://arxiv.org/abs/2406.17803v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can LLMs Generate Visualizations with Dataless Prompts?", "abstract": "Recent advancements in large language models have revolutionized information\naccess, as these models harness data available on the web to address complex\nqueries, becoming the preferred information source for many users. In certain\ncases, queries are about publicly available data, which can be effectively\nanswered with data visualizations. In this paper, we investigate the ability of\nlarge language models to provide accurate data and relevant visualizations in\nresponse to such queries. Specifically, we investigate the ability of GPT-3 and\nGPT-4 to generate visualizations with dataless prompts, where no data\naccompanies the query. We evaluate the results of the models by comparing them\nto visualization cheat sheets created by visualization experts.", "published": "2024-06-22 22:59:09", "link": "http://arxiv.org/abs/2406.17805v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations", "abstract": "In this paper we examine the limitations of Large Language Models (LLMs) for\ncomplex reasoning tasks. Although recent works have started to employ formal\nlanguages as an intermediate representation for reasoning tasks, they often\nface challenges in accurately generating and refining these formal\nspecifications to ensure correctness. To address these issues, this paper\nproposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs\nto do pairwise comparisons, allowing the evaluation of the refinements\nsuggested by the LLM. The paper demonstrates that Logic-LM++ outperforms\nLogic-LM and other contemporary techniques across natural language reasoning\ntasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average\nimprovement of 18.5% on standard prompting, 12.3% on chain of thought prompting\nand 5% on Logic-LM.", "published": "2024-06-22 12:50:41", "link": "http://arxiv.org/abs/2407.02514v3", "categories": ["cs.LO", "cs.AI", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Multimodal Segmentation for Vocal Tract Modeling", "abstract": "Accurate modeling of the vocal tract is necessary to construct articulatory\nrepresentations for interpretable speech processing and linguistics. However,\nvocal tract modeling is challenging because many internal articulators are\noccluded from external motion capture technologies. Real-time magnetic\nresonance imaging (RT-MRI) allows measuring precise movements of internal\narticulators during speech, but annotated datasets of MRI are limited in size\ndue to time-consuming and computationally expensive labeling methods. We first\npresent a deep labeling strategy for the RT-MRI video using a vision-only\nsegmentation approach. We then introduce a multimodal algorithm using audio to\nimprove segmentation of vocal articulators. Together, we set a new benchmark\nfor vocal tract modeling in MRI video segmentation and use this to release\nlabels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public\nRT-MRI data of the vocal tract by over a factor of 9. The code and dataset\nlabels can be found at \\url{rishiraij.github.io/multimodal-mri-avatar/}.", "published": "2024-06-22 06:44:38", "link": "http://arxiv.org/abs/2406.15754v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Real-time Speech Summarization for Medical Conversations", "abstract": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum", "published": "2024-06-22 16:37:51", "link": "http://arxiv.org/abs/2406.15888v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe\n  Queries?", "abstract": "Humans are prone to cognitive distortions -- biased thinking patterns that\nlead to exaggerated responses to specific stimuli, albeit in very different\ncontexts. This paper demonstrates that advanced Multimodal Large Language\nModels (MLLMs) exhibit similar tendencies. While these models are designed to\nrespond queries under safety mechanism, they sometimes reject harmless queries\nin the presence of certain visual stimuli, disregarding the benign nature of\ntheir contexts. As the initial step in investigating this behavior, we identify\nthree types of stimuli that trigger the oversensitivity of existing MLLMs:\nExaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To\nsystematically evaluate MLLMs' oversensitivity to these stimuli, we propose the\nMultimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300\nmanually collected benign multimodal queries, cross-verified by third-party\nreviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several\ninsights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal\nrates reaching up to 76% for harmless queries. (2). Safer models are more\noversensitive: increasing safety may inadvertently raise caution and\nconservatism in the model's responses. (3). Different types of stimuli tend to\ncause errors at specific stages -- perception, intent reasoning, and safety\njudgement -- in the response process of MLLMs. These findings highlight the\nneed for refined safety mechanisms that balance caution with contextually\nappropriate responses, improving the reliability of MLLMs in real-world\napplications. We make our project available at\nhttps://turningpoint-ai.github.io/MOSSBench/.", "published": "2024-06-22 23:26:07", "link": "http://arxiv.org/abs/2406.17806v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self Training and Ensembling Frequency Dependent Networks with Coarse\n  Prediction Pooling and Sound Event Bounding Boxes", "abstract": "To tackle sound event detection (SED), we propose frequency dependent\nnetworks (FreDNets), which heavily leverage frequency-dependent methods. We\napply frequency warping and FilterAugment, which are frequency-dependent data\naugmentation methods. The model architecture consists of 3 branches: audio\nteacher-student transformer (ATST) branch, BEATs branch and CNN branch\nincluding either partial dilated frequency dynamic convolution (PDFD conv) or\nsqueeze-and-Excitation (SE) with time-frame frequency-wise SE (tfwSE). To train\nMAESTRO labels with coarse temporal resolution, we applied max pooling on\nprediction for the MAESTRO dataset. Using best ensemble model, we applied self\ntraining to obtain pseudo label from DESED weak set, unlabeled set and\nAudioSet. AudioSet pseudo labels, filtered to focus on high-confidence labels,\nare used to train on DESED dataset only. We used change-detection-based sound\nevent bounding boxes (cSEBBs) as post processing for ensemble models on self\ntraining and submission models. The resulting FreDNet was ranked 2nd in DCASE\n2024 Challenge Task 4.", "published": "2024-06-22 04:07:55", "link": "http://arxiv.org/abs/2406.15725v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Unsupervised Clean-to-Rendered Guitar Tone Transformation\n  Using GANs and Integrated Unaligned Clean Data", "abstract": "Recent years have seen increasing interest in applying deep learning methods\nto the modeling of guitar amplifiers or effect pedals. Existing methods are\nmainly based on the supervised approach, requiring temporally-aligned data\npairs of unprocessed and rendered audio. However, this approach does not scale\nwell, due to the complicated process involved in creating the data pairs. A\nvery recent work done by Wright et al. has explored the potential of leveraging\nunpaired data for training, using a generative adversarial network (GAN)-based\nframework. This paper extends their work by using more advanced discriminators\nin the GAN, and using more unpaired data for training. Specifically, drawing\ninspiration from recent advancements in neural vocoders, we employ in our\nGAN-based model for guitar amplifier modeling two sets of discriminators, one\nbased on multi-scale discriminator (MSD) and the other multi-period\ndiscriminator (MPD). Moreover, we experiment with adding unprocessed audio\nsignals that do not have the corresponding rendered audio of a target tone to\nthe training data, to see how much the GAN model benefits from the unpaired\ndata. Our experiments show that the proposed two extensions contribute to the\nmodeling of both low-gain and high-gain guitar amplifiers.", "published": "2024-06-22 06:36:05", "link": "http://arxiv.org/abs/2406.15751v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AI-based Drone Assisted Human Rescue in Disaster Environments:\n  Challenges and Opportunities", "abstract": "In this survey we are focusing on utilizing drone-based systems for the\ndetection of individuals, particularly by identifying human screams and other\ndistress signals. This study has significant relevance in post-disaster\nscenarios, including events such as earthquakes, hurricanes, military\nconflicts, wildfires, and more. These drones are capable of hovering over\ndisaster-stricken areas that may be challenging for rescue teams to access\ndirectly. Unmanned aerial vehicles (UAVs), commonly referred to as drones, are\nfrequently deployed for search-and-rescue missions during disaster situations.\nTypically, drones capture aerial images to assess structural damage and\nidentify the extent of the disaster. They also employ thermal imaging\ntechnology to detect body heat signatures, which can help locate individuals.\nIn some cases, larger drones are used to deliver essential supplies to people\nstranded in isolated disaster-stricken areas. In our discussions, we delve into\nthe unique challenges associated with locating humans through aerial acoustics.\nThe auditory system must distinguish between human cries and sounds that occur\nnaturally, such as animal calls and wind. Additionally, it should be capable of\nrecognizing distinct patterns related to signals like shouting, clapping, or\nother ways in which people attempt to signal rescue teams. To tackle this\nchallenge, one solution involves harnessing artificial intelligence (AI) to\nanalyze sound frequencies and identify common audio signatures. Deep\nlearning-based networks, such as convolutional neural networks (CNNs), can be\ntrained using these signatures to filter out noise generated by drone motors\nand other environmental factors. Furthermore, employing signal processing\ntechniques like the direction of arrival (DOA) based on microphone array\nsignals can enhance the precision of tracking the source of human noises.", "published": "2024-06-22 15:39:46", "link": "http://arxiv.org/abs/2406.15875v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "68U10, 68T50(Primary) 68T45 (Secondary)", "I.2.7; I.2.10; I.4.0"], "primary_category": "cs.SD"}
{"title": "The Music Maestro or The Musically Challenged, A Massive Music\n  Evaluation Benchmark for Large Language Models", "abstract": "Benchmark plays a pivotal role in assessing the advancements of large\nlanguage models (LLMs). While numerous benchmarks have been proposed to\nevaluate LLMs' capabilities, there is a notable absence of a dedicated\nbenchmark for assessing their musical abilities. To address this gap, we\npresent ZIQI-Eval, a comprehensive and large-scale music benchmark specifically\ndesigned to evaluate the music-related capabilities of LLMs. ZIQI-Eval\nencompasses a wide range of questions, covering 10 major categories and 56\nsubcategories, resulting in over 14,000 meticulously curated data entries. By\nleveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to\nevaluate and analyze LLMs' performance in the domain of music. Results indicate\nthat all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant\nroom for improvement in their musical capabilities. With ZIQI-Eval, we aim to\nprovide a standardized and robust evaluation framework that facilitates a\ncomprehensive assessment of LLMs' music-related abilities. The dataset is\navailable at GitHub\\footnote{https://github.com/zcli-charlie/ZIQI-Eval} and\nHuggingFace\\footnote{https://huggingface.co/datasets/MYTH-Lab/ZIQI-Eval}.", "published": "2024-06-22 16:24:42", "link": "http://arxiv.org/abs/2406.15885v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fusing Audio and Metadata Embeddings Improves Language-based Audio\n  Retrieval", "abstract": "Matching raw audio signals with textual descriptions requires understanding\nthe audio's content and the description's semantics and then drawing\nconnections between the two modalities. This paper investigates a hybrid\nretrieval system that utilizes audio metadata as an additional clue to\nunderstand the content of audio signals before matching them with textual\nqueries. We experimented with metadata often attached to audio recordings, such\nas keywords and natural-language descriptions, and we investigated late and\nmid-level fusion strategies to merge audio and metadata. Our hybrid approach\nwith keyword metadata and late fusion improved the retrieval performance over a\ncontent-based baseline by 2.36 and 3.69 pp. mAP@10 on the ClothoV2 and\nAudioCaps benchmarks, respectively.", "published": "2024-06-22 17:19:51", "link": "http://arxiv.org/abs/2406.15897v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
