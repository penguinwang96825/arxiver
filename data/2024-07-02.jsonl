{"title": "Indian Stock Market Prediction using Augmented Financial Intelligence ML", "abstract": "This paper presents price prediction models using Machine Learning algorithms\naugmented with Superforecasters predictions, aimed at enhancing investment\ndecisions. Five Machine Learning models are built, including Bidirectional\nLSTM, ARIMA, a combination of CNN and LSTM, GRU, and a model built using LSTM\nand GRU algorithms. The models are evaluated using the Mean Absolute Error to\ndetermine their predictive accuracy. Additionally, the paper suggests\nincorporating human intelligence by identifying Superforecasters and tracking\ntheir predictions to anticipate unpredictable shifts or changes in stock prices\n. The predictions made by these users can further enhance the accuracy of stock\nprice predictions when combined with Machine Learning and Natural Language\nProcessing techniques. Predicting the price of any commodity can be a\nsignificant task but predicting the price of a stock in the stock market deals\nwith much more uncertainty. Recognising the limited knowledge and exposure to\nstocks among certain investors, this paper proposes price prediction models\nusing Machine Learning algorithms. In this work, five Machine learning models\nare built using Bidirectional LSTM, ARIMA, a combination of CNN and LSTM, GRU\nand the last one is built using LSTM and GRU algorithms. Later these models are\nassessed using MAE scores to find which model is predicting with the highest\naccuracy. In addition to this, this paper also suggests the use of human\nintelligence to closely predict the shift in price patterns in the stock market\nThe main goal is to identify Superforecasters and track their predictions to\nanticipate unpredictable shifts or changes in stock prices. By leveraging the\ncombined power of Machine Learning and the Human Intelligence, predictive\naccuracy can be significantly increased.", "published": "2024-07-02 12:58:50", "link": "http://arxiv.org/abs/2407.02236v1", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "VSP: Assessing the dual challenges of perception and reasoning in\n  spatial planning tasks for VLMs", "abstract": "Vision language models (VLMs) are an exciting emerging class of language\nmodels (LMs) that have merged classic LM capabilities with those of image\nprocessing systems. However, the ways that these capabilities combine are not\nalways intuitive and warrant direct investigation. One understudied capability\nin VLMs is visual spatial planning -- the ability to comprehend the spatial\narrangements of objects and devise action plans to achieve desired outcomes in\nvisual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates\nthe spatial planning capability in these models in general, and 2) breaks down\nthe visual planning task into finer-grained sub-tasks, including perception and\nreasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation\nshows that both open-source and private VLMs fail to generate effective plans\nfor even simple spatial planning tasks. Evaluations on the fine-grained\nanalytical tasks further reveal fundamental deficiencies in the models' visual\nperception and bottlenecks in reasoning abilities, explaining their worse\nperformance in the general spatial planning tasks. Our work illuminates future\ndirections for improving VLMs' abilities in spatial planning. Our benchmark is\npublicly available at\nhttps://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.", "published": "2024-07-02 00:24:01", "link": "http://arxiv.org/abs/2407.01863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability", "abstract": "Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters.", "published": "2024-07-02 01:37:56", "link": "http://arxiv.org/abs/2407.01878v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proposal Report for the 2nd SciCAP Competition 2024", "abstract": "In this paper, we propose a method for document summarization using auxiliary\ninformation. This approach effectively summarizes descriptions related to\nspecific images, tables, and appendices within lengthy texts. Our experiments\ndemonstrate that leveraging high-quality OCR data and initially extracted\ninformation from the original text enables efficient summarization of the\ncontent related to described objects. Based on these findings, we enhanced\npopular text generation model models by incorporating additional auxiliary\nbranches to improve summarization performance. Our method achieved top scores\nof 4.33 and 4.66 in the long caption and short caption tracks, respectively, of\nthe 2024 SciCAP competition, ranking highest in both categories.", "published": "2024-07-02 02:42:29", "link": "http://arxiv.org/abs/2407.01897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scope-enhanced Compositional Semantic Parsing for DRT", "abstract": "Discourse Representation Theory (DRT) distinguishes itself from other\nsemantic representation frameworks by its ability to model complex semantic and\ndiscourse phenomena through structural nesting and variable binding. While\nseq2seq models hold the state of the art on DRT parsing, their accuracy\ndegrades with the complexity of the sentence, and they sometimes struggle to\nproduce well-formed DRT representations. We introduce the AMS parser, a\ncompositional, neurosymbolic semantic parser for DRT. It rests on a novel\nmechanism for predicting quantifier scope. We show that the AMS parser reliably\nproduces well-formed outputs and performs well on DRT parsing, especially on\ncomplex sentences.", "published": "2024-07-02 02:50:15", "link": "http://arxiv.org/abs/2407.01899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient-Empathy: Towards Efficient and Effective Selection of Empathy\n  Data", "abstract": "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capability has become a crucial\nprerequisite. Consequently, managing and understanding large-scale video\ndatasets has gained increasing importance. However, empathetic data are\ntypically trained without any quality selection, leading to inefficient data\nusage and wasted computational resources. Additionally, using raw data can\nresult in low performance in empathetic dialogues. In this work, we present\nEfficient-Empathy, a sensibility and rationality score-based data selection\nalgorithm that automatically selects sensibility and rationality data while\ndiscarding low-quality data. With only the sensibility data (59% of the full\ndataset), our trained sensibility model efficiently achieves state-of-the-art\n(SoTA) performance. Furthermore, with multiple data selection hyperparameters,\nthe sensibility model demonstrates SoTA performance, showcasing the robustness\nof our method. By integrating sensibility and rationality data with a MoE\nstructure, we achieve even higher performance, demonstrating the effectiveness\nof our Efficient-Empathy algorithm.", "published": "2024-07-02 04:11:52", "link": "http://arxiv.org/abs/2407.01937v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested\n  Large Language Models", "abstract": "Deployment of autoregressive large language models (LLMs) is costly, and as\nthese models increase in size, the associated costs will become even more\nconsiderable. Consequently, different methods have been proposed to accelerate\nthe token generation process and reduce costs. Speculative decoding (SD) is\namong the most promising approaches to speed up the LLM decoding process by\nverifying multiple tokens in parallel and using an auxiliary smaller draft\nmodel to generate the possible tokens. In SD, usually, one draft model is used\nto serve a specific target model; however, in practice, LLMs are diverse, and\nwe might need to deal with many target models or more than one target model\nsimultaneously. In this scenario, it is not clear which draft model should be\nused for which target model, and searching among different draft models or\ntraining customized draft models can further increase deployment costs. In this\npaper, we first introduce a novel multi-target scenario for the deployment of\ndraft models for faster inference. Then, we present a novel, more efficient\nsorted speculative decoding mechanism that outperforms regular baselines in\nmulti-target settings. We evaluated our method on Spec-Bench in different\nsettings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our\nresults suggest that our draft models perform better than baselines for\nmultiple target models at the same time.", "published": "2024-07-02 05:14:15", "link": "http://arxiv.org/abs/2407.01955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "abstract": "Legal judgment prediction is essential for enhancing judicial efficiency. In\nthis work, we identify that existing large language models (LLMs) underperform\nin this domain due to challenges in understanding case complexities and\ndistinguishing between similar charges. To adapt LLMs for effective legal\njudgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT)\nreasoning framework inspired by human judicial reasoning. ADAPT involves\ndecomposing case facts, discriminating among potential charges, and predicting\nthe final judgment. We further enhance LLMs through fine-tuning with multi-task\nsynthetic trajectories to improve legal judgment prediction accuracy and\nefficiency under our ADAPT framework. Extensive experiments conducted on two\nwidely-used datasets demonstrate the superior performance of our framework in\nlegal judgment prediction, particularly when dealing with complex and confusing\ncharges.", "published": "2024-07-02 05:43:15", "link": "http://arxiv.org/abs/2407.01964v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?", "abstract": "Recent work shows that large language models (LLMs) can answer\nmultiple-choice questions using only the choices, but does this mean that MCQA\nleaderboard rankings of LLMs are largely influenced by abilities in\nchoices-only settings? To answer this, we use a contrast set that probes if\nLLMs over-rely on choices-only shortcuts in MCQA. While previous works build\ncontrast sets via expensive human annotations or model-generated data which can\nbe biased, we employ graph mining to extract contrast sets from existing MCQA\ndatasets. We use our method on UnifiedQA, a group of six commonsense reasoning\ndatasets with high choices-only accuracy, to build an 820-question contrast\nset. After validating our contrast set, we test 12 LLMs, finding that these\nmodels do not exhibit reliance on choice-only shortcuts when given both the\nquestion and choices. Thus, despite the susceptibility~of MCQA to high\nchoices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA\nleaderboards just due to their ability to exploit choices-only shortcuts.", "published": "2024-07-02 07:06:53", "link": "http://arxiv.org/abs/2407.01992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social\n  Biases in LLMs via Contact Hypothesis", "abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices\nin their training data and reinforcing societal stereotypes and inequalities.\nOur work explores the potential of the Contact Hypothesis, a concept from\nsocial psychology for debiasing LLMs. We simulate various forms of social\ncontact through LLM prompting to measure their influence on the model's biases,\nmirroring how intergroup interactions can reduce prejudices in social contexts.\nWe create a dataset of 108,000 prompts following a principled approach\nreplicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and\nNousHermes) across 13 social bias dimensions. We propose a unique debiasing\ntechnique, Social Contact Debiasing (SCD), that instruction-tunes these models\nwith unbiased responses to prompts. Our research demonstrates that LLM\nresponses exhibit social biases when subject to contact probing, but more\nimportantly, these biases can be significantly reduced by up to 40% in 1 epoch\nof instruction tuning LLaMA 2 following our SCD strategy. Our code and data are\navailable at https://github.com/chahatraj/breakingbias.", "published": "2024-07-02 07:58:46", "link": "http://arxiv.org/abs/2407.02030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Stability Scoring for Text Annotation with Large Language Models", "abstract": "Researchers are increasingly using language models (LMs) for text annotation.\nThese approaches rely only on a prompt telling the model to return a given\noutput according to a set of instructions. The reproducibility of LM outputs\nmay nonetheless be vulnerable to small changes in the prompt design. This calls\ninto question the replicability of classification routines. To tackle this\nproblem, researchers have typically tested a variety of semantically similar\nprompts to determine what we call ``prompt stability.\" These approaches remain\nad-hoc and task specific. In this article, we propose a general framework for\ndiagnosing prompt stability by adapting traditional approaches to intra- and\ninter-coder reliability scoring. We call the resulting metric the Prompt\nStability Score (PSS) and provide a Python package \\texttt{promptstability} for\nits estimation. Using six different datasets and twelve outcomes, we classify\n$\\sim$3.1m rows of data and $\\sim$300m input tokens to: a) diagnose when prompt\nstability is low; and b) demonstrate the functionality of the package. We\nconclude by providing best practice recommendations for applied researchers.", "published": "2024-07-02 08:11:18", "link": "http://arxiv.org/abs/2407.02039v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concise and Precise Context Compression for Tool-Using Language Models", "abstract": "Through reading the documentation in the context, tool-using language models\ncan dynamically extend their capability using external tools. The cost is that\nwe have to input lengthy documentation every time the model needs to use the\ntool, occupying the input window as well as slowing down the decoding process.\n  Given the progress in general-purpose compression, soft context compression\nis a suitable approach to alleviate the problem. However, when compressing tool\ndocumentation, existing methods suffer from the weaknesses of key information\nloss (specifically, tool/parameter name errors) and difficulty in adjusting the\nlength of compressed sequences based on documentation lengths.\n  To address these problems, we propose two strategies for compressing tool\ndocumentation into concise and precise summary sequences for tool-using\nlanguage models. 1) Selective compression strategy mitigates key information\nloss by deliberately retaining key information as raw text tokens. 2) Block\ncompression strategy involves dividing tool documentation into short chunks and\nthen employing a fixed-length compression model to achieve variable-length\ncompression. This strategy facilitates the flexible adjustment of the\ncompression ratio.\n  Results on API-Bank and APIBench show that our approach reaches a performance\ncomparable to the upper-bound baseline under up to 16x compression ratio.", "published": "2024-07-02 08:17:00", "link": "http://arxiv.org/abs/2407.02043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "abstract": "Existing works examining Vision-Language Models (VLMs) for social biases\npredominantly focus on a limited set of documented bias associations, such as\ngender:profession or race:crime. This narrow scope often overlooks a vast range\nof unexamined implicit associations, restricting the identification and, hence,\nmitigation of such biases. We address this gap by probing VLMs to (1) uncover\nhidden, implicit associations across 9 bias dimensions. We systematically\nexplore diverse input and output modalities and (2) demonstrate how biased\nassociations vary in their negativity, toxicity, and extremity. Our work (3)\nidentifies subtle and extreme biases that are typically not recognized by\nexisting methodologies. We make the Dataset of retrieved associations, (Dora),\npublicly available here https://github.com/chahatraj/BiasDora.", "published": "2024-07-02 08:55:40", "link": "http://arxiv.org/abs/2407.02066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crossroads of Continents: Automated Artifact Extraction for Cultural\n  Adaptation with Large Multimodal Models", "abstract": "We present a comprehensive three-phase study to examine (1) the cultural\nunderstanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a\nlarge-scale dataset generated by DALL-E 3 and validated by humans, containing\n9,935 images of 67 countries and 10 concept classes; (2) the underlying\nimplicit and potentially stereotypical cultural associations with a cultural\nartifact extraction task; and (3) an approach to adapt cultural representation\nin an image based on extracted associations using a modular pipeline,\nCultureAdapt. We find disparities in cultural understanding at geographic\nsub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)\nmodels on DalleStreet and other existing benchmarks, which we try to understand\nusing over 18,000 artifacts that we identify in association to different\ncountries. Our findings reveal a nuanced picture of the cultural competence of\nLMMs, highlighting the need to develop culture-aware systems. Dataset and code\nare available at https://github.com/iamshnoo/crossroads", "published": "2024-07-02 08:55:41", "link": "http://arxiv.org/abs/2407.02067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Helpful assistant or fruitful facilitator? Investigating how personas\n  affect language model behavior", "abstract": "One way to personalize and steer generations from large language models (LLM)\nis to assign a persona: a role that describes how the user expects the LLM to\nbehave (e.g., a helpful assistant, a teacher, a woman). This paper investigates\nhow personas affect diverse aspects of model behavior. We assign to seven LLMs\n162 personas from 12 categories spanning variables like gender, sexual\norientation, and occupation. We prompt them to answer questions from five\ndatasets covering objective (e.g., questions about math and history) and\nsubjective tasks (e.g., questions about beliefs and values). We also compare\npersona's generations to two baseline settings: a control persona setting with\n30 paraphrases of \"a helpful assistant\" to control for models' prompt\nsensitivity, and an empty persona setting where no persona is assigned. We find\nthat for all models and datasets, personas show greater variability than the\ncontrol setting and that some measures of persona behavior generalize across\nmodels.", "published": "2024-07-02 09:36:54", "link": "http://arxiv.org/abs/2407.02099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at\n  Scale", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.", "published": "2024-07-02 10:06:41", "link": "http://arxiv.org/abs/2407.02118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News Detection: It's All in the Data!", "abstract": "This comprehensive survey serves as an indispensable resource for researchers\nembarking on the journey of fake news detection. By highlighting the pivotal\nrole of dataset quality and diversity, it underscores the significance of these\nelements in the effectiveness and robustness of detection models. The survey\nmeticulously outlines the key features of datasets, various labeling systems\nemployed, and prevalent biases that can impact model performance. Additionally,\nit addresses critical ethical issues and best practices, offering a thorough\noverview of the current state of available datasets. Our contribution to this\nfield is further enriched by the provision of GitHub repository, which\nconsolidates publicly accessible datasets into a single, user-friendly portal.\nThis repository is designed to facilitate and stimulate further research and\ndevelopment efforts aimed at combating the pervasive issue of fake news.", "published": "2024-07-02 10:12:06", "link": "http://arxiv.org/abs/2407.02122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Black Big Boxes: Do Language Models Hide a Theory of Adjective Order?", "abstract": "In English and other languages, multiple adjectives in a complex noun phrase\nshow intricate ordering patterns that have been a target of much linguistic\ntheory. These patterns offer an opportunity to assess the ability of language\nmodels (LMs) to learn subtle rules of language involving factors that cross the\ntraditional divisions of syntax, semantics, and pragmatics. We review existing\nhypotheses designed to explain Adjective Order Preferences (AOPs) in humans and\ndevelop a setup to study AOPs in LMs: we present a reusable corpus of adjective\npairs and define AOP measures for LMs. With these tools, we study a series of\nLMs across intermediate checkpoints during training. We find that all models'\npredictions are much closer to human AOPs than predictions generated by factors\nidentified in theoretical linguistics. At the same time, we demonstrate that\nthe observed AOPs in LMs are strongly correlated with the frequency of the\nadjective pairs in the training data and report limited generalization to\nunseen combinations. This highlights the difficulty in establishing the link\nbetween LM performance and linguistic theory. We therefore conclude with a road\nmap for future studies our results set the stage for, and a discussion of key\nquestions about the nature of knowledge in LMs and their ability to generalize\nbeyond the training sets.", "published": "2024-07-02 10:29:09", "link": "http://arxiv.org/abs/2407.02136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Holistic Framework for Multimodal Large Language Models in\n  Three-dimensional Brain CT Report Generation", "abstract": "Multi-modal large language models (MLLMs) have been given free rein to\nexplore exciting medical applications with a primary focus on radiology report\ngeneration. Nevertheless, the preliminary success in 2D radiology captioning is\nincompetent to reflect the real-world diagnostic challenge in the volumetric 3D\nanatomy. To mitigate three crucial limitation aspects in the existing\nliterature, including (1) data complexity, (2) model capacity, and (3)\nevaluation metric fidelity, we collected an 18,885 text-scan pairs 3D-BrainCT\ndataset and applied clinical visual instruction tuning (CVIT) to train BrainGPT\nmodels to generate radiology-adherent 3D brain CT reports. Statistically, our\nBrainGPT scored BLEU-1 = 44.35, BLEU-4 = 20.38, METEOR = 30.13, ROUGE-L = 47.6,\nand CIDEr-R = 211.77 during internal testing and demonstrated an accuracy of\n0.91 in captioning midline shifts on the external validation CQ500 dataset. By\nfurther inspecting the captioned report, we reported that the traditional\nmetrics appeared to measure only the surface text similarity and failed to\ngauge the information density of the diagnostic purpose. To close this gap, we\nproposed a novel Feature-Oriented Radiology Task Evaluation (FORTE) to estimate\nthe report's clinical relevance (lesion feature and landmarks). Notably, the\nBrainGPT model scored an average FORTE F1-score of 0.71 (degree=0.661;\nlandmark=0.706; feature=0.693; impression=0.779). To demonstrate that BrainGPT\nmodels possess objective readiness to generate human-like radiology reports, we\nconducted a Turing test that enrolled 11 physician evaluators, and around 74%\nof the BrainGPT-generated captions were indistinguishable from those written by\nhumans. Our work embodies a holistic framework that showcased the first-hand\nexperience of curating a 3D brain CT dataset, fine-tuning anatomy-sensible\nlanguage models, and proposing robust radiology evaluation metrics.", "published": "2024-07-02 12:58:35", "link": "http://arxiv.org/abs/2407.02235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Alignment in Multilingual Trolley Problems", "abstract": "We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called MultiTP. This dataset enables the assessment of LLMs'\ndecision-making processes in diverse linguistic contexts. Our analysis explores\nthe alignment of 19 different LLMs with human judgments, capturing preferences\nacross six moral dimensions: species, gender, fitness, status, age, and the\nnumber of lives involved. By correlating these preferences with the demographic\ndistribution of language speakers and examining the consistency of LLM\nresponses to various prompt paraphrasings, our findings provide insights into\ncross-lingual and ethical biases of LLMs and their intersection. We discover\nsignificant variance in alignment across languages, challenging the assumption\nof uniform moral reasoning in AI systems and highlighting the importance of\nincorporating diverse perspectives in AI ethics. The results underscore the\nneed for further research on the integration of multilingual dimensions in\nresponsible AI research to ensure fair and equitable AI interactions worldwide.\nOur code and data are at https://github.com/causalNLP/moralmachine", "published": "2024-07-02 14:02:53", "link": "http://arxiv.org/abs/2407.02273v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Renard: A Modular Pipeline for Extracting Character Networks from\n  Narrative Texts", "abstract": "Renard (Relationships Extraction from NARrative Documents) is a Python\nlibrary that allows users to define custom natural language processing (NLP)\npipelines to extract character networks from narrative texts. Contrary to the\nfew existing tools, Renard can extract dynamic networks, as well as the more\ncommon static networks. Renard pipelines are modular: users can choose the\nimplementation of each NLP subtask needed to extract a character network. This\nallows users to specialize pipelines to particular types of texts and to study\nthe impact of each subtask on the extracted network.", "published": "2024-07-02 14:14:59", "link": "http://arxiv.org/abs/2407.02284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFinBench: A Comprehensive Chinese Financial Benchmark for Large\n  Language Models", "abstract": "Large language models (LLMs) have achieved remarkable performance on various\nNLP tasks, yet their potential in more challenging and domain-specific task,\nsuch as finance, has not been fully explored. In this paper, we present\nCFinBench: a meticulously crafted, the most comprehensive evaluation benchmark\nto date, for assessing the financial knowledge of LLMs under Chinese context.\nIn practice, to better align with the career trajectory of Chinese financial\npractitioners, we build a systematic evaluation from 4 first-level categories:\n(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge\nof financial subjects, such as economics, statistics and auditing. (2)\nFinancial Qualification: whether LLMs can obtain the needed financial qualified\ncertifications, such as certified public accountant, securities qualification\nand banking qualification. (3) Financial Practice: whether LLMs can fulfill the\npractical financial jobs, such as tax consultant, junior accountant and\nsecurities analyst. (4) Financial Law: whether LLMs can meet the requirement of\nfinancial laws and regulations, such as tax law, insurance law and economic\nlaw. CFinBench comprises 99,100 questions spanning 43 second-level categories\nwith 3 question types: single-choice, multiple-choice and judgment. We conduct\nextensive experiments of 50 representative LLMs with various model size on\nCFinBench. The results show that GPT4 and some Chinese-oriented models lead the\nbenchmark, with the highest average accuracy being 60.16%, highlighting the\nchallenge presented by CFinBench. The dataset and evaluation code are available\nat https://cfinbench.github.io/.", "published": "2024-07-02 14:34:36", "link": "http://arxiv.org/abs/2407.02301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Human Understanding of Paraphrase Types in Large Language Models", "abstract": "Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 800 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT and a\nDPO-trained LLama 7B model can generate simple APTs, such as additions and\ndeletions, but struggle with complex structures (e.g., subordination changes).\nThis study contributes to understanding which aspects of paraphrasing language\nmodels have already succeeded at understanding and what remains elusive. In\naddition, we show how our curated datasets can be used to develop language\nmodels with specific linguistic capabilities.", "published": "2024-07-02 14:35:10", "link": "http://arxiv.org/abs/2407.02302v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining\n  Tasks", "abstract": "The process mining community has recently recognized the potential of large\nlanguage models (LLMs) for tackling various process mining tasks. Initial\nstudies report the capability of LLMs to support process analysis and even, to\nsome extent, that they are able to reason about how processes work. This latter\nproperty suggests that LLMs could also be used to tackle process mining tasks\nthat benefit from an understanding of process behavior. Examples of such tasks\ninclude (semantic) anomaly detection and next activity prediction, which both\ninvolve considerations of the meaning of activities and their inter-relations.\nIn this paper, we investigate the capabilities of LLMs to tackle such\nsemantics-aware process mining tasks. Furthermore, whereas most works on the\nintersection of LLMs and process mining only focus on testing these models out\nof the box, we provide a more principled investigation of the utility of LLMs\nfor process mining, including their ability to obtain process mining knowledge\npost-hoc by means of in-context learning and supervised fine-tuning.\nConcretely, we define three process mining tasks that benefit from an\nunderstanding of process semantics and provide extensive benchmarking datasets\nfor each of them. Our evaluation experiments reveal that (1) LLMs fail to solve\nchallenging process mining tasks out of the box and when provided only a\nhandful of in-context examples, (2) but they yield strong performance when\nfine-tuned for these tasks, consistently surpassing smaller, encoder-based\nlanguage models.", "published": "2024-07-02 14:44:49", "link": "http://arxiv.org/abs/2407.02310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Soft Language Prompts for Language Transfer", "abstract": "Cross-lingual knowledge transfer, especially between high- and low-resource\nlanguages, remains challenging in natural language processing (NLP). This study\noffers insights for improving cross-lingual NLP applications through the\ncombination of parameter-efficient fine-tuning methods. We systematically\nexplore strategies for enhancing cross-lingual transfer through the\nincorporation of language-specific and task-specific adapters and soft prompts.\nWe present a detailed investigation of various combinations of these methods,\nexploring their efficiency across 16 languages, focusing on 10 mid- and\nlow-resource languages. We further present to our knowledge the first use of\nsoft prompts for language transfer, a technique we call soft language prompts.\nOur findings demonstrate that in contrast to claims of previous work, a\ncombination of language and task adapters does not always work best; instead,\ncombining a soft language prompt with a task adapter outperforms most\nconfigurations in many cases.", "published": "2024-07-02 14:50:03", "link": "http://arxiv.org/abs/2407.02317v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Sparse Attention needs Adaptive Token Release", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide array of text-centric tasks. However, their `large'\nscale introduces significant computational and storage challenges, particularly\nin managing the key-value states of the transformer, which limits their wider\napplicability. Therefore, we propose to adaptively release resources from\ncaches and rebuild the necessary key-value states. Particularly, we accomplish\nthis by a lightweight controller module to approximate an ideal top-$K$ sparse\nattention. This module retains the tokens with the highest top-$K$ attention\nweights and simultaneously rebuilds the discarded but necessary tokens, which\nmay become essential for future decoding. Comprehensive experiments in natural\nlanguage generation and modeling reveal that our method is not only competitive\nwith full attention in terms of performance but also achieves a significant\nthroughput improvement of up to 221.8%. The code for replication is available\non the https://github.com/WHUIR/ADORE.", "published": "2024-07-02 14:58:44", "link": "http://arxiv.org/abs/2407.02328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open foundation models for Azerbaijani language", "abstract": "The emergence of multilingual large language models has enabled the\ndevelopment of language understanding and generation systems in Azerbaijani.\nHowever, most of the production-grade systems rely on cloud solutions, such as\nGPT-4. While there have been several attempts to develop open foundation models\nfor Azerbaijani, these works have not found their way into common use due to a\nlack of systemic benchmarking. This paper encompasses several lines of work\nthat promote open-source foundation models for Azerbaijani. We introduce (1) a\nlarge text corpus for Azerbaijani, (2) a family of encoder-only language models\ntrained on this dataset, (3) labeled datasets for evaluating these models, and\n(4) extensive evaluation that covers all major open-source models with\nAzerbaijani support.", "published": "2024-07-02 15:05:47", "link": "http://arxiv.org/abs/2407.02337v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring\n  and Utilizing Latent Space", "abstract": "Personalized Dialogue Generation (PDG) aims to create coherent responses\naccording to roles or personas. Traditional PDG relies on external role data,\nwhich can be scarce and raise privacy concerns. Approaches address these issues\nby extracting role information from dialogue history, which often fail to\ngenerically model roles in continuous space. To overcome these limitations, we\nintroduce a novel framework \\textbf{MO}dels \\textbf{R}oles from\n\\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and\n\\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage\ntraining process. Specifically, we create a persona codebook to represent roles\nin latent space compactly, and this codebook is used to construct a posterior\ndistribution of role information. This method enables the model to generalize\nacross roles, allowing the generation of personalized dialogues even for unseen\nroles. Experiments on both Chinese and English datasets demonstrate that\nMORPHEUS enhances the extraction of role information, and improves response\ngeneration without external role data. Additionally, MORPHEUS can be considered\nan efficient fine-tuning for large language models.", "published": "2024-07-02 15:12:34", "link": "http://arxiv.org/abs/2407.02345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Large Language Models in Automated Fact-Checking: A Survey", "abstract": "The dissemination of false information on online platforms presents a serious\nsocietal challenge. While manual fact-checking remains crucial, Large Language\nModels (LLMs) offer promising opportunities to support fact-checkers with their\nvast knowledge and advanced reasoning capabilities. This survey explores the\napplication of generative LLMs in fact-checking, highlighting various\napproaches and techniques for prompting or fine-tuning these models. By\nproviding an overview of existing methods and their limitations, the survey\naims to enhance the understanding of how LLMs can be used in fact-checking and\nto facilitate further progress in their integration into the fact-checking\nprocess.", "published": "2024-07-02 15:16:46", "link": "http://arxiv.org/abs/2407.02351v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition\n  and Program of Thought Verification", "abstract": "Large Visual Language Models (LVLMs) struggle with hallucinations in visual\ninstruction following task(s), limiting their trustworthiness and real-world\napplicability. We propose Pelican -- a novel framework designed to detect and\nmitigate hallucinations through claim verification. Pelican first decomposes\nthe visual claim into a chain of sub-claims based on first-order predicates.\nThese sub-claims consist of (predicate, question) pairs and can be\nconceptualized as nodes of a computational graph. We then use\nProgram-of-Thought prompting to generate Python code for answering these\nquestions through flexible composition of external tools. Pelican improves over\nprior work by introducing (1) intermediate variables for precise grounding of\nobject instances, and (2) shared computation for answering the sub-question to\nenable adaptive corrections and inconsistency identification. We finally use\nreasoning abilities of LLMs to verify the correctness of the claim by\nconsidering the consistency and confidence of the (question, answer) pairs from\neach sub-claim. Our experiments reveal a drop in hallucination rate by ~ 8% -\n32% across various baseline LVLMs and a 27% drop compared to approaches\nproposed for hallucination mitigation on MMHal-Bench. Results on two other\nbenchmarks further corroborate our results.", "published": "2024-07-02 15:17:44", "link": "http://arxiv.org/abs/2407.02352v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Refine with Fine-Grained Natural Language Feedback", "abstract": "Recent work has explored the capability of large language models (LLMs) to\nidentify and correct errors in LLM-generated responses. These refinement\napproaches frequently evaluate what sizes of models are able to do refinement\nfor what problems, but less attention is paid to what effective feedback for\nrefinement looks like. In this work, we propose looking at refinement with\nfeedback as a composition of three distinct LLM competencies: (1) detection of\nbad generations; (2) fine-grained natural language critique generation; (3)\nrefining with fine-grained feedback. The first step can be implemented with a\nhigh-performing discriminative model and steps 2 and 3 can be implemented\neither via prompted or fine-tuned LLMs. A key property of the proposed Detect,\nCritique, Refine (\"DCR\") method is that the step 2 critique model can give\nfine-grained feedback about errors, made possible by offloading the\ndiscrimination to a separate model in step 1. We show that models of different\ncapabilities benefit from refining with DCR on the task of improving factual\nconsistency of document grounded summaries. Overall, DCR consistently\noutperforms existing end-to-end refinement approaches and current trained\nmodels not fine-tuned for factuality critiquing.", "published": "2024-07-02 16:15:01", "link": "http://arxiv.org/abs/2407.02397v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ValueScope: Unveiling Implicit Norms and Values via Return Potential\n  Model of Social Interactions", "abstract": "This study introduces ValueScope, a framework leveraging language models to\nquantify social norms and values within online communities, grounded in social\nscience perspectives on normative structures. We employ ValueScope to dissect\nand analyze linguistic and stylistic expressions across 13 Reddit communities\ncategorized under gender, politics, science, and finance. Our analysis provides\na quantitative foundation showing that even closely related communities exhibit\nremarkably diverse norms. This diversity supports existing theories and adds a\nnew dimension--community preference--to understanding community interactions.\nValueScope not only delineates differing social norms among communities but\nalso effectively traces their evolution and the influence of significant\nexternal events like the U.S. presidential elections and the emergence of new\nsub-communities. The framework thus highlights the pivotal role of social norms\nin shaping online interactions, presenting a substantial advance in both the\ntheory and application of social norm studies in digital spaces.", "published": "2024-07-02 17:51:27", "link": "http://arxiv.org/abs/2407.02472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey on Knowledge Distillation for Large Language Models: Methods,\n  Evaluation, and Application", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in\nvarious domains, attracting significant interest from both academia and\nindustry. Despite their impressive performance, the substantial size and\ncomputational demands of LLMs pose considerable challenges for practical\ndeployment, particularly in environments with limited resources. The endeavor\nto compress language models while maintaining their accuracy has become a focal\npoint of research. Among the various methods, knowledge distillation has\nemerged as an effective technique to enhance inference speed without greatly\ncompromising performance. This paper presents a thorough survey from three\naspects: method, evaluation, and application, exploring knowledge distillation\ntechniques tailored specifically for LLMs. Specifically, we divide the methods\ninto white-box KD and black-box KD to better illustrate their differences.\nFurthermore, we also explored the evaluation tasks and distillation effects\nbetween different distillation methods, and proposed directions for future\nresearch. Through in-depth understanding of the latest advancements and\npractical applications, this survey provides valuable resources for\nresearchers, paving the way for sustained progress in this field.", "published": "2024-07-02 02:14:42", "link": "http://arxiv.org/abs/2407.01885v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial\n  Reasoning", "abstract": "Spatial reasoning, an important faculty of human cognition with many\npractical applications, is one of the core commonsense skills that is not\npurely language-based and, for satisfying (as opposed to optimal) solutions,\nrequires some minimum degree of planning. Existing benchmarks of Commonsense\nSpatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)\ninterpret text-based spatial $\\textit{descriptions}$ rather than directly\nevaluate a plan produced by the LLM in response to a $\\textit{specific}$\nspatial reasoning problem. In this paper, we construct a large-scale benchmark\ncalled GRASP, which consists of 16,000 grid-based environments where the agent\nis tasked with an energy collection problem. These environments include 100\ngrid instances instantiated using each of the 160 different grid settings,\ninvolving five different energy distributions, two modes of agent starting\nposition, and two distinct obstacle configurations, as well as three kinds of\nagent constraints. Using GRASP, we compare classic baseline approaches, such as\nrandom walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,\nGPT-4o, and GPT-o1-mini. The experimental results indicate that even these\nadvanced LLMs struggle to consistently achieve satisfactory solutions.", "published": "2024-07-02 02:27:46", "link": "http://arxiv.org/abs/2407.01892v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In\n  Log Analysis", "abstract": "Log analysis is crucial for ensuring the orderly and stable operation of\ninformation systems, particularly in the field of Artificial Intelligence for\nIT Operations (AIOps). Large Language Models (LLMs) have demonstrated\nsignificant potential in natural language processing tasks. In the AIOps\ndomain, they excel in tasks such as anomaly detection, root cause analysis of\nfaults, operations and maintenance script generation, and alert information\nsummarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a\ncomprehensive benchmark suite designed to evaluate the capabilities of LLMs in\nvarious log analysis tasks for the first time. This benchmark covers tasks such\nas log parsing, log anomaly detection, log fault diagnosis, and log\nsummarization. LogEval evaluates each task using 4,000 publicly available log\ndata entries and employs 15 different prompts for each task to ensure a\nthorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance,\nfocusing on aspects such as self-consistency and few-shot contextual learning.\nWe also discuss findings related to model quantification, Chinese-English\nquestion-answering evaluation, and prompt engineering. These findings provide\ninsights into the strengths and weaknesses of LLMs in multilingual environments\nand the effectiveness of different prompt strategies. Various evaluation\nmethods are employed for different tasks to accurately measure the performance\nof LLMs in log analysis, ensuring a comprehensive assessment. The insights\ngained from LogEvals evaluation reveal the strengths and limitations of LLMs in\nlog analysis tasks, providing valuable guidance for researchers and\npractitioners.", "published": "2024-07-02 02:39:33", "link": "http://arxiv.org/abs/2407.01896v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and\n  the Ship of Language Models", "abstract": "The term Language Models (LMs) as a time-specific collection of models of\ninterest is constantly reinvented, with its referents updated much like the\n$\\textit{Ship of Theseus}$ replaces its parts but remains the same ship in\nessence. In this paper, we investigate this $\\textit{Ship of Language Models}$\nproblem, wherein scientific evolution takes the form of continuous, implicit\nretrofits of key existing terms. We seek to initiate a novel perspective of\nscientific progress, in addition to the more well-studied emergence of new\nterms. To this end, we construct the data infrastructure based on recent NLP\npublications. Then, we perform a series of text-based analyses toward a\ndetailed, quantitative understanding of the use of Language Models as a term of\nart. Our work highlights how systems and theories influence each other in\nscientific discourse, and we call for attention to the transformation of this\nShip that we all are contributing to.", "published": "2024-07-02 03:45:55", "link": "http://arxiv.org/abs/2407.01929v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaCQR: Enhancing Query Reformulation for Conversational Search via\n  Sparse and Dense Retrieval Alignment", "abstract": "Conversational Query Reformulation (CQR) has significantly advanced in\naddressing the challenges of conversational search, particularly those stemming\nfrom the latent user intent and the need for historical context. Recent works\naimed to boost the performance of CQR through alignment. However, they are\ndesigned for one specific retrieval system, which potentially results in\nsub-optimal generalization. To overcome this limitation, we present a novel\nframework AdaCQR. By aligning reformulation models with both term-based and\nsemantic-based retrieval systems, AdaCQR enhances the generalizability of\ninformation-seeking queries among diverse retrieval environments through a\ntwo-stage training strategy. Moreover, two effective approaches are proposed to\nobtain superior labels and diverse input candidates, boosting the efficiency\nand robustness of the framework. Experimental results on the TopiOCQA and QReCC\ndatasets demonstrate that AdaCQR outperforms the existing methods in a more\nefficient framework, offering both quantitative and qualitative improvements in\nconversational query reformulation.", "published": "2024-07-02 05:50:16", "link": "http://arxiv.org/abs/2407.01965v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fake News Detection and Manipulation Reasoning via Large Vision-Language\n  Models", "abstract": "Fake news becomes a growing threat to information security and public opinion\nwith the rapid sprawl of media manipulation. Therefore, fake news detection\nattracts widespread attention from academic community. Traditional fake news\ndetection models demonstrate remarkable performance on authenticity binary\nclassification but their ability to reason detailed faked traces based on the\nnews content remains under-explored. Furthermore, due to the lack of external\nknowledge, the performance of existing methods on fact-related news is\nquestionable, leaving their practical implementation unclear. In this paper, we\npropose a new multi-media research topic, namely manipulation reasoning.\nManipulation reasoning aims to reason manipulations based on news content. To\nsupport the research, we introduce a benchmark for fake news detection and\nmanipulation reasoning, referred to as Human-centric and Fact-related Fake News\n(HFFN). The benchmark highlights the centrality of human and the high factual\nrelevance, with detailed manual annotations. HFFN encompasses four realistic\ndomains with fake news samples generated through three manipulation approaches.\nMoreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is\npresented not only to judge on the authenticity of multi-modal news, but also\nraise analytical reasoning about potential manipulations. On the feature\nextraction level, a cross-attention mechanism is employed to extract\nfine-grained fusion features from multi-modal inputs. On the reasoning level, a\nlarge vision-language model (LVLM) serves as the backbone to facilitate\nfact-related reasoning. A two-stage training framework is deployed to better\nactivate the capacity of identification and reasoning. Comprehensive\nexperiments demonstrate that our model outperforms state-of-the-art (SOTA) fake\nnews detection models and powerful LVLMs like GPT-4 and LLaVA.", "published": "2024-07-02 08:16:43", "link": "http://arxiv.org/abs/2407.02042v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained\n  Self-Consistency for Free-Form Language Generation", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows\nsignificant gains on various reasoning tasks but struggles with free-form\ngeneration due to the difficulty of aggregating answers. Its variants, UCS and\nUSC, rely on sample selection or voting mechanisms to improve output quality.\nThese methods, however, face limitations due to their inability to fully\nutilize the nuanced consensus knowledge present within multiple candidate\nsamples, often resulting in suboptimal outputs. We propose Fine-Grained\nSelf-Consistency (FSC) to addresses these limitations by extracting and\nintegrating segment-level commonalities from candidate samples, enhancing the\nperformance of LLMs both in open-ended and reasoning tasks. Based on this, we\npresent two additional strategies: candidate filtering, which enhances overall\nquality by identifying highly similar candidate sets, and merging, which\nreduces input token requirements by combining similar samples. The\neffectiveness of FSC is demonstrated through extensive experiments on various\ntasks, including summarization, code generation, and mathematical reasoning,\nusing GPT-3.5-turbo and GPT-4. The results indicate significant improvements\nover baseline methods, showcasing the potential of FSC to optimize output\nquality by effectively synthesizing fine-grained consensus knowledge from\nmultiple samples.", "published": "2024-07-02 08:38:31", "link": "http://arxiv.org/abs/2407.02056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning", "abstract": "Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.", "published": "2024-07-02 10:43:49", "link": "http://arxiv.org/abs/2407.02147v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Adaptation Rule Optimization via Large Language Models", "abstract": "Rule-based adaptation is a foundational approach to self-adaptation,\ncharacterized by its human readability and rapid response. However, building\nhigh-performance and robust adaptation rules is often a challenge because it\nessentially involves searching the optimal design in a complex (variables)\nspace. In response, this paper attempt to employ large language models (LLMs)\nas a optimizer to construct and optimize adaptation rules, leveraging the\ncommon sense and reasoning capabilities inherent in LLMs. Preliminary\nexperiments conducted in SWIM have validated the effectiveness and limitation\nof our method.", "published": "2024-07-02 12:06:40", "link": "http://arxiv.org/abs/2407.02203v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise\n  in Machine Translation", "abstract": "The massive amounts of web-mined parallel data contain large amounts of\nnoise. Semantic misalignment, as the primary source of the noise, poses a\nchallenge for training machine translation systems. In this paper, we first\nintroduce a process for simulating misalignment controlled by semantic\nsimilarity, which closely resembles misaligned sentences in real-world\nweb-crawled corpora. Under our simulated misalignment noise settings, we\nquantitatively analyze its impact on machine translation and demonstrate the\nlimited effectiveness of widely used pre-filters for noise detection. This\nunderscores the necessity of more fine-grained ways to handle hard-to-detect\nmisalignment noise. With an observation of the increasing reliability of the\nmodel's self-knowledge for distinguishing misaligned and clean data at the\ntoken level, we propose self-correction, an approach that gradually increases\ntrust in the model's self-knowledge to correct the training supervision.\nComprehensive experiments show that our method significantly improves\ntranslation performance both in the presence of simulated misalignment noise\nand when applied to real-world, noisy web-mined datasets, across a range of\ntranslation tasks.", "published": "2024-07-02 12:15:15", "link": "http://arxiv.org/abs/2407.02208v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Monoculture in Large Language Models", "abstract": "We introduce {\\em generative monoculture}, a behavior observed in large\nlanguage models (LLMs) characterized by a significant narrowing of model output\ndiversity relative to available training data for a given task: for example,\ngenerating only positive book reviews for books with a mixed reception. While\nin some cases, generative monoculture enhances performance (e.g., LLMs more\noften produce efficient code), the dangers are exacerbated in others (e.g.,\nLLMs refuse to share diverse opinions). As LLMs are increasingly used in\nhigh-impact settings such as education and web search, careful maintenance of\nLLM output diversity is essential to ensure a variety of facts and perspectives\nare preserved over time. We experimentally demonstrate the prevalence of\ngenerative monoculture through analysis of book review and code generation\ntasks, and find that simple countermeasures such as altering sampling or\nprompting strategies are insufficient to mitigate the behavior. Moreover, our\nresults suggest that the root causes of generative monoculture are likely\nembedded within the LLM's alignment processes, suggesting a need for developing\nfine-tuning paradigms that preserve or promote diversity.", "published": "2024-07-02 12:17:07", "link": "http://arxiv.org/abs/2407.02209v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Transliteration in In-Context Learning for\n  Low-resource Languages Written in Non-Latin Scripts", "abstract": "Decoder-only large language models (LLMs) excel in high-resource languages\nacross various tasks through few-shot or even zero-shot in-context learning\n(ICL). However, their performance often does not transfer well to low-resource\nlanguages, especially those written in non-Latin scripts. Inspired by recent\nwork that leverages transliteration in encoder-only models, we investigate\nwhether transliteration is also effective in improving LLMs' performance for\nlow-resource languages written in non-Latin scripts. To this end, we propose\nthree prompt templates, where the target-language text is represented in (1)\nits original script, (2) Latin script, or (3) both. We apply these methods to\nseveral representative LLMs of different sizes on various tasks including text\nclassification and sequential labeling. Our findings show that the\neffectiveness of transliteration varies by task type and model size. For\ninstance, all models benefit from transliterations for sequential labeling\n(with increases of up to 25%).", "published": "2024-07-02 14:51:20", "link": "http://arxiv.org/abs/2407.02320v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why do LLaVA Vision-Language Models Reply to Images in English?", "abstract": "We uncover a surprising multilingual bias occurring in a popular class of\nmultimodal vision-language models (VLMs). Including an image in the query to a\nLLaVA-style VLM significantly increases the likelihood of the model returning\nan English response, regardless of the language of the query. This paper\ninvestigates the causes of this loss with a two-pronged approach that combines\nextensive ablation of the design space with a mechanistic analysis of the\nmodels' internal representations of image and text inputs. Both approaches\nindicate that the issue stems in the language modelling component of the LLaVA\nmodel. Statistically, we find that switching the language backbone for a\nbilingual language model has the strongest effect on reducing this error.\nMechanistically, we provide compelling evidence that visual inputs are not\nmapped to a similar space as text ones, and that intervening on intermediary\nattention layers can reduce this bias. Our findings provide important insights\nto researchers and engineers seeking to understand the crossover between\nmultimodal and multilingual spaces, and contribute to the goal of developing\ncapable and inclusive VLMs for non-English contexts.", "published": "2024-07-02 15:01:55", "link": "http://arxiv.org/abs/2407.02333v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RVISA: Reasoning and Verification for Implicit Sentiment Analysis", "abstract": "With an increasing social demand for fine-grained sentiment analysis (SA),\nimplicit sentiment analysis (ISA) poses a significant challenge with the\nabsence of salient cue words in expressions. It necessitates reliable reasoning\nto understand how the sentiment is aroused and thus determine implicit\nsentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)\nLLMs have gained popularity to serve as backbone models for SA applications,\nconsidering impressive text comprehension and reasoning ability among diverse\ntasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural\nlanguage generation and in-context learning capabilities. However, their\nresponses may contain misleading or inaccurate information. To identify\nimplicit sentiment with reliable reasoning, this study proposes RVISA, a\ntwo-stage reasoning framework that harnesses the generation ability of DO LLMs\nand the reasoning ability of ED LLMs to train an enhanced reasoner.\nSpecifically, we adopt three-hop reasoning prompting to explicitly furnish\nsentiment elements as cues. The generated rationales are utilized to fine-tune\nan ED LLM into a skilled reasoner. Additionally, we develop a straightforward\nyet effective verification mechanism to ensure the reliability of the reasoning\nlearning. We evaluated the proposed method on two benchmark datasets and\nachieved state-of-the-art results in ISA performance.", "published": "2024-07-02 15:07:54", "link": "http://arxiv.org/abs/2407.02340v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Your AI-Generated Code Really Safe? Evaluating Large Language Models\n  on Secure Code Generation with CodeSecEval", "abstract": "Large language models (LLMs) have brought significant advancements to code\ngeneration and code repair, benefiting both novice and experienced developers.\nHowever, their training using unsanitized data from open-source repositories,\nlike GitHub, raises the risk of inadvertently propagating security\nvulnerabilities. Despite numerous studies investigating the safety of code\nLLMs, there remains a gap in comprehensively addressing their security\nfeatures. In this work, we aim to present a comprehensive study aimed at\nprecisely evaluating and enhancing the security aspects of code LLMs. To\nsupport our research, we introduce CodeSecEval, a meticulously curated dataset\ndesigned to address 44 critical vulnerability types with 180 distinct samples.\nCodeSecEval serves as the foundation for the automatic evaluation of code\nmodels in two crucial tasks: code generation and code repair, with a strong\nemphasis on security. Our experimental results reveal that current models\nfrequently overlook security issues during both code generation and repair\nprocesses, resulting in the creation of vulnerable code. In response, we\npropose different strategies that leverage vulnerability-aware information and\ninsecure code explanations to mitigate these security vulnerabilities.\nFurthermore, our findings highlight that certain vulnerability types\nparticularly challenge model performance, influencing their effectiveness in\nreal-world applications. Based on these findings, we believe our study will\nhave a positive impact on the software engineering community, inspiring the\ndevelopment of improved methods for training and utilizing LLMs, thereby\nleading to safer and more trustworthy model deployment.", "published": "2024-07-02 16:13:21", "link": "http://arxiv.org/abs/2407.02395v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language\n  Models", "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various\nnatural language processing (NLP) tasks, concerns regarding the potential\nnegative societal impacts of LLM-generated content have also arisen. To\nevaluate the biases exhibited by LLMs, researchers have recently proposed a\nvariety of datasets. However, existing bias evaluation efforts often focus on\nonly a particular type of bias and employ inconsistent evaluation metrics,\nleading to difficulties in comparison across different datasets and LLMs. To\naddress these limitations, we collect a variety of datasets designed for the\nbias evaluation of LLMs, and further propose CEB, a Compositional Evaluation\nBenchmark that covers different types of bias across different social groups\nand tasks. The curation of CEB is based on our newly proposed compositional\ntaxonomy, which characterizes each dataset from three dimensions: bias types,\nsocial groups, and tasks. By combining the three dimensions, we develop a\ncomprehensive evaluation strategy for the bias in LLMs. Our experiments\ndemonstrate that the levels of bias vary across these dimensions, thereby\nproviding guidance for the development of specific bias mitigation methods.", "published": "2024-07-02 16:31:37", "link": "http://arxiv.org/abs/2407.02408v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Robustness of Adverse Drug Event Classification Models\n  Using Templates", "abstract": "An adverse drug effect (ADE) is any harmful event resulting from medical drug\ntreatment. Despite their importance, ADEs are often under-reported in official\nchannels. Some research has therefore turned to detecting discussions of ADEs\nin social media. Impressive results have been achieved in various attempts to\ndetect ADEs. In a high-stakes domain such as medicine, however, an in-depth\nevaluation of a model's abilities is crucial. We address the issue of thorough\nperformance evaluation in English-language ADE detection with hand-crafted\ntemplates for four capabilities: Temporal order, negation, sentiment, and\nbeneficial effect. We find that models with similar performance on held-out\ntest sets have varying results on these capabilities.", "published": "2024-07-02 17:09:24", "link": "http://arxiv.org/abs/2407.02432v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting vs. Acting: A Trade-off Between World Modeling & Agent\n  Modeling", "abstract": "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and\nlong-form text generation, yet they struggle with one foundational task:\nnext-token prediction. As RLHF models become agent models aimed at interacting\nwith humans, they seem to lose their world modeling -- the ability to predict\nwhat comes next in arbitrary documents, which is the foundational training\nobjective of the Base LMs that RLHF adapts.\n  Besides empirically demonstrating this trade-off, we propose a potential\nexplanation: to perform coherent long-form generation, RLHF models restrict\nrandomness via implicit blueprints. In particular, RLHF models concentrate\nprobability on sets of anchor spans that co-occur across multiple generations\nfor the same prompt, serving as textual scaffolding but also limiting a model's\nability to generate documents that do not include these spans. We study this\ntrade-off on the most effective current agent models, those aligned with RLHF,\nwhile exploring why this may remain a fundamental trade-off between models that\nact and those that predict, even as alignment techniques improve.", "published": "2024-07-02 17:22:54", "link": "http://arxiv.org/abs/2407.02446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ensemble of pre-trained language models and data augmentation for hate\n  speech detection from Arabic tweets", "abstract": "Today, hate speech classification from Arabic tweets has drawn the attention\nof several researchers. Many systems and techniques have been developed to\nresolve this classification task. Nevertheless, two of the major challenges\nfaced in this context are the limited performance and the problem of imbalanced\ndata. In this study, we propose a novel approach that leverages ensemble\nlearning and semi-supervised learning based on previously manually labeled. We\nconducted experiments on a benchmark dataset by classifying Arabic tweets into\n5 distinct classes: non-hate, general hate, racial, religious, or sexism.\nExperimental results show that: (1) ensemble learning based on pre-trained\nlanguage models outperforms existing related works; (2) Our proposed data\naugmentation improves the accuracy results of hate speech detection from Arabic\ntweets and outperforms existing related works. Our main contribution is the\nachievement of encouraging results in Arabic hate speech detection.", "published": "2024-07-02 17:26:26", "link": "http://arxiv.org/abs/2407.02448v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study", "abstract": "Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks.", "published": "2024-07-02 17:55:03", "link": "http://arxiv.org/abs/2407.02477v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "abstract": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit\nlimited generality and often fall short when compared to specialized models.\nRecently, LLM-based agents have been developed to address these challenges by\nselecting appropriate specialized models as tools based on user inputs.\nHowever, such advancements have not been extensively explored within the\nmedical domain. To bridge this gap, this paper introduces the first agent\nexplicitly designed for the medical field, named \\textbf{M}ulti-modal\n\\textbf{Med}ical \\textbf{Agent} (MMedAgent). We curate an instruction-tuning\ndataset comprising six medical tools solving seven tasks across five\nmodalities, enabling the agent to choose the most suitable tools for a given\ntask. Comprehensive experiments demonstrate that MMedAgent achieves superior\nperformance across a variety of medical tasks compared to state-of-the-art\nopen-source methods and even the closed-source model, GPT-4o. Furthermore,\nMMedAgent exhibits efficiency in updating and integrating new medical tools.\nCodes and models are all available.", "published": "2024-07-02 17:58:23", "link": "http://arxiv.org/abs/2407.02483v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention", "abstract": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.", "published": "2024-07-02 17:59:56", "link": "http://arxiv.org/abs/2407.02490v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Practical Review of Mechanistic Interpretability for Transformer-Based\n  Language Models", "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of\ninterpretability that seeks to understand a neural network model by\nreverse-engineering its internal computations. Recently, MI has garnered\nsignificant attention for interpreting transformer-based language models (LMs),\nresulting in many novel insights yet introducing new challenges. However, there\nhas not been work that comprehensively reviews these insights and challenges,\nparticularly as a guide for newcomers to this field. To fill this gap, we\nprovide a comprehensive survey from a task-centric perspective, organizing the\ntaxonomy of MI research around specific research questions or tasks. We outline\nthe fundamental objects of study in MI, along with the techniques, evaluation\nmethods, and key findings for each task in the taxonomy. In particular, we\npresent a task-centric taxonomy as a roadmap for beginners to navigate the\nfield by helping them quickly identify impactful problems in which they are\nmost interested and leverage MI for their benefit. Finally, we discuss the\ncurrent gaps in the field and suggest potential future directions for MI\nresearch.", "published": "2024-07-02 20:28:16", "link": "http://arxiv.org/abs/2407.02646v3", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison", "abstract": "In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.", "published": "2024-07-02 20:49:21", "link": "http://arxiv.org/abs/2407.02659v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning in Large Language Models: A Geometric Perspective", "abstract": "The advancement of large language models (LLMs) for real-world applications\nhinges critically on enhancing their reasoning capabilities. In this work, we\nexplore the reasoning abilities of large language models (LLMs) through their\ngeometrical understanding. We establish a connection between the expressive\npower of LLMs and the density of their self-attention graphs. Our analysis\ndemonstrates that the density of these graphs defines the intrinsic dimension\nof the inputs to the MLP blocks. We demonstrate through theoretical analysis\nand toy examples that a higher intrinsic dimension implies a greater expressive\ncapacity of the LLM. We further provide empirical evidence linking this\ngeometric framework to recent advancements in methods aimed at enhancing the\nreasoning capabilities of LLMs.", "published": "2024-07-02 21:39:53", "link": "http://arxiv.org/abs/2407.02678v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Lightweight Large Language Model for Medication Enquiry: Med-Pal", "abstract": "Large Language Models (LLMs) have emerged as a potential solution to assist\ndigital health development with patient education, commonly medication-related\nenquires. We trained and validated Med-Pal, a medication domain-specific\nLLM-chatbot fine-tuned with a fine-grained and expert curated dataset from a\nselection of five light-weighted open-source LLMs of smaller parameter size (7\nbillion or less) regarding computational constraints and prioritizing\noperational efficiency. A multi-disciplinary team performed a clinical\nevaluation of LLMs responses using the SCORE criteria, focusing on safety,\naccuracy, bias, reproducibility, and ease of understanding. Best performing\nlight-weighted LLM was chosen as Med-Pal for further engineering with\nguard-railing using adversarial prompting. Med-Pal and existing light-weighted\nLLMs, including pretrained Biomistral and finetuned Meerkat, were validated on\nan independent dataset on a broad range of medication-related questions (231 in\ntotal), 12 different question types across 14 different medication classes.\nMistral-7b emerged as the top performer among selected lightweight LLMs,\nachieving the highest median score of 14 and 71.9% high-quality responses in\naccuracy and safety domains, hence chosen as the backbone LLM for Med-Pal. When\ncompared against Biomistral, Med-pal outperformed in generating responses\nappropriate for patient communication, with significant reductions bias and\nerrors typical of general LLMs. Comparable performance was observed when\ncomparing Med-Pal with Meerkat. Med-Pal showcases the feasibility of developing\nand employing fine-tuned light-weighted LLMs to enhance digital health\ncommunications.", "published": "2024-07-02 03:32:39", "link": "http://arxiv.org/abs/2407.12822v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large\n  Language Models", "abstract": "Although Large Language Models (LLMs) excel in NLP tasks, they still need\nexternal tools to extend their ability. Current research on tool learning with\nLLMs often assumes mandatory tool use, which does not always align with\nreal-world situations, where the necessity for tools is uncertain, and\nincorrect or unnecessary use of tools can damage the general abilities of LLMs.\nTherefore, we propose to explore whether LLMs can discern their ability\nboundaries and use tools flexibly. We then introduce the Whether-or-not tool\nusage Evaluation benchmark (WTU-Eval) to assess LLMs with eleven datasets,\nwhere six of them are tool-usage datasets, and five are general datasets. LLMs\nare prompted to use tools according to their needs. The results of eight LLMs\non WTU-Eval reveal that LLMs frequently struggle to determine tool use in\ngeneral datasets, and LLMs' performance in tool-usage datasets improves when\ntheir ability is similar to ChatGPT. In both datasets, incorrect tool usage\nsignificantly impairs LLMs' performance. To mitigate this, we also develop the\nfinetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B\nresults in a 14\\% average performance improvement and a 16.8\\% decrease in\nincorrect tool usage. We will release the WTU-Eval benchmark.", "published": "2024-07-02 12:07:38", "link": "http://arxiv.org/abs/2407.12823v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in\n  Language Models", "abstract": "An important issue with Large Language Models (LLMs) is their undesired\nability to generate toxic language. In this work, we show that the neurons\nresponsible for toxicity can be determined by their power to discriminate toxic\nsentences, and that toxic language can be mitigated by reducing their\nactivation levels proportionally to this power. We propose AUROC adaptation\n(AurA), an intervention that can be applied to any pre-trained LLM to mitigate\ntoxicity. As the intervention is proportional to the ability of each neuron to\ndiscriminate toxic content, it is free of any model-dependent hyperparameters.\nWe show that AurA can achieve up to $2.2 \\times$ reduction in toxicity with\nonly a $0.72$ perplexity increase. We also show that AurA is effective with\nmodels of different scale (from 1.5B to 40B parameters), and its effectiveness\nin mitigating toxic language, while preserving common-sense zero-shot\nabilities, holds across all scales. AurA can be combined with pre-prompting\nstrategies, boosting its average mitigation potential from $1.28\\times$ to\n$2.35\\times$. Moreover, AurA can counteract adversarial pre-prompts that\nmaliciously elicit toxic content, making it an effective method for deploying\nsafer and less toxic models.", "published": "2024-07-02 12:48:29", "link": "http://arxiv.org/abs/2407.12824v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing the Effectiveness of GPT-4o in Climate Change Evidence\n  Synthesis and Systematic Assessments: Preliminary Insights", "abstract": "In this research short, we examine the potential of using GPT-4o, a\nstate-of-the-art large language model (LLM) to undertake evidence synthesis and\nsystematic assessment tasks. Traditional workflows for such tasks involve large\ngroups of domain experts who manually review and synthesize vast amounts of\nliterature. The exponential growth of scientific literature and recent advances\nin LLMs provide an opportunity to complementing these traditional workflows\nwith new age tools. We assess the efficacy of GPT-4o to do these tasks on a\nsample from the dataset created by the Global Adaptation Mapping Initiative\n(GAMI) where we check the accuracy of climate change adaptation related feature\nextraction from the scientific literature across three levels of expertise. Our\nresults indicate that while GPT-4o can achieve high accuracy in low-expertise\ntasks like geographic location identification, their performance in\nintermediate and high-expertise tasks, such as stakeholder identification and\nassessment of depth of the adaptation response, is less reliable. The findings\nmotivate the need for designing assessment workflows that utilize the strengths\nof models like GPT-4o while also providing refinements to improve their\nperformance on these tasks.", "published": "2024-07-02 13:14:57", "link": "http://arxiv.org/abs/2407.12826v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Solution for The PST-KDD-2024 OAG-Challenge", "abstract": "In this paper, we introduce the second-place solution in the KDD-2024\nOAG-Challenge paper source tracing track. Our solution is mainly based on two\nmethods, BERT and GCN, and combines the reasoning results of BERT and GCN in\nthe final submission to achieve complementary performance. In the BERT\nsolution, we focus on processing the fragments that appear in the references of\nthe paper, and use a variety of operations to reduce the redundant interference\nin the fragments, so that the information received by BERT is more refined. In\nthe GCN solution, we map information such as paper fragments, abstracts, and\ntitles to a high-dimensional semantic space through an embedding model, and try\nto build edges between titles, abstracts, and fragments to integrate contextual\nrelationships for judgment. In the end, our solution achieved a remarkable\nscore of 0.47691 in the competition.", "published": "2024-07-02 14:15:05", "link": "http://arxiv.org/abs/2407.12827v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "abstract": "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs.", "published": "2024-07-02 14:33:44", "link": "http://arxiv.org/abs/2407.12828v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predictive Simultaneous Interpretation: Harnessing Large Language Models\n  for Democratizing Real-Time Multilingual Communication", "abstract": "This study introduces a groundbreaking approach to simultaneous\ninterpretation by directly leveraging the predictive capabilities of Large\nLanguage Models (LLMs). We present a novel algorithm that generates real-time\ntranslations by predicting speaker utterances and expanding multiple\npossibilities in a tree-like structure. This method demonstrates unprecedented\nflexibility and adaptability, potentially overcoming the structural differences\nbetween languages more effectively than existing systems. Our theoretical\nanalysis, supported by illustrative examples, suggests that this approach could\nlead to more natural and fluent translations with minimal latency. The primary\npurpose of this paper is to share this innovative concept with the academic\ncommunity, stimulating further research and development in this field. We\ndiscuss the theoretical foundations, potential advantages, and implementation\nchallenges of this technique, positioning it as a significant step towards\ndemocratizing multilingual communication.", "published": "2024-07-02 13:18:28", "link": "http://arxiv.org/abs/2407.14269v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "Automated Text Scoring in the Age of Generative AI for the GPU-poor", "abstract": "Current research on generative language models (GLMs) for automated text\nscoring (ATS) has focused almost exclusively on querying proprietary models via\nApplication Programming Interfaces (APIs). Yet such practices raise issues\naround transparency and security, and these methods offer little in the way of\nefficiency or customizability. With the recent proliferation of smaller,\nopen-source models, there is the option to explore GLMs with computers equipped\nwith modest, consumer-grade hardware, that is, for the \"GPU poor.\" In this\nstudy, we analyze the performance and efficiency of open-source, small-scale\nGLMs for ATS. Results show that GLMs can be fine-tuned to achieve adequate,\nthough not state-of-the-art, performance. In addition to ATS, we take small\nsteps towards analyzing models' capacity for generating feedback by prompting\nGLMs to explain their scores. Model-generated feedback shows promise, but\nrequires more rigorous evaluation focused on targeted use cases.", "published": "2024-07-02 01:17:01", "link": "http://arxiv.org/abs/2407.01873v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents", "abstract": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.", "published": "2024-07-02 02:18:14", "link": "http://arxiv.org/abs/2407.01887v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters", "abstract": "The widespread applications of large language models (LLMs) have brought\nabout concerns regarding their potential misuse. Although aligned with human\npreference data before release, LLMs remain vulnerable to various malicious\nattacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety\nand introduce SeqAR, a simple yet effective framework to design jailbreak\nprompts automatically. The SeqAR framework generates and optimizes multiple\njailbreak characters and then applies sequential jailbreak characters in a\nsingle query to bypass the guardrails of the target LLM. Different from\nprevious work which relies on proprietary LLMs or seed jailbreak templates\ncrafted by human expertise, SeqAR can generate and optimize the jailbreak\nprompt in a cold-start scenario using open-sourced LLMs without any seed\njailbreak templates. Experimental results show that SeqAR achieves attack\nsuccess rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106\nand GPT-4, respectively. Furthermore, we extensively evaluate the\ntransferability of the generated templates across different LLMs and held-out\nmalicious requests, while also exploring defense strategies against the\njailbreak attack designed by SeqAR.", "published": "2024-07-02 02:58:29", "link": "http://arxiv.org/abs/2407.01902v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for\n  Sparse Architectural Large Language Models", "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large\nLanguage Models (LLMs) with constrained resources. Although there have been\nvarious PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture\nLLMs is still underexplored. In this work, we study the PEFT method for LLMs\nwith the Mixture-of-Experts (MoE) architecture and the contents of this work\nare mainly threefold: (1) We investigate the dispersion degree of the activated\nexperts in customized tasks, and found that the routing distribution for a\nspecific task tends to be highly concentrated, while the distribution of\nactivated experts varies significantly across different tasks. (2) We propose\nExpert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant\nto downstream tasks while freezing the other experts and modules; experimental\nresults demonstrate that our method not only improves the tuning efficiency,\nbut also matches or even surpasses the performance of full-parameter\nfine-tuning. (3) We further analyze the impact of the MoE architecture on\nexpert-specialized fine-tuning. We find that MoE models with finer-grained\nexperts are more advantageous in selecting the combination of experts that are\nmost relevant to downstream tasks, thereby enhancing both the training\nefficiency and effectiveness. Our code is available at\nhttps://github.com/deepseek-ai/ESFT.", "published": "2024-07-02 03:11:13", "link": "http://arxiv.org/abs/2407.01906v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pinyin Regularization in Error Correction for Chinese Speech Recognition\n  with Large Language Models", "abstract": "Recent studies have demonstrated the efficacy of large language models (LLMs)\nin error correction for automatic speech recognition (ASR). However, much of\nthe research focuses on the English language. This paper redirects the\nattention to Chinese. Firstly, we construct a specialized benchmark dataset\naimed at error correction for Chinese ASR with 724K hypotheses-transcription\npairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which\ncontains a wide range of scenarios and presents significant challenges.\nSubsequently, we conduct a preliminary evaluation using the dataset for both\ndirect-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a\nstraightforward method of Pinyin regularization for prompts, which involves the\ntranscription of Pinyin directly from text hypotheses. The experimental results\nreveal that Pinyin regularization consistently enhances the error-correcting\nability of LLMs when compared with those without regularization. The dataset is\navailable on the website.", "published": "2024-07-02 03:16:47", "link": "http://arxiv.org/abs/2407.01909v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and\n  Different Speech Foundation Model on Dialogue Generative Spoken Language\n  Model", "abstract": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue\nwithout the need for direct transcription, thereby preserving the wealth of\nnon-textual information inherent in speech. However, this approach faces a\nchallenge when speakers talk simultaneously, requiring stereo dialogue data\nwith speakers recorded on separate channels, a notably scarce resource. To\naddress this, we have developed an innovative pipeline capable of transforming\nsingle-channel dialogue data into pseudo-stereo data. This expanded our\ntraining dataset from a mere 2,000 to an impressive 17,600 hours, significantly\nenriching the diversity and quality of the training examples available. The\ninclusion of this pseudo-stereo data has proven to be effective in improving\nthe performance of spoken dialogue language models. Additionally, we explored\nthe use of discrete units of different speech foundation models for spoken\ndialogue generation.", "published": "2024-07-02 03:22:41", "link": "http://arxiv.org/abs/2407.01911v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and\n  Aleatoric Awareness", "abstract": "The ability to acknowledge the inevitable uncertainty in their knowledge and\nreasoning is a prerequisite for AI systems to be truly truthful and reliable.\nIn this paper, we present a taxonomy of uncertainty specific to vision-language\nAI systems, distinguishing between epistemic uncertainty (arising from a lack\nof information) and aleatoric uncertainty (due to inherent unpredictability),\nand further explore finer categories within. Based on this taxonomy, we\nsynthesize a benchmark dataset, CertainlyUncertain, featuring 178K visual\nquestion answering (VQA) samples as contrastive pairs. This is achieved by 1)\ninpainting images to make previously answerable questions into unanswerable\nones; and 2) using image captions to prompt large language models for both\nanswerable and unanswerable questions. Additionally, we introduce a new metric\nconfidence-weighted accuracy, that is well correlated with both accuracy and\ncalibration error, to address the shortcomings of existing metrics.", "published": "2024-07-02 04:23:54", "link": "http://arxiv.org/abs/2407.01942v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Extracting and Encoding: Leveraging Large Language Models and Medical\n  Knowledge to Enhance Radiological Text Representation", "abstract": "Advancing representation learning in specialized fields like medicine remains\nchallenging due to the scarcity of expert annotations for text and images. To\ntackle this issue, we present a novel two-stage framework designed to extract\nhigh-quality factual statements from free-text radiology reports in order to\nimprove the representations of text encoders and, consequently, their\nperformance on various downstream tasks. In the first stage, we propose a\n\\textit{Fact Extractor} that leverages large language models (LLMs) to identify\nfactual statements from well-curated domain-specific datasets. In the second\nstage, we introduce a \\textit{Fact Encoder} (CXRFE) based on a BERT model\nfine-tuned with objective functions designed to improve its representations\nusing the extracted factual data. Our framework also includes a new\nembedding-based metric (CXRFEScore) for evaluating chest X-ray text generation\nsystems, leveraging both stages of our approach. Extensive evaluations show\nthat our fact extractor and encoder outperform current state-of-the-art methods\nin tasks such as sentence ranking, natural language inference, and label\nextraction from radiology reports. Additionally, our metric proves to be more\nrobust and effective than existing metrics commonly used in the radiology\nreport generation literature. The code of this project is available at\n\\url{https://github.com/PabloMessina/CXR-Fact-Encoder}.", "published": "2024-07-02 04:39:19", "link": "http://arxiv.org/abs/2407.01948v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding", "abstract": "Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.2% increase on\nKIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based\nLLMs on KIE tasks.", "published": "2024-07-02 06:29:05", "link": "http://arxiv.org/abs/2407.01976v2", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph\n  Completion", "abstract": "High-quality and high-coverage rule sets are imperative to the success of\nNeuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form\nthe basis of all symbolic inferences. Recent literature builds neural models\nfor generating rule sets, however, preliminary experiments show that they\nstruggle with maintaining high coverage. In this work, we suggest three simple\naugmentations to existing rule sets: (1) transforming rules to their abductive\nforms, (2) generating equivalent rules that use inverse forms of constituent\nrelations and (3) random walks that propose new rules. Finally, we prune\npotentially low quality rules. Experiments over four datasets and five\nruleset-baseline settings suggest that these simple augmentations consistently\nimprove results, and obtain up to 7.1 pt MRR and 8.5 pt Hits@1 gains over using\nrules without augmentations.", "published": "2024-07-02 07:07:59", "link": "http://arxiv.org/abs/2407.01994v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "An End-to-End Speech Summarization Using Large Language Model", "abstract": "Abstractive Speech Summarization (SSum) aims to generate human-like text\nsummaries from spoken content. It encounters difficulties in handling long\nspeech input and capturing the intricate cross-modal mapping between long\nspeech inputs and short text summaries. Research on large language models\n(LLMs) and multimodal information fusion has provided new insights for\naddressing these challenges. In this paper, we propose an end-to-end SSum model\nthat utilizes Q-Former as a connector for the audio-text modality and employs\nLLMs to generate text summaries directly from speech features. We adopt a\nmulti-stage training approach that includes LLM based ASR and Text\nSummarization (TSum) tasks as auxiliary tasks. ASR tasks are used to align\nfeature spaces and enhance the LLM's ability to handle longer speech. Then, we\nutilize a curriculum learning strategy to facilitate the model's transition\nfrom TSum to SSum. Finally, our model achieves competitive performance on the\nHow-2 dataset.", "published": "2024-07-02 07:22:57", "link": "http://arxiv.org/abs/2407.02005v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Why does in-context learning fail sometimes? Evaluating in-context\n  learning on open and closed questions", "abstract": "We measure the performance of in-context learning as a function of task\nnovelty and difficulty for open and closed questions. For that purpose, we\ncreated a novel benchmark consisting of hard scientific questions, each paired\nwith a context of various relevancy. We show that counter-intuitively, a\ncontext that is more aligned with the topic does not always help more than a\nless relevant context. This effect is especially visible for open questions and\nquestions of high difficulty or novelty. This result reveals a fundamental\ndifference between the treatment of close-form and open-form questions by\nlarge-language models and shows a need for a more robust evaluation of\nin-context learning on the variety of different types of questions. It also\nposes a new question of how to optimally select a context for large language\nmodels, especially in the context of Retrieval Augmented Generation (RAG)\nsystems. Our results suggest that the answer to this question can be highly\napplication-dependent and might be contingent on factors including the format\nof the question, the perceived difficulty level of the questions, and the\nnovelty or popularity of the information we seek.", "published": "2024-07-02 07:52:30", "link": "http://arxiv.org/abs/2407.02028v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accompanied Singing Voice Synthesis with Fully Text-controlled Melody", "abstract": "Text-to-song (TTSong) is a music generation task that synthesizes accompanied\nsinging voices. Current TTSong methods, inherited from singing voice synthesis\n(SVS), require melody-related information that can sometimes be impractical,\nsuch as music scores or MIDI sequences. We present MelodyLM, the first TTSong\nmodel that generates high-quality song pieces with fully text-controlled\nmelodies, achieving minimal user requirements and maximum control flexibility.\nMelodyLM explicitly models MIDI as the intermediate melody-related feature and\nsequentially generates vocal tracks in a language model manner, conditioned on\ntextual and vocal prompts. The accompaniment music is subsequently synthesized\nby a latent diffusion model with hybrid conditioning for temporal alignment.\nWith minimal requirements, users only need to input lyrics and a reference\nvoice to synthesize a song sample. For full control, just input textual prompts\nor even directly input MIDI. Experimental results indicate that MelodyLM\nachieves superior performance in terms of both objective and subjective\nmetrics. Audio samples are available at https://melodylm666.github.io.", "published": "2024-07-02 08:23:38", "link": "http://arxiv.org/abs/2407.02049v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Are Data Augmentation Methods in Named Entity Recognition Applicable for\n  Uncertainty Estimation?", "abstract": "This work investigates the impact of data augmentation on confidence\ncalibration and uncertainty estimation in Named Entity Recognition (NER) tasks.\nFor the future advance of NER in safety-critical fields like healthcare and\nfinance, it is essential to achieve accurate predictions with calibrated\nconfidence when applying Deep Neural Networks (DNNs), including Pre-trained\nLanguage Models (PLMs), as a real-world application. However, DNNs are prone to\nmiscalibration, which limits their applicability. Moreover, existing methods\nfor calibration and uncertainty estimation are computational expensive. Our\ninvestigation in NER found that data augmentation improves calibration and\nuncertainty in cross-genre and cross-lingual setting, especially in-domain\nsetting. Furthermore, we showed that the calibration for NER tends to be more\neffective when the perplexity of the sentences generated by data augmentation\nis lower, and that increasing the size of the augmentation further improves\ncalibration and uncertainty.", "published": "2024-07-02 08:49:43", "link": "http://arxiv.org/abs/2407.02062v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active\n  Learning", "abstract": "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs.", "published": "2024-07-02 10:09:19", "link": "http://arxiv.org/abs/2407.02119v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural\n  Language Processing Tasks", "abstract": "Trustworthiness in model predictions is crucial for safety-critical\napplications in the real world. However, deep neural networks often suffer from\nthe issues of uncertainty estimation, such as miscalibration. In this study, we\npropose $k$-Nearest Neighbor Uncertainty Estimation ($k$NN-UE), which is a new\nuncertainty estimation method that uses not only the distances from the\nneighbors, but also the ratio of labels in the neighbors. Experiments on\nsentiment analysis, natural language inference, and named entity recognition\nshow that our proposed method outperforms the baselines and recent\ndensity-based methods in several calibration and uncertainty metrics. Moreover,\nour analyses indicate that approximate nearest neighbor search techniques\nreduce the inference overhead without significantly degrading the uncertainty\nestimation performance when they are appropriately combined.", "published": "2024-07-02 10:33:31", "link": "http://arxiv.org/abs/2407.02138v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt\n  during Large Language Model Fine-tuning", "abstract": "Recent advances in fine-tuning large language models (LLMs) have greatly\nenhanced their usage in domain-specific tasks. Despite the success, fine-tuning\ncontinues to rely on repeated and lengthy prompts, which escalate computational\nexpenses, require more resources, and lead to slower inference. In this paper,\nwe present a novel approach, PromptIntern, which internalizes prompt knowledge\nduring model fine-tuning to achieve efficient inference and save costs. Instead\nof compressing the prompts for a vanilla model, PromptIntern aims to embed the\nrecurrent prompt directly into the model parameters. We design a fine-tuning\npipeline that includes instruction template compression, few-shot example\nabsorption, and a progressive internalization strategy, effectively diminishing\nthe need for intricate prompts during inference. Comprehensive experiments on\nchallenging NL2Code tasks demonstrate that our method reduces input tokens by\nmore than 90%, accelerates inference by 4.2 times, and reduces monetary\ninference costs by 88.3%.", "published": "2024-07-02 12:21:14", "link": "http://arxiv.org/abs/2407.02211v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Multimodal Question Generation", "abstract": "Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to\nquestion-answering over multimodal documents. A key challenge with evaluating\nMMRAG is the paucity of high-quality datasets matching the question styles and\nmodalities of interest. In light of this, we propose SMMQG, a synthetic data\ngeneration framework. SMMQG leverages interplay between a retriever, large\nlanguage model (LLM) and large multimodal model (LMM) to generate question and\nanswer pairs directly from multimodal documents, with the questions conforming\nto specified styles and modalities. We use SMMQG to generate an MMRAG dataset\nof 1024 questions over Wikipedia documents and evaluate state-of-the-art models\nusing it, revealing insights into model performance that are attainable only\nthrough style- and modality-specific evaluation data. Next, we measure the\nquality of data produced by SMMQG via a human study. We find that the quality\nof SMMQG-generated synthetic data is on par with the quality of the\ncrowdsourced benchmark MMQA and that downstream evaluation results using both\ndatasets strongly concur.", "published": "2024-07-02 12:57:42", "link": "http://arxiv.org/abs/2407.02233v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference\n  Optimization", "abstract": "In this paper, we propose reverse inference optimization (RIO), a simple and\neffective method designed to enhance the robustness of\nautoregressive-model-based zero-shot text-to-speech (TTS) systems using\nreinforcement learning from human feedback (RLHF). To assess the quality of\nspeech produced by the TTS system without human annotations, RIO introduces a\nnovel concept termed as reverse inference based on the Bayesian principle,\nwhich suggests that a high-quality generated speech should be able to be used\nas a prompt for subsequent generation using the same TTS model. By leveraging\nreverse inference as the standard to select exemplars used in RLHF from the\nspeech samples generated by the TTS system itself, RIO steers the subsequent\noptimization towards a direction of enhancing the TTS robustness. The RIO\nframework, comprising sampling, automatic annotating, and learning, obviates\nthe need for a reward model or pairwise preference data, and significantly\nimproves the stability of zero-shot TTS performance by reducing the\ndiscrepancies between training and inference conditions. Our experimental\nresults verify that RIO can effectively improve both subjective and objective\nmetrics, including mean opinion scores, word error rates, and speaker\nsimilarity. Remarkably, RIO can also diminish the incidence of bad outputs to\nnearly zero percent, rivalling the robustness when using ground-truth speech as\nthe prompt.", "published": "2024-07-02 13:04:04", "link": "http://arxiv.org/abs/2407.02243v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Talking to Machines: do you read me?", "abstract": "In this dissertation I would like to guide the reader to the research on\ndialogue but more precisely the research I have conducted during my career\nsince my PhD thesis. Starting from modular architectures with machine\nlearning/deep learning and reinforcement learning to end-to-end deep neural\nnetworks. Besides my work as research associate, I also present the work I have\nsupervised in the last years.\n  I review briefly the state of the art and highlight the open research\nproblems on conversational agents. Afterwards, I present my contribution to\nTask-Oriented Dialogues (TOD), both as research associate and as the industrial\nsupervisor of CIFRE theses. I discuss conversational QA. Particularly, I\npresent the work of two PhD candidates Thibault Cordier and Sebastien Montella;\nas well as the work of the young researcher Quentin Brabant. Finally, I present\nthe scientific project, where I discuss about Large Language Models (LLMs) for\nTask-Oriented Dialogue and Multimodal Task-Oriented Dialogue.", "published": "2024-07-02 15:19:46", "link": "http://arxiv.org/abs/2407.02354v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in\n  LLMs", "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.", "published": "2024-07-02 17:59:17", "link": "http://arxiv.org/abs/2407.02485v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling", "abstract": "This paper introduces Neurocache, an approach to extend the effective context\nsize of large language models (LLMs) using an external vector cache to store\nits past states. Like recent vector retrieval approaches, Neurocache uses an\nefficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states\nand incorporate them into the attention process. Neurocache improves upon\nprevious methods by (1) storing compressed states, which reduces cache size;\n(2) performing a single retrieval operation per token which increases inference\nspeed; and (3) extending the retrieval window to neighboring states, which\nimproves both language modeling and downstream task accuracy. Our experiments\nshow the effectiveness of Neurocache both for models trained from scratch and\nfor pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the\ncache mechanism. We also compare Neurocache with text retrieval methods and\nshow improvements in single-document question-answering and few-shot learning\ntasks. We made the source code available under:\nhttps://github.com/alisafaya/neurocache", "published": "2024-07-02 17:59:29", "link": "http://arxiv.org/abs/2407.02486v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference\n  Optimization for LLMs", "abstract": "Preference optimization techniques have become a standard final stage for\ntraining state-of-art large language models (LLMs). However, despite widespread\nadoption, the vast majority of work to-date has focused on first-class citizen\nlanguages like English and Chinese. This captures a small fraction of the\nlanguages in the world, but also makes it unclear which aspects of current\nstate-of-the-art research transfer to a multilingual setting. In this work, we\nperform an exhaustive study to achieve a new state-of-the-art in aligning\nmultilingual LLMs. We introduce a novel, scalable method for generating\nhigh-quality multilingual feedback data to balance data coverage. We establish\nthe benefits of cross-lingual transfer and increased dataset size in preference\ntraining. Our preference-trained model achieves a 54.4% win-rate against Aya 23\n8B, the current state-of-the-art multilingual LLM in its parameter class, and a\n69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it,\nLlama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we\nexpand the frontier of alignment techniques to 23 languages covering half of\nthe world's population.", "published": "2024-07-02 17:42:30", "link": "http://arxiv.org/abs/2407.02552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards More Realistic Extraction Attacks: An Adversarial Perspective", "abstract": "Language models are prone to memorizing parts of their training data which\nmakes them vulnerable to extraction attacks. Existing research often examines\nisolated setups--such as evaluating extraction risks from a single model or\nwith a fixed prompt design. However, a real-world adversary could access models\nacross various sizes and checkpoints, as well as exploit prompt sensitivity,\nresulting in a considerably larger attack surface than previously studied. In\nthis paper, we revisit extraction attacks from an adversarial perspective,\nfocusing on how to leverage the brittleness of language models and the\nmulti-faceted access to the underlying data. We find significant churn in\nextraction trends, i.e., even unintuitive changes to the prompt, or targeting\nsmaller models and earlier checkpoints, can extract distinct information. By\ncombining information from multiple attacks, our adversary is able to increase\nthe extraction risks by up to $2 \\times$. Furthermore, even with mitigation\nstrategies like data deduplication, we find the same escalation of extraction\nrisks against a real-world adversary. We conclude with a set of case studies,\nincluding detecting pre-training data, copyright violations, and extracting\npersonally identifiable information, showing how our more realistic adversary\ncan outperform existing adversaries in the literature.", "published": "2024-07-02 18:33:49", "link": "http://arxiv.org/abs/2407.02596v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions", "abstract": "Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.", "published": "2024-07-02 18:43:10", "link": "http://arxiv.org/abs/2407.02604v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.AI"}
{"title": "Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective\n  Shifts in Large Multi-modal Models", "abstract": "Recent work has demonstrated that the unequal representation of cultures and\nsocioeconomic groups in training data leads to biased Large Multi-modal (LMM)\nmodels. To improve LMM model performance on underrepresented data, we propose\nand evaluate several prompting strategies using non-English, geographic, and\nsocioeconomic attributes. We show that these geographic and socioeconomic\nintegrated prompts favor retrieving topic appearances commonly found in data\nfrom low-income households across different countries leading to improved LMM\nmodel performance on lower-income data. Our analyses identify and highlight\ncontexts where these strategies yield the most improvements.", "published": "2024-07-02 19:27:00", "link": "http://arxiv.org/abs/2407.02623v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "K.4; I.2.7; I.2.8"], "primary_category": "cs.CY"}
{"title": "Nollywood: Let's Go to the Movies!", "abstract": "Nollywood, based on the idea of Bollywood from India, is a series of\noutstanding movies that originate from Nigeria. Unfortunately, while the movies\nare in English, they are hard to understand for many native speakers due to the\ndialect of English that is spoken. In this article, we accomplish two goals:\n(1) create a phonetic sub-title model that is able to translate Nigerian\nEnglish speech to American English and (2) use the most advanced toxicity\ndetectors to discover how toxic the speech is. Our aim is to highlight the text\nin these videos which is often times ignored for lack of dialectal\nunderstanding due the fact that many people in Nigeria speak a native language\nlike Hausa at home.", "published": "2024-07-02 19:50:55", "link": "http://arxiv.org/abs/2407.02631v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Change My Frame: Reframing in the Wild in r/ChangeMyView", "abstract": "Recent work in reframing, within the scope of text style transfer, has so far\nmade use of out-of-context, task-prompted utterances in order to produce\nneutralizing or optimistic reframes. Our work aims to generalize reframing\nbased on the subreddit r/ChangeMyView (CMV). We build a dataset that leverages\nCMV's community's interactions and conventions to identify high-value,\ncommunity-recognized utterances that produce changes of perspective. With this\ndata, we widen the scope of the direction of reframing since the changes in\nperspective do not only occur in neutral or positive directions. We fine tune\ntransformer-based models, make use of a modern LLM to refine our dataset, and\nexplore challenges in the dataset creation and evaluation around this type of\nreframing.", "published": "2024-07-02 20:09:11", "link": "http://arxiv.org/abs/2407.02637v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Supporters and Skeptics: LLM-based Analysis of Engagement with Mental\n  Health (Mis)Information Content on Video-sharing Platforms", "abstract": "Over one in five adults in the US lives with a mental illness. In the face of\na shortage of mental health professionals and offline resources, online\nshort-form video content has grown to serve as a crucial conduit for\ndisseminating mental health help and resources. However, the ease of content\ncreation and access also contributes to the spread of misinformation, posing\nrisks to accurate diagnosis and treatment. Detecting and understanding\nengagement with such content is crucial to mitigating their harmful effects on\npublic health. We perform the first quantitative study of the phenomenon using\nYouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,\na novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos\n(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an\nexpert-driven annotation schema. We first found that few-shot in-context\nlearning with large language models (LLMs) are effective in detecting MHMisinfo\nvideos. Next, we discover distinct and potentially alarming linguistic patterns\nin how audiences engage with MHMisinfo videos through commentary on both\nvideo-sharing platforms. Across the two platforms, comments could exacerbate\nprevailing stigma with some groups showing heightened susceptibility to and\nalignment with MHMisinfo. We discuss technical and public health-driven\nadaptive solutions to tackling the \"epidemic\" of mental health misinformation\nonline.", "published": "2024-07-02 20:51:06", "link": "http://arxiv.org/abs/2407.02662v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "LLM-Select: Feature Selection with Large Language Models", "abstract": "In this paper, we demonstrate a surprising capability of large language\nmodels (LLMs): given only input feature names and a description of a prediction\ntask, they are capable of selecting the most predictive features, with\nperformance rivaling the standard tools of data science. Remarkably, these\nmodels exhibit this capacity across various query mechanisms. For example, we\nzero-shot prompt an LLM to output a numerical importance score for a feature\n(e.g., \"blood pressure\") in predicting an outcome of interest (e.g., \"heart\nfailure\"), with no additional context. In particular, we find that the latest\nmodels, such as GPT-4, can consistently identify the most predictive features\nregardless of the query mechanism and across various prompting strategies. We\nillustrate these findings through extensive experiments on real-world data,\nwhere we show that LLM-based feature selection consistently achieves strong\nperformance competitive with data-driven methods such as the LASSO, despite\nnever having looked at the downstream training data. Our findings suggest that\nLLMs may be useful not only for selecting the best features for training but\nalso for deciding which features to collect in the first place. This could\npotentially benefit practitioners in domains like healthcare, where collecting\nhigh-quality data comes at a high cost.", "published": "2024-07-02 22:23:40", "link": "http://arxiv.org/abs/2407.02694v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "abstract": "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities.", "published": "2024-07-02 07:12:51", "link": "http://arxiv.org/abs/2407.12043v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Depression Detection Method Based on Multi-Modal Feature Fusion Using\n  Cross-Attention", "abstract": "Depression, a prevalent and serious mental health issue, affects\napproximately 3.8\\% of the global population. Despite the existence of\neffective treatments, over 75\\% of individuals in low- and middle-income\ncountries remain untreated, partly due to the challenge in accurately\ndiagnosing depression in its early stages. This paper introduces a novel method\nfor detecting depression based on multi-modal feature fusion utilizing\ncross-attention. By employing MacBERT as a pre-training model to extract\nlexical features from text and incorporating an additional Transformer module\nto refine task-specific contextual understanding, the model's adaptability to\nthe targeted task is enhanced. Diverging from previous practices of simply\nconcatenating multimodal features, this approach leverages cross-attention for\nfeature integration, significantly improving the accuracy in depression\ndetection and enabling a more comprehensive and precise analysis of user\nemotions and behaviors. Furthermore, a Multi-Modal Feature Fusion Network based\non Cross-Attention (MFFNC) is constructed, demonstrating exceptional\nperformance in the task of depression identification. The experimental results\nindicate that our method achieves an accuracy of 0.9495 on the test dataset,\nmarking a substantial improvement over existing approaches. Moreover, it\noutlines a promising methodology for other social media platforms and tasks\ninvolving multi-modal processing. Timely identification and intervention for\nindividuals with depression are crucial for saving lives, highlighting the\nimmense potential of technology in facilitating early intervention for mental\nhealth issues.", "published": "2024-07-02 13:13:35", "link": "http://arxiv.org/abs/2407.12825v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Model for Small Molecules with Latent Space RL Fine-Tuning to\n  Protein Targets", "abstract": "A specific challenge with deep learning approaches for molecule generation is\ngenerating both syntactically valid and chemically plausible molecular string\nrepresentations. To address this, we propose a novel generative latent-variable\ntransformer model for small molecules that leverages a recently proposed\nmolecular string representation called SAFE. We introduce a modification to\nSAFE to reduce the number of invalid fragmented molecules generated during\ntraining and use this to train our model. Our experiments show that our model\ncan generate novel molecules with a validity rate > 90% and a fragmentation\nrate < 1% by sampling from a latent space. By fine-tuning the model using\nreinforcement learning to improve molecular docking, we significantly increase\nthe number of hit candidates for five specific protein targets compared to the\npre-trained model, nearly doubling this number for certain targets.\nAdditionally, our top 5% mean docking scores are comparable to the current\nstate-of-the-art (SOTA), and we marginally outperform SOTA on three of the five\ntargets.", "published": "2024-07-02 16:01:37", "link": "http://arxiv.org/abs/2407.13780v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,\n  Whose Perspectives?", "abstract": "We argue for the epistemic and ethical advantages of pluralism in\nReinforcement Learning from Human Feedback (RLHF) in the context of Large\nLanguage Models (LLM). Drawing on social epistemology and pluralist philosophy\nof science, we suggest ways in which RHLF can be made more responsive to human\nneeds and how we can address challenges along the way. The paper concludes with\nan agenda for change, i.e. concrete, actionable steps to improve LLM\ndevelopment.", "published": "2024-07-02 08:07:27", "link": "http://arxiv.org/abs/2407.17482v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large\n  Language Models", "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain\nsensitive data, such as personal privacy information and copyrighted material.\nRecent advancements in knowledge unlearning involve updating LLM parameters to\nerase specific knowledge. However, current unlearning paradigms are mired in\nvague forgetting boundaries, often erasing knowledge indiscriminately. In this\nwork, we introduce KnowUnDo, a benchmark containing copyrighted content and\nuser privacy domains to evaluate if the unlearning process inadvertently erases\nessential knowledge. Our findings indicate that existing unlearning methods\noften suffer from excessive unlearning. To address this, we propose a simple\nyet effective method, MemFlex, which utilizes gradient information to precisely\ntarget and unlearn sensitive parameters. Experimental results show that MemFlex\nis superior to existing methods in both precise knowledge unlearning and\ngeneral knowledge retaining of LLMs. Code and dataset are released at\nhttps://github.com/zjunlp/KnowUnDo.", "published": "2024-07-02 03:34:16", "link": "http://arxiv.org/abs/2407.01920v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring\n  Expression Segmentation", "abstract": "Referring Expression Segmentation (RES) aims to provide a segmentation mask\nof the target object in an image referred to by the text (i.e., referring\nexpression). Existing methods require large-scale mask annotations. Moreover,\nsuch approaches do not generalize well to unseen/zero-shot scenarios. To\naddress the aforementioned issues, we propose a weakly-supervised bootstrapping\narchitecture for RES with several new algorithmic innovations. To the best of\nour knowledge, ours is the first approach that considers only a fraction of\nboth mask and box annotations (shown in Figure 1 and Table 1) for training. To\nenable principled training of models in such low-annotation settings, improve\nimage-text region-level alignment, and further enhance spatial localization of\nthe target object in the image, we propose Cross-modal Fusion with Attention\nConsistency module. For automatic pseudo-labeling of unlabeled samples, we\nintroduce a novel Mask Validity Filtering routine based on a spatially aware\nzero-shot proposal scoring approach. Extensive experiments show that with just\n30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to\n58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR\nrespectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi also\noutperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a\nfully-supervised setting and demonstrates strong generalization capabilities in\nunseen/zero-shot tasks.", "published": "2024-07-02 16:02:25", "link": "http://arxiv.org/abs/2407.02389v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Towards the Next Frontier in Speech Representation Learning Using\n  Disentanglement", "abstract": "The popular frameworks for self-supervised learning of speech representations\nhave largely focused on frame-level masked prediction of speech regions. While\nthis has shown promising downstream task performance for speech recognition and\nrelated tasks, this has largely ignored factors of speech that are encoded at\ncoarser level, like characteristics of the speaker or channel that remain\nconsistent through-out a speech utterance. In this work, we propose a framework\nfor Learning Disentangled Self Supervised (termed as Learn2Diss)\nrepresentations of speech, which consists of frame-level and an utterance-level\nencoder modules. The two encoders are initially learned independently, where\nthe frame-level model is largely inspired by existing self supervision\ntechniques, thereby learning pseudo-phonemic representations, while the\nutterance-level encoder is inspired by constrastive learning of pooled\nembeddings, thereby learning pseudo-speaker representations. The joint learning\nof these two modules consists of disentangling the two encoders using a mutual\ninformation based criterion. With several downstream evaluation experiments, we\nshow that the proposed Learn2Diss achieves state-of-the-art results on a\nvariety of tasks, with the frame-level encoder representations improving\nsemantic tasks, while the utterance-level representations improve non-semantic\ntasks.", "published": "2024-07-02 07:13:35", "link": "http://arxiv.org/abs/2407.02543v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Unsupervised Speaker Diarization System for Multilingual\n  Telephone Calls Using Pre-trained Whisper Model and Mixture of Sparse\n  Autoencoders", "abstract": "Existing speaker diarization systems typically rely on large amounts of\nmanually annotated data, which is labor-intensive and difficult to obtain,\nespecially in real-world scenarios. Additionally, language-specific constraints\nin these systems significantly hinder their effectiveness and scalability in\nmultilingual settings. In this paper, we propose a cluster-based speaker\ndiarization system designed for multilingual telephone call applications. Our\nproposed system supports multiple languages and eliminates the need for\nlarge-scale annotated data during training by utilizing the multilingual\nWhisper model to extract speaker embeddings. Additionally, we introduce a\nnetwork architecture called Mixture of Sparse Autoencoders (Mix-SAE) for\nunsupervised speaker clustering. Experimental results on the evaluation dataset\nderived from two-speaker subsets of benchmark CALLHOME and CALLFRIEND\ntelephonic speech corpora demonstrate the superior performance of the proposed\nMix-SAE network to other autoencoder-based clustering methods. The overall\nperformance of our proposed system also highlights the promising potential for\ndeveloping unsupervised, multilingual speaker diarization systems within the\ncontext of limited annotated data. It also indicates the system's capability\nfor integration into multi-task speech analysis applications based on\ngeneral-purpose models such as those that combine speech-to-text, language\ndetection, and speaker diarization.", "published": "2024-07-02 05:42:32", "link": "http://arxiv.org/abs/2407.01963v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SOT Triggered Neural Clustering for Speaker Attributed ASR", "abstract": "This paper introduces a novel approach to speaker-attributed ASR\ntranscription using a neural clustering method. With a parallel processing\nmechanism, diarisation and ASR can be applied simultaneously, helping to\nprevent the accumulation of errors from one sub-system to the next in a\ncascaded system. This is achieved by the use of ASR, trained using a serialised\noutput training method, together with segment-level discriminative neural\nclustering (SDNC) to assign speaker labels. With SDNC, our system does not\nrequire an extra non-neural clustering method to assign speaker labels, thus\nallowing the entire system to be based on neural networks. Experimental results\non the AMI meeting dataset demonstrate that SDNC outperforms spectral\nclustering (SC) by a 19% relative diarisation error rate (DER) reduction on the\nAMI Eval set. When compared with the cascaded system with SC, the parallel\nsystem with SDNC gives a 7%/4% relative improvement in cpWER on the Dev/Eval\nset.", "published": "2024-07-02 07:26:29", "link": "http://arxiv.org/abs/2407.02007v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TTSlow: Slow Down Text-to-Speech with Efficiency Robustness Evaluations", "abstract": "Text-to-speech (TTS) has been extensively studied for generating high-quality\nspeech with textual inputs, playing a crucial role in various real-time\napplications. For real-world deployment, ensuring stable and timely generation\nin TTS models against minor input perturbations is of paramount importance.\nTherefore, evaluating the robustness of TTS models against such perturbations,\ncommonly known as adversarial attacks, is highly desirable. In this paper, we\npropose TTSlow, a novel adversarial approach specifically tailored to slow down\nthe speech generation process in TTS systems. To induce long TTS waiting time,\nwe design novel efficiency-oriented adversarial loss to encourage endless\ngeneration process. TTSlow encompasses two attack strategies targeting both\ntext inputs and speaker embedding. Specifically, we propose TTSlow-text, which\nutilizes a combination of homoglyphs-based and swap-based perturbations, along\nwith TTSlow-spk, which employs a gradient optimization attack approach for\nspeaker embedding. TTSlow serves as the first attack approach targeting a wide\nrange of TTS models, including autoregressive and non-autoregressive TTS ones,\nthereby advancing exploration in audio security. Extensive experiments are\nconducted to evaluate the inference efficiency of TTS models, and in-depth\nanalysis of generated speech intelligibility is performed using Gemini. The\nresults demonstrate that TTSlow can effectively slow down two TTS models across\nthree publicly available datasets. We are committed to releasing the source\ncode upon acceptance, facilitating further research and benchmarking in this\ndomain.", "published": "2024-07-02 03:43:56", "link": "http://arxiv.org/abs/2407.01927v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Unsupervised Face-Masked Speech Enhancement Using Generative Adversarial\n  Networks With Human-in-the-Loop Assessment Metrics", "abstract": "The utilization of face masks is an essential healthcare measure,\nparticularly during times of pandemics, yet it can present challenges in\ncommunication in our daily lives. To address this problem, we propose a novel\napproach known as the human-in-the-loop StarGAN (HL-StarGAN) face-masked speech\nenhancement method. HL-StarGAN comprises discriminator, classifier, metric\nassessment predictor, and generator that leverages an attention mechanism. The\nmetric assessment predictor, referred to as MaskQSS, incorporates human\nparticipants in its development and serves as a \"human-in-the-loop\" module\nduring the learning process of HL-StarGAN. The overall HL-StarGAN model was\ntrained using an unsupervised learning strategy that simultaneously focuses on\nthe reconstruction of the original clean speech and the optimization of human\nperception. To implement HL-StarGAN, we curated a face-masked speech database\nnamed \"FMVD,\" which comprises recordings from 34 speakers in three distinct\nface-masked scenarios and a clean condition. We conducted subjective and\nobjective tests on the proposed HL-StarGAN using this database. The outcomes of\nthe test results are as follows: (1) MaskQSS successfully predicted the quality\nscores of face mask voices, outperforming several existing speech assessment\nmethods. (2) The integration of the MaskQSS predictor enhanced the ability of\nHL-StarGAN to transform face mask voices into high-quality speech; this\nenhancement is evident in both objective and subjective tests, outperforming\nconventional StarGAN and CycleGAN-based systems.", "published": "2024-07-02 04:13:59", "link": "http://arxiv.org/abs/2407.01939v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The USTC-NERCSLIP Systems for The ICMC-ASR Challenge", "abstract": "This report describes the submitted system to the In-Car Multi-Channel\nAutomatic Speech Recognition (ICMC-ASR) challenge, which considers the ASR task\nwith multi-speaker overlapping and Mandarin accent dynamics in the ICMC case.\nWe implement the front-end speaker diarization using the self-supervised\nlearning representation based multi-speaker embedding and beamforming using the\nspeaker position, respectively. For ASR, we employ an iterative pseudo-label\ngeneration method based on fusion model to obtain text labels of unsupervised\ndata. To mitigate the impact of accent, an Accent-ASR framework is proposed,\nwhich captures pronunciation-related accent features at a fine-grained level\nand linguistic information at a coarse-grained level. On the ICMC-ASR eval set,\nthe proposed system achieves a CER of 13.16% on track 1 and a cpCER of 21.48%\non track 2, which significantly outperforms the official baseline system and\nobtains the first rank on both tracks.", "published": "2024-07-02 08:30:51", "link": "http://arxiv.org/abs/2407.02052v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GMM-ResNet2: Ensemble of Group ResNet Networks for Synthetic Speech\n  Detection", "abstract": "Deep learning models are widely used for speaker recognition and spoofing\nspeech detection. We propose the GMM-ResNet2 for synthesis speech detection.\nCompared with the previous GMM-ResNet model, GMM-ResNet2 has four improvements.\nFirstly, the different order GMMs have different capabilities to form smooth\napproximations to the feature distribution, and multiple GMMs are used to\nextract multi-scale Log Gaussian Probability features. Secondly, the grouping\ntechnique is used to improve the classification accuracy by exposing the group\ncardinality while reducing both the number of parameters and the training time.\nThe final score is obtained by ensemble of all group classifier outputs using\nthe averaging method. Thirdly, the residual block is improved by including one\nactivation function and one batch normalization layer. Finally, an\nensemble-aware loss function is proposed to integrate the independent loss\nfunctions of all ensemble members. On the ASVspoof 2019 LA task, the\nGMM-ResNet2 achieves a minimum t-DCF of 0.0227 and an EER of 0.79\\%. On the\nASVspoof 2021 LA task, the GMM-ResNet2 achieves a minimum t-DCF of 0.2362 and\nan EER of 2.19\\%, and represents a relative reductions of 31.4\\% and 76.3\\%\ncompared with the LFCC-LCNN baseline.", "published": "2024-07-02 11:25:42", "link": "http://arxiv.org/abs/2407.02170v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music\n  Processing", "abstract": "In the domain of symbolic music research, the progress of developing scalable\nsystems has been notably hindered by the scarcity of available training data\nand the demand for models tailored to specific tasks. To address these issues,\nwe propose MelodyT5, a novel unified framework that leverages an\nencoder-decoder architecture tailored for symbolic music processing in ABC\nnotation. This framework challenges the conventional task-specific approach,\nconsidering various symbolic music tasks as score-to-score transformations.\nConsequently, it integrates seven melody-centric tasks, from generation to\nharmonization and segmentation, within a single model. Pre-trained on\nMelodyHub, a newly curated collection featuring over 261K unique melodies\nencoded in ABC notation and encompassing more than one million task instances,\nMelodyT5 demonstrates superior performance in symbolic music processing via\nmulti-task transfer learning. Our findings highlight the efficacy of multi-task\ntransfer learning in symbolic music processing, particularly for data-scarce\ntasks, challenging the prevailing task-specific paradigms and offering a\ncomprehensive dataset and framework for future explorations in this domain.", "published": "2024-07-02 14:06:31", "link": "http://arxiv.org/abs/2407.02277v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Bit Transmission of Adaptive Pre- and De-emphasis Filters for\n  Speech and Audio Coding", "abstract": "This paper introduces a novel adaptation approach for first-order pre- and\nde-emphasis filters, an essential tool in many speech and audio codecs to\nincrease coding efficiency and perceived quality. The proposed zero-bit\nself-adaptation approach differs from classical forward and backward adaptation\napproaches in that the de-emphasis coefficient is estimated at the receiver,\nfrom the decoded pre-emphasized signal. This eliminates the need to transmit\ninformation that arises from forward adaptation as well as the signal-filter\nlag that is inherent in backward adaptation. Evaluation results show that the\nde-emphasis coefficient can be estimated accurately from the decoded\npre-emphasized signal and that the proposed zero-bit self-adaptation approach\nprovides comparable subjective improvement to forward adaptation.", "published": "2024-07-02 21:23:14", "link": "http://arxiv.org/abs/2407.02672v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Constant Directivity Loudspeaker Beamforming", "abstract": "Loudspeaker array beamforming is a common signal processing technique for\nacoustic directivity control and robust audio reproduction. Unlike their\nmicrophone counterpart, loudspeaker constraints are often heterogeneous due to\narrayed transducers with varying operating ranges in frequency,\nacoustic-electrical sensitivity, efficiency, and directivity. This work\nproposes a frequency-regularization method for generalized Rayleigh quotient\ndirectivity specifications and two novel beamformer designs that optimize for\nmaximum efficiency constant directivity (MECD) and maximum sensitivity constant\ndirectivity (MSCD). We derive fast converging and analytic solutions from their\nquadratic equality constrained quadratic program formulations. Experiments\noptimize generalized directivity index constrained beamformer designs for a\nfull-band heterogeneous array.", "published": "2024-07-02 00:13:07", "link": "http://arxiv.org/abs/2407.01860v3", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model", "abstract": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify\nand locate auditory elements within visual scenes by accurately predicting\nsegmentation masks at the pixel level. Achieving this involves comprehensively\nconsidering data and model aspects to address this task effectively. This study\npresents a lightweight approach, SAVE, which efficiently adapts the pre-trained\nsegment anything model (SAM) to the AVS task. By incorporating an image encoder\nadapter into the transformer blocks to better capture the distinct dataset\ninformation and proposing a residual audio encoder adapter to encode the audio\nfeatures as a sparse prompt, our proposed model achieves effective audio-visual\nfusion and interaction during the encoding stage. Our proposed method\naccelerates the training and inference speed by reducing the input resolution\nfrom 1024 to 256 pixels while achieving higher performance compared with the\nprevious SOTA. Extensive experimentation validates our approach, demonstrating\nthat our proposed model outperforms other SOTA methods significantly. Moreover,\nleveraging the pre-trained model on synthetic data enhances performance on real\nAVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on\nthe MS3 (V1M) set with only 256 pixels for input images. This increases up to\n86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024\npixels.", "published": "2024-07-02 07:22:28", "link": "http://arxiv.org/abs/2407.02004v2", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SOAF: Scene Occlusion-aware Neural Acoustic Field", "abstract": "This paper tackles the problem of novel view audio-visual synthesis along an\narbitrary trajectory in an indoor scene, given the audio-video recordings from\nother known trajectories of the scene. Existing methods often overlook the\neffect of room geometry, particularly wall occlusions on sound propagation,\nmaking them less accurate in multi-room environments. In this work, we propose\na new approach called Scene Occlusion-aware Acoustic Field (SOAF) for accurate\nsound generation. Our approach derives a global prior for the sound field using\ndistance-aware parametric sound-propagation modeling and then transforms it\nbased on the scene structure learned from the input video. We extract features\nfrom the local acoustic field centered at the receiver using a Fibonacci Sphere\nto generate binaural audio for novel views with a direction-aware attention\nmechanism. Extensive experiments on the real dataset RWAVS and the synthetic\ndataset SoundSpaces demonstrate that our method outperforms previous\nstate-of-the-art techniques in audio generation.", "published": "2024-07-02 13:40:56", "link": "http://arxiv.org/abs/2407.02264v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Towards Training Music Taggers on Synthetic Data", "abstract": "Most contemporary music tagging systems rely on large volumes of annotated\ndata. As an alternative, we investigate the extent to which synthetically\ngenerated music excerpts can improve tagging systems when only small annotated\ncollections are available. To this end, we release GTZAN-synth, a synthetic\ndataset that follows the taxonomy of the well-known GTZAN dataset while being\nten times larger in data volume. We first observe that simply adding this\nsynthetic dataset to the training split of GTZAN does not result into\nperformance improvements. We then proceed to investigating domain adaptation,\ntransfer learning and fine-tuning strategies for the task at hand and draw the\nconclusion that the last two options yield an increase in accuracy. Overall,\nthe proposed approach can be considered as a first guide in a promising field\nfor future research.", "published": "2024-07-02 10:54:23", "link": "http://arxiv.org/abs/2407.02156v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS", "I.2"], "primary_category": "cs.SD"}
