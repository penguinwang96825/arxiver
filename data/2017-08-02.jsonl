{"title": "Low-Rank Hidden State Embeddings for Viterbi Sequence Labeling", "abstract": "In textual information extraction and other sequence labeling tasks it is now\ncommon to use recurrent neural networks (such as LSTM) to form rich embedded\nrepresentations of long-term input co-occurrence patterns. Representation of\noutput co-occurrence patterns is typically limited to a hand-designed graphical\nmodel, such as a linear-chain CRF representing short-term Markov dependencies\namong successive labels. This paper presents a method that learns embedded\nrepresentations of latent output structure in sequence data. Our model takes\nthe form of a finite-state machine with a large number of latent states per\nlabel (a latent variable CRF), where the state-transition matrix is\nfactorized---effectively forming an embedded representation of\nstate-transitions capable of enforcing long-term label dependencies, while\nsupporting exact Viterbi inference over output labels. We demonstrate accuracy\nimprovements and interpretable latent structure in a synthetic but complex task\nbased on CoNLL named entity recognition.", "published": "2017-08-02 00:05:10", "link": "http://arxiv.org/abs/1708.00553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Neural MT Search and Model Performance", "abstract": "In this paper, we offer an in-depth analysis about the modeling and search\nperformance. We address the question if a more complex search algorithm is\nnecessary. Furthermore, we investigate the question if more complex models\nwhich might only be applicable during rescoring are promising.\n  By separating the search space and the modeling using $n$-best list\nreranking, we analyze the influence of both parts of an NMT system\nindependently. By comparing differently performing NMT systems, we show that\nthe better translation is already in the search space of the translation\nsystems with less performance. This results indicate that the current search\nalgorithms are sufficient for the NMT systems. Furthermore, we could show that\neven a relatively small $n$-best list of $50$ hypotheses already contain\nnotably better translations.", "published": "2017-08-02 00:48:35", "link": "http://arxiv.org/abs/1708.00563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Data Selection for Neural Machine Translation", "abstract": "Intelligent selection of training data has proven a successful technique to\nsimultaneously increase training efficiency and translation performance for\nphrase-based machine translation (PBMT). With the recent increase in popularity\nof neural machine translation (NMT), we explore in this paper to what extent\nand how NMT can also benefit from data selection. While state-of-the-art data\nselection (Axelrod et al., 2011) consistently performs well for PBMT, we show\nthat gains are substantially lower for NMT. Next, we introduce dynamic data\nselection for NMT, a method in which we vary the selected subset of training\ndata between different training epochs. Our experiments show that the best\nresults are achieved when applying a technique we call gradual fine-tuning,\nwith improvements up to +2.6 BLEU over the original data selection approach and\nup to +3.1 BLEU over a general baseline.", "published": "2017-08-02 11:55:57", "link": "http://arxiv.org/abs/1708.00712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The University of Edinburgh's Neural MT Systems for WMT17", "abstract": "This paper describes the University of Edinburgh's submissions to the WMT17\nshared news translation and biomedical translation tasks. We participated in 12\ntranslation directions for news, translating between English and Czech, German,\nLatvian, Russian, Turkish and Chinese. For the biomedical task we submitted\nsystems for English to Czech, German, Polish and Romanian. Our systems are\nneural machine translation systems trained with Nematus, an attentional\nencoder-decoder. We follow our setup from last year and build BPE-based models\nwith parallel and back-translated monolingual training data. Novelties this\nyear include the use of deep architectures, layer normalization, and more\ncompact models due to weight tying and improvements in BPE segmentations. We\nperform extensive ablative experiments, reporting on the effectivenes of layer\nnormalization, deep architectures, and different ensembling techniques.", "published": "2017-08-02 12:48:32", "link": "http://arxiv.org/abs/1708.00726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Generative and Discriminative Approaches to Unsupervised\n  Dependency Parsing via Dual Decomposition", "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from\nunannotated sentences. Existing work focuses on either learning generative\nmodels using the expectation-maximization algorithm and its variants, or\nlearning discriminative models using the discriminative clustering algorithm.\nIn this paper, we propose a new learning strategy that learns a generative\nmodel and a discriminative model jointly based on the dual decomposition\nmethod. Our method is simple and general, yet effective to capture the\nadvantages of both models and improve their learning results. We tested our\nmethod on the UD treebank and achieved a state-of-the-art performance on thirty\nlanguages.", "published": "2017-08-02 15:10:28", "link": "http://arxiv.org/abs/1708.00790v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Grammar Induction with Neural Lexicalization and Big Training\n  Data", "abstract": "We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.", "published": "2017-08-02 15:43:30", "link": "http://arxiv.org/abs/1708.00801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enterprise to Computer: Star Trek chatbot", "abstract": "Human interactions and human-computer interactions are strongly influenced by\nstyle as well as content. Adding a persona to a chatbot makes it more\nhuman-like and contributes to a better and more engaging user experience. In\nthis work, we propose a design for a chatbot that captures the \"style\" of Star\nTrek by incorporating references from the show along with peculiar tones of the\nfictional characters therein. Our Enterprise to Computer bot (E2Cbot) treats\nStar Trek dialog style and general dialog style differently, using two\nrecurrent neural network Encoder-Decoder models. The Star Trek dialog style\nuses sequence to sequence (SEQ2SEQ) models (Sutskever et al., 2014; Bahdanau et\nal., 2014) trained on Star Trek dialogs. The general dialog style uses Word\nGraph to shift the response of the SEQ2SEQ model into the Star Trek domain. We\nevaluate the bot both in terms of perplexity and word overlap with Star Trek\nvocabulary and subjectively using human evaluators.", "published": "2017-08-02 16:51:01", "link": "http://arxiv.org/abs/1708.00818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Semantic Modeling of Contradictions and Disagreements: A Case\n  Study of Medical Guidelines", "abstract": "We introduce a formal distinction between contradictions and disagreements in\nnatural language texts, motivated by the need to formally reason about\ncontradictory medical guidelines. This is a novel and potentially very useful\ndistinction, and has not been discussed so far in NLP and logic. We also\ndescribe a NLP system capable of automated finding contradictory medical\nguidelines; the system uses a combination of text analysis and information\nretrieval modules. We also report positive evaluation results on a small corpus\nof contradictory medical recommendations.", "published": "2017-08-02 17:54:32", "link": "http://arxiv.org/abs/1708.00850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Aware Neural Dialog System", "abstract": "We investigate the task of building a domain aware chat system which\ngenerates intelligent responses in a conversation comprising of different\ndomains. The domain, in this case, is the topic or theme of the conversation.\nTo achieve this, we present DOM-Seq2Seq, a domain aware neural network model\nbased on the novel technique of using domain-targeted sequence-to-sequence\nmodels (Sutskever et al., 2014) and a domain classifier. The model captures\nfeatures from current utterance and domains of the previous utterances to\nfacilitate the formation of relevant responses. We evaluate our model on\nautomatic metrics and compare our performance with the Seq2Seq model.", "published": "2017-08-02 19:04:52", "link": "http://arxiv.org/abs/1708.00897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization", "abstract": "We propose a new framework for abstractive text summarization based on a\nsequence-to-sequence oriented encoder-decoder model equipped with a deep\nrecurrent generative decoder (DRGN).\n  Latent structure information implied in the target summaries is learned based\non a recurrent latent random model for improving the summarization quality.\n  Neural variational inference is employed to address the intractable posterior\ninference for the recurrent latent variables.\n  Abstractive summaries are generated based on both the generative latent\nvariables and the discriminative deterministic states.\n  Extensive experiments on some benchmark datasets in different languages show\nthat DRGN achieves improvements over the state-of-the-art methods.", "published": "2017-08-02 07:47:14", "link": "http://arxiv.org/abs/1708.00625v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Learning for Inquiry Dialog Policies with Logical\n  Formula Embeddings", "abstract": "This paper is the first attempt to learn the policy of an inquiry dialog\nsystem (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks\nrepresent dialog states and dialog acts with logical formulae. In order to make\nlearning inquiry dialog policies more effective, we introduce a logical formula\nembedding framework based on a recursive neural network. The results of\nexperiments to evaluate the effect of 1) the DRL and 2) the logical formula\nembedding framework show that the combination of the two are as effective or\neven better than existing rule-based methods for inquiry dialog policies.", "published": "2017-08-02 09:40:42", "link": "http://arxiv.org/abs/1708.00667v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dynamic Entity Representations in Neural Language Models", "abstract": "Understanding a long document requires tracking how entities are introduced\nand evolve over time. We present a new type of language model, EntityNLM, that\ncan explicitly model entities, dynamically update their representations, and\ncontextually generate their mentions. Our model is generative and flexible; it\ncan model an arbitrary number of entities in context while generating each\nentity mention at an arbitrary length. In addition, it can be used for several\ndifferent tasks such as language modeling, coreference resolution, and entity\nprediction. Experimental results with all these tasks demonstrate that our\nmodel consistently outperforms strong baselines and prior work.", "published": "2017-08-02 14:49:03", "link": "http://arxiv.org/abs/1708.00781v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
