{"title": "Neural Deepfake Detection with Factual Structure of Text", "abstract": "Deepfake detection, the task of automatically discriminating\nmachine-generated text, is increasingly critical with recent advances in\nnatural language generative models. Existing approaches to deepfake detection\ntypically represent documents with coarse-grained representations. However,\nthey struggle to capture factual structures of documents, which is a\ndiscriminative factor between machine-generated and human-written text\naccording to our statistical analysis. To address this, we propose a\ngraph-based model that utilizes the factual structure of a document for\ndeepfake detection of text. Our approach represents the factual structure of a\ngiven document as an entity graph, which is further utilized to learn sentence\nrepresentations with a graph neural network. Sentence representations are then\ncomposed to a document representation for making predictions, where consistent\nrelations between neighboring sentences are sequentially modeled. Results of\nexperiments on two public deepfake datasets show that our approach\nsignificantly improves strong base models built with RoBERTa. Model analysis\nfurther indicates that our model can distinguish the difference in the factual\nstructure between machine-generated text and human-written text.", "published": "2020-10-15 02:35:31", "link": "http://arxiv.org/abs/2010.07475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Cross-Lingual Abstractive Summarization", "abstract": "We present a multi-task learning framework for cross-lingual abstractive\nsummarization to augment training data. Recent studies constructed pseudo\ncross-lingual abstractive summarization data to train their neural\nencoder-decoders. Meanwhile, we introduce existing genuine data such as\ntranslation pairs and monolingual abstractive summarization data into training.\nOur proposed method, Transum, attaches a special token to the beginning of the\ninput sentence to indicate the target task. The special token enables us to\nincorporate the genuine data into the training data easily. The experimental\nresults show that Transum achieves better performance than the model trained\nwith only pseudo cross-lingual summarization data. In addition, we achieve the\ntop ROUGE score on Chinese-English and Arabic-English abstractive\nsummarization. Moreover, Transum also has a positive effect on machine\ntranslation. Experimental results indicate that Transum improves the\nperformance from the strong baseline, Transformer, in Chinese-English,\nArabic-English, and English-Japanese translation datasets.", "published": "2020-10-15 04:03:00", "link": "http://arxiv.org/abs/2010.07503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RNNs can generate bounded hierarchical languages with optimal memory", "abstract": "Recurrent neural networks empirically generate natural language with high\nsyntactic fidelity. However, their success is not well-understood\ntheoretically. We provide theoretical insight into this success, proving in a\nfinite-precision setting that RNNs can efficiently generate bounded\nhierarchical languages that reflect the scaffolding of natural language syntax.\nWe introduce Dyck-($k$,$m$), the language of well-nested brackets (of $k$\ntypes) and $m$-bounded nesting depth, reflecting the bounded memory needs and\nlong-distance dependencies of natural language syntax. The best known results\nuse $O(k^{\\frac{m}{2}})$ memory (hidden units) to generate these languages. We\nprove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential\nreduction in memory, by an explicit construction. Finally, we show that no\nalgorithm, even with unbounded computation, can suffice with $o(m \\log k)$\nhidden units.", "published": "2020-10-15 04:42:29", "link": "http://arxiv.org/abs/2010.07515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition and Relation Extraction using Enhanced Table\n  Filling by Contextualized Representations", "abstract": "In this study, a novel method for extracting named entities and relations\nfrom unstructured text based on the table representation is presented. By using\ncontextualized word embeddings, the proposed method computes representations\nfor entity mentions and long-range dependencies without complicated\nhand-crafted features or neural-network architectures. We also adapt a tensor\ndot-product to predict relation labels all at once without resorting to\nhistory-based predictions or search strategies. These advances significantly\nsimplify the model and algorithm for the extraction of named entities and\nrelations. Despite its simplicity, the experimental results demonstrate that\nthe proposed method outperforms the state-of-the-art methods on the CoNLL04 and\nACE05 English datasets. We also confirm that the proposed method achieves a\ncomparable performance with the state-of-the-art NER models on the ACE05\ndatasets when multiple sentences are provided for context aggregation.", "published": "2020-10-15 04:58:23", "link": "http://arxiv.org/abs/2010.07522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Constituency Parsing with Span Attention", "abstract": "Constituency parsing is a fundamental and important task for natural language\nunderstanding, where a good representation of contextual information can help\nthis task. N-grams, which is a conventional type of feature for contextual\ninformation, have been demonstrated to be useful in many tasks, and thus could\nalso be beneficial for constituency parsing if they are appropriately modeled.\nIn this paper, we propose span attention for neural chart-based constituency\nparsing to leverage n-gram information. Considering that current chart-based\nparsers with Transformer-based encoder represent spans by subtraction of the\nhidden states at the span boundaries, which may cause information loss\nespecially for long spans, we incorporate n-grams into span representations by\nweighting them according to their contributions to the parsing process.\nMoreover, we propose categorical span attention to further enhance the model by\nweighting n-grams within different length categories, and thus benefit\nlong-sentence parsing. Experimental results on three widely used benchmark\ndatasets demonstrate the effectiveness of our approach in parsing Arabic,\nChinese, and English, where state-of-the-art performance is obtained by our\napproach on all of them.", "published": "2020-10-15 06:36:39", "link": "http://arxiv.org/abs/2010.07543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token Sequence Labeling vs. Clause Classification for English Emotion\n  Stimulus Detection", "abstract": "Emotion stimulus detection is the task of finding the cause of an emotion in\na textual description, similar to target or aspect detection for sentiment\nanalysis. Previous work approached this in three ways, namely (1) as text\nclassification into an inventory of predefined possible stimuli (\"Is the\nstimulus category A or B?\"), (2) as sequence labeling of tokens (\"Which tokens\ndescribe the stimulus?\"), and (3) as clause classification (\"Does this clause\ncontain the emotion stimulus?\"). So far, setting (3) has been evaluated broadly\non Mandarin and (2) on English, but no comparison has been performed.\nTherefore, we aim to answer whether clause classification or sequence labeling\nis better suited for emotion stimulus detection in English. To accomplish that,\nwe propose an integrated framework which enables us to evaluate the two\ndifferent approaches comparably, implement models inspired by state-of-the-art\napproaches in Mandarin, and test them on four English data sets from different\ndomains. Our results show that sequence labeling is superior on three out of\nfour datasets, in both clause-based and sequence-based evaluation. The only\ncase in which clause classification performs better is one data set with a high\ndensity of clause annotations. Our error analysis further confirms\nquantitatively and qualitatively that clauses are not the appropriate stimulus\nunit in English.", "published": "2020-10-15 07:11:04", "link": "http://arxiv.org/abs/2010.07557v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction in Low Error Density Domains: A New\n  Benchmark and Analyses", "abstract": "Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.", "published": "2020-10-15 07:52:01", "link": "http://arxiv.org/abs/2010.07574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretrained Language Models for Dialogue Generation with Multiple Input\n  Sources", "abstract": "Large-scale pretrained language models have achieved outstanding performance\non natural language understanding tasks. However, it is still under\ninvestigating how to apply them to dialogue generation tasks, especially those\nwith responses conditioned on multiple sources. Previous work simply\nconcatenates all input sources or averages information from different input\nsources. In this work, we study dialogue models with multiple input sources\nadapted from the pretrained language model GPT2. We explore various methods to\nfuse multiple separate attention information corresponding to different\nsources. Our experimental results show that proper fusion methods deliver\nhigher relevance with dialogue history than simple fusion baselines.", "published": "2020-10-15 07:53:28", "link": "http://arxiv.org/abs/2010.07576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Better Representation for Tables by Self-Supervised Tasks", "abstract": "Table-to-text generation aims at automatically generating natural text to\nhelp people to conveniently obtain the important information in tables.\nAlthough neural models for table-to-text have achieved remarkable progress,\nsome problems still overlooked. The first is that the values recorded in many\ntables are mostly numbers in practice. The existing approaches do not do\nspecial treatment for these, and still regard these as words in natural\nlanguage text. Secondly, the target texts in training dataset may contain\nredundant information or facts do not exist in the input tables. These may give\nwrong supervision signals to some methods based on content selection and\nplanning and auxiliary supervision. To solve these problems, we propose two\nself-supervised tasks, Number Ordering and Significance Ordering, to help to\nlearn better table representation. The former works on the column dimension to\nhelp to incorporate the size property of numbers into table representation. The\nlatter acts on row dimension and help to learn a significance-aware table\nrepresentation. We test our methods on the widely used dataset ROTOWIRE which\nconsists of NBA game statistic and related news. The experimental results\ndemonstrate that the model trained together with these two self-supervised\ntasks can generate text that contains more salient and well-organized facts,\neven without modeling context selection and planning. And we achieve the\nstate-of-the-art performance on automatic metrics.", "published": "2020-10-15 09:03:38", "link": "http://arxiv.org/abs/2010.07606v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses", "abstract": "Popular Neural Machine Translation model training uses strategies like\nbacktranslation to improve BLEU scores, requiring large amounts of additional\ndata and training. We introduce a class of conditional\ngenerative-discriminative hybrid losses that we use to fine-tune a trained\nmachine translation model. Through a combination of targeted fine-tuning\nobjectives and intuitive re-use of the training data the model has failed to\nadequately learn from, we improve the model performance of both a\nsentence-level and a contextual model without using any additional data. We\ntarget the improvement of pronoun translations through our fine-tuning and\nevaluate our models on a pronoun benchmark testset. Our sentence-level model\nshows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets,\nwhile our contextual model achieves the best results, improving from 31.81 to\n32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En\ntestset, with corresponding improvements in pronoun translation. We further\nshow the generalizability of our method by reproducing the improvements on two\nadditional language pairs, Fr-En and Cs-En. Code available at\n<https://github.com/ntunlp/pronoun-finetuning>.", "published": "2020-10-15 10:11:40", "link": "http://arxiv.org/abs/2010.07638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse Keyphrase Generation with Neural Unlikelihood Training", "abstract": "In this paper, we study sequence-to-sequence (S2S) keyphrase generation\nmodels from the perspective of diversity. Recent advances in neural natural\nlanguage generation have made possible remarkable progress on the task of\nkeyphrase generation, demonstrated through improvements on quality metrics such\nas F1-score. However, the importance of diversity in keyphrase generation has\nbeen largely ignored. We first analyze the extent of information redundancy\npresent in the outputs generated by a baseline model trained using maximum\nlikelihood estimation (MLE). Our findings show that repetition of keyphrases is\na major issue with MLE training. To alleviate this issue, we adopt neural\nunlikelihood (UL) objective for training the S2S model. Our version of UL\ntraining operates at (1) the target token level to discourage the generation of\nrepeating tokens; (2) the copy token level to avoid copying repetitive tokens\nfrom the source text. Further, to encourage better model planning during the\ndecoding process, we incorporate K-step ahead token prediction objective that\ncomputes both MLE and UL losses on future tokens as well. Through extensive\nexperiments on datasets from three different domains we demonstrate that the\nproposed approach attains considerably large diversity gains, while maintaining\ncompetitive output quality.", "published": "2020-10-15 11:12:26", "link": "http://arxiv.org/abs/2010.07665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Chinese BERT Encode Word Structure?", "abstract": "Contextualized representations give significantly improved results for a wide\nrange of NLP tasks. Much work has been dedicated to analyzing the features\ncaptured by representative models such as BERT. Existing work finds that\nsyntactic, semantic and word sense knowledge are encoded in BERT. However,\nlittle work has investigated word features for character-based languages such\nas Chinese. We investigate Chinese BERT using both attention weight\ndistribution statistics and probing tasks, finding that (1) word information is\ncaptured by BERT; (2) word-level features are mostly in the middle\nrepresentation layers; (3) downstream tasks make different use of word features\nin BERT, with POS tagging and chunking relying the most on word features, and\nnatural language inference relying the least on such features.", "published": "2020-10-15 12:40:56", "link": "http://arxiv.org/abs/2010.07711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization Repair in the Presence of Spelling Errors", "abstract": "We consider the following tokenization repair problem: Given a natural\nlanguage text with any combination of missing or spurious spaces, correct\nthese. Spelling errors can be present, but it's not part of the problem to\ncorrect them. For example, given: \"Tispa per isabout token izaionrep air\",\ncompute \"Tis paper is about tokenizaion repair\". We identify three key\ningredients of high-quality tokenization repair, all missing from previous\nwork: deep language models with a bidirectional component, training the models\non text with spelling errors, and making use of the space information already\npresent. Our methods also improve existing spell checkers by fixing not only\nmore tokenization errors but also more spelling errors: once it is clear which\ncharacters form a word, it is much easier for them to figure out the correct\nword. We provide six benchmarks that cover three use cases (OCR errors, text\nextraction from PDF, human errors) and the cases of partially correct space\ninformation and all spaces missing. We evaluate our methods against the best\nexisting methods and a non-trivial baseline. We provide full reproducibility\nunder https://ad.cs.uni-freiburg.de/publications .", "published": "2020-10-15 16:55:45", "link": "http://arxiv.org/abs/2010.07878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Neural Abstractive Summarization Models via Uncertainty", "abstract": "An advantage of seq2seq abstractive summarization models is that they\ngenerate text in a free-form manner, but this flexibility makes it difficult to\ninterpret model behavior. In this work, we analyze summarization decoders in\nboth blackbox and whitebox ways by studying on the entropy, or uncertainty, of\nthe model's token-level predictions. For two strong pre-trained models, PEGASUS\nand BART on two summarization datasets, we find a strong correlation between\nlow prediction entropy and where the model copies tokens rather than generating\nnovel text. The decoder's uncertainty also connects to factors like sentence\nposition and syntactic distance between adjacent pairs of tokens, giving a\nsense of what factors make a context particularly selective for the model's\nnext output token. Finally, we study the relationship of decoder uncertainty\nand attention behavior to understand how attention gives rise to these observed\neffects in the model. We show that uncertainty is a useful perspective for\nanalyzing summarization and text generation models more broadly.", "published": "2020-10-15 16:57:27", "link": "http://arxiv.org/abs/2010.07882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressive Summarization with Plausibility and Salience Modeling", "abstract": "Compressive summarization systems typically rely on a crafted set of\nsyntactic rules to determine what spans of possible summary sentences can be\ndeleted, then learn a model of what to actually delete by optimizing for\ncontent selection (ROUGE). In this work, we propose to relax the rigid\nsyntactic constraints on candidate spans and instead leave compression\ndecisions to two data-driven criteria: plausibility and salience. Deleting a\nspan is plausible if removing it maintains the grammaticality and factuality of\na sentence, and spans are salient if they contain important information from\nthe summary. Each of these is judged by a pre-trained Transformer model, and\nonly deletions that are both plausible and not salient can be applied. When\nintegrated into a simple extraction-compression pipeline, our method achieves\nstrong in-domain results on benchmark summarization datasets, and human\nevaluation shows that the plausibility model generally selects for grammatical\nand factual deletions. Furthermore, the flexibility of our approach allows it\nto generalize cross-domain: our system fine-tuned on only 500 samples from a\nnew domain can match or exceed an in-domain extractive model trained on much\nmore data.", "published": "2020-10-15 17:07:10", "link": "http://arxiv.org/abs/2010.07886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets --\n  RoBERTa Ensembles and The Continued Relevance of Handcrafted Features", "abstract": "This paper presents our submission to Task 2 of the Workshop on Noisy\nUser-generated Text. We explore improving the performance of a pre-trained\ntransformer-based language model fine-tuned for text classification through an\nensemble implementation that makes use of corpus level information and a\nhandcrafted feature. We test the effectiveness of including the aforementioned\nfeatures in accommodating the challenges of a noisy data set centred on a\nspecific subject outside the remit of the pre-training data. We show that\ninclusion of additional features can improve classification results and achieve\na score within 2 points of the top performing team.", "published": "2020-10-15 19:12:52", "link": "http://arxiv.org/abs/2010.07988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GSum: A General Framework for Guided Neural Abstractive Summarization", "abstract": "Neural abstractive summarization models are flexible and can produce coherent\nsummaries, but they are sometimes unfaithful and can be difficult to control.\nWhile previous studies attempt to provide different types of guidance to\ncontrol the output and increase faithfulness, it is not clear how these\nstrategies compare and contrast to each other. In this paper, we propose a\ngeneral and extensible guided summarization framework (GSum) that can\neffectively take different kinds of external guidance as input, and we perform\nexperiments across several different varieties. Experiments demonstrate that\nthis model is effective, achieving state-of-the-art performance according to\nROUGE on 4 popular summarization datasets when using highlighted sentences as\nguidance. In addition, we show that our guided model can generate more faithful\nsummaries and demonstrate how different types of guidance generate\nqualitatively different summaries, lending a degree of controllability to the\nlearned models.", "published": "2020-10-15 20:46:14", "link": "http://arxiv.org/abs/2010.08014v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Montague Grammar Induction", "abstract": "We propose a computational modeling framework for inducing combinatory\ncategorial grammars from arbitrary behavioral data. This framework provides the\nanalyst fine-grained control over the assumptions that the induced grammar\nshould conform to: (i) what the primitive types are; (ii) how complex types are\nconstructed; (iii) what set of combinators can be used to combine types; and\n(iv) whether (and to what) the types of some lexical items should be fixed. In\na proof-of-concept experiment, we deploy our framework for use in\ndistributional analysis. We focus on the relationship between\ns(emantic)-selection and c(ategory)-selection, using as input a lexicon-scale\nacceptability judgment dataset focused on English verbs' syntactic distribution\n(the MegaAcceptability dataset) and enforcing standard assumptions from the\nsemantics literature on the induced grammar.", "published": "2020-10-15 23:25:01", "link": "http://arxiv.org/abs/2010.08067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Label Smoothing for Sequence to Sequence Problems", "abstract": "Label smoothing has been shown to be an effective regularization strategy in\nclassification, that prevents overfitting and helps in label de-noising.\nHowever, extending such methods directly to seq2seq settings, such as Machine\nTranslation, is challenging: the large target output space of such problems\nmakes it intractable to apply label smoothing over all possible outputs. Most\nexisting approaches for seq2seq settings either do token level smoothing, or\nsmooth over sequences generated by randomly substituting tokens in the target\nsequence. Unlike these works, in this paper, we propose a technique that\nsmooths over \\emph{well formed} relevant sequences that not only have\nsufficient n-gram overlap with the target sequence, but are also\n\\emph{semantically similar}. Our method shows a consistent and significant\nimprovement over the state-of-the-art techniques on different datasets.", "published": "2020-10-15 00:31:15", "link": "http://arxiv.org/abs/2010.07447v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Language of Food during the Pandemic: Hints about the Dietary\n  Effects of Covid-19", "abstract": "We study the language of food on Twitter during the pandemic lockdown in the\nUnited States, focusing on the two month period of March 15 to May 15, 2020.\nSpecifically, we analyze over770,000 tweets published during the lockdown and\nthe equivalent period in the five previous years and highlight several worrying\ntrends. First, we observe that during the lockdown there was a notable shift\nfrom mentions of healthy foods to unhealthy foods. Second, we show an increased\npointwise mutual information of depression hashtags with food-related tweets\nposted during the lockdown and an increased association between depression\nhashtags and unhealthy foods, tobacco, and alcohol during the lockdown.", "published": "2020-10-15 01:33:05", "link": "http://arxiv.org/abs/2010.07466v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MedDG: An Entity-Centric Medical Consultation Dataset for Entity-Aware\n  Medical Dialogue Generation", "abstract": "Developing conversational agents to interact with patients and provide\nprimary clinical advice has attracted increasing attention due to its huge\napplication potential, especially in the time of COVID-19 Pandemic. However,\nthe training of end-to-end neural-based medical dialogue system is restricted\nby an insufficient quantity of medical dialogue corpus. In this work, we make\nthe first attempt to build and release a large-scale high-quality Medical\nDialogue dataset related to 12 types of common Gastrointestinal diseases named\nMedDG, with more than 17K conversations collected from the online health\nconsultation community. Five different categories of entities, including\ndiseases, symptoms, attributes, tests, and medicines, are annotated in each\nconversation of MedDG as additional labels. To push forward the future research\non building expert-sensitive medical dialogue system, we proposes two kinds of\nmedical dialogue tasks based on MedDG dataset. One is the next entity\nprediction and the other is the doctor response generation. To acquire a clear\ncomprehension on these two medical dialogue tasks, we implement several\nstate-of-the-art benchmarks, as well as design two dialogue models with a\nfurther consideration on the predicted entities. Experimental results show that\nthe pre-train language models and other baselines struggle on both tasks with\npoor performance in our dataset, and the response quality can be enhanced with\nthe help of auxiliary entity information. From human evaluation, the simple\nretrieval model outperforms several state-of-the-art generative models,\nindicating that there still remains a large room for improvement on generating\nmedically meaningful responses.", "published": "2020-10-15 03:34:33", "link": "http://arxiv.org/abs/2010.07497v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.", "published": "2020-10-15 05:01:20", "link": "http://arxiv.org/abs/2010.07523v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language Rationales with Full-Stack Visual Reasoning: From\n  Pixels to Semantic Frames to Commonsense Graphs", "abstract": "Natural language rationales could provide intuitive, higher-level\nexplanations that are easily understandable by humans, complementing the more\nbroadly studied lower-level explanations based on gradients or attention\nweights. We present the first study focused on generating natural language\nrationales across several complex visual reasoning tasks: visual commonsense\nreasoning, visual-textual entailment, and visual question answering. The key\nchallenge of accurate rationalization is comprehensive image understanding at\nall levels: not just their explicit content at the pixel level, but their\ncontextual contents at the semantic and pragmatic levels. We present\nRationale^VT Transformer, an integrated model that learns to generate free-text\nrationales by combining pretrained language models with object recognition,\ngrounded visual semantic frames, and visual commonsense graphs. Our experiments\nshow that the base pretrained language model benefits from visual adaptation\nand that free-text rationalization is a promising research direction to\ncomplement model interpretability for complex visual-textual reasoning tasks.", "published": "2020-10-15 05:08:56", "link": "http://arxiv.org/abs/2010.07526v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GMH: A General Multi-hop Reasoning Model for KG Completion", "abstract": "Knowledge graphs are essential for numerous downstream natural language\nprocessing applications, but are typically incomplete with many facts missing.\nThis results in research efforts on multi-hop reasoning task, which can be\nformulated as a search process and current models typically perform short\ndistance reasoning. However, the long-distance reasoning is also vital with the\nability to connect the superficially unrelated entities. To the best of our\nknowledge, there lacks a general framework that approaches multi-hop reasoning\nin mixed long-short distance reasoning scenarios. We argue that there are two\nkey issues for a general multi-hop reasoning model: i) where to go, and ii)\nwhen to stop. Therefore, we propose a general model which resolves the issues\nwith three modules: 1) the local-global knowledge module to estimate the\npossible paths, 2) the differentiated action dropout module to explore a\ndiverse set of paths, and 3) the adaptive stopping search module to avoid over\nsearching. The comprehensive results on three datasets demonstrate the\nsuperiority of our model with significant improvements against baselines in\nboth short and long distance reasoning scenarios.", "published": "2020-10-15 09:30:46", "link": "http://arxiv.org/abs/2010.07620v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DialogueTRM: Exploring the Intra- and Inter-Modal Emotional Behaviors in\n  the Conversation", "abstract": "Emotion Recognition in Conversations (ERC) is essential for building\nempathetic human-machine systems. Existing studies on ERC primarily focus on\nsummarizing the context information in a conversation, however, ignoring the\ndifferentiated emotional behaviors within and across different modalities.\nDesigning appropriate strategies that fit the differentiated multi-modal\nemotional behaviors can produce more accurate emotional predictions. Thus, we\npropose the DialogueTransformer to explore the differentiated emotional\nbehaviors from the intra- and inter-modal perspectives. For intra-modal, we\nconstruct a novel Hierarchical Transformer that can easily switch between\nsequential and feed-forward structures according to the differentiated context\npreference within each modality. For inter-modal, we constitute a novel\nMulti-Grained Interactive Fusion that applies both neuron- and vector-grained\nfeature interactions to learn the differentiated contributions across all\nmodalities. Experimental results show that DialogueTRM outperforms the\nstate-of-the-art by a significant margin on three benchmark datasets.", "published": "2020-10-15 10:10:41", "link": "http://arxiv.org/abs/2010.07637v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Inducing Alignment Structure with Gated Graph Attention Networks for\n  Sentence Matching", "abstract": "Sentence matching is a fundamental task of natural language processing with\nvarious applications. Most recent approaches adopt attention-based neural\nmodels to build word- or phrase-level alignment between two sentences. However,\nthese models usually ignore the inherent structure within the sentences and\nfail to consider various dependency relationships among text units. To address\nthese issues, this paper proposes a graph-based approach for sentence matching.\nFirst, we represent a sentence pair as a graph with several carefully design\nstrategies. We then employ a novel gated graph attention network to encode the\nconstructed graph for sentence matching. Experimental results demonstrate that\nour method substantially achieves state-of-the-art performance on two datasets\nacross tasks of natural language and paraphrase identification. Further\ndiscussions show that our model can learn meaningful graph structure,\nindicating its superiority on improved interpretability.", "published": "2020-10-15 11:25:54", "link": "http://arxiv.org/abs/2010.07668v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reliable Evaluations for Natural Language Inference based on a Unified\n  Cross-dataset Benchmark", "abstract": "Recent studies show that crowd-sourced Natural Language Inference (NLI)\ndatasets may suffer from significant biases like annotation artifacts. Models\nutilizing these superficial clues gain mirage advantages on the in-domain\ntesting set, which makes the evaluation results over-estimated. The lack of\ntrustworthy evaluation settings and benchmarks stalls the progress of NLI\nresearch. In this paper, we propose to assess a model's trustworthy\ngeneralization performance with cross-datasets evaluation. We present a new\nunified cross-datasets benchmark with 14 NLI datasets, and re-evaluate 9\nwidely-used neural network-based NLI models as well as 5 recently proposed\ndebiasing methods for annotation artifacts. Our proposed evaluation scheme and\nexperimental baselines could provide a basis to inspire future reliable NLI\nresearch.", "published": "2020-10-15 11:50:12", "link": "http://arxiv.org/abs/2010.07676v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wasserstein Distance Regularized Sequence Representation for Text\n  Matching in Asymmetrical Domains", "abstract": "One approach to matching texts from asymmetrical domains is projecting the\ninput sequences into a common semantic space as feature vectors upon which the\nmatching function can be readily defined and learned. In real-world matching\npractices, it is often observed that with the training goes on, the feature\nvectors projected from different domains tend to be indistinguishable. The\nphenomenon, however, is often overlooked in existing matching models. As a\nresult, the feature vectors are constructed without any regularization, which\ninevitably increases the difficulty of learning the downstream matching\nfunctions. In this paper, we propose a novel match method tailored for text\nmatching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein\ndistance-based regularizer is defined to regularize the features vectors\nprojected from different domains. As a result, the method enforces the feature\nprojection function to generate vectors such that those correspond to different\ndomains cannot be easily discriminated. The training process of WD-Match\namounts to a game that minimizes the matching loss regularized by the\nWasserstein distance. WD-Match can be used to improve different text matching\nmethods, by using the method as its underlying matching model. Four popular\ntext matching methods have been exploited in the paper. Experimental results\nbased on four publicly available benchmarks showed that WD-Match consistently\noutperformed the underlying methods and the baselines.", "published": "2020-10-15 12:52:09", "link": "http://arxiv.org/abs/2010.07717v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unsupervised Bitext Mining and Translation via Self-trained Contextual\n  Embeddings", "abstract": "We describe an unsupervised method to create pseudo-parallel corpora for\nmachine translation (MT) from unaligned text. We use multilingual BERT to\ncreate source and target sentence embeddings for nearest-neighbor search and\nadapt the model via self-training. We validate our technique by extracting\nparallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a\n24.5 point increase (absolute) in F1 scores over previous unsupervised methods.\nWe then improve an XLM-based unsupervised neural MT system pre-trained on\nWikipedia by supplementing it with pseudo-parallel text mined from the same\ncorpus, boosting unsupervised translation performance by up to 3.5 BLEU on the\nWMT'14 French-English and WMT'16 German-English tasks and outperforming the\nprevious state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese\ncorpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU\nimprovement on the low-resource MT task. We demonstrate that unsupervised\nbitext mining is an effective way of augmenting MT datasets and complements\nexisting techniques like initializing with pre-trained contextual embeddings.", "published": "2020-10-15 14:04:03", "link": "http://arxiv.org/abs/2010.07761v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Response Selection for Multi-Party Conversations with Dynamic Topic\n  Tracking", "abstract": "While participants in a multi-party multi-turn conversation simultaneously\nengage in multiple conversation topics, existing response selection methods are\ndeveloped mainly focusing on a two-party single-conversation scenario. Hence,\nthe prolongation and transition of conversation topics are ignored by current\nmethods. In this work, we frame response selection as a dynamic topic tracking\ntask to match the topic between the response and relevant conversation context.\nWith this new formulation, we propose a novel multi-task learning framework\nthat supports efficient encoding through large pretrained models with only two\nutterances at once to perform dynamic topic disentanglement and response\nselection. We also propose Topic-BERT an essential pretraining step to embed\ntopic information into BERT with self-supervised learning. Experimental results\non the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response\nselection and topic disentanglement tasks outperforming existing methods by a\ngood margin.", "published": "2020-10-15 14:21:38", "link": "http://arxiv.org/abs/2010.07785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Poset Decoding for Compositional Generalization in Language", "abstract": "We formalize human language understanding as a structured prediction task\nwhere the output is a partially ordered set (poset). Current encoder-decoder\narchitectures do not take the poset structure of semantics into account\nproperly, thus suffering from poor compositional generalization ability. In\nthis paper, we propose a novel hierarchical poset decoding paradigm for\ncompositional generalization in language. Intuitively: (1) the proposed\nparadigm enforces partial permutation invariance in semantics, thus avoiding\noverfitting to bias ordering information; (2) the hierarchical mechanism allows\nto capture high-level structures of posets. We evaluate our proposed decoder on\nCompositional Freebase Questions (CFQ), a large and realistic natural language\nquestion answering dataset that is specifically designed to measure\ncompositional generalization. Results show that it outperforms current\ndecoders.", "published": "2020-10-15 14:34:26", "link": "http://arxiv.org/abs/2010.07792v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Where's the Question? A Multi-channel Deep Convolutional Neural Network\n  for Question Identification in Textual Data", "abstract": "In most clinical practice settings, there is no rigorous reviewing of the\nclinical documentation, resulting in inaccurate information captured in the\npatient medical records. The gold standard in clinical data capturing is\nachieved via \"expert-review\", where clinicians can have a dialogue with a\ndomain expert (reviewers) and ask them questions about data entry rules.\nAutomatically identifying \"real questions\" in these dialogues could uncover\nambiguities or common problems in data capturing in a given clinical setting.\n  In this study, we proposed a novel multi-channel deep convolutional neural\nnetwork architecture, namely Quest-CNN, for the purpose of separating real\nquestions that expect an answer (information or help) about an issue from\nsentences that are not questions, as well as from questions referring to an\nissue mentioned in a nearby sentence (e.g., can you clarify this?), which we\nwill refer as \"c-questions\". We conducted a comprehensive performance\ncomparison analysis of the proposed multi-channel deep convolutional neural\nnetwork against other deep neural networks. Furthermore, we evaluated the\nperformance of traditional rule-based and learning-based methods for detecting\nquestion sentences. The proposed Quest-CNN achieved the best F1 score both on a\ndataset of data entry-review dialogue in a dialysis care setting, and on a\ngeneral domain dataset.", "published": "2020-10-15 15:11:22", "link": "http://arxiv.org/abs/2010.07816v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A\n  Contrastive-Regularized Self-Training Approach", "abstract": "Fine-tuned pre-trained language models (LMs) have achieved enormous success\nin many natural language processing (NLP) tasks, but they still require\nexcessive labeled data in the fine-tuning stage. We study the problem of\nfine-tuning pre-trained LMs using only weak supervision, without any labeled\ndata. This problem is challenging because the high capacity of LMs makes them\nprone to overfitting the noisy labels generated by weak supervision. To address\nthis problem, we develop a contrastive self-training framework, COSINE, to\nenable fine-tuning LMs with weak supervision. Underpinned by contrastive\nregularization and confidence-based reweighting, this contrastive self-training\nframework can gradually improve model fitting while effectively suppressing\nerror propagation. Experiments on sequence, token, and sentence pair\nclassification tasks show that our model outperforms the strongest baseline by\nlarge margins on 7 benchmarks in 6 tasks, and achieves competitive performance\nwith fully-supervised fine-tuning methods.", "published": "2020-10-15 15:55:08", "link": "http://arxiv.org/abs/2010.07835v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Update Frequently, Update Fast: Retraining Semantic Parsing Systems in a\n  Fraction of Time", "abstract": "Currently used semantic parsing systems deployed in voice assistants can\nrequire weeks to train. Datasets for these models often receive small and\nfrequent updates, data patches. Each patch requires training a new model. To\nreduce training time, one can fine-tune the previously trained model on each\npatch, but naive fine-tuning exhibits catastrophic forgetting - degradation of\nthe model performance on the data not represented in the data patch. In this\nwork, we propose a simple method that alleviates catastrophic forgetting and\nshow that it is possible to match the performance of a model trained from\nscratch in less than 10% of a time via fine-tuning. The key to achieving this\nis supersampling and EWC regularization. We demonstrate the effectiveness of\nour method on multiple splits of the Facebook TOP and SNIPS datasets.", "published": "2020-10-15 16:37:41", "link": "http://arxiv.org/abs/2010.07865v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Natural Language Processing Tasks with Human Gaze-Guided\n  Neural Attention", "abstract": "A lack of corpora has so far limited advances in integrating human gaze data\nas a supervisory signal in neural attention mechanisms for natural language\nprocessing(NLP). We propose a novel hybrid text saliency model(TSM) that, for\nthe first time, combines a cognitive model of reading with explicit human gaze\nsupervision in a single machine learning framework. On four different corpora\nwe demonstrate that our hybrid TSM duration predictions are highly correlated\nwith human gaze ground truth. We further propose a novel joint modeling\napproach to integrate TSM predictions into the attention layer of a network\ndesigned for a specific upstream NLP task without the need for any\ntask-specific human gaze data. We demonstrate that our joint model outperforms\nthe state of the art in paraphrase generation on the Quora Question Pairs\ncorpus by more than 10% in BLEU-4 and achieves state of the art performance for\nsentence compression on the challenging Google Sentence Compression corpus. As\nsuch, our work introduces a practical approach for bridging between data-driven\nand cognitive models and demonstrates a new way to integrate human gaze-guided\nneural attention into NLP tasks.", "published": "2020-10-15 17:14:09", "link": "http://arxiv.org/abs/2010.07891v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explicit Alignment Objectives for Multilingual Bidirectional Encoders", "abstract": "Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and\nXLMR (Conneau et al., 2020) have proven to be impressively effective at\nenabling transfer-learning of NLP systems from high-resource languages to\nlow-resource languages. This success comes despite the fact that there is no\nexplicit objective to align the contextual embeddings of words/sentences with\nsimilar meanings across languages together in the same space. In this paper, we\npresent a new method for learning multilingual encoders, AMBER (Aligned\nMultilingual Bidirectional EncodeR). AMBER is trained on additional parallel\ndata using two explicit alignment objectives that align the multilingual\nrepresentations at different granularities. We conduct experiments on zero-shot\ncross-lingual transfer learning for different tasks including sequence tagging,\nsentence retrieval and sentence classification. Experimental results show that\nAMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to\n27.3 average accuracy on retrieval over the XLMR-large model which has 3.2x the\nparameters of AMBER. Our code and models are available at\nhttp://github.com/junjiehu/amber.", "published": "2020-10-15 18:34:13", "link": "http://arxiv.org/abs/2010.07972v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TopicBERT for Energy Efficient Document Classification", "abstract": "Prior research notes that BERT's computational cost grows quadratically with\nsequence length thus leading to longer training times, higher GPU memory\nconstraints and carbon emissions. While recent work seeks to address these\nscalability issues at pre-training, these issues are also prominent in\nfine-tuning especially for long sequence tasks like document classification.\nOur work thus focuses on optimizing the computational cost of fine-tuning for\ndocument classification. We achieve this by complementary learning of both\ntopic and language models in a unified framework, named TopicBERT. This\nsignificantly reduces the number of self-attention operations - a main\nperformance bottleneck. Consequently, our model achieves a 1.4x ($\\sim40\\%$)\nspeedup with $\\sim40\\%$ reduction in $CO_2$ emission while retaining $99.9\\%$\nperformance over 5 datasets.", "published": "2020-10-15 00:56:54", "link": "http://arxiv.org/abs/2010.16407v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Languages with Decidable Hypotheses", "abstract": "In language learning in the limit, the most common type of hypothesis is to\ngive an enumerator for a language. This so-called $W$-index allows for naming\narbitrary computably enumerable languages, with the drawback that even the\nmembership problem is undecidable. In this paper we use a different system\nwhich allows for naming arbitrary decidable languages, namely programs for\ncharacteristic functions (called $C$-indices). These indices have the drawback\nthat it is now not decidable whether a given hypothesis is even a legal\n$C$-index.\n  In this first analysis of learning with $C$-indices, we give a structured\naccount of the learning power of various restrictions employing $C$-indices,\nalso when compared with $W$-indices. We establish a hierarchy of learning power\ndepending on whether $C$-indices are required (a) on all outputs; (b) only on\noutputs relevant for the class to be learned and (c) only in the limit as\nfinal, correct hypotheses. Furthermore, all these settings are weaker than\nlearning with $W$-indices (even when restricted to classes of computable\nlanguages). We analyze all these questions also in relation to the mode of data\npresentation.\n  Finally, we also ask about the relation of semantic versus syntactic\nconvergence and derive the map of pairwise relations for these two kinds of\nconvergence coupled with various forms of data presentation.", "published": "2020-10-15 09:27:47", "link": "http://arxiv.org/abs/2011.09866v1", "categories": ["cs.LO", "cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.LO"}
{"title": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of\n  Code-Mixed Dravidian text using XLNet", "abstract": "Social media has penetrated into multilingual societies, however most of them\nuse English to be a preferred language for communication. So it looks natural\nfor them to mix their cultural language with English during conversations\nresulting in abundance of multilingual data, call this code-mixed data,\navailable in todays' world.Downstream NLP tasks using such data is challenging\ndue to the semantic nature of it being spread across multiple languages.One\nsuch Natural Language Processing task is sentiment analysis, for this we use an\nauto-regressive XLNet model to perform sentiment analysis on code-mixed\nTamil-English and Malayalam-English datasets.", "published": "2020-10-15 14:09:02", "link": "http://arxiv.org/abs/2010.07773v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "The LL(finite) strategy for optimal LL(k) parsing", "abstract": "The LL(finite) parsing strategy for parsing of LL(k) grammars where k needs\nnot to be known is presented. The strategy parses input in linear time, uses\narbitrary but always minimal lookahead necessary to disambiguate between\nalternatives of nonterminals, and it is optimal in the number of lookahead\nterminal scans performed. Modifications to the algorithm are shown that allow\nfor resolution of grammar ambiguities by precedence -- effectively interpreting\nthe input as a parsing expression grammar -- as well as for the use of\npredicates, and a proof of concept, the open-source parser generator Astir,\nemploys the LL(finite) strategy in the output it generates.", "published": "2020-10-15 16:52:29", "link": "http://arxiv.org/abs/2010.07874v2", "categories": ["cs.PL", "cs.CL", "cs.FL"], "primary_category": "cs.PL"}
{"title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense\n  Spatiotemporal Grounding", "abstract": "We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation\n(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger\n(more paths and instructions) than other VLN datasets. It emphasizes the role\nof language in VLN by addressing known biases in paths and eliciting more\nreferences to visible entities. Furthermore, each word in an instruction is\ntime-aligned to the virtual poses of instruction creators and validators. We\nestablish baseline scores for monolingual and multilingual settings and\nmultitask learning when including Room-to-Room annotations. We also provide\nresults for a model that learns from synchronized pose traces by focusing only\non portions of the panorama attended to in human demonstrations. The size,\nscope and detail of RxR dramatically expands the frontier for research on\nembodied language agents in simulated, photo-realistic environments.", "published": "2020-10-15 18:01:15", "link": "http://arxiv.org/abs/2010.07954v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Empirical Study of Transformers for Source Code", "abstract": "Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.", "published": "2020-10-15 19:09:15", "link": "http://arxiv.org/abs/2010.07987v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "What is More Likely to Happen Next? Video-and-Language Future Event\n  Prediction", "abstract": "Given a video with aligned dialogue, people can often infer what is more\nlikely to happen next. Making such predictions requires not only a deep\nunderstanding of the rich dynamics underlying the video and dialogue, but also\na significant amount of commonsense knowledge. In this work, we explore whether\nAI models are able to learn to make such multimodal commonsense next-event\npredictions. To support research in this direction, we collect a new dataset,\nnamed Video-and-Language Event Prediction (VLEP), with 28,726 future event\nprediction examples (along with their rationales) from 10,234 diverse TV Show\nand YouTube Lifestyle Vlog video clips. In order to promote the collection of\nnon-trivial challenging examples, we employ an adversarial\nhuman-and-model-in-the-loop data collection procedure. We also present a strong\nbaseline incorporating information from video, dialogue, and commonsense\nknowledge. Experiments show that each type of information is useful for this\nchallenging task, and that compared to the high human performance on VLEP, our\nmodel provides a good starting point but leaves large room for future work. Our\ndataset and code are available at:\nhttps://github.com/jayleicn/VideoLanguageFuturePred", "published": "2020-10-15 19:56:47", "link": "http://arxiv.org/abs/2010.07999v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical\n  Attention", "abstract": "This paper presents MAST, a new model for Multimodal Abstractive Text\nSummarization that utilizes information from all three modalities -- text,\naudio and video -- in a multimodal video. Prior work on multimodal abstractive\ntext summarization only utilized information from the text and video\nmodalities. We examine the usefulness and challenges of deriving information\nfrom the audio modality and present a sequence-to-sequence trimodal\nhierarchical attention-based model that overcomes these challenges by letting\nthe model pay more attention to the text modality. MAST outperforms the current\nstate of the art model (video-text) by 2.51 points in terms of Content F1 score\nand 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal\nlanguage understanding.", "published": "2020-10-15 21:08:20", "link": "http://arxiv.org/abs/2010.08021v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Fixed-Length Protein Embeddings using Contextual Lenses", "abstract": "The Basic Local Alignment Search Tool (BLAST) is currently the most popular\nmethod for searching databases of biological sequences. BLAST compares\nsequences via similarity defined by a weighted edit distance, which results in\nit being computationally expensive. As opposed to working with edit distance, a\nvector similarity approach can be accelerated substantially using modern\nhardware or hashing techniques. Such an approach would require fixed-length\nembeddings for biological sequences. There has been recent interest in learning\nfixed-length protein embeddings using deep learning models under the hypothesis\nthat the hidden layers of supervised or semi-supervised models could produce\npotentially useful vector embeddings. We consider transformer (BERT) protein\nlanguage models that are pretrained on the TrEMBL data set and learn\nfixed-length embeddings on top of them with contextual lenses. The embeddings\nare trained to predict the family a protein belongs to for sequences in the\nPfam database. We show that for nearest-neighbor family classification,\npretraining offers a noticeable boost in performance and that the corresponding\nlearned embeddings are competitive with BLAST. Furthermore, we show that the\nraw transformer embeddings, obtained via static pooling, do not perform well on\nnearest-neighbor family classification, which suggests that learning embeddings\nin a supervised manner via contextual lenses may be a compute-efficient\nalternative to fine-tuning.", "published": "2020-10-15 14:54:55", "link": "http://arxiv.org/abs/2010.15065v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Automatic Analysis and Influence of Hierarchical Structure on Melody,\n  Rhythm and Harmony in Popular Music", "abstract": "Repetition is a basic indicator of musical structure. This study introduces\nnew algorithms for identifying musical phrases based on repetition. Phrases\ncombine to form sections yielding a two-level hierarchical structure.\nAutomatically detected hierarchical repetition structures reveal significant\ninteractions between structure and chord progressions, melody and rhythm.\nDifferent levels of hierarchy interact differently, providing evidence that\nstructural hierarchy plays an important role in music beyond simple notions of\nrepetition or similarity. Our work suggests new applications for music\ngeneration and music evaluation.", "published": "2020-10-15 04:52:02", "link": "http://arxiv.org/abs/2010.07518v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The NeteaseGames System for Voice Conversion Challenge 2020 with\n  Vector-quantization Variational Autoencoder and WaveNet", "abstract": "This paper presents the description of our submitted system for Voice\nConversion Challenge (VCC) 2020 with vector-quantization variational\nautoencoder (VQ-VAE) with WaveNet as the decoder, i.e., VQ-VAE-WaveNet.\nVQ-VAE-WaveNet is a nonparallel VAE-based voice conversion that reconstructs\nthe acoustic features along with separating the linguistic information with\nspeaker identity. The model is further improved with the WaveNet cycle as the\ndecoder to generate the high-quality speech waveform, since WaveNet, as an\nautoregressive neural vocoder, has achieved the SoTA result of waveform\ngeneration. In practice, our system can be developed with VCC 2020 dataset for\nboth Task 1 (intra-lingual) and Task 2 (cross-lingual). However, we only submit\nour system for the intra-lingual voice conversion task. The results of VCC 2020\ndemonstrate that our system VQ-VAE-WaveNet achieves: 3.04 mean opinion score\n(MOS) in naturalness and a 3.28 average score in similarity ( the speaker\nsimilarity percentage (Sim) of 75.99%) for Task 1. The subjective evaluations\nalso reveal that our system gives top performance when no supervised learning\nis involved. What's more, our system performs well in some objective\nevaluations. Specifically, our system achieves an average score of 3.95 in\nnaturalness in automatic naturalness prediction and ranked the 6th and 8th,\nrespectively in ASV-based speaker similarity and spoofing countermeasures.", "published": "2020-10-15 09:53:56", "link": "http://arxiv.org/abs/2010.07630v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Convolutional Neural Network-based Inverse Filtering Approach for\n  Speech De-reverberation", "abstract": "In this paper, we introduce a spectral-domain inverse filtering approach for\nsingle-channel speech de-reverberation using deep convolutional neural network\n(CNN). The main goal is to better handle realistic reverberant conditions where\nthe room impulse response (RIR) filter is longer than the short-time Fourier\ntransform (STFT) analysis window. To this end, we consider the convolutive\ntransfer function (CTF) model for the reverberant speech signal. In the\nproposed framework, the CNN architecture is trained to directly estimate the\ninverse filter of the CTF model. Among various choices for the CNN structure,\nwe consider the U-net which consists of a fully-convolutional auto-encoder\nnetwork with skip-connections. Experimental results show that the proposed\nmethod provides better de-reverberation performance than the prevalent\nbenchmark algorithms under various reverberation conditions.", "published": "2020-10-15 17:19:57", "link": "http://arxiv.org/abs/2010.07895v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dataset artefacts in anti-spoofing systems: a case study on the ASVspoof\n  2017 benchmark", "abstract": "The Automatic Speaker Verification Spoofing and Countermeasures Challenges\nmotivate research in protecting speech biometric systems against a variety of\ndifferent access attacks. The 2017 edition focused on replay spoofing attacks,\nand involved participants building and training systems on a provided dataset\n(ASVspoof 2017). More than 60 research papers have so far been published with\nthis dataset, but none have sought to answer why countermeasures appear\nsuccessful in detecting spoofing attacks. This article shows how artefacts\ninherent to the dataset may be contributing to the apparent success of\npublished systems. We first inspect the ASVspoof 2017 dataset and summarize\nvarious artefacts present in the dataset. Second, we demonstrate how\ncountermeasure models can exploit these artefacts to appear successful in this\ndataset. Third, for reliable and robust performance estimates on this dataset\nwe propose discarding nonspeech segments and silence before and after the\nspeech utterance during training and inference. We create speech start and\nendpoint annotations in the dataset and demonstrate how using them helps\ncountermeasure models become less vulnerable from being manipulated using\nartefacts found in the dataset. Finally, we provide several new benchmark\nresults for both frame-level and utterance-level models that can serve as new\nbaselines on this dataset.", "published": "2020-10-15 17:46:49", "link": "http://arxiv.org/abs/2010.07913v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Melody Classification based on Performance Event Vector and BRNN", "abstract": "We proposed a model for the Conference of Music and Technology (CSMT2020)\ndata challenge of melody classification. Our model used the Performance Event\nVector as the input sequence to build a Bidirectional RNN network for\nclassfication. The model achieved a satisfying performance on the development\ndataset and Wikifonia dataset. We also discussed the effect of several\nhyper-parameters, and created multiple prediction outputs for the evaluation\ndataset.", "published": "2020-10-15 07:21:58", "link": "http://arxiv.org/abs/2010.07562v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lightweight End-to-End Speech Recognition from Raw Audio Data Using\n  Sinc-Convolutions", "abstract": "Many end-to-end Automatic Speech Recognition (ASR) systems still rely on\npre-processed frequency-domain features that are handcrafted to emulate the\nhuman hearing. Our work is motivated by recent advances in integrated learnable\nfeature extraction. For this, we propose Lightweight Sinc-Convolutions (LSC)\nthat integrate Sinc-convolutions with depthwise convolutions as a low-parameter\nmachine-learnable feature extraction for end-to-end ASR systems.\n  We integrated LSC into the hybrid CTC/attention architecture for evaluation.\nThe resulting end-to-end model shows smooth convergence behaviour that is\nfurther improved by applying SpecAugment in time-domain. We also discuss\nfilter-level improvements, such as using log-compression as activation\nfunction. Our model achieves a word error rate of 10.7% on the TEDlium v2 test\ndataset, surpassing the corresponding architecture with log-mel filterbank\nfeatures by an absolute 1.9%, but only has 21% of its model size.", "published": "2020-10-15 08:43:57", "link": "http://arxiv.org/abs/2010.07597v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Music Classification in MIDI Format based on LSTM Mdel", "abstract": "Music classification between music made by AI or human composers can be done\nby deep learning networks. We first transformed music samples in midi format to\nnatural language sequences, then classified these samples by mLSTM\n(multiplicative Long Short Term Memory) + logistic regression. The accuracy of\nthe result evaluated by 10-fold cross validation can reach 90%. Our work\nindicates that music generated by AI and human composers do have different\ncharacteristics, which can be learned by deep learning networks.", "published": "2020-10-15 13:30:40", "link": "http://arxiv.org/abs/2010.07739v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Transformer Based Pitch Sequence Autoencoder with MIDI Augmentation", "abstract": "Despite recent achievements of deep learning automatic music generation\nalgorithms, few approaches have been proposed to evaluate whether a\nsingle-track music excerpt is composed by automatons or Homo sapiens. To tackle\nthis problem, we apply a masked language model based on ALBERT for composers\nclassification. The aim is to obtain a model that can suggest the probability a\nMIDI clip might be composed condition on the auto-generation hypothesis, and\nwhich is trained with only AI-composed single-track MIDI. In this paper, the\namount of parameters is reduced, two methods on data augmentation are proposed\nas well as a refined loss function to prevent overfitting. The experiment\nresults show our model ranks $3^{rd}$ in all the $7$ teams in the data\nchallenge in CSMT(2020). Furthermore, this inspiring method could be spread to\nother music information retrieval tasks that are based on a small dataset.", "published": "2020-10-15 13:59:58", "link": "http://arxiv.org/abs/2010.07758v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Muse: Multi-modal target speaker extraction with visual cues", "abstract": "Speaker extraction algorithm relies on the speech sample from the target\nspeaker as the reference point to focus its attention. Such a reference speech\nis typically pre-recorded. On the other hand, the temporal synchronization\nbetween speech and lip movement also serves as an informative cue. Motivated by\nthis idea, we study a novel technique to use speech-lip visual cues to extract\nreference target speech directly from mixture speech during inference time,\nwithout the need of pre-recorded reference speech. We propose a multi-modal\nspeaker extraction network, named MuSE, that is conditioned only on a lip image\nsequence. MuSE not only outperforms other competitive baselines in terms of\nSI-SDR and PESQ, but also shows consistent improvement in cross-dataset\nevaluations.", "published": "2020-10-15 14:10:37", "link": "http://arxiv.org/abs/2010.07775v3", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
