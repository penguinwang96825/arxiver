{"title": "CHARP: Conversation History AwaReness Probing for Knowledge-grounded\n  Dialogue Systems", "abstract": "In this work, we dive deep into one of the popular knowledge-grounded\ndialogue benchmarks that focus on faithfulness, FaithDial. We show that a\nsignificant portion of the FaithDial data contains annotation artifacts, which\nmay bias models towards completely ignoring the conversation history. We\ntherefore introduce CHARP, a diagnostic test set, designed for an improved\nevaluation of hallucinations in conversational model. CHARP not only measures\nhallucination but also the compliance of the models to the conversation task.\nOur extensive analysis reveals that models primarily exhibit poor performance\non CHARP due to their inability to effectively attend to and reason over the\nconversation history. Furthermore, the evaluation methods of FaithDial fail to\ncapture these shortcomings, neglecting the conversational history. Our findings\nindicate that there is substantial room for contribution in both dataset\ncreation and hallucination evaluation for knowledge-grounded dialogue, and that\nCHARP can serve as a tool for monitoring the progress in this particular\nresearch area. CHARP is publicly available at\nhttps://huggingface.co/datasets/huawei-noah/CHARP", "published": "2024-05-24 00:00:30", "link": "http://arxiv.org/abs/2405.15110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizable and Scalable Multistage Biomedical Concept Normalization\n  Leveraging Large Language Models", "abstract": "Background: Biomedical entity normalization is critical to biomedical\nresearch because the richness of free-text clinical data, such as progress\nnotes, can often be fully leveraged only after translating words and phrases\ninto structured and coded representations suitable for analysis. Large Language\nModels (LLMs), in turn, have shown great potential and high performance in a\nvariety of natural language processing (NLP) tasks, but their application for\nnormalization remains understudied.\n  Methods: We applied both proprietary and open-source LLMs in combination with\nseveral rule-based normalization systems commonly used in biomedical research.\nWe used a two-step LLM integration approach, (1) using an LLM to generate\nalternative phrasings of a source utterance, and (2) to prune candidate UMLS\nconcepts, using a variety of prompting methods. We measure results by\n$F_{\\beta}$, where we favor recall over precision, and F1.\n  Results: We evaluated a total of 5,523 concept terms and text contexts from a\npublicly available dataset of human-annotated biomedical abstracts.\nIncorporating GPT-3.5-turbo increased overall $F_{\\beta}$ and F1 in\nnormalization systems +9.5 and +7.3 (MetaMapLite), +13.9 and +10.9 (QuickUMLS),\nand +10.5 and +10.3 (BM25), while the open-source Vicuna model achieved +10.8\nand +12.2 (MetaMapLite), +14.7 and +15 (QuickUMLS), and +15.6 and +18.7 (BM25).\n  Conclusions: Existing general-purpose LLMs, both propriety and open-source,\ncan be leveraged at scale to greatly improve normalization performance using\nexisting tools, with no fine-tuning.", "published": "2024-05-24 00:31:04", "link": "http://arxiv.org/abs/2405.15122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Biomedical Entity Linking: Clinical Text Standardization with\n  Low-Resource Techniques", "abstract": "Clinical text is rich in information, with mentions of treatment, medication\nand anatomy among many other clinical terms. Multiple terms can refer to the\nsame core concepts which can be referred as a clinical entity. Ontologies like\nthe Unified Medical Language System (UMLS) are developed and maintained to\nstore millions of clinical entities including the definitions, relations and\nother corresponding information. These ontologies are used for standardization\nof clinical text by normalizing varying surface forms of a clinical term\nthrough Biomedical entity linking. With the introduction of transformer-based\nlanguage models, there has been significant progress in Biomedical entity\nlinking. In this work, we focus on learning through synonym pairs associated\nwith the entities. As compared to the existing approaches, our approach\nsignificantly reduces the training data and resource consumption. Moreover, we\npropose a suite of context-based and context-less reranking techniques for\nperforming the entity disambiguation. Overall, we achieve similar performance\nto the state-of-the-art zero-shot and distant supervised entity linking\ntechniques on the Medmentions dataset, the largest annotated dataset on UMLS,\nwithout any domain-based training. Finally, we show that retrieval performance\nalone might not be sufficient as an evaluation metric and introduce an article\nlevel quantitative and qualitative analysis to reveal further insights on the\nperformance of entity linking methods.", "published": "2024-05-24 01:14:33", "link": "http://arxiv.org/abs/2405.15134v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks", "abstract": "As the adoption of large language models increases and the need for per-user\nor per-task model customization grows, the parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur\nsubstantial storage and transmission costs. To further reduce stored\nparameters, we introduce a \"divide-and-share\" paradigm that breaks the barriers\nof low-rank decomposition across matrix dimensions, modules, and layers by\nsharing parameters globally via a vector bank. As an instantiation of the\nparadigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of\nLoRA from a shared vector bank with a differentiable top-k admixture module.\nVB-LoRA achieves extreme parameter efficiency while maintaining comparable or\nbetter performance compared to state-of-the-art PEFT methods. Extensive\nexperiments demonstrate the effectiveness of VB-LoRA on natural language\nunderstanding, natural language generation, instruction tuning, and\nmathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA\nonly uses 0.4% of LoRA's stored parameters, yet achieves superior results. Our\nsource code is available at https://github.com/leo-yangli/VB-LoRA. This method\nhas been merged into the Hugging Face PEFT package.", "published": "2024-05-24 03:24:34", "link": "http://arxiv.org/abs/2405.15179v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAEE: A Robust Retrieval-Augmented Early Exiting Framework for Efficient\n  Inference", "abstract": "Deploying large language model inference remains challenging due to their\nhigh computational overhead. Early exiting optimizes model inference by\nadaptively reducing the number of inference layers. Existing methods typically\ntrain internal classifiers to determine whether to exit at intermediate layers.\nHowever, such classifier-based early exiting frameworks require significant\neffort to train the classifiers while can only achieve comparable performance\nat best. To address these limitations, this paper proposes RAEE, a robust\nRetrieval-Augmented Early Exiting framework for efficient inference. First,\nthis paper demonstrates that the early exiting problem can be modeled as a\ndistribution prediction problem, where the distribution is approximated using\nsimilar data's exiting information. Then, this paper details the process of\ncollecting exiting information to build the retrieval database. Finally, based\non the pre-built retrieval database, RAEE leverages the retrieved similar\ndata's exiting information to guide the backbone model to exit at the layer,\nwhich is predicted by the approximated distribution. Experimental results\ndemonstrate that the proposed RAEE can significantly accelerate inference. More\nimportantly, RAEE can also achieve a robust zero-shot performance on 8\ndownstream tasks.", "published": "2024-05-24 04:01:24", "link": "http://arxiv.org/abs/2405.15198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Before Generation, Align it! A Novel and Effective Strategy for\n  Mitigating Hallucinations in Text-to-SQL Generation", "abstract": "Large Language Models (LLMs) driven by In-Context Learning (ICL) have\nsignificantly improved the performance of text-to-SQL. Previous methods\ngenerally employ a two-stage reasoning framework, namely 1) schema linking and\n2) logical synthesis, making the framework not only effective but also\ninterpretable. Despite these advancements, the inherent bad nature of the\ngeneralization of LLMs often results in hallucinations, which limits the full\npotential of LLMs. In this work, we first identify and categorize the common\ntypes of hallucinations at each stage in text-to-SQL. We then introduce a novel\nstrategy, Task Alignment (TA), designed to mitigate hallucinations at each\nstage. TA encourages LLMs to take advantage of experiences from similar tasks\nrather than starting the tasks from scratch. This can help LLMs reduce the\nburden of generalization, thereby mitigating hallucinations effectively. We\nfurther propose TA-SQL, a text-to-SQL framework based on this strategy. The\nexperimental results and comprehensive analysis demonstrate the effectiveness\nand robustness of our framework. Specifically, it enhances the performance of\nthe GPT-4 baseline by 21.23% relatively on BIRD dev and it yields significant\nimprovements across six models and four mainstream, complex text-to-SQL\nbenchmarks.", "published": "2024-05-24 07:51:08", "link": "http://arxiv.org/abs/2405.15307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DnA-Eval: Enhancing Large Language Model Evaluation through\n  Decomposition and Aggregation", "abstract": "The acceleration of Large Language Models (LLMs) research has opened up new\npossibilities for evaluating generated texts. They serve as scalable and\neconomical evaluators, but the question of how reliable these evaluators are\nhas emerged as a crucial research question. Prior research efforts in the\nmeta-evaluation of LLMs as judges limit the prompting of an LLM to a single use\nto obtain a final evaluation decision. They then compute the agreement between\nLLMs' outputs and human labels. This lacks interpretability in understanding\nthe evaluation capability of LLMs. In light of this challenge, we propose\nDecompose and Aggregate, which breaks down the evaluation process into\ndifferent stages based on pedagogical practices. Our experiments illustrate\nthat it not only provides a more interpretable window for how well LLMs\nevaluate, but also leads to improvements up to 39.6% for different LLMs on a\nvariety of meta-evaluation benchmarks.", "published": "2024-05-24 08:12:30", "link": "http://arxiv.org/abs/2405.15329v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models", "abstract": "Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like \"local layer key-value\nstorage\" and \"term-driven optimization\", as used in previous methods like\nMEMIT, are not effective for handling unstructured knowledge. To address these\nchallenges, we propose a novel Unstructured Knowledge Editing method, namely\nUnKE, which extends previous assumptions in the layer dimension and token\ndimension. Firstly, in the layer dimension, we propose non-local block\nkey-value storage to replace local layer key-value storage, increasing the\nrepresentation ability of key-value pairs and incorporating attention layer\nknowledge. Secondly, in the token dimension, we replace \"term-driven\noptimization\" with \"cause-driven optimization\", which edits the last token\ndirectly while preserving context, avoiding the need to locate terms and\npreventing the loss of context information. Results on newly proposed\nunstructured knowledge editing dataset (UnKEBench) and traditional structured\ndatasets demonstrate that UnKE achieves remarkable performance, surpassing\nstrong baselines. In addition, UnKE has robust batch editing and sequential\nediting capabilities.", "published": "2024-05-24 08:42:40", "link": "http://arxiv.org/abs/2405.15349v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models can Deliver Accurate and Interpretable Time Series\n  Anomaly Detection", "abstract": "Time series anomaly detection (TSAD) plays a crucial role in various\nindustries by identifying atypical patterns that deviate from standard trends,\nthereby maintaining system integrity and enabling prompt response measures.\nTraditional TSAD models, which often rely on deep learning, require extensive\ntraining data and operate as black boxes, lacking interpretability for detected\nanomalies. To address these challenges, we propose LLMAD, a novel TSAD method\nthat employs Large Language Models (LLMs) to deliver accurate and interpretable\nTSAD results. LLMAD innovatively applies LLMs for in-context anomaly detection\nby retrieving both positive and negative similar time series segments,\nsignificantly enhancing LLMs' effectiveness. Furthermore, LLMAD employs the\nAnomaly Detection Chain-of-Thought (AnoCoT) approach to mimic expert logic for\nits decision-making process. This method further enhances its performance and\nenables LLMAD to provide explanations for their detections through versatile\nperspectives, which are particularly important for user decision-making.\nExperiments on three datasets indicate that our LLMAD achieves detection\nperformance comparable to state-of-the-art deep learning methods while offering\nremarkable interpretability for detections. To the best of our knowledge, this\nis the first work that directly employs LLMs for TSAD.", "published": "2024-05-24 09:07:02", "link": "http://arxiv.org/abs/2405.15370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergence of a High-Dimensional Abstraction Phase in Language\n  Transformers", "abstract": "A language model (LM) is a mapping from a linguistic context to an output\ntoken. However, much remains to be known about this mapping, including how its\ngeometric properties relate to its function. We take a high-level geometric\napproach to its analysis, observing, across five pre-trained transformer-based\nLMs and three input datasets, a distinct phase characterized by high intrinsic\ndimensionality. During this phase, representations (1) correspond to the first\nfull linguistic abstraction of the input; (2) are the first to viably transfer\nto downstream tasks; (3) predict each other across different LMs. Moreover, we\nfind that an earlier onset of the phase strongly predicts better language\nmodelling performance. In short, our results suggest that a central\nhigh-dimensionality phase underlies core linguistic processing in many common\nLM architectures.", "published": "2024-05-24 11:49:07", "link": "http://arxiv.org/abs/2405.15471v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Matrix in Large Language Model Fine-tuning", "abstract": "LoRA and its variants have become popular parameter-efficient fine-tuning\n(PEFT) methods due to their ability to avoid excessive computational costs.\nHowever, an accuracy gap often exists between PEFT methods and full fine-tuning\n(FT), and this gap has yet to be systematically studied. In this work, we\nintroduce a method for selecting sparse sub-matrices that aim to minimize the\nperformance gap between PEFT vs. full fine-tuning (FT) while also reducing both\nfine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT)\nmethod begins by identifying the most significant sub-matrices in the gradient\nupdate, updating only these blocks during the fine-tuning process. In our\nexperiments, we demonstrate that SMT consistently surpasses other PEFT baseline\n(e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA\nacross a broad spectrum of tasks, while reducing the GPU memory footprint by\n67% compared to FT. We also examine how the performance of LoRA and DoRA tends\nto plateau and decline as the number of trainable parameters increases, in\ncontrast, our SMT method does not suffer from such issue.", "published": "2024-05-24 13:12:14", "link": "http://arxiv.org/abs/2405.15525v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems", "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings.", "published": "2024-05-24 14:13:54", "link": "http://arxiv.org/abs/2405.15585v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Profiling checkpointing schedules in adjoint ST-AD", "abstract": "Checkpointing is a cornerstone of data-flow reversal in adjoint algorithmic\ndifferentiation. Checkpointing is a storage/recomputation trade-off that can be\napplied at different levels, one of which being the call tree. We are looking\nfor good placements of checkpoints onto the call tree of a given application,\nto reduce run time and memory footprint of its adjoint. There is no known\noptimal solution to this problem other than a combinatorial search on all\nplacements. We propose a heuristics based on run-time profiling of the adjoint\ncode. We describe implementation of this profiling tool in an existing\nsource-transformation AD tool. We demonstrate the interest of this approach on\ntest cases taken from the MITgcm ocean and atmospheric global circulation\nmodel. We discuss the limitations of our approach and propose directions to\nlift them.", "published": "2024-05-24 14:20:45", "link": "http://arxiv.org/abs/2405.15590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Generation: A Systematic Literature Review of Tasks, Evaluation,\n  and Challenges", "abstract": "Text generation has become more accessible than ever, and the increasing\ninterest in these systems, especially those using large language models, has\nspurred an increasing number of related publications. We provide a systematic\nliterature review comprising 244 selected papers between 2017 and 2024. This\nreview categorizes works in text generation into five main tasks: open-ended\ntext generation, summarization, translation, paraphrasing, and question\nanswering. For each task, we review their relevant characteristics, sub-tasks,\nand specific challenges (e.g., missing datasets for multi-document\nsummarization, coherence in story generation, and complex reasoning for\nquestion answering). Additionally, we assess current approaches for evaluating\ntext generation systems and ascertain problems with current metrics. Our\ninvestigation shows nine prominent challenges common to all tasks and sub-tasks\nin recent text generation publications: bias, reasoning, hallucinations,\nmisuse, privacy, interpretability, transparency, datasets, and computing. We\nprovide a detailed analysis of these challenges, their potential solutions, and\nwhich gaps still require further engagement from the community. This systematic\nliterature review targets two main audiences: early career researchers in\nnatural language processing looking for an overview of the field and promising\nresearch directions, as well as experienced researchers seeking a detailed view\nof tasks, evaluation methodologies, open challenges, and recent mitigation\nstrategies.", "published": "2024-05-24 14:38:11", "link": "http://arxiv.org/abs/2405.15604v3", "categories": ["cs.CL", "A.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "EmpathicStories++: A Multimodal Dataset for Empathy towards Personal\n  Experiences", "abstract": "Modeling empathy is a complex endeavor that is rooted in interpersonal and\nexperiential dimensions of human interaction, and remains an open problem\nwithin AI. Existing empathy datasets fall short in capturing the richness of\nempathy responses, often being confined to in-lab or acted scenarios, lacking\nlongitudinal data, and missing self-reported labels. We introduce a new\nmultimodal dataset for empathy during personal experience sharing: the\nEmpathicStories++ dataset\n(https://mitmedialab.github.io/empathic-stories-multimodal/) containing 53\nhours of video, audio, and text data of 41 participants sharing vulnerable\nexperiences and reading empathically resonant stories with an AI agent.\nEmpathicStories++ is the first longitudinal dataset on empathy, collected over\na month-long deployment of social robots in participants' homes, as\nparticipants engage in natural, empathic storytelling interactions with AI\nagents. We then introduce a novel task of predicting individuals' empathy\ntoward others' stories based on their personal experiences, evaluated in two\ncontexts: participants' own personal shared story context and their reflections\non stories they read. We benchmark this task using state-of-the-art models to\npave the way for future improvements in contextualized and longitudinal empathy\nmodeling. Our work provides a valuable resource for further research in\ndeveloping empathetic AI systems and understanding the intricacies of human\nempathy within genuine, real-world settings.", "published": "2024-05-24 16:57:18", "link": "http://arxiv.org/abs/2405.15708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Augmentative and Alternative Communication with Card\n  Prediction and Colourful Semantics", "abstract": "This paper presents an approach to enhancing Augmentative and Alternative\nCommunication (AAC) systems by integrating Colourful Semantics (CS) with\ntransformer-based language models specifically tailored for Brazilian\nPortuguese. We introduce an adapted BERT model, BERTptCS, which incorporates\nthe CS framework for improved prediction of communication cards. The primary\naim is to enhance the accuracy and contextual relevance of communication card\npredictions, which are essential in AAC systems for individuals with complex\ncommunication needs (CCN). We compared BERTptCS with a baseline model,\nBERTptAAC, which lacks CS integration. Our results demonstrate that BERTptCS\nsignificantly outperforms BERTptAAC in various metrics, including top-k\naccuracy, Mean Reciprocal Rank (MRR), and Entropy@K. Integrating CS into the\nlanguage model improves prediction accuracy and offers a more intuitive and\ncontextual understanding of user inputs, facilitating more effective\ncommunication.", "published": "2024-05-24 19:33:34", "link": "http://arxiv.org/abs/2405.15896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLIDE: A Framework Integrating Small and Large Language Models for\n  Open-Domain Dialogues Evaluation", "abstract": "The long-standing one-to-many problem of gold standard responses in\nopen-domain dialogue systems presents challenges for automatic evaluation\nmetrics. Though prior works have demonstrated some success by applying powerful\nLarge Language Models (LLMs), existing approaches still struggle with the\none-to-many problem, and exhibit subpar performance in domain-specific\nscenarios. We assume the commonsense reasoning biases within LLMs may hinder\ntheir performance in domainspecific evaluations. To address both issues, we\npropose a novel framework SLIDE (Small and Large Integrated for Dialogue\nEvaluation), that leverages both a small, specialised model (SLM), and LLMs for\nthe evaluation of open domain dialogues. Our approach introduces several\ntechniques: (1) Contrastive learning to differentiate between robust and\nnon-robust response embeddings; (2) A novel metric for semantic sensitivity\nthat combines embedding cosine distances with similarity learned through neural\nnetworks, and (3) a strategy for incorporating the evaluation results from both\nthe SLM and LLMs. Our empirical results demonstrate that our approach achieves\nstate-of-the-art performance in both the classification and evaluation tasks,\nand additionally the SLIDE evaluator exhibits better correlation with human\njudgements. Our code is available at https://\ngithub.com/hegehongcha/SLIDE-ACL2024.", "published": "2024-05-24 20:32:49", "link": "http://arxiv.org/abs/2405.15924v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A hierarchical Bayesian model for syntactic priming", "abstract": "The effect of syntactic priming exhibits three well-documented empirical\nproperties: the lexical boost, the inverse frequency effect, and the\nasymmetrical decay. We aim to show how these three empirical phenomena can be\nreconciled in a general learning framework, the hierarchical Bayesian model\n(HBM). The model represents syntactic knowledge in a hierarchical structure of\nsyntactic statistics, where a lower level represents the verb-specific biases\nof syntactic decisions, and a higher level represents the abstract bias as an\naggregation of verb-specific biases. This knowledge is updated in response to\nexperience by Bayesian inference. In simulations, we show that the HBM captures\nthe above-mentioned properties of syntactic priming. The results indicate that\nsome properties of priming which are usually explained by a residual activation\naccount can also be explained by an implicit learning account. We also discuss\nthe model's implications for the lexical basis of syntactic priming.", "published": "2024-05-24 22:26:53", "link": "http://arxiv.org/abs/2405.15964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expert-Token Resonance MoE: Bidirectional Routing with Efficiency\n  Affinity-Driven Active Selection", "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We propose a novel expert routing framework that incorporates:\n(1) An efficient routing mechanism with lightweight computation. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) A module that determines the lower bounds of expert capacity\nbased on dynamic token distribution analysis, specifically designed to address\ndrop-and-pad strategies. It is also integrated with orthogonal feature\nextraction module and an optimized loss function for expert localization. This\nframework effectively reduces expert homogeneity while enhancing the\nperformance of the expert selection module. Additionally, we introduce a local\nexpert strategy that simultaneously improves load balancing and reduces network\ncommunication overhead. It achieves a 40\\% reduction in token processed by each\nexpert without compromising model convergence or efficacy. When coupled with\ncommunication optimizations, the training efficiency improvements of 5.4\\% to\n46.6\\% can be observed. After supervised fine-tuning, it exhibits performance\ngains of 9.7\\% to 14.1\\% across GDAD, GPQA, and TeleQnA benchmarks.", "published": "2024-05-24 02:50:44", "link": "http://arxiv.org/abs/2406.00023v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Unlearning in Large Language Models", "abstract": "Machine unlearning, a novel area within artificial intelligence, focuses on\naddressing the challenge of selectively forgetting or reducing undesirable\nknowledge or behaviors in machine learning models, particularly in the context\nof large language models (LLMs). This paper introduces a methodology to align\nLLMs, such as Open Pre-trained Transformer Language Models, with ethical,\nprivacy, and safety standards by leveraging the gradient ascent algorithm for\nknowledge unlearning. Our approach aims to selectively erase or modify learned\ninformation in LLMs, targeting harmful responses and copyrighted content. This\npaper presents a dual-pronged approach to enhance the ethical and safe behavior\nof large language models (LLMs) by addressing the issues of harmful responses\nand copyrighted content. To mitigate harmful responses, we applied gradient\nascent on the PKU dataset, achieving a 75\\% reduction in harmful responses for\nOpen Pre-trained Transformer Language Models (OPT1.3b and OPT2.7b)\n\\citet{zhang2022opt} while retaining previous knowledge using the TruthfulQA\ndataset \\citet{DBLP:journals/corr/abs-2109-07958}. For handling copyrighted\ncontent, we constructed a custom dataset based on the Lord of the Rings corpus\nand aligned LLMs (OPT1.3b and OPT2.7b) \\citet{zhang2022opt} through LoRA:\nLow-Rank Adaptation of Large Language Models\n\\citet{DBLP:journals/corr/abs-2106-09685} finetuning. Subsequently, we employed\ngradient ascent to unlearn the Lord of the Rings content, resulting in a\nremarkable reduction in the presence of copyrighted material. To maintain a\ndiverse knowledge base, we utilized the Book Corpus dataset. Additionally, we\npropose a new evaluation technique for assessing the effectiveness of harmful\nunlearning.", "published": "2024-05-24 02:12:51", "link": "http://arxiv.org/abs/2405.15152v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EffiLearner: Enhancing Efficiency of Generated Code via\n  Self-Optimization", "abstract": "Large language models (LLMs) have shown remarkable progress in code\ngeneration, but their generated code often suffers from inefficiency, resulting\nin longer execution times and higher memory consumption. To address this issue,\nwe propose \\textbf{EffiLearner}, a self-optimization framework that utilizes\nexecution overhead profiles to improve the efficiency of LLM-generated code.\nEffiLearner first generates code using an LLM, then executes it locally to\ncapture execution time and memory usage profiles. These profiles are fed back\nto the LLM, which then revises the code to reduce overhead. To evaluate the\neffectiveness of EffiLearner, we conduct extensive experiments on the\nEffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models.\nOur evaluation results demonstrate that through iterative self-optimization,\nEffiLearner significantly enhances the efficiency of LLM-generated code. For\nexample, the execution time (ET) of StarCoder2-15B for the EffiBench decreases\nfrom 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement\ncompared with the initial code. The total memory usage (TMU) of StarCoder2-15B\nalso decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total\nmemory consumption during the execution process. The source code of EffiLearner\nwas released in \\url{https://github.com/huangd1999/EffiLearner}.", "published": "2024-05-24 03:48:15", "link": "http://arxiv.org/abs/2405.15189v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Cross-Task Defense: Instruction-Tuning LLMs for Content Safety", "abstract": "Recent studies reveal that Large Language Models (LLMs) face challenges in\nbalancing safety with utility, particularly when processing long texts for NLP\ntasks like summarization and translation. Despite defenses against malicious\nshort questions, the ability of LLMs to safely handle dangerous long content,\nsuch as manuals teaching illicit activities, remains unclear. Our work aims to\ndevelop robust defenses for LLMs in processing malicious documents alongside\nbenign NLP task queries. We introduce a defense dataset comprised of\nsafety-related examples and propose single-task and mixed-task losses for\ninstruction tuning. Our empirical results demonstrate that LLMs can\nsignificantly enhance their capacity to safely manage dangerous content with\nappropriate instruction tuning. Additionally, strengthening the defenses of\ntasks most susceptible to misuse is effective in protecting LLMs against\nprocessing harmful information. We also observe that trade-offs between utility\nand safety exist in defense strategies, where Llama2, utilizing our proposed\napproach, displays a significantly better balance compared to Llama1.", "published": "2024-05-24 04:14:32", "link": "http://arxiv.org/abs/2405.15202v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Decoding at the Speed of Thought: Harnessing Parallel Decoding of\n  Lexical Units for LLMs", "abstract": "Large language models have demonstrated exceptional capability in natural\nlanguage understanding and generation. However, their generation speed is\nlimited by the inherently sequential nature of their decoding process, posing\nchallenges for real-time applications. This paper introduces Lexical Unit\nDecoding (LUD), a novel decoding methodology implemented in a data-driven\nmanner, accelerating the decoding process without sacrificing output quality.\nThe core of our approach is the observation that a pre-trained language model\ncan confidently predict multiple contiguous tokens, forming the basis for a\n\\textit{lexical unit}, in which these contiguous tokens could be decoded in\nparallel. Extensive experiments validate that our method substantially reduces\ndecoding time while maintaining generation quality, i.e., 33\\% speed up on\nnatural language generation with no quality loss, and 30\\% speed up on code\ngeneration with a negligible quality loss of 3\\%. Distinctively, LUD requires\nno auxiliary models and does not require changes to existing architectures. It\ncan also be integrated with other decoding acceleration methods, thus achieving\nan even more pronounced inference efficiency boost. We posit that the\nfoundational principles of LUD could define a new decoding paradigm for future\nlanguage models, enhancing their applicability for a broader spectrum of\napplications. All codes are be publicly available at\nhttps://github.com/tjunlp-lab/Lexical-Unit-Decoding-LUD-. Keywords: Parallel\nDecoding, Lexical Unit Decoding, Large Language Model", "published": "2024-05-24 04:35:13", "link": "http://arxiv.org/abs/2405.15208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEEM: Diffusion Models Serve as the Eyes of Large Language Models for\n  Image Perception", "abstract": "The development of large language models (LLMs) has significantly advanced\nthe emergence of large multimodal models (LMMs). While LMMs have achieved\ntremendous success by promoting the synergy between multimodal comprehension\nand creation, they often face challenges when confronted with\nout-of-distribution data, such as which can hardly distinguish orientation,\nquantity, color, structure, etc. This is primarily due to their reliance on\nimage encoders trained to encode images into task-relevant features, which may\nlead them to disregard irrelevant details. Delving into the modeling\ncapabilities of diffusion models for images naturally prompts the question: Can\ndiffusion models serve as the eyes of large language models for image\nperception? In this paper, we propose DEEM, a simple but effective approach\nthat utilizes the generative feedback of diffusion models to align the semantic\ndistributions of the image encoder. This addresses the drawbacks of previous\nmethods that solely relied on image encoders like CLIP-ViT, thereby enhancing\nthe model's resilience against out-of-distribution samples and reducing visual\nhallucinations. Importantly, this is achieved without requiring additional\ntraining modules and with fewer training parameters. We extensively evaluated\nDEEM on both our newly constructed RobustVQA benchmark and other well-known\nbenchmarks, POPE and MMVP, for visual hallucination and perception. In\nparticular, DEEM improves LMM's visual perception performance to a large extent\n(e.g., 4% higher on RobustVQA, 6.5% higher on MMVP and 12.8 % higher on POPE ).\nCompared to the state-of-the-art interleaved content generation models, DEEM\nexhibits enhanced robustness and a superior capacity to alleviate model\nhallucinations while utilizing fewer trainable parameters, less pre-training\ndata (10%), and a smaller base model size.", "published": "2024-05-24 05:46:04", "link": "http://arxiv.org/abs/2405.15232v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DeTikZify: Synthesizing Graphics Programs for Scientific Figures and\n  Sketches with TikZ", "abstract": "Creating high-quality scientific figures can be time-consuming and\nchallenging, even though sketching ideas on paper is relatively easy.\nFurthermore, recreating existing figures that are not stored in formats\npreserving semantic information is equally complex. To tackle this problem, we\nintroduce DeTikZify, a novel multimodal language model that automatically\nsynthesizes scientific figures as semantics-preserving TikZ graphics programs\nbased on sketches and existing figures. To achieve this, we create three new\ndatasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k\nhuman-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn\nsketches with their corresponding scientific figures; and MetaFig, a collection\nof diverse scientific figures and associated metadata. We train DeTikZify on\nMetaFig and DaTikZv2, along with synthetically generated sketches learned from\nSketchFig. We also introduce an MCTS-based inference algorithm that enables\nDeTikZify to iteratively refine its outputs without the need for additional\ntraining. Through both automatic and human evaluation, we demonstrate that\nDeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ\nprograms, with the MCTS algorithm effectively boosting its performance. We make\nour code, models, and datasets publicly available.", "published": "2024-05-24 07:48:35", "link": "http://arxiv.org/abs/2405.15306v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Are Long-LLMs A Necessity For Long-Context Tasks?", "abstract": "The learning and deployment of long-LLMs remains a challenging problem\ndespite recent progresses. In this work, we argue that the long-LLMs are not a\nnecessity to solve long-context tasks, as common long-context tasks are\nshort-context solvable, i.e. they can be solved by purely working with oracle\nshort-contexts within the long-context tasks' inputs. On top of this argument,\nwe propose a framework called LC-Boost (Long-Context Bootstrapper), which\nenables a short-LLM to address the long-context tasks in a bootstrapping\nmanner. In our framework, the short-LLM prompts itself to reason for two\ncritical decisions: 1) how to access to the appropriate part of context within\nthe input, 2) how to make effective use of the accessed context. By adaptively\naccessing and utilizing the context based on the presented tasks, LC-Boost can\nserve as a general framework to handle diversified long-context processing\nproblems. We comprehensively evaluate different types of tasks from popular\nlong-context benchmarks, where LC-Boost is able to achieve a substantially\nimproved performance with a much smaller consumption of resource.", "published": "2024-05-24 07:59:30", "link": "http://arxiv.org/abs/2405.15318v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient\n  LLM Pre-Training", "abstract": "LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.", "published": "2024-05-24 08:00:00", "link": "http://arxiv.org/abs/2405.15319v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Organic Data-Driven Approach for Turkish Grammatical Error Correction\n  and LLMs", "abstract": "Grammatical Error Correction has seen significant progress with the recent\nadvancements in deep learning. As those methods require huge amounts of data,\nsynthetic datasets are being built to fill this gap. Unfortunately, synthetic\ndatasets are not organic enough in some cases and even require clean data to\nstart with. Furthermore, most of the work that has been done is focused mostly\non English. In this work, we introduce a new organic data-driven approach,\nclean insertions, to build parallel Turkish Grammatical Error Correction\ndatasets from any organic data, and to clean the data used for training Large\nLanguage Models. We achieve state-of-the-art results on two Turkish Grammatical\nError Correction test sets out of the three publicly available ones. We also\nshow the effectiveness of our method on the training losses of training\nlanguage models.", "published": "2024-05-24 08:00:24", "link": "http://arxiv.org/abs/2405.15320v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detection and Positive Reconstruction of Cognitive Distortion sentences:\n  Mandarin Dataset and Evaluation", "abstract": "This research introduces a Positive Reconstruction Framework based on\npositive psychology theory. Overcoming negative thoughts can be challenging,\nour objective is to address and reframe them through a positive\nreinterpretation. To tackle this challenge, a two-fold approach is necessary:\nidentifying cognitive distortions and suggesting a positively reframed\nalternative while preserving the original thought's meaning. Recent studies\nhave investigated the application of Natural Language Processing (NLP) models\nin English for each stage of this process. In this study, we emphasize the\ntheoretical foundation for the Positive Reconstruction Framework, grounded in\nbroaden-and-build theory. We provide a shared corpus containing 4001 instances\nfor detecting cognitive distortions and 1900 instances for positive\nreconstruction in Mandarin. Leveraging recent NLP techniques, including\ntransfer learning, fine-tuning pretrained networks, and prompt engineering, we\ndemonstrate the effectiveness of automated tools for both tasks. In summary,\nour study contributes to multilingual positive reconstruction, highlighting the\neffectiveness of NLP in cognitive distortion detection and positive\nreconstruction.", "published": "2024-05-24 08:17:20", "link": "http://arxiv.org/abs/2405.15334v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks", "abstract": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks.", "published": "2024-05-24 11:30:37", "link": "http://arxiv.org/abs/2405.15453v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language\n  Models", "abstract": "The immense datasets used to develop Large Language Models (LLMs) often\ninclude copyright-protected content, typically without the content creator's\nconsent. Copyright traps have been proposed to be injected into the original\ncontent, improving content detectability in newly released LLMs. Traps,\nhowever, rely on the exact duplication of a unique text sequence, leaving them\nvulnerable to commonly deployed data deduplication techniques. We here propose\nthe generation of fuzzy copyright traps, featuring slight modifications across\nduplication. When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy\ntrap sequences to be memorized nearly as well as exact duplicates.\nSpecifically, the Membership Inference Attack (MIA) ROC AUC only drops from\n0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates. We also\nfind that selecting replacement positions to minimize the exact overlap between\nfuzzy duplicates leads to similar memorization, while making fuzzy duplicates\nhighly unlikely to be removed by any deduplication process. Lastly, we argue\nthat the fact that LLMs memorize across fuzzy duplicates challenges the study\nof LLM memorization relying on naturally occurring duplicates. Indeed, we find\nthat the commonly used training dataset, The Pile, contains significant amounts\nof fuzzy duplicates. This introduces a previously unexplored confounding factor\nin post-hoc studies of LLM memorization, and questions the effectiveness of\n(exact) data deduplication as a privacy protection technique.", "published": "2024-05-24 13:05:05", "link": "http://arxiv.org/abs/2405.15523v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large\n  Multimodal Models", "abstract": "Multilingual multimodal reasoning is a core component in achieving\nhuman-level intelligence. However, most existing benchmarks for multilingual\nmultimodal reasoning struggle to differentiate between models of varying\nperformance; even language models without visual capabilities can easily\nachieve high scores. This leaves a comprehensive evaluation of leading\nmultilingual multimodal models largely unexplored. In this work, we introduce\nM4U, a novel and challenging benchmark for assessing the capability of\nmulti-discipline multilingual multimodal understanding and reasoning. M4U\ncontains 8,931 samples covering 64 disciplines across 16 subfields in Science,\nEngineering, and Healthcare in Chinese, English, and German. Using M4U, we\nconduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and\nLarge Language Models (LLMs) with external tools. The evaluation results show\nthat the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy\non M4U. Additionally, we observe that the leading LMMs exhibit significant\nlanguage preferences. Our in-depth analysis indicates that leading LMMs,\nincluding GPT-4o, suffer performance degradation when prompted with\ncross-lingual multimodal questions, such as images with key textual information\nin Chinese while the question is in German. We believe that M4U can serve as a\ncrucial tool for systematically evaluating LMMs based on their multilingual\nmultimodal reasoning capabilities and monitoring their development. The\nhomepage, codes and data are public available.", "published": "2024-05-24 15:25:28", "link": "http://arxiv.org/abs/2405.15638v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GECKO: Generative Language Model for English, Code and Korean", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for\nKorean and English, along with programming languages. GECKO is pretrained on\nthe balanced, high-quality corpus of Korean and English employing LLaMA\narchitecture. In this report, we share the experiences of several efforts to\nbuild a better data pipeline for the corpus and to train our model. GECKO shows\ngreat efficiency in token generations for both Korean and English, despite its\nsmall size of vocabulary. We measure the performance on the representative\nbenchmarks in terms of Korean, English and Code, and it exhibits great\nperformance on KMMLU (Korean MMLU) and modest performance in English and Code,\neven with its smaller number of trained tokens compared to English-focused\nLLMs. GECKO is available to the open-source community under a permissive\nlicense. We hope our work offers a research baseline and practical insights for\nKorean LLM research. The model can be found at:\nhttps://huggingface.co/kifai/GECKO-7B", "published": "2024-05-24 15:30:41", "link": "http://arxiv.org/abs/2405.15640v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness\n  Benchmark Construction", "abstract": "Social biases in LLMs are usually measured via bias benchmark datasets.\nCurrent benchmarks have limitations in scope, grounding, quality, and human\neffort required. Previous work has shown success with a community-sourced,\nrather than crowd-sourced, approach to benchmark development. However, this\nwork still required considerable effort from annotators with relevant lived\nexperience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo)\ncan assist with the task of developing a bias benchmark dataset from responses\nto an open-ended community survey. We also extend the previous work to a new\ncommunity and set of biases: the Jewish community and antisemitism. Our\nanalysis shows that GPT-3.5-Turbo has poor performance on this annotation task\nand produces unacceptable quality issues in its output. Thus, we conclude that\nGPT-3.5-Turbo is not an appropriate substitute for human annotation in\nsensitive tasks related to social biases, and that its use actually negates\nmany of the benefits of community-sourcing bias benchmarks.", "published": "2024-05-24 17:56:03", "link": "http://arxiv.org/abs/2405.15760v1", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.2"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Discriminative Classification in Large Language Models", "abstract": "Modern large language models (LLMs) represent a paradigm shift in what can\nplausibly be expected of machine learning models. The fact that LLMs can\neffectively generate sensible answers to a diverse range of queries suggests\nthat they would be useful in customer support applications. While powerful,\nLLMs have been observed to be prone to hallucination which unfortunately makes\ntheir near term use in customer support applications challenging. To address\nthis issue we present a system that allows us to use an LLM to augment our\ncustomer support advocates by re-framing the language modeling task as a\ndiscriminative classification task. In this framing, we seek to present the\ntop-K best template responses for a customer support advocate to use when\nresponding to a customer. We present the result of both offline and online\nexperiments where we observed offline gains and statistically significant\nonline lifts for our experimental system. Along the way, we present observed\nscaling curves for validation loss and top-K accuracy, resulted from model\nparameter ablation studies. We close by discussing the space of trade-offs with\nrespect to model size, latency, and accuracy as well as and suggesting future\napplications to explore.", "published": "2024-05-24 17:58:38", "link": "http://arxiv.org/abs/2405.15765v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Spam Email Classification Using Pre-trained Large Language\n  Models", "abstract": "This paper investigates the application of pre-trained large language models\n(LLMs) for spam email classification using zero-shot prompting. We evaluate the\nperformance of both open-source (Flan-T5) and proprietary LLMs (ChatGPT, GPT-4)\non the well-known SpamAssassin dataset. Two classification approaches are\nexplored: (1) truncated raw content from email subject and body, and (2)\nclassification based on summaries generated by ChatGPT. Our empirical analysis,\nleveraging the entire dataset for evaluation without further training, reveals\npromising results. Flan-T5 achieves a 90% F1-score on the truncated content\napproach, while GPT-4 reaches a 95% F1-score using summaries. While these\ninitial findings on a single dataset suggest the potential for classification\npipelines of LLM-based subtasks (e.g., summarisation and classification),\nfurther validation on diverse datasets is necessary. The high operational costs\nof proprietary models, coupled with the general inference costs of LLMs, could\nsignificantly hinder real-world deployment for spam filtering.", "published": "2024-05-24 20:55:49", "link": "http://arxiv.org/abs/2405.15936v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transformers represent belief state geometry in their residual stream", "abstract": "What computational structure are we building into large language models when\nwe train them on next-token prediction? Here, we present evidence that this\nstructure is given by the meta-dynamics of belief updating over hidden states\nof the data-generating process. Leveraging the theory of optimal prediction, we\nanticipate and then find that belief states are linearly represented in the\nresidual stream of transformers, even in cases where the predicted belief state\ngeometry has highly nontrivial fractal structure. We investigate cases where\nthe belief state geometry is represented in the final residual stream or\ndistributed across the residual streams of multiple layers, providing a\nframework to explain these observations. Furthermore we demonstrate that the\ninferred belief states contain information about the entire future, beyond the\nlocal next-token prediction that the transformers are explicitly trained on.\nOur work provides a general framework connecting the structure of training data\nto the geometric structure of activations inside transformers.", "published": "2024-05-24 21:14:10", "link": "http://arxiv.org/abs/2405.15943v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating and Safeguarding the Adversarial Robustness of\n  Retrieval-Based In-Context Learning", "abstract": "With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl", "published": "2024-05-24 23:56:36", "link": "http://arxiv.org/abs/2405.15984v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCALM: Towards Semantic Caching for Automated Chat Services with Large\n  Language Models", "abstract": "Large Language Models (LLMs) have become increasingly popular, transforming a\nwide range of applications across various domains. However, the real-world\neffectiveness of their query cache systems has not been thoroughly\ninvestigated. In this work, we for the first time conducted an analysis on\nreal-world human-to-LLM interaction data, identifying key challenges in\nexisting caching solutions for LLM-based chat services. Our findings reveal\nthat current caching methods fail to leverage semantic connections, leading to\ninefficient cache performance and extra token costs. To address these issues,\nwe propose SCALM, a new cache architecture that emphasizes semantic analysis\nand identifies significant cache entries and patterns. We also detail the\nimplementations of the corresponding cache storage and eviction strategies. Our\nevaluations show that SCALM increases cache hit ratios and reduces operational\ncosts for LLMChat services. Compared with other state-of-the-art solutions in\nGPTCache, SCALM shows, on average, a relative increase of 63% in cache hit\nratio and a relative improvement of 77% in tokens savings.", "published": "2024-05-24 08:16:22", "link": "http://arxiv.org/abs/2406.00025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Sense Disambiguation in Persian: Can AI Finally Get It Right?", "abstract": "Homograph disambiguation, the task of distinguishing words with identical\nspellings but different meanings, poses a substantial challenge in natural\nlanguage processing. In this study, we introduce a novel dataset tailored for\nPersian homograph disambiguation. Our work encompasses a thorough exploration\nof various embeddings, evaluated through the cosine similarity method and their\nefficacy in downstream tasks like classification. Our investigation entails\ntraining a diverse array of lightweight machine learning and deep learning\nmodels for phonograph disambiguation. We scrutinize the models' performance in\nterms of Accuracy, Recall, and F1 Score, thereby gaining insights into their\nrespective strengths and limitations. The outcomes of our research underscore\nthree key contributions. First, we present a newly curated Persian dataset,\nproviding a solid foundation for future research in homograph disambiguation.\nSecond, our comparative analysis of embeddings highlights their utility in\ndifferent contexts, enriching the understanding of their capabilities. Third,\nby training and evaluating a spectrum of models, we extend valuable guidance\nfor practitioners in selecting suitable strategies for homograph disambiguation\ntasks. In summary, our study unveils a new dataset, scrutinizes embeddings\nthrough diverse perspectives, and benchmarks various models for homograph\ndisambiguation. These findings empower researchers and practitioners to\nnavigate the intricate landscape of homograph-related challenges effectively.", "published": "2024-05-24 14:56:36", "link": "http://arxiv.org/abs/2406.00028v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clustered Retrieved Augmented Generation (CRAG)", "abstract": "Providing external knowledge to Large Language Models (LLMs) is a key point\nfor using these models in real-world applications for several reasons, such as\nincorporating up-to-date content in a real-time manner, providing access to\ndomain-specific knowledge, and contributing to hallucination prevention. The\nvector database-based Retrieval Augmented Generation (RAG) approach has been\nwidely adopted to this end. Thus, any part of external knowledge can be\nretrieved and provided to some LLM as the input context. Despite RAG approach's\nsuccess, it still might be unfeasible for some applications, because the\ncontext retrieved can demand a longer context window than the size supported by\nLLM. Even when the context retrieved fits into the context window size, the\nnumber of tokens might be expressive and, consequently, impact costs and\nprocessing time, becoming impractical for most applications. To address these,\nwe propose CRAG, a novel approach able to effectively reduce the number of\nprompting tokens without degrading the quality of the response generated\ncompared to a solution using RAG. Through our experiments, we show that CRAG\ncan reduce the number of tokens by at least 46\\%, achieving more than 90\\% in\nsome cases, compared to RAG. Moreover, the number of tokens with CRAG does not\nincrease considerably when the number of reviews analyzed is higher, unlike\nRAG, where the number of tokens is almost 9x higher when there are 75 reviews\ncompared to 4 reviews.", "published": "2024-05-24 16:36:47", "link": "http://arxiv.org/abs/2406.00029v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AMGPT: a Large Language Model for Contextual Querying in Additive\n  Manufacturing", "abstract": "Generalized large language models (LLMs) such as GPT-4 may not provide\nspecific answers to queries formulated by materials science researchers. These\nmodels may produce a high-level outline but lack the capacity to return\ndetailed instructions on manufacturing and material properties of novel alloys.\nEnhancing a smaller model with specialized domain knowledge may provide an\nadvantage over large language models which cannot be retrained quickly enough\nto keep up with the rapid pace of research in metal additive manufacturing\n(AM). We introduce \"AMGPT,\" a specialized LLM text generator designed for metal\nAM queries. The goal of AMGPT is to assist researchers and users in navigating\nthe extensive corpus of literature in AM. Instead of training from scratch, we\nemploy a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented\nGeneration (RAG) setup, utilizing it to dynamically incorporate information\nfrom $\\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert\nthese PDF documents into TeX format, facilitating their integration into the\nRAG pipeline managed by LlamaIndex. Expert evaluations of this project\nhighlight that specific embeddings from the RAG setup accelerate response times\nand maintain coherence in the generated text.", "published": "2024-05-24 20:03:32", "link": "http://arxiv.org/abs/2406.00031v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Understanding of In-Context Learning Ability from\n  In-Context Uncertainty Quantification", "abstract": "Predicting simple function classes has been widely used as a testbed for\ndeveloping theory and understanding of the trained Transformer's in-context\nlearning (ICL) ability. In this paper, we revisit the training of Transformers\non linear regression tasks, and different from all the existing literature, we\nconsider a bi-objective prediction task of predicting both the conditional\nexpectation $\\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This\nadditional uncertainty quantification objective provides a handle to (i) better\ndesign out-of-distribution experiments to distinguish ICL from in-weight\nlearning (IWL) and (ii) make a better separation between the algorithms with\nand without using the prior information of the training distribution.\nTheoretically, we show that the trained Transformer reaches near Bayes-optimum,\nsuggesting the usage of the information of the training distribution. Our\nmethod can be extended to other cases. Specifically, with the Transformer's\ncontext window $S$, we prove a generalization bound of\n$\\tilde{\\mathcal{O}}(\\sqrt{\\min\\{S, T\\}/(n T)})$ on $n$ tasks with sequences of\nlength $T$, providing sharper analysis compared to previous results of\n$\\tilde{\\mathcal{O}}(\\sqrt{1/n})$. Empirically, we illustrate that while the\ntrained Transformer behaves as the Bayes-optimal solution as a natural\nconsequence of supervised training in distribution, it does not necessarily\nperform a Bayesian inference when facing task shifts, in contrast to the\n\\textit{equivalence} between these two proposed in many existing literature. We\nalso demonstrate the trained Transformer's ICL ability over covariates shift\nand prompt-length shift and interpret them as a generalization over a meta\ndistribution.", "published": "2024-05-24 00:08:55", "link": "http://arxiv.org/abs/2405.15115v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "OptLLM: Optimal Assignment of Queries to Large Language Models", "abstract": "Large Language Models (LLMs) have garnered considerable attention owing to\ntheir remarkable capabilities, leading to an increasing number of companies\noffering LLMs as services. Different LLMs achieve different performance at\ndifferent costs. A challenge for users lies in choosing the LLMs that best fit\ntheir needs, balancing cost and performance. In this paper, we propose a\nframework for addressing the cost-effective query allocation problem for LLMs.\nGiven a set of input queries and candidate LLMs, our framework, named OptLLM,\nprovides users with a range of optimal solutions to choose from, aligning with\ntheir budget constraints and performance preferences, including options for\nmaximizing accuracy and minimizing cost. OptLLM predicts the performance of\ncandidate LLMs on each query using a multi-label classification model with\nuncertainty estimation and then iteratively generates a set of non-dominated\nsolutions by destructing and reconstructing the current solution. To evaluate\nthe effectiveness of OptLLM, we conduct extensive experiments on various types\nof tasks, including text classification, question answering, sentiment\nanalysis, reasoning, and log parsing. Our experimental results demonstrate that\nOptLLM substantially reduces costs by 2.40% to 49.18% while achieving the same\naccuracy as the best LLM. Compared to other multi-objective optimization\nalgorithms, OptLLM improves accuracy by 2.94% to 69.05% at the same cost or\nsaves costs by 8.79% and 95.87% while maintaining the highest attainable\naccuracy.", "published": "2024-05-24 01:05:37", "link": "http://arxiv.org/abs/2405.15130v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation\n  Models", "abstract": "Go-Explore is a powerful family of algorithms designed to solve\nhard-exploration problems built on the principle of archiving discovered\nstates, and iteratively returning to and exploring from the most promising\nstates. This approach has led to superhuman performance across a wide variety\nof challenging problems including Atari games and robotic control, but requires\nmanually designing heuristics to guide exploration (i.e., determine which\nstates to save and explore from, and what actions to consider next), which is\ntime-consuming and infeasible in general. To resolve this, we propose\nIntelligent Go-Explore (IGE) which greatly extends the scope of the original\nGo-Explore by replacing these handcrafted heuristics with the intelligence and\ninternalized human notions of interestingness captured by giant pretrained\nfoundation models (FMs). This provides IGE with a human-like ability to\ninstinctively identify how interesting or promising any new state is (e.g.,\ndiscovering new objects, locations, or behaviors), even in complex environments\nwhere heuristics are hard to define. Moreover, IGE offers the exciting\nopportunity to recognize and capitalize on serendipitous discoveries -- states\nencountered during exploration that are valuable in terms of exploration, yet\nwhere what makes them interesting was not anticipated by the human user. We\nevaluate our algorithm on a diverse range of language and vision-based tasks\nthat require search and exploration. Across these tasks, IGE strongly exceeds\nclassic reinforcement learning and graph search baselines, and also succeeds\nwhere prior state-of-the-art FM agents like Reflexion completely fail. Overall,\nIntelligent Go-Explore combines the tremendous strengths of FMs and the\npowerful Go-Explore algorithm, opening up a new frontier of research into\ncreating more generally capable agents with impressive exploration\ncapabilities.", "published": "2024-05-24 01:45:27", "link": "http://arxiv.org/abs/2405.15143v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models", "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.", "published": "2024-05-24 01:49:02", "link": "http://arxiv.org/abs/2405.15145v3", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A Solution-based LLM API-using Methodology for Academic Information\n  Seeking", "abstract": "Applying large language models (LLMs) for academic API usage shows promise in\nreducing researchers' academic information seeking efforts. However, current\nLLM API-using methods struggle with complex API coupling commonly encountered\nin academic queries. To address this, we introduce SoAy, a solution-based LLM\nAPI-using methodology for academic information seeking. It uses code with a\nsolution as the reasoning method, where a solution is a pre-constructed API\ncalling sequence. The addition of the solution reduces the difficulty for the\nmodel to understand the complex relationships between APIs. Code improves the\nefficiency of reasoning.\n  To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied\nby SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental\nresults demonstrate a 34.58-75.99\\% performance improvement compared to\nstate-of-the-art LLM API-based baselines. All datasets, codes, tuned models,\nand deployed online services are publicly accessible at\nhttps://github.com/RUCKBReasoning/SoAy.", "published": "2024-05-24 02:44:14", "link": "http://arxiv.org/abs/2405.15165v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "An Evaluation of Estimative Uncertainty in Large Language Models", "abstract": "Words of estimative probability (WEPs), such as ''maybe'' or ''probably not''\nare ubiquitous in natural language for communicating estimative uncertainty,\ncompared with direct statements involving numerical probability. Human\nestimative uncertainty, and its calibration with numerical estimates, has long\nbeen an area of study -- including by intelligence agencies like the CIA. This\nstudy compares estimative uncertainty in commonly used large language models\n(LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other. Here we\nshow that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but\nnot all, WEPs presented in English. Divergence is also observed when the LLM is\npresented with gendered roles and Chinese contexts. Further study shows that an\nadvanced LLM like GPT-4 can consistently map between statistical and estimative\nuncertainty, but a significant performance gap remains. The results contribute\nto a growing body of research on human-LLM alignment.", "published": "2024-05-24 03:39:31", "link": "http://arxiv.org/abs/2405.15185v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Denoising LM: Pushing the Limits of Error Correction Models for Speech\n  Recognition", "abstract": "Language models (LMs) have long been used to improve results of automatic\nspeech recognition (ASR) systems, but they are unaware of the errors that ASR\nsystems make. Error correction models are designed to fix ASR errors, however,\nthey showed little improvement over traditional LMs mainly due to the lack of\nsupervised training data. In this paper, we present Denoising LM (DLM), which\nis a $\\textit{scaled}$ error correction model trained with vast amounts of\nsynthetic data, significantly exceeding prior attempts meanwhile achieving new\nstate-of-the-art ASR performance. We use text-to-speech (TTS) systems to\nsynthesize audio, which is fed into an ASR system to produce noisy hypotheses,\nwhich are then paired with the original texts to train the DLM. DLM has several\n$\\textit{key ingredients}$: (i) up-scaled model and data; (ii) usage of\nmulti-speaker TTS systems; (iii) combination of multiple noise augmentation\nstrategies; and (iv) new decoding techniques. With a Transformer-CTC ASR, DLM\nachieves 1.5% word error rate (WER) on $\\textit{test-clean}$ and 3.3% WER on\n$\\textit{test-other}$ on Librispeech, which to our knowledge are the best\nreported numbers in the setting where no external audio data are used and even\nmatch self-supervised methods which use external audio data. Furthermore, a\nsingle DLM is applicable to different ASRs, and greatly surpassing the\nperformance of conventional LM based beam-search rescoring. These results\nindicate that properly investigated error correction models have the potential\nto replace conventional LMs, holding the key to a new level of accuracy in ASR\nsystems.", "published": "2024-05-24 05:05:12", "link": "http://arxiv.org/abs/2405.15216v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "The Buffer Mechanism for Multi-Step Information Reasoning in Language\n  Models", "abstract": "Large language models have consistently struggled with complex reasoning\ntasks, such as mathematical problem-solving. Investigating the internal\nreasoning mechanisms of these models can help us design better model\narchitectures and training strategies, ultimately enhancing their reasoning\ncapability. In this study, we constructed a symbolic dataset to investigate the\nmechanisms by which Transformer models employ vertical thinking strategy based\non their inherent structure and horizontal thinking strategy based on Chain of\nThought to achieve multi-step reasoning. We introduced the concept of buffer\nmechanism: the model stores various information in distinct buffers and\nselectively extracts them through the query-key matrix. We proposed a random\nmatrix-based algorithm to enhance the model's reasoning ability, resulting in a\n75% reduction in the training time required for the GPT-2 model to achieve\ngeneralization capability on the PrOntoQA dataset. These findings provide new\ninsights into understanding the mechanisms of large language models.", "published": "2024-05-24 07:41:26", "link": "http://arxiv.org/abs/2405.15302v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "BiSup: Bidirectional Quantization Error Suppression for Large Language\n  Models", "abstract": "As the size and context length of Large Language Models (LLMs) grow,\nweight-activation quantization has emerged as a crucial technique for efficient\ndeployment of LLMs. Compared to weight-only quantization, weight-activation\nquantization presents greater challenges due to the presence of outliers in\nactivations. Existing methods have made significant progress by exploring\nmixed-precision quantization and outlier suppression. However, these methods\nprimarily focus on optimizing the results of single matrix multiplication,\nneglecting the bidirectional propagation of quantization errors in LLMs.\nSpecifically, errors accumulate vertically within the same token through\nlayers, and diffuse horizontally across different tokens due to self-attention\nmechanisms. To address this issue, we introduce BiSup, a Bidirectional\nquantization error Suppression method. By constructing appropriate optimizable\nparameter spaces, BiSup utilizes a small amount of data for quantization-aware\nparameter-efficient fine-tuning to suppress the error vertical accumulation.\nBesides, BiSup employs prompt mixed-precision quantization strategy, which\npreserves high precision for the key-value cache of system prompts, to mitigate\nthe error horizontal diffusion. Extensive experiments on Llama and Qwen\nfamilies demonstrate that BiSup can improve performance over two\nstate-of-the-art methods (the average WikiText2 perplexity decreases from 13.26\nto 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128\nconfiguration), further facilitating the practical applications of low-bit\nweight-activation quantization.", "published": "2024-05-24 08:39:27", "link": "http://arxiv.org/abs/2405.15346v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pipeline Parallelism with Controllable Memory", "abstract": "Pipeline parallelism has been widely explored, but most existing schedules\nlack a systematic methodology. In this paper, we propose a framework to\ndecompose pipeline schedules as repeating a building block, and show that the\nlifespan of the building block decides the peak activation memory of the\npipeline schedule. Guided by the observations, we find that almost all existing\npipeline schedules, to the best of our knowledge, are memory inefficient. To\naddress this, we introduce a family of memory efficient building blocks with\ncontrollable activation memory, which can reduce the peak activation memory to\n1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable\nthroughput. We can also achieve almost zero pipeline bubbles while maintaining\nthe same activation memory as 1F1B. Our evaluations demonstrate that in pure\npipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in\nterms of throughput. When employing a grid search over hybrid parallelism\nhyperparameters in practical scenarios, our methods demonstrate a 16%\nthroughput improvement over the 1F1B baseline for large language models. The\nimplementation is open-sourced at\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism.", "published": "2024-05-24 08:54:36", "link": "http://arxiv.org/abs/2405.15362v4", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Leveraging Large Language Models for Semantic Query Processing in a\n  Scholarly Knowledge Graph", "abstract": "The proposed research aims to develop an innovative semantic query processing\nsystem that enables users to obtain comprehensive information about research\nworks produced by Computer Science (CS) researchers at the Australian National\nUniversity (ANU). The system integrates Large Language Models (LLMs) with the\nANU Scholarly Knowledge Graph (ASKG), a structured repository of all\nresearch-related artifacts produced at ANU in the CS field. Each artifact and\nits parts are represented as textual nodes stored in a Knowledge Graph (KG).\n  To address the limitations of traditional scholarly KG construction and\nutilization methods, which often fail to capture fine-grained details, we\npropose a novel framework that integrates the Deep Document Model (DDM) for\ncomprehensive document representation and the KG-enhanced Query Processing\n(KGQP) for optimized complex query handling. DDM enables a fine-grained\nrepresentation of the hierarchical structure and semantic relationships within\nacademic papers, while KGQP leverages the KG structure to improve query\naccuracy and efficiency with LLMs.\n  By combining the ASKG with LLMs, our approach enhances knowledge utilization\nand natural language understanding capabilities. The proposed system employs an\nautomatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from\nthe ASKG. Initial experiments demonstrate that our framework is superior to\nbaseline methods in terms of accuracy retrieval and query efficiency.\n  We showcase the practical application of our framework in academic research\nscenarios, highlighting its potential to revolutionize scholarly knowledge\nmanagement and discovery. This work empowers researchers to acquire and utilize\nknowledge from documents more effectively and provides a foundation for\ndeveloping precise and reliable interactions with LLMs.", "published": "2024-05-24 09:19:45", "link": "http://arxiv.org/abs/2405.15374v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3; I.2.4; I.7.5; I.2.7"], "primary_category": "cs.IR"}
{"title": "Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top", "abstract": "Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key\nchallenge in Large Language Models (LLMs). While best-performing solutions in\nthis domain use a plan and solve paradigm to split a question into\nsub-questions followed by response generation, we claim that this approach is\nsub-optimal as it fails for hard to decompose questions, and it does not\nexplicitly cater to correlated knowledge updates resulting as a consequence of\nknowledge edits. This has a detrimental impact on the overall consistency of\nthe updated knowledge. To address these issues, in this paper, we propose a\nnovel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a\ncherry on the top for augmenting the performance of all existing MQA methods\nunder KE. Specifically, RULE-KE leverages rule discovery to discover a set of\nlogical rules. Then, it uses these discovered rules to update knowledge about\nfacts highly correlated with the edit. Experimental evaluation using existing\nand newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment\nboth performances of parameter-based and memory-based solutions up to 92% and\n112.9%, respectively.", "published": "2024-05-24 11:30:00", "link": "http://arxiv.org/abs/2405.15452v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linearly Controlled Language Generation with Performative Guarantees", "abstract": "The increasing prevalence of Large Language Models (LMs) in critical\napplications highlights the need for controlled language generation strategies\nthat are not only computationally efficient but that also enjoy performance\nguarantees. To achieve this, we use a common model of concept semantics as\nlinearly represented in an LM's latent space. In particular, we take the view\nthat natural language generation traces a trajectory in this continuous\nsemantic space, realized by the language model's hidden activations. This view\npermits a control-theoretic treatment of text generation in latent space, in\nwhich we propose a lightweight, gradient-free intervention that dynamically\nsteers trajectories away from regions corresponding to undesired meanings.\nCrucially, we show that this intervention, which we compute in closed form, is\nguaranteed (in probability) to steer the output into the allowed region.\nFinally, we demonstrate on a toxicity avoidance objective that the intervention\nsteers language away from undesired content while maintaining text quality.", "published": "2024-05-24 11:30:44", "link": "http://arxiv.org/abs/2405.15454v1", "categories": ["cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in\n  LLMs", "abstract": "We are beginning to see progress in language model assisted scientific\ndiscovery. Motivated by the use of LLMs as a general scientific assistant, this\npaper assesses the domain knowledge of LLMs through its understanding of\ndifferent mathematical skills required to solve problems. In particular, we\nlook at not just what the pre-trained model already knows, but how it learned\nto learn from information during in-context learning or instruction-tuning\nthrough exploiting the complex knowledge structure within mathematics.\nMotivated by the Neural Tangent Kernel (NTK), we propose \\textit{NTKEval} to\nassess changes in LLM's probability distribution via training on different\nkinds of math data. Our systematic analysis finds evidence of domain\nunderstanding during in-context learning. By contrast, certain\ninstruction-tuning leads to similar performance changes irrespective of\ntraining on different data, suggesting a lack of domain understanding across\ndifferent skills.", "published": "2024-05-24 12:04:54", "link": "http://arxiv.org/abs/2405.15485v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Certifiably Robust RAG against Retrieval Corruption", "abstract": "Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval\ncorruption attacks: an attacker can inject malicious passages into retrieval\nresults to induce inaccurate responses. In this paper, we propose RobustRAG as\nthe first defense framework against retrieval corruption attacks. The key\ninsight of RobustRAG is an isolate-then-aggregate strategy: we get LLM\nresponses from each passage in isolation and then securely aggregate these\nisolated responses. To instantiate RobustRAG, we design keyword-based and\ndecoding-based algorithms for securely aggregating unstructured text responses.\nNotably, RobustRAG can achieve certifiable robustness: we can formally prove\nand certify that, for certain queries, RobustRAG can always return accurate\nresponses, even when the attacker has full knowledge of our defense and can\narbitrarily inject a small number of malicious passages. We evaluate RobustRAG\non open-domain QA and long-form text generation datasets and demonstrate its\neffectiveness and generalizability across various tasks and datasets.", "published": "2024-05-24 13:44:25", "link": "http://arxiv.org/abs/2405.15556v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Visual Description Grounding Reduces Hallucinations and Boosts Reasoning\n  in LVLMs", "abstract": "Large Vision-Language Models (LVLMs) often produce responses that misalign\nwith factual information, a phenomenon known as hallucinations. While\nhallucinations are well-studied, the exact causes behind them remain\nunderexplored. In this paper, we first investigate the root causes of\nhallucinations in LVLMs. Our findings reveal that existing mitigation\ntechniques primarily reduce hallucinations for visual recognition prompts-those\nthat require simple descriptions of visual elements-but fail for cognitive\nprompts that demand deliberate reasoning. We identify the core issue as a lack\nof true visual perception in LVLMs: although they can accurately recognize\nvisual elements, they struggle to fully interpret these elements in the context\nof the input prompt and effectively link this recognition to their internal\nknowledge, which is critical for reasoning. To address this gap, we introduce\nVisual Description Grounded Decoding (VDGD), a simple, robust, and\ntraining-free method designed to enhance visual perception and improve\nreasoning capabilities in LVLMs. VDGD works by first generating a detailed\ndescription of the image and appending it as a prefix to the instruction.\nDuring response generation, tokens are sampled based on their KL divergence to\nthe description, favoring candidates with lower divergence. Experimental\nresults on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD\nconsistently outperforms existing baselines 2% - 33%. Finally, we introduce\nVaLLu, a benchmark designed for comprehensive evaluation of the cognitive\ncapabilities of LVLMs.", "published": "2024-05-24 16:21:59", "link": "http://arxiv.org/abs/2405.15683v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Optimizing Large Language Models for OpenAPI Code Completion", "abstract": "Recent advancements in Large Language Models (LLMs) and their utilization in\ncode generation tasks have significantly reshaped the field of software\ndevelopment. Despite the remarkable efficacy of code completion solutions in\nmainstream programming languages, their performance lags when applied to less\nubiquitous formats such as OpenAPI definitions. This study evaluates the\nOpenAPI completion performance of GitHub Copilot, a prevalent commercial code\ncompletion tool, and proposes a set of task-specific optimizations leveraging\nMeta's open-source model Code Llama. A semantics-aware OpenAPI completion\nbenchmark proposed in this research is used to perform a series of experiments\nthrough which the impact of various prompt-engineering and fine-tuning\ntechniques on the Code Llama model's performance is analyzed. The fine-tuned\nCode Llama model reaches a peak correctness improvement of 55.2% over GitHub\nCopilot despite utilizing 25 times fewer parameters than the commercial\nsolution's underlying Codex model. Additionally, this research proposes an\nenhancement to a widely used code infilling training technique, addressing the\nissue of underperformance when the model is prompted with context sizes smaller\nthan those used during training. The dataset, the benchmark, and the model\nfine-tuning code are made publicly available.", "published": "2024-05-24 17:19:03", "link": "http://arxiv.org/abs/2405.15729v2", "categories": ["cs.SE", "cs.CL", "cs.LG", "68T07, 68T50, 68T05", "I.2.2; I.2.6; I.2.7; D.1.2; D.2.1; D.2.3; D.2.6"], "primary_category": "cs.SE"}
{"title": "Filtered Corpus Training (FiCT) Shows that Language Models can\n  Generalize from Indirect Evidence", "abstract": "This paper introduces Filtered Corpus Training, a method that trains language\nmodels (LMs) on corpora with certain linguistic constructions filtered out from\nthe training data, and uses it to measure the ability of LMs to perform\nlinguistic generalization on the basis of indirect evidence. We apply the\nmethod to both LSTM and Transformer LMs (of roughly comparable size),\ndeveloping filtered corpora that target a wide range of linguistic phenomena.\nOur results show that while transformers are better qua LMs (as measured by\nperplexity), both models perform equally and surprisingly well on linguistic\ngeneralization measures, suggesting that they are capable of generalizing from\nindirect evidence.", "published": "2024-05-24 17:47:20", "link": "http://arxiv.org/abs/2405.15750v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus\n  Creation and Model Development", "abstract": "The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance,\nenhancing patient safety by identifying potential risks associated with\nmedications, facilitating early detection of adverse events, and guiding\nregulatory decision-making. Traditional ADE detection methods are reliable but\nslow, not easily adaptable to large-scale operations, and offer limited\ninformation. With the exponential increase in data sources like social media\ncontent, biomedical literature, and Electronic Medical Records (EMR),\nextracting relevant ADE-related information from these unstructured texts is\nimperative. Previous ADE mining studies have focused on text-based\nmethodologies, overlooking visual cues, limiting contextual comprehension, and\nhindering accurate interpretation. To address this gap, we present a MultiModal\nAdverse Drug Event (MMADE) detection dataset, merging ADE-related textual\ninformation with visual aids. Additionally, we introduce a framework that\nleverages the capabilities of LLMs and VLMs for ADE detection by generating\ndetailed descriptions of medical images depicting ADEs, aiding healthcare\nprofessionals in visually identifying adverse events. Using our MMADE dataset,\nwe showcase the significance of integrating visual cues from images to enhance\noverall performance. This approach holds promise for patient safety, ADE\nawareness, and healthcare accessibility, paving the way for further exploration\nin personalized healthcare.", "published": "2024-05-24 17:58:42", "link": "http://arxiv.org/abs/2405.15766v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications", "abstract": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques.", "published": "2024-05-24 18:40:20", "link": "http://arxiv.org/abs/2405.15877v1", "categories": ["cs.LG", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs", "abstract": "The recent leaps in complexity and fluency of Large Language Models (LLMs)\nmean that, for the first time in human history, people can interact with\ncomputers using natural language alone. This creates monumental possibilities\nof automation and accessibility of computing, but also raises severe security\nand safety threats: When everyone can interact with LLMs, everyone can\npotentially break into the systems running LLMs. All it takes is creative use\nof language. This paper presents Hacc-Man, a game which challenges its players\nto \"jailbreak\" an LLM: subvert the LLM to output something that it is not\nintended to. Jailbreaking is at the intersection between creative problem\nsolving and LLM security. The purpose of the game is threefold: 1. To heighten\nawareness of the risks of deploying fragile LLMs in everyday systems, 2. To\nheighten people's self-efficacy in interacting with LLMs, and 3. To discover\nthe creative problem solving strategies, people deploy in this novel context.", "published": "2024-05-24 19:55:20", "link": "http://arxiv.org/abs/2405.15902v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CR"}
{"title": "Enhancing Visual-Language Modality Alignment in Large Vision Language\n  Models via Self-Improvement", "abstract": "Large vision-language models (LVLMs) have achieved impressive results in\nvisual question-answering and reasoning tasks through vision instruction tuning\non specific datasets. However, there remains significant room for improvement\nin aligning visual and language modalities. Existing methods often depend on\nexternal models or data, leading to uncontrollable and unstable alignment\nresults. In this paper, we propose SIMA, a self-improvement framework that\nenhances visual and language modality alignment without external dependencies.\nSIMA leverages existing vision instruction tuning datasets to self-generate\nresponses, incorporating an in-context self-critic mechanism that constructs\npreference pairs for tuning. Crucially, our approach allows LVLMs to act as\ncritics by designing effective critic prompts, eliminating the need for\nadditional fine-tuning with external instruction data. We introduce three novel\nvisual metrics within the self-critic process to guide judgment, significantly\nimproving the accuracy of self-critic. Through extensive experiments across 14\nhallucination and comprehensive benchmarks, we demonstrate that SIMA\nsignificantly improves LVLM's performance and outperforms previous approaches,\nachieving superior modality alignment.", "published": "2024-05-24 23:09:27", "link": "http://arxiv.org/abs/2405.15973v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Athena: Efficient Block-Wise Post-Training Quantization for Large\n  Language Models Using Second-Order Matrix Derivative Information", "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing tasks such as machine translation, text generation, and sentiment\nanalysis. However, their large size, often consisting of billions of\nparameters, poses challenges for storage, computation, and deployment,\nparticularly in resource-constrained environments like mobile devices and edge\ncomputing platforms. Effective compression and quantization techniques are\ncrucial for addressing these issues, reducing memory footprint and\ncomputational requirements without significantly compromising performance.\nTraditional methods that uniformly map parameters to compressed spaces fail to\naccount for the uneven distribution of parameters, leading to substantial\naccuracy loss. In this work, we propose Athena, a novel algorithm for efficient\nblock-wise post-training quantization of LLMs. Athena leverages Second-Order\nMatrix Derivative Information to guide the quantization process using the\ncurvature information of the loss landscape. By grouping parameters by columns\nor rows and iteratively optimizing the quantization process, Athena updates the\nmodel parameters and Hessian matrix to achieve significant compression while\nmaintaining high accuracy. This makes Athena a practical solution for deploying\nLLMs in various settings.", "published": "2024-05-24 03:14:29", "link": "http://arxiv.org/abs/2405.17470v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Culturally Aware are Vision-Language Models?", "abstract": "An image is often considered worth a thousand words, and certain images can\ntell rich and insightful stories. Can these stories be told via image\ncaptioning? Images from folklore genres, such as mythology, folk dance,\ncultural signs, and symbols, are vital to every culture. Our research compares\nthe performance of four popular vision-language models (GPT-4V, Gemini Pro\nVision, LLaVA, and OpenFlamingo) in identifying culturally specific information\nin such images and creating accurate and culturally sensitive image captions.\nWe also propose a new evaluation metric, the Cultural Awareness Score (CAS),\nwhich measures the degree of cultural awareness in image captions. We provide a\ndataset MOSAIC-1.5k labeled with ground truth for images containing cultural\nbackground and context and a labeled dataset with assigned Cultural Awareness\nScores that can be used with unseen data. Creating culturally appropriate image\ncaptions is valuable for scientific research and can be beneficial for many\npractical applications. We envision our work will promote a deeper integration\nof cultural sensitivity in AI applications worldwide. By making the dataset and\nCultural Awareness Score available to the public, we aim to facilitate further\nresearch in this area, encouraging the development of more culturally aware AI\nsystems that respect and celebrate global diversity.", "published": "2024-05-24 04:45:14", "link": "http://arxiv.org/abs/2405.17475v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "abstract": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.", "published": "2024-05-24 07:23:56", "link": "http://arxiv.org/abs/2405.20770v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Embedding-Aligned Language Models", "abstract": "We propose a novel approach for training large language models (LLMs) to\nadhere to objectives defined within a latent embedding space. Our method\nleverages reinforcement learning (RL), treating a pre-trained LLM as an\nenvironment. Our embedding-aligned guided language (EAGLE) agent is trained to\niteratively steer the LLM's generation towards optimal regions of the latent\nembedding space, w.r.t. some predefined criterion. We demonstrate the\neffectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review\ndatasets to surface content gaps that satisfy latent user demand. We also\ndemonstrate the benefit of using an optimal design of a state-dependent action\nset to improve EAGLE's efficiency. Our work paves the way for controlled and\ngrounded text generation using LLMs, ensuring consistency with domain-specific\nknowledge and data representations.", "published": "2024-05-24 06:11:17", "link": "http://arxiv.org/abs/2406.00024v2", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting PromptORE for Modern History: Information Extraction from\n  Hispanic Monarchy Documents of the XVIth Century", "abstract": "Semantic relations among entities are a widely accepted method for relation\nextraction. PromptORE (Prompt-based Open Relation Extraction) was designed to\nimprove relation extraction with Large Language Models on generalistic\ndocuments. However, it is less effective when applied to historical documents,\nin languages other than English. In this study, we introduce an adaptation of\nPromptORE to extract relations from specialized documents, namely digital\ntranscripts of trials from the Spanish Inquisition. Our approach involves\nfine-tuning transformer models with their pretraining objective on the data\nthey will perform inference. We refer to this process as \"biasing\". Our Biased\nPromptORE addresses complex entity placements and genderism that occur in\nSpanish texts. We solve these issues by prompt engineering. We evaluate our\nmethod using Encoder-like models, corroborating our findings with experts'\nassessments. Additionally, we evaluate the performance using a binomial\nclassification benchmark. Our results show a substantial improvement in\naccuracy -up to a 50% improvement with our Biased PromptORE models in\ncomparison to the baseline models using standard PromptORE.", "published": "2024-05-24 13:39:47", "link": "http://arxiv.org/abs/2406.00027v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3; H.3.2; H.4; I.7; I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Language Model Pruning", "abstract": "We surely enjoy the larger the better models for their superior performance\nin the last couple of years when both the hardware and software support the\nbirth of such extremely huge models. The applied fields include text mining and\nothers. In particular, the success of LLMs on text understanding and text\ngeneration draws attention from researchers who have worked on NLP and related\nareas for years or even decades. On the side, LLMs may suffer from problems\nlike model overfitting, hallucination, and device limitation to name a few. In\nthis work, we suggest a model pruning technique specifically focused on LLMs.\nThe proposed methodology emphasizes the explainability of deep learning models.\nBy having the theoretical foundation, we obtain a trustworthy deep model so\nthat huge models with a massive number of model parameters become not quite\nnecessary. A mutual information-based estimation is adopted to find neurons\nwith redundancy to eliminate. Moreover, an estimator with well-tuned parameters\nhelps to find precise estimation to guide the pruning procedure. At the same\ntime, we also explore the difference between pruning on large-scale models vs.\npruning on small-scale models. The choice of pruning criteria is sensitive in\nsmall models but not for large-scale models. It is a novel finding through this\nwork. Overall, we demonstrate the superiority of the proposed model to the\nstate-of-the-art models.", "published": "2024-05-24 18:22:15", "link": "http://arxiv.org/abs/2406.00030v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bayesian WeakS-to-Strong from Text Classification to Generation", "abstract": "Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.", "published": "2024-05-24 13:33:11", "link": "http://arxiv.org/abs/2406.03199v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "E2Vec: Feature Embedding with Temporal Information for Analyzing Student\n  Actions in E-Book Systems", "abstract": "Digital textbook (e-book) systems record student interactions with textbooks\nas a sequence of events called EventStream data. In the past, researchers\nextracted meaningful features from EventStream, and utilized them as inputs for\ndownstream tasks such as grade prediction and modeling of student behavior.\nPrevious research evaluated models that mainly used statistical-based features\nderived from EventStream logs, such as the number of operation types or access\nfrequencies. While these features are useful for providing certain insights,\nthey lack temporal information that captures fine-grained differences in\nlearning behaviors among different students. This study proposes E2Vec, a novel\nfeature representation method based on word embeddings. The proposed method\nregards operation logs and their time intervals for each student as a string\nsequence of characters and generates a student vector of learning activity\nfeatures that incorporates time information. We applied fastText to generate an\nembedding vector for each of 305 students in a dataset from two years of\ncomputer science courses. Then, we investigated the effectiveness of E2Vec in\nan at-risk detection task, demonstrating potential for generalizability and\nperformance.", "published": "2024-05-24 10:17:43", "link": "http://arxiv.org/abs/2407.13053v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "SoundLoCD: An Efficient Conditional Discrete Contrastive Latent\n  Diffusion Model for Text-to-Sound Generation", "abstract": "We present SoundLoCD, a novel text-to-sound generation framework, which\nincorporates a LoRA-based conditional discrete contrastive latent diffusion\nmodel. Unlike recent large-scale sound generation models, our model can be\nefficiently trained under limited computational resources. The integration of a\ncontrastive learning strategy further enhances the connection between text\nconditions and the generated outputs, resulting in coherent and high-fidelity\nperformance. Our experiments demonstrate that SoundLoCD outperforms the\nbaseline with greatly reduced computational resources. A comprehensive ablation\nstudy further validates the contribution of each component within SoundLoCD.\nDemo page: \\url{https://XinleiNIU.github.io/demo-SoundLoCD/}.", "published": "2024-05-24 08:18:58", "link": "http://arxiv.org/abs/2405.15338v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HiddenSpeaker: Generate Imperceptible Unlearnable Audios for Speaker\n  Verification System", "abstract": "In recent years, the remarkable advancements in deep neural networks have\nbrought tremendous convenience. However, the training process of a highly\neffective model necessitates a substantial quantity of samples, which brings\nhuge potential threats, like unauthorized exploitation with privacy leakage. In\nresponse, we propose a framework named HiddenSpeaker, embedding imperceptible\nperturbations within the training speech samples and rendering them unlearnable\nfor deep-learning-based speaker verification systems that employ large-scale\nspeakers for efficient training. The HiddenSpeaker utilizes a simplified\nerror-minimizing method named Single-Level Error-Minimizing (SLEM) to generate\nspecific and effective perturbations. Additionally, a hybrid objective function\nis employed for human perceptual optimization, ensuring the perturbation is\nindistinguishable from human listeners. We conduct extensive experiments on\nmultiple state-of-the-art (SOTA) models in the speaker verification domain to\nevaluate HiddenSpeaker. Our results demonstrate that HiddenSpeaker not only\ndeceives the model with unlearnable samples but also enhances the\nimperceptibility of the perturbations, showcasing strong transferability across\ndifferent models.", "published": "2024-05-24 15:49:00", "link": "http://arxiv.org/abs/2405.15655v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music\n  Generation", "abstract": "In recent years, diffusion-based text-to-music (TTM) generation has gained\nprominence, offering an innovative approach to synthesizing musical content\nfrom textual descriptions. Achieving high accuracy and diversity in this\ngeneration process requires extensive, high-quality data, including both\nhigh-fidelity audio waveforms and detailed text descriptions, which often\nconstitute only a small portion of available datasets. In open-source datasets,\nissues such as low-quality music waveforms, mislabeling, weak labeling, and\nunlabeled data significantly hinder the development of music generation models.\nTo address these challenges, we propose a novel paradigm for high-quality music\ngeneration that incorporates a quality-aware training strategy, enabling\ngenerative models to discern the quality of input music waveforms during\ntraining. Leveraging the unique properties of musical signals, we first adapted\nand implemented a masked diffusion transformer (MDT) model for the TTM task,\ndemonstrating its distinct capacity for quality control and enhanced\nmusicality. Additionally, we address the issue of low-quality captions in TTM\nwith a caption refinement data processing approach. Experiments demonstrate our\nstate-of-the-art (SOTA) performance on MusicCaps and the Song-Describer\nDataset. Our demo page can be accessed at https://qa-mdt.github.io/.", "published": "2024-05-24 18:09:27", "link": "http://arxiv.org/abs/2405.15863v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spiketrum: An FPGA-based Implementation of a Neuromorphic Cochlea", "abstract": "This paper presents a novel FPGA-based neuromorphic cochlea, leveraging the\ngeneral-purpose spike-coding algorithm, Spiketrum. The focus of this study is\non the development and characterization of this cochlea model, which excels in\ntransforming audio vibrations into biologically realistic auditory spike\ntrains. These spike trains are designed to withstand neural fluctuations and\nspike losses while accurately encapsulating the spatial and precise temporal\ncharacteristics of audio, along with the intensity of incoming vibrations.\nNoteworthy features include the ability to generate real-time spike trains with\nminimal information loss and the capacity to reconstruct original signals. This\nfine-tuning capability allows users to optimize spike rates, achieving an\noptimal balance between output quality and power consumption. Furthermore, the\nintegration of a feedback system into Spiketrum enables selective amplification\nof specific features while attenuating others, facilitating adaptive power\nconsumption based on application requirements. The hardware implementation\nsupports both spike-based and non-spike-based processors, making it versatile\nfor various computing systems. The cochlea's ability to encode diverse sensory\ninformation, extending beyond sound waveforms, positions it as a promising\nsensory input for current and future spike-based intelligent computing systems,\noffering compact and real-time spike train generation.", "published": "2024-05-24 20:31:47", "link": "http://arxiv.org/abs/2405.15923v3", "categories": ["eess.SP", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
