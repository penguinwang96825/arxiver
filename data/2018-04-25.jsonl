{"title": "Gender Bias in Coreference Resolution", "abstract": "We present an empirical study of gender bias in coreference resolution\nsystems. We first introduce a novel, Winograd schema-style set of minimal pair\nsentences that differ only by pronoun gender. With these \"Winogender schemas,\"\nwe evaluate and confirm systematic gender bias in three publicly-available\ncoreference resolution systems, and correlate this bias with real-world and\ntextual gender statistics.", "published": "2018-04-25 00:46:14", "link": "http://arxiv.org/abs/1804.09301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical RNN for Information Extraction from Lawsuit Documents", "abstract": "Every lawsuit document contains the information about the party's claim,\ncourt's analysis, decision and others, and all of this information are helpful\nto understand the case better and predict the judge's decision on similar case\nin the future. However, the extraction of these information from the document\nis difficult because the language is too complicated and sentences varied at\nlength. We treat this problem as a task of sequence labeling, and this paper\npresents the first research to extract relevant information from the civil\nlawsuit document in China with the hierarchical RNN framework.", "published": "2018-04-25 02:18:51", "link": "http://arxiv.org/abs/1804.09321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factors Influencing the Surprising Instability of Word Embeddings", "abstract": "Despite the recent popularity of word embedding methods, there is only a\nsmall body of work exploring the limitations of these representations. In this\npaper, we consider one aspect of embedding spaces, namely their stability. We\nshow that even relatively high frequency words (100-200 occurrences) are often\nunstable. We provide empirical evidence for how various factors contribute to\nthe stability of word embeddings, and we analyze the effects of stability on\ndownstream tasks.", "published": "2018-04-25 17:40:20", "link": "http://arxiv.org/abs/1804.09692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation", "abstract": "Interacting with relational databases through natural language helps users of\nany background easily query and analyze a vast amount of data. This requires a\nsystem that understands users' questions and converts them to SQL queries\nautomatically. In this paper we present a novel approach, TypeSQL, which views\nthis problem as a slot filling task. Additionally, TypeSQL utilizes type\ninformation to better understand rare entities and numbers in natural language\nquestions. We test this idea on the WikiSQL dataset and outperform the prior\nstate-of-the-art by 5.5% in much less time. We also show that accessing the\ncontent of databases can significantly improve the performance when users'\nqueries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute\nimprovement compared to the previous content-sensitive model.", "published": "2018-04-25 19:35:56", "link": "http://arxiv.org/abs/1804.09769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Evaluation of Semantic Phenomena in Neural Machine Translation\n  Using Natural Language Inference", "abstract": "We propose a process for investigating the extent to which sentence\nrepresentations arising from neural machine translation (NMT) systems encode\ndistinct semantic phenomena. We use these representations as features to train\na natural language inference (NLI) classifier based on datasets recast from\nexisting semantic annotations. In applying this process to a representative NMT\nsystem, we find its encoder appears most suited to supporting inferences at the\nsyntax-semantics interface, as compared to anaphora resolution requiring\nworld-knowledge. We conclude with a discussion on the merits and potential\ndeficiencies of the existing process, and how it may be improved and extended\nas a broader framework for evaluating semantic coverage.", "published": "2018-04-25 20:03:09", "link": "http://arxiv.org/abs/1804.09779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP\n  Applications", "abstract": "Peer reviewing is a central component in the scientific publishing process.\nWe present the first public dataset of scientific peer reviews available for\nresearch purposes (PeerRead v1) providing an opportunity to study this\nimportant artifact. The dataset consists of 14.7K paper drafts and the\ncorresponding accept/reject decisions in top-tier venues including ACL, NIPS\nand ICLR. The dataset also includes 10.7K textual peer reviews written by\nexperts for a subset of the papers. We describe the data collection process and\nreport interesting observed phenomena in the peer reviews. We also propose two\nnovel NLP tasks based on this dataset and provide simple baseline models. In\nthe first task, we show that simple models can predict whether a paper is\naccepted with up to 21% error reduction compared to the majority baseline. In\nthe second task, we predict the numerical scores of review aspects and show\nthat simple models can outperform the mean baseline for aspects with high\nvariance such as 'originality' and 'impact'.", "published": "2018-04-25 15:41:15", "link": "http://arxiv.org/abs/1804.09635v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalized Language Model for Query Auto-Completion", "abstract": "Query auto-completion is a search engine feature whereby the system suggests\ncompleted queries as the user types. Recently, the use of a recurrent neural\nnetwork language model was suggested as a method of generating query\ncompletions. We show how an adaptable language model can be used to generate\npersonalized completions and how the model can use online updating to make\npredictions for users not seen during training. The personalized predictions\nare significantly better than a baseline that uses no user information.", "published": "2018-04-25 16:26:39", "link": "http://arxiv.org/abs/1804.09661v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cheap Non-standard Analysis and Computability", "abstract": "Non standard analysis is an area of Mathematics dealing with notions of\ninfinitesimal and infinitely large numbers, in which many statements from\nclassical analysis can be expressed very naturally. Cheap non-standard analysis\nintroduced by Terence Tao in 2012 is based on the idea that considering that a\nproperty holds eventually is sufficient to give the essence of many of its\nstatements. This provides constructivity but at some (acceptable) price. We\nconsider computability in cheap non-standard analysis. We prove that many\nconcepts from computable analysis as well as several concepts from\ncomputability can be very elegantly and alternatively presented in this\nframework. It provides a dual view and dual proofs to several statements\nalready known in these fields.", "published": "2018-04-25 18:36:17", "link": "http://arxiv.org/abs/1804.09746v2", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Recent Progresses in Deep Learning based Acoustic Models (Updated)", "abstract": "In this paper, we summarize recent progresses made in deep learning based\nacoustic models and the motivation and insights behind the surveyed techniques.\nWe first discuss acoustic models that can effectively exploit variable-length\ncontextual information, such as recurrent neural networks (RNNs), convolutional\nneural networks (CNNs), and their various combination with other models. We\nthen describe acoustic models that are optimized end-to-end with emphasis on\nfeature representations learned jointly with rest of the system, the\nconnectionist temporal classification (CTC) criterion, and the attention-based\nsequence-to-sequence model. We further illustrate robustness issues in speech\nrecognition systems, and discuss acoustic model adaptation, speech enhancement\nand separation, and robust training strategies. We also cover modeling\ntechniques that lead to more efficient decoding and discuss possible future\ndirections in acoustic model research.", "published": "2018-04-25 00:24:39", "link": "http://arxiv.org/abs/1804.09298v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models", "abstract": "Neural Sequence-to-Sequence models have proven to be accurate and robust for\nmany sequence prediction tasks, and have become the standard approach for\nautomatic translation of text. The models work in a five stage blackbox process\nthat involves encoding a source sequence to a vector space and then decoding\nout to a new target sequence. This process is now standard, but like many deep\nlearning methods remains quite difficult to understand or debug. In this work,\nwe present a visual analysis tool that allows interaction with a trained\nsequence-to-sequence model through each stage of the translation process. The\naim is to identify which patterns have been learned and to detect model errors.\nWe demonstrate the utility of our tool through several real-world large-scale\nsequence-to-sequence use cases.", "published": "2018-04-25 00:32:45", "link": "http://arxiv.org/abs/1804.09299v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Strong Baselines for Neural Semi-supervised Learning under Domain Shift", "abstract": "Novel neural models have been proposed in recent years for learning under\ndomain shift. Most models, however, only evaluate on a single task, on\nproprietary datasets, or compare to weak baselines, which makes comparison of\nmodels difficult. In this paper, we re-evaluate classic general-purpose\nbootstrapping approaches in the context of neural networks under domain shifts\nvs. recent neural approaches and propose a novel multi-task tri-training method\nthat reduces the time and space complexity of classic tri-training. Extensive\nexperiments on two benchmarks are negative: while our novel method establishes\na new state-of-the-art for sentiment analysis, it does not fare consistently\nthe best. More importantly, we arrive at the somewhat surprising conclusion\nthat classic tri-training, with some additions, outperforms the state of the\nart. We conclude that classic approaches constitute an important and strong\nbaseline.", "published": "2018-04-25 13:06:29", "link": "http://arxiv.org/abs/1804.09530v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-End Multimodal Speech Recognition", "abstract": "Transcription or sub-titling of open-domain videos is still a challenging\ndomain for Automatic Speech Recognition (ASR) due to the data's challenging\nacoustics, variable signal processing and the essentially unrestricted domain\nof the data. In previous work, we have shown that the visual channel --\nspecifically object and scene features -- can help to adapt the acoustic model\n(AM) and language model (LM) of a recognizer, and we are now expanding this\nwork to end-to-end approaches. In the case of a Connectionist Temporal\nClassification (CTC)-based approach, we retain the separation of AM and LM,\nwhile for a sequence-to-sequence (S2S) approach, both information sources are\nadapted together, in a single model. This paper also analyzes the behavior of\nCTC and S2S models on noisy video data (How-To corpus), and compares it to\nresults on the clean Wall Street Journal (WSJ) corpus, providing insight into\nthe robustness of both approaches.", "published": "2018-04-25 22:54:06", "link": "http://arxiv.org/abs/1804.09713v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Estimation with Low-Rank Time-Frequency Synthesis Models", "abstract": "Many state-of-the-art signal decomposition techniques rely on a low-rank\nfactorization of a time-frequency (t-f) transform. In particular, nonnegative\nmatrix factorization (NMF) of the spectrogram has been considered in many audio\napplications. This is an analysis approach in the sense that the factorization\nis applied to the squared magnitude of the analysis coefficients returned by\nthe t-f transform. In this paper we instead propose a synthesis approach, where\nlow-rankness is imposed to the synthesis coefficients of the data signal over a\ngiven t-f dictionary (such as a Gabor frame). As such we offer a novel modeling\nparadigm that bridges t-f synthesis modeling and traditional analysis-based NMF\napproaches. The proposed generative model allows in turn to design more\nsophisticated multi-layer representations that can efficiently capture diverse\nforms of structure. Additionally, the generative modeling allows to exploit t-f\nlow-rankness for compressive sensing. We present efficient iterative shrinkage\nalgorithms to perform estimation in the proposed models and illustrate the\ncapabilities of the new modeling paradigm over audio signal processing\nexamples.", "published": "2018-04-25 12:03:25", "link": "http://arxiv.org/abs/1804.09497v2", "categories": ["eess.SP", "cs.LG", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Speaker-independent raw waveform model for glottal excitation", "abstract": "Recent speech technology research has seen a growing interest in using\nWaveNets as statistical vocoders, i.e., generating speech waveforms from\nacoustic features. These models have been shown to improve the generated speech\nquality over classical vocoders in many tasks, such as text-to-speech synthesis\nand voice conversion. Furthermore, conditioning WaveNets with acoustic features\nallows sharing the waveform generator model across multiple speakers without\nadditional speaker codes. However, multi-speaker WaveNet models require large\namounts of training data and computation to cover the entire acoustic space.\nThis paper proposes leveraging the source-filter model of speech production to\nmore effectively train a speaker-independent waveform generator with limited\nresources. We present a multi-speaker 'GlotNet' vocoder, which utilizes a\nWaveNet to generate glottal excitation waveforms, which are then used to excite\nthe corresponding vocal tract filter to produce speech. Listening tests show\nthat the proposed model performs favourably to a direct WaveNet vocoder trained\nwith the same model architecture and data.", "published": "2018-04-25 14:29:08", "link": "http://arxiv.org/abs/1804.09593v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "t-DCF: a Detection Cost Function for the Tandem Assessment of Spoofing\n  Countermeasures and Automatic Speaker Verification", "abstract": "The ASVspoof challenge series was born to spearhead research in anti-spoofing\nfor automatic speaker verification (ASV). The two challenge editions in 2015\nand 2017 involved the assessment of spoofing countermeasures (CMs) in isolation\nfrom ASV using an equal error rate (EER) metric. While a strategic approach to\nassessment at the time, it has certain shortcomings. First, the CM EER is not\nnecessarily a reliable predictor of performance when ASV and CMs are combined.\nSecond, the EER operating point is ill-suited to user authentication\napplications, e.g. telephone banking, characterised by a high target user prior\nbut a low spoofing attack prior. We aim to migrate from CM- to ASV-centric\nassessment with the aid of a new tandem detection cost function (t-DCF) metric.\nIt extends the conventional DCF used in ASV research to scenarios involving\nspoofing attacks. The t-DCF metric has 6 parameters: (i) false alarm and miss\ncosts for both systems, and (ii) prior probabilities of target and spoof trials\n(with an implied third, nontarget prior). The study is intended to serve as a\nself-contained, tutorial-like presentation. We analyse with the t-DCF a\nselection of top-performing CM submissions to the 2015 and 2017 editions of\nASVspoof, with a focus on the spoofing attack prior. Whereas there is little to\nchoose between countermeasure systems for lower priors, system rankings derived\nwith the EER and t-DCF show differences for higher priors. We observe some\nranking changes. Findings support the adoption of the DCF-based metric into the\nroadmap for future ASVspoof challenges, and possibly for other biometric\nanti-spoofing evaluations.", "published": "2018-04-25 15:16:48", "link": "http://arxiv.org/abs/1804.09618v2", "categories": ["eess.AS", "cs.CR", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Off the Beaten Track: Using Deep Learning to Interpolate Between Music\n  Genres", "abstract": "We describe a system based on deep learning that generates drum patterns in\nthe electronic dance music domain. Experimental results reveal that generated\npatterns can be employed to produce musically sound and creative transitions\nbetween different genres, and that the process of generation is of interest to\npractitioners in the field.", "published": "2018-04-25 21:39:39", "link": "http://arxiv.org/abs/1804.09808v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutional Generative Adversarial Networks with Binary Neurons for\n  Polyphonic Music Generation", "abstract": "It has been shown recently that deep convolutional generative adversarial\nnetworks (GANs) can learn to generate music in the form of piano-rolls, which\nrepresent music by binary-valued time-pitch matrices. However, existing models\ncan only generate real-valued piano-rolls and require further post-processing,\nsuch as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final\nbinary-valued results. In this paper, we study whether we can have a\nconvolutional GAN model that directly creates binary-valued piano-rolls by\nusing binary neurons. Specifically, we propose to append to the generator an\nadditional refiner network, which uses binary neurons at the output layer. The\nwhole network is trained in two stages. Firstly, the generator and the\ndiscriminator are pretrained. Then, the refiner network is trained along with\nthe discriminator to learn to binarize the real-valued piano-rolls the\npretrained generator creates. Experimental results show that using binary\nneurons instead of HT or BS indeed leads to better results in a number of\nobjective measures. Moreover, deterministic binary neurons perform better than\nstochastic ones in both objective measures and a subjective test. The source\ncode, training data and audio examples of the generated results can be found at\nhttps://salu133445.github.io/bmusegan/ .", "published": "2018-04-25 07:35:39", "link": "http://arxiv.org/abs/1804.09399v3", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
