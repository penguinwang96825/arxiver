{"title": "Enhanced LSTM for Natural Language Inference", "abstract": "Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result---it further improves the performance even when\nadded to the already very strong model.", "published": "2016-09-20 06:59:31", "link": "http://arxiv.org/abs/1609.06038v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Quality Assessment for Speech Translation Using Joint ASR and\n  MT Features", "abstract": "This paper addresses automatic quality assessment of spoken language\ntranslation (SLT). This relatively new task is defined and formalized as a\nsequence labeling problem where each word in the SLT hypothesis is tagged as\ngood or bad according to a large feature set. We propose several word\nconfidence estimators (WCE) based on our automatic evaluation of transcription\n(ASR) quality, translation (MT) quality, or both (combined ASR+MT). This\nresearch work is possible because we built a specific corpus which contains\n6.7k utterances for which a quintuplet containing: ASR output, verbatim\ntranscript, text translation, speech translation and post-edition of\ntranslation is built. The conclusion of our multiple experiments using joint\nASR and MT features for WCE is that MT features remain the most influent while\nASR feature can bring interesting complementary information. Our robust quality\nestimators for SLT can be used for re-scoring speech translation graphs or for\nproviding feedback to the user in interactive speech translation or\ncomputer-assisted speech-to-text scenarios.", "published": "2016-09-20 08:04:33", "link": "http://arxiv.org/abs/1609.06049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Robust Representations of Text", "abstract": "Deep neural networks have achieved remarkable results across many language\nprocessing tasks, however these methods are highly sensitive to noise and\nadversarial attacks. We present a regularization based method for limiting\nnetwork sensitivity to its inputs, inspired by ideas from computer vision, thus\nlearning models that are more robust. Empirical evaluation over a range of\nsentiment datasets with a convolutional neural network shows that, compared to\na baseline model and the dropout method, our method achieves superior\nperformance over noisy inputs and out-of-domain data.", "published": "2016-09-20 10:23:47", "link": "http://arxiv.org/abs/1609.06082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Italy goes to Stanford: a collection of CoreNLP modules for Italian", "abstract": "In this we paper present Tint, an easy-to-use set of fast, accurate and\nextendable Natural Language Processing modules for Italian. It is based on\nStanford CoreNLP and is freely available as a standalone software or a library\nthat can be integrated in an existing project.", "published": "2016-09-20 14:53:05", "link": "http://arxiv.org/abs/1609.06204v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Politically-Relevant Event Data", "abstract": "Automatically generated political event data is an important part of the\nsocial science data ecosystem. The approaches for generating this data, though,\nhave remained largely the same for two decades. During this time, the field of\ncomputational linguistics has progressed tremendously. This paper presents an\noverview of political event data, including methods and ontologies, and a set\nof experiments to determine the applicability of deep neural networks to the\nextraction of political events from news text.", "published": "2016-09-20 16:19:54", "link": "http://arxiv.org/abs/1609.06239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A framework for mining process models from emails logs", "abstract": "Due to its wide use in personal, but most importantly, professional contexts,\nemail represents a valuable source of information that can be harvested for\nunderstanding, reengineering and repurposing undocumented business processes of\ncompanies and institutions. Towards this aim, a few researchers investigated\nthe problem of extracting process oriented information from email logs in order\nto take benefit of the many available process mining techniques and tools. In\nthis paper we go further in this direction, by proposing a new method for\nmining process models from email logs that leverage unsupervised machine\nlearning techniques with little human involvement. Moreover, our method allows\nto semi-automatically label emails with activity names, that can be used for\nactivity recognition in new incoming emails. A use case demonstrates the\nusefulness of the proposed solution using a modest in size, yet real-world,\ndataset containing emails that belong to two different process models.", "published": "2016-09-20 12:29:15", "link": "http://arxiv.org/abs/1609.06127v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity Strategies for Job Title Classification", "abstract": "Automatic and accurate classification of items enables numerous downstream\napplications in many domains. These applications can range from faceted\nbrowsing of items to product recommendations and big data analytics. In the\nonline recruitment domain, we refer to classifying job ads to pre-defined or\ncustom occupation categories as job title classification. A large-scale job\ntitle classification system can power various downstream applications such as\nsemantic search, job recommendations and labor market analytics. In this paper,\nwe discuss experiments conducted to improve our in-house job title\nclassification system. The classification component of the system is composed\nof a two-stage coarse and fine level classifier cascade that classifies input\ntext such as job title and/or job ads to one of the thousands of job titles in\nour taxonomy. To improve classification accuracy and effectiveness, we\nexperiment with various semantic representation strategies such as average W2V\nvectors and document similarity measures such as Word Movers Distance (WMD).\nOur initial results show an overall improvement in accuracy of Carotene[1].", "published": "2016-09-20 17:54:47", "link": "http://arxiv.org/abs/1609.06268v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural\n  Networks with Multi-Level Attention", "abstract": "Recognizing implicit discourse relations is a challenging but important task\nin the field of Natural Language Processing. For such a complex text processing\ntask, different from previous studies, we argue that it is necessary to\nrepeatedly read the arguments and dynamically exploit the efficient features\nuseful for recognizing discourse relations. To mimic the repeated reading\nstrategy, we propose the neural networks with multi-level attention (NNMA),\ncombining the attention mechanism and external memories to gradually fix the\nattention on some specific words helpful to judging the discourse relations.\nExperiments on the PDTB dataset show that our proposed method achieves the\nstate-of-art results. The visualization of the attention weights also\nillustrates the progress that our model observes the arguments on each level\nand progressively locates the important words.", "published": "2016-09-20 22:59:19", "link": "http://arxiv.org/abs/1609.06380v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
