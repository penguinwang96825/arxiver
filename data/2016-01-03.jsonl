{"title": "Contrastive Entropy: A new evaluation metric for unnormalized language\n  models", "abstract": "Perplexity (per word) is the most widely used metric for evaluating language\nmodels. Despite this, there has been no dearth of criticism for this metric.\nMost of these criticisms center around lack of correlation with extrinsic\nmetrics like word error rate (WER), dependence upon shared vocabulary for model\ncomparison and unsuitability for unnormalized language model evaluation. In\nthis paper, we address the last problem and propose a new discriminative\nentropy based intrinsic metric that works for both traditional word level\nmodels and unnormalized language models like sentence level models. We also\npropose a discriminatively trained sentence level interpretation of recurrent\nneural network based language model (RNN) as an example of unnormalized\nsentence level model. We demonstrate that for word level models, contrastive\nentropy shows a strong correlation with perplexity. We also observe that when\ntrained at lower distortion levels, sentence level RNN considerably outperforms\ntraditional RNNs on this new metric.", "published": "2016-01-03 05:47:42", "link": "http://arxiv.org/abs/1601.00248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
