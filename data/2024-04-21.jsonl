{"title": "IMO: Greedy Layer-Wise Sparse Representation Learning for\n  Out-of-Distribution Text Classification with Pre-trained Models", "abstract": "Machine learning models have made incredible progress, but they still\nstruggle when applied to examples from unseen domains. This study focuses on a\nspecific problem of domain generalization, where a model is trained on one\nsource domain and tested on multiple target domains that are unseen during\ntraining. We propose IMO: Invariant features Masks for Out-of-Distribution text\nclassification, to achieve OOD generalization by learning invariant features.\nDuring training, IMO would learn sparse mask layers to remove irrelevant\nfeatures for prediction, where the remaining features keep invariant.\nAdditionally, IMO has an attention module at the token level to focus on tokens\nthat are useful for prediction. Our comprehensive experiments show that IMO\nsubstantially outperforms strong baselines in terms of various evaluation\nmetrics and settings.", "published": "2024-04-21 02:15:59", "link": "http://arxiv.org/abs/2404.13504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E-QGen: Educational Lecture Abstract-based Question Generation System", "abstract": "To optimize the preparation process for educators in academic lectures and\nassociated question-and-answer sessions, this paper presents E-QGen, a lecture\nabstract-based question generation system. Given a lecture abstract, E-QGen\ngenerates potential student inquiries. The questions suggested by our system\nare expected to not only facilitate teachers in preparing answers in advance\nbut also enable them to supply additional resources when necessary.", "published": "2024-04-21 06:03:43", "link": "http://arxiv.org/abs/2404.13547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"A good pun is its own reword\": Can Large Language Models Understand\n  Puns?", "abstract": "Puns play a vital role in academic research due to their distinct structure\nand clear definition, which aid in the comprehensive analysis of linguistic\nhumor. However, the understanding of puns in large language models (LLMs) has\nnot been thoroughly examined, limiting their use in creative writing and humor\ncreation. In this paper, we leverage three popular tasks, i.e., pun\nrecognition, explanation and generation to systematically evaluate the\ncapabilities of LLMs in pun understanding. In addition to adopting the\nautomated evaluation metrics from prior research, we introduce new evaluation\nmethods and metrics that are better suited to the in-context learning paradigm\nof LLMs. These new metrics offer a more rigorous assessment of an LLM's ability\nto understand puns and align more closely with human cognition than previous\nmetrics. Our findings reveal the \"lazy pun generation\" pattern and identify the\nprimary challenges LLMs encounter in understanding puns.", "published": "2024-04-21 09:42:05", "link": "http://arxiv.org/abs/2404.13599v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEACH: Pretrained-embedding Explanation Across Contextual and\n  Hierarchical Structure", "abstract": "In this work, we propose a novel tree-based explanation technique, PEACH\n(Pretrained-embedding Explanation Across Contextual and Hierarchical\nStructure), that can explain how text-based documents are classified by using\nany pretrained contextual embeddings in a tree-based human-interpretable\nmanner. Note that PEACH can adopt any contextual embeddings of the PLMs as a\ntraining input for the decision tree. Using the proposed PEACH, we perform a\ncomprehensive analysis of several contextual embeddings on nine different NLP\ntext classification benchmarks. This analysis demonstrates the flexibility of\nthe model by applying several PLM contextual embeddings, its attribute\nselections, scaling, and clustering methods. Furthermore, we show the utility\nof explanations by visualising the feature selection and important trend of\ntext classification via human-interpretable word-cloud-based trees, which\nclearly identify model mistakes and assist in dataset debugging. Besides\ninterpretability, PEACH outperforms or is similar to those from pretrained\nmodels.", "published": "2024-04-21 12:41:02", "link": "http://arxiv.org/abs/2404.13645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trojan Detection in Large Language Models: Insights from The Trojan\n  Detection Challenge", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, but their vulnerability to trojan or backdoor attacks poses\nsignificant security risks. This paper explores the challenges and insights\ngained from the Trojan Detection Competition 2023 (TDC2023), which focused on\nidentifying and evaluating trojan attacks on LLMs. We investigate the\ndifficulty of distinguishing between intended and unintended triggers, as well\nas the feasibility of reverse engineering trojans in real-world scenarios. Our\ncomparative analysis of various trojan detection methods reveals that achieving\nhigh Recall scores is significantly more challenging than obtaining high\nReverse-Engineering Attack Success Rate (REASR) scores. The top-performing\nmethods in the competition achieved Recall scores around 0.16, comparable to a\nsimple baseline of randomly sampling sentences from a distribution similar to\nthe given training prefixes. This finding raises questions about the\ndetectability and recoverability of trojans inserted into the model, given only\nthe harmful targets. Despite the inability to fully solve the problem, the\ncompetition has led to interesting observations about the viability of trojan\ndetection and improved techniques for optimizing LLM input prompts. The\nphenomenon of unintended triggers and the difficulty in distinguishing them\nfrom intended triggers highlights the need for further research into the\nrobustness and interpretability of LLMs. The TDC2023 has provided valuable\ninsights into the challenges and opportunities associated with trojan detection\nin LLMs, laying the groundwork for future research in this area to ensure their\nsafety and reliability in real-world applications.", "published": "2024-04-21 13:31:16", "link": "http://arxiv.org/abs/2404.13660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embarrassingly Simple Unsupervised Aspect Based Sentiment Tuple\n  Extraction", "abstract": "Aspect Based Sentiment Analysis (ABSA) tasks involve the extraction of\nfine-grained sentiment tuples from sentences, aiming to discern the author's\nopinions. Conventional methodologies predominantly rely on supervised\napproaches; however, the efficacy of such methods diminishes in low-resource\ndomains lacking labeled datasets since they often lack the ability to\ngeneralize across domains. To address this challenge, we propose a simple and\nnovel unsupervised approach to extract opinion terms and the corresponding\nsentiment polarity for aspect terms in a sentence. Our experimental\nevaluations, conducted on four benchmark datasets, demonstrate compelling\nperformance to extract the aspect oriented opinion words as well as assigning\nsentiment polarity. Additionally, unsupervised approaches for opinion word\nmining have not been explored and our work establishes a benchmark for the\nsame.", "published": "2024-04-21 19:20:42", "link": "http://arxiv.org/abs/2404.13751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Encode Domain Information in Relation Classification", "abstract": "Current language models require a lot of training data to obtain high\nperformance. For Relation Classification (RC), many datasets are\ndomain-specific, so combining datasets to obtain better performance is\nnon-trivial. We explore a multi-domain training setup for RC, and attempt to\nimprove performance by encoding domain information. Our proposed models improve\n> 2 Macro-F1 against the baseline setup, and our analysis reveals that not all\nthe labels benefit the same: The classes which occupy a similar space across\ndomains (i.e., their interpretation is close across them, for example\n\"physical\") benefit the least, while domain-dependent relations (e.g.,\n\"part-of'') improve the most when encoding domain information.", "published": "2024-04-21 20:16:35", "link": "http://arxiv.org/abs/2404.13760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Adaptive Empathetic Responses for Teaching English", "abstract": "Existing English-teaching chatbots rarely incorporate empathy explicitly in\ntheir feedback, but empathetic feedback could help keep students engaged and\nreduce learner anxiety. Toward this end, we propose the task of negative\nemotion detection via audio, for recognizing empathetic feedback opportunities\nin language learning. We then build the first spoken English-teaching chatbot\nwith adaptive, empathetic feedback. This feedback is synthesized through\nautomatic prompt optimization of ChatGPT and is evaluated with English\nlearners. We demonstrate the effectiveness of our system through a preliminary\nuser study.", "published": "2024-04-21 20:21:24", "link": "http://arxiv.org/abs/2404.13764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lightweight Connective Detection Using Gradient Boosting", "abstract": "In this work, we introduce a lightweight discourse connective detection\nsystem. Employing gradient boosting trained on straightforward, low-complexity\nfeatures, this proposed approach sidesteps the computational demands of the\ncurrent approaches that rely on deep neural networks. Considering its\nsimplicity, our approach achieves competitive results while offering\nsignificant gains in terms of time even on CPU. Furthermore, the stable\nperformance across two unrelated languages suggests the robustness of our\nsystem in the multilingual scenario. The model is designed to support the\nannotation of discourse relations, particularly in scenarios with limited\nresources, while minimizing performance loss.", "published": "2024-04-21 23:14:02", "link": "http://arxiv.org/abs/2404.13793v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust\n  Conversational Dense Retrieval", "abstract": "Conversational search requires accurate interpretation of user intent from\ncomplex multi-turn contexts. This paper presents ChatRetriever, which inherits\nthe strong generalization capability of large language models to robustly\nrepresent complex conversational sessions for dense retrieval. To achieve this,\nwe propose a simple and effective dual-learning approach that adapts LLM for\nretrieval via contrastive learning while enhancing the complex session\nunderstanding through masked instruction tuning on high-quality conversational\ninstruction tuning data. Extensive experiments on five conversational search\nbenchmarks demonstrate that ChatRetriever substantially outperforms existing\nconversational dense retrievers, achieving state-of-the-art performance on par\nwith LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits\nsuperior robustness in handling diverse conversational contexts. Our work\nhighlights the potential of adapting LLMs for retrieval with complex inputs\nlike conversational search sessions and proposes an effective approach to\nadvance this research direction.", "published": "2024-04-21 07:03:55", "link": "http://arxiv.org/abs/2404.13556v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Video sentence grounding with temporally global textual knowledge", "abstract": "Temporal sentence grounding involves the retrieval of a video moment with a\nnatural language query. Many existing works directly incorporate the given\nvideo and temporally localized query for temporal grounding, overlooking the\ninherent domain gap between different modalities. In this paper, we utilize\npseudo-query features containing extensive temporally global textual knowledge\nsourced from the same video-query pair, to enhance the bridging of domain gaps\nand attain a heightened level of similarity between multi-modal features.\nSpecifically, we propose a Pseudo-query Intermediary Network (PIN) to achieve\nan improved alignment of visual and comprehensive pseudo-query features within\nthe feature space through contrastive learning. Subsequently, we utilize\nlearnable prompts to encapsulate the knowledge of pseudo-queries, propagating\nthem into the textual encoder and multi-modal fusion module, further enhancing\nthe feature alignment between visual and language for better temporal\ngrounding. Extensive experiments conducted on the Charades-STA and\nActivityNet-Captions datasets demonstrate the effectiveness of our method.", "published": "2024-04-21 10:41:04", "link": "http://arxiv.org/abs/2404.13611v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Branch Not Taken: Predicting Branching in Online Conversations", "abstract": "Multi-participant discussions tend to unfold in a tree structure rather than\na chain structure. Branching may occur for multiple reasons -- from the\nasynchronous nature of online platforms to a conscious decision by an\ninterlocutor to disengage with part of the conversation. Predicting branching\nand understanding the reasons for creating new branches is important for many\ndownstream tasks such as summarization and thread disentanglement and may help\ndevelop online spaces that encourage users to engage in online discussions in\nmore meaningful ways. In this work, we define the novel task of branch\nprediction and propose GLOBS (Global Branching Score) -- a deep neural network\nmodel for predicting branching. GLOBS is evaluated on three large discussion\nforums from Reddit, achieving significant improvements over an array of\ncompetitive baselines and demonstrating better transferability. We affirm that\nstructural, temporal, and linguistic features contribute to GLOBS success and\nfind that branching is associated with a greater number of conversation\nparticipants and tends to occur in earlier levels of the conversation tree. We\npublicly release GLOBS and our implementation of all baseline models to allow\nreproducibility and promote further research on this important task.", "published": "2024-04-21 10:49:41", "link": "http://arxiv.org/abs/2404.13613v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on\n  Negotiation Surrounding", "abstract": "Large Language Models (LLMs) have sparked substantial interest and debate\nconcerning their potential emergence of Theory of Mind (ToM) ability. Theory of\nmind evaluations currently focuses on testing models using machine-generated\ndata or game settings prone to shortcuts and spurious correlations, which lacks\nevaluation of machine ToM ability in real-world human interaction scenarios.\nThis poses a pressing demand to develop new real-world scenario benchmarks. We\nintroduce NegotiationToM, a new benchmark designed to stress-test machine ToM\nin real-world negotiation surrounding covered multi-dimensional mental states\n(i.e., desires, beliefs, and intentions). Our benchmark builds upon the\nBelief-Desire-Intention (BDI) agent modeling theory and conducts the necessary\nempirical experiments to evaluate large language models. Our findings\ndemonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as\nthey consistently perform significantly worse than humans, even when employing\nthe chain-of-thought (CoT) method.", "published": "2024-04-21 11:51:13", "link": "http://arxiv.org/abs/2404.13627v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Different Verbal Cues to Improve Text-Based\n  Computer-Delivered Health Messaging", "abstract": "The ubiquity of smartphones has led to an increase in on demand healthcare\nbeing supplied. For example, people can share their illness-related experiences\nwith others similar to themselves, and healthcare experts can offer advice for\nbetter treatment and care for remediable, terminal and mental illnesses. As\nwell as this human-to-human communication, there has been an increased use of\nhuman-to-computer digital health messaging, such as chatbots. These can prove\nadvantageous as they offer synchronous and anonymous feedback without the need\nfor a human conversational partner. However, there are many subtleties involved\nin human conversation that a computer agent may not properly exhibit. For\nexample, there are various conversational styles, etiquettes, politeness\nstrategies or empathic responses that need to be chosen appropriately for the\nconversation. Encouragingly, computers are social actors (CASA) posits that\npeople apply the same social norms to computers as they would do to people. On\nfrom this, previous studies have focused on applying conversational strategies\nto computer agents to make them embody more favourable human characteristics.\nHowever, if a computer agent fails in this regard it can lead to negative\nreactions from users. Therefore, in this dissertation we describe a series of\nstudies we carried out to lead to more effective human-to-computer digital\nhealth messaging.\n  In our first study, we use the crowd [...]\n  Our second study investigates the effect of a health chatbot's conversational\nstyle [...]\n  In our final study, we investigate the format used by a chatbot when [...]\n  In summary, we have researched how to create more effective digital health\ninterventions starting from generating health messages, to choosing an\nappropriate formality of messaging, and finally to formatting messages which\nreference a user's previous utterances.", "published": "2024-04-21 12:15:44", "link": "http://arxiv.org/abs/2404.13633v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "The Framework of a Design Process Language", "abstract": "The thesis develops a view of design in a concept formation framework and\noutlines a language to describe both the object of the design and the process\nof designing. The unknown object at the outset of the design work may be seen\nas an unknown concept that the designer is to define. Throughout the process,\nshe develops a description of this object by relating it to known concepts. The\nsearch stops when the designer is satisfied that the design specification is\ncomplete enough to satisfy the requirements from it once built. It is then a\ncollection of propositions that all contribute towards defining the design\nobject - a collection of sentences describing relationships between the object\nand known concepts. Also, the design process itself may be described by\nrelating known concepts - by organizing known abilities into particular\npatterns of activation, or mobilization. In view of the demands posed to a\nlanguage to use in this concept formation process, the framework of a Design\nProcess Language (DPL) is developed. The basis for the language are linguistic\ncategories that act as classes of relations used to combine concepts,\ncontaining relations used for describing process and object within the same\ngeneral system, with some relations being process specific, others being object\nspecific, and with the bulk being used both for process and object description.\nAnother outcome is the distinction of modal relations, or relations describing\nfuturity, possibility, willingness, hypothetical events, and the like. The\ndesign process almost always includes aspects such as these, and it is thus\nnecessary for a language facilitating design process description to support\nsuch relationships to be constructed. The DPL is argued to be a foundation\nwhereupon to build a language that can be used for enabling computers to be\nmore useful - act more intelligently - in the design process.", "published": "2024-04-21 17:20:19", "link": "http://arxiv.org/abs/2404.13721v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Automated Text Mining of Experimental Methodologies from Biomedical\n  Literature", "abstract": "Biomedical literature is a rapidly expanding field of science and technology.\nClassification of biomedical texts is an essential part of biomedicine\nresearch, especially in the field of biology. This work proposes the fine-tuned\nDistilBERT, a methodology-specific, pre-trained generative classification\nlanguage model for mining biomedicine texts. The model has proven its\neffectiveness in linguistic understanding capabilities and has reduced the size\nof BERT models by 40\\% but by 60\\% faster. The main objective of this project\nis to improve the model and assess the performance of the model compared to the\nnon-fine-tuned model. We used DistilBert as a support model and pre-trained on\na corpus of 32,000 abstracts and complete text articles; our results were\nimpressive and surpassed those of traditional literature classification methods\nby using RNN or LSTM. Our aim is to integrate this highly specialised and\nspecific model into different research industries.", "published": "2024-04-21 21:19:36", "link": "http://arxiv.org/abs/2404.13779v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation", "abstract": "Evaluating retrieval-augmented generation (RAG) presents challenges,\nparticularly for retrieval models within these systems. Traditional end-to-end\nevaluation methods are computationally expensive. Furthermore, evaluation of\nthe retrieval model's performance based on query-document relevance labels\nshows a small correlation with the RAG system's downstream performance. We\npropose a novel evaluation approach, eRAG, where each document in the retrieval\nlist is individually utilized by the large language model within the RAG\nsystem. The output generated for each document is then evaluated based on the\ndownstream task ground truth labels. In this manner, the downstream performance\nfor each document serves as its relevance label. We employ various downstream\ntask metrics to obtain document-level annotations and aggregate them using\nset-based or ranking metrics. Extensive experiments on a wide range of datasets\ndemonstrate that eRAG achieves a higher correlation with downstream RAG\nperformance compared to baseline methods, with improvements in Kendall's $\\tau$\ncorrelation ranging from 0.168 to 0.494. Additionally, eRAG offers significant\ncomputational advantages, improving runtime and consuming up to 50 times less\nGPU memory than end-to-end evaluation.", "published": "2024-04-21 21:22:28", "link": "http://arxiv.org/abs/2404.13781v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reinforcement of Explainability of ChatGPT Prompts by Embedding Breast\n  Cancer Self-Screening Rules into AI Responses", "abstract": "Addressing the global challenge of breast cancer, this research explores the\nfusion of generative AI, focusing on ChatGPT 3.5 turbo model, and the\nintricacies of breast cancer risk assessment. The research aims to evaluate\nChatGPT's reasoning capabilities, emphasizing its potential to process rules\nand provide explanations for screening recommendations. The study seeks to\nbridge the technology gap between intelligent machines and clinicians by\ndemonstrating ChatGPT's unique proficiency in natural language reasoning. The\nmethodology employs a supervised prompt-engineering approach to enforce\ndetailed explanations for ChatGPT's recommendations. Synthetic use cases,\ngenerated algorithmically, serve as the testing ground for the encoded rules,\nevaluating the model's processing prowess. Findings highlight ChatGPT's\npromising capacity in processing rules comparable to Expert System Shells, with\na focus on natural language reasoning. The research introduces the concept of\nreinforcement explainability, showcasing its potential in elucidating outcomes\nand facilitating user-friendly interfaces for breast cancer risk assessment.", "published": "2024-04-21 09:20:16", "link": "http://arxiv.org/abs/2404.14454v2", "categories": ["cs.CL", "cs.AI", "I.2; I.2.1"], "primary_category": "cs.CL"}
{"title": "Parameter Efficient Fine Tuning: A Comprehensive Analysis Across\n  Applications", "abstract": "The rise of deep learning has marked significant progress in fields such as\ncomputer vision, natural language processing, and medical imaging, primarily\nthrough the adaptation of pre-trained models for specific tasks. Traditional\nfine-tuning methods, involving adjustments to all parameters, face challenges\ndue to high computational and memory demands. This has led to the development\nof Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update\nparameters to balance computational efficiency with performance. This review\nexamines PEFT approaches, offering a detailed comparison of various strategies\nhighlighting applications across different domains, including text generation,\nmedical imaging, protein modeling, and speech synthesis. By assessing the\neffectiveness of PEFT methods in reducing computational load, speeding up\ntraining, and lowering memory usage, this paper contributes to making deep\nlearning more accessible and adaptable, facilitating its wider application and\nencouraging innovation in model optimization. Ultimately, the paper aims to\ncontribute towards insights into PEFT's evolving landscape, guiding researchers\nand practitioners in overcoming the limitations of conventional fine-tuning\napproaches.", "published": "2024-04-21 02:26:15", "link": "http://arxiv.org/abs/2404.13506v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Listen Then See: Video Alignment with Speaker Attention", "abstract": "Video-based Question Answering (Video QA) is a challenging task and becomes\neven more intricate when addressing Socially Intelligent Question Answering\n(SIQA). SIQA requires context understanding, temporal reasoning, and the\nintegration of multimodal information, but in addition, it requires processing\nnuanced human behavior. Furthermore, the complexities involved are exacerbated\nby the dominance of the primary modality (text) over the others. Thus, there is\na need to help the task's secondary modalities to work in tandem with the\nprimary modality. In this work, we introduce a cross-modal alignment and\nsubsequent representation fusion approach that achieves state-of-the-art\nresults (82.06\\% accuracy) on the Social IQ 2.0 dataset for SIQA. Our approach\nexhibits an improved ability to leverage the video modality by using the audio\nmodality as a bridge with the language modality. This leads to enhanced\nperformance by reducing the prevalent issue of language overfitting and\nresultant video modality bypassing encountered by current existing techniques.\nOur code and models are publicly available at\nhttps://github.com/sts-vlcc/sts-vlcc", "published": "2024-04-21 04:55:13", "link": "http://arxiv.org/abs/2404.13530v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exploring Diverse Methods in Visual Question Answering", "abstract": "This study explores innovative methods for improving Visual Question\nAnswering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and\nattention mechanisms. Leveraging a balanced VQA dataset, we investigate three\ndistinct strategies. Firstly, GAN-based approaches aim to generate answer\nembeddings conditioned on image and question inputs, showing potential but\nstruggling with more complex tasks. Secondly, autoencoder-based techniques\nfocus on learning optimal embeddings for questions and images, achieving\ncomparable results with GAN due to better ability on complex questions. Lastly,\nattention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),\naddress language priors and attention modeling, albeit with a\ncomplexity-performance trade-off. This study underscores the challenges and\nopportunities in VQA and suggests avenues for future research, including\nalternative GAN formulations and attentional mechanisms.", "published": "2024-04-21 07:34:44", "link": "http://arxiv.org/abs/2404.13565v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Mixture of LoRA Experts", "abstract": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained\nmodels to cater to a diverse array of downstream tasks, showcasing notable\neffectiveness and efficiency, thereby solidifying its position as one of the\nmost prevalent fine-tuning techniques. Due to the modular nature of LoRA's\nplug-and-play plugins, researchers have delved into the amalgamation of\nmultiple LoRAs to empower models to excel across various downstream tasks.\nNonetheless, extant approaches for LoRA fusion grapple with inherent\nchallenges. Direct arithmetic merging may result in the loss of the original\npre-trained model's generative capabilities or the distinct identity of LoRAs,\nthereby yielding suboptimal outcomes. On the other hand, Reference tuning-based\nfusion exhibits limitations concerning the requisite flexibility for the\neffective combination of multiple LoRAs. In response to these challenges, this\npaper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses\nhierarchical control and unfettered branch selection. The MoLE approach not\nonly achieves superior LoRA fusion performance in comparison to direct\narithmetic merging but also retains the crucial flexibility for combining LoRAs\neffectively. Extensive experimental evaluations conducted in both the Natural\nLanguage Processing (NLP) and Vision & Language (V&L) domains substantiate the\nefficacy of MoLE.", "published": "2024-04-21 11:59:53", "link": "http://arxiv.org/abs/2404.13628v1", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Utilizing Deep Learning to Optimize Software Development Processes", "abstract": "This study explores the application of deep learning technologies in software\ndevelopment processes, particularly in automating code reviews, error\nprediction, and test generation to enhance code quality and development\nefficiency. Through a series of empirical studies, experimental groups using\ndeep learning tools and control groups using traditional methods were compared\nin terms of code error rates and project completion times. The results\ndemonstrated significant improvements in the experimental group, validating the\neffectiveness of deep learning technologies. The research also discusses\npotential optimization points, methodologies, and technical challenges of deep\nlearning in software development, as well as how to integrate these\ntechnologies into existing software development workflows.", "published": "2024-04-21 12:06:05", "link": "http://arxiv.org/abs/2404.13630v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Iteratively Prompting Multimodal LLMs to Reproduce Natural and\n  AI-Generated Images", "abstract": "With the digital imagery landscape rapidly evolving, image stocks and\nAI-generated image marketplaces have become central to visual media.\nTraditional stock images now exist alongside innovative platforms that trade in\nprompts for AI-generated visuals, driven by sophisticated APIs like DALL-E 3\nand Midjourney. This paper studies the possibility of employing multi-modal\nmodels with enhanced visual understanding to mimic the outputs of these\nplatforms, introducing an original attack strategy. Our method leverages\nfine-tuned CLIP models, a multi-label classifier, and the descriptive\ncapabilities of GPT-4V to create prompts that generate images similar to those\navailable in marketplaces and from premium stock image providers, yet at a\nmarkedly lower expense. In presenting this strategy, we aim to spotlight a new\nclass of economic and security considerations within the realm of digital\nimagery. Our findings, supported by both automated metrics and human\nassessment, reveal that comparable visual content can be produced for a\nfraction of the prevailing market prices ($0.23 - $0.27 per image), emphasizing\nthe need for awareness and strategic discussions about the integrity of digital\nmedia in an increasingly AI-integrated landscape. Our work also contributes to\nthe field by assembling a dataset consisting of approximately 19 million\nprompt-image pairs generated by the popular Midjourney platform, which we plan\nto release publicly.", "published": "2024-04-21 21:30:17", "link": "http://arxiv.org/abs/2404.13784v1", "categories": ["cs.CR", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "Counterfactual Reasoning Using Predicted Latent Personality Dimensions\n  for Optimizing Persuasion Outcome", "abstract": "Customizing persuasive conversations related to the outcome of interest for\nspecific users achieves better persuasion results. However, existing persuasive\nconversation systems rely on persuasive strategies and encounter challenges in\ndynamically adjusting dialogues to suit the evolving states of individual users\nduring interactions. This limitation restricts the system's ability to deliver\nflexible or dynamic conversations and achieve suboptimal persuasion outcomes.\nIn this paper, we present a novel approach that tracks a user's latent\npersonality dimensions (LPDs) during ongoing persuasion conversation and\ngenerates tailored counterfactual utterances based on these LPDs to optimize\nthe overall persuasion outcome. In particular, our proposed method leverages a\nBi-directional Generative Adversarial Network (BiCoGAN) in tandem with a\nDialogue-based Personality Prediction Regression (DPPR) model to generate\ncounterfactual data. This enables the system to formulate alternative\npersuasive utterances that are more suited to the user. Subsequently, we\nutilize the D3QN model to learn policies for optimized selection of system\nutterances on counterfactual data. Experimental results we obtained from using\nthe PersuasionForGood dataset demonstrate the superiority of our approach over\nthe existing method, BiCoGAN. The cumulative rewards and Q-values produced by\nour method surpass ground truth benchmarks, showcasing the efficacy of\nemploying counterfactual reasoning and LPDs to optimize reinforcement learning\npolicy in online interactions.", "published": "2024-04-21 23:03:47", "link": "http://arxiv.org/abs/2404.13792v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.MM"}
{"title": "EPI-SQL: Enhancing Text-to-SQL Translation with Error-Prevention\n  Instructions", "abstract": "The conversion of natural language queries into SQL queries, known as\nText-to-SQL, is a critical yet challenging task. This paper introduces EPI-SQL,\na novel methodological framework leveraging Large Language Models (LLMs) to\nenhance the performance of Text-to-SQL tasks. EPI-SQL operates through a\nfour-step process. Initially, the method involves gathering instances from the\nSpider dataset on which LLMs are prone to failure. These instances are then\nutilized to generate general error-prevention instructions (EPIs).\nSubsequently, LLMs craft contextualized EPIs tailored to the specific context\nof the current task. Finally, these context-specific EPIs are incorporated into\nthe prompt used for SQL generation. EPI-SQL is distinguished in that it\nprovides task-specific guidance, enabling the model to circumvent potential\nerrors for the task at hand. Notably, the methodology rivals the performance of\nadvanced few-shot methods despite being a zero-shot approach. An empirical\nassessment using the Spider benchmark reveals that EPI-SQL achieves an\nexecution accuracy of 85.1\\%, underscoring its effectiveness in generating\naccurate SQL queries through LLMs. The findings indicate a promising direction\nfor future research, i.e. enhancing instructions with task-specific and\ncontextualized rules, for boosting LLMs' performance in NLP tasks.", "published": "2024-04-21 03:52:46", "link": "http://arxiv.org/abs/2404.14453v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied\n  Instruction Following", "abstract": "Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks.", "published": "2024-04-21 08:10:20", "link": "http://arxiv.org/abs/2404.15190v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "68T01 (Primary) 68T40, 68T50, 68T45 (Secondary)"], "primary_category": "cs.AI"}
{"title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs", "abstract": "While recently Large Language Models (LLMs) have achieved remarkable\nsuccesses, they are vulnerable to certain jailbreaking attacks that lead to\ngeneration of inappropriate or harmful content. Manual red-teaming requires\nfinding adversarial prompts that cause such jailbreaking, e.g. by appending a\nsuffix to a given instruction, which is inefficient and time-consuming. On the\nother hand, automatic adversarial prompt generation often leads to semantically\nmeaningless attacks that can easily be detected by perplexity-based filters,\nmay require gradient information from the TargetLLM, or do not scale well due\nto time-consuming discrete optimization processes over the token space. In this\npaper, we present a novel method that uses another LLM, called the AdvPrompter,\nto generate human-readable adversarial prompts in seconds, $\\sim800\\times$\nfaster than existing optimization-based approaches. We train the AdvPrompter\nusing a novel algorithm that does not require access to the gradients of the\nTargetLLM. This process alternates between two steps: (1) generating\nhigh-quality target adversarial suffixes by optimizing the AdvPrompter\npredictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated\nadversarial suffixes. The trained AdvPrompter generates suffixes that veil the\ninput instruction without changing its meaning, such that the TargetLLM is\nlured to give a harmful response. Experimental results on popular open source\nTargetLLMs show state-of-the-art results on the AdvBench dataset, that also\ntransfer to closed-source black-box LLM APIs. Further, we demonstrate that by\nfine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made\nmore robust against jailbreaking attacks while maintaining performance, i.e.\nhigh MMLU scores.", "published": "2024-04-21 22:18:13", "link": "http://arxiv.org/abs/2404.16873v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Stream State-tying for Sign Language Recognition", "abstract": "In this paper, a novel approach to sign language recognition based on state\ntying in each of data streams is presented. In this framework, it is assumed\nthat hand gesture signal is represented in terms of six synchronous data\nstreams, i.e., the left/right hand position, left/right hand orientation and\nleft/right handshape. This approach offers a very accurate representation of\nthe sign space and keeps the number of parameters reasonably small in favor of\na fast decoding. Experiments were carried out for 5177 Chinese signs. The real\ntime isolated recognition rate is 94.8%. For continuous sign recognition, the\nword correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign\nlanguage translation; Hand gesture recognition; Hidden Markov models;\nState-tying; Multimodal user interface; Virtual reality; Man-machine systems.", "published": "2024-04-21 23:21:52", "link": "http://arxiv.org/abs/2407.10975v1", "categories": ["cs.OH", "cs.AI", "cs.CL"], "primary_category": "cs.OH"}
{"title": "Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models", "abstract": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.", "published": "2024-04-21 19:24:15", "link": "http://arxiv.org/abs/2404.13752v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "primary_category": "cs.LG"}
{"title": "AudioRepInceptionNeXt: A lightweight single-stream architecture for\n  efficient audio recognition", "abstract": "Recent research has successfully adapted vision-based convolutional neural\nnetwork (CNN) architectures for audio recognition tasks using Mel-Spectrograms.\nHowever, these CNNs have high computational costs and memory requirements,\nlimiting their deployment on low-end edge devices. Motivated by the success of\nefficient vision models like InceptionNeXt and ConvNeXt, we propose\nAudioRepInceptionNeXt, a single-stream architecture. Its basic building block\nbreaks down the parallel multi-branch depth-wise convolutions with descending\nscales of k x k kernels into a cascade of two multi-branch depth-wise\nconvolutions. The first multi-branch consists of parallel multi-scale 1 x k\ndepth-wise convolutional layers followed by a similar multi-branch employing\nparallel multi-scale k x 1 depth-wise convolutional layers. This reduces\ncomputational and memory footprint while separating time and frequency\nprocessing of Mel-Spectrograms. The large kernels capture global frequencies\nand long activities, while small kernels get local frequencies and short\nactivities. We also reparameterize the multi-branch design during inference to\nfurther boost speed without losing accuracy. Experiments show that\nAudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves\ninference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while\nmaintaining comparable accuracy. It also learns robustly across a variety of\naudio recognition tasks. Codes are available at\nhttps://github.com/StevenLauHKHK/AudioRepInceptionNeXt.", "published": "2024-04-21 06:33:04", "link": "http://arxiv.org/abs/2404.13551v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sparse Direction of Arrival Estimation Method Based on Vector Signal\n  Reconstruction with a Single Vector Sensor", "abstract": "This study investigates the application of single vector hydrophones in\nunderwater acoustic signal processing for Direction of Arrival (DOA)\nestimation. Addressing the limitations of traditional DOA estimation methods in\nmulti-source environments and under noise interference, this research proposes\na Vector Signal Reconstruction (VSR) technique. This technique transforms the\ncovariance matrix of single vector hydrophone signals into a Toeplitz structure\nsuitable for gridless sparse methods through complex calculations and vector\nsignal reconstruction. Furthermore, two sparse DOA estimation algorithms based\non vector signal reconstruction are introduced. Theoretical analysis and\nsimulation experiments demonstrate that the proposed algorithms significantly\nimprove the accuracy and resolution of DOA estimation in multi-source signals\nand low Signal-to-Noise Ratio (SNR) environments compared to traditional\nalgorithms. The contribution of this study lies in providing an effective new\nmethod for DOA estimation with single vector hydrophones in complex\nenvironments, introducing new research directions and solutions in the field of\nvector hydrophone signal processing.", "published": "2024-04-21 08:10:34", "link": "http://arxiv.org/abs/2404.13568v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical Word Embedding for Music Tagging and Retrieval", "abstract": "Word embedding has become an essential means for text-based information\nretrieval. Typically, word embeddings are learned from large quantities of\ngeneral and unstructured text data. However, in the domain of music, the word\nembedding may have difficulty understanding musical contexts or recognizing\nmusic-related entities like artists and tracks. To address this issue, we\npropose a new approach called Musical Word Embedding (MWE), which involves\nlearning from various types of texts, including both everyday and music-related\nvocabulary. We integrate MWE into an audio-word joint representation framework\nfor tagging and retrieving music, using words like tag, artist, and track that\nhave different levels of musical specificity. Our experiments show that using a\nmore specific musical word like track results in better retrieval performance,\nwhile using a less specific term like tag leads to better tagging performance.\nTo balance this compromise, we suggest multi-prototype training that uses words\nwith different levels of musical specificity jointly. We evaluate both word\nembedding and audio-word joint embedding on four tasks (tag rank prediction,\nmusic tagging, query-by-tag, and query-by-track) across two datasets (Million\nSong Dataset and MTG-Jamendo). Our findings show that the suggested MWE is more\nefficient and robust than the conventional word embedding.", "published": "2024-04-21 08:19:20", "link": "http://arxiv.org/abs/2404.13569v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MFHCA: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion and\n  Hierarchical Cooperative Attention", "abstract": "Speech emotion recognition is crucial in human-computer interaction, but\nextracting and using emotional cues from audio poses challenges. This paper\nintroduces MFHCA, a novel method for Speech Emotion Recognition using\nMulti-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and\nraw audio. We employ the Multi-Spatial Fusion module (MF) to efficiently\nidentify emotion-related spectrogram regions and integrate Hubert features for\nhigher-level acoustic information. Our approach also includes a Hierarchical\nCooperative Attention module (HCA) to merge features from various auditory\nlevels. We evaluate our method on the IEMOCAP dataset and achieve 2.6\\% and\n1.87\\% improvements on the weighted accuracy and unweighted accuracy,\nrespectively. Extensive experiments demonstrate the effectiveness of the\nproposed method.", "published": "2024-04-21 02:44:17", "link": "http://arxiv.org/abs/2404.13509v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anchor-aware Deep Metric Learning for Audio-visual Retrieval", "abstract": "Metric learning minimizes the gap between similar (positive) pairs of data\npoints and increases the separation of dissimilar (negative) pairs, aiming at\ncapturing the underlying data structure and enhancing the performance of tasks\nlike audio-visual cross-modal retrieval (AV-CMR). Recent works employ sampling\nmethods to select impactful data points from the embedding space during\ntraining. However, the model training fails to fully explore the space due to\nthe scarcity of training data points, resulting in an incomplete representation\nof the overall positive and negative distributions. In this paper, we propose\nan innovative Anchor-aware Deep Metric Learning (AADML) method to address this\nchallenge by uncovering the underlying correlations among existing data points,\nwhich enhances the quality of the shared embedding space. Specifically, our\nmethod establishes a correlation graph-based manifold structure by considering\nthe dependencies between each sample as the anchor and its semantically similar\nsamples. Through dynamic weighting of the correlations within this underlying\nmanifold structure using an attention-driven mechanism, Anchor Awareness (AA)\nscores are obtained for each anchor. These AA scores serve as data proxies to\ncompute relative distances in metric learning approaches. Extensive experiments\nconducted on two audio-visual benchmark datasets demonstrate the effectiveness\nof our proposed AADML method, significantly surpassing state-of-the-art models.\nFurthermore, we investigate the integration of AA proxies with various metric\nlearning methods, further highlighting the efficacy of our approach.", "published": "2024-04-21 22:44:44", "link": "http://arxiv.org/abs/2404.13789v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
